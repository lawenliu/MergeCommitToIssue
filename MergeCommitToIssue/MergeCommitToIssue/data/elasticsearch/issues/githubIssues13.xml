<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>renames: ElasticSearchIntegrationTest/SingleNodeTest -&gt; TestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10659</link><project id="" key="" /><description>I wanted to do this in #10656, but lets do it as a followup, so we don't have to fight git's crappy merging on that issue.
</description><key id="69422819">10659</key><summary>renames: ElasticSearchIntegrationTest/SingleNodeTest -&gt; TestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-19T13:06:06Z</created><updated>2015-08-04T00:51:08Z</updated><resolved>2015-08-04T00:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Shield - Circular Dependency - start failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10658</link><project id="" key="" /><description>Based on the latest version of shield and ES,
These other plugins are used :
   loaded [WebPlugin, license, shield, mapper-attachments, QuartzPlugin], sites [HQ]

But the start of ES fails:

[2015-04-17 16:47:10,470][INFO ][http                     ] [Node1] Using [org.elasticsearch.shield.transport.netty.ShieldNettyHttpServerTransport] as http transport, overridden by [shield]
{1.5.1}: Initialization Failed ...
1) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]2) Tried proxying org.elasticsearch.transport.TransportService to support a circular dependency, but it is not an interface.
</description><key id="69403554">10658</key><summary>Shield - Circular Dependency - start failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienlust</reporter><labels /><created>2015-04-19T09:15:40Z</created><updated>2015-04-25T16:26:48Z</updated><resolved>2015-04-25T15:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T15:15:41Z" id="96220731">Hi @damienlust 

I'm guessing the clash is with the WebPlugin?  Given that (a) we have no control over that plugin and (b) rivers are deprecated, I don't think there is much we can do to fix this.
</comment><comment author="damienlust" created="2015-04-25T15:54:21Z" id="96231753">Thanks for the feedback,
What's the alternative to crawl a website?
The river webplugin seems the most popular?
Thanks
</comment><comment author="clintongormley" created="2015-04-25T16:16:22Z" id="96233080">@damienlust I'm not sure. I had a look for a suitable logstash plugin, but can't find one. Really, you can use any web scraping framework, because it runs outside Elasticsearch.
</comment><comment author="dadoonet" created="2015-04-25T16:26:48Z" id="96233920">I heard about some efforts around nutch 2.
And some plugins like https://github.com/duffj/nutch-elasticsearch

Never tested it though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix broken download link.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10657</link><project id="" key="" /><description>Current download link gives 404.
</description><key id="69393487">10657</key><summary>Fix broken download link.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GauravButola</reporter><labels><label>non-issue</label></labels><created>2015-04-19T06:52:38Z</created><updated>2015-04-24T19:28:37Z</updated><resolved>2015-04-24T19:28:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-24T19:28:27Z" id="96042130">I fixed this already in 1ae87ca4a2ccc1fe3e163b49849172eb6f691358 - thanks for the PR still!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integrate better with lucene test framework and mockfilesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10656</link><project id="" key="" /><description>A few of the changes:
- single `REPRODUCE WITH:` and exception
- fail the build if you typo the test name from mvn.
- clean up output and stacktraces to work better with IDE integration and easier to debug.
- fail tests if they leak file handles.
- fail tests if they use an extreme number of file handles (&gt; 2k)
- simulate windows behavior randomly (no deletion of open files) for the filesystem.
- add extra files to simulate .DS_Store/thumbs.db type behavior.
- running tests with `-Dtests.verbose=true` shows all destructive operations to the filesystem and indexwriter streams.
- parallellized rest tests.
- more randomization/asserting consistently via lucenetestcase
- refactor base test classes (including removing a few) for consistency.
- fix some test reproducibility problems.
- speed up execution of full test suite (~ 6:45 for full test run versus ~ 9:00 on master for me)
- mark slower tests with `@Slow` annotation, default `tests.slow` to false.

The last change speeds up junit times to ~ 3:15 for a test run, which gives most bang for the buck: still 73% coverage vs 79% with tests.slow=true, but a good deal faster, and you can always turn on slow tests easily/in jenkins.

Things I want to defer to future issues:
- make sure internaltestcluster randomization is less chaotic / more reasonable perf.
- use lucenetestcase iwconfig randomization to the extent we can.
- try to get to 2 minutes 'mvn test' with good coverage. likely requires adding many unit tests.
- try to narrow the coverage gap more between tests.slow=true/false
- speed up old backwards index tests (they spend a lot of time merging indexes)
- try to design a more safe and efficient integration test base class (the suite-scoped one today has a confusing lifecycle).
- hook in more `Asserting*` classes from lucene to find bugs in query impls etc.
- fixing code to work safe with multiple JDK filesystems / allowing them to be used 'for real', anything like that.

Things i investigated and weren't worth the trouble:
- parallelizing old backwards index test. not faster.
- plugging in ram filesystem. runs on tmpfs are not really faster.
</description><key id="69362134">10656</key><summary>Integrate better with lucene test framework and mockfilesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-19T00:00:13Z</created><updated>2015-04-21T20:21:26Z</updated><resolved>2015-04-20T12:14:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-19T11:36:03Z" id="94268286">I did a full round of reviews and added some minor comments. I love this change and I think we should move forward here quickly. I think there are only a handful of things I had and I think we should give others a chance to look as well but push this very soon! guys thanks for all the work here that's awesome!
</comment><comment author="s1monw" created="2015-04-19T11:37:07Z" id="94268320">one other thing that I was wondering is if we should make `test.slow=false` in the `-Pdev` profile? I think it would help iterating quickly?
</comment><comment author="s1monw" created="2015-04-19T11:55:48Z" id="94269939">nice the REST parallelization safed another 30 seconds on my notebooks with 4 JVMs 
</comment><comment author="rmuir" created="2015-04-19T12:40:37Z" id="94272559">&gt; one other thing that I was wondering is if we should make test.slow=false in the -Pdev profile? I think it would help iterating quickly?

it is the default in all cases already in the pom. If you want slow tests, you have to ask for them.
</comment><comment author="rmuir" created="2015-04-19T18:21:56Z" id="94303241">I added a commit to turn on lucene codec randomization. it found two bugs immediately in use of lucene apis (#10660, #10661). For now, I just limit it to AssertingCodec, because it gives clear exceptions about what you are doing wrong, like this:

```
java.lang.AssertionError: docFreq() called on unpositioned TermsEnum
```

The other codecs (including the default) are not so useful for testing. They may find bugs but give you a cryptic error or wrong behavior which is harder to debug.
</comment><comment author="bleskes" created="2015-04-19T19:56:45Z" id="94309056">I like the simplification and removal of test base class. I'm not 100% familiar with the test infra - I think @javanna should have a look as well.  

I'm also doubting about the exclusion of slow tests by default. I feels to me that a sanity check run (i.e., run only fast things) is something one should opt in to as opposed to opt out. This is mostly due to the fact that some parts of are system are only tested by slow tests (for example, the discovery runs which add and kill nodes). 

Last minor detail : `org.elasticsearch.test.ElasticsearchIntegrationTest#afterTestFailed` can be removed. It's not used (but also not part of any diff, so I can't comment inline). 
</comment><comment author="rmuir" created="2015-04-19T20:00:56Z" id="94309231">&gt; I'm also doubting about the exclusion of slow tests by default. I feels to me that a sanity check run (i.e., run only fast things) is something one should opt in to as opposed to opt out. This is mostly due to the fact that some parts of are system are only tested by slow tests (for example, the discovery runs which add and kill nodes).

Well these are unit test runs. Maybe such tests belong in an integration test suite or somewhere else? Or maybe they should be done as unit test, but they are simply too slow.

Again disabling these non-unit-tests this provides the majority of test coverage (73% vs 79%) with a fraction of the time. Its the right tradeoff.
</comment><comment author="rmuir" created="2015-04-19T20:07:40Z" id="94309555">Also, we really need to draw a line in the sand (great time is now) where we don't coddle and encourage slow tests. If we aggressively keep the test suite fast, by default, then people will follow suit, because they want their tests to execute and be fast. Furthermore, people will run the tests more often, because it does not take 15-30 minutes to do so. So it ultimately will result in better coverage than just letting things get slower and slower and less reliable.
</comment><comment author="s1monw" created="2015-04-20T07:32:30Z" id="94380168">&gt; Again disabling these non-unit-tests this provides the majority of test coverage (73% vs 79%) with a fraction of the time. Its the right tradeoff.

+1000 this is the right way to go here. We can still configure CI to run all these things but it should be sufficient to make a commit / push if the non-slowish tests pass. 

&gt; Also, we really need to draw a line in the sand (great time is now) where we don't coddle and encourage slow tests. 

this is the way to go - we have to work towards more coverage as you said in the description with the fast tests. That's a reasonable goal and straight forward IMO, yet it will take time but that's fine. This is actually a step forward while it might seem to be a step backwards in terms of coverage but it encourages since tests are faster and have good coverage. you can still set this as your default if you wanna be on the totally safe end. It's the same discussion as using local transport by default for speed it bypasses the entire network stack but it's the right tradeoff. Please lets move forward here and don't loos ourself in endless discussions. if it turns out to be a problem we can still go back. 
</comment><comment author="javanna" created="2015-04-20T09:50:29Z" id="94410576">left a couple of comments around REST tests and restarting the suite cluster after a failure. Note that if we go ahead and remove this mechanism there is more code that can be removed (as Boaz pointed out):  `ElasticsearchIntegrationTest#afterTestFailed`, `ElasticsearchRestTestCase#afterTestFailed`, and I think the rest client creation in RestTestExecutionContext can be simplified (we might be able to make the rest client final).
</comment><comment author="s1monw" created="2015-04-20T11:53:18Z" id="94432063">+1 on removing more stuff if possible as luca said but we should not be blocked by ANY project downstream that use some of our test. We are on master and we can literally do whatever we want as long as there is an upgrade path. Yet, this is test only so we can move forward even if we break client builds.

this change LGTM and I think we should not waste time and letting it go out of date. this is a massive improvement in our infra. +1 to push and take it from here iterating further.
</comment><comment author="clintongormley" created="2015-04-21T10:35:43Z" id="94739407">@javanna sorry for the late response, been away.  For the record, i'm happy with breaking the clients version parsing - it's a tiny fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms aggregation ERROR : parse failure on include array of values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10655</link><project id="" key="" /><description>Hi,
I'm using a terms aggregation and would like to use the include feature to only include certain terms on which buckets should be created.
Unfortunately I am not able to find the right syntax.
The documentation mentions the following:

"It is possible to filter the values for which buckets will be created. This can be done using the include and exclude parameters which are based on regular expression strings or arrays of exact values."

How do you pass an array of exact values?
Here is my example:
{
   "query" :{..},
   "aggregations":{
         "sector_ids":{
             "terms":{
                 "field":"sector_id"
             },
             "aggs":{
                 "unique_keywords":{
                     "terms":{
                         "field":"keywords.analyzed_name",
                         "include": ["xeon","argon"]
                     }
                 }
             }
         }
     }
}

This throws an error:

"Parse Failure [Unknown key for a START_ARRAY in [unique_keywords]: [include].]];"

I am using ES 1.3.5, however I don't think it has anything to do with my version.

Any help is appreciated, until then I will have my program generate a pattern of all the keywords that I wish to include, but that becomes a  really lousy piece of monkey patching.

Regards,
RR.
</description><key id="69254065">10655</key><summary>terms aggregation ERROR : parse failure on include array of values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rrphotosoft</reporter><labels /><created>2015-04-18T01:55:08Z</created><updated>2015-04-21T15:06:35Z</updated><resolved>2015-04-20T22:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-18T19:56:52Z" id="94196521">&gt; I am using ES 1.3.5, however I don't think it has anything to do with my version.

If I'm  not mistaken this was added in 1.5.0 (previously only regexps were supported) viq https://github.com/elastic/elasticsearch/pull/7529. Assigning this to myself, I'll fix the documentation to make it clearer.
</comment><comment author="jpountz" created="2015-04-20T22:49:03Z" id="94586441">I pushed a commit to the docs, this should reflect on the website soon.
</comment><comment author="rrphotosoft" created="2015-04-21T15:06:35Z" id="94832625">Thanks for the clarification!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change environment variable for terrible JVM workaround</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10654</link><project id="" key="" /><description>If the JVM check fails, the "if you absolutely cannot upgrade" message mentions adding a workaround flag to the JVM_OPTS environment variable. Shouldn't this be the JAVA_OPTS environment variable instead? I had to modify JAVA_OPTS to get the workaround to work (at least when launching through elasticsearch.bat on Windows).
</description><key id="69225214">10654</key><summary>Change environment variable for terrible JVM workaround</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arosequist</reporter><labels /><created>2015-04-17T21:55:55Z</created><updated>2015-04-20T18:38:58Z</updated><resolved>2015-04-20T18:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-18T10:29:18Z" id="94152578">actually, this is so use case specific we should probably just say add "workaround" to the Java process parameters, hopefully this will cause people to upgrade even more
</comment><comment author="arosequist" created="2015-04-19T20:27:13Z" id="94310639">Do you mean make the message say "If you absolutely cannot upgrade, please add ______ to the _Java process parameters_"?
</comment><comment author="MaineC" created="2015-04-20T17:44:46Z" id="94521650">I think this is a duplicate of #10525
</comment><comment author="arosequist" created="2015-04-20T18:38:56Z" id="94533525">Yeah, it's a duplicate. Sorry, my search skills failed me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested documents:  include_in_parent does not seem to work with geo_shape objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10653</link><project id="" key="" /><description>To repro:

```
PUT ltest

PUT ltest/_mapping/test
{
  "test":{
    "properties":{
      "rootField1" : { "type" : "string" },
      "nestedDoc" : { 
        "type" : "nested", 
        "include_in_parent" : true, 
        "dynamic" : false, 
        "path" : "full", 
        "properties" : { 
          "innerField1" : { "type" : "string" }, 
          "innerField2" : { "type" : "geo_shape" }, 
          "innerField3" : { "type" : "geo_point" }, 
          "innerField4" : { "type" : "boolean" } 
        } 
      } 
    } 
  }
}

PUT ltest/test/1
{
  "rootField1":"testname",
  "nestedDoc":{
    "innerField1":"innerstring",
  "innerField2":{
    "type" : "point",
    "coordinates" : [-77.5, 38.5]
  },
  "innerField3":{
      "lat":"41.12",
      "lon":"-71.34"
  },
  "innerField4":true
}
}

GET ltest/_search
{
  "query":{
    "match_all":{}
  },
  "filter":{
    "nested":{
      "path":"nestedDoc",
      "filter":{
    "geo_shape":{
      "nestedDoc.innerField2":{
         "shape":{
        "type":"envelope",
        "coordinates":[[-78,38],[-77,39]]
      }
    }
  }
}
}
}
}

GET ltest/_search
{
  "query":{
    "match_all":{}
  },
  "filter":{
    "geo_shape":{
      "nestedDoc.innerField2":{
         "shape":{
        "type":"envelope",
        "coordinates":[[-78,38],[-77,39]]
      }
    }
  }
}
}
```

The first search works correctly using a nested filter.  The second does not return a result, even though the object should be available at the parent level.  Term filters using the 'innerfield1' string type work as expected, even without the nested filter.
</description><key id="69214687">10653</key><summary>Nested documents:  include_in_parent does not seem to work with geo_shape objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2015-04-17T20:46:50Z</created><updated>2015-04-25T14:50:39Z</updated><resolved>2015-04-25T14:50:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-17T23:33:50Z" id="94097576">The problem here is how geo shapes interact with lucene, mixed with the implementation of `include_in_parent` (note that this would also fail for `include_in_root`). Geo shapes values are generated while indexing using a stream of binary tokens. Due to the fact that these could be extremely memory intensive, this stream is not resettable.  So what happens here is the child doc exhausts the stream while indexing, then the parent tries to use the same stream, but it just gets an empty iteration.
</comment><comment author="clintongormley" created="2015-04-25T14:49:10Z" id="96217390">Thanks for the explanation @rjernst - given how expensive this is, i'd say that we shouldn't change this behaviour.  In fact, the `include_in_*` options were mostly a workaround for the early days of nested objects, when the query/agg framework was not as evolved.

Probably all we need is a doc update.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix OOM for high precision exotic shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10652</link><project id="" key="" /><description>This is currently submitted as a patch in LUCENE-6422 (placed in our lucene package until its committed to lucene 5.x). It removes unnecessary transient memory usage for QuadPrefixTree and, for 1.6.0+ shape indexes adds a new compact bit encoded representation for each quadcell. This is the heart of numerous false positive matches, OOM exceptions, and all around poor shape indexing performance. The compact bit representation will also allows for encoding 3D shapes in future enhancements.

closes #2361
closes #9860
closes #10583 
</description><key id="69213149">10652</key><summary>Fix OOM for high precision exotic shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.6.0</label></labels><created>2015-04-17T20:38:03Z</created><updated>2015-05-29T17:54:16Z</updated><resolved>2015-04-21T19:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-17T21:04:27Z" id="94075047">LGTM, but I would rename the lucene classes XBlah to make it clear we will remove once these are in lucene?
</comment><comment author="nknize" created="2015-04-20T12:13:31Z" id="94436096">Thanks @ryan!  I had given that a shot and it turned into a mass copy/refactor.  Because of the abstraction layers in lucene-spatial refactoring the abstract base class names to X\* requires copying almost the entire lucene-spatial module. 

I can put a comment above the classes indicating they will be removed once LUCENE-6422 is committed?
</comment><comment author="rjernst" created="2015-04-20T22:02:58Z" id="94579947">@nknize Ok, please add the comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scroll API: inconsistency between clearing and searching on a closed scroll id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10651</link><project id="" key="" /><description>When scrolling to completion, clearing the scroll id explicitly via the clear scroll API will return a 404.  I'm assuming because the scroll is closed automatically.

However, scrolling again on an "automatically closed" scroll id still returns a 200.

Simple repro:

``` json
PUT /foo/bar/1
{
    "foo": "bar"
}
```

Clearing scroll after all documents are returned results in a 404:

```
POST /foo/_search?search_type=scan&amp;scroll=5m
```

```
POST /_search/scroll&amp;scroll=5m
```

```
DELETE /_search/scroll  // 404 returned
```

Scrolling again after all documents returned results in a 200:

```
POST /foo/_search?search_type=scan&amp;scroll=5m
```

```
POST /_search/scroll&amp;scroll=5m
```

```
POST /_search/scroll&amp;scroll=5m // 200 returned
```

I would expect scrolling on an automatically cleared scroll id to also return a 404.

On a related note, it would be useful if the response also returned a boolean that indicates whether or not there are more documents to be scrolled for:

```
"_scroll_id" : "c2NhbjswOzE7dG90YWxfaGl0czoxODs=",
"done" : true
```

This way the flag could be checked before sending another scroll request rather than waiting for a 404.
</description><key id="69204109">10651</key><summary>Scroll API: inconsistency between clearing and searching on a closed scroll id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Search</label><label>bug</label><label>discuss</label></labels><created>2015-04-17T19:40:18Z</created><updated>2015-08-24T14:46:56Z</updated><resolved>2015-08-24T14:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-25T14:22:53Z" id="96213562">I agree that a 200 response when searching on a finished scroll ID is inconsistent.

&gt; This way the flag could be checked before sending another scroll request rather than waiting for a 404.

@gmarz The last scroll request returns zero results, indicating that scrolling has finished.  That said, a flag in the response would save that final request.
</comment><comment author="jpountz" created="2015-08-24T14:46:56Z" id="134230963">Closing: this only applies to scan scrolls which are now deprecated. Regular scrolls are never closed automatically so running a DELETE always returns a 2xx code. https://github.com/elastic/elasticsearch/pull/12994
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>marvel.agent.exporter: error connecting to [[0:0:0:0:0:0:0:0]:9200] [No route to host]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10650</link><project id="" key="" /><description>On a default install of elasticsearch v1.4.4-1 from RPM, the elasticsearch.log reports that the marvel.agent.exporter complains as follows, and marvel doesn't work (queries to marvel return {}):

[2015-04-17 00:00:31,853][ERROR][marvel.agent.exporter    ] [Osiris] could not connect to any configured elasticsearch instances: [[0:0:0:0:0:0:0:0]:9200]
[2015-04-17 00:00:32,982][ERROR][marvel.agent.exporter    ] [Osiris] error connecting to [[0:0:0:0:0:0:0:0]:9200] [No route to host]

Setting network.bind_host to the external IP address is our case worked around the problem.

It appears that marvel is trying to using the bind address to bind to, instead of a valid IP address obtained through discovery. When the bind address is 0.0.0.0 the attempt fails.
</description><key id="69194175">10650</key><summary>marvel.agent.exporter: error connecting to [[0:0:0:0:0:0:0:0]:9200] [No route to host]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">minfrin</reporter><labels><label>feedback_needed</label></labels><created>2015-04-17T18:40:06Z</created><updated>2016-01-17T17:44:07Z</updated><resolved>2016-01-17T17:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-19T17:25:12Z" id="94298679">Marvel is shipping data over http to configurable list of targets (fallback on error). This is so to support the recommended setup where the monitoring data is sent to another cluster, to be available in case of trouble. By default, and for easy of first use, we ship data locally by pick up on the address of the local node. Since the traffic is going from the node to itself, we choose to pick on what ES calls the binding host and not the published IP (which what ES tells to other nodes to use when connecting to it). If not configured, ES will pick up the any local address, IPv4 or IPv6 depending on defaults. In your ES bound to `[0:0:0:0:0:0:0:0]:9200` but it doesn't route correctly. To work this, you can either configure marvel to use an explicit IP, or tell ES to bind ES to a specific ip via `network.bind_host` in your elasticsearch.yml file. There are also some special reserved "hosts", see http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#modules-network . 

Let me know if this works for you.
</comment><comment author="clintongormley" created="2016-01-17T17:44:07Z" id="172357736">Closing as Marvel 2 has been released
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added a new script construct</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10649</link><project id="" key="" /><description>Added an initial script construct to unify the parameters typically passed to
methods in the ScriptService. This changes the way several public
methods are called in the ScriptService along with all the callers since
they must wrap the parameters passed in into a script object. In the future,
parsing parameters can also be moved into this construct along with ToXContent.
</description><key id="69186487">10649</key><summary>Added a new script construct</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-17T17:58:34Z</created><updated>2015-06-06T17:23:04Z</updated><resolved>2015-04-22T22:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-20T13:23:55Z" id="94450033">I did a first review round, looks good, left some comments
</comment><comment author="uboness" created="2015-04-20T17:16:10Z" id="94513522">we also talked about moving the xcontent parsing to the `Script` class (centralize it where it belongs). Would also be nice if this class implements `ToXContent` and can emit itself to xcontent based on the same parsing logic.

Also 
</comment><comment author="jdconrad" created="2015-04-20T20:38:47Z" id="94562635">Thanks for the first review @javanna.  Second round of code incoming in a bit.
</comment><comment author="jdconrad" created="2015-04-20T20:40:26Z" id="94562921">@uboness If it's all right, I would like to get this portion in master, then add the ToXContent once the final parameters for the class are decided upon.
</comment><comment author="jdconrad" created="2015-04-20T22:22:04Z" id="94582937">@javanna @uboness I pushed a new commit.
</comment><comment author="javanna" created="2015-04-21T10:57:39Z" id="94744110">left some more comments
</comment><comment author="jdconrad" created="2015-04-21T17:53:19Z" id="94888330">@javanna @uboness Thanks for the reviews so far.  Just pushed a new commit that I believe addresses the rest of the comments.
</comment><comment author="javanna" created="2015-04-22T08:36:59Z" id="95077568">I did another review round, left a few comments, it's close though.
</comment><comment author="jdconrad" created="2015-04-22T22:25:43Z" id="95355904">@javanna @uboness Added another commit.  Ready for the next round.
</comment><comment author="javanna" created="2015-04-22T22:41:53Z" id="95358202">LGTM, looking forward to the next step where we unify the parsing and we can also use this new construct a bit more
</comment><comment author="jdconrad" created="2015-04-22T22:51:57Z" id="95359650">@javanna @uboness Thanks for the reviews, guys.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single value numeric queries shouldn't be handled by NumericRangeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10648</link><project id="" key="" /><description>For performance reasons this change uses a TermQuery wrapped in a ConstantScoreQuery instead of a NumericRangeQuery with start and end values the same.

Closes #10646
</description><key id="69164630">10648</key><summary>Single value numeric queries shouldn't be handled by NumericRangeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-17T15:48:11Z</created><updated>2015-05-29T18:19:30Z</updated><resolved>2015-04-22T11:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-17T17:51:38Z" id="94038469">LGTM
</comment><comment author="markharwood" created="2015-04-17T17:52:17Z" id="94038607">Working on a change to address termFilter too...
</comment><comment author="markharwood" created="2015-04-21T12:37:44Z" id="94777597">@rmuir Can I get a review of this now that I've added termFilter support?
Thx
</comment><comment author="rmuir" created="2015-04-21T12:45:53Z" id="94779584">This change looks great to me. Its very important for master where otherwise we'd always read all the postings for the term into a bitset in memory, which is bad if we are e.g. AND'ing with a bunch of other clauses. Even for older branches, we would sometimes do this if the term has a high docfreq (which is exactly when you do not want to do this). So it is best to explicitly make term query/filters instead. I also like the internal cleanup, +1.
</comment><comment author="markharwood" created="2015-04-21T12:54:40Z" id="94781626">Thanks @rmuir - which branches would you like to see have this?
</comment><comment author="rmuir" created="2015-04-21T12:57:40Z" id="94782221">I think we should do 2.0 and 1.6 personally. Its not technically a bug, so I would omit from 1.5.2 just to be on the safe side.
</comment><comment author="markharwood" created="2015-04-21T13:02:40Z" id="94784934">OK - I'll push to 1.x and master if there are no objections
</comment><comment author="jpountz" created="2015-04-21T13:03:57Z" id="94785877">+1 on master and 1.x
</comment><comment author="s1monw" created="2015-04-21T13:14:42Z" id="94789814">&gt; I think we should do 2.0 and 1.6 personally. Its not technically a bug, so I would omit from 1.5.2 just to be on the safe side.

+1
</comment><comment author="markharwood" created="2015-04-22T11:30:45Z" id="95144844">Pushed to 1.x in https://github.com/elastic/elasticsearch/commit/f9e044d073b464cb08701574ae0da3746a25afe5 and master in https://github.com/elastic/elasticsearch/commit/05c3d05cff9149c140a21da10cb096f39e0fb8b8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>post_filter breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10647</link><project id="" key="" /><description>While upgrading from version 1.1.1 to 1.5.1 the post_filter fails with :
nested: ElasticsearchParseException[Expected field name but got END_ARRAY \"post_filter\"];

--FAILS
  "post_filter": [
    {
      "script": {
        "script": "(!doc.containsKey('DebutVisibilite') || doc['DebutVisibilite'].empty || doc['DebutVisibilite'].value &lt;= today) &amp;&amp; (!doc.containsKey('FinVisibilite') || doc['FinVisibilite'].empty || doc['FinVisibilite'].value &gt;= today) &amp;&amp; (!doc.containsKey('FinValidite') || doc['FinValidite'].empty || doc['FinValidite'].value &gt;= date)",
        "params": {
          "today": 1429228800000,
          "date": 1429228800000
        }
      }
    }
  ]

--SUCCEEDS
  "post_filter":
    {
      "script": {
        "script": "(!doc.containsKey('DebutVisibilite') || doc['DebutVisibilite'].empty || doc['DebutVisibilite'].value &lt;= today) &amp;&amp; (!doc.containsKey('FinVisibilite') || doc['FinVisibilite'].empty || doc['FinVisibilite'].value &gt;= today) &amp;&amp; (!doc.containsKey('FinValidite') || doc['FinValidite'].empty || doc['FinValidite'].value &gt;= date)",
        "params": {
          "today": 1429228800000,
          "date": 1429228800000
        }
      }
    }

So how can i apply multiple post filter conditions, ex : 1 script + 1 term etc... ?
</description><key id="69162558">10647</key><summary>post_filter breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gge-etudes</reporter><labels /><created>2015-04-17T15:37:02Z</created><updated>2015-04-20T12:18:12Z</updated><resolved>2015-04-17T15:42:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-17T15:42:40Z" id="94012123">I guess you could use a `bool filter` with many `must` clauses to wrap all your filters? Would this work for you?

I don't think it's an issue here but more a question.
You should prefer the mailing list for that kind of question.

Closing but feel free to reopen if you think it's a bug.
</comment><comment author="gge-etudes" created="2015-04-20T08:24:32Z" id="94391098">Hi, thanks for your reply.

Well i think it's an issue because it used to work in previous releases.
Anyhow, i had to apply those filters outside of the query because of caching and faceting issues.

Right now it's ok as i only have one post_filter clause in this particular project.
But still, i'd like to know what are the guidelines to achieve the exact same result as before.

I tried to use a bool filter inside a post_filter without success, but it might be a matter of syntax.

By the way, i am unable to re-open this issue.
</comment><comment author="dadoonet" created="2015-04-20T10:36:14Z" id="94418256">I just tried this on 1.1.1:

```
DELETE index
PUT index/type/1
{
  "foo": "bar"
}

GET index/type/_search
{
  "post_filter": [
    "term": {
      "foo": "bar"
    }
  ]
}
```

It fails as expected:

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[nKIQFEpFTQu0s_qXbmoNtg][index][1]: SearchParseException[[index][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"post_filter\": [\n    \"term\": {\n      \"foo\": \"bar\"\n    }\n  ]\n}\n]]]; nested: QueryParsingException[[index] [_na] filter malformed, must start with start_object]; }{[nKIQFEpFTQu0s_qXbmoNtg][index][0]: SearchParseException[[index][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"post_filter\": [\n    \"term\": {\n      \"foo\": \"bar\"\n    }\n  ]\n}\n]]]; nested: QueryParsingException[[index] [_na] filter malformed, must start with start_object]; }{[nKIQFEpFTQu0s_qXbmoNtg][index][4]: SearchParseException[[index][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"post_filter\": [\n    \"term\": {\n      \"foo\": \"bar\"\n    }\n  ]\n}\n]]]; nested: QueryParsingException[[index] [_na] filter malformed, must start with start_object]; }{[nKIQFEpFTQu0s_qXbmoNtg][index][2]: SearchParseException[[index][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"post_filter\": [\n    \"term\": {\n      \"foo\": \"bar\"\n    }\n  ]\n}\n]]]; nested: QueryParsingException[[index] [_na] filter malformed, must start with start_object]; }{[nKIQFEpFTQu0s_qXbmoNtg][index][3]: SearchParseException[[index][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"post_filter\": [\n    \"term\": {\n      \"foo\": \"bar\"\n    }\n  ]\n}\n]]]; nested: QueryParsingException[[index] [_na] filter malformed, must start with start_object]; }]",
   "status": 400
}
```

Note that the oldest doc don't mention you can use an array of filters: http://www.elastic.co/guide/en/elasticsearch/reference/0.90/search-request-post-filter.html

So you might hit something else here.
Could you check on your end and create a full SENSE script as I wrote which runs in 1.1.1 and breaks in 1.5.1?
</comment><comment author="gge-etudes" created="2015-04-20T12:11:35Z" id="94435863">OK, so your sample works in v1.1.1 (not in v1.5.1) if you add curly braces around the term filter :

```
POST index/type/_search
{
   "post_filter":[
      {
         "term":{
            "foo":"bar"
         }
      }
   ]
}
```

But only with a sole filter, so this query fails in v1.1.1 (as in v1.5.1) :

```
POST index/type/_search
{
   "post_filter":[
      {
         "term":{
            "foo":"bar"
         }
      },
      {
         "term":{
            "foo":"bar"
         }
      }
   ]
}
```

Whereas the bool filter approach works like a charmi n v1.1.1 (and in v1.5.1) :

```
POST index/type/_search
{
   "post_filter":{
      "bool":{
         "must":[
            {
               "term":{
                  "foo":"bar"
               }
            },
            {
               "term":{
                  "foo":"bar"
               }
            }
         ]
      }
   }
}
```

Strange thing... you can express a post filter as an array made of a single item.
So I was assuming it could be an array populated by multiple items... but no.
Still, there is a difference between v1.1.1 and  v1.5.1.
The bool filter way is fine to me.

Thank you very much.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single value numeric queries shouldn't be handled by NumericRangeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10646</link><project id="" key="" /><description>Various clauses in the Query DSL allow criteria that matches a single numeric term and these are resolved behind the scenes by calling NumericFieldMapper.termQuery().
Currently all subclasses of NumericFieldMapper implement this method by returning a NumericRangeQuery with the same start and end value. Instead it would be more efficient to return a TermQuery wrapped in a ConstantScoreQuery. 

As well as providing a performance improvement this change will lay the foundations for https://github.com/elastic/elasticsearch/issues/10628 which will add an option to use the TermQueries, unwrapped, to allow numeric fields to make use of the usual Lucene ranking behaviour (IDF + norms).
</description><key id="69156226">10646</key><summary>Single value numeric queries shouldn't be handled by NumericRangeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label></labels><created>2015-04-17T15:05:38Z</created><updated>2015-04-22T11:29:08Z</updated><resolved>2015-04-22T11:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Warning about the conflict with the Standard Tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10645</link><project id="" key="" /><description>The examples given requires a specific Tokenizer to work.
</description><key id="69134408">10645</key><summary>Warning about the conflict with the Standard Tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">bdelbosc</reporter><labels /><created>2015-04-17T13:00:47Z</created><updated>2015-06-19T14:21:11Z</updated><resolved>2015-06-19T14:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T19:16:52Z" id="95689807">Thanks @bdelbosc - merged 
</comment><comment author="clintongormley" created="2015-06-19T14:21:10Z" id="113529587">Merged in 342dab35bdefe63aa0497acdee1ee0b079fae06a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure netty I/O thread is not blocked in TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10644</link><project id="" key="" /><description>Whenever a transport client executes a request, it uses a built-in
RetryListener which tries to execute the request on another node.

However, if a connection error occurs, the onFailure() callback of
the listener is triggered, the netty I/O thread might still be used
to whatever failure has been added.

This commit offloads the onFailure handling to the generic thread pool. There could potential be other solutions than running in specific thread pools, but this might be the most simple one.

This problem was triggered in one of our test runs, see http://build-us-00.elastic.co/job/es_bwc_1x/9543/CHECK_BRANCH=tags%2Fv1.3.9,jdk=JDK7,label=bwc/consoleFull
</description><key id="69119847">10644</key><summary>Ensure netty I/O thread is not blocked in TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Java API</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-17T11:20:42Z</created><updated>2017-03-03T02:50:43Z</updated><resolved>2015-04-26T20:03:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-17T11:58:09Z" id="93972416">left 2 comments thanks @spinscale 
</comment><comment author="spinscale" created="2015-04-17T17:21:16Z" id="94031064">fixed both issues, now that we can log failures I am fine with using the `LISTENER` threadpool
</comment><comment author="s1monw" created="2015-04-18T12:37:55Z" id="94162866">thanks @spinscale I think once you fixed the lasts comments feel free to push ie. LGTM then
</comment><comment author="kimchy" created="2015-04-18T18:51:10Z" id="94190970">actually, @spinscale, I have been thinking about it..., if the listener threaded flag is set, then it will already be execute on the listener thread pool.

I think that solving it here on this level might be wrong, what we need is to always execute on the listener thread pool when we either use a TransportClient, or a NodeClient (in pure client mode). We need to be careful not to use the listener thread pool when we use the NodeClient as a Client within a standalone data/master nodes.

I have been thinking of how to maybe do it. I think we need to have a system wide setting that controls by default if actions listeners are threaded or not. Once we have it, the question is what the default would be, here, we have 2 options:
- In Bootstrap, we opt into _not_ having threaded listeners, and by default, action listeners are always threaded. This is safest, since we take the more safer route of having listener threaded.
- In TransportClient, and NodeClient (when client set to true), option into having threaded listeners, and by default, action listeners are not threaded.

Regardless, I think we should remove the threaded flag on each request (not needed really), and have a system wide default that is sensible?

In tests, since we don't use Bootstrap for example for the first option, we should randomize the listener flag, but always have them threaded for the transport client case?
</comment><comment author="s1monw" created="2015-04-21T13:26:23Z" id="94793196">@spinscale what's the status of this?
</comment><comment author="spinscale" created="2015-04-21T17:13:10Z" id="94876122">@s1monw in general I am agreeing with @kimchy 's comment here, but I'd rather see this fixed short term as well.

I am personally for pushing this and then remove the `listenerThreaded` flag and allow for settings specific configuration in its own PR
</comment><comment author="s1monw" created="2015-04-26T18:35:30Z" id="96418664">&gt; I am personally for pushing this and then remove the listenerThreaded flag and allow for settings specific configuration in its own PR

+1 @kimchy are you ok with this?
</comment><comment author="kimchy" created="2015-04-26T18:36:34Z" id="96418767">sorry, I missed @'ing me, yea, I am good with it
</comment><comment author="henushang" created="2017-03-03T02:50:43Z" id="283850521">hi, guys, I find it that the fixed code is not included in the jar in maven repository, eg. version 2.1.2 and version 2.3.5.  How can I get the fixed jar elegantly ? 
Thanks</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Render REST errors in a structural way</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10643</link><project id="" key="" /><description>This is a first cut aiming at progress rather than perfection for #3303

It renders errors on the REST layer as JSON to allow more insight into the error message ie
the query with a missing index renders like this:

```
GET localhost:9200/foo1/_search/?q=~3&amp;pretty=true
{
  "error" : {
    "type" : "index_missing_exception",
    "reason" : "[foo1] missing"
  },
  "status" : 404
}
```

or a search phase exception:

```
GET localhost:9200/_search?q=~3&amp;pretty=true
{
  "error" : {
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "foo",
      "node" : "0-75uocCQhyX4clLGbE6OQ",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"~3\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]",
        "caused_by" : {
          "type" : "query_parsing_exception",
          "reason" : "[foo] Failed to parse query [~3]",
          "caused_by" : {
            "type" : "parse_exception",
            "reason" : "Cannot parse '~3': Encountered \" &lt;FUZZY_SLOP&gt; \"~3 \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ",
            "caused_by" : {
              "type" : "parse_exception",
              "reason" : "Encountered \" &lt;FUZZY_SLOP&gt; \"~3 \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    "
            }
          }
        }
      }
    }]
  },
  "status" : 400
}
```
</description><key id="69108393">10643</key><summary>Render REST errors in a structural way</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rashidkpc/following{/other_user}', u'events_url': u'https://api.github.com/users/rashidkpc/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rashidkpc/orgs', u'url': u'https://api.github.com/users/rashidkpc', u'gists_url': u'https://api.github.com/users/rashidkpc/gists{/gist_id}', u'html_url': u'https://github.com/rashidkpc', u'subscriptions_url': u'https://api.github.com/users/rashidkpc/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1250387?v=4', u'repos_url': u'https://api.github.com/users/rashidkpc/repos', u'received_events_url': u'https://api.github.com/users/rashidkpc/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rashidkpc/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rashidkpc', u'type': u'User', u'id': 1250387, u'followers_url': u'https://api.github.com/users/rashidkpc/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-04-17T10:04:20Z</created><updated>2015-06-06T19:27:52Z</updated><resolved>2015-04-24T07:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2015-04-17T15:58:19Z" id="94015373">Few minutes of fiddling feedback. Definitely an improvement! We can dig into the objects until we find a `type` that we know about, eg `query_parsing_exception`, and create our own error message from there. 

I don't know much of the code but I'm guessing `getMessage()` is responsible for `"[foo] Failed to parse query [~3]"`? That's where things probably get a bit hectic?

**De-duping the `reason`**
For example, in this first request we're asking logstash-\* a question. Which means we're going to get a `reason` for every `query_parsing_exception` in every shard of every index that matches that pattern. The every shard isn't a problem, since all of those reasons will be the same. But since the index name is embedded in the string it will cause a different message for each index, even though they all failed for the same reason.

```
GET /logstash-*/_search/?q=~3&amp;pretty=true

[...]
{
            "shard": 0,
            "index": "logstash-2015.04.09",
            "node": "C1kmGmoPTHmdfQIpIa_p0Q",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"~3\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]",
               "caused_by": {
                  "type": "query_parsing_exception",
                  "reason": "[logstash-2015.04.09] Failed to parse query [~3]",
                  "caused_by": {
                     "type": "parse_exception",
                     "reason": "Cannot parse '~3': Encountered \" &lt;FUZZY_SLOP&gt; \"~3 \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ",
                     "caused_by": {
                        "type": "parse_exception",
                        "reason": "Encountered \" &lt;FUZZY_SLOP&gt; \"~3 \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    "
                     }
                  }
               }
            }
         },
[...]
```

If we look at a sort failure, we have a much better scenario. The message will be the same every index and every shard, so we de-dupe them and tell the user `"No mapping found for [foo] in order to sort on"`:

```

[...]
{
            "shard": 0,
            "index": "logstash-2015.04.09",
            "node": "C1kmGmoPTHmdfQIpIa_p0Q",
            "reason": {
               "type": "search_parse_exception",
               "reason": "Failed to parse source [{\"sort\":[{\"foo\":{}}]}]",
               "caused_by": {
                  "type": "search_parse_exception",
                  "reason": "No mapping found for [foo] in order to sort on"
               }
            }
         },
[...]
```

**Figuring out which `reason` to use**
We need to pick a simple error to show the user. As a human I'm pretty good at scanning that object and finding the subjectively "simple" reason, but we'd need a rule for doing the same programmatically.

From these two examples I would probably pick the "deepest" `type` we understand. For example, if we understand `search_parse_exception` we would dig into the sort error until we got to the deepest one: `No mapping found for [foo] in order to sort on`

In the query example, we'd say we still understand `search_parse_exception`, but we know `query_parsing_exception` also, and because its deeper, we'll use its `reason`. This would depend on us blacklisting `parse_exception` and making the assumption that anywhere a `parse_exception` could occur we should never show its reason because its simply too verbose.

Ideally we could do this on the Elasticsearch side and float the "simple" error to the top somehow, but I don't know that my rule devised from 2 examples is going to apply globally :smile: 
</comment><comment author="s1monw" created="2015-04-18T12:32:22Z" id="94162629">@rashidkpc thanks a lot for your feedback, I tried another interation for this including some possible solutions to your problems:

**De-duping the reason** I added an optional deduplication for the shard failures that only returns on of the shard failures if there are multiple like this:

```
GET /_search/?q=::&amp;pretty=true&amp;group_shard_failures=true
{
  "error" : {
    "human_error" : [ "Failed to parse query [::]" ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "foo",
      "node" : "7iBpoq5uTWyhDxMtBLHkVw",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"::\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]",
        "caused_by" : {
          "type" : "query_parsing_exception",
          "reason" : "Failed to parse query [::]",
          "index" : "foo",
          "caused_by" : {
            "type" : "parse_exception",
            "reason" : "Cannot parse '::': Encountered \" \":\" \": \"\" at line 1, ... ",
            "caused_by" : {
              "type" : "parse_exception",
              "reason" : "Encountered \" \":\" \": \"\" at line 1,... "
            }
          }
        }
      }
    } ]
  },
  "status" : 400
}
```

I added a best effort human readable error on the top level that basically uses all the unique lowest level Elasticsearch created exceptions (the first level we control) which looks like this:

```
GET /_search/?q=::&amp;pretty=true&amp;group_shard_failures=true
{
  "error" : {
    "human_error" : [ "Failed to parse query [::]" ],
    ....
  }
}

GET /_search/asf?pretty=true
{
  "error" : {
    "human_error" : [ "No feature for name [asf]" ],
    "type" : "elasticsearch_illegal_argument_exception",
    "reason" : "No feature for name [asf]"
  },
  "status" : 400
}

GET /bar/_search/?pretty=true
{
  "error" : {
    "human_error" : [ "no such index" ],
    "type" : "index_missing_exception",
    "reason" : "no such index",
    "index" : "bar"
  },
  "status" : 404
}

GET /_search?pretty=true&amp;group_shard_failures=true' -d '{"sort" : [{"foo":{}}]}'
{
  "error" : {
    "human_error" : [ "No mapping found for [foo] in order to sort on" ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "foo",
      "node" : "7iBpoq5uTWyhDxMtBLHkVw",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Failed to parse source [{\"sort\" : [{\"foo\":{}}]}]",
        "caused_by" : {
          "type" : "search_parse_exception",
          "reason" : "No mapping found for [foo] in order to sort on"
        }
      }
    } ]
  },
  "status" : 400
}

```

Those are all best effort and simple heuristics but I think it's an improvement over what we have today though.
</comment><comment author="kimchy" created="2015-04-22T20:59:58Z" id="95334850">LGTM except for the minor comment on the `elasticsearch_` prefix, great stuff!
</comment><comment author="s1monw" created="2015-04-22T21:00:15Z" id="95334949">w00t
</comment><comment author="s1monw" created="2015-04-23T08:46:34Z" id="95493224">@kimchy @clintongormley @rashidkpc I think we are ready here any more comments on the API? Changes I made are this:
- `group_shard_failures` defaults to `true` 
- `human_error` is cut over to `root_cause` including a type (see below)
- `elasticsearch_` prefix is removed...

``` JSON
GET /_search/asf?q=::&amp;pretty=true
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "No feature for name [asf]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "No feature for name [asf]"
  },
  "status" : 400
}
```
- Grouped failures are reflected in the response (see `"grouped" : true`) use `group_shard_failures=false` in the request to get all the response

``` JSON
GET /_search?q=::&amp;pretty=true
{
  "error" : {
    "root_cause" : [ {
      "type" : "search_phase_execution_exception",
      "reason" : "all shards failed"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "foo",
      "node" : "SvSibBJsTGKcgfyIDchoyA",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"::\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]",
        "caused_by" : {
          "type" : "query_parsing_exception",
          "reason" : "Failed to parse query [::]",
          "index" : "foo",
          "caused_by" : {
            "type" : "parse_exception",
            "reason" : "Cannot parse '::': Encountered \" \":\" \": \"\" at line 1, column 0.\n..  ",
            "caused_by" : {
              "type" : "parse_exception",
              "reason" : "Encountered \" \":\" \": \"\" at line 1, column 0.\n..."
            }
          }
        }
      }
    } ]
  },
  "status" : 400
}
```

I think we are ready here... we can now improve stuff as we go...
</comment><comment author="clintongormley" created="2015-04-23T10:21:20Z" id="95525469">@s1monw In your second example in https://github.com/elastic/elasticsearch/pull/10643#issuecomment-95493224, surely the `root_cause` should be `parse_exception`, not `search_phase_execution_exception`?

An improvement in the second example (and not necessarily as part of this PR) might be to mark a particular exception in the tree as the one to use for the root cause, eg it may make more sense to stop at `query_parsing_exception` (with an improved `reason`) instead of going all the way down to the final `parse_exception`.
</comment><comment author="s1monw" created="2015-04-23T13:23:12Z" id="95581252">&gt; An improvement in the second example (and not necessarily as part of this PR) might be to mark a particular exception in the tree as the one to use for the root cause, eg it may make more sense to stop at query_parsing_exception (with an improved reason) instead of going all the way down to the final parse_exception.

I guess it would make more sense to just dont' render the verbose part if verbose=false?
</comment><comment author="s1monw" created="2015-04-23T13:25:01Z" id="95582131">&gt; @s1monw In your second example in #10643 (comment), surely the root_cause should be parse_exception, not search_phase_execution_exception?

yeah that is right! I fixed it, it now looks like this: 

``` JSON
GET /_search?q=::&amp;pretty=true

{
  "error" : {
    "root_cause" : [ {
      "type" : "query_parsing_exception",
      "reason" : "Failed to parse query [:::]",
      "index" : "foo"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "foo",
      "node" : "CdPVI-Y-QHyBRctNFXVmSA",
      "reason" : {
        "type" : "search_parse_exception",
        "reason" : "Failed to parse source [{\"query\":{\"query_string\":{\"query\":\":::\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]",
        "caused_by" : {
          "type" : "query_parsing_exception",
          "reason" : "Failed to parse query [:::]",
          "index" : "foo",
          "caused_by" : {
            "type" : "parse_exception",
            "reason" : "Cannot parse ':::': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one...",
            "caused_by" : {
              "type" : "parse_exception",
              "reason" : "Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one... "
            }
          }
        }
      }
    } ]
  },
  "status" : 400
}
```
</comment><comment author="s1monw" created="2015-04-23T19:58:55Z" id="95701459">unless anybody objects I'd push this tomorrow morning CEST
</comment><comment author="clintongormley" created="2015-04-23T20:06:39Z" id="95704435">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `async` replication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10642</link><project id="" key="" /><description>Follow up for PR #10171.

As we discussed https://github.com/elastic/elasticsearch/issues/10114#issuecomment-87989649, we should also mark the code as deprecated so plugin developers know in advance that they need to adapt their code.

I also added a `log` when using `replication` parameter in REST APIs but as `info` Level. I wonder if I should better log that as a `WARN` if `debug` level is set? So people in debug mode will see this information as a `WARN` while it won't pollute logs for each call when in default `INFO` log level.
</description><key id="69096004">10642</key><summary>Deprecate `async` replication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-04-17T08:39:32Z</created><updated>2015-05-30T10:47:48Z</updated><resolved>2015-05-29T10:30:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-17T08:45:56Z" id="93945503">Thanks for openinig this @dadoonet , it LGTM. As for deprecation warnings, I think they should be warn, but let's hear what @clintongormley thinks. It would be great to have all of the deprecation logs from the same logger, so that they can all be enabled at once when needed, but this might require some more work and overlap with other work in progress that I'm not sure about.
</comment><comment author="dadoonet" created="2015-04-17T08:47:36Z" id="93945831">Ah! So we could create a DeprecationLogger class and allow a global activation or not. Would be super nice indeed! 
</comment><comment author="jpountz" created="2015-04-20T09:27:49Z" id="94406048">@dadoonet @javanna I think we should remove deprecation warnings from this PR and instead make sure that the replication type is parsed with ParseField so that we have a single entry point if we want to log about usage of deprecated APIs.
</comment><comment author="clintongormley" created="2015-04-23T18:48:15Z" id="95682315">I think the deprecation message should be a warning, and I agree with https://github.com/elastic/elasticsearch/pull/10642#issuecomment-94406048 (see https://github.com/elastic/elasticsearch/issues/8963)
</comment><comment author="clintongormley" created="2015-05-12T08:24:29Z" id="101186158">@dadoonet any movement here?
</comment><comment author="dadoonet" created="2015-05-12T08:27:27Z" id="101186617">@clintongormley I was wondering if I should wait for #11033 to be merged in so I can update the deprecation notice there?
</comment><comment author="jpountz" created="2015-05-12T08:50:04Z" id="101193866">Hmm, actually ParsedField might not be applicable here (or in an awkward way) since we do not perform parsing, however I think this PR should use #11033 once it's in. 
</comment><comment author="s1monw" created="2015-05-28T15:59:22Z" id="106442969">@dadoonet can you move this forward if possible?
</comment><comment author="dadoonet" created="2015-05-29T08:56:37Z" id="106749073">@s1monw Thanks for the heads up. Now that #11285 has been merged, I'm going to update the PR and use the deprecation logger. Stay tuned! :)
</comment><comment author="dadoonet" created="2015-05-29T09:09:44Z" id="106752211">Actually, the deprecation logger has been merged only in master (2.0) but is not available in 1.x.
The plan is to deprecate in docs in 1.x and use the deprecation logger in master.
</comment><comment author="javanna" created="2015-05-29T10:17:37Z" id="106765887">LGTM besides the tiny javadoc comment I left
</comment><comment author="dadoonet" created="2015-05-29T10:30:02Z" id="106767602">merged in 1.x with 1493cfbb91d83eea0740907a45ab59051262a910
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene-5.2-snapshot-1674183.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10641</link><project id="" key="" /><description>This update contains the extractTerms refactoring (LUCENE-6425) and the TermContext.setDocFreq removal (LUCENE-6429).
</description><key id="69091495">10641</key><summary>Upgrade to Lucene-5.2-snapshot-1674183.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-17T08:09:32Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-17T08:43:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-17T08:11:45Z" id="93940156">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>With min_doc_count: 0, terms agg now creating null bucket (new in 1.5.1?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10640</link><project id="" key="" /><description>Running this query on my data:

```
GET beta/place/_search?search_type=count
{
 "query": {
   "match_all": {}
 },
 "aggs": {
   "ratings": {
     "terms": {
       "field": "price",
       "size": 100,
       "min_doc_count": 0
     }
   }
 }
} 
```

I'm getting this back:

```
   "aggregations": {
      "ratings": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "2",
               "doc_count": 612985
            },
            {
               "key": "1",
               "doc_count": 542859
            },
            {
               "key": "3",
               "doc_count": 125294
            },
            {
               "key": "4",
               "doc_count": 57340
            },
            {
               "key": "null",
               "doc_count": 0
            }
         ]
      }
   }
```

At the very least, I can say I'm not getting null buckets in version 1.3.9. I don't have any 1.4 servers up right now to check.

Also, it only happens for min_doc_count=0. If I remove that, it goes away.

Is this desired behavior?

I found this existing issue: https://github.com/elastic/elasticsearch/issues/6273
but that seems to asking for a null bucket, and it's still open.
</description><key id="69054623">10640</key><summary>With min_doc_count: 0, terms agg now creating null bucket (new in 1.5.1?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylerprete</reporter><labels /><created>2015-04-17T03:27:06Z</created><updated>2015-04-20T21:47:29Z</updated><resolved>2015-04-18T20:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-18T20:02:37Z" id="94196912">This can happen if you have a deleted document which is only marked as deleted for now and has "null" as a value for "price". See first note under http://www.elastic.co/guide/en/elasticsearch//reference/1.5/search-aggregations-bucket-terms-aggregation.html#_minimum_document_count
</comment><comment author="tylerprete" created="2015-04-20T18:29:30Z" id="94531366">Just FYI, this index doesn't have any deleted documents. We never issue deletes with our workflow. And I only started seeing it in ES 1.5+. But it's fine, I can workaround it.
</comment><comment author="jpountz" created="2015-04-20T21:47:29Z" id="94577336">@tylerprete Then maybe you have other types on the same index and `null` is a possible value on those types for the `price` field?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add back really old upgrade test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10639</link><project id="" key="" /><description>Spinoff from #10540 ... this just adds back the really old upgrade test we have on 1.x, but uses 0.90.6 index instead of the too-old-for-master 0.20 index.

I also exercise upgrade_only_ancient_segments=true ...
</description><key id="69008082">10639</key><summary>add back really old upgrade test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T20:55:42Z</created><updated>2015-04-17T18:01:06Z</updated><resolved>2015-04-17T18:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-17T07:13:59Z" id="93928640">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow fuzziness specified as a `minimum similarity` and require edits [0,2]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10638</link><project id="" key="" /><description>Lucene deprecated this in 4.0 and we only try best effort to support it. Folks should only use edit distance rather than some length based similarity. Yet the formular is simple enough such that users can still do it in the client if they really need to.
</description><key id="69008078">10638</key><summary>Don't allow fuzziness specified as a `minimum similarity` and require edits [0,2]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T20:55:41Z</created><updated>2015-07-14T15:11:18Z</updated><resolved>2015-07-14T15:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-16T21:12:34Z" id="93840755">+1
</comment><comment author="s1monw" created="2015-04-17T10:03:51Z" id="93957419">@clintongormley are you ok with this
</comment><comment author="s1monw" created="2015-04-21T18:14:03Z" id="94893200">ping @clintongormley 
</comment><comment author="clintongormley" created="2015-04-24T09:50:48Z" id="95876295">+1 remove min_similarity and percentages of fuzziness.  Fuzziness should accept 1, 2, or AUTO
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add retry mechanism for searcher manager when using shadow replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10637</link><project id="" key="" /><description>It's possible when using a shared filesystem for the contents to be slightly stale due to latency in the FS. When this happens opening a new `ShadowEngine` can fail because there are no segments exist in the directory.

The failure when this happens looks like:

```
[2015-04-14 01:27:50,283][ERROR][index.engine ] [n1] [index][0] failed to access searcher manager java.nio.file.NoSuchFileException: /path/to/data/index/segments_2
```

In these situations, we should attempt to wait, then retry to open the directory as an index. If after waiting and retrying (maybe multiple times) the ShadowEngine should fail and throw an exception like we currently do.
</description><key id="69003781">10637</key><summary>Add retry mechanism for searcher manager when using shadow replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label></labels><created>2015-04-16T20:30:51Z</created><updated>2015-04-22T20:34:57Z</updated><resolved>2015-04-22T20:34:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES having trouble parsing </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10636</link><project id="" key="" /><description>We have a troubles with some messages going to ES. It looks like messages are silently dropped from Kibana. We would expect JSON message, or at very least, a unparsed message with the proper tag.

command:

```
logger -p notice "{\"myname\":\"vadim\", \"date\":\"`date`\",\"foo\":[0,1]}"
```

Stack trace in ES log (/var/log/elasticsearch/mozstash.log):

```
[2015-04-16 18:00:08,776][DEBUG][action.bulk              ] [mozstash1404-ubuntu-1404] [logstash-2015.04.16][3] failed to execute bulk item (index) index {[logstash-2015.04.16][logs][dlshi6ycRJmp1TR46Icolg], source[{"@timestamp":"2015-04-16T18:00:08.798Z","tier":"_default","hostname":"emitter1204-ubuntu-1204","severity":"notice","service":"vagrant","collector":"collector-ubuntu-1404","message":"{\"myname\":\"vadim\", \"date\":\"Thu Apr 16 18:00:08 UTC 2015\",\"foo\":[0,1]}","@version":"1","host":"10.85.25.25:55269","myname":"vadim","date":"Thu Apr 16 18:00:08 UTC 2015","foo":[0,1]}]}
org.elasticsearch.index.mapper.MapperParsingException: object mapping [foo] trying to serialize a value with no field associated with it, current value [0]
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:702)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:695)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:438)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:432)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:149)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Another example:

```
logger -p notice "{\"myname\":\"vadim\", \"date\":\"`date`\",\"foo\":{\"bar\":\"baz\"}}"
```

Stack trace:

```
[2015-04-16 18:07:44,757][DEBUG][action.bulk              ] [mozstash1404-ubuntu-1404] [logstash-2015.04.16][3] failed to execute bulk item (index) index {[logstash-2015.04.16][logs][NkUpFrrzTwqMRnPRvE2RYg], source[{"@timestamp":"2015-04-16T18:07:44.898Z","tier":"_default","hostname":"emitter1204-ubuntu-1204","severity":"notice","service":"vagrant","collector":"collector-ubuntu-1404","message":"{\"myname\":\"vadim\", \"date\":\"Thu Apr 16 18:07:44 UTC 2015\",\"foo\":{\"bar\":\"baz\"}}","@version":"1","host":"10.85.25.25:55269","myname":"vadim","date":"Thu Apr 16 18:07:44 UTC 2015","foo":{"bar":"baz"}}]}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [foo.bar]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:410)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:487)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:438)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:432)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:149)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "baz"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:441)
    at java.lang.Long.parseLong(Long.java:483)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:300)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:236)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:400)
    ... 14 more
```

Elastic version info:

```
# curl -XGET 'localhost:9200'
{
  "status" : 200,
  "name" : "mozstash1404-ubuntu-1404",
  "cluster_name" : "mozstash",
  "version" : {
    "number" : "1.5.0",
    "build_hash" : "544816042d40151d3ce4ba4f95399d7860dc2e92",
    "build_timestamp" : "2015-03-23T14:30:58Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

Thank you for your help
</description><key id="68979876">10636</key><summary>ES having trouble parsing </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vadim-ex</reporter><labels /><created>2015-04-16T18:13:03Z</created><updated>2015-04-26T15:11:49Z</updated><resolved>2015-04-23T18:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T18:33:33Z" id="95678994">Hi @vadim-moz 

It looks like you are relying on dynamic mapping to introduce new fields, and you are sending fields with different datatypes.  For instance:

```
object mapping [foo] trying to serialize a value with no field associated with it, current value [0]
```

Field `foo` has already been introduced into your mapping as type `object`, so indexing `foo` as a number won't work.

Similarly:

```
... failed to parse [foo.bar]
Caused by: java.lang.NumberFormatException: For input string: "baz"
at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
```

`foo.bar` has already been introduced as a number (probably a `long`), but now you're trying to index a `string`.

You can't change the data type of an existing field.  Once a field has been added, it's `type` is set forever more.
</comment><comment author="vadim-ex" created="2015-04-23T18:56:10Z" id="95685210">This is possible, however I am confused: we are using Logstash for log storage. It means unpredictable (application defined fields, which are changed as log objects change). Do we have wrong expectations?
</comment><comment author="clintongormley" created="2015-04-23T18:59:57Z" id="95686198">@vadim-moz not quite sure what you mean, but if you are sending logstash fields containing different data types, then those same mixed data types are going to be forwarded to Elasticsearch.  Logstash does have the ability to (eg) change a string like `"10"` to the number `10`.

If you have no control over the data type in field `foo`, then you can set it to type `object` and set `enabled: false`.  Then it will just ignore everything in that field (meaning you can't search on it either).  Otherwise you have to find some way of rewriting `foo` based on the data type
</comment><comment author="vadim-ex" created="2015-04-23T20:54:45Z" id="95715985">Thank you very much. Let me clarify my question:

I am looking for implementing logging facility for big application:

``` rb
log.info({description: 'something', exception: ErrorObject, user: something, account: something})
```

Of course (and during lifecycle of application) available fields and their representation might change unpredictably. My goal was to convert it to reasonable schema-free json, and use logstash / kibana to interface it.

The naive approach obviously does not work (judging by your comments). So, for example, I might convert the example above in something like (I presume `data` is an object you was talking about):

``` json
{
  "timestamp": "2015-04-23",
  "pid": 2132131,
  "url": "https://github.com/my-repo",
  "data": {
     "description": "something",
     "exception": {
       "class": "SuperError",
       "message": "error! error!",
       "backtrace": [
          "loc1",
          "loc2",
       ]
     },
     "user": "John Smith",
     "accoount": 123456
  }
}
```

(hope, it is valid json).

The big question: will I be able send tomorrow something like:

``` json
{
  "timestamp": "2015-04-24",
  "pid": 5697865,
  "data": {
     "description": "something else",
     "user": {
        "first-name": "John",
        "last-name": "Smith"
     },
     "accoount": "123456-pr"
  }
}
```

(I do not mean to automatically identify that it is the same `John Smith`, I need just to get these messages stored in ES.)

Thank you very much,
/vadim
</comment><comment author="clintongormley" created="2015-04-26T15:11:49Z" id="96398663">HI @vadim-moz 

&gt; The big question: will I be able send tomorrow something like:

No you won't.  The `data.user` field from the first example is already  a `string`, but in the second example you're sending it as on `object`.   Unless....

If you set the `data` object to `enabled: false` then it will ignore everything under it. The JSON will still be stored in the `_source` field, but it won't try to parse or index it.  This also means that you can't search for anything in the `data` field.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tests progress indicator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10635</link><project id="" key="" /><description>Looks like this:

```
Suite: org.elasticsearch.search.aggregations.bucket.HistogramTests
Completed [10/713] on J0 in 8.32s, 27 tests

Suite: org.elasticsearch.indices.state.OpenCloseIndexTests
Completed [11/713] on J2 in 16.23s, 20 tests
```

Upgrades randomizedtesting to 2.1.13.
</description><key id="68972158">10635</key><summary>Add tests progress indicator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T17:31:58Z</created><updated>2015-06-08T13:34:03Z</updated><resolved>2015-04-16T17:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-16T17:32:38Z" id="93792038">Awesome! LGTM
</comment><comment author="jpountz" created="2015-04-16T18:41:47Z" id="93811400">w00t
</comment><comment author="nik9000" created="2015-04-16T19:02:46Z" id="93815965">And seeing this made me upgrade to the newest junit4 plugin as well.
</comment><comment author="tlrx" created="2015-04-17T07:24:50Z" id="93933284">Nice! Now I can see if I have the time to grab a coffee before the end of the tests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate dynamic mappings updates on the master node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10634</link><project id="" key="" /><description>This commit changes dynamic mappings updates so that they are synchronous on the
entire cluster and their validity is checked by the master node. There are some
important consequences of this commit:
- a failing index request on a non-existing type does not implicitely create
  the type anymore
- dynamic mappings updates cannot create inconsistent mappings on different
  shards
- indexing requests that introduce new fields might induce latency spikes
  because of the overhead to update the mappings on the master node

Closes #8688
Closes #8650
</description><key id="68966583">10634</key><summary>Validate dynamic mappings updates on the master node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>enhancement</label><label>release highlight</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T17:01:29Z</created><updated>2015-06-06T16:53:47Z</updated><resolved>2015-04-21T09:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-16T17:04:10Z" id="93785848">This is still a work-in-progress as I have test failures on RiverTests. I am suspecting this is because this test triggers lots of concurrent dynamic mappings updates and there is a deadlock somehow because of my ignorange of how our transport works (eg. it is ok to update the mappings like I do in MapperUtils.validateDynamicMappingsUpdateOnMaster?). Any help would be greatly appreciated.
</comment><comment author="rjernst" created="2015-04-17T07:11:37Z" id="93927332">The mappings side looks great! My main question is about what to do on translog replay if sending mapping updates to the master fails.
</comment><comment author="jpountz" created="2015-04-17T07:24:31Z" id="93933201">Good question. This should hopefully not happen once mappings updates are synchronous but I believe this could happen when upgrading from 1.x. In that case I don't think we can do better than warning a scary message to the logs?
</comment><comment author="rjernst" created="2015-04-19T02:01:22Z" id="94222797">@jpountz Ok, let's put it at warn level then?
</comment><comment author="bleskes" created="2015-04-19T13:02:15Z" id="94274880">I think we should fail the shard in this case. It means that some data was indexed in a way that is inconsistent with the &#8220;master&#8221; mapping in which case you probably have rogue lucene indices. Picking up on another copy is a better alternative. If we ending up having no copies we are at least clearly communicating things are bad (master have a different mapping that is inconsistent with any shard). If people want to still recover they can intervene and delete the translog (which may loose some data but I think it&#8217;s an acceptable solution for this bad place to be in). Also note that we flush on close during node shutdown, so the chance this will happen is small...

&gt; On 19 Apr 2015, at 04:01, Ryan Ernst notifications@github.com wrote:
&gt; 
&gt; @jpountz Ok, let's put it at warn level then?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jpountz" created="2015-04-20T09:35:50Z" id="94408151">I added (and beasted) more tests about concurrent indexing requests and the rivers issue seems to be specific to rivers actually so I left mapping updates handled like today when it comes to rivers (ie. updated locally first and then propagated to the master).

Also failing to apply a mapping update on recovery now fails the shard.
</comment><comment author="jpountz" created="2015-04-20T13:10:24Z" id="94447584">@bleskes @kimchy Pushed a new commit to address your comments
</comment><comment author="purduemike" created="2015-04-22T00:33:55Z" id="94983824">+1 this is happening everyday for me during an index rollover.
Looks like this fix is slated for v2.0.0.  Can we instead merged into the next minor release?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added context for significant_terms scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10633</link><project id="" key="" /><description>Scoring heuristics can now see the value of the term being scored.
Scripted heuristic can access the raw value through the variable _term.

Closes #10613
</description><key id="68966541">10633</key><summary>Added context for significant_terms scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T17:01:09Z</created><updated>2015-06-06T19:00:23Z</updated><resolved>2015-04-17T09:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-17T09:22:51Z" id="93950968">I have closed this PR because :
a) It exposes internals we may want to keep private (prevents use of global ordinals)
b) the performance implications are questionable
c) It is servicing an edge case
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple_query_string returns nothing for "*" search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10632</link><project id="" key="" /><description>My simple_query_string returns nothing with "*" search.

If I use a query_string, it works (it returns hits).
The fields and analyzer are the same.

If I wrap my simple_query_string with a bool+must (or should) query, it works. (???)

Is it a bug?

Elasticsearch version: 1.5.1
</description><key id="68946027">10632</key><summary>simple_query_string returns nothing for "*" search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">pierrre</reporter><labels><label>:Query DSL</label><label>bug</label><label>discuss</label><label>low hanging fruit</label><label>stalled</label></labels><created>2015-04-16T15:16:18Z</created><updated>2017-03-30T10:11:13Z</updated><resolved>2017-03-30T10:11:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-16T15:18:39Z" id="93761234">I believe this may be related to #9866, which was fixed in Lucene 5.1 (see: https://issues.apache.org/jira/browse/LUCENE-6298). I'll look and see whether it is the same issue.
</comment><comment author="pierrre" created="2015-04-16T15:23:37Z" id="93762537">My Lucene version is 4.10.4.

What should happen if I search "*" with a simple_query_string?
Return all docs or nothing?
The query_string returns all docs.
</comment><comment author="clintongormley" created="2016-01-17T17:43:49Z" id="172357671">This has been fixed.  \* will return all docs
</comment><comment author="pierrre" created="2016-01-17T17:45:50Z" id="172357927">Thank you!
</comment><comment author="pierrre" created="2016-02-05T15:36:46Z" id="180404655">Actually, this search:

```
{
  "fields": [],
  "query": {
    "simple_query_string": {
      "fields": [
        "shortDescription"
      ],
      "query": "*"
    }
  }
}
```

returns nothing with Elasticsearch 2.2

@clintongormley is the fix for this issue released?
</comment><comment author="clintongormley" created="2016-02-13T19:39:09Z" id="183732901">Initially I thought this might be related to https://github.com/elastic/elasticsearch/issues/13214 but no, a query string of `"*"` doesn't match any docs, with or without custom `fields`:

```
PUT t/t/1
{}

GET t/_search
{
  "query": {
    "simple_query_string": {
      "query": "*"
    }
  }
}

GET _validate/query?explain
{
  "query": {
    "simple_query_string": {
      "query": "*"
    }
  }
}
```

The explanation returned by the last request is simply `""`
</comment><comment author="cbuescher" created="2016-09-16T12:34:39Z" id="247589373">I looked into this a bit by adding an IT test. It looks like Lucenes SimpleQueryParser only recognizes a terms with a closing '*' as prefix query. A single wildcard without leading charactes is treated as a regular term in SimpleQueryParser#consumeToken(). This might even be the intended behaviour. For `query_string`  the situation is different since it uses MapperQueryParser internally which has an `allowLeadingWildcard` option. Maybe @jimferenczi or @dakrone have an opinion on whether this can or should be fixed in SimpleQueryParser or is even intended. 
</comment><comment author="dakrone" created="2016-09-16T15:55:39Z" id="247637700">@cbuescher personally I think a SQS query for `*` should be internally rewritten to a `MatchAllDocsQuery`, which I think means this would go into Lucene. If people agree I can work on a patch for the next Lucene release
</comment><comment author="clintongormley" created="2016-09-19T14:48:17Z" id="248014929">@dakrone not so sure - should it be match all or should it match all docs that have a value for the specified fields?  See #13214 for a similar discussion
</comment><comment author="dakrone" created="2016-09-26T14:44:55Z" id="249591059">@clintongormley ahh okay, in light of that discussion:

&gt;    `"*"` always means match all documents
&gt;   `"field:*"` means find all documents that contain a non-null value in the field (incl empty string)

Since `simple_query_string` can either use the `default_field` or have `fields`
specified, I'm thinking we should do:
- If query is `"*"` and no `fields` are specified, treat it as match all
  documents
- If query is `"*"` and `fields` are specified, query is essentially `field1:*
  OR field2:* OR field3:*` which matches documents that have non-null values in
  the field

Do you think that'd be a better solution?
</comment><comment author="clintongormley" created="2016-10-06T16:05:44Z" id="252009414">@dakrone Given that the end user can't specify fields in the SQS like you can in QS (`user:kimchy`) I've come around to thinking that `*` should always mean match all docs.
</comment><comment author="dakrone" created="2016-10-11T21:17:06Z" id="253049180">I opened https://issues.apache.org/jira/browse/LUCENE-7490 for this with a patch
</comment><comment author="dakrone" created="2016-10-12T17:59:24Z" id="253289294">Looks like this will be in Lucene 6.3, so I'm marking this as "stalled" until 6.3 is released and incorporated.
</comment><comment author="weberhofer" created="2017-03-30T09:38:10Z" id="290357030">I think this issue has been solved with ES 5.3</comment><comment author="cbuescher" created="2017-03-30T10:11:13Z" id="290365064">@weberhofer thanks a lot, I just checked with 5.1 (which is based in Lucene 6.3) and 5.2 and it seems to be fixed. I will close this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query scoring change for single-value queries on numeric fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10631</link><project id="" key="" /><description>Change Long and Integer fields to use a TermQuery rather than a non-scoring NumericRangeQuery when querying single values. Allows for relevance-ranked retrieval on non-text fields with example use case being recommendation engine finding users with similar tastes in movies where users' liked movie ids are held as arrays of numbers.

Closes #10628
</description><key id="68944666">10631</key><summary>Query scoring change for single-value queries on numeric fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T15:10:20Z</created><updated>2015-04-17T15:55:06Z</updated><resolved>2015-04-17T15:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-16T17:48:33Z" id="93799080">Maybe Lucene should have a NumericTermQuery?
</comment><comment author="rmuir" created="2015-04-16T17:49:42Z" id="93799907">For the legacy numeric fields? won't this be confusing for autoprefix? what would this query do? "Analyze" the input number? why not just make NumericTokenStream work as a regular analyzer. I can think of many better ways to do it.
</comment><comment author="rmuir" created="2015-04-16T17:50:24Z" id="93800313">But mark's patch is fine (my complaints are about numeric ranges in lucene). mark's patch changes this to use a single term instead of a one-term range.
</comment><comment author="rmuir" created="2015-04-16T18:00:26Z" id="93803089">looks good to me. This pull request may have performance benefits as well. I do not think a one-term numericrangequery is as efficient (it likely pulls all the bits for matching docs for that single term eagerly instead of lazily for intersection, etc).
</comment><comment author="rmuir" created="2015-04-16T18:04:33Z" id="93803957">I wonder if there is some reason Integer and Long are singled out. Should we do it for all numeric types/dates/ip/etc?
</comment><comment author="markharwood" created="2015-04-17T08:56:28Z" id="93947115">@clintongormley  what do you think of the impact this change might have on existing queries?
For example, we steer people to use the "match" query because it is simple and "does the right thing" - this change will mean that those users who match on numeric fields will have changes to the ranking behaviour.
Ordinarily users might expect numeric queries to not rank at all and be accustomed to that. My use case for the recommendation scenario requires ranking and is perhaps an edge case that deserves special extensions to the DSL?

So is our default behaviour ranked-matching-on-everything or ranked-on-text-only?
</comment><comment author="rmuir" created="2015-04-17T11:16:55Z" id="93967239">IMO termquery does the right thing because it acts like a normal term that appears once in matching documents. Numerics already omit norms and frequencies so the score for numerics will be a constant for all documents: so with the default sim and querynorm, will users even care?

Anyway, if for some reason we want to change what this exact constant value is, we should wrap the termquery in ConstantScoreQuery (the user can do that too, always!), instead of using a less efficient RangeQuery...
</comment><comment author="brwe" created="2015-04-17T11:26:39Z" id="93968347">What about idf? Using something like idf for scoring assumes that relevance of a term can actually be estimated by its doc freq and I thought this is mainly a good heuristic for fields containing natural language. But is it also true for most numeric fields? I worry that it is not so and that this change would break things for users. Anybody knows what the original motivation for using range queries for numerics?
</comment><comment author="rmuir" created="2015-04-17T11:27:35Z" id="93968441">This is crazy. I will make my own PR that changes this to a constantscorequery(termquery()) then and you guys can debate the finer aspects of scoring.

But the performance here today is shit, the "range query of one" is completely stupid.
</comment><comment author="markharwood" created="2015-04-17T11:37:21Z" id="93969720">@rmuir that's a one line change in my existing PR if you want to take that as the basis.
There's some Junit tests that check for the existance of TermQuery as outputs from the parser that would need adjusting
</comment><comment author="rmuir" created="2015-04-17T11:40:03Z" id="93970055">Or we can commit your PR with a constantscorequery as a step and discuss the scoring as a followup? I just want these "terms" to be able to use skipdata when participating in a boolean :)
</comment><comment author="markharwood" created="2015-04-17T11:45:13Z" id="93970628">Would we want to backport the constantscorequery change to other branches?
</comment><comment author="rmuir" created="2015-04-17T11:50:06Z" id="93971241">Absolutely! its not breaking at all, unless you consider making queries faster breaking :)
</comment><comment author="markharwood" created="2015-04-17T11:52:54Z" id="93971605">:) So that might be a good reason to separate this PR into 2 stages - removing range queries for the benefit of multiple branches and adding the new/changed scoring capabilities into master
</comment><comment author="rmuir" created="2015-04-17T11:55:12Z" id="93972000">+1
</comment><comment author="markharwood" created="2015-04-17T15:55:05Z" id="94014618">The guts of this change (moving from NumericRangeQuery to CSQ wrapped TermQuery) are now in https://github.com/elastic/elasticsearch/pull/10648
I'll close this PR and open a rebased one once the above PR is landed and we have decided how to allow a plain TermQuery to be used to get scoring for numerics. Let's continue the discussion on how to achieve that over on the issue https://github.com/elastic/elasticsearch/issues/10628
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible NPE in InternalClusterService$NotifyTimeout, the `future` field is set from a different thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10630</link><project id="" key="" /><description>This is likely cause of the following failure: http://build-us-00.elastic.co/job/es_g1gc_1x_metal/7976/

```
REPRODUCE WITH  : mvn clean test -Dtests.seed=CD6728614E507BB9 -Dtests.class=org.elasticsearch.search.basic.SearchWithRandomExceptionsTests -Dtests.method="testRandomDirectoryIOExceptions" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.security.manager=false -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=pl -Dtests.timezone=Pacific/Johnston -Dtests.processors=8
  1&gt; Throwable:
  1&gt; com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=4658, name=elasticsearch[node_s4][generic][T#3], state=RUNNABLE, group=TGRP-SearchWithRandomExceptionsTests]
  1&gt;     __randomizedtesting.SeedInfo.seed([CD6728614E507BB9:849B444950169018]:0)
  1&gt; Caused by: java.lang.NullPointerException
  1&gt;     __randomizedtesting.SeedInfo.seed([CD6728614E507BB9]:0)
  1&gt;     org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:532)
  1&gt;     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;     java.lang.Thread.run(Thread.java:745)
```
</description><key id="68920383">10630</key><summary>Fix possible NPE in InternalClusterService$NotifyTimeout, the `future` field is set from a different thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T13:11:20Z</created><updated>2015-05-18T23:25:55Z</updated><resolved>2015-04-17T08:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-16T14:20:55Z" id="93746542">I think we also need a null protection at `if (future.isCancelled()) {` .
</comment><comment author="martijnvg" created="2015-04-16T14:36:07Z" id="93750138">@bleskes I added the null check.
</comment><comment author="bleskes" created="2015-04-16T15:30:52Z" id="93764402">LGTM. I think this can go to 1.5 too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch inner_hits query does not return second nested field in object field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10629</link><project id="" key="" /><description>Hi All

Please refer to issue &lt;a href="https://github.com/elastic/elasticsearch/issues/10334"&gt;10334&lt;/a&gt; that was marked as fixed in the latest release (1.5.1). I have tested this today and it still doesnt work correctly. The ArrayOutOfBoundsException is gone, however I now get incorrect data back.  Only the first entry is returned regardless of which entry has matched the search request.

To Replicate: Setup the same template and data as in issue &lt;a href="https://github.com/elastic/elasticsearch/issues/10334"&gt;10334&lt;/a&gt;:

&lt;pre&gt;&lt;code&gt;curl -XPOST 'http://localhost:9200/twitter'

curl -XPOST 'http://localhost:9200/twitter/_mapping/tweet' -d '
{
    "tweet": {
        "properties": {
            "comments": {
                "properties": {
                    "messages": {
                        "type": "nested",
                        "properties": {
                            "message": {
                                "type" : "string", 
                                "index": "not_analyzed"
                            }   
                        }
                    } 
                }
            }
        }
    }
}'

curl -XPOST 'http://localhost:9200/twitter/tweet' -d '
{
    "comments": {
        "messages": [
            {"message": "Nice website"},
            {"message": "Worst ever"}
        ]
    }
}'

&lt;/code&gt;&lt;/pre&gt;


Now search for the message "Worst ever"

&lt;pre&gt;&lt;code&gt;
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty' -d '
{
    "query": {
        "nested": {
            "path": "comments.messages",
            "query": {
                "match": {"comments.messages.message": "Worst ever"}
            },
            "inner_hits" : {}
        }
    }
}'
&lt;/code&gt;&lt;/pre&gt;


The following gets returned:

&lt;pre&gt;&lt;code&gt;
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.4054651,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "AUzCMIjnYDWhRGHuJRhC",
      "_score" : 1.4054651,
      "_source":
{
    "comments": {
        "messages": [
            {"message": "Nice website"},
            {"message": "Worst ever"}
        ]
    }
},
      "inner_hits" : {
        "comments.messages" : {
          "hits" : {
            "total" : 1,
            "max_score" : 1.4054651,
            "hits" : [ {
              "_index" : "twitter",
              "_type" : "tweet",
              "_id" : "AUzCMIjnYDWhRGHuJRhC",
              "_nested" : {
                "field" : "comments",
                "offset" : 0,
                "_nested" : {
                  "field" : "messages",
                  "offset" : 0
                }
              },
              "_score" : 1.4054651,
              "_source":{"message":"Nice website"}
            } ]
          }
        }
      }
    } ]
  }
}
&lt;/code&gt;&lt;/pre&gt;


This is clearly the incorrect inner hit. As discussed previously with martijnvg a workaround is if I change the object field in my mapping to be nested as well, like this:

&lt;pre&gt;&lt;code&gt;
curl -XPOST 'http://localhost:9200/twitter/_mapping/tweet' -d '
{
    "tweet": {
        "properties": {
            "comments": {
                "type": "nested",
                "properties": {
                    "messages": {
                        "type": "nested",
                        "properties": {
                            "message": {
                                "type" : "string", 
                                "index": "not_analyzed"
                            }   
                        }
                    } 
                }
            }
        }
    }
}'
&lt;/code&gt;&lt;/pre&gt;


however doing so results in a larger memory footprint which is undesirable. Also note the use case in #10334 in my second post on the issue that explains why I only need a normal object field containing nested fields.
</description><key id="68914384">10629</key><summary>Elasticsearch inner_hits query does not return second nested field in object field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mariusdw</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-04-16T12:34:14Z</created><updated>2015-04-30T15:05:33Z</updated><resolved>2015-04-30T15:02:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-04-16T12:56:19Z" id="93727309">@mariusdw agreed, that looks incorrect
</comment><comment author="martijnvg" created="2015-04-30T15:05:33Z" id="97830006">Fixed via #10663. If a non nested object field the direct parent of a nested object field, its field is inlined with the nested identify of the child nested object. Since a normal object isn't nested using a dummy nested identity didn't make sense. (this was the fix for #10334)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Numeric fields can't make use of Lucene ranking behaviours</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10628</link><project id="" key="" /><description>## Why you need this

I have a use case where I want the normal rules of Lucene relevance ranking to apply to numeric fields, the rules being
1) TF - repetition of a term in a doc
2) IDF - scarcity of the term in the index
3) norms - normalizing for the length of the field.

The use case might be a product recommendation engine where each doc represents a user and the field of interest is a list of numeric product IDs that the user has liked (this is the format used in the MovieLens dataset to represent users and the movies they like).
My query might be to list the movies I like and find the users who share most of my interests. Lucene relevance ranking should be doing all the heavy-lifting here. The above rules would apply as follows:
1) TF - would favour users who had repeatedly watched one of my choices
2) IDF - would ensure any common movie IDs (e.g. most people have watched Shawshank Redemption) would be scored less highly.
3) norms - would favour those users who have a concentrated list of movies than those with encyclopaedic lists of favourites.
## Example failure

In [this gist](https://gist.github.com/markharwood/d94f8c4c1a167297b3dd) we can see a simple example of where 2 users have their list of "likes" encoded as both strings or numerics. One user has a short list of likes and the other a long list. When we search we would hope to rank the user with the short list of likes that reflect our movie choices higher than the user with many choices. The example queries show how all of the string-based queries use norms effectively and how none of the numeric-based queries make use of this.
## Why it fails

All of the query parsers resolve the various different ways of expressing a numeric query down to a call to IntegerFieldMapper.termQuery(..) which produces a NumericRangeQuery with none of the usual Lucene ranking behaviour.
## Solutions

I can think of 4 options:
1) Do nothing - users wanting this ranking behaviour would have to encode numerics as strings
2) Add a new query clause to the DSL e.g. RankingNumericTerms
3) Add a "ranking_please" flag to existing query clauses e.g. Terms
4) Introduce ranking scores to all existing numeric queries

Option 1 may not be awful as this is a bit of an edge case and we may want to just add some docs around this approach.
Options 2 and 3 may warrant further investigation
Option 4 doesn't feel right for performance and backward compatibility reasons.
</description><key id="68900403">10628</key><summary>Numeric fields can't make use of Lucene ranking behaviours</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T11:07:00Z</created><updated>2015-04-27T09:05:36Z</updated><resolved>2015-04-27T09:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-16T11:20:16Z" id="93711189">I think we should just build a TermQuery manually with the help of NumericUtils?
</comment><comment author="markharwood" created="2015-04-16T11:34:04Z" id="93713322">How does that look from the user's perspective in the DSL?
</comment><comment author="jpountz" created="2015-04-16T11:39:16Z" id="93714106">Maybe we don't need to change the DSL at all and can just fix our number mappers to generate a term query instead of a range on a single term. This way term queries against numbers would always be aware of the IDF?
</comment><comment author="markharwood" created="2015-04-16T11:41:43Z" id="93714513">Nice. 
Is there a behind-the-scenes overhead to representing IDs as integers in the index? 
I'm thinking all of the trie-range encoding stuff that helps support efficient range queries. If we know something is an ID and therefore not subject to range queries maybe there's gains to be had there too?
</comment><comment author="jpountz" created="2015-04-16T11:53:23Z" id="93716214">If you don't use range queries then you can just disable trie encoding by setting precision_step=MAX_VALUE. Other than that numeric terms use a fixed-length binary encoding which is more space-efficient than string for large numbers and less space-efficient for small integers such as "1". But net/net I don't think this would be a big difference.
</comment><comment author="markharwood" created="2015-04-16T13:20:52Z" id="93732276">OK. 

&gt; Maybe we don't need to change the DSL at all and can just fix our number mappers to generate a term query instead of a range on a single term.

Would there be BWC concerns ie unexpected changes in relevance ranking for existing queries?
</comment><comment author="jpountz" created="2015-04-16T13:28:02Z" id="93733855">Field mappers know the version of the index for which they have been created so we could handle bw compat if we want. But I don't think we should worry about it?
</comment><comment author="markharwood" created="2015-04-16T14:25:23Z" id="93747613">OK running tests on a PR now..
</comment><comment author="markharwood" created="2015-04-17T16:22:50Z" id="94020496">I have broken the problem into 2 pieces now.
1) https://github.com/elastic/elasticsearch/issues/10646 Moving 2.x, 1.x and 1.5 away from NumericRangeQuery and onto ConstantScoreQuery+TermQuery
2) This issue which is about how and when we can request relevance ranking on numerics removing the ConstantScoreQuery wrapper on the TermQuery added by 1).

The key remaining question on this issue is if users expect to have relevance ranking of numerics on or off by default?

#### Arguments for no ranking by default:

1) Most users don't want, need or expect ranking on numeric fields and adding it may break the ranking on some queries (one customer is known to maintain a test suite that fails if the scores for any existing queries changes between system versions)
2) Ranges don't rank the individual numeric values, so logically why should requests for an individual numeric?

#### Arguments for ranking numerics by default:

1) There is an established means of turning off ranking in the DSL (ConstantScoreQuery wrapper) 
2) If we don't rank numerics by default we'd need to change the DSL or mapping to add an option allowing users to explicitly turn on ranking
</comment><comment author="rjernst" created="2015-04-17T18:48:23Z" id="94050875">I would say the argument is simpler: queries rank, filters do not. Since this is a numeric query, it should rank.

&gt; one customer is known to maintain a test suite that fails if the scores for any existing queries changes between system versions

They are just asking for trouble. Lucene makes no guarantees scoring will not change.
</comment><comment author="markharwood" created="2015-04-17T22:10:23Z" id="94086280">&gt; queries rank, filters do not

That's the problem - they don't always in elasticsearch. While it is cut-and-dry in Lucene land, rightly or wrongly at some stage we decided in elasticsearch that changing the query DSL ranking behaviour based on the field type being queried was usually helpful. 

While changing to the purity of the Lucene model may upset some user expectations it seems that we never documented the current behaviour with numerics - in fact [TermQuery docs](http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html) claim the functionality is exactly that of Lucene's TermQuery.

So we either document the existing "helpful" behaviour and provide new DSL options to override its defaults for odd cases or switch to the pure Query/Filter model you propose and typically require users to be smarter about ranking behaviours eg use of ConstantScoreQuery.
</comment><comment author="markharwood" created="2015-04-21T15:15:24Z" id="94834970">The consensus from a quick poll is in favour of retaining the existing default of queries not ranking on numeric fields. This means this issue is now about adding new options in the Query DSL for allowing user overrides which explicitly enable Lucene scoring for numerics.
</comment><comment author="jpountz" created="2015-04-21T15:27:24Z" id="94838737">Why do we need new options in the query DSL, users who don't want scoring can just wrap in `constant_score`?
</comment><comment author="rmuir" created="2015-04-21T15:42:03Z" id="94843711">I think so too. i think it should be IDF-based, but norms and frequencies should remain omitted. This way the score is the same for every document that matches the term, it just reflects the popularity of the term.

This is completely consistent with querying other non-full text fields, like a `not_analyzed` string.

if someone wants to omit the IDF, we have `constant_score` for that, just like with anything else.
</comment><comment author="jpountz" created="2015-04-21T15:46:44Z" id="94846300">To elaborate a bit more on my previous comment, I don't have a strong opinion about whether these queries should score or not by default (I like the scoring one a bit more but could go the other way if there is consensus) but I'm against adding a new option to the query DSL for that. To me there are only two things we can do: either make numeric term queries score and require users to wrap in `constant_score` if they don't want scoring or make numeric term queries never score and require to index as strings to have IDF contributions to the score.
</comment><comment author="markharwood" created="2015-04-21T15:56:05Z" id="94851510">I'm in favour of continuing with the constant-scoring default for numerics (that's arguably what most users expect).

For the power user (OK, me) both IDF and norms are things I would like to be ranking factors for the reasons I outlined in the opening description. 
The encode-numbers-as-strings solution feels hacky but it avoids complicating the DSL and is probably only required for edge cases like the one I outlined. 
I'm happy to close this issue if we feel that's the favoured approach - at least we had the debate and uncovered some stuff along the way.
</comment><comment author="markharwood" created="2015-04-21T17:35:10Z" id="94881364">@brwe raises another interesting option - tackling the issue via mapping definitions.
A single flag held against a numeric field could be used to determine the scoring/non-scoring query policy used by default.
</comment><comment author="jpountz" created="2015-04-21T20:22:58Z" id="94928736">&gt; I'm in favour of continuing with the constant-scoring default for numerics (that's arguably what most users expect).

I don't know if that many users have scoring expectations when it comes to numerics. I don't have data to back me but I was assuming numerics were mostly useful for filtering (and sorting) in which case scoring does not matter?

I don't like the mapping option better than the query DSL option. In my opinion there are already too many options and settings in general, and even if most of them look harmless, they are hindering our ability to move forward. So I would really prefer not to add new options, especially given that we already have a way to turn scoring off using `constant_score`.
</comment><comment author="rjernst" created="2015-04-21T20:36:16Z" id="94931958">I agree we already have this ability and shouldn't add an alternative way to turn off scoring. It is _very_ difficult to remove such options later; we should aggressively guard from adding new options and find ways to do things without further complicating the api (query, mappings, etc).
</comment><comment author="markharwood" created="2015-04-21T21:52:35Z" id="94953986">&gt; and shouldn't add an alternative way to turn off scoring.

Just in case there's any confusion - we were talking about adding a way to turn _on_ scoring.

We have 3 options:
1) Do nothing - meaning we stick with a model where you can't relevance rank on numeric fields.
2) Stick with current model of no-ranking on numeric fields but add an option (query DSL or mapping) to enable scoring.
3) Change the default query behaviour so numeric queries are ranked (there are existing ways to disable scoring in the DSL)

I understand you're not happy with 2) and I'm nervous about 3 - at some stage we obviously felt that deliberately disabling ranking on numerics was a generally useful thing for most users and we've had no complaints so a change may be upsetting. 
Given the need for ranking on numerics is rare, 1) might be the safest option because a client workaround is numerics can always be provided as strings.
</comment><comment author="markharwood" created="2015-04-24T10:10:07Z" id="95881446">Spoke with @clintongormley and we agreed to go with option 3) in my previous comment - queries on numeric fields will be changed to use Lucene ranking by default.
This change will involve: 
- removing the ConstantScoreQuery wrapper around the TermQuery objects produced in NumberFieldMapper. 
- Adding a note to migration docs to document this change in default ranking behaviour.
</comment><comment author="clintongormley" created="2015-04-24T10:11:41Z" id="95881670">At the moment there is no way of using scoring with numerics, but I'd be against adding new parameters to enable this.  I agree with @jpountz that it should be done within the existing framework.

Currently, the constant-scoring of numeric queries is not documented (and indeed surprised me early on when I was experimenting with Elasticsearch).  With the move to Lucene 5, queries = filters + scoring.  I'd be happy for making that true for numeric term queries as well.

We already tell people that queries on structured data which shouldn't affect scoring should be specified as filters, so I think that the majority of people will see absolutely no change in behaviour. For the minority who do see a change, the fix is simple: use constant_score or the `filter` clause in the `bool` query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: require_field_match on by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10627</link><project id="" key="" /><description>The current default value for the `require_field_match` option is `false` for all highlighters. That means that when it comes to highlighting field names are pretty much ignored. Highlighted fragments will get returned if they contain occurrences of the terms extracted out of the query, regardless of which fields were queried. That seems like an odd default (not to mention this option is odd and should probably be removed at some point).

I've been seeing users confused by this too (e.g. #10559 &amp; #10496), which made me think we might want to at least change the default to `require_field_match: true` for 2.0.
</description><key id="68879664">10627</key><summary>Highlighting: require_field_match on by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T09:13:30Z</created><updated>2015-05-15T19:39:27Z</updated><resolved>2015-05-15T19:39:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove XMoreLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10626</link><project id="" key="" /><description>This class is a fork of Lucene's `MoreLikeThis` but already diverged quite a lot. If we want to be able to maintain this functionality in the long term, we need to merge improvements back to Lucene, otherwise at some point changes coming from Lucene upgrades will be impossible to reconciliate with changes of this fork.
</description><key id="68875691">10626</key><summary>Remove XMoreLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-04-16T08:47:39Z</created><updated>2016-01-17T17:41:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove XPostingsHighlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10625</link><project id="" key="" /><description>This class is a fork of Lucene's PostingsHighlighter. Quoting its javadocs:

```
FORKED from Lucene 4.5 to be able to:
1) support discrete highlighting for multiple values, so that we can return a different snippet per value when highlighting the whole text
2) call the highlightField method directly from subclasses and provide the terms by ourselves
3) Applied LUCENE-4906 to allow PassageFormatter to return arbitrary objects (LUCENE 4.6)

All our changes start with //BEGIN EDIT
```

I don't think we can maintain such forks in the long term as reconciliating changes that come from Lucene upgrades with changes that we did ourselves might even be impossible at some point. So I think it's important that everything that we need gets merged back to Lucene's PostingsHighlighter and that we remove XPostingsHighlighter from Elasticsearch.
</description><key id="68873795">10625</key><summary>Remove XPostingsHighlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Highlighting</label><label>PITA</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T08:36:00Z</created><updated>2015-05-15T19:00:54Z</updated><resolved>2015-05-15T19:00:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-16T09:03:55Z" id="93684886">I agree this is a problem, it is impossible to maintain. 

To give you some more background here, the idea behind this experiment was to support the postings highlighter in elasticsearch, but trying to make it work as similar as possible to the other highlighters in terms of supported options and the result they provide.

One problem was supporting `fragment_size: 0` which means highlighting the whole content of a field, against multiple values. Other highlighters return an array, one fragment per value, while the postings highlighter from lucene used to return all values in a single string. That's why we added "discrete per value highlighting" as a short-term solution.

One other problem was the `require_field_match` option that wasn't and probably will never be supported in lucene's postings highlighters, but other highlighters do support it. That's the ability to ignore which fields were queried, and just look for the terms in any field that needs to be highlighted.

In order to get rid of this custom class, either we 
1) accept these differences between the standard postings highlighter from lucene and the other highlighters. We document them and we break bw comp as for the above two behaviours

2) find a way to contribute these two changes back to lucene

To be honest, I don't see 2) happen, especially for `require_field_match` so I'd vote for 1). At the end of the day, it is a different highlighter and we already tell people to try out the different highlighters without expecting exactly the same results.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple recoveries from engine flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10624</link><project id="" key="" /><description>In order to safely complete recoveries / relocations we have to keep all operation done since the recovery start at available for replay. At the moment we do so by preventing the engine from flushing and thus making sure that the operations are kept in the translog. A side effect of this is that the translog keeps on growing until the recovery is done. This is not a problem as we do need these operations but if the another recovery starts concurrently it may have an unneededly long translog to replay. Also, if we shutdown the engine for some reason at this point (like when a node is restarted)  we have to recover a long translog when we come back.

To void this, the translog is changed to be based on multiple files instead of a single one. This allows recoveries to keep hold to the files they need while allowing the engine to flush and do a lucene commit (which will create a new translog files bellow the hood).

Change highlights:
- Refactor Translog file management to allow for multiple files.
- Translog maintains a list of referenced files, both by outstanding recoveries and files containing operations not yet committed to Lucene.
- A new Translog.View concept is introduced, allowing recoveries to get a reference to all currently uncommitted translog files plus all future translog files created until the view is closed. They can use this view to iterate over operations.
- Recovery phase3 is removed. That phase was replaying operations while preventing new writes to the engine. This is unneeded as standard indexing also send all operations from the start of the recovery  to the recovering shard. Replay all ops in the view acquired in recovery start is enough to guarantee no operation is lost.
- Opening and closing the translog is now the responsibility of the IndexShard. ShadowIndexShards do not open the translog.
- Moved the ownership of translog fsyncing to the translog it self, changing the responsible setting to `index.translog.sync_interval` (was `index.gateway.local.sync`)

There are a still some no commits and some open issues around the fact that ShadowIndexShards doesn't have a translog (I have some ideas for solutions but I want to discuss before making the PR even bigger). Finally  `testConcurrentWriteViewsAndSnapshot` has a concurrency issue (test bug) I need to solve but I think we can start the review cycles. @s1monw @dakrone can you have a look?
</description><key id="68869842">10624</key><summary>Decouple recoveries from engine flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>breaking</label><label>enhancement</label><label>release highlight</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-04-16T08:13:05Z</created><updated>2016-05-11T14:23:45Z</updated><resolved>2015-05-05T10:01:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-16T23:23:05Z" id="93861185">@bleskes whew, left a lot of comments
</comment><comment author="bleskes" created="2015-04-17T15:50:13Z" id="94013599">@dakrone thx for the thorough review. I pushed an update.
</comment><comment author="s1monw" created="2015-04-23T14:21:17Z" id="95602885">I like what I see a lot. Yet, I think we still need to work on the unittest end to add more basic tests, it's just a feeling but we should have more tests that just make use of these classes as they are intended. I can help here once we are closer!
</comment><comment author="bleskes" created="2015-04-24T21:28:47Z" id="96069741">@s1monw @dakrone pushed another commit or addressing yours and @kimchy 's feedback.
</comment><comment author="s1monw" created="2015-04-28T14:39:53Z" id="97087324">@bleskes I added some answers to your comments
</comment><comment author="bleskes" created="2015-04-28T19:38:49Z" id="97180753">@s1monw I pushed another round. Also added a test and removed the last no commit. I think this is ready now?
</comment><comment author="s1monw" created="2015-04-28T19:45:36Z" id="97183592">@bleskes I added some minor commetns on the commit - LGTM feel free up push once fixing
</comment><comment author="s1monw" created="2015-04-29T09:35:00Z" id="97369854">LGTM
</comment><comment author="yiguolei" created="2016-04-22T03:34:34Z" id="213230331"> @bleskes When user delete a doc during recovery, and then the recovery process will send a index request to replica, the replica will index the doc again, so that the doc will appeared again since we removed phase3
</comment><comment author="bleskes" created="2016-05-11T14:23:45Z" id="218475024">@yiguolei sorry for the late response, but doc deletes are versioned just like any other write operation and can arrive out of order to the replicas. When the indexing operation is replayed it will not override the delete operation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix missing comma in context suggester docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10623</link><project id="" key="" /><description /><key id="68840793">10623</key><summary>Fix missing comma in context suggester docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snikch</reporter><labels /><created>2015-04-16T04:18:18Z</created><updated>2015-04-23T12:05:32Z</updated><resolved>2015-04-23T12:05:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T12:05:22Z" id="95560686">thanks @snikch - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] Fixing typo in module.scripting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10622</link><project id="" key="" /><description>Script filename in example seems to have a typo
</description><key id="68830617">10622</key><summary>[Doc] Fixing typo in module.scripting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bzz</reporter><labels /><created>2015-04-16T02:37:15Z</created><updated>2015-04-23T12:01:37Z</updated><resolved>2015-04-23T12:01:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T12:01:25Z" id="95560105">thanks @bzz - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Fix shadow engine tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10621</link><project id="" key="" /><description>After a3f0789 these tests fail because the translog getPath returns a
path that is a CWD path (even though it is unneeded).

This also adds the `@SuppressFileSystems("*")` annotation to the base classes `ElasticsearchTestCase` and `ElasticsearchLuceneTestCase`.

This is a short term fix as @rmuir is addressing this in a better way long-term.
</description><key id="68794859">10621</key><summary>[TEST] Fix shadow engine tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T21:52:16Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-15T22:21:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-15T21:57:40Z" id="93580560">+1
</comment><comment author="rjernst" created="2015-04-15T22:10:38Z" id="93583685">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ability to specify default mapping on disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10620</link><project id="" key="" /><description>Having to set special files on every node seems like an anti-pattern for one of the main goals for ES (everything available through an API).  We already have the ability to set the default mapping through the API, and the code handling disk based default mappings just adds complication without benefit.  We should remove this for 2.0.
</description><key id="68783585">10620</key><summary>Remove ability to specify default mapping on disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T20:46:26Z</created><updated>2015-04-30T20:51:15Z</updated><resolved>2015-04-30T20:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-15T21:24:44Z" id="93573625">+1
</comment><comment author="clintongormley" created="2015-04-23T11:45:08Z" id="95558069">I'm strongly in favour of this.  Using config files to set mappings is an anti-pattern, which just creates more maintenance and more room for error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detect "tag" fields and disable term frequency check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10619</link><project id="" key="" /><description>With the current default value of `min_term_freq == 2`, the common use case of
performing MLT on a tag field (a field which only contains unique values)
results in an empty result set. Instead of changing the default value of
`min_term_freq`, this commit proposes to automatically detect such a tag
field, and in this case to skip the term frequency check altogether.
</description><key id="68757353">10619</key><summary>Detect "tag" fields and disable term frequency check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>review</label></labels><created>2015-04-15T18:36:15Z</created><updated>2015-09-14T12:18:56Z</updated><resolved>2015-09-09T09:52:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-16T08:28:26Z" id="93677109">I like this change a lot but on the other hand I don't like seeing XMoreLikeThis diverging from Lucene's MoreLikeThis. I'm good with letting this one in, but I think it should be very high priority to merge improvements to XMoreLikeThis back to Lucene and remove it from elasticsearch.
</comment><comment author="jpountz" created="2015-04-16T08:48:16Z" id="93680723">&gt; merge improvements to XMoreLikeThis back to Lucene and remove it from elasticsearch

I created #10626 to track this.
</comment><comment author="alexksikes" created="2015-04-16T09:05:29Z" id="93685320">Thanks for submitting this issue. I agree!

Something to keep in mind is that the check on frequencies is only performed at the document level, not at the index. So if the field is not really a tag field, documents with all unique terms maybe yield more results than those without. The way I see it is that we try to do our best to return results in the common case of a tag field, without having to change the default on `min_term_freq`.
</comment><comment author="alexksikes" created="2015-09-09T09:52:01Z" id="138859880">This is completely outdated. Will open a new PR at some point. Closing for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slow Search Requests For A Couple Seconds Every Minute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10618</link><project id="" key="" /><description>Running ElasticSearch 1.5.1 under Java 7 and tried Java 8 as well.

I am seeing slow search requests (~2 seconds) at the exact same second every minute across all my Java/Transport Client nodes. I have ruled out GC and can't find any other background threads running at the time. There is also no other process affecting performance on the machines.

Here is our environment setup. We are running on AWS. All nodes are located in the same region, so latency between nodes is not an issue.

![es setup](https://cloud.githubusercontent.com/assets/2270905/7165176/fe510ef0-e373-11e4-93d2-66d9ded64681.png)

The load generators are generating consistent light to moderate load on the API servers. The exact second that the slow down occurs will differ between runs but the results will always be consistent across all client nodes for the run.

It seems like some sort of heartbeat or timeout is being hit. I don't see anything in the slowlog at the times of the slow down. However, entries do get written when the cluster is first starting up and everything has been cached, so that at least confirms that it's working.

The exact number of data nodes may differ by run but there are always at least 2.

The API nodes are completely independent except for communicating with the ES cluster via the transport client (port 9300).

Here is an example run:

![screenshot 2015-04-15 13 15 47](https://cloud.githubusercontent.com/assets/2270905/7164921/ce99e2c4-e371-11e4-80fd-7ac0e953e249.png)

The load being generated seems to have no impact on the issue. It happens as consistently under light load vs heavier load. None of the servers was over 80% CPU for any of these tests and many were run under 40 - 60% CPU.

There are times when no nodes or a subset of nodes are affected in a given minute but they are all affected more often than not. And they normally resume seeing the same performance slowdown at the same second within a minute or two.

Any ideas on how to troubleshoot this issue further would be greatly appreciated. Thanks very much!
</description><key id="68744211">10618</key><summary>Slow Search Requests For A Couple Seconds Every Minute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darylrobbins</reporter><labels /><created>2015-04-15T17:45:10Z</created><updated>2015-04-15T23:45:26Z</updated><resolved>2015-04-15T23:45:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="darylrobbins" created="2015-04-15T23:45:25Z" id="93599475">This issue was actually unrelated to ElasticSearch after all. It was a problem with the load balancer configuration.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Consolidate duplicate Bounding Box classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10617</link><project id="" key="" /><description>Aggregations, Filters, Builders all have some form of a Bounding Box implementation. This task is to consolidate all of these duplicate "rogue" classes into one (possibly in GeoUtils).

Note: this consolidation could eventually be replaced w/ an integration of the Apache SIS geometry package and using the Envelope geometry.
</description><key id="68737524">10617</key><summary>[GEO] Consolidate duplicate Bounding Box classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-15T17:10:34Z</created><updated>2016-01-20T15:31:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:41:31Z" id="172357139">@MaineC has this been resolved by https://github.com/elastic/elasticsearch/pull/11969 ?
</comment><comment author="MaineC" created="2016-01-18T09:49:13Z" id="172480993">@clintongormley no, #11969 only touches the GeoBoundingBox_QueryBuilder_. What I believe @nknize is talking about is how bounding boxes are represented in general - though he'd have to comment on which classes he is referring to in particular. The two I found with a quick search github:
- [Internal class of InternalGeoBounds](https://github.com/elastic/elasticsearch/blob/148265bd164cd5a614cd020fb480d5974f523d81/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/InternalGeoBounds.java#L202)
- [Two points in GeoBoundingBoxQueryBuilder](https://github.com/elastic/elasticsearch/blob/76fa9023b6378681de34672d2e94227e8b464cfd/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java#L59)
</comment><comment author="nknize" created="2016-01-20T15:31:57Z" id="173239042">There's also 2 BoundingBox classes within Lucene. 

The full objective is to refactor all geometry logic out of elasticsearch and into lucene. This issue starts small by simply consolidating all of the different bounding box classes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unbounded threadpools in Netty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10616</link><project id="" key="" /><description>There are unbounded thread pools exist in NettyTransport (https://github.com/elastic/elasticsearch/blob/1032429d31efce215a8c9abfe4a41830d821da72/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java#L312)

I would expect them configured the same way index/search/&amp;etc threadpools. Could create pull request if needed

This is related to #5152
</description><key id="68732246">10616</key><summary>Unbounded threadpools in Netty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dernasherbrezon</reporter><labels><label>:Network</label><label>stalled</label></labels><created>2015-04-15T16:42:01Z</created><updated>2016-09-27T14:25:56Z</updated><resolved>2016-09-27T14:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T11:36:20Z" id="95555171">@spinscale what do you think?
</comment><comment author="spinscale" created="2015-04-23T12:39:04Z" id="95572778">those unbounded thread pools should go away with netty 4, as you can configure the worker count in there. Even though we can change to the configuration parameters to look like regular thread pools, there is no possibility for  resizing IIRC, because it is configured on startup due to the `NioEventLoopGroup` class of netty (4).
</comment><comment author="dakrone" created="2016-09-27T14:25:56Z" id="249880846">We are on Netty 4 now for 5.0 and this is no longer an issue as the transport does not create its own threadpools
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timestamp querying, sliding window</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10615</link><project id="" key="" /><description>When querying Elasticsearch often users need to be able to look at a time window for a period of time. Currently, Kibana is handling this translation of window from "Last x Days" to the timestamp and adding the query parts

```
"@timestamp" : {
   "gte": [timestamp]
}
```

Would be an awesome feature to be able to add the relative times to the elasticsearch query and have elasticsearch do the timestamp processing
ie

```
"@timestamp": {
    "from": "x days ago"
}
```

or something like that
</description><key id="68726821">10615</key><summary>Timestamp querying, sliding window</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgerlach</reporter><labels /><created>2015-04-15T16:19:50Z</created><updated>2015-04-15T16:30:26Z</updated><resolved>2015-04-15T16:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-15T16:26:27Z" id="93476106">I think you can do it using `now/d-1d` or `now-1h` for example.

have a look at http://www.elastic.co/guide/en/elasticsearch/guide/current/_filter_order.html

May be I misundertood your question or your question is more for Kibana project?
</comment><comment author="sgerlach" created="2015-04-15T16:30:22Z" id="93477176">Nope, that's exactly what I meant! My search-foo is lacking this morning!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to update discovery.zen.minimum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10614</link><project id="" key="" /><description>I've been trying to update `discovery.zen.minimum_master_nodes` on my elasticsearch cluster. It seems that nothing gets updated and nothing logged.

Running version 1.5.1

I also tried updating only `transient` which behaved the same way.

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 3
    },
    "transient" : {
        "discovery.zen.minimum_master_nodes" : 3
    }
}'
{"acknowledged":true,"persistent":{},"transient":{}}
```

Checking the settings afterwards I get:

```
curl -s http://localhost:9200/_cluster/settings?pretty=true
{
  "persistent" : {
    "discovery" : {
      "zen" : {
        "minimum_master_nodes" : "1"
      }
    }
  },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "all"
        }
      }
    },
    "discovery" : {
      "zen" : {
        "minimum_master_nodes" : "1"
      }
    }
  }
}
```
</description><key id="68704016">10614</key><summary>Unable to update discovery.zen.minimum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tomas-edwardsson</reporter><labels><label>bug</label></labels><created>2015-04-15T14:55:47Z</created><updated>2015-04-29T13:12:08Z</updated><resolved>2015-04-29T13:12:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-04-16T03:06:23Z" id="93625174">@tomas-edwardsson how many master eligible nodes do you have in the cluster?
</comment><comment author="tomas-edwardsson" created="2015-04-16T11:27:27Z" id="93712188">I have 5 nodes, 3 with node.data: true and 2 with node.data: false
</comment><comment author="imotov" created="2015-04-17T02:25:50Z" id="93880263">If it means that you have only 2 nodes with node.master:true setting minimum master nodes to 3 would make your cluster to inoperational until the third node with node.master : true is added. So elasticsearch prevents this situation by ignoring the setting. Check log on master. There should be a warning there.
</comment><comment author="tomas-edwardsson" created="2015-04-17T15:35:55Z" id="94010833">All the nodes have `node.master: true`.
</comment><comment author="imotov" created="2015-04-21T01:05:32Z" id="94604775">@tomas-edwardsson that's strange, could you post the output of the following command here

```
curl "localhost:9200/_nodes/settings?pretty" 
```
</comment><comment author="tomas-edwardsson" created="2015-04-21T13:13:21Z" id="94789158">```
{
  "cluster_name" : "app",
  "nodes" : {
    "cwFOF14ETsKldIRdQ_L3Yg" : {
      "name" : "app/node01",
      "transport_address" : "inet[/192.168.1.1:9300]",
      "host" : "app-1",
      "ip" : "127.0.0.1",
      "version" : "1.5.1",
      "build" : "5e38401",
      "http_address" : "inet[/192.168.1.1:9200]",
      "attributes" : {
        "aws_availability_zone" : "us-west-1a",
        "master" : "true"
      },
      "settings" : {
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "10",
          "search" : {
            "slowlog" : {
              "threshold" : {
                "fetch" : {
                  "warn" : "1s",
                  "debug" : "500ms",
                  "trace" : "200ms",
                  "info" : "800ms"
                }
              }
            }
          },
          "refresh_interval" : "30"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "script" : {
          "disable_dynamic" : "true"
        },
        "plugin" : {
          "mandatory" : "cloud-aws"
        },
        "node" : {
          "data" : "true",
          "master" : "true",
          "name" : "app/node01"
        },
        "http" : {
          "cors" : {
            "enabled" : "true",
            "allow-origin" : "/https?:\\/\\/.*\\.example\\.com/"
          }
        },
        "name" : "app/node01",
        "path" : {
          "data" : "/var/lib/elasticsearch/data",
          "work" : "/var/lib/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch",
          "logs" : "/var/log/elasticsearch"
        },
        "cloud" : {
          "aws" : {
            "region" : "us-west-1"
          },
          "node" : {
            "auto_attributes" : "true"
          }
        },
        "config" : "/etc/elasticsearch/elasticsearch.yml",
        "cluster" : {
          "name" : "app"
        },
        "indices" : {
          "cache" : {
            "filter" : {
              "size" : "30%"
            }
          },
          "memory" : {
            "index_buffer_size" : "50%"
          }
        },
        "discovery" : {
          "type" : "ec2",
          "ec2" : {
            "groups" : "appfeed"
          }
        },
        "foreground" : "yes"
      }
    },
    "Pg3-SmS9RrqKbia2fPU13A" : {
      "name" : "feeder/node01",
      "transport_address" : "inet[/192.168.1.2:9300]",
      "host" : "feeder-1",
      "ip" : "127.0.0.1",
      "version" : "1.5.1",
      "build" : "5e38401",
      "http_address" : "inet[/192.168.1.2:9200]",
      "attributes" : {
        "aws_availability_zone" : "us-west-1a",
        "data" : "false",
        "master" : "true"
      },
      "settings" : {
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "10",
          "search" : {
            "slowlog" : {
              "threshold" : {
                "fetch" : {
                  "warn" : "1s",
                  "debug" : "500ms",
                  "trace" : "200ms",
                  "info" : "800ms"
                }
              }
            }
          },
          "refresh_interval" : "30"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "script" : {
          "disable_dynamic" : "true"
        },
        "plugin" : {
          "mandatory" : "cloud-aws"
        },
        "node" : {
          "data" : "false",
          "master" : "true",
          "name" : "feeder/node01"
        },
        "http" : {
          "cors" : {
            "enabled" : "true",
            "allow-origin" : "/https?:\\/\\/.*\\.example\\.com/"
          }
        },
        "name" : "feeder/node01",
        "path" : {
          "data" : "/var/lib/elasticsearch/data",
          "work" : "/var/lib/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch",
          "logs" : "/var/log/elasticsearch"
        },
        "cloud" : {
          "aws" : {
            "region" : "us-west-1"
          },
          "node" : {
            "auto_attributes" : "true"
          }
        },
        "config" : "/etc/elasticsearch/elasticsearch.yml",
        "cluster" : {
          "name" : "app"
        },
        "indices" : {
          "cache" : {
            "filter" : {
              "size" : "30%"
            }
          },
          "memory" : {
            "index_buffer_size" : "50%"
          }
        },
        "discovery" : {
          "type" : "ec2",
          "ec2" : {
            "groups" : "appfeed"
          }
        },
        "foreground" : "yes"
      }
    },
    "UyCcs5A1Q-mHdy5AubnW5w" : {
      "name" : "app/node03",
      "transport_address" : "inet[/192.168.1.3:9300]",
      "host" : "app-3",
      "ip" : "127.0.0.1",
      "version" : "1.5.1",
      "build" : "5e38401",
      "http_address" : "inet[/192.168.1.3:9200]",
      "attributes" : {
        "aws_availability_zone" : "us-west-1c",
        "master" : "true"
      },
      "settings" : {
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "10",
          "search" : {
            "slowlog" : {
              "threshold" : {
                "fetch" : {
                  "warn" : "1s",
                  "debug" : "500ms",
                  "trace" : "200ms",
                  "info" : "800ms"
                }
              }
            }
          },
          "refresh_interval" : "30"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "script" : {
          "disable_dynamic" : "true"
        },
        "plugin" : {
          "mandatory" : "cloud-aws"
        },
        "node" : {
          "data" : "true",
          "master" : "true",
          "name" : "app/node03"
        },
        "http" : {
          "cors" : {
            "enabled" : "true",
            "allow-origin" : "/https?:\\/\\/.*\\.example\\.com/"
          }
        },
        "name" : "app/node03",
        "path" : {
          "data" : "/var/lib/elasticsearch/data",
          "work" : "/var/lib/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch",
          "logs" : "/var/log/elasticsearch"
        },
        "cloud" : {
          "aws" : {
            "region" : "us-west-1"
          },
          "node" : {
            "auto_attributes" : "true"
          }
        },
        "config" : "/etc/elasticsearch/elasticsearch.yml",
        "cluster" : {
          "name" : "app"
        },
        "indices" : {
          "cache" : {
            "filter" : {
              "size" : "30%"
            }
          },
          "memory" : {
            "index_buffer_size" : "50%"
          }
        },
        "discovery" : {
          "type" : "ec2",
          "ec2" : {
            "groups" : "appfeed"
          }
        },
        "foreground" : "yes"
      }
    },
    "LOyQBO60Q8qMfWqEksCAbQ" : {
      "name" : "feeder/node02",
      "transport_address" : "inet[/192.168.1.4:9300]",
      "host" : "feeder-2",
      "ip" : "127.0.0.1",
      "version" : "1.5.1",
      "build" : "5e38401",
      "http_address" : "inet[/192.168.1.4:9200]",
      "attributes" : {
        "aws_availability_zone" : "us-west-1b",
        "data" : "false",
        "master" : "true"
      },
      "settings" : {
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "10",
          "search" : {
            "slowlog" : {
              "threshold" : {
                "fetch" : {
                  "warn" : "1s",
                  "debug" : "500ms",
                  "trace" : "200ms",
                  "info" : "800ms"
                }
              }
            }
          },
          "refresh_interval" : "30"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "script" : {
          "disable_dynamic" : "true"
        },
        "plugin" : {
          "mandatory" : "cloud-aws"
        },
        "node" : {
          "data" : "false",
          "master" : "true",
          "name" : "feeder/node02"
        },
        "http" : {
          "cors" : {
            "enabled" : "true",
            "allow-origin" : "/https?:\\/\\/.*\\.example\\.com/"
          }
        },
        "name" : "feeder/node02",
        "path" : {
          "data" : "/var/lib/elasticsearch/data",
          "work" : "/var/lib/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch",
          "logs" : "/var/log/elasticsearch"
        },
        "cloud" : {
          "aws" : {
            "region" : "us-west-1"
          },
          "node" : {
            "auto_attributes" : "true"
          }
        },
        "config" : "/etc/elasticsearch/elasticsearch.yml",
        "cluster" : {
          "name" : "app"
        },
        "indices" : {
          "cache" : {
            "filter" : {
              "size" : "30%"
            }
          },
          "memory" : {
            "index_buffer_size" : "50%"
          }
        },
        "discovery" : {
          "type" : "ec2",
          "ec2" : {
            "groups" : "appfeed"
          }
        },
        "foreground" : "yes"
      }
    },
    "m-36w4hyQN24gMH1lRsLPA" : {
      "name" : "app/node02",
      "transport_address" : "inet[/192.168.1.5:9300]",
      "host" : "app-2",
      "ip" : "127.0.0.1",
      "version" : "1.5.1",
      "build" : "5e38401",
      "http_address" : "inet[/192.168.1.5:9200]",
      "attributes" : {
        "aws_availability_zone" : "us-west-1b",
        "master" : "true"
      },
      "settings" : {
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "10",
          "search" : {
            "slowlog" : {
              "threshold" : {
                "fetch" : {
                  "warn" : "1s",
                  "debug" : "500ms",
                  "trace" : "200ms",
                  "info" : "800ms"
                }
              }
            }
          },
          "refresh_interval" : "30"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "script" : {
          "disable_dynamic" : "true"
        },
        "plugin" : {
          "mandatory" : "cloud-aws"
        },
        "node" : {
          "data" : "true",
          "master" : "true",
          "name" : "app/node02"
        },
        "http" : {
          "cors" : {
            "enabled" : "true",
            "allow-origin" : "/https?:\\/\\/.*\\.example\\.com/"
          }
        },
        "name" : "app/node02",
        "path" : {
          "data" : "/var/lib/elasticsearch/data",
          "work" : "/var/lib/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch",
          "logs" : "/var/log/elasticsearch"
        },
        "cloud" : {
          "aws" : {
            "region" : "us-west-1"
          },
          "node" : {
            "auto_attributes" : "true"
          }
        },
        "config" : "/etc/elasticsearch/elasticsearch.yml",
        "cluster" : {
          "name" : "app"
        },
        "indices" : {
          "cache" : {
            "filter" : {
              "size" : "30%"
            }
          },
          "memory" : {
            "index_buffer_size" : "50%"
          }
        },
        "discovery" : {
          "type" : "ec2",
          "ec2" : {
            "groups" : "appfeed"
          }
        },
        "foreground" : "yes"
      }
    }
  }
}
```
</comment><comment author="imotov" created="2015-04-21T17:21:07Z" id="94878494">@tomas-edwardsson thanks! We can reproduce it now. An important part that was missing is that this issue only occurs with EC2 discovery plugin. 
</comment><comment author="dadoonet" created="2015-04-29T13:12:05Z" id="97421618">Closing this issue as it has been fixed in cloud-aws, cloud-azure and cloud-gce plugins (versions not released yet)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable SignificanceHeuristics which are aware of the term being scored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10613</link><project id="" key="" /><description>We would like to implement custom significance heuristic for the significant_terms aggregation which are aware of the term being scored. In our use case the terms have part-of-speech tags as prefixes which we want to take into account when we compute the score. 

Here is an example: 
- the terms "blue" and "car" have the absolutely same document frequencies in the index
- blue is an adjective and is indexed as adj_blue 
- car is a noun and is indexed as n_car 
- nouns have score modifier 1.3, adjectives have score modifier 0.3

We want to to multiply the term scores from an underlying significance heuristic (jlh score, mutual information, chi square, gnd) with the respective modifier. 
</description><key id="68698750">10613</key><summary>Enable SignificanceHeuristics which are aware of the term being scored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">dimitardenev</reporter><labels /><created>2015-04-15T14:35:34Z</created><updated>2015-04-17T09:39:43Z</updated><resolved>2015-04-17T09:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-16T13:46:23Z" id="93737879">Thanks for opening this issue. The solution will add an extra parameter to the SignificanceHeuristic.getScore(..) method with an object representing the term being scored.

&gt; We want to to multiply the term scores from an underlying significance heuristic

We suggest that your custom heuristic simply wraps and calls a choice of existing heuristic, multiplying the result by your POS weightings for the given term. We don't want to complicate the Query DSL to support this nesting of scoring functions as it is probably an exceptional use case.
</comment><comment author="markharwood" created="2015-04-17T09:09:18Z" id="93948953">I coded up the solution to this but it presents a couple of issues:
1) It exposes internals of the term selection process that we may later want to refactor. Ultimately we would like to avoid dealing with terms and use global ordinals internally for efficiency's sake and exposing the terms to scoring heuristics would prevent us from making such a change or complicate it.
2) It adds extra performance costs to the scoring process as raw values (long primitives or ByteRefs) have to be materialized as Long or String values to be provided as context to scoring heuristics e.g. the Scripted heuristic needs to access to Long or String objects. This adds overhead and is extra work for the garbage collector.

Given these two general concerns and the fact that this use case is an uncommon one we have chosen not to implement this feature at this stage. 
There is a work-around for you in that you can potentially channel your parts-of-speech values to different indexed fields (eg. "adjText" and "nounText") and perform significant terms analysis on those fields separately.
If sufficient numbers of other people request access to terms in scoring heuristics we may choose to reconsider this feature. Thanks again for opening this issue and prompting this investigation.
</comment><comment author="dimitardenev" created="2015-04-17T09:24:32Z" id="93951192">Hello, 
first of all, thanks for the time you took to look in such a depth at the issue! I understand that an implementation will slow down considerably the computation time. Fortunately, the suggested work-around will solve our problem.
</comment><comment author="markharwood" created="2015-04-17T09:31:24Z" id="93952212">Just out of interest - have you demonstrated improved results by splitting nouns/adjectives or is this just theoretical at the moment? My assumption was that significance could always be drawn from pure stats without the need for special consideration of actual values.
</comment><comment author="dimitardenev" created="2015-04-17T09:39:43Z" id="93953301">This theoretical at the moment. It is promising though. It solves a long-standing problem with the right boosts for nouns, adjectives and verbs. Now we can return top-K nouns, verbs, adjectives to our clients and avoid merging ourselves. No need anymore to express business logic by separate boosts for nouns, verbs and adjectives.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 5.2 r1673726</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10612</link><project id="" key="" /><description>Mainly for the fixes in https://issues.apache.org/jira/browse/LUCENE-6424

This can cause confusing test failures if we try to enable lucene's mockfilesystems!!!
</description><key id="68698133">10612</key><summary>Upgrade to Lucene 5.2 r1673726</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T14:33:04Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-15T14:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-15T14:33:30Z" id="93424383">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch using wrong jvm ,force?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10611</link><project id="" key="" /><description>Hi,
I recently downloaded es 1.5.0.
When I try a simple bin/elasticsearch
I get the following:
    - RuntimeException[Java version: 1.7.0_51 suffers from critical bug https://bugs.openjdk.java.net/browse/JDK-8024830 which can cause data corruption.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
If you absolutely cannot upgrade, please add -XX:-UseSuperWord to the JVM_OPTS environment variable.
Upgrading is preferred, this workaround will result in degraded performance.]

However my java version is correctly set:
    java version "1.7.0_75"
OpenJDK Runtime Environment (IcedTea 2.5.4) (7u75-2.5.4-1~precise1)
OpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)

What could be wrong here? How do I force it to use the right version? 
I am downgrading to 1.3.5, because I don't see this error with 1.3.5

Regards,
Bhargav
</description><key id="68688806">10611</key><summary>elasticsearch using wrong jvm ,force?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rrphotosoft</reporter><labels><label>feedback_needed</label></labels><created>2015-04-15T13:52:54Z</created><updated>2015-10-30T21:03:04Z</updated><resolved>2015-10-30T21:03:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-15T14:15:58Z" id="93419622">Look and see if JAVA_HOME is set to an older one.
</comment><comment author="rrphotosoft" created="2015-04-15T16:53:30Z" id="93485245">Hi,
My java home variable is set to the correct java.
In both the .bashrc and the /etc/environment.
</comment><comment author="rmuir" created="2015-04-15T19:39:09Z" id="93543070">The java version it prints out is the version it is reporting. So something about your environment is off, you are not using the java you think you are.
</comment><comment author="rjernst" created="2015-04-15T20:05:24Z" id="93550728">&gt; I am downgrading to 1.3.5, because I don't see this error with 1.3.5
&gt; First, this won't work if you have indexed any documents (Lucene is not "forwards compatible", it has no idea how to read data written from a newer version).

But more importantly, the error is there because this is a serious bug that can cause index corruption. Ignoring it by trying to mask the problem is not going to save you from the potential corruption.

@rmuir is right, something is not right about your environment. 
</comment><comment author="rrphotosoft" created="2015-04-15T20:23:20Z" id="93555200">Hi guys,
Thanks for the input, but I'm not getting very far with this.
Where does ES look to determine the JVM?
I can't think of anything more than setting the JAVA_HOME variable.
If you have any suggestions on how I can try to debug this, it would be really useful.
Furthermore, with respect to forward compatibility, I won't be facing that, the data was originally indexed on 1.3.5, I wanted to update to leverage some of the newer features.

Someone else seems to have faced a similar issue, but there was no answer to the question on SO, wonder what they did to solve it:
http://stackoverflow.com/questions/28924660/elasticsearch-using-incorrect-jvm

Thanks again,
Bhargav
</comment><comment author="rjernst" created="2015-04-15T20:33:05Z" id="93560268">Are you on a *nix machine? Try running these commands:

```
$ readlink `which java`
/System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java
$ java -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)
$ echo $JAVA_HOME
/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home
```

If you do not have JAVA_HOME set (are you sure it is set for the user running elasticsearch? is elasticsearch started via init.d?) then elasticsearch uses whichver java it finds on `PATH`.
</comment><comment author="rrphotosoft" created="2015-04-24T02:26:01Z" id="95778102">Hi,

1.I am not starting es using init.d
I start it manually via bin/elasticsearch.
I am using Ubuntu 12.04 LTS,

2.I have checked rechecked and triple checked the home variable, it correctly points to the upgraded version of JAVA.
The JAVA_HOME variable is set for the user running ES, and is correctly pointing to my updated version.

3.The issue is that the JAVA that ES is picking up is different from the one on my HOME  variable.
Is there any way to debug this through the .jar files and look at what path ES  is using and why?
Which files should I be looking at? I don't mind compiling from source. I'm getting stonewalled trying to use newer features, and each time, I find that upgrading to 1.5+ is what I need to be doing ,a version upgrade is now really important.

Regards,
RR.
</comment><comment author="imotov" created="2015-04-24T02:48:19Z" id="95780505">@rrphotosoft if you are running elasticsearch using `bin/elasticsearch` script, java selection occurs [here](https://github.com/elastic/elasticsearch/blob/1.5/bin/elasticsearch#L111).  You don't need to recompile anything to debug it. It's just a shell script. Could you run the following command to make sure that JAVA_HOME is configured correctly:

```
ls -l $JAVA_HOME/bin/java
```
</comment><comment author="dakrone" created="2015-10-30T21:03:04Z" id="152650529">Closing since we have not heard back in over 6 months.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ref count write operations on IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10610</link><project id="" key="" /><description>This commit adds a counter for IndexShard that keeps track of how many write operations
are currently in flight on a shard. The counter is incremented whenever a write request is submitted in TransportShardReplicationOperationAction and decremented when it is finished. On a primary it stays incremented while replicas are being processed. The counter is an instance of AbstractRefCounted. Once this counter reaches 0 each write operation will be rejected with an IndexShardClosedException.

As a follow up we could use this counter to block closing of a shard until all pending write operations have been processed. Currently we only decrement the counter once on IndexShard.close() but do not wait for the counter to reach 0.
</description><key id="68662261">10610</key><summary>Ref count write operations on IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T12:00:33Z</created><updated>2015-05-29T17:53:50Z</updated><resolved>2015-05-05T09:24:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-04-16T15:10:43Z" id="93759161">Addressed all comments. Want to take another look?
I am unsure about the Integration test I added: SimpleIndexShardCounterIntegrationTests. It tests if replicas are failed in case the counter cannot be increased anymore. We should never get in this state so I do not know if we actually need to test this. If not I can also remove this check I guess: https://github.com/elastic/elasticsearch/pull/10610/files#diff-9669e07f0556311d187e534e321a0393R728
</comment><comment author="s1monw" created="2015-04-16T15:37:23Z" id="93766034">I did another review of this
</comment><comment author="s1monw" created="2015-04-16T19:54:59Z" id="93825969">I did another review...

&gt;  am unsure about the Integration test I added: SimpleIndexShardCounterIntegrationTests. It tests if 

I think we don't need any integration test here really, we have enough integration tests that test this and your assertions are good too?
</comment><comment author="brwe" created="2015-04-27T07:36:54Z" id="96537145">I rebased on master now that #10749 has been pushed and replaced all integration tests by unit tests similar to the ones in #10749. I like the new test much more than the old ones but I had some trouble simulating an IndexShard and am unsure if the way I do it is a little too hacky. Please have another look! 
</comment><comment author="s1monw" created="2015-04-28T15:40:18Z" id="97110051">I did another review round looks very good. Left some cosmetic comments mainly
</comment><comment author="s1monw" created="2015-04-28T16:14:33Z" id="97123801">btw. @brwe I think you can trash most of the test code in here by simply using `RefCounted` here instead of hard coding IndexShard and IndicesService. I think this abstraction is totally ok and we should just test what we really need instead of mocking all this stuff everywhere.
</comment><comment author="brwe" created="2015-04-30T13:16:18Z" id="97774959">Thanks again for the review! Addressed all comments or answered. 

&gt; I think you can trash most of the test code in here by simply using RefCounted here instead of hard 
&gt; coding IndexShard and IndicesService.

Not sure what 'use' means here.
I now moved the ref counting in IndexShard to a separate class that I can use in tests and TransportShardReplicationOperationAction. @s1monw is this what you meant? I made it inherit from AbstractRefCounted also but am not sure this is needed or a good idea. I liked the method names incrementOperationsCounter better than incRef. I am not passionate though.
</comment><comment author="s1monw" created="2015-04-30T20:00:23Z" id="97950981">&gt; incrementOperationsCounter better than incRef. I am not passionate though

I hear you! I added a comment with a suggestion to simplify it and bring `incrementOperationsCounter` back :) lemme know what you think
</comment><comment author="brwe" created="2015-05-04T10:29:21Z" id="98671531">Chatted with @s1monw and implemented all changes now. Want to have another look?
</comment><comment author="brwe" created="2015-05-04T11:37:42Z" id="98683923">pushed another commit
</comment><comment author="s1monw" created="2015-05-04T11:51:18Z" id="98686038">LGTM
</comment><comment author="brwe" created="2015-05-04T16:16:42Z" id="98770247">still have to backport to 1.x...
</comment><comment author="brwe" created="2015-05-05T09:24:43Z" id="99007348">pushed to 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Failed to parse filter for alias/no field mapping can be found" with field declared in _default_ mapping when using _bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10609</link><project id="" key="" /><description>This is very closely related to scenario described in #10038:
there is an index template with aliases using filter on some fields and a new events come in, failing to created the index when this doesn't yet exists (typical use case is logstash)

The difference here is that field used in index template to define the alias _is_ declared under default mapping, but still , the problem happens if _bulk endpoint is in use. normal single index request works fine.

Repro using ES 1.5.0 - LS 1.4.2 (same behaviour on ES 1.4.1)

```

# user in _default_ mapping , term filter in alias

PUT _template/template_1
{
  "template": "test1*",
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "_default_": {
      "properties": {
        "user": {
          "type": "string"
        }
      }
    }
  },
  "aliases": {
    "filtered-alias1": {
      "filter": {
        "term": {
          "user": "john"
        }
      }
    }
  }
}
```

Using logstash 1.4.2 or 1.5.0beta1

```
abonuccelli@w530 /opt/elk/TEST/logstash-1.4.2 $ cat config/template-filter-alias-stdout.cnf 
input{ 
stdin{}
}
filter{
}
output{
stdout{
    codec=&gt;rubydebug
}
elasticsearch {
        host =&gt; "localhost"
        protocol =&gt; http
        manage_template =&gt; false
        index =&gt; "test1-%{+YYYY.MM.dd}"
    }
}
```

start logstash parse a doc via logstash

```
abonuccelli@w530 /opt/elk/TEST/logstash-1.4.2 $ ./bin/logstash -f config/template-filter-alias-stdout.cnf --debug 
Reading config file {:file=&gt;"logstash/agent.rb", :level=&gt;:debug, :line=&gt;"301"}
Compiled pipeline code:
@inputs = []
@filters = []
@outputs = []
@input_stdin_1 = plugin("input", "stdin")

@inputs &lt;&lt; @input_stdin_1

@output_stdout_2 = plugin("output", "stdout", LogStash::Util.hash_merge_many({ "codec" =&gt; ("rubydebug".force_encoding("UTF-8")) }))

@outputs &lt;&lt; @output_stdout_2
@output_elasticsearch_3 = plugin("output", "elasticsearch", LogStash::Util.hash_merge_many({ "host" =&gt; ("localhost".force_encoding("UTF-8")) }, { "protocol" =&gt; ("http".force_encoding("UTF-8")) }, { "manage_template" =&gt; ("false".force_encoding("UTF-8")) }, { "index" =&gt; ("test1-%{+YYYY.MM.dd}".force_encoding("UTF-8")) }))

@outputs &lt;&lt; @output_elasticsearch_3
  @filter_func = lambda do |event, &amp;block|
    extra_events = []
    @logger.debug? &amp;&amp; @logger.debug("filter received", :event =&gt; event.to_hash)

    extra_events.each(&amp;block)
  end
  @output_func = lambda do |event, &amp;block|
    @logger.debug? &amp;&amp; @logger.debug("output received", :event =&gt; event.to_hash)
    @output_stdout_2.handle(event)
    @output_elasticsearch_3.handle(event)

  end {:level=&gt;:debug, :file=&gt;"logstash/pipeline.rb", :line=&gt;"26"}
config LogStash::Codecs::Line/@charset = "UTF-8" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Inputs::Stdin/@debug = false {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Inputs::Stdin/@codec = &lt;LogStash::Codecs::Line charset=&gt;"UTF-8"&gt; {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Inputs::Stdin/@add_field = {} {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::Stdout/@codec = &lt;LogStash::Codecs::RubyDebug &gt; {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::Stdout/@type = "" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::Stdout/@tags = [] {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::Stdout/@exclude_tags = [] {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::Stdout/@workers = 1 {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Codecs::Plain/@charset = "UTF-8" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@host = "localhost" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@protocol = "http" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@manage_template = false {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@index = "test1-%{+YYYY.MM.dd}" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@type = "" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@tags = [] {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@exclude_tags = [] {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@codec = &lt;LogStash::Codecs::Plain charset=&gt;"UTF-8"&gt; {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@workers = 1 {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@template_name = "logstash" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@template_overwrite = false {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@document_id = nil {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@embedded = false {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@embedded_http_port = "9200-9300" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@max_inflight_requests = 50 {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@flush_size = 5000 {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@idle_flush_time = 1 {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
config LogStash::Outputs::ElasticSearch/@action = "index" {:level=&gt;:debug, :file=&gt;"logstash/config/mixin.rb", :line=&gt;"105"}
Pipeline started {:level=&gt;:info, :file=&gt;"logstash/pipeline.rb", :line=&gt;"78"}
New Elasticsearch output {:cluster=&gt;nil, :host=&gt;"localhost", :port=&gt;"9200", :embedded=&gt;false, :protocol=&gt;"http", :level=&gt;:info, :file=&gt;"logstash/outputs/elasticsearch.rb", :line=&gt;"252"}
asdasdasd
output received {:event=&gt;{"message"=&gt;"asdasdasd", "@version"=&gt;"1", "@timestamp"=&gt;"2015-04-15T09:51:18.096Z", "host"=&gt;"w530"}, :level=&gt;:debug, :file=&gt;"(eval)", :line=&gt;"21"}
{
       "message" =&gt; "asdasdasd",
      "@version" =&gt; "1",
    "@timestamp" =&gt; "2015-04-15T09:51:18.096Z",
          "host" =&gt; "w530"
}
Flushing output {:outgoing_count=&gt;1, :time_since_last_flush=&gt;2.154, :outgoing_events=&gt;{nil=&gt;[["index", {:_id=&gt;nil, :_index=&gt;"test1-2015.04.15", :_type=&gt;"logs"}, {"message"=&gt;"asdasdasd", "@version"=&gt;"1", "@timestamp"=&gt;"2015-04-15T09:51:18.096Z", "host"=&gt;"w530"}]]}, :batch_timeout=&gt;1, :force=&gt;nil, :final=&gt;nil, :level=&gt;:debug, :file=&gt;"stud/buffer.rb", :line=&gt;"207"}
```

tcpdump capture of it

```
11:52:54.717052 IP (tos 0x0, ttl 64, id 57438, offset 0, flags [DF], proto TCP (6), length 138)
    127.0.0.1.32768 &gt; 127.0.0.1.9200: Flags [P.], cksum 0xfe7e (incorrect -&gt; 0x05af), seq 245:331, ack 537, win 350, options [nop,nop,TS val 167425274 ecr 167401133], length 86
..........#.Z....g_-...^.~.....
    ... .V.POST /_bulk HTTP/1.1
host: localhost
connection: keep-alive
content-length: 159


11:52:54.717234 IP (tos 0x0, ttl 64, id 57439, offset 0, flags [DF], proto TCP (6), length 211)
    127.0.0.1.32768 &gt; 127.0.0.1.9200: Flags [P.], cksum 0xfec7 (incorrect -&gt; 0xb5b8), seq 331:490, ack 537, win 350, options [nop,nop,TS val 167425274 ecr 167401133], length 159
E...._@.@.[...........#.Z..=.g_-...^.......
    ... .V.{"index":{"_id":null,"_index":"test1-2015.04.15","_type":"logs"}}
{"message":"asdasdasd","@version":"1","@timestamp":"2015-04-15T09:52:54.697Z","host":"w530"}
```

elasticsearch 1.5.0 log

```

[2015-04-15 11:52:54,728][DEBUG][action.admin.indices.create] [nodeM1] [test1-2015.04.15] failed to create
org.elasticsearch.ElasticsearchIllegalArgumentException: failed to parse filter for alias [filtered-alias1]
    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:142)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:413)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:365)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.query.QueryParsingException: [test1-2015.04.15] Strict field resolution and no field mapping can be found for the field with name [user]
    at org.elasticsearch.index.query.QueryParseContext.failIfFieldMappingNotFound(QueryParseContext.java:422)
    at org.elasticsearch.index.query.QueryParseContext.smartFieldMappers(QueryParseContext.java:397)
    at org.elasticsearch.index.query.TermFilterParser.parse(TermFilterParser.java:111)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:368)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:349)
    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:151)
    at org.elasticsearch.cluster.metadata.AliasValidator.validateAliasFilter(AliasValidator.java:140)
    ... 7 more
```

doing the same via curl - same problem

```
Antonios-MacBook-Air-2:~ abonuccelli$ curl -XPUT w530:9200/_bulk -d '{"index":{"_id":null,"_index":"test1-2015.04.15","_type":"logs"}}
{"message":"asdasdasd","@version":"1","@timestamp":"2015-04-15T09:52:54.697Z","host":"w530"}
'
{"took":22,"errors":true,"items":[{"index":{"_index":"test1-2015.04.15","_type":"logs","_id":null,"status":400,"error":"RemoteTransportException[[nodeM1][inet[/192.168.0.101:9304]][indices:admin/create]]; nested: ElasticsearchIllegalArgumentException[failed to parse filter for alias [filtered-alias1]]; nested: QueryParsingException[[test1-2015.04.15] Strict field resolution and no field mapping can be found for the field with name [user]]; "}}]
```

if using normal single index request

```
Antonios-MacBook-Air-2:~ abonuccelli$ curl -XPUT w530:9200/test1-2015.04.15/type/1 -d '
{"message":"asdasdasd","@version":"1","@timestamp":"2015-04-15T09:52:54.697Z","host":"w530"}'
```

there is no problem

```
[2015-04-15 12:07:34,356][DEBUG][cluster.service          ] [nodeM1] processing [create-index [test1-2015.04.15], cause [auto(index api)]]: execute
[2015-04-15 12:07:34,357][DEBUG][indices                  ] [nodeM1] creating Index [test1-2015.04.15], shards [1]/[2]
[2015-04-15 12:07:34,373][DEBUG][index.mapper             ] [nodeM1] [test1-2015.04.15] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/PROD/nodeM1/elasticsearch-1.5.0/lib/elasticsearch-1.5.0.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]
[2015-04-15 12:07:34,373][DEBUG][index.cache.query.parser.resident] [nodeM1] [test1-2015.04.15] using [resident] query cache with max_size [100], expire [null]
[2015-04-15 12:07:34,374][DEBUG][index.store.fs           ] [nodeM1] [test1-2015.04.15] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]
[2015-04-15 12:07:34,377][INFO ][cluster.metadata         ] [nodeM1] [test1-2015.04.15] creating index, cause [auto(index api)], templates [template_1], shards [1]/[2], mappings [_default_, type]
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing ... (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index service (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index cache (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][index.cache.filter.weighted] [nodeM1] [test1-2015.04.15] full cache clear, reason [close]
[2015-04-15 12:07:34,382][DEBUG][index.cache.fixedbitset  ] [nodeM1] [test1-2015.04.15] clearing all bitsets because [close]
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] clearing index field data (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing analysis service (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index engine (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index gateway (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing mapper service (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index query parser service (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closing index service (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,382][DEBUG][indices                  ] [nodeM1] [test1-2015.04.15] closed... (reason [cleaning up after validating index on master])
[2015-04-15 12:07:34,383][DEBUG][cluster.service          ] [nodeM1] cluster state updated, version [183], source [create-index [test1-2015.04.15], cause [auto(index api)]]
[2015-04-15 12:07:34,387][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] creating index
[2015-04-15 12:07:34,387][DEBUG][indices                  ] [nodeD2] creating Index [test1-2015.04.15], shards [1]/[2]
[2015-04-15 12:07:34,398][DEBUG][index.mapper             ] [nodeD2] [test1-2015.04.15] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[jar:file:/opt/elk/PROD/nodeD2/elasticsearch-1.5.0/lib/elasticsearch-1.5.0.jar!/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]
[2015-04-15 12:07:34,398][DEBUG][index.cache.query.parser.resident] [nodeD2] [test1-2015.04.15] using [resident] query cache with max_size [100], expire [null]
[2015-04-15 12:07:34,399][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]
[2015-04-15 12:07:34,399][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding mapping [_default_], source [{"_default_":{"properties":{"user":{"type":"string"}}}}]
[2015-04-15 12:07:34,399][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding mapping [type], source [{"type":{"properties":{"user":{"type":"string"}}}}]
[2015-04-15 12:07:34,400][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15] adding alias [filtered-alias1], filter [{"term":{"user":"john"}}]
[2015-04-15 12:07:34,400][DEBUG][indices.cluster          ] [nodeD2] [test1-2015.04.15][0] creating shard
[2015-04-15 12:07:34,400][DEBUG][index                    ] [nodeD2] [test1-2015.04.15] creating shard_id [test1-2015.04.15][0]
[2015-04-15 12:07:34,402][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using [/opt/elk/PROD/FS/data/nodeD2/tony_prod/nodes/0/indices/test1-2015.04.15/0/index] as shard's index location
[2015-04-15 12:07:34,402][DEBUG][index.store              ] [nodeD2] [test1-2015.04.15][0] store stats are refreshed with refresh_interval [10s]
[2015-04-15 12:07:34,402][DEBUG][index.merge.scheduler    ] [nodeD2] [test1-2015.04.15][0] using [concurrent] merge scheduler with max_thread_count[3], max_merge_count[5]
[2015-04-15 12:07:34,402][DEBUG][index.store.fs           ] [nodeD2] [test1-2015.04.15] using [/opt/elk/PROD/FS/data/nodeD2/tony_prod/nodes/0/indices/test1-2015.04.15/0/translog] as shard's translog location
[2015-04-15 12:07:34,403][DEBUG][index.deletionpolicy     ] [nodeD2] [test1-2015.04.15][0] Using [keep_only_last] deletion policy
[2015-04-15 12:07:34,403][DEBUG][index.merge.policy       ] [nodeD2] [test1-2015.04.15][0] using [tiered] merge mergePolicy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]
[2015-04-15 12:07:34,403][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [CREATED]
[2015-04-15 12:07:34,403][DEBUG][index.translog           ] [nodeD2] [test1-2015.04.15][0] interval [5s], flush_threshold_ops [2147483647], flush_threshold_size [512mb], flush_threshold_period [30m]
[2015-04-15 12:07:34,403][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [CREATED]-&gt;[RECOVERING], reason [from gateway]
[2015-04-15 12:07:34,403][DEBUG][index.gateway            ] [nodeD2] [test1-2015.04.15][0] starting recovery from local ...
[2015-04-15 12:07:34,403][DEBUG][index.engine             ] [nodeD2] [test1-2015.04.15][0] no 3.x segments needed upgrading
[2015-04-15 12:07:34,424][DEBUG][cluster.service          ] [nodeM1] processing [create-index [test1-2015.04.15], cause [auto(index api)]]: done applying updated cluster_state (version: 183)
[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] scheduling refresher every 1s
[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] scheduling optimizer / merger every 1s
[2015-04-15 12:07:34,432][DEBUG][index.shard              ] [nodeD2] [test1-2015.04.15][0] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from gateway, no translog]
[2015-04-15 12:07:34,432][DEBUG][index.gateway            ] [nodeD2] [test1-2015.04.15][0] recovery completed from [local], took [29ms]
[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeD2] sending shard started for [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]
[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeM1] received shard started for [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]
[2015-04-15 12:07:34,432][DEBUG][cluster.service          ] [nodeM1] processing [shard-started ([test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2015-04-15 12:07:34,432][DEBUG][cluster.action.shard     ] [nodeM1] [test1-2015.04.15][0] will apply shard started [test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING], indexUUID [Lyn-pvs-QRS_InyMh9bi6w], reason [after recovery from gateway]
[2015-04-15 12:07:34,433][DEBUG][indices.store            ] [nodeD2] [test1-2015.04.15][0] loaded store meta data (took [0s])
[2015-04-15 12:07:34,440][DEBUG][cluster.service          ] [nodeM1] cluster state updated, version [184], source [shard-started ([test1-2015.04.15][0], node[ci5IBs99RxmpMNOBvRWAHQ], [P], s[INITIALIZING]), reason [after recovery from gateway]]

```
</description><key id="68637087">10609</key><summary>"Failed to parse filter for alias/no field mapping can be found" with field declared in _default_ mapping when using _bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Bulk</label><label>blocker</label><label>bug</label><label>v1.5.2</label></labels><created>2015-04-15T10:13:21Z</created><updated>2015-04-24T07:44:11Z</updated><resolved>2015-04-24T07:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-21T11:33:11Z" id="94755569">Hi @martijnvg 

Please can you look at this.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve the error message when attempting to snapshot a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10608</link><project id="" key="" /><description>Currently the error message is the same when index is closed and when it is missing shards. This commit will generate a specific failure message when a user tries to create a snapshot of a closed index.

Related to #10579
</description><key id="68628860">10608</key><summary>Improve the error message when attempting to snapshot a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T09:41:13Z</created><updated>2015-04-20T21:06:21Z</updated><resolved>2015-04-20T21:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-15T18:04:23Z" id="93506481">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Acknowledged: false for successful delete operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10607</link><project id="" key="" /><description>When deleting a "large" index (13M documents), it gives "acknowledged: false", even though no warnings are logged. The index is correctly deleted from all nodes. When deleting a smaller index, it says "acknowledged: true".
What is the semantics here? If it was due to a timeout, I'd expect a "ProcessClusterEventTimeoutException".
As a user, I'd be less confused if the API would be consistent, so I wouldn't go looking for stray data that was not deleted properly in case of a "acknowledged: false".

Example:

```
~$ curl -XDELETE localhost:2500/small_index?pretty
{
  "acknowledged" : true
}
~$ curl -XDELETE localhost:2500/big_index?pretty
{
  "acknowledged" : false
}
```

I'm running a clean install of ES 1.4.2.
</description><key id="68623605">10607</key><summary>Acknowledged: false for successful delete operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnhaug</reporter><labels><label>discuss</label></labels><created>2015-04-15T09:17:28Z</created><updated>2016-01-17T17:39:37Z</updated><resolved>2016-01-17T17:39:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:39:37Z" id="172357004">Fixed by https://github.com/elastic/elasticsearch/pull/11258
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to recover shards after power outage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10606</link><project id="" key="" /><description>We have ElasticSearch running on a single node. After a power outage, some of the shards could not be recovered. The log was filled with occurences of the log messages below.

Removing the *.recovering files in the elasticsearch data directories and restarting ElasticSearch fixed the problem.

I would be nice if ElasticSearch would ignore the corrupt data, and recover as much as is possible.

```
[2015-01-24 23:13:49,683][WARN ][indices.cluster          ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [adsb-tracks-historic-2015-01-24][0] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:287)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:70)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:257)
    ... 4 more
Caused by: java.io.EOFException
    at org.elasticsearch.common.io.stream.InputStreamStreamInput.readBytes(InputStreamStreamInput.java:53)
    at org.elasticsearch.index.translog.BufferedChecksumStreamInput.readBytes(BufferedChecksumStreamInput.java:55)
    at org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:86)
    at org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:74)
    at org.elasticsearch.index.translog.Translog$Create.readFrom(Translog.java:353)
    at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:68)
    ... 5 more
[2015-01-24 23:13:49,715][WARN ][cluster.action.shard     ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] sending failed shard for [adsb-tracks-historic-2015-01-24][0], node[44Mul_1jQMyvxBZOtrbJSQ], [P], s[INITIALIZING], indexUUID [pxoE803TSFa5YMDEdB3a1g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[adsb-tracks-historic-2015-01-24][0] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: EOFException; ]]
[2015-01-24 23:13:49,715][WARN ][cluster.action.shard     ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] received shard failed for [adsb-tracks-historic-2015-01-24][0], node[44Mul_1jQMyvxBZOtrbJSQ], [P], s[INITIALIZING], indexUUID [pxoE803TSFa5YMDEdB3a1g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[adsb-tracks-historic-2015-01-24][0] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: EOFException; ]]
```

This issue was originally discovered on ElasticSearch 1.4.1 however we have also seen this issue for 1.4.4 and 1.5.0
</description><key id="68610546">10606</key><summary>Failure to recover shards after power outage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">WellingR</reporter><labels /><created>2015-04-15T08:16:16Z</created><updated>2016-03-30T10:38:49Z</updated><resolved>2015-04-16T14:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-15T10:23:10Z" id="93306385">Hmm, I think https://github.com/elastic/elasticsearch/pull/9797 should have
fixed this as of 1.5.0.  Are you sure you see the same problem in 1.5.0?

Mike McCandless

On Wed, Apr 15, 2015 at 4:16 AM, R Welling notifications@github.com wrote:

&gt; We have ElasticSearch running on a single node. After a power outage, some
&gt; of the shards could not be recovered. The log was filled with occurences of
&gt; the log messages below.
&gt; 
&gt; Removing the *.recovering files in the elasticsearch data directories and
&gt; restarting ElasticSearch fixed the problem.
&gt; 
&gt; I would be nice if ElasticSearch would ignore the corrupt data, and
&gt; recover as much as is possible.
&gt; 
&gt; [2015-01-24 23:13:49,683][WARN ][indices.cluster          ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] failed to start shard
&gt; org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [adsb-tracks-historic-2015-01-24][0] failed to recover shard
&gt;     at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:287)
&gt;     at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
&gt;     at java.lang.Thread.run(Unknown Source)
&gt; Caused by: org.elasticsearch.index.translog.TranslogCorruptedException: translog corruption while reading from stream
&gt;     at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:70)
&gt;     at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:257)
&gt;     ... 4 more
&gt; Caused by: java.io.EOFException
&gt;     at org.elasticsearch.common.io.stream.InputStreamStreamInput.readBytes(InputStreamStreamInput.java:53)
&gt;     at org.elasticsearch.index.translog.BufferedChecksumStreamInput.readBytes(BufferedChecksumStreamInput.java:55)
&gt;     at org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:86)
&gt;     at org.elasticsearch.common.io.stream.StreamInput.readBytesReference(StreamInput.java:74)
&gt;     at org.elasticsearch.index.translog.Translog$Create.readFrom(Translog.java:353)
&gt;     at org.elasticsearch.index.translog.ChecksummedTranslogStream.read(ChecksummedTranslogStream.java:68)
&gt;     ... 5 more
&gt; [2015-01-24 23:13:49,715][WARN ][cluster.action.shard     ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] sending failed shard for [adsb-tracks-historic-2015-01-24][0], node[44Mul_1jQMyvxBZOtrbJSQ], [P], s[INITIALIZING], indexUUID [pxoE803TSFa5YMDEdB3a1g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[adsb-tracks-historic-2015-01-24][0] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: EOFException; ]]
&gt; [2015-01-24 23:13:49,715][WARN ][cluster.action.shard     ] [synergia-nat3] [adsb-tracks-historic-2015-01-24][0] received shard failed for [adsb-tracks-historic-2015-01-24][0], node[44Mul_1jQMyvxBZOtrbJSQ], [P], s[INITIALIZING], indexUUID [pxoE803TSFa5YMDEdB3a1g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[adsb-tracks-historic-2015-01-24][0] failed to recover shard]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: EOFException; ]]
&gt; 
&gt; This issue was originally discovered on ElasticSearch 1.4.1 however we
&gt; have also seen this issue for 1.4.4 and 1.5.0
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10606.
</comment><comment author="WellingR" created="2015-04-16T07:57:08Z" id="93670581">I will try to reproduce the issue again in 1.5.0
</comment><comment author="WellingR" created="2015-04-16T14:33:44Z" id="93749566">I did quite a lot of tries today to reproduce the issue on 1.5.0, however I could not reproduce the problem. So it no longer seems to occur for ElasticSearch 1.5.0
</comment><comment author="bleskes" created="2015-04-16T14:34:57Z" id="93749862">cool. Thx @WellingR  for trying. PS - use 1.5.1 if you can. It has an important fix to the relocation code.
</comment><comment author="mseneby" created="2016-03-28T21:02:01Z" id="202579474">I have exact problem: power outage happened on all nodes and elastic search never recovered back;
these are the typical exceptions:

[2016-03-28 13:59:04,464][WARN ][cluster.action.shard     ] [Ahab] [tower][3] received shard failed for [tower][3], node[VhLi7qJNRhe6qr_pEP6Vpw], [P], v[596], s[INITIALIZING], a[id=RTJb9YYrRQeY0TLi9wBuSQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-28T20:58:57.865Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]]; ]], indexUUID [wH16M_qLRyy3f-a1GpFbGQ], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]]; ]
[tower][[tower][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [tower][[tower][3]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]];
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:178)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        ... 5 more
Caused by: [tower][[tower][3]] EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]];
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
        ... 11 more
Caused by: ElasticsearchException[unexpected exception reading from translog snapshot of /livetest/elasticsearch-2.2.0/data/opersearch-m4/nodes/0/indices/tower/3/translog/translog-82.tlog]; nested: EOFException[read past EOF. pos [7319569] length: [4] end: [7319569]];
        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:102)
        at org.elasticsearch.index.translog.TranslogReader.access$000(TranslogReader.java:46)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:297)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
        at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
        ... 12 more
Caused by: java.io.EOFException: read past EOF. pos [7319569] length: [4] end: [7319569]
        at org.elasticsearch.common.io.Channels.readFromFileChannelWithEofException(Channels.java:102)
        at org.elasticsearch.index.translog.ImmutableTranslogReader.readBytes(ImmutableTranslogReader.java:84)
        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:91)
        ... 17 more

It was heavily writing to the nodes during outage.
</comment><comment author="bleskes" created="2016-03-30T10:38:49Z" id="203373260">@mseneby these are indication of translog truncations. What type FS/OS are your running on? I also suspect it's a 2.2.0 ES , right? do you still have the files?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Forbid tests from writing to CWD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10605</link><project id="" key="" /><description>Allowing tests writing to the working directory can mask problems.
For example, multiple tests running in the same jvm, and using the
same relative path, may cause issues if the first test to run
leaves data in the directory, and the second test does not remember
to cleanup the path before using it.

This change adds security manager rules to disallow tests writing
to the working directory. Instead, tests create a temp dir with
the existing test framework.
</description><key id="68596207">10605</key><summary>Tests: Forbid tests from writing to CWD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T07:05:48Z</created><updated>2015-04-15T19:46:07Z</updated><resolved>2015-04-15T19:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-15T07:29:59Z" id="93238186">+1
</comment><comment author="s1monw" created="2015-04-15T10:31:59Z" id="93312457">@rjernst I didn't port the other changes to 1.x yet and I think we should only do that on master to be honest
</comment><comment author="s1monw" created="2015-04-15T10:37:26Z" id="93315914">@rjernst LGTM how did u fix the failures we saw yesterday?
</comment><comment author="rmuir" created="2015-04-15T10:44:12Z" id="93317263">+1, please push this one as-is. mockfilesystems need to be their own issues. Otherwise the tests stay as they are right now, and rot, if we try to do too much at once
</comment><comment author="s1monw" created="2015-04-15T12:25:46Z" id="93367294">I agree with @rmuir +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>master left problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10604</link><project id="" key="" /><description>```
 Hi ,We have four nodes(1 master and data  node  &amp;&amp;  3 only data nodes) ,however,when the index increased to  134044389 ,our data nodes often left the cluster. After I looked the ES logs,I discovered that the GC time was too long .
    the  dropped data node log just like the following:
```

![tm](https://cloud.githubusercontent.com/assets/10142769/7152859/a814890c-e375-11e4-9434-7726da66bb83.png)
       and the master node log like this:

![tm 111](https://cloud.githubusercontent.com/assets/10142769/7152893/5874382e-e376-11e4-875d-0baccba218f1.png)

```
  and our server info  : 
   RAM  8g  
   ES_HEAP_SIZE 4G

  Who can help me,Thanks for all !  (My English is very poor...)
```
</description><key id="68584542">10604</key><summary>master left problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huangpeng1990</reporter><labels /><created>2015-04-15T05:58:33Z</created><updated>2015-04-15T06:03:50Z</updated><resolved>2015-04-15T06:03:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-15T06:03:49Z" id="93208637">No problem with your english.
But Please ask question on the mailing list. We can help you there.
You should provide there also your full logs (use gist.github.com to share them).

Does not look like an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoNodeAvailableException: None of the configured nodes are available: []</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10603</link><project id="" key="" /><description>I build a es cluster with 2 nodes on EC2. and they are accessible by 9200 and 9300 port. I even do the telnet test on 9300 port, it is ok.

and I write the following code:

&gt; elastic.host=172.31.10.168:9300,172.31.10.169:9300
&gt; 
&gt; Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;();
&gt;         params.put("cluster.name", "test");
&gt;         Settings settings = ImmutableSettings.settingsBuilder().put(params).build();
&gt;         client = new TransportClient(settings);
&gt;         String[] hosts = Config.getString("elastic.host").split(",");
&gt;         for (String host : hosts) {
&gt;             String[] hp = host.split(":");
&gt;             if (hp.length != 2) {
&gt;                 continue;
&gt;             }
&gt;             client.addTransportAddress(new InetSocketTransportAddress(hp[0], Integer.valueOf(hp[1])));
&gt;         }

but when I do a query, error happen as follows:

&gt; org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []
&gt;     at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:279)
&gt;     at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:198)
&gt;     at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)
&gt;     at org.elasticsearch.client.support.AbstractClient.count(AbstractClient.java:379)
&gt;     at org.elasticsearch.client.transport.TransportClient.count(TransportClient.java:396)
&gt;     at org.elasticsearch.action.count.CountRequestBuilder.doExecute(CountRequestBuilder.java:146)
&gt;     at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)
&gt;     at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)

and my es config is as follows:

&gt; cluster.name: test
&gt; node.name: ip-172-31-10-168.cn-north-1.compute.internal
&gt; node.max_local_storage_nodes: 1
&gt; node.master: true
&gt; node.data: false
&gt; index.mapper.dynamic: true
&gt; action.auto_create_index: true
&gt; action.disable_delete_all_indices: true
&gt; path.conf: /usr/local/etc/elasticsearch
&gt; path.data: /usr/local/var/data/elasticsearch
&gt; path.logs: /usr/local/var/log/elasticsearch
&gt; bootstrap.mlockall: true
&gt; http.port: 9200
&gt; gateway.expected_nodes: 1
&gt; discovery.zen.minimum_master_nodes: 1
&gt; discovery.zen.ping.multicast.enabled: false
&gt; discovery.zen.ping.unicast.hosts: ["172.31.10.168", "172.31.15.169"]
&gt; cloud.node.auto_attributes: true

anyone could please give me a suggestion?
</description><key id="68553170">10603</key><summary>NoNodeAvailableException: None of the configured nodes are available: []</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffreyji666</reporter><labels /><created>2015-04-15T02:26:14Z</created><updated>2015-04-15T05:49:11Z</updated><resolved>2015-04-15T05:49:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-15T05:49:09Z" id="93204494">A better place for questions like this is on the mailing list as it's more a configuration issue than a bug.
I think it's a security issue. If you are running your code outside AWS, you need to open 9300 port to the outside as well.

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make GeoContext mapping idempotent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10602</link><project id="" key="" /><description>closes #10581
closes #8937
</description><key id="68543304">10602</key><summary>Make GeoContext mapping idempotent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Mapping</label><label>:Suggesters</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-15T01:28:30Z</created><updated>2015-04-26T11:21:48Z</updated><resolved>2015-04-21T23:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-15T07:19:13Z" id="93233380">@areek The fix LGTM, just a couple questions about the test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Report max queue size reached for thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10601</link><project id="" key="" /><description>When tuning bulk thread pool queue sizes, it can be helpful if there is a way to report on the max queue size "reached" so far to help developers/admins plan ahead for further queue size increases where appropriate (without waiting for rejections to occur).
</description><key id="68540755">10601</key><summary>Report max queue size reached for thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2015-04-15T01:16:16Z</created><updated>2016-01-17T17:31:14Z</updated><resolved>2016-01-17T17:31:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:31:13Z" id="172355285">A max queue size reached probably isn't that much use, given that it is the max size since the node was first started.  A high value could come from a one-off abnormality.  Much more useful to monitor the current queue size over time. I don't think we should implement this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include pool and queue sizes in default cat thread_pool output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10600</link><project id="" key="" /><description>Currently, the cat thread_pool api returns the following thread pool metrics by default for the bulk, index and search pools:

```
.active
.queue
.rejected
```

It will be helpful to also return the .size and .queueSize values.
</description><key id="68538270">10600</key><summary>Include pool and queue sizes in default cat thread_pool output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label></labels><created>2015-04-15T00:58:55Z</created><updated>2015-04-26T15:43:25Z</updated><resolved>2015-04-26T15:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T10:46:59Z" id="95535777">@ppf2 I'm not sure it makes sense to return those values by default, because they are config options which don't change.  The point of the cat API is to return the numbers which DO change and need monitoring.

Instead, I would propose a different feature: to support wildcards in header names, so you can do:

```
GET _cat/thread_pool?v&amp;h=bulk.*
```

What do you think=
</comment><comment author="ppf2" created="2015-04-24T01:49:23Z" id="95772619">I like the idea of supporting wildcard :+1: 
</comment><comment author="clintongormley" created="2015-04-26T15:43:24Z" id="96401588">Closing in favour of https://github.com/elastic/elasticsearch/issues/10811
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display full col name when header option is used with cat apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10599</link><project id="" key="" /><description>Currently, if you specify header options for the cat api using the `h=` and `v` options, eg. `http://localhost:9200/_cat/thread_pool?h=host,ip,bs,ba,bq,br&amp;v`, it returns the short form for the header/column names in the verbose header output which is not that user friendly.  It will be nice to provide an option for it to display the full column name when header options are used.

```
host   ip           bs ba bq br 
Iceman 192.168.56.1  4  0  0  0 
```
</description><key id="68537711">10599</key><summary>Display full col name when header option is used with cat apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-04-15T00:55:12Z</created><updated>2015-04-24T01:52:03Z</updated><resolved>2015-04-23T10:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-23T10:42:18Z" id="95531914">Hi @ppf2 

This is already supported - just use the long names in the `h=` spec, eg:

```
GET _cat/thread_pool?h=bulk.active,bulk.size&amp;v
```

returns:

```
bulk.active bulk.size 
          0         4 
```

And the short/long names can always be accessed via:

```
GET _cat/thread_pool?help
```
</comment><comment author="ppf2" created="2015-04-24T01:52:03Z" id="95772844">oh nice !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simply scripts to only support one file type in script engines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10598</link><project id="" key="" /><description>In the onFileInit method as part of the class ScriptService it appears that an assumption is made that there will always only be one type of file for each of the ScriptServiceEngines that is available.  This can be simplified by only allowing one type of file per ScriptServiceEngine since that assumption is already made anyway.
</description><key id="68533807">10598</key><summary>simply scripts to only support one file type in script engines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>adoptme</label><label>breaking</label></labels><created>2015-04-15T00:24:22Z</created><updated>2016-05-13T15:50:54Z</updated><resolved>2016-05-13T15:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-15T12:27:43Z" id="93368171">+1 on the simplification
</comment><comment author="javanna" created="2015-04-15T12:47:07Z" id="93377340">Hi @jdconrad you mean [here](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/script/ScriptService.java#L486)? If that's the case that is not the assumption we are making there, we have to pick one type, which one doesn't matter, the call will return the same result for the same engine.

I am all for simplifying this in master, but I think we properly support multiple types, this is not a bug. The only script engine that I can remember which supports multiple types is javascript (`js` and `javascript`), maybe we want to change that at some point and change the `ScriptEngineService` interface but that's a breaking change for every script engine plugin.
</comment><comment author="rjernst" created="2015-04-15T19:22:16Z" id="93538017">Sure this is breaking, but I also don't see how this "feature" is actually helping anything.  +1 to break in master. And I would just remove the `javascript` type.

Do we also really need separate extension/type? They seem redundant? It seems like we could have one, and that is both the `lang` parameter value, as well as the extension for on disk scripts.
</comment><comment author="javanna" created="2015-04-16T07:38:40Z" id="93667132">I agree that we don't need multiple types. Breaking all scripting plugins is a bit scary but might be worth it here, on master only. Maybe @dadoonet and @uboness want to comment as well.

That said, @jdconrad if I got this right and what you found is not a bug, can you please update the title and description of the issue or open a new one? It feels weird that the whole (good) discussion started from a bug that is not :)
</comment><comment author="jdconrad" created="2015-04-17T20:49:22Z" id="94072309">@javanna That's the location.  After further reading of the code, I realize my initial understanding was incorrect.  I still believe that we can simplify this supporting only one file type and users should be able to follow that convention via the documentation.  Good point about the javascript type, but since there are both extensions and language types, we could at least only use a single language type here as @rjernst mentions there is some redundancy.
</comment><comment author="clintongormley" created="2015-04-25T14:33:25Z" id="96215699">+1 to having only a single file type per script language
</comment><comment author="dadoonet" created="2015-04-25T16:17:20Z" id="96233148">About extension/type separation, we just need to think of some languages like ruby. I think that a common extension is rb not ruby.
So it would mean that language type would have to be set as rb. Not a big deal though.
</comment><comment author="clintongormley" created="2016-01-17T17:17:49Z" id="172354353">Let's do this
</comment><comment author="dakrone" created="2016-05-10T22:33:09Z" id="218311838">I'm going to do this as part of https://github.com/elastic/elasticsearch/pull/18226
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>My elastic search setup will not complete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10597</link><project id="" key="" /><description>I am on a windows 8 machine.  I have the latest jdk package loaded.  I have elastic search 1.5.  My setup will not complete. What do I need to do?  Here are the results after running the elasticsearch.bat file:

C:\Users\jensedr\elasticsearch-1.5.1\bin&gt;elasticsearch
[2015-04-14 14:29:55,547][INFO ][node                     ] [Robert Kelly] versi
on[1.5.1], pid[4544], build[5e38401/2015-04-09T13:41:35Z]
[2015-04-14 14:29:55,547][INFO ][node                     ] [Robert Kelly] initi
alizing ...
[2015-04-14 14:29:55,563][INFO ][plugins                  ] [Robert Kelly] loade
d [], sites []
[2015-04-14 14:29:59,922][INFO ][node                     ] [Robert Kelly] initi
alized
[2015-04-14 14:30:00,078][INFO ][node                     ] [Robert Kelly] start
ing ...
[2015-04-14 14:30:00,594][INFO ][transport                ] [Robert Kelly] bound
_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.105.221.122:93
00]}
[2015-04-14 14:30:01,078][INFO ][discovery                ] [Robert Kelly] elast
icsearch/DFHd5w_wTtiPSOkXkrhFpw
[2015-04-14 14:30:04,907][INFO ][cluster.service          ] [Robert Kelly] new_m
aster [Robert Kelly][DFHd5w_wTtiPSOkXkrhFpw][CaliberSecDan][inet[/10.105.221.122
:9300]], reason: zen-disco-join (elected_as_master)
[2015-04-14 14:30:05,047][INFO ][gateway                  ] [Robert Kelly] recov
ered [0] indices into cluster_state
[2015-04-14 14:30:05,125][INFO ][http                     ] [Robert Kelly] bound
_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.105.221.122:92
00]}
[2015-04-14 14:30:05,125][INFO ][node                     ] [Robert Kelly] start
ed
</description><key id="68512748">10597</key><summary>My elastic search setup will not complete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danjensen6075</reporter><labels /><created>2015-04-14T22:13:06Z</created><updated>2015-04-15T05:20:44Z</updated><resolved>2015-04-15T05:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-15T05:20:43Z" id="93193710">Please ask questions like this on the mailing list. We can help you there.
Nothing looks strange in logs so you'll need to provide more details.

Not an issue closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Crash Error: "NoClassDefFoundError"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10596</link><project id="" key="" /><description>Hi all, 
I'm working on getting started with elastic search (latest version).  All of my simple tests work great and the guide (which I have read several times and definitely completed every necessary step) is really helpful for general things.  However I plan on using elastic search as a dependency in a storm project so I need to deploy it to a single jar.  I've researched my error a lot and it seems like an issue with maven's shading that you guys use and I'm wondering if someone has some experience and can point me in the right direction.  I'm thinking that there is something wrong with my POM file that doesn't allow me to use certain functions I'm just not sure what to do at all.

The storm node successfully creates a node and client that connect to elastic search and this error comes when I attempt a client.prepareIndex().actionGet() call (specifics omitted).  The index and type that it tries to put into are already created on the elastic side (using curl).

My stack trace is something like this:
{ there are a bunch of storm errors here emanating from my elasticsearch storage bolt}
 Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.index.codec.postingsformat.PostingFormats
     at org.elasticsearch.index.codec.CodecModule.configurePostingsFormats(CodecModule.java:126) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.index.codec.CodecModule.configure(CodecModule.java:178) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:204) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:85) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:130) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:328) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:372) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:365) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158) ~[mdec-storm-1.0-SNAPSHOT-jar-with-dependencies.jar:na]
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_75]
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_75]

and my pom file looks like this:
&lt;project

```
...
 &lt;parent&gt;
     ...
 &lt;/parent&gt;
 ...
 &lt;properties&gt;
     &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
     &lt;skipTests&gt;true&lt;/skipTests&gt;
 &lt;/properties&gt;

 &lt;repositories&gt;
     ...
 &lt;/repositories&gt;

 &lt;dependencies&gt;
```

 ...
         &lt;!-- elastic search dependencies start --&gt;
         &lt;dependency&gt;
             &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
             &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
         &lt;/dependency&gt;
         &lt;!-- elastic search dependencies end --&gt;  
         ...
     &lt;/dependencies&gt;

```
 &lt;build&gt;
     &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;
     &lt;testSourceDirectory&gt;src/test&lt;/testSourceDirectory&gt;
     &lt;resources&gt;
         &lt;resource&gt;
             &lt;directory&gt;src/main/resources&lt;/directory&gt;
         &lt;/resource&gt;
     &lt;/resources&gt;
     &lt;plugins&gt;
         &lt;!--
        bind the maven-assembly-plugin to the package phase
         this will create a jar file without the storm dependencies
         suitable for deployment to a cluster.
          --&gt;
         &lt;plugin&gt;
             &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
             &lt;configuration&gt;
                 &lt;descriptorRefs&gt;
                     &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                 &lt;/descriptorRefs&gt;
                 &lt;archive&gt;
                     &lt;manifest&gt;
                         &lt;mainClass&gt;&lt;/mainClass&gt;
                     &lt;/manifest&gt;
                 &lt;/archive&gt;
             &lt;/configuration&gt;
             &lt;executions&gt;
                 &lt;execution&gt;
                     &lt;id&gt;make-assembly&lt;/id&gt;
                     &lt;phase&gt;package&lt;/phase&gt;
                     &lt;goals&gt;
                         &lt;goal&gt;single&lt;/goal&gt;
                     &lt;/goals&gt;
                 &lt;/execution&gt;
             &lt;/executions&gt;
         &lt;/plugin&gt;
         &lt;!-- added to try and fix elasticsearch  --&gt;
         &lt;plugin&gt;
             &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
             &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
             &lt;version&gt;2.3&lt;/version&gt;
             &lt;executions&gt;
                 &lt;execution&gt;
                     &lt;phase&gt;package&lt;/phase&gt;
                     &lt;goals&gt;
                         &lt;goal&gt;shade&lt;/goal&gt;
                     &lt;/goals&gt;
                 &lt;/execution&gt;
             &lt;/executions&gt;
         &lt;/plugin&gt;
         ...
     &lt;/plugins&gt;
 &lt;/build&gt;
```

 &lt;/project&gt;

Thanks so much in advance
</description><key id="68464391">10596</key><summary>Crash Error: "NoClassDefFoundError"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daveharmon</reporter><labels /><created>2015-04-14T18:37:30Z</created><updated>2015-04-15T16:39:57Z</updated><resolved>2015-04-15T16:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-15T05:58:21Z" id="93206607">Does your final jar file contains the missing class? How does it look like?
Does it contain as well lucene classes we don't shade in Elasticsearch jar?
Try to add as a dependency as well all jars marked as optional in our pom.xml. 
</comment><comment author="daveharmon" created="2015-04-15T16:35:41Z" id="93479240">Thanks for your reply dadoonet,
I figured it out,  I had been using the maven assembly plugin to achieve a single jar as shown above.  Instead I decided to go with the maven shade plugin (v2.3) via this issue: https://github.com/elastic/elasticsearch/issues/3350 and add a series of AppendingTransformer transformers to the configuration section that combine those three specific META-INF/services files. 

Apparently these files get ignored in succeeding calls from the first, and elasticsearch requires all three, so I needed to use shading to append the similar files together

All good now, thanks for your help!
</comment><comment author="dadoonet" created="2015-04-15T16:39:56Z" id="93480779">Thanks for the heads up. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Standardization of packages structure and install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10595</link><project id="" key="" /><description>The existing DEB/RPM packages have a lot of differences: they don't execute the same actions when installing or removing the package. They also don't declare exactly the same environment variables at the same place. At the end of the day the global behavior and configuration have _few variations_ and it's difficult to maintain the scripts.

This commits unifies the package behavior:
- DEB/RPM use the same package scripts (pre installation, post installation etc) in order to execute exactly the same actions
- Use of a unique environment vars file that declares everything needed by scripts (~~the goal is to delete vars declaration in init.d and systemd scripts, this will be done in another PR~~ see comment below)
- Variables like directory paths are centralized and replaced according to the target platform (using #10330)
- Move /etc/rc.d/init.d to standard /etc/init.d (RPM only)
- Add PID_DIR env var
- Always set ES_USER, ES_GROUP,MAX_MAP_COUNT and MAX_OPEN_FILES in env vars file
- Create log, data, work and plugins directories with DEB/RPM packaging system
- Change to elastic.co domain in copyright and control files
- Add Bats files to automate testing of DEB and RPM packages
- Update TESTING.asciidoc

More info on Bats here:  https://github.com/sstephenson/bats
</description><key id="68436827">10595</key><summary>Standardization of packages structure and install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T16:39:38Z</created><updated>2015-05-29T18:18:17Z</updated><resolved>2015-04-20T13:07:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-14T16:40:14Z" id="92967816">@spinscale @electrical Can you have a look please? Thanks :)
</comment><comment author="tlrx" created="2015-04-14T16:45:13Z" id="92971847">Note: the Bats test files have been executed with success on Debian 7.8, Debian 8, Ubuntu 12.04, Ubuntu 14.04, OpenSUSE 13, CentOS 6.6 and Fedora 21.
</comment><comment author="electrical" created="2015-04-14T16:49:52Z" id="92975028">&gt; the goal is to delete vars declaration in init.d and systemd scripts, this will be done in another PR

This is anti pattern. the init scripts should be able to work even if the defaults/sysconfig files don't exist or are empty.
The defaults/sysconfig file should only function to override defaults, not provide the defaults.
</comment><comment author="tlrx" created="2015-04-15T08:04:19Z" id="93252102">@electrical thanks for your precision! I'm OK with self sufficient scripts since #10330 helps to centralize default values in one place.I'd like to see variable definitions placed in specific location(s) and not spread all over the different scripts like it is today :)

To follow your comment maybe I can update this pull request to provide an empty environment file with all known variables names (no default values) each with a comment. Something like this:

```
################################
# Elasticsearch
################################

# Elasticsearch log directory
#LOG_DIR=/var/log/elasticsearch

# Elasticsearch data directory
#DATA_DIR=/var/lib/elasticsearch
...
```

Then in init.d and bin/elasticsearch and bin/plugin scripts we just check if a variable is already known and if not sets a default value if needed.
</comment><comment author="electrical" created="2015-04-15T09:19:09Z" id="93279375">As discussed internally with @tlrx. 
The env / defaults files should have all possible settings listed but commented.
These files are only used when overriding any default values which are specified in the init / systemd scripts.
It makes sense to ensure we use the same variable setting names across all packages.

Beyond that PR looks good.
</comment><comment author="tlrx" created="2015-04-15T13:35:45Z" id="93402393">@electrical I just updated with your comment. I commented all vars in default env vars file and added vars definitions in appropriate scripts. Tests are OK.
</comment><comment author="electrical" created="2015-04-15T14:35:05Z" id="93424937">@tlrx looks good as far as i can see.
Would you be able to generate a rpm and deb package from the PR so i can validate those as well?

Thanks!
</comment><comment author="electrical" created="2015-04-17T08:48:15Z" id="93945936">All good from my side!
</comment><comment author="tlrx" created="2015-04-17T08:49:53Z" id="93946177">@electrical Thanks for your review.

@spinscale Can you have a look please? Thanks :)
</comment><comment author="spinscale" created="2015-04-20T11:29:57Z" id="94428568">tested on debian, installing plugins work, tested on centos, installing plugins work as well

Please make sure we put the bats tests in CI

one problem occured: when installing a plugin that has own configuration files, those are put into `/usr/share/elasticsearch/config` instead of `/etc/elasticsearch`
</comment><comment author="electrical" created="2015-04-20T12:34:04Z" id="94440805">@spinscale the issue of plugin installs with configs is caused by the plugin tool not setting the right jvm options.
That's something that should be solved in a separate PR after this is all done.
</comment><comment author="tlrx" created="2015-04-20T12:35:57Z" id="94441084">@spinscale @electrical right, it is part of #7946

@spinscale Do you want this issue solved here? And do you want me to add Bats tests that install/remove plugins?
</comment><comment author="spinscale" created="2015-04-20T12:44:22Z" id="94442444">@tlrx no need to solve it here, but lets try to fix it soon

LGTM otherwise, CI is next step. Thank you!
</comment><comment author="tlrx" created="2015-04-20T13:07:56Z" id="94447152">Merged in 867955188e6e5f86331cbd7ea7273f637ced1f17 and 0dad33f17f3a3dc62e8226cdeced5e17b6976459
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce non-null ValueFormat in Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10594</link><project id="" key="" /><description>This was suggested by @jpountz in https://github.com/elastic/elasticsearch/pull/10571#issuecomment-92499177.

Off the back of #10571 it would be easier if the ValueFormat in aggregations was always non-null so that in the cases where null would currently be returned from the `ValuesSourceParser` it instead returns `ValueFormat.RAW`. This would mean that instead of checking for null AND `ValueFormat.RAW` we could instead just check for `ValueFormat.RAW` in the places where we only output on a specialised format and we can just blindly call the `ValueFormat` for cases where we don't care if the format is specialised or raw.
</description><key id="68408527">10594</key><summary>Enforce non-null ValueFormat in Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T15:19:15Z</created><updated>2015-07-01T07:58:07Z</updated><resolved>2015-07-01T07:58:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Same code path for dynamic mappings updates and updates coming from the API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10593</link><project id="" key="" /><description>We have two completely different code paths for mappings updates, depending on
whether they come from the API or are guessed based on the parsed documents.
This commit makes dynamic mappings updates execute like updates from the API.

The only change in behaviour is that a document that fails parsing can not
modify mappings anymore (useful to prevent issues such as #9851). Other than
that, this change should be fairly transparent to users but working this way
opens doors to other changes such as validating dynamic mappings updates on the
master node (#8688).

The way it works internally is that Mapper.parse now returns a Mapper instead
of being void. The returned Mapper represents a mapping update that has been
performed in order to parse the document. Mappings updates are propagated
recursively back to the root mapper, and once parsing is finished, we check
that the mappings update can be applied, and either fail the parsing if the
update cannot be merged (eg. because of a concurrent mapping update from the
API) or merge the update into the mappings.

However not all mappings updates can be applied recursively, `copy_to` for
instance can add mappings at totally different places in the tree. Because of
it I added ParseContext.rootMapperUpdates which `copy_to` fills when the
field to copy data to does not exist in the mappings yet. These mappings
updates are merged from the ones generated by regular parsing.

One particular mapping update was the `auto_boost` setting on the `all` root
mapper. Being tricky to work on, I removed it in favour of search-time checks
that payloads have been indexed.

One interesting side-effect of the change is that concurrency on ObjectMapper
is greatly simplified since we do not have to care anymore about having
concurrent dynamic mappings and API updates.

Closes #9364
</description><key id="68358336">10593</key><summary>Same code path for dynamic mappings updates and updates coming from the API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T12:34:57Z</created><updated>2015-06-07T10:54:28Z</updated><resolved>2015-04-16T08:17:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-15T23:15:33Z" id="93594779">@jpountz This is _awesome_! LGTM 
</comment><comment author="jpountz" created="2015-04-16T08:30:12Z" id="93677470">Thanks @rjernst !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused normsField from MatchAllQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10592</link><project id="" key="" /><description>The normsField in the MatchAllQuery seems unused in Lucene for quiet some time. In connection to the current refactoring we should remove it or decide what else to do with it. Not sure if setting the normsField in the FullRestartStressTest still serves its purpose of avoiding the cache.
</description><key id="68336554">10592</key><summary>Remove unused normsField from MatchAllQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T10:42:59Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T11:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-14T10:43:45Z" id="92764129">+1
</comment><comment author="rmuir" created="2015-04-14T10:48:23Z" id="92765627">&gt; Not sure if setting the normsField in the FullRestartStressTest still serves its purpose of avoiding the cache.

if _any_ test is using this, i'd remove the whole test class completely: its a sign its just flat out unmaintained. normsField has been removed from this query since lucene 4.0 !!!!! years!!!!!!
</comment><comment author="s1monw" created="2015-04-14T10:49:20Z" id="92765751">can you @Deprecate is on  1.x too? LGTM
</comment><comment author="cbuescher" created="2015-04-14T12:52:13Z" id="92807114">Deprecated setter in the query on 1.x with 26afd16a6a37c20a1c6bc377f73bc60076a4994d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>expand_wildcards:closed does not work to indices with alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10591</link><project id="" key="" /><description>If the index logstash-2015.04.14 has an alias named logstash-2015.04.14-test, 
I could get result from logstash-*/_settings?expand_wildcards=closed.

I am not sure if it is bug ?
</description><key id="68334697">10591</key><summary>expand_wildcards:closed does not work to indices with alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">childe</reporter><labels><label>:Aliases</label><label>adoptme</label><label>bug</label></labels><created>2015-04-14T10:33:11Z</created><updated>2016-01-17T17:16:39Z</updated><resolved>2016-01-17T17:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-14T14:04:55Z" id="92863884">So the wildcard is matching the alias, which points to the closed index.  Interesting...
</comment><comment author="javanna" created="2015-04-15T14:21:55Z" id="93421022">We used not to check the state of concrete indices after aliases resolution, which might explain this behaviour. That should be fixed with #9057 though. @childe which version of elasticsearch are you running on?
</comment><comment author="childe" created="2015-04-16T08:53:51Z" id="93681903">1.4.4
I thought logstash-*/_settings?expand_wildcards=closed would NOT get result of open indices. am I right?
logstash-2015.04.14  is open , but i got it from the api.
</comment><comment author="javanna" created="2015-04-16T09:32:14Z" id="93691437">I can confirm this is a bug, could repro with 1.5.1 too.

```
curl -XPUT localhost:9200/test
curl -XPUT localhost:9200/test/_alias/testaliasopen
curl -XPUT localhost:9200/testclosed
curl -XPUT localhost:9200/testclosed/_alias/testaliasclosed

#following call returns closed index too
curl 'localhost:9200/test*/_settings?pretty&amp;expand_wildcards=open'

#following call returns open index too
curl 'localhost:9200/test*/_settings?pretty&amp;expand_wildcards=closed'
```
</comment><comment author="clintongormley" created="2016-01-17T17:16:39Z" id="172354299">This has been fixed in 2.2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing field filter not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10590</link><project id="" key="" /><description>I have an Elasticsearch 1.5.0 environment with 3 nodes and 3 shards of each index.

I have the following document indexed:

``` json
{
  "_index": "content-1",
  "_type": "share-site-article",
  "_id": "5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b",
  "_source": {
    "preamble": "X and Y presents",
    "nativeAdCampaignName": "Company",
    "_site": "docs"
  }
}
```

However when I make queries and combine it with using the missing field filter the actual missing field filter is not being used.

``` json
POST /content/_search
{
    "query": {
     "match": {
        "_all": "X and Y presents"
    }
   },
   "filter": {
      "and": {
         "filters": [
            {
               "missing": {
                  "field": "nativeAdCampaignName"
               }
            },             
            {
               "term": {
                  "_id": "5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b"
               }
            },
            {
               "term": {
                  "_site": "docs"
               }
            }
         ]
      }
   }
}
```

Response output for that query:

``` json
 {
   "took": 38,
   "timed_out": false,
   "_shards": {
      "total": 6,
      "successful": 6,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1.8740479,
      "hits": [
         {
            "_index": "content-1",
            "_type": "share-site-article",
            "_id": "5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b",
            "_score": 1.8740479,
            "_source": {
               "preamble": "X and Y presents",
               "nativeAdCampaignName": "Company",
               "_site": "docs"
            }
         }
      ]
   }
}
```

The query above should not return the document, since I am querying for document that has missing field values in the specific field 'nativeAdCampaignName'.

The field I'm unable to use with missing field is indeed indexed, i.e. I can search on that field and have the document returned.

(A problematic implication is that I cannot reproduce this problem in a local single-node environment running same ES version and mapping. There's different JVM versions and difference in number of nodes though.)

This environment runs on OpenJDK 64-Bit Server VM, 1.7.0_75.

Any ideas why this happens?
</description><key id="68315377">10590</key><summary>Missing field filter not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dling</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label></labels><created>2015-04-14T09:26:26Z</created><updated>2015-06-26T10:51:54Z</updated><resolved>2015-04-28T08:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-14T13:59:03Z" id="92861480">Hi @dling 

First, you shouldn't be using `filter` here.  This filter is only applied AFTER your query has been run and aggregations have been calculated.  So it removes results from the `hits`, but the results will still be taken into account by aggregations.  Instead, you should use a `filtered` query.

Next, I note that you're querying the `content` index, but the document is actually in `content-1`. I assume `content` is actually an alias? (This should work regardless)

Then, like you, I'm unable to replicate this locally on 1.5.0.  Please could you upload the output of the following:

Mapping for all fields:

```
GET content/_mapping
```

Mapping for `_field_names`:

```
GET content/_mapping/field/_field_names?include_defaults
```

Explanation of query: 

```
POST /content-1/_validate/query?explain
```

Values in `_field_names` for this document: 

```
POST /content/_search
{
  "query": {
    "term": {
      "_id": "5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b"
    }
  },
  "fielddata_fields": ["_field_names"]
}
```

An explanation of how this document matched:

```
POST /content-1/share-site-article/5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b/_explain
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "_all": "X and Y presents"
        }
      },
      "filter": {
        "and": {
          "filters": [
            {
              "missing": {
                "field": "nativeAdCampaignName"
              }
            },
            {
              "term": {
                "_id": "5c7aca72-f1fd-48dd-b2ee-1ed6d3ef134b"
              }
            },
            {
              "term": {
                "_site": "docs"
              }
            }
          ]
        }
      }
    }
  }
}
```
</comment><comment author="rjernst" created="2015-04-14T19:41:21Z" id="93035675">@dling Was your index created before 1.3? It may be the same issues as #10414 which was fixed in #10268 for 1.5.1.
</comment><comment author="dling" created="2015-04-15T14:22:27Z" id="93421146">Thanks a lot for the responses. Yes the index was indeed created on an earlier version, I believe before 1.3. 

I will upgrade to 1.5.1 in the next few days and see how that works out, and if it doesn't solve it I'll upload the requested output.
</comment><comment author="HenleyChiu" created="2015-04-21T14:13:20Z" id="94810244">FYI,

I don't think this was ever fixed in 1.5.1 as I've ran into the same problem with the latest version.

I assume this is going to be in a future release?
</comment><comment author="HenleyChiu" created="2015-04-21T14:19:10Z" id="94811328">@dling I'm not sure if you're lucky, but as a workaround, you can replace the missing filter with a bool filter. This is only if you know the value can be from a limited set of possible values.

"filter":{  "bool" : {
                "must_not" : [
                    {
                        "term" : { "nativeAdCampaignName" : "VALUE" }
                    }
                ]
            }
    }
</comment><comment author="rjernst" created="2015-04-23T06:35:04Z" id="95461389">@HenleyChiu Can you describe in more detail what exact behavior you are seeing? Do you have an index created before 1.3.0?

@dling Were you able to upgrade to 1.5.1 and confirm it was the same issue as in #10414?
</comment><comment author="dling" created="2015-04-27T08:56:38Z" id="96573309">**The cluster was upgraded to 1.5.1 but this issue remains.**

(We also upgraded Java to OpenJDK Runtime Environment (build 1.8.0_45-b13)).

@HenleyChiu Thanks for the workaround suggestion but we have a big variety of possible values so that wouldn't be manageable in our case.

@rjernst I've read the #10414 issue, yes these issues are likely linked and related. Do you see a possibility that this could be resolved in an upcoming 1.5.x release or is the re-indexing to a new index the recommended workaround ? I'd have to start looking into the possibilities of that.

@clintongormley The `filtered` query is among the types I tried initially but it has the same problem. The `content` is indeed an index alias yes. Are you and @rjernst on top of what causes this issue or would any of the requested debug information help ?
</comment><comment author="clintongormley" created="2015-04-27T09:57:48Z" id="96588791">@rjernst I can confirm that the `missing/exists` functionality is broken when moving from (eg) and index created in 1.1.0 to 1.5.0, 1.5.1, or 1.5.2.
</comment><comment author="briar354" created="2015-04-27T11:02:34Z" id="96610373">Hi.
We upgraded to 1.5.1 this weekend and now the missing filter does not work any longer. Can I provide more data for you that might help in your investigations or have you already located the problem?
</comment><comment author="rjernst" created="2015-04-28T08:19:25Z" id="96970009">This should now be fixed, and will be in 1.5.3.  The issue was already fixed on master, but the missing filter portion of the fix was not correctly tested or backported.
</comment><comment author="briar354" created="2015-04-28T08:48:03Z" id="96980070">When is 1.5.3 scheduled for?
</comment><comment author="clintongormley" created="2015-04-28T09:31:09Z" id="96988272">As a workaround, you can use:

```
{ "not": { "exists": { "field": "field_name" }}}
```
</comment><comment author="alainperry" created="2015-06-12T17:45:00Z" id="111571369">@clintongormley I'm on 1.5.2 with the same problem, and the exists filter is affected as well. Which means that this workaround does not work either.
</comment><comment author="dling" created="2015-06-12T17:47:51Z" id="111572296">Was the fix included in 1.6.0?
</comment><comment author="s1monw" created="2015-06-12T18:02:14Z" id="111576221">@dling yes I will add the label
</comment><comment author="alainperry" created="2015-06-12T18:30:25Z" id="111582205">My "missing" filter works again. Thank you.
</comment><comment author="clintongormley" created="2015-06-12T18:48:33Z" id="111587135">thanks for letting us know @alainperry 
</comment><comment author="nilsga" created="2015-06-26T10:35:05Z" id="115636388">Does this also affect the `missing` aggregation?
</comment><comment author="jpountz" created="2015-06-26T10:51:54Z" id="115643683">No, the `missing` aggregation works in a very different way.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove useless random translog directory selection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10589</link><project id="" key="" /><description>Using ThreadLocalRandom only prevents reproducibilty but doesn't buy us
anything. In production different datapaths won't have the same since
anyway or at least with a low likelyhood.
</description><key id="68308950">10589</key><summary>Remove useless random translog directory selection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T08:56:08Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T14:01:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-14T09:11:25Z" id="92709903">LGTM, just left one small question.
</comment><comment author="s1monw" created="2015-04-14T10:40:57Z" id="92763384">good point @rjernst I removed it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc/log enhancement: disk watermark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10588</link><project id="" key="" /><description>The documentation and key name of the cluster setting `cluster.routing.allocation.disk.watermark.low` is inconsistent with the message from the logfile. When value is set to default of `85%` (according to [documentation](http://www.elastic.co/guide/en/elasticsearch/reference/1.5/index-modules-allocation.html#disk)), the warning message says:

```
low disk watermark [15%] exceeded
```

It is IMHO misleading
</description><key id="68305183">10588</key><summary>doc/log enhancement: disk watermark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels><label>:Allocation</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-04-14T08:40:43Z</created><updated>2015-06-01T22:14:13Z</updated><resolved>2015-06-01T22:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-14T13:20:01Z" id="92825291">You're right, this should be pretty easy to fix by either standardizing on displaying the used space or free space everywhere.
</comment><comment author="mkis-" created="2015-05-19T17:44:25Z" id="103611729">Hello! I've attempted a fix for this by changing the logging so that it displays used space when dealing with percentages.

I've also created a pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use tanuki wrapper for running in debian</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10587</link><project id="" key="" /><description>It gives the following (the most valuable):
- autorestart
- bounce if JVM failed to respond to ping command. Usually when full GC consumes all CPU
  (Full list of features at: http://wrapper.tanukisoftware.com/doc/english/introduction.html )

Tanuki wrapper could be easily integrated with init.d on debian:
- https://packages.debian.org/source/sid/service-wrapper-java it already contains start/stop scripts for wrapper configurations.
</description><key id="68301084">10587</key><summary>Use tanuki wrapper for running in debian</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dernasherbrezon</reporter><labels /><created>2015-04-14T08:21:56Z</created><updated>2015-04-14T13:41:29Z</updated><resolved>2015-04-14T13:41:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-14T08:41:32Z" id="92697148">Tanuki wrapper is a great tool but I'm a bit scared to use it with sysv-init or systemd scripts. For what I've [read)(http://wrapper.tanukisoftware.com/doc/english/prop-ping-timeout.html), there is a chance that in case of a "JVM ping timeout" the wrapper will restart a new JVM without maybe correctly shutting down the first one.
</comment><comment author="dernasherbrezon" created="2015-04-14T08:57:22Z" id="92704975">We used wrapper in production for a long time, but I never encounter such shutdowns. Wrapper just sends normal "kill -9" to JVM. Anyway, if JVM hangs it's not possible to correctly execute shutdown/restart.

Unfortunately I'm not aware about any plans for elastic packaging..As a suggestion I would create separate package configuration for debian. With https://github.com/tcurdt/jdeb and https://github.com/dernasherbrezon/maven-apt-plugin it might be fairly easy.
</comment><comment author="tlrx" created="2015-04-14T09:14:31Z" id="92711485">Debian and RPM packages already exist and are released for every version of elasticsearch (see  https://www.elastic.co/blog/apt-and-yum-repositories). And yes, we're using jdeb :)

I don't like the ping timeout feature because it introduces something outside elasticsearch that can kill/restart a node while it's working or freezing. I'm curious to know what others think about this.
</comment><comment author="clintongormley" created="2015-04-14T13:41:28Z" id="92848240">In fact, we used to use the java service wrapper (see https://github.com/elastic/elasticsearch-servicewrapper) but decided against it because of the licensing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move back to single EngineConfig</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10586</link><project id="" key="" /><description>We need to preserve settings (yet transient) even though the engine is not yet
started. This commit moves back to a single EngineConfig to simplify IndexShard
and settings state.

Closes #10584
</description><key id="68298729">10586</key><summary>Move back to single EngineConfig</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-14T08:10:49Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T08:44:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-14T08:35:01Z" id="92694810">LGTM. left one minor comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow rebalancing primary shards on shared filesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10585</link><project id="" key="" /><description>Instead of failing the Engine for a shared filesystem, this change
allows a "soft close" of the Engine, where only the IndexWriter is
closed so that the replica can open an IndexWriter using the same
filesystem directory/mount.

Fixes #10469
</description><key id="68265260">10585</key><summary>Allow rebalancing primary shards on shared filesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>blocker</label><label>enhancement</label><label>v1.5.2</label><label>v1.6.0</label></labels><created>2015-04-14T05:21:06Z</created><updated>2015-06-01T22:34:44Z</updated><resolved>2015-04-22T17:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-14T05:22:56Z" id="92605218">@s1monw this PR is still missing a test where replication failure is simulated (like you asked for), but I wanted to vet the idea of a "soft-close" of the Engine, I had to make some changes after #10452 was merged to allow an Engine to be closed without closing the Translog, because closing the translog caused all future recovery to hang when `translog.snapshot()` is called.

Let me know what you think and I will work on adding more tests tomorrow.
</comment><comment author="s1monw" created="2015-04-14T08:50:35Z" id="92701509">@dakrone I don't think we should do the sep close methods an all that kind of stuff. I'd rather just call sync instead of close on the translog. Especially given the fact that @bleskes works on larger refactorings on that end so all the changes here are likely only needed on 1.x
</comment><comment author="dakrone" created="2015-04-14T12:36:02Z" id="92796257">@s1monw wouldn't that leave the translog in a never-closed state then? Or is the translog closed somewhere else? Does that just rely on the injector being closed to close it?
</comment><comment author="s1monw" created="2015-04-14T13:45:35Z" id="92852043">@dakrone the translog is close later - the sync is the important part..
</comment><comment author="s1monw" created="2015-04-21T11:06:49Z" id="94745501">@dakrone  I think this is good functionality wise but I think the implementation needs to be less intrusive ie. I think we should implement this as a subclass of the engine instead of adding all these settings and changing how the recovery handler works and calling back into it.  I took a quick step copying your test and adding this quick and dirty to the engien factory and I think it's cleaner what do you think about this https://github.com/s1monw/elasticsearch/commit/5fd56da1f4cd321975c4b3eea1d5e6df413d7448
</comment><comment author="dakrone" created="2015-04-21T16:27:41Z" id="94861592">@s1monw I updated this to use the method that you came up with
</comment><comment author="s1monw" created="2015-04-21T18:13:34Z" id="94893041">left minor comments LGTM otherwise
</comment><comment author="bleskes" created="2015-04-22T07:56:38Z" id="95067194">@dakrone this is the on that only goes to 1.x, right? asking because it still has the 2.0.0 label on it...
</comment><comment author="s1monw" created="2015-04-22T09:09:13Z" id="95084182">@bleskes the plan is to push this impl to 1.x and another impl to master based on your refactoring in https://github.com/elastic/elasticsearch/pull/10624 makes sense? I removed the 2.0 label for now
</comment><comment author="bleskes" created="2015-04-22T09:09:55Z" id="95084582">yeah, makes total sense, just checking because of the label. Thx,
</comment><comment author="s1monw" created="2015-04-22T16:19:49Z" id="95253881">sweet! LGTM
</comment><comment author="dakrone" created="2015-04-22T17:21:05Z" id="95272654">This has been merged to 1.x only, I will rewrite and open a new PR once #10624 is merged into master, since it refactors much of the recovery process.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test IndexingMemoryControllerTests#testIndexBufferSizeUpdateAfterCreationRemoval fails post #10452</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10584</link><project id="" key="" /><description>I was looking at a test failure to see if it relates to my change with the JVM stats aspect. Turns out it didn't, but managed to find the commit that did (#10452). Still did not chase it down into why. Running only the test on IntelliJ with `-Dtests.seed=F617D748C91B4DD4` causes it to fail on my machine every time, but this for some reason doesn't repro it (still not sure why, weird): 

```
mvn clean test -Dtests.seed=F617D748C91B4DD4 -Dtests.class=org.elasticsearch.indices.memory.IndexingMemoryControllerTests -Dtests.method="testIndexBufferSizeUpdateAfterCreationRemoval" -Dtests.locale=uk -Dtests.timezone=America/Indiana/Tell_City -Dtests.processors=4
```
</description><key id="68219721">10584</key><summary>Test IndexingMemoryControllerTests#testIndexBufferSizeUpdateAfterCreationRemoval fails post #10452</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels /><created>2015-04-13T23:38:30Z</created><updated>2015-04-14T08:44:31Z</updated><resolved>2015-04-14T08:44:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] False positive match using geo_shape</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10583</link><project id="" key="" /><description>Hi Elasticsearch Team,

I wanted to surface an issue regarding a geo query that seems to be giving me a false positive result. I have [a gist here that can be used to recreate what I am seeing](https://gist.github.com/nedcampion/0bbc30f2a999dc904ebf)

I am testing against 1.5.1. In short I create an index with a `geo_shape` field with the maximum precision, I then index a document containing a polygon that describes the boundary of Mexico and I then query against that index using an envelope that describes a small area in the southern tip of Texas. The query returns the document containing Mexico which is a false positive match.

I realize that [geo_shape does not provide 100% accuracy depending on how it is configured](http://www.elastic.co/guide/en/elasticsearch/reference/1.3/mapping-geo-shape-type.html#_accuracy) but the region I am querying with seems many miles away from the Mexico border and the precision is set to 1m. Is this a bug or am I missing something.
</description><key id="68215783">10583</key><summary>[GEO] False positive match using geo_shape</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nedcampion</reporter><labels><label>:Geo</label></labels><created>2015-04-13T23:11:04Z</created><updated>2015-04-21T19:03:55Z</updated><resolved>2015-04-21T19:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-14T13:17:38Z" id="92822530">@nknize please could you take a look
</comment><comment author="nknize" created="2015-04-14T13:30:09Z" id="92835505">This is related to #9691 (and similar #9860) a fix will be in place very very shortly.

tl;dr:  if you really want full precision you need to lower the distance_error_pct value. Be careful, you can quickly blow out memory.

In the meantime, if you want to get accurate results and preserve the most memory your best bet is to post-filter the results in the application layer (resultShape.relate(candidateShape))
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup local code transport execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10582</link><project id="" key="" /><description>Now that we handle automatically the local execution within the transport service, we can remove parts of the code that handle it in actions.
</description><key id="68196601">10582</key><summary>Cleanup local code transport execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T21:17:39Z</created><updated>2015-06-07T11:43:55Z</updated><resolved>2015-04-20T08:43:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-14T10:59:23Z" id="92768324">left one comment otherwise LGTM
</comment><comment author="bleskes" created="2015-04-15T12:23:47Z" id="93366682">Nice cleanup. Left one comment.
</comment><comment author="kimchy" created="2015-04-15T14:56:40Z" id="93433365">@s1monw great idea, I went ahead and took the cleanup a bit more, tell me what you think...
</comment><comment author="kimchy" created="2015-04-16T15:16:36Z" id="93760735">@bleskes I actually reverted the replica part, its not simple to do this refactor now (like forced execution, ...), how does it look now?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Endless re-syncing mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10581</link><project id="" key="" /><description>We're running into an recurring issue of late where any change in cluster state (typically a node restart during an upgrade for example) results in the following errors streaming to the log of the currently elected master, as well as very high I/O on all eligible masters:

```
[2015-04-13 21:02:37,917][WARN ][cluster.metadata         ] [elasticsearch-master2.localdomain] [pelias] 
re-syncing mappings with cluster state for types [[osmway, locality, openaddresses, geonames
osmaddress, admin0, neighborhood, admin2, osmnode, admin1, local_admin]]
```

We are presently running 1.5.1, but we had the same issue on 1.5.0. I'm cannot say with certainty whether we experienced the issue under 1.4.x. The production cluster setup is as follows, but we have also seen this problem in local development (single node):
- 3 master eligible nodes
- 2 routing (no data, no master) nodes
- 8 data nodes
- 40 shards, no replicas, ~600gb of data

The problematic mapping seems to be the `location` part of the suggester field:

```
PUT pelias
{
  "mappings": {
    "admin0": {
      "properties": {
        "suggest": {
          "context": {
            "dataset": {
              "default": [],
              "path": "_type",
              "type": "category"
            },
            "location": {
              "default": [],
              "neighbors": true,
              "path": "center_point",
              "precision": [2,3,1,5,4],
              "type": "geo"
            }
          },
          "max_input_length": 50,
          "payloads": false,
          "preserve_position_increments": false,
          "preserve_separators": false,
          "type": "completion"
        }
      }
    }
  }
}
```
</description><key id="68196392">10581</key><summary>Endless re-syncing mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">heffergm</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-04-13T21:16:43Z</created><updated>2015-04-15T16:02:22Z</updated><resolved>2015-04-15T16:02:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-14T13:16:01Z" id="92821000">@areek please could you take a look at this.  It appears to be the reordering of the `precision` field, as mentioned in #8937
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: Introduce toQuery() and fromXContent() methods in QueryBuilders and QueryParsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10580</link><project id="" key="" /><description>The currently planed refactoring of search queries layed out in #9901 and currently tracked in #10217 requires the current "parse()" methods into two methods, first a "fromXContent(...)" method that allows parsing to an intermediate query representation and second a "Query toQuery(...)" method these parsed query objects that create the actual lucene queries.

This PR is a first step in that direction as it introduces the interface changes necessary for the further refactoring. It introduces the new interface methods while for now keeping the old Builder/Parser API still in place by delegating the new "toQuery()" implementations to the existing "parse()" methods, and by introducing a "catch-all" "fromXContent()" implementation in a BaseQueryParser that returns a temporary QueryBuilder wrapper implementation. This allows us to refactor the existing QueryBuilders step by step while already beeing able to start refactoring queries with nested inner queries.
</description><key id="68193458">10580</key><summary>Query refactoring: Introduce toQuery() and fromXContent() methods in QueryBuilders and QueryParsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-13T21:02:12Z</created><updated>2015-04-17T20:09:44Z</updated><resolved>2015-04-17T20:09:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-14T04:59:56Z" id="92597622">Left a minor comment but this looks pretty good to me, @javanna what do you think?
</comment><comment author="cbuescher" created="2015-04-15T13:01:40Z" id="93383542">Working on BoolQueryBuilder based in this I realized that I also needed to split QueryParseContext#parseInnerQuery() into two parts, will be used to build the inner QueryBuilder in the parse-step and the second to later do some cache checking that is in place there at the moment. This part will only be called once the lucene queries are created. 
I added this split already to this PR we will need it for some of the nestes queries. I already encountered problems with existing tests when refactoring the BoolQueryBuilder that can be solved by having the cache-check available in the "toQuery()" step.
</comment><comment author="javanna" created="2015-04-16T15:40:45Z" id="93766756">I really like this change, it allows us to migrate queries gradually without changing everything at the same time. The only concern I have is whether we should merge this into master or start a new feature branch for the query refactoring. I do know that we are afraid of merge conflicts here, but I am also afraid that the solution is not merging temporary code. I am not doubting the quality of this PR, it's just that part of it will go away at the end of different steps of the refactoring and we don't know yet how long it will take to get it done. That said I have the feeling we want to work on this on a feature branch and pay the price of merge conflicts. Should get easier already though with this PR as we don't merge parser and builder anymore.
</comment><comment author="cbuescher" created="2015-04-16T17:12:37Z" id="93787527">Changes according to your comments. Main thing I did is pulling the "toQuery()" logic to the BaseQueryBuilder and letting the different builders provide the name of their corresponding parsers by a final method. Open to suggestions on this one though.
</comment><comment author="javanna" created="2015-04-17T08:09:58Z" id="93939957">I like it, left a few more comments.
</comment><comment author="cbuescher" created="2015-04-17T08:51:26Z" id="93946395">Added changes for your last comments, left out the second BaseQueryBuilder class you suggested for now, but I can still add that if you like. Just mentioned my thought about it in the comments.
</comment><comment author="javanna" created="2015-04-17T16:07:57Z" id="94017495">LGTM, this is ready to go on a feature branch as discussed, at least till the first step of the query refactoring is completed (moving over all of the queries to the new fromXContent and toQuery code, but leaving parsing on the data nodes as-is).
</comment><comment author="cbuescher" created="2015-04-17T20:09:43Z" id="94064614">I pushed this PR to a new feature branch here:
https://github.com/elastic/elasticsearch/commits/feature/query-refactoring
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added short comment on snapshotting closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10579</link><project id="" key="" /><description>Trying again, didn't pick up changed file last time.
</description><key id="68187326">10579</key><summary>Added short comment on snapshotting closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>docs</label></labels><created>2015-04-13T20:29:41Z</created><updated>2015-04-23T11:15:42Z</updated><resolved>2015-04-23T11:15:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-04-14T02:38:03Z" id="92568150">I think it should be "If the `indices` parameter specified includes _any_ closed indices....". Otherwise LGTM.
</comment><comment author="bleskes" created="2015-04-14T07:18:36Z" id="92671892">I might be missing something but I think we can also improve on the error message in this case? If the error message was clear like "can't snapshot closed indices" ,we could just say that only open yellow indices can be snapshotted. @imotov , what do you think?
</comment><comment author="ppf2" created="2015-04-14T17:40:29Z" id="92992533">+1 on @bleskes 's idea on improving the reason message
</comment><comment author="imotov" created="2015-04-15T09:46:58Z" id="93289654">@bleskes, @ppf2 I think you are right. I opened PR, which makes the message more clear.
</comment><comment author="clintongormley" created="2015-04-23T11:15:42Z" id="95543788">Hi @ppf2 

It looks like you've mixed two different changes into this PR: one for Java versions and one for snapshot/restore. The latter is no longer accurate since https://github.com/elastic/elasticsearch/pull/10608 was pushed.

I'm going to close this PR - please feel free to open a new one with just the Java changes (or just push those changes directly)

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added short comment on snapshotting closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10578</link><project id="" key="" /><description /><key id="68185926">10578</key><summary>Added short comment on snapshotting closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-04-13T20:22:38Z</created><updated>2015-04-13T20:24:01Z</updated><resolved>2015-04-13T20:24:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES having trouble parsing Hash of Array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10577</link><project id="" key="" /><description>hey guys,

I noticed that ES was failing on the following message on versions 1.2.2 and 1.50 when using it as an Outputter in Logstash. Any Ideas?  Also, if I should be posting this elsewhere, please let me know! Thank You.

Message:

```
{"myname":"terrell", "foo":{[1 , 2 , 3]}}
```

Error:

```
[2015-03-17 23:51:16,225][DEBUG][action.bulk              ] [mozstash-ubuntu-1404] [logstash-2015.03.17][3] failed to execute bulk item (index) index {[logstash-2015.03.17][logs][DdfZBz4qSKOMAOzT-NNp5w], source[{"@timestamp":"2015-03-17T23:51:16.235Z","tier":"_default","hostname":"emitter-ubuntu-1204","severity":"notice","service":"vagrant","collector":"collector-ubuntu-1404","message":"{\"myname\":\"terrell\", \"foo\":[1 , 2 , 3]}","@version":"1","host":"172.28.128.5:41237","myname":"terrell","foo":[1,2,3]}]}
org.elasticsearch.index.mapper.MapperParsingException: object mapping [foo] trying to serialize a value with no field associated with it, current value [1]
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:644)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:501)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:648)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:636)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:493)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:534)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:483)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:376)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:430)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:527)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="68185886">10577</key><summary>ES having trouble parsing Hash of Array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">domofactor</reporter><labels /><created>2015-04-13T20:22:21Z</created><updated>2015-04-14T13:19:23Z</updated><resolved>2015-04-14T13:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-04-14T02:47:41Z" id="92569352">Hash is a set of key - value pairs. I think your hash `{[1 , 2 , 3]}` is missing the key portion. It should be `{"arr": [1, 2, 3]}` to be a hash. Would you agree? Another question is why such hash is getting generated, and this would be a great question for the [logstash mailing list](https://groups.google.com/forum/#!forum/logstash-users). 
</comment><comment author="clintongormley" created="2015-04-14T13:19:22Z" id="92824540">Yep - this is malformed JSON. You should try to find where this bad JSON is coming from
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Circuit Breakers not triggering before OOM on a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10576</link><project id="" key="" /><description>I will have to prepare the logfile as it has some confidential information in it, but I will post the basic rundown of what is happening.
Our situation is that the circuit breakers do not seem to keep us from throwing an OOM/"stop-the-world" GC event, causing the node(s) to become unresponsive and very quickly bringing down our cluster. We have seen this happen once a day for the last week. The little background I can give you without posting the log file is that it seems like a large query comes in and one node gets an OOM while the other nodes trigger the circuit breakers. It would be great if the OOM node would come back up and not bring down our cluster however that is not the case.  

We have 3 master nodes, 26 data only nodes and 1 client node in production.
1. Can someone who has experimented with the circuit breakers give me some feedback as to why we are still getting OOMs related to a specific api request even if we set all 3 circuit breakers to 1%?
2. Circuit Breakers seem to only work against single queries (not a single api request) which does not help much when it comes to an enterprise solution like ours. Is this a correct assumption? 
3. Is there anything I can do on each node to ensure that we avoid OOMs? 
- Change the max heap size? 
- Change to G1GC? 
- Change the setting index.cache.field.type to soft to allow for more aggressive GC? 
- Change the following JVM option settings CMSInitiatingOccupancyFraction and UseCMSInitiatingOccupancyOnly?

Currently we are running 1.4.2, 

Each of the data nodes is running 2 instances of ES, except the 3 master nodes and the single client node.

Each data node box has 40  cores and 128GB of RAM, 28GB is allocated to the heap. Here are the settings in our yml file:

```
node.master: false
node.data: true
node.max_local_storage_nodes: 2
bootstrap.mlockall: true
transport.tcp.port: 9300
http.port: 9200
http.max_content_length: 400mb
gateway.recover_after_nodes: 25
gateway.recover_after_time: 1m
gateway.expected_nodes: 29
cluster.routing.allocation.node_concurrent_recoveries: 20
indices.recovery.max_bytes_per_sec: 200mb
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.timeout: 3s
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ['HM01:9300', 'HM02:9300', 'HM03:9300']
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.debug: 2s
monitor.jvm.gc.young.warn: 1000ms
monitor.jvm.gc.young.info: 700ms
monitor.jvm.gc.young.debug: 400ms
monitor.jvm.gc.old.warn: 10s
monitor.jvm.gc.old.info: 5s
monitor.jvm.gc.old.debug: 2s
action.auto_create_index: .marvel-*
action.disable_delete_all_indices: true
indices.cache.filter.size: 15%
index.refresh_interval: -1
threadpool.search.type: fixed
threadpool.search.size: 48
threadpool.search.queue_size: 10000000
cluster.routing.allocation.cluster_concurrent_rebalance: 6
indices.store.throttle.type: none
index.reclaim_deletes_weight: 4.0
index.merge.policy.max_merge_at_once: 5
index.merge.policy.segments_per_tier: 5
marvel.agent.exporter.es.hosts: ['172.16.110.238:9200', '172.16.110.237:9200']
marvel.agent.enabled: true
marvel.agent.interval: 30s
script.disable_dynamic: false
```

Please let me know if there are other settings you would like to see.
</description><key id="68151444">10576</key><summary>Circuit Breakers not triggering before OOM on a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wgodwin</reporter><labels><label>feedback_needed</label></labels><created>2015-04-13T17:28:57Z</created><updated>2015-08-26T19:29:57Z</updated><resolved>2015-08-26T19:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T18:21:15Z" id="92451064">Hi @wgodwin 

Please upload an example query which causes the OOM, plus the logs, and the output of these requests:

```
GET /_nodes
GET /_nodes/stats?fields=*
```
</comment><comment author="breakaway9" created="2015-04-13T19:23:57Z" id="92468865">Clinton,
I can't post the text files here, but have zipped them up and stuck them on dropbox here:
https://www.dropbox.com/s/7vr7qeyay1judav/nodes.zip?dl=0

We'll get the query posted shortly.
</comment><comment author="clintongormley" created="2015-04-13T19:38:43Z" id="92473600">@breakaway9 you posted `GET _nodes` request twice, but not the `GET _nodes/stats?fields=*``
</comment><comment author="breakaway9" created="2015-04-13T20:05:28Z" id="92482078">Sorry I had posted the results of "/_nodes?fields=*" here is the corrected version:
https://www.dropbox.com/s/sosznrtrkgec1e0/nodes_fields.txt.zip?dl=0
</comment><comment author="clintongormley" created="2015-04-14T12:46:02Z" id="92801788">The info you've provided looks pretty normal, but then you've restarted some nodes.  Would be interested in that query.

Also, I think you're running more than one instance of ES per box?  If so, you should set `processors` to the number of processors/cores to give to each instance.
</comment><comment author="jpountz" created="2015-08-26T19:29:57Z" id="135146761">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use suggester will throw some errors&#65281;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10575</link><project id="" key="" /><description>This is my index's metadata.

```
{
    "state": "open",
    "settings": {
        "index": {
            "creation_date": "1428906895254",
            "uuid": "4MHyTCKrRdmPJWRikbWHsw",
            "number_of_replicas": "1",
            "number_of_shards": "5",
            "version": {
                "created": "1040299"
            }
        }
    },
    "mappings": {
        "post": {
            "properties": {
                "content": {
                    "properties": {
                        "extended": {
                            "type": "string"
                        },
                        "brief": {
                            "type": "string"
                        }
                    }
                },
                "title": {
                    "type": "string"
                },
                "views": {
                    "type": "long"
                },
                "likes": {
                    "type": "long"
                },
                "areas": {
                    "type": "string"
                },
                "published": {
                    "format": "dateOptionalTime",
                    "type": "date"
                },
                "comments": {
                    "type": "long"
                }
            }
        }
    },
    "aliases": []
}

```

This is my request

```
curl -XPOST http://localhost:9200/posts/_suggest -d '

{
  "mysuggest":{
     "text":"hello world",
     "completion":{
        "field":"title"
     }
  }
}

'

```

This is response.

```
{
    "_shards": {
        "total": 5,
        "successful": 0,
        "failed": 5,
        "failures": [
            {
                "index": "posts",
                "shard": 0,
                "reason": "BroadcastShardOperationFailedException[[posts][0] ]; nested: ElasticsearchException[failed to execute suggest]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; "
            }
            ,
            {
                "index": "posts",
                "shard": 1,
                "reason": "BroadcastShardOperationFailedException[[posts][1] ]; nested: ElasticsearchException[failed to execute suggest]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; "
            }
            ,
            {
                "index": "posts",
                "shard": 2,
                "reason": "BroadcastShardOperationFailedException[[posts][2] ]; nested: ElasticsearchException[failed to execute suggest]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; "
            }
            ,
            {
                "index": "posts",
                "shard": 3,
                "reason": "BroadcastShardOperationFailedException[[posts][3] ]; nested: ElasticsearchException[failed to execute suggest]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; "
            }
            ,
            {
                "index": "posts",
                "shard": 4,
                "reason": "BroadcastShardOperationFailedException[[posts][4] ]; nested: ElasticsearchException[failed to execute suggest]; nested: ClassCastException[org.elasticsearch.index.mapper.core.StringFieldMapper cannot be cast to org.elasticsearch.index.mapper.core.CompletionFieldMapper]; "
            }
        ]
    }
}

```

I really can not find any way to solve this problem, please help me, I am very grateful&#65281;
</description><key id="68145791">10575</key><summary>Use suggester will throw some errors&#65281;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">janryWang</reporter><labels /><created>2015-04-13T17:08:55Z</created><updated>2015-04-13T19:33:19Z</updated><resolved>2015-04-13T17:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-04-13T17:20:11Z" id="92432879">Hi @janryWang,

You have to explicitly set the mapping for `title` to completion `type`. Here is a sample of how to setup and use the Completion Suggester:

``` json
// Mapping configuration
PUT posts
{
  "mappings": {
    "post" : {
      "properties": {
        "title" : {
          "type": "completion"
        }
      }
    }
  }
}

// indexing
PUT posts/post/1 
{
"title": "hello world"
}

// querying
GET posts/_suggest
{
  "_suggestion": {
    "text": "hel",
    "completion": {
      "field": "title"
      }
  }
} 
```

For more configuration info, see [completion suggester](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html)
</comment><comment author="janryWang" created="2015-04-13T19:33:19Z" id="92471884">@areek Thank you very much! I made it!&#128516;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Contrary to the javadoc, cluster health check fails with null indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10574</link><project id="" key="" /><description>In 1.4.4, the javadoc for `Requests.clusterHealthRequest` states that the `indices` parameter can "use `null` or `_all` to execute against all indices. However, passing `null` causes the health request to throw an exception that's caused by an NPE:

```
java.lang.NullPointerException
    at org.elasticsearch.action.admin.cluster.health.TransportClusterHealthAction.masterOperation(TransportClusterHealthAction.java:133)
    at org.elasticsearch.action.admin.cluster.health.TransportClusterHealthAction.masterOperation(TransportClusterHealthAction.java:46)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.run(TransportMasterNodeOperationAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```
</description><key id="68127523">10574</key><summary>Contrary to the javadoc, cluster health check fails with null indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wilkinsona</reporter><labels><label>:Java API</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-04-13T15:34:07Z</created><updated>2015-09-18T17:18:59Z</updated><resolved>2015-09-18T17:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T15:48:00Z" id="92408248">@luca what do we normally do here? Require `_all` or accept null?
</comment><comment author="javanna" created="2015-04-16T08:28:30Z" id="93677121">This is a bug, `null` becomes an empty array when the request goes through the wire, but if the request gets executed immediately on the node where it's created it causes NPE. In the context of cluster health empty indices mean no indices (only the global cluster health is returned), it is probably appropriate to check for null in TransportClusterHealthAction where the NPE gets thrown.
</comment><comment author="andrestc" created="2015-08-22T20:11:08Z" id="133750944">@javanna I think I will take a shot on this one. Should we treat null as "_all"?
</comment><comment author="andrestc" created="2015-09-16T23:27:22Z" id="140925013">I decided to give this a try but im not able to reproduce this on master.
I added the following test case:

``` java
@Test //issue #10574
    public void testLocalHealthWithNullIndicesActLikeAll() {
        createIndex("test");
        ensureGreen();

        for (String node : internalCluster().getNodeNames()) {
            logger.info("--&gt; running cluster local health with null indices");
            ClusterHealthResponse health = client(node).admin().cluster().prepareHealth(null).setLocal(true).setWaitForEvents(Priority.LANGUID).setTimeout("30s").get("10s");
            assertThat(health.getStatus(), equalTo(ClusterHealthStatus.GREEN));
            assertThat(health.isTimedOut(), equalTo(false));
            assertThat(health.getIndices().get("test").getStatus(), equalTo(ClusterHealthStatus.GREEN));
        }
    }
```

And it passes. I belive this bug is fixed in master, because we handle this case in [IndexNameExpressionResolver](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java#L116).

@clintongormley @javanna do we need this fixed in 1.x?
If this is indeed fixed in master, do you want me to PR the test case?

Thanks.
</comment><comment author="clintongormley" created="2015-09-18T15:59:33Z" id="141493082">@andrestc this is a minor bug with an easy workaround - i wouldn't fix it for 1.x.  It sounds like it is already fixed in master? If so, then I'd just close this issue.
</comment><comment author="andrestc" created="2015-09-18T16:02:17Z" id="141493684">@clintongormley Cool. Yea, It looks fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow ActionListener to be called on the network thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10573</link><project id="" key="" /><description>To protect  ourselves against running blocking operations on a network thread we have added an assertion that triggers that verifies that the thread calling a BaseFuture.get() is not a networking one. While this assert is good, it wrongly triggers when the get() is called in order to pass it's result to  a listener of AbstractListenableActionFuture who is marked to run on the same thread as the callee. At that point, we know that the operation has been completed and the get() call will not block.

To solve this, we change the assertion to ignore a get with a timeout of 0 and use that AbstractListenableActionFuture

Relates to #10402
</description><key id="68126624">10573</key><summary>Allow ActionListener to be called on the network thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T15:29:30Z</created><updated>2015-06-04T20:36:10Z</updated><resolved>2015-06-04T20:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-06-03T16:48:57Z" id="108520312">@jpountz I applied your feedback and rebased. Can you have another look?
</comment><comment author="jpountz" created="2015-06-04T08:10:50Z" id="108777238">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Restrict read permission to project.basedir/target if security manager is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10572</link><project id="" key="" /><description>@rmuir can you run the tests to verify
</description><key id="68124435">10572</key><summary>[BUILD] Restrict read permission to project.basedir/target if security manager is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T15:20:08Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T07:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-13T21:59:47Z" id="92512743">+1, this is great. all tests pass with the changes here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `_as_string` output to only show when format specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10571</link><project id="" key="" /><description>Closes #10284
</description><key id="68119723">10571</key><summary>Fix `_as_string` output to only show when format specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T15:00:44Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T15:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-13T21:08:00Z" id="92499177">Seeing the fix, it looks like we should either remove ValueFormatter.RAW or enforce valueFormatter to be non-null?
</comment><comment author="colings86" created="2015-04-14T09:47:16Z" id="92725729">@jpountz Good point. I tend to agree. I had a look into removing ValueFormatter.RAW and I'm not sure that's the way to go since if we remove it we will end up just recreating what it is doing inside `if (valueFormatter == null)` statements which feels like a step backwards to me.

I also had a look at enforcing that the valueFormatter is not null. This should be ok providing that we are ok with using ValueFormatter.Raw in the case where we cannot determine the valueType for the field? ([see here](https://github.com/elastic/elasticsearch/blob/7bd7ea8f138d2f6c5e330c5abc8e98b8d4cfb419/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java#L192-200)). WDYT?
</comment><comment author="jpountz" created="2015-04-14T12:45:38Z" id="92801323">@colings86 It sounds good to me!
</comment><comment author="colings86" created="2015-04-14T14:08:10Z" id="92865437">@jpountz is it worth applying the fix as is to 1.5.2, 1.6, and 2.0 and then work on a new PR to force the valueFormat to be non-null from 2.0 onwards? I am a bit worried about maintaining backwards compatibility if we force the ValueFormat to be non-null as there are a number of cases where we currently set the ValueFormat, ValueFormatter and/or ValueParser to null.
</comment><comment author="jpountz" created="2015-04-14T14:10:38Z" id="92866603">@colings86 +1 I'm fine with merging this PR if we also commit to clean it up in 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename START phase into VERIFY_INDEX</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10570</link><project id="" key="" /><description>we really only optionally run checkindex in this phase and moving
the engine start into translog is move cleaner code wise.
</description><key id="68119596">10570</key><summary>Rename START phase into VERIFY_INDEX</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T15:00:23Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T07:55:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-13T17:26:14Z" id="92437477">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Restrict read permission to project.basedir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10569</link><project id="" key="" /><description>This prevents reads from anywhere outside of the elasticsearch
clone when running tests with security manager enabled.

I would really appreciate if folks could test this with `mvn clean test  -Dtests.security.manager=true` all tests pass for me here on linux and macos
</description><key id="68108662">10569</key><summary>[BUILD] Restrict read permission to project.basedir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T14:17:24Z</created><updated>2015-04-13T14:49:59Z</updated><resolved>2015-04-13T14:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-13T14:31:09Z" id="92381403">+1, tests work for me. huge improvement over ALL_FILES
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline aggregations: Ability to perform computations on aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10568</link><project id="" key="" /><description>Adds a new type of aggregation called 'reducers' which act on the output of aggregations and compute extra information that they add to the aggregation tree. Reducers look much like any other aggregation in the request but have a `buckets_path` parameter which references the aggregation(s) to use.

Internally there are two types of reducer; the first is given the output of its parent aggregation and computes new aggregations to add to the buckets of its parent, and the second (a specialisation of the first) is given a sibling aggregation and outputs an aggregation to be a sibling at the same level as that aggregation.

This PR includes the framework for the reducers, the derivative reducer (https://github.com/elastic/elasticsearch/issues/9293), the moving average reducer(https://github.com/elastic/elasticsearch/issues/10002) and the maximum bucket reducer(https://github.com/elastic/elasticsearch/issues/10000). These reducer implementations are not all yet fully complete.

Known work left to do (these points will be done once this PR is merged into the master branch):
- Add x-axis normalisation to the derivative reducer
- Add lots more JUnit tests for all reducers

Contributes to #9876, #10002, #9293, and #10000
</description><key id="68107079">10568</key><summary>Pipeline aggregations: Ability to perform computations on aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T14:09:49Z</created><updated>2015-06-06T17:49:21Z</updated><resolved>2015-04-29T15:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-24T08:32:49Z" id="95852377">I like the fact that it's not too invasive and makes efforts to not have side-effects on other aggs by copying the agg trees instead of modifying them in-place.

Most of my comments are cosmetics, in particular there are lots of indentation issues in aggregator constructor and anonymous LeafCollector definitions.

Regarding documentation, I think we need to move general informations to the main page about aggs/reducers instead of having it scattered across individual reducers/aggs.
</comment><comment author="colings86" created="2015-04-29T08:33:28Z" id="97352057">@jpountz I pushed an update and replied to some of your comments. Any chance you could have another look?
</comment><comment author="jpountz" created="2015-04-29T13:14:46Z" id="97422029">It looks to me like there are still some open TODOs about documentation but other than that LGTM. I pushed commits for some indentation issues I found.
</comment><comment author="polyfractal" created="2015-04-29T13:24:28Z" id="97423970">I'm working on the larger doc TODOs (e.g. centralize docs about `_count`, `buckets_path` and path syntax, `gap_policy`, etc).  I don't think it should block merging this though, especially since I'm restructuring part of the aggs docs as a whole and want some eyeballs...don't want to hold up this PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk update should support script_file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10567</link><project id="" key="" /><description>Bulk update supports `script_id` to provide the name for an indexed script and `script` for inline script, but no `script_file` for using scripts stored in `config/scripts/`. When I tried to use `script_file` I got an

```
{
  "error": "ActionRequestValidationException[Validation Failed: 1: script or doc is missing;]",
  "status": 400
}
```
</description><key id="68106329">10567</key><summary>Bulk update should support script_file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jakrol</reporter><labels /><created>2015-04-13T14:06:30Z</created><updated>2015-04-13T15:44:32Z</updated><resolved>2015-04-13T15:44:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T15:44:32Z" id="92407358">Hiya @jakrol 

This is fixed in 1.5.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>LinkageError causes ElasticSearch (and VM) to crash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10566</link><project id="" key="" /><description>ES version: 1.4.4.
One of our clients is running ES in a virtual server (OVH VPS). 
From what I understand, ElasticSearch stopped working after one hour with the following error.
This error was repeated a couple of times at the end of the log file, then the whole VM crashed.

```
[2015-04-13 06:02:13,567][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.LinkageError: loader constraint violation: when resolving method "java.nio.channels.Selector.wakeup()Ljava/nio/channels/Selector;" the class loader (instance of sun/misc/Launcher$AppClassLoader) of the current class, org/elasticsearch/common/netty/channel/socket/nio/AbstractNioSelector, and the class loader (instance of &lt;bootloader&gt;) for the method's defining class, java/nio/channels/Selector, have different Class objects for the type java/nio/channels/Selector used in the signature
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-04-13 06:02:13,568][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.LinkageError: loader constraint violation: when resolving method "java.nio.channels.Selector.wakeup()Ljava/nio/channels/Selector;" the class loader (instance of sun/misc/Launcher$AppClassLoader) of the current class, org/elasticsearch/common/netty/channel/socket/nio/AbstractNioSelector, and the class loader (instance of &lt;bootloader&gt;) for the method's defining class, java/nio/channels/Selector, have different Class objects for the type java/nio/channels/Selector used in the signature
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-04-13 06:02:13,609][WARN ][netty.util.HashedWheelTimer] An exception was thrown by TimerTask.
java.lang.LinkageError: loader constraint violation: when resolving method "java.nio.channels.Selector.wakeup()Ljava/nio/channels/Selector;" the class loader (instance of sun/misc/Launcher$AppClassLoader) of the current class, org/elasticsearch/common/netty/channel/socket/nio/NioClientBoss$1, and the class loader (instance of &lt;bootloader&gt;) for the method's defining class, java/nio/channels/Selector, have different Class objects for the type java/nio/channels/Selector used in the signature
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss$1.run(NioClientBoss.java:54)
    at org.elasticsearch.common.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:556)
    at org.elasticsearch.common.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:632)
    at org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:369)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at java.lang.Thread.run(Thread.java:745)
[2015-04-13 06:02:23,566][WARN ][transport.netty          ] [Linkurious Index] exception caught on transport layer [[id: 0x5f0a5b48]], closing connection
java.lang.LinkageError: loader constraint violation: when resolving method "java.nio.channels.Selector.wakeup()Ljava/nio/channels/Selector;" the class loader (instance of sun/misc/Launcher$AppClassLoader) of the current class, org/elasticsearch/common/netty/channel/socket/nio/AbstractNioSelector, and the class loader (instance of &lt;bootloader&gt;) for the method's defining class, java/nio/channels/Selector, have different Class objects for the type java/nio/channels/Selector used in the signature
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:115)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.register(AbstractNioSelector.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.register(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:121)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
    at org.elasticsearch.common.netty.channel.Channels.connect(Channels.java:634)
    at org.elasticsearch.common.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
    at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
    at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:788)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:741)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:714)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:150)
    at org.elasticsearch.cluster.service.InternalClusterService$ReconnectToNodes.run(InternalClusterService.java:544)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="68061520">10566</key><summary>LinkageError causes ElasticSearch (and VM) to crash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidrapin</reporter><labels /><created>2015-04-13T10:15:55Z</created><updated>2015-04-13T11:07:21Z</updated><resolved>2015-04-13T11:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dwerder" created="2015-04-13T10:50:10Z" id="92305899">This duplicates: https://github.com/elastic/elasticsearch/issues/9582
</comment><comment author="davidrapin" created="2015-04-13T11:07:21Z" id="92314229">Ok thanks! I tried finding related tickets based on the error messages but could'nt find any.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade From 1.4.2 to 1.5.1 (Exception:  Could not find a state file to recover from)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10565</link><project id="" key="" /><description>Hello,
When I upgrading our machine form 1.4.2 to 1.5.1.
copying data/config from 1.4.2  folder to 1.5.1 data/config folder.
when execute elasticsearch.bat, following exception throw and elasticsearch crash.

```
[2015-04-13 17:50:07,869][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-04-13 17:50:07,929][INFO ][node                     ] [bimgo] version[1.5.1], pid[3436], build[5e38401/2015-04-09T13:41:35Z]
[2015-04-13 17:50:07,929][INFO ][node                     ] [bimgo] initializing
 ...
[2015-04-13 17:50:07,988][INFO ][plugins                  ] [bimgo] loaded [mapper-attachments, analysis-smartcn], sites []
[2015-04-13 17:50:11,014][ERROR][gateway.local.state.shards] [bimgo] failed to read local state (started shards), exiting...
org.elasticsearch.ElasticsearchIllegalStateException: Could not find a state file to recover from among [[id:18, legacy:true, file:D:\elasticsearch-1.5.1\data\elasticsearchaa\nodes\0\indices\8934\1\_state\state-18], [id:8, legacy:false,file:D:\elasticsearch-1.5.1\data\elasticsearchaa\nodes\0\indices\8934\1\_state\state-8.st]]
        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:300)
        at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.loadShardStateInfo(LocalGatewayShardsState.java:188)
        at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.loadShardsStateInfo(LocalGatewayShardsState.java:173)
        at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.&lt;init&gt;(LocalGatewayShardsState.java:66)
```

We also try upgrading to other version too.
It's working upgrading from 1.4.2 to 1.4.4.
When upgrading from 1.4.2 to 1.5.0 , same exception also throw.

Best regards
</description><key id="68061110">10565</key><summary>Upgrade From 1.4.2 to 1.5.1 (Exception:  Could not find a state file to recover from)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wumingwuan</reporter><labels><label>:Recovery</label></labels><created>2015-04-13T10:12:53Z</created><updated>2016-03-10T16:32:57Z</updated><resolved>2015-05-04T19:33:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T12:16:02Z" id="92331688">@s1monw what should be done here? delete the legacy file? or can the non-legacy file not be read for some reason, so we should bump up logging?
</comment><comment author="s1monw" created="2015-04-13T12:21:15Z" id="92333020">you are running windows here right? did you upgrade and downgrade again by any chance?
</comment><comment author="s1monw" created="2015-04-13T12:22:31Z" id="92333163">or did you upgrade to 1.5.0 first for some reason?
</comment><comment author="wumingwuan" created="2015-04-14T02:47:57Z" id="92569379">This my upgrade process.
1. backup 1.4.2 data/config folder.
2.copy 1.4.2 data/config to extracted 1.5.0 folder 
3.execute 1.5.0/elasticsearch.bat  =&gt; exception throw
I thought maybe 1.5.1 fix this problem. so i download 1.5.1
and copy 1.4.2 data/config to extracted 1.5.1 folder =&gt;same exception throw.
Finally, I download 1.4.4 for test.
and copy 1.4.2 data/config to extracted 1.4.4 folder =&gt;working.

We tried the last week 1.4.2 backup data in 1.5.1=&gt; working .
maybe this week's UAT 1.4.2 data corrupt or something.
We will try this upgrade on our production machine , if same problem happen , we will let you know.
Thank you for your help :)
</comment><comment author="nellicus" created="2015-04-16T09:28:39Z" id="93690745">I am seeing this in my test system on Linux/1.5.0, though not right after upgrade, 1.5.0 has been running for a while with several stop/restart.
It's my test box where I do lot of test/hacking so not sure about sequence of events :/
</comment><comment author="HenrikOssipoff" created="2015-05-04T08:59:30Z" id="98641602">Seeing the same in an upgrade from 1.4.3 to 1.5.2, running Linux.

In our process we actually upgraded to 1.5.2, but had to downgrade back to 1.4.3 (no indices were created while running 1.5.2, so theoretically this shouldn't have been an issue?).

After we settled an issue we had, we once again tried upgrading to 1.5.2, but we're now seeing this issue.

What's the best way to get around this issue once we're in this place?
</comment><comment author="clintongormley" created="2015-05-04T13:48:13Z" id="98708679">&gt; In our process we actually upgraded to 1.5.2, but had to downgrade back to 1.4.3 (no indices were created while running 1.5.2, so theoretically this shouldn't have been an issue?).

Not true - any writes could have changed the index (and in fact it looks like it wrote a new state file during the failed upgrade).

@s1monw what can be done once an index is in this state?
</comment><comment author="HenrikOssipoff" created="2015-05-04T13:51:06Z" id="98709398">Worth noting that no new indices were created while running 1.5.x and downgrading the node to 1.4.x and it runs without issues.
</comment><comment author="s1monw" created="2015-05-04T14:32:27Z" id="98727781">you can just remove the state file in without the `.st` ending in this case to recover one you are on 1.5
ie. `file:D:\elasticsearch-1.5.1\data\elasticsearchaa\nodes\0\indices\8934\1\_state\state-18`
</comment><comment author="HenrikOssipoff" created="2015-05-04T17:24:00Z" id="98785669">Worked perfectly, thank you.
</comment><comment author="clintongormley" created="2015-05-04T19:33:13Z" id="98826299">thanks for letting us know @HenrikOssipoff - closing
</comment><comment author="geekamit" created="2015-09-19T20:54:46Z" id="141706784">Thanks. Removing .st file worked perfectly.
</comment><comment author="skylarmb" created="2015-12-17T23:38:35Z" id="165616334">Just in cast anyone comes across this in the future and the above solution does not work for you, here is what I did to fix it. DISCLAIMER: I am setting up a local dev machine, so all my data is replaceable and unimportant. I DO NOT know the effects of this on a server or other machine where you don't have a backup of this data in case it doesn't work... 

I installed 2.1 and later found out the app I was trying to work on required 1.7.3 and tried to downgrade. I received this error 

`[ERROR][gateway.local.state.shards] [Lilandra Neramani] failed to read local state (started shards), exiting...
org.elasticsearch.ElasticsearchException: unexpected field in shard state [index_uuid]`

After trying many things that didnt work, including the above solution, I resorted to simply deleting the entire data folder for elasticsearch and rebuilding it. `rm -rf /usr/local/var/elasticsearch`. Then I just ran a reindex which rebuilt that folder `rake search:reindex` and everything worked just fine. 
</comment><comment author="dlamichhane" created="2015-12-21T10:42:31Z" id="166267893">+1
</comment><comment author="cerealkill" created="2016-01-20T18:38:55Z" id="173319202">Thank you @skylarmb .
</comment><comment author="skylarmb" created="2016-01-27T22:23:54Z" id="175891610">rake is a command for Ruby on Rails applications. If you aren't developing
in Rails then you should figure out what the equivalent re-index command is
on whatever technology stack you are using.

On Wed, Jan 27, 2016 at 7:47 AM, ckc6cz notifications@github.com wrote:

&gt; @skylarmb https://github.com/skylarmb I'm having the issue that you
&gt; describe and deleted the elasticsearch folder as per your suggestion, but
&gt; the "rake search:reindex" command did not do anything for me. I'm pretty
&gt; new to this and lost so was wondering if you had any additional suggestions
&gt; as to what to do?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10565#issuecomment-175695855
&gt; .

## 

_Skylar Morkner Brown_
_(916) 743-5921_
</comment><comment author="ckc6cz" created="2016-02-11T14:51:28Z" id="182895610">Not an issue, just me being stupid. Sorry!

On Thu, Feb 11, 2016 at 3:59 AM, Pradeep Gowda notifications@github.com
wrote:

&gt; We lost all the data after removing .st file. Is this a know issue?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10565#issuecomment-182772523
&gt; .

## 

Chelsea Chandler
University of Virginia Class of 2016
B.A. Computer Science and Mathematics
</comment><comment author="pradbk" created="2016-02-11T20:00:39Z" id="183039900">I did a upgrade from 1.4.4 to 1.7.1 in production. Ended up in same state (ElasticsearchIllegalStateException). removed all state file and I lost months worth of data! Do you guys have any fix for this? How to recover safely from this state? 
</comment><comment author="HenleyChiu" created="2016-03-10T16:32:57Z" id="194939716">I had this same problem. 

I'm not sure what fixed it but I did these 2 things that I did NOT do before:
1) Sync-Flush the nodes running with old version: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-synced-flush.html
2) Shutdown the nodes properly rather than do a kill -9.
curl -XPOST 'http://localhost:9200/_cluster/nodes/_local/_shutdown'

Then when I copied the data files over, it was able to start successfully with the new version with no errors.
This is from 1.0.1 to 2.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot operation stuck, delete command doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10564</link><project id="" key="" /><description>We take snpashots every hour to AWS S3 directly using cloud-aws plugin
snapshot was stuck because of a reboot on master node during the snapshot, ES version 1.4.0, ArchLinux
Cluster was later upgraded to 1.4.2 at which point we noticed that there is a snapshot stuck
while trying to create a new snapshot
`{"error":"ConcurrentSnapshotExecutionException[[my_s3_repository:2015.04.13.09.16.42] a snapshot is already running]","status":503}`
curl localhost:9200/_snapshot/my_s3_repository/_all?pretty doesn't have the stuck snapshot in the list.

Delete snapshot command never completes, it hangs forever.
rolling restart (restarting one node after the other) didn't help.
Clean up script doesn't work for me https://github.com/imotov/elasticsearch-snapshot-cleanup/issues/2

In S3, there is no metadata file for the stuck snapshot, there are snapshot-&lt;snapshot-name&gt; files for some of the indices

How do I atleast forget/discard old snapshots..create a new repository and start fresh. I can not stop the whole cluster as it is a production cluster.

curl localhost:9200/_snapshot/_status?pretty

```
{
  "snapshots" : [ {
    "snapshot" : "2015.03.11.06.04.39",
    "repository" : "my_s3_repository",
    "state" : "ABORTED",
    "shards_stats" : {
      "initializing" : 0,
      "started" : 0,
      "finalizing" : 0,
      "done" : 17,
      "failed" : 3,
      "total" : 20
    },
    "stats" : {
      "number_of_files" : 0,
      "processed_files" : 0,
      "total_size_in_bytes" : 0,
      "processed_size_in_bytes" : 0,
      "start_time_in_millis" : 0,
      "time_in_millis" : 0
    },
    "indices" : {
      "playground_video" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 5,
          "failed" : 0,
          "total" : 5
        },
        "stats" : {
          "number_of_files" : 0,
          "processed_files" : 0,
          "total_size_in_bytes" : 0,
          "processed_size_in_bytes" : 0,
          "start_time_in_millis" : 0,
          "time_in_millis" : 0
        },
        "shards" : {
          "4" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "1" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          }
        }
      },
      "playground_feed" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 4,
          "failed" : 1,
          "total" : 5
        },
        "stats" : {
          "number_of_files" : 0,
          "processed_files" : 0,
          "total_size_in_bytes" : 0,
          "processed_size_in_bytes" : 0,
          "start_time_in_millis" : 0,
          "time_in_millis" : 0
        },
        "shards" : {
          "4" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "1" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          }
        }
      },
      "playground_profile" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 4,
          "failed" : 1,
          "total" : 5
        },
        "stats" : {
          "number_of_files" : 0,
          "processed_files" : 0,
          "total_size_in_bytes" : 0,
          "processed_size_in_bytes" : 0,
          "start_time_in_millis" : 0,
          "time_in_millis" : 0
        },
        "shards" : {
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "1" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "4" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "0" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          }
        }
      },
      "playground_admin" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 4,
          "failed" : 1,
          "total" : 5
        },
        "stats" : {
          "number_of_files" : 0,
          "processed_files" : 0,
          "total_size_in_bytes" : 0,
          "processed_size_in_bytes" : 0,
          "start_time_in_millis" : 0,
          "time_in_millis" : 0
        },
        "shards" : {
          "2" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "1" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "4" : {
            "stage" : "FAILURE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          },
          "3" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 0,
              "processed_files" : 0,
              "total_size_in_bytes" : 0,
              "processed_size_in_bytes" : 0,
              "start_time_in_millis" : 0,
              "time_in_millis" : 0
            }
          }
        }
      }
    }
  } ]
}
```
</description><key id="68054902">10564</key><summary>Snapshot operation stuck, delete command doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vanga</reporter><labels><label>:Snapshot/Restore</label></labels><created>2015-04-13T09:35:25Z</created><updated>2016-01-07T15:36:43Z</updated><resolved>2015-04-13T12:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T12:03:19Z" id="92328084">@imotov any ideas here?
</comment><comment author="imotov" created="2015-04-13T12:07:34Z" id="92329909">@clintongormley not sure I tried to reproduce the issue with cleanup script that @vanga described in https://github.com/imotov/elasticsearch-snapshot-cleanup/issues/2 but I cannot. The issue itself seems to be a duplicate of #9924 and should be fixed in 1.4.5. 
</comment><comment author="clintongormley" created="2015-04-13T12:08:30Z" id="92330222">thanks @imotov 

@vanga sounds like you will need to upgrade.
</comment><comment author="vanga" created="2015-04-13T12:10:50Z" id="92330762">@clintongormley .. If I update to version &gt; 1.4.5 , this issue will go away and i can start taking snapshots?
</comment><comment author="clintongormley" created="2015-04-13T12:17:53Z" id="92332249">@vanga sounds like that is the case (but you'll need to upgrade to 1.5.1, as 1.4.5 has not been released yet)
</comment><comment author="vanga" created="2015-04-24T10:47:50Z" id="95890118">We have joined new nodes to the clsuter with 1.5.1 version and removed old nodes. New cluster doesn't have the problem . 
Thanks.
</comment><comment author="allthedrones" created="2016-01-06T23:19:53Z" id="169493579">I'm experiencing this issue in 1.6.1 ... or at least a _very_ similar issue; in my case, the snapshot state (as reported by ~/_snapshot/repo/_status) is ABORTED, but the hung shard still reports a stage of "STARTED". Curiously, the output from ~/_snapshot/repo/_all does not agree with the output from .../_status -- it shows the hung snapshot as "IN_PROGRESS". Issuing a REST call to DELETE the target snapshot hangs indefinitely -- never returns, and never times out (I also couldn't find any relevant activity in pending_tasks, hot_threads, nor the log files).

For clarity, the master node did not restart, suffer a network partition, etc ... there was no interruption of any kind during the time the snapshot was running and nothing in the logs on the master node or the node with the 'stuck' shard. The repo _is_ using the Azure plugin. The indices (1 per snapshot) being snapshot'd are large so the operation is long running ... typically between 30 and 90 minutes. This cluster has never run any version other than 1.6.1, and there are no other clusters pointing to this Azure container, etc.

Nothing but a full restart seems to fix this problem. I expected, based on https://github.com/elastic/elasticsearch/pull/11450, that I would be able to clean this up by simply restarting the current master and allowing one of the other two [dedicated] master nodes to be elected, but that wasn't enough to fix the problem (I tried twice for good measure). I wasn't able to get @imotov 's snapshot cleanup tool to build against 1.6.1 (maybe due to #15427?). As long as the snapshot is stuck in this state, I cannot initiate other snapshots, the `reroute` API refuses to move the affected shard, and `_close`ing the index goes badly, because all shards report as unallocated (index=red), but the index will not fully close (nor re-open) because the shard is more or less 'locked' by the stuck snapshot.

The downtime from a full restart is unacceptable in production so this is currently preventing me from being able to use snapshots there, but I have a non-production cluster currently exhibiting the problem and I'll leave it in this 'stuck' state for a bit so I can answer any questions or produce API outputs, etc.

I uploaded some relevant output (/_shapshot/repo/_all, /_snapshot/repo/_status, /index/_stats?level=shards) here: https://gist.github.com/allthedrones/898f282be33a33dfec67
</comment><comment author="imotov" created="2016-01-07T00:54:49Z" id="169512227">@allthedrones could you post the snapshot section of `_cluster/state`?
</comment><comment author="allthedrones" created="2016-01-07T05:46:07Z" id="169560497">Here's the snapshot section of `_cluster/state`. I verified that the other two url endpoints under `_snapshot` still return the same results as posted earlier, and that attempting DELETE on the snapshot still hangs. Strange that all 3 places report slightly different statuses for the snapshot!

```
"snapshots" : [{
    "repository" : "my_stuff",
    "snapshot" : "idx-15923",
    "include_global_state" : false,
    "state" : "ABORTED",
    "indices" : [ "idx-15923" ],
    "shards" : [ {
      "index" : "idx-15923",
      "shard" : 3,
      "state" : "SUCCESS",
      "node" : "cvnYD0WlQEi9-AmEiHTC4g"
    }, {
      "index" : "idx-15923",
      "shard" : 4,
      "state" : "SUCCESS",
      "node" : "rloJ197jS9eSWiDqy29QHA"
    }, {
      "index" : "idx-15923",
      "shard" : 1,
      "state" : "ABORTED",
      "node" : "q-F03vzxTBqdbUojCWeY9g"
    }, {
      "index" : "idx-15923",
      "shard" : 2,
      "state" : "SUCCESS",
      "node" : "KFVhLk0eRDmuGxLzZ1HEww"
    }, {
      "index" : "idx-15923",
      "shard" : 0,
      "state" : "SUCCESS",
      "node" : "FZ8pwJqsTgiz2rvDCBVrgw"
    } ]
}]
```
</comment><comment author="imotov" created="2016-01-07T06:06:45Z" id="169564327">@allthedrones is the node `q-F03vzxTBqdbUojCWeY9g` still part of the cluster? If it is, could you run the following command and post here the result 

`curl localhost:9200/_nodes/q-F03vzxTBqdbUojCWeY9g/hot_threads?threads=1000`
</comment><comment author="allthedrones" created="2016-01-07T13:51:08Z" id="169669086">@imotov Here's the `hot_threads` result. The last block looks relevant ... like maybe a stream read operation from the Azure repo connector is waiting indefinitely?

```
::: [mc2-ed6][q-F03vzxTBqdbUojCWeY9g][mc2-ed6][inet[/10.31.0.11:9300]]{master=false}
   Hot threads at 2016-01-07T13:40:47.042Z, interval=500ms, busiestThreads=1000, ignoreIdleThreads=true:

    3.9% (19.5ms out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][bulk][T#2]'
     2/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    2.5% (12.3ms out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][bulk][T#3]'
     3/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    0.6% (2.8ms out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][bulk][T#4]'
     2/10 snapshots sharing following 10 elements
       org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:371)
       org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:511)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:413)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:148)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:574)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    0.5% (2.6ms out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][bulk][T#1]'
     3/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    0.0% (205.7micros out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)

    0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[mc2-ed6][snapshot][T#1]'
     10/10 snapshots sharing following 21 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
       java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
       java.util.concurrent.ExecutorCompletionService.take(ExecutorCompletionService.java:193)
       com.microsoft.azure.storage.blob.BlobOutputStream.waitForTaskToComplete(BlobOutputStream.java:456)
       com.microsoft.azure.storage.blob.BlobOutputStream.dispatchWrite(BlobOutputStream.java:348)
       com.microsoft.azure.storage.blob.BlobOutputStream.writeInternal(BlobOutputStream.java:578)
       com.microsoft.azure.storage.blob.BlobOutputStream.write(BlobOutputStream.java:506)
       com.microsoft.azure.storage.blob.BlobOutputStream.write(BlobOutputStream.java:482)
       com.microsoft.azure.storage.blob.BlobOutputStream.write(BlobOutputStream.java:541)
       org.elasticsearch.cloud.azure.blobstore.AzureOutputStream.write(AzureOutputStream.java:35)
       java.io.OutputStream.write(OutputStream.java:116)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:564)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:507)
       org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:140)
       org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:85)
       org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:871)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

    0.0% (0s out of 500ms) cpu usage by thread 'DestroyJavaVM'
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot
     unique snapshot

    0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[keepAlive/1.6.1]'
     10/10 snapshots sharing following 8 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
       java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
       org.elasticsearch.bootstrap.Bootstrap$4.run(Bootstrap.java:267)
       java.lang.Thread.run(Thread.java:745)

    0.0% (0s out of 500ms) cpu usage by thread 'pool-3853-thread-1'
     10/10 snapshots sharing following 23 elements
       java.net.SocketInputStream.socketRead0(Native Method)
       java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
       java.net.SocketInputStream.read(SocketInputStream.java:170)
       java.net.SocketInputStream.read(SocketInputStream.java:141)
       java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
       java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
       java.io.BufferedInputStream.read(BufferedInputStream.java:345)
       sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704)
       sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647)
       sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536)
       sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441)
       java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
       com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:124)
       com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlockInternal(CloudBlockBlob.java:714)
       com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlock(CloudBlockBlob.java:686)
       com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:362)
       com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:358)
       java.util.concurrent.FutureTask.run(FutureTask.java:266)
       java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
       java.util.concurrent.FutureTask.run(FutureTask.java:266)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="imotov" created="2016-01-07T14:09:14Z" id="169674409">@allthedrones indeed the 6th block and the last block show stuck azure operations. It looks like azure never came back with responses for these requests. I think this problem has been [fixed](https://github.com/elastic/elasticsearch/issues/14277) for the next major release, but the only solution for 1.6.1 that I can think of is to restart the node where azure is stuck. 

@dadoonet, @tlrx what do you think?
</comment><comment author="dadoonet" created="2016-01-07T14:51:30Z" id="169684925">I agree with your analysis @imotov. The azure client until #15080 wait indefinitely. There is no way to unblock it without restarting the node.

I wonder then if we should backport the change #15080 in 2.x branch.
</comment><comment author="imotov" created="2016-01-07T15:16:21Z" id="169690968">@dadoonet I think it would be reasonable to backport it to 2.x branch considering the impact on the users. 
</comment><comment author="allthedrones" created="2016-01-07T15:36:43Z" id="169697774">Thanks guys, I can confirm that restarting the node with the stalled shard corrects the problem. After restarting that data node, the snapshot in question was fully deleted and all of the snapshot URLs mentioned previously report a correct &amp; consistent status. Restarting a single data node is a _much_ more acceptable impact than downtime for the whole cluster, so thank you again for your help arriving at a resolution &amp; root cause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix simple_query_string example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10563</link><project id="" key="" /><description>The `&amp;` is not part of the simple_query_string DSL
</description><key id="68047026">10563</key><summary>Fix simple_query_string example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bdelbosc</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-04-13T09:00:04Z</created><updated>2015-04-13T12:48:16Z</updated><resolved>2015-04-13T12:48:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T12:01:38Z" id="92327262">Hi @bdelbosc 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
https://www.elastic.co/contributor-agreement
</comment><comment author="bdelbosc" created="2015-04-13T12:23:44Z" id="92333311">Done
</comment><comment author="clintongormley" created="2015-04-13T12:48:07Z" id="92340659">many thanks @bdelbosc - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.2.0-snapshot-1673124.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10562</link><project id="" key="" /><description /><key id="68043922">10562</key><summary>Upgrade to lucene-5.2.0-snapshot-1673124.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T08:45:06Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-14T14:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-13T08:56:22Z" id="92276540">LGTM, I like the added checks in AllTermSpanScorer.
</comment><comment author="rmuir" created="2015-04-13T12:12:10Z" id="92330913">I actually disagree with mike. Paul went the route of adding these asserts to every single spans, and yet they could still not be debugged. 

Problem is: when trying to duplicate checks like this across N classes then its inevitable that some will be incomplete. you are also bound to have some buggy asserts (false fails) and it also makes it really hard to read the code.

So I think the asserts should be removed (from lucene too). I opened https://issues.apache.org/jira/browse/LUCENE-6411 yesterday and worked on it all day long to fix the root cause. 
</comment><comment author="jpountz" created="2015-04-13T12:21:02Z" id="92332994">While I do agree that AssertingSpans is a better way to find bugs, all that this patch does is aligning AllTermQuery with SpanTermQuery from which it is derived. I can push a new update once LUCENE-6411 is done.
</comment><comment author="rmuir" created="2015-04-13T12:23:28Z" id="92333285">LUCENE-6411 is already done. Please don't add these redundant asserts. it just causes me more work to remove them.
</comment><comment author="jpountz" created="2015-04-13T12:26:49Z" id="92334113">I can remove them but then you might want to remove them from SpanScorer.setFreqCurrentDoc too.
</comment><comment author="rmuir" created="2015-04-13T12:28:21Z" id="92334733">The only reason they are still there is because i needed to get some amount of sleep last night. It is work to cleanup all this stuff.
</comment><comment author="rmuir" created="2015-04-13T22:58:21Z" id="92526444">See my commit, i cleaned up this query: it doesn't manipulate Spans anymore.
</comment><comment author="jpountz" created="2015-04-14T11:01:33Z" id="92769500">Thanks @rmuir !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient timeout Bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10561</link><project id="" key="" /><description>Hi,

After an OOM in one of multiple client nodes (data=false, master=false),
the java client (Transport client) can not connect to the other nodes that are still running.

The java client does not see that the node is down and still sends requests.
</description><key id="68042053">10561</key><summary>TransportClient timeout Bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polgl</reporter><labels /><created>2015-04-13T08:34:05Z</created><updated>2015-04-13T11:58:36Z</updated><resolved>2015-04-13T11:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T11:58:35Z" id="92326148">Hi @polgl 

The problem is that a node that has OOM'ed still carries on running, so it is not actually down (unless you're saying that it as killed by the OOM killer?)

Essentially, there are only two things to do here:
- monitor for OOMs and kill a node that has one
- avoid OOMs (ie figure out why it occurred and avoid it in the future)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Use SuppressForbidden annotation instead of class level excludes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10560</link><project id="" key="" /><description>Forbidden APIs 1.8 allows excludes based on annotations which can now be on methods etc. for more find grained control.
</description><key id="68035740">10560</key><summary>[BUILD] Use SuppressForbidden annotation instead of class level excludes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T08:05:09Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-13T10:24:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-04-13T08:26:19Z" id="92267662">Exactly as it was intended to be used :-)
+1
</comment><comment author="jpountz" created="2015-04-13T10:02:56Z" id="92298406">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting targeted fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10559</link><project id="" key="" /><description>I am having problems highlighting targeted fields. Lets say I have a query as below
{
   "size": 10,
   "from": 0,
   "query": {
      "filtered": {
         "query": {
            "query_string": {
               "query": "subject:Librar? AND title:event"
            }
         }
      }
   },
   "highlight": {
      "fields": {
         "subject": {
            "number_of_fragments": 0
         },
         "title": {
            "number_of_fragments": 0
         }
      }
   }
}

What I expect is that - all words in the field subject matching the expression "Librar?" to get highlighted and all words in the field title containing "event" should get highlighted. However, I get library in titles as highlighted. If I change the query to .. "subject:Library AND title:event", i get the expected result. The wildcard \* or ? seems to throw off the highlighting. Is there a fix or am i doing something wrong.
</description><key id="68028384">10559</key><summary>Highlighting targeted fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anshudutta</reporter><labels /><created>2015-04-13T07:24:23Z</created><updated>2015-04-13T11:52:50Z</updated><resolved>2015-04-13T11:52:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T11:52:50Z" id="92325172">Hi @anshudutta 

You're looking for `require_field_match`. See http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#field-match
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to recover into a folder containing a corrupted shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10558</link><project id="" key="" /><description>At the moment, we are very strict when handling data folders containing corrupted shards and will fail any recovery attempt into it. Typically this wouldn't be a problem as the shard will be assigned to another node (which we try first anyway when a shard fails). However, it has been proven to be too strict for smaller clusters which may not have an extra node available (either because of allocation filtering, disk space issues etc.). This commit changes the behavior to force a full recovery. Once all the new files are verified we remove the old corrupted data and start the shard.

This also fixes a small issue where the shard state file wasn't deleted on an engine failure (we had a protection against deleting the state file on an active shard, but in this case the shard is still active but will be removed). The state deletion is also moved to before the failure handlers are called, to avoid race conditions when calling the master (it will potentially try to read it when allocating the shard)
</description><key id="68021425">10558</key><summary>Allow to recover into a folder containing a corrupted shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T06:48:37Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-13T13:18:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-13T07:24:28Z" id="92248560">I don't think this needs to go in to 1.5.2 it's not a bugfix it a new way of handling things as well as a pretty controversial change IMO. Yet I think it LGTM but doesn't qualify as a bugfix
</comment><comment author="bleskes" created="2015-04-13T08:29:14Z" id="92269500">@s1monw  fair enough - I removed the 1.5.2 label. Double checking - are you +1 on this as it is now?
</comment><comment author="s1monw" created="2015-04-13T08:33:51Z" id="92271152">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include OS details in _nodes output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10557</link><project id="" key="" /><description>It'd be handy if a call to _nodes gave you some basic information around the OS, things like;
- Vendor - Debian, RHEL, MS etc
- Version - 12.04, Win 7, RHEL 7 etc
- Release - point release info
- Architecture - x32, x64 etc

Dunno how possible this is with sigar, but it'd be awesome on the support side of things.
</description><key id="68005827">10557</key><summary>Include OS details in _nodes output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Stats</label><label>discuss</label><label>enhancement</label></labels><created>2015-04-13T04:44:12Z</created><updated>2016-01-13T23:43:14Z</updated><resolved>2016-01-13T23:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-01-13T21:52:03Z" id="171448527">The _nodes API now (#12049) provides

```
os: {
  refresh_interval_in_millis: 1000,
  name: "Mac OS X",
  arch: "x86_64",
  version: "10.11.2",
  available_processors: 4,
  allocated_processors: 4
}
```

I think this is about everything you need.  The only partially remaining one is Vendor, which I'm not sure can/be accurately inferred down to the "RHEL" level in any case.  It is down to Linux / Windows / Mac OSX.  Does this fulfill what you need @markwalkom ?
</comment><comment author="markwalkom" created="2016-01-13T22:21:51Z" id="171455798">Yep, thanks :D
</comment><comment author="eskibars" created="2016-01-13T23:43:14Z" id="171473418">Great.  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update links in README file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10556</link><project id="" key="" /><description>Change-Id: Ife01695355db77c91246efd4f34a25998a3ca01f
</description><key id="67997298">10556</key><summary>Update links in README file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dpursehouse</reporter><labels /><created>2015-04-13T03:35:41Z</created><updated>2015-04-14T09:19:53Z</updated><resolved>2015-04-13T11:47:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T11:47:49Z" id="92324572">thanks @dpursehouse - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update forbiddenapis to version 1.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10555</link><project id="" key="" /><description>Update forbidden-apis plugin to 1.8:
- Initial support for Java 9 including JIGSAW
- Errors are now reported sorted by line numbers and correctly grouped (synthetic methods/lambdas)
- Package-level forbids: Deny all classes from a package: `org.hatedpkg.**` (also other globs work)
- In addition to file-level excludes, forbiddenapis now supports fine granular excludes using Java annotations. You can use the one shipped, but define your own, e.g. inside elasticsearch and pass its name to forbidden (e.g. using a glob: `**.SuppressForbidden` would any annotation in any package to suppress errors). Annotation need to be on class level, no runtime annotation required.

This pull request does not yet use the new features, it just updates dependency.
</description><key id="67976355">10555</key><summary>Update forbiddenapis to version 1.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-13T00:04:51Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-13T07:29:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-13T07:30:12Z" id="92250990">thx uwe!
</comment><comment author="uschindler" created="2015-04-13T08:02:34Z" id="92259928">If you need help with using the new annotation feature =&gt; I am available :-)
</comment><comment author="s1monw" created="2015-04-13T08:21:06Z" id="92265589">@uschindler https://github.com/elastic/elasticsearch/pull/10560
</comment><comment author="uschindler" created="2015-04-13T08:54:47Z" id="92275893">Perfect!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve exception handling in transport local execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10554</link><project id="" key="" /><description>Local execution of transport messages failures can create a more detailed remote transport exceptions. Also, when failing to handle an exception, the error should be logged, and not call the handler again with another exception
</description><key id="67928199">10554</key><summary>Improve exception handling in transport local execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-12T15:42:00Z</created><updated>2015-06-08T15:22:54Z</updated><resolved>2015-04-13T23:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-13T06:54:51Z" id="92235742">LGTM. Good catch about the double exception handling.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup JVM info and stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10553</link><project id="" key="" /><description>Remove reflection since its not needed with Java 7, remove lastGcInfo since its not used, and move to only provide getters
</description><key id="67924811">10553</key><summary>Cleanup JVM info and stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Stats</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-12T14:56:39Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-13T22:43:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-12T15:02:01Z" id="92076912">this looks like a great cleanup to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Doc: Updates to resiliency page for 1.5.0/1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10552</link><project id="" key="" /><description>@bleskes @s1monw @kimchy Please could you take a look and let me know if I've missed anything
</description><key id="67919432">10552</key><summary>Doc: Updates to resiliency page for 1.5.0/1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2015-04-12T14:03:16Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-14T13:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-13T22:37:46Z" id="92519879">left one comment, other than that, LGTM
</comment><comment author="clintongormley" created="2015-04-14T13:31:14Z" id="92836581">Changes addressed - thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian repositories don't work for e.g. armhf although packages are arch independent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10551</link><project id="" key="" /><description>Following http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html#_apt
will yield this:  
    W: Failed to fetch http://packages.elasticsearch.org/elasticsearch/1.5/debian/dists/stable/Release \
    Unable to find expected entry 'main/binary-armhf/Packages' in Release file (Wrong sources.list \
    entry or malformed file)

I'm not particularly familiar with the inner workings of apt repositories, but I know for sure that the packages, if downloaded and installed manually, work just fine on ARMhf. It would be nice if users wouldn't need to jump through hoops.

Stumbled into this while trying to set up an ELK stack on an C1 instance of http://scaleway.com

As there is no directory indexing when trying to browse the repository URL, retrieving the packages manually is unnecessarily hard.
</description><key id="67876729">10551</key><summary>Debian repositories don't work for e.g. armhf although packages are arch independent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dm8tbr</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-04-12T06:05:58Z</created><updated>2017-05-03T03:12:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-12T08:09:06Z" id="92006605">We don't have ARM packages sorry. You can see what is supported here - https://www.elastic.co/subscriptions/matrix

If you have a JVM you can just use the tar.gz/zip.
</comment><comment author="dm8tbr" created="2015-04-12T08:15:23Z" id="92006778">You do realize that I tested your packages on ARM and they work?
_"I know for sure that the packages, if downloaded and installed manually, work just fine on ARMhf."_

After all they are architecture independent. Of course it's your choice to pretend that they don't. Have a nice day.
</comment><comment author="markwalkom" created="2015-04-12T08:18:31Z" id="92007414">I personally don't have ARM to test on, and I don't believe that we have infrastructure to do it either.
So unfortunately, for now, it's not supported.
</comment><comment author="kimchy" created="2015-04-12T10:56:14Z" id="92032526">I think there are 2 levels here:
1. Make the packages work for this case, even though we don't continuously test on ARM.
2. Support ARM officially.

The first part, making our packages work and be installable in this case is worth it I think, @electrical can you maybe chime in into what we need to do here?

The second part is more complex. In order to officially support ARM, we need to make sure we continuously run both Lucene and Elasticsearch tests on it, and commit to fixing or at least raising bugs that are relevant to this platform. Even though there is a version of Java that runs on ARM, we see that those things matter and can expose bugs that are related to the environment it runs on.

@markwalkom, I will reopen the issue and see what @electrical has to say about the packaging aspect, it might be simple fix, dunno....
</comment><comment author="dm8tbr" created="2015-04-12T12:31:50Z" id="92049236">Thanks @kimchy, indeed at no point I was looking for official, or even commercial support, as @markwalkom implied.
Your architecture independent packages, as said earlier, install just fine and from my limited testing the resulting elasticsearch and logstash installations work well. Also because the packaging seems to explicitly work with armhf builds of openJDK for quite a while: https://github.com/elastic/elasticsearch/commit/6356ad2228c47ade41f2f717f60f22363a987211
The only lacking bit is to adjust the deb repository to make the packages discoverable. Preferably for any architecture, if not possible then at least armhf (but be prepared for someone asking about MIPS or powerPC down the road). Sadly I can't help with that detail as I am not that well familiar with Debian repositories and their metadata.

PS: Should you at any point be interested to do testing on ARM, then looking at http://scaleway.com might be worth it, as they have implemented a "cloud" interface for reasonably powerful, native ARMv7 server hardware. That's where I noticed the issue.
PPS: On a tangent, you may want to explicitly state on your website that anything else than x86/AMD64 is not officially supported, because the page that @markwalkom pointed to, saying it would state so, does in fact nowhere state that. I went and searched the whole website and couldn't find any mention of it, actually. Partly understandable as, aside from architecture specific JRE bugs, it should just work anywhere.
</comment><comment author="rmuir" created="2015-04-12T12:41:56Z" id="92051540">From the lucene side, I don't think ARM is possible to support, its not open source or in openjdk, as opposed to other platforms. 

Aarch64 is different, its open source, etc. But personally i won't spend one iota of time on oracle "commercial" platforms, I don't know about anyone else.
</comment><comment author="dm8tbr" created="2015-04-12T12:53:28Z" id="92055951">@rmuir I'm not sure I follow you. What isn't open source or in openjdk?
If this is about the JDK/JRE, I'm running openJDK on ARMv7 right now:
`ii  openjdk-8-jre:armhf  8u40-b27-1  armhf   OpenJDK Java runtime, using Hotspot Zero`
That's on what is soon to be released as Ubuntu 15.04, but 14.04 also has openjdk 7.
I couldn't care less about Oracle.
</comment><comment author="rmuir" created="2015-04-12T13:07:13Z" id="92057511">I feel differently about zero/shark. Its open source. 

However for other technical reasons I am suspicious about it, especially its level of testing, additional llvm dependency, etc. This configuration is still called `experimental` by IcedTea. Is it ready for prime time? If not, trying to get lucene's test suite working will not be a fun task.
</comment><comment author="electrical" created="2015-04-13T10:54:07Z" id="92307813">Regarding the repository it is indeed limited to i386 and x86_64/amd64. you can however force it for your case in your repo config.

'deb [arch=amd64] http://packages.elasticsearch.org/elasticsearch/1.5/debian stable main'
</comment><comment author="driegel" created="2016-03-07T16:29:21Z" id="193330501">@electrical Thank you for that info. It helped to install logstash from the apt repo on Raspberry Pi 2B.
My raspberry is currently running Ubuntu 15.10.

There is a good use case for installing Logstash on Raspberry Pi 2/3: collecting logs at a small satellite office and forward them to the central site. I will do some more testing of performance...

+1 to support Logstash on armhf and aarch64
</comment><comment author="vielmetti" created="2017-05-03T03:12:07Z" id="298814192">This has been about a year since the last comment; has there been any activity?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel showing IOPS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10550</link><project id="" key="" /><description>Marvel is not showing IOPS and also Elasticsearch in node stats doesn't show anything.
Elasticsearch 1.5 and Marvel the latest
</description><key id="67869636">10550</key><summary>Marvel showing IOPS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iemem15</reporter><labels><label>:Stats</label></labels><created>2015-04-12T04:22:44Z</created><updated>2015-04-29T18:48:22Z</updated><resolved>2015-04-29T17:43:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-12T04:51:52Z" id="91992639">Please use the mailing list for these sorts of problems - https://groups.google.com/forum/#!forum/elasticsearch
</comment><comment author="clintongormley" created="2015-04-13T11:22:26Z" id="92319090">Hi @iemem15 

Make sure you have `sigar` in your `lib` directory.  You can check if sigar is being loaded by editing `config/logging.yml` and setting `logger.monitor: TRACE`
</comment><comment author="iemem15" created="2015-04-13T18:04:08Z" id="92447326">Thanks I asked again in the groups, I enabled the logger.monitor and sigar is loaded, but I can't get info on IOPS
</comment><comment author="clintongormley" created="2015-04-13T18:22:15Z" id="92451278">@iemem15 What OS are you on?  And please upload the output of:

```
GET /_nodes/stats
```
</comment><comment author="iemem15" created="2015-04-13T19:03:34Z" id="92462406">SLES 12 64 Bits
{
   "cluster_name": "tqcilproescluslog01",
   "nodes": {
      "qH3BfKZfQb2fizG7z-7Rbw": {
         "timestamp": 1428951784601,
         "name": "tqcilproesda05.cilos.sat.gob.mx",
         "transport_address": "inet[/10.57.140.81:9300]",
         "host": "tqcilproesda05",
         "ip": [
            "inet[/10.57.140.81:9300]",
            "NONE"
         ],
         "attributes": {
            "master": "false"
         },
         "indices": {
            "docs": {
               "count": 1613973,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 319095708,
               "throttle_time_in_millis": 15488
            },
            "indexing": {
               "index_total": 2051679,
               "index_time_in_millis": 338473,
               "index_current": 7015,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 2,
               "query_time_in_millis": 24,
               "query_current": 0,
               "fetch_total": 2,
               "fetch_time_in_millis": 9,
               "fetch_current": 0
            },
            "merges": {
               "current": 1,
               "current_docs": 2450162,
               "current_size_in_bytes": 483350693,
               "total": 63,
               "total_time_in_millis": 204173,
               "total_docs": 8541237,
               "total_size_in_bytes": 1739072130
            },
            "refresh": {
               "total": 585,
               "total_time_in_millis": 33677
            },
            "flush": {
               "total": 5,
               "total_time_in_millis": 1773
            },
            "warmer": {
               "current": 0,
               "total": 963,
               "total_time_in_millis": 95
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 17,
               "memory_in_bytes": 1125066,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 512000,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 17
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            },
            "recovery": {
               "current_as_source": 0,
               "current_as_target": 0,
               "throttle_time_in_millis": 86889
            }
         },
         "os": {
            "timestamp": 1428951784601,
            "uptime_in_millis": 2933118,
            "load_average": [
               0,
               0.01,
               0.05
            ],
            "cpu": {
               "sys": 0,
               "user": 0,
               "idle": 98,
               "usage": 0,
               "stolen": 0
            },
            "mem": {
               "free_in_bytes": 13801619456,
               "used_in_bytes": 2938011648,
               "free_percent": 90,
               "used_percent": 9,
               "actual_free_in_bytes": 15182929920,
               "actual_used_in_bytes": 1556701184
            },
            "swap": {
               "used_in_bytes": 0,
               "free_in_bytes": 12889092096
            }
         },
         "process": {
            "timestamp": 1428951784602,
            "open_file_descriptors": 442,
            "cpu": {
               "percent": 0,
               "sys_in_millis": 28630,
               "user_in_millis": 636550,
               "total_in_millis": 665180
            },
            "mem": {
               "resident_in_bytes": 942260224,
               "share_in_bytes": 15699968,
               "total_virtual_in_bytes": 18546003968
            }
         },
         "jvm": {
            "timestamp": 1428951784602,
            "uptime_in_millis": 6274556,
            "mem": {
               "heap_used_in_bytes": 399924216,
               "heap_used_percent": 2,
               "heap_committed_in_bytes": 14557118464,
               "heap_max_in_bytes": 14557118464,
               "non_heap_used_in_bytes": 72973912,
               "non_heap_committed_in_bytes": 73826304,
               "pools": {
                  "young": {
                     "used_in_bytes": 276117960,
                     "max_in_bytes": 279183360,
                     "peak_used_in_bytes": 279183360,
                     "peak_max_in_bytes": 279183360
                  },
                  "survivor": {
                     "used_in_bytes": 15486928,
                     "max_in_bytes": 34865152,
                     "peak_used_in_bytes": 34865152,
                     "peak_max_in_bytes": 34865152
                  },
                  "old": {
                     "used_in_bytes": 108321424,
                     "max_in_bytes": 14243069952,
                     "peak_used_in_bytes": 108321424,
                     "peak_max_in_bytes": 14243069952
                  }
               }
            },
            "threads": {
               "count": 52,
               "peak_count": 71
            },
            "gc": {
               "collectors": {
                  "young": {
                     "collection_count": 220,
                     "collection_time_in_millis": 11392
                  },
                  "old": {
                     "collection_count": 1,
                     "collection_time_in_millis": 63
                  }
               }
            },
            "buffer_pools": {
               "direct": {
                  "count": 248,
                  "used_in_bytes": 12784975,
                  "total_capacity_in_bytes": 12784975
               },
               "mapped": {
                  "count": 8,
                  "used_in_bytes": 32885179,
                  "total_capacity_in_bytes": 32885179
               }
            }
         },
         "thread_pool": {
            "percolate": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "listener": {
               "threads": 2,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 2
            },
            "index": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "refresh": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 584
            },
            "suggest": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "generic": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 9,
               "completed": 18481
            },
            "warmer": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 658
            },
            "search": {
               "threads": 4,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 4,
               "completed": 4
            },
            "flush": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 583
            },
            "optimize": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "management": {
               "threads": 5,
               "queue": 0,
               "active": 1,
               "rejected": 0,
               "largest": 5,
               "completed": 2903
            },
            "get": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "merge": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 64
            },
            "bulk": {
               "threads": 4,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 4,
               "completed": 2431
            },
            "snapshot": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            }
         },
         "network": {
            "tcp": {
               "active_opens": 54865,
               "passive_opens": 6011,
               "curr_estab": 322,
               "in_segs": 24175468,
               "out_segs": 26702208,
               "retrans_segs": 34443,
               "estab_resets": 229,
               "attempt_fails": 47346,
               "in_errs": 1003,
               "out_rsts": 8189
            }
         },
         "fs": {
            "timestamp": 1428951784603,
            "total": {
               "total_in_bytes": 60653830144,
               "free_in_bytes": 53881384960,
               "available_in_bytes": 53362110464
            },
            "data": [
               {
                  "path":
"/var/lib/elasticsearch/tqcilproescluslog01/nodes/0",
                  "mount": "/",
                  "dev": "/dev/mapper/system-root",
                  "total_in_bytes": 60653830144,
                  "free_in_bytes": 53881384960,
                  "available_in_bytes": 53362110464
               }
            ]
         },
         "transport": {
            "server_open": 156,
            "rx_count": 25711,
            "rx_size_in_bytes": 549800802,
            "tx_count": 26552,
            "tx_size_in_bytes": 1347508648
         },
         "breakers": {
            "request": {
               "limit_size_in_bytes": 5822847385,
               "limit_size": "5.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            },
            "fielddata": {
               "limit_size_in_bytes": 8734271078,
               "limit_size": "8.1gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1.03,
               "tripped": 0
            },
            "parent": {
               "limit_size_in_bytes": 10189982924,
               "limit_size": "9.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            }
         }
      },
      "WLV6zWZ9Tu2knL2h_S2_ZA": {
         "timestamp": 1428951792995,
         "name": "tqcilproesda01.cilos.sat.gob.mx",
         "transport_address": "inet[/10.57.140.77:9300]",
         "host": "tqcilproesda01",
         "ip": [
            "inet[/10.57.140.77:9300]",
            "NONE"
         ],
         "attributes": {
            "master": "false"
         },
         "indices": {
            "docs": {
               "count": 3228184,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 637992304,
               "throttle_time_in_millis": 61369
            },
            "indexing": {
               "index_total": 3500885,
               "index_time_in_millis": 488995,
               "index_current": 27360,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 4,
               "query_time_in_millis": 34,
               "query_current": 0,
               "fetch_total": 2,
               "fetch_time_in_millis": 9,
               "fetch_current": 0
            },
            "merges": {
               "current": 0,
               "current_docs": 0,
               "current_size_in_bytes": 0,
               "total": 109,
               "total_time_in_millis": 251950,
               "total_docs": 11870530,
               "total_size_in_bytes": 2442473284
            },
            "refresh": {
               "total": 987,
               "total_time_in_millis": 48426
            },
            "flush": {
               "total": 10,
               "total_time_in_millis": 3254
            },
            "warmer": {
               "current": 0,
               "total": 1924,
               "total_time_in_millis": 138
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 30,
               "memory_in_bytes": 2218196,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 537382912,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 17
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            },
            "recovery": {
               "current_as_source": 0,
               "current_as_target": 0,
               "throttle_time_in_millis": 98871
            }
         },
         "os": {
            "timestamp": 1428951792995,
            "uptime_in_millis": 2936894,
            "load_average": [
               0,
               0.01,
               0.05
            ],
            "cpu": {
               "sys": 0,
               "user": 0,
               "idle": 99,
               "usage": 0,
               "stolen": 0
            },
            "mem": {
               "free_in_bytes": 13521121280,
               "used_in_bytes": 3218509824,
               "free_percent": 90,
               "used_percent": 9,
               "actual_free_in_bytes": 15169634304,
               "actual_used_in_bytes": 1569996800
            },
            "swap": {
               "used_in_bytes": 0,
               "free_in_bytes": 12889092096
            }
         },
         "process": {
            "timestamp": 1428951792996,
            "open_file_descriptors": 469,
            "cpu": {
               "percent": 0,
               "sys_in_millis": 26260,
               "user_in_millis": 851490,
               "total_in_millis": 877750
            },
            "mem": {
               "resident_in_bytes": 940101632,
               "share_in_bytes": 15642624,
               "total_virtual_in_bytes": 18565521408
            }
         },
         "jvm": {
            "timestamp": 1428951792996,
            "uptime_in_millis": 6538624,
            "mem": {
               "heap_used_in_bytes": 365436376,
               "heap_used_percent": 2,
               "heap_committed_in_bytes": 14557118464,
               "heap_max_in_bytes": 14557118464,
               "non_heap_used_in_bytes": 75423544,
               "non_heap_committed_in_bytes": 76447744,
               "pools": {
                  "young": {
                     "used_in_bytes": 247087688,
                     "max_in_bytes": 279183360,
                     "peak_used_in_bytes": 279183360,
                     "peak_max_in_bytes": 279183360
                  },
                  "survivor": {
                     "used_in_bytes": 2949240,
                     "max_in_bytes": 34865152,
                     "peak_used_in_bytes": 34865152,
                     "peak_max_in_bytes": 34865152
                  },
                  "old": {
                     "used_in_bytes": 115399448,
                     "max_in_bytes": 14243069952,
                     "peak_used_in_bytes": 115399448,
                     "peak_max_in_bytes": 14243069952
                  }
               }
            },
            "threads": {
               "count": 54,
               "peak_count": 60
            },
            "gc": {
               "collectors": {
                  "young": {
                     "collection_count": 352,
                     "collection_time_in_millis": 17141
                  },
                  "old": {
                     "collection_count": 1,
                     "collection_time_in_millis": 56
                  }
               }
            },
            "buffer_pools": {
               "direct": {
                  "count": 267,
                  "used_in_bytes": 13076137,
                  "total_capacity_in_bytes": 13076137
               },
               "mapped": {
                  "count": 16,
                  "used_in_bytes": 65858765,
                  "total_capacity_in_bytes": 65858765
               }
            }
         },
         "thread_pool": {
            "percolate": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "listener": {
               "threads": 2,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 2
            },
            "index": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "refresh": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 986
            },
            "suggest": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "generic": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 9,
               "completed": 19144
            },
            "warmer": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 1113
            },
            "search": {
               "threads": 6,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 6,
               "completed": 6
            },
            "flush": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 986
            },
            "optimize": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "management": {
               "threads": 5,
               "queue": 0,
               "active": 1,
               "rejected": 0,
               "largest": 5,
               "completed": 3945
            },
            "get": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "merge": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 109
            },
            "bulk": {
               "threads": 4,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 4,
               "completed": 4185
            },
            "snapshot": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            }
         },
         "network": {
            "tcp": {
               "active_opens": 59134,
               "passive_opens": 6201,
               "curr_estab": 322,
               "in_segs": 21167599,
               "out_segs": 25381938,
               "retrans_segs": 42961,
               "estab_resets": 256,
               "attempt_fails": 51354,
               "in_errs": 1020,
               "out_rsts": 8149
            }
         },
         "fs": {
            "timestamp": 1428951792996,
            "total": {
               "total_in_bytes": 60653830144,
               "free_in_bytes": 53182361600,
               "available_in_bytes": 52665479168
            },
            "data": [
               {
                  "path":
"/var/lib/elasticsearch/tqcilproescluslog01/nodes/0",
                  "mount": "/",
                  "dev": "/dev/mapper/system-root",
                  "total_in_bytes": 60653830144,
                  "free_in_bytes": 53182361600,
                  "available_in_bytes": 52665479168
               }
            ]
         },
         "transport": {
            "server_open": 156,
            "rx_count": 28943,
            "rx_size_in_bytes": 1009556257,
            "tx_count": 29834,
            "tx_size_in_bytes": 1023611624
         },
         "breakers": {
            "request": {
               "limit_size_in_bytes": 5822847385,
               "limit_size": "5.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            },
            "fielddata": {
               "limit_size_in_bytes": 8734271078,
               "limit_size": "8.1gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1.03,
               "tripped": 0
            },
            "parent": {
               "limit_size_in_bytes": 10189982924,
               "limit_size": "9.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            }
         }
      },
      "4px0AiviRq6oBdhJojDECw": {
         "timestamp": 1428951795673,
         "name": "tqcilprolidx01.cilos.sat.gob.mx",
         "transport_address": "inet[/10.57.140.68:9300]",
         "host": "tqcilprolidx01",
         "ip": [
            "inet[/10.57.140.68:9300]",
            "NONE"
         ],
         "attributes": {
            "client": "true",
            "data": "false"
         },
         "indices": {
            "docs": {
               "count": 0,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 0,
               "throttle_time_in_millis": 0
            },
            "indexing": {
               "index_total": 0,
               "index_time_in_millis": 0,
               "index_current": 0,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 0,
               "query_time_in_millis": 0,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 0,
               "current_docs": 0,
               "current_size_in_bytes": 0,
               "total": 0,
               "total_time_in_millis": 0,
               "total_docs": 0,
               "total_size_in_bytes": 0
            },
            "refresh": {
               "total": 0,
               "total_time_in_millis": 0
            },
            "flush": {
               "total": 0,
               "total_time_in_millis": 0
            },
            "warmer": {
               "current": 0,
               "total": 0,
               "total_time_in_millis": 0
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 0,
               "memory_in_bytes": 0,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 0,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 0
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            }
         },
         "os": {
            "timestamp": 1428951795673
         },
         "process": {
            "timestamp": 1428951795673,
            "open_file_descriptors": 380
         },
         "jvm": {
            "timestamp": 1428951795673,
            "uptime_in_millis": 5934453,
            "mem": {
               "heap_used_in_bytes": 272251416,
               "heap_used_percent": 4,
               "heap_committed_in_bytes": 591499264,
               "heap_max_in_bytes": 6084624384,
               "non_heap_used_in_bytes": 70395856,
               "non_heap_committed_in_bytes": 97386496,
               "pools": {
                  "young": {
                     "used_in_bytes": 8687344,
                     "max_in_bytes": 279183360,
                     "peak_used_in_bytes": 34537472,
                     "peak_max_in_bytes": 279183360
                  },
                  "survivor": {
                     "used_in_bytes": 4259840,
                     "max_in_bytes": 34865152,
                     "peak_used_in_bytes": 4259840,
                     "peak_max_in_bytes": 34865152
                  },
                  "old": {
                     "used_in_bytes": 259304232,
                     "max_in_bytes": 5770575872,
                     "peak_used_in_bytes": 549059640,
                     "peak_max_in_bytes": 5770575872
                  }
               }
            },
            "threads": {
               "count": 72,
               "peak_count": 78
            },
            "gc": {
               "collectors": {
                  "young": {
                     "collection_count": 19628,
                     "collection_time_in_millis": 211904
                  },
                  "old": {
                     "collection_count": 782,
                     "collection_time_in_millis": 21208
                  }
               }
            },
            "buffer_pools": {
               "direct": {
                  "count": 88,
                  "used_in_bytes": 10319778,
                  "total_capacity_in_bytes": 10319778
               },
               "mapped": {
                  "count": 0,
                  "used_in_bytes": 0,
                  "total_capacity_in_bytes": 0
               }
            }
         },
         "thread_pool": {
            "generic": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 4,
               "completed": 6527
            },
            "index": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "bench": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "get": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "snapshot": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "merge": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "suggest": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "bulk": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "optimize": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "warmer": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "flush": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "search": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "listener": {
               "threads": 2,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 3
            },
            "percolate": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "management": {
               "threads": 2,
               "queue": 0,
               "active": 1,
               "rejected": 0,
               "largest": 2,
               "completed": 1567
            },
            "refresh": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            }
         },
         "network": {},
         "fs": {
            "timestamp": 1428951795673,
            "total": {},
            "data": []
         },
         "transport": {
            "server_open": 143,
            "rx_count": 23047,
            "rx_size_in_bytes": 333267936,
            "tx_count": 23323,
            "tx_size_in_bytes": 568381251
         },
         "breakers": {
            "request": {
               "limit_size_in_bytes": 2433849753,
               "limit_size": "2.2gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            },
            "fielddata": {
               "limit_size_in_bytes": 3650774630,
               "limit_size": "3.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1.03,
               "tripped": 0
            },
            "parent": {
               "limit_size_in_bytes": 4259237068,
               "limit_size": "3.9gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            }
         }
      },
      "vdFUBYDwRfSwndf3NxFgRA": {
         "timestamp": 1428951797457,
         "name": "tqcilproesda06.cilos.sat.gob.mx",
         "transport_address": "inet[/10.57.140.82:9300]",
         "host": "tqcilproesda06",
         "ip": [
            "inet[/10.57.140.82:9300]",
            "NONE"
         ],
         "attributes": {
            "master": "false"
         },
         "indices": {
            "docs": {
               "count": 1614259,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 318833929,
               "throttle_time_in_millis": 16035
            },
            "indexing": {
               "index_total": 1866911,
               "index_time_in_millis": 278566,
               "index_current": 13140,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 3,
               "query_time_in_millis": 33,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 1,
               "current_docs": 2612295,
               "current_size_in_bytes": 515415889,
               "total": 57,
               "total_time_in_millis": 80234,
               "total_docs": 3832580,
               "total_size_in_bytes": 810150358
            },
            "refresh": {
               "total": 529,
               "total_time_in_millis": 27022
            },
            "flush": {
               "total": 5,
               "total_time_in_millis": 1559
            },
            "warmer": {
               "current": 0,
               "total": 963,
               "total_time_in_millis": 104
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 16,
               "memory_in_bytes": 1121776,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 512000,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 17
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            },
            "recovery": {
               "current_as_source": 0,
               "current_as_target": 0,
               "throttle_time_in_millis": 38184
            }
         },
         "os": {
            "timestamp": 1428951797457,
            "uptime_in_millis": 2936617,
            "load_average": [
               0.01,
               0.02,
               0.05
            ],
            "cpu": {
               "sys": 0,
               "user": 0,
               "idle": 100,
               "usage": 0,
               "stolen": 0
            },
            "mem": {
               "free_in_bytes": 14001172480,
               "used_in_bytes": 2738458624,
               "free_percent": 91,
               "used_percent": 8,
               "actual_free_in_bytes": 15257915392,
               "actual_used_in_bytes": 1481715712
            },
            "swap": {
               "used_in_bytes": 0,
               "free_in_bytes": 12889092096
            }
         },
         "process": {
            "timestamp": 1428951797458,
            "open_file_descriptors": 441,
            "cpu": {
               "percent": 0,
               "sys_in_millis": 20520,
               "user_in_millis": 482690,
               "total_in_millis": 503210
            },
            "mem": {
               "resident_in_bytes": 850669568,
               "share_in_bytes": 15642624,
               "total_virtual_in_bytes": 18541740032
            }
         },
         "jvm": {
            "timestamp": 1428951797458,
            "uptime_in_millis": 6274621,
            "mem": {
               "heap_used_in_bytes": 55158176,
               "heap_used_percent": 0,
               "heap_committed_in_bytes": 14557118464,
               "heap_max_in_bytes": 14557118464,
               "non_heap_used_in_bytes": 72885224,
               "non_heap_committed_in_bytes": 73900032,
               "pools": {
                  "young": {
                     "used_in_bytes": 6062248,
                     "max_in_bytes": 279183360,
                     "peak_used_in_bytes": 279183360,
                     "peak_max_in_bytes": 279183360
                  },
                  "survivor": {
                     "used_in_bytes": 5013848,
                     "max_in_bytes": 34865152,
                     "peak_used_in_bytes": 30354000,
                     "peak_max_in_bytes": 34865152
                  },
                  "old": {
                     "used_in_bytes": 44082080,
                     "max_in_bytes": 14243069952,
                     "peak_used_in_bytes": 44082080,
                     "peak_max_in_bytes": 14243069952
                  }
               }
            },
            "threads": {
               "count": 50,
               "peak_count": 69
            },
            "gc": {
               "collectors": {
                  "young": {
                     "collection_count": 185,
                     "collection_time_in_millis": 8345
                  },
                  "old": {
                     "collection_count": 1,
                     "collection_time_in_millis": 65
                  }
               }
            },
            "buffer_pools": {
               "direct": {
                  "count": 261,
                  "used_in_bytes": 12988231,
                  "total_capacity_in_bytes": 12988231
               },
               "mapped": {
                  "count": 8,
                  "used_in_bytes": 32934843,
                  "total_capacity_in_bytes": 32934843
               }
            }
         },
         "thread_pool": {
            "percolate": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "listener": {
               "threads": 2,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 2
            },
            "index": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "refresh": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 528
            },
            "suggest": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "generic": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 9,
               "completed": 16366
            },
            "warmer": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 596
            },
            "search": {
               "threads": 3,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 3,
               "completed": 3
            },
            "flush": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 527
            },
            "optimize": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "management": {
               "threads": 4,
               "queue": 0,
               "active": 1,
               "rejected": 0,
               "largest": 4,
               "completed": 2826
            },
            "get": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "merge": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 58
            },
            "bulk": {
               "threads": 4,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 4,
               "completed": 2224
            },
            "snapshot": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            }
         },
         "network": {
            "tcp": {
               "active_opens": 54802,
               "passive_opens": 6061,
               "curr_estab": 322,
               "in_segs": 21916136,
               "out_segs": 23322089,
               "retrans_segs": 39310,
               "estab_resets": 246,
               "attempt_fails": 47411,
               "in_errs": 922,
               "out_rsts": 8236
            }
         },
         "fs": {
            "timestamp": 1428951797458,
            "total": {
               "total_in_bytes": 60653830144,
               "free_in_bytes": 53891244032,
               "available_in_bytes": 53362171904
            },
            "data": [
               {
                  "path":
"/var/lib/elasticsearch/tqcilproescluslog01/nodes/0",
                  "mount": "/",
                  "dev": "/dev/mapper/system-root",
                  "total_in_bytes": 60653830144,
                  "free_in_bytes": 53891244032,
                  "available_in_bytes": 53362171904
               }
            ]
         },
         "transport": {
            "server_open": 156,
            "rx_count": 23104,
            "rx_size_in_bytes": 535988199,
            "tx_count": 23358,
            "tx_size_in_bytes": 701800076
         },
         "breakers": {
            "request": {
               "limit_size_in_bytes": 5822847385,
               "limit_size": "5.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            },
            "fielddata": {
               "limit_size_in_bytes": 8734271078,
               "limit_size": "8.1gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1.03,
               "tripped": 0
            },
            "parent": {
               "limit_size_in_bytes": 10189982924,
               "limit_size": "9.4gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            }
         }
      },
      "0UccJ9QrSRu2upShp3Zq4Q": {
         "timestamp": 1428951746859,
         "name": "tqcilproescl01.cilos.sat.gob.mx",
         "transport_address": "inet[tqcilproescl01/10.57.140.74:9300]",
         "host": "tqcilproescl01",
         "ip": [
            "inet[tqcilproescl01/10.57.140.74:9300]",
            "NONE"
         ],
         "attributes": {
            "data": "false",
            "master": "false"
         },
         "indices": {
            "docs": {
               "count": 0,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 0,
               "throttle_time_in_millis": 0
            },
            "indexing": {
               "index_total": 0,
               "index_time_in_millis": 0,
               "index_current": 0,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 0,
               "query_time_in_millis": 0,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 0,
               "current_docs": 0,
               "current_size_in_bytes": 0,
               "total": 0,
               "total_time_in_millis": 0,
               "total_docs": 0,
               "total_size_in_bytes": 0
            },
            "refresh": {
               "total": 0,
               "total_time_in_millis": 0
            },
            "flush": {
               "total": 0,
               "total_time_in_millis": 0
            },
            "warmer": {
               "current": 0,
               "total": 0,
               "total_time_in_millis": 0
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 0,
               "memory_in_bytes": 0,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 0,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 0
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            },
            "recovery": {
               "current_as_source": 0,
               "current_as_target": 0,
               "throttle_time_in_millis": 0
            }
         },
         "os": {
            "timestamp": 1428951746859,
            "uptime_in_millis": 2939079,
            "load_average": [
               0.01,
               0.03,
               0.05
            ],
            "cpu": {
               "sys": 0,
               "user": 0,
               "idle": 99,
               "usage": 0,
               "stolen": 0
            },
            "mem": {
               "free_in_bytes": 6181113856,
               "used_in_bytes": 2086060032,
               "free_percent": 86,
               "used_percent": 13,
               "actual_free_in_bytes": 7183175680,
               "actual_used_in_bytes": 1083998208
            },
            "swap": {
               "used_in_bytes": 0,
               "free_in_bytes": 12889092096
            }
         },
         "process": {
            "timestamp": 1428951746859,
            "open_file_descriptors": 439,
            "cpu": {
               "percent": 0,
               "sys_in_millis": 6400,
               "user_in_millis": 40620,
               "total_in_millis": 47020
            },
            "mem": {
               "resident_in_bytes": 569778176,
               "share_in_bytes": 15441920,
               "total_virtual_in_bytes": 7825641472
            }
         },
         "jvm": {
            "timestamp": 1428951746860,
            "uptime_in_millis": 3333732,
            "mem": {
               "heap_used_in_bytes": 89711088,
               "heap_used_percent": 2,
               "heap_committed_in_bytes": 4098621440,
               "heap_max_in_bytes": 4098621440,
               "non_heap_used_in_bytes": 50614304,
               "non_heap_committed_in_bytes": 51617792,
               "pools": {
                  "young": {
                     "used_in_bytes": 70614584,
                     "max_in_bytes": 279183360,
                     "peak_used_in_bytes": 279183360,
                     "peak_max_in_bytes": 279183360
                  },
                  "survivor": {
                     "used_in_bytes": 9041888,
                     "max_in_bytes": 34865152,
                     "peak_used_in_bytes": 28883024,
                     "peak_max_in_bytes": 34865152
                  },
                  "old": {
                     "used_in_bytes": 10054616,
                     "max_in_bytes": 3784572928,
                     "peak_used_in_bytes": 10054616,
                     "peak_max_in_bytes": 3784572928
                  }
               }
            },
            "threads": {
               "count": 46,
               "peak_count": 60
            },
            "gc": {
               "collectors": {
                  "young": {
                     "collection_count": 2,
                     "collection_time_in_millis": 139
                  },
                  "old": {
                     "collection_count": 1,
                     "collection_time_in_millis": 25
                  }
               }
            },
            "buffer_pools": {
               "direct": {
                  "count": 64,
                  "used_in_bytes": 14341250,
                  "total_capacity_in_bytes": 14341250
               },
               "mapped": {
                  "count": 0,
                  "used_in_bytes": 0,
                  "total_capacity_in_bytes": 0
               }
            }
         },
         "thread_pool": {
            "percolate": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "listener": {
               "threads": 2,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 2,
               "completed": 22
            },
            "index": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "refresh": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "suggest": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "generic": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 6,
               "completed": 7031
            },
            "warmer": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "search": {
               "threads": 1,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 1,
               "completed": 1
            },
            "flush": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "optimize": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "management": {
               "threads": 1,
               "queue": 0,
               "active": 1,
               "rejected": 0,
               "largest": 1,
               "completed": 737
            },
            "get": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "merge": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "bulk": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            },
            "snapshot": {
               "threads": 0,
               "queue": 0,
               "active": 0,
               "rejected": 0,
               "largest": 0,
               "completed": 0
            }
         },
         "network": {
            "tcp": {
               "active_opens": 119658,
               "passive_opens": 222271,
               "curr_estab": 326,
               "in_segs": 83642594,
               "out_segs": 127667397,
               "retrans_segs": 820851,
               "estab_resets": 335,
               "attempt_fails": 83841,
               "in_errs": 606,
               "out_rsts": 3515508
            }
         },
         "fs": {
            "timestamp": 1428951746860,
            "total": {},
            "data": []
         },
         "transport": {
            "server_open": 156,
            "rx_count": 11910,
            "rx_size_in_bytes": 9365990,
            "tx_count": 11921,
            "tx_size_in_bytes": 3392065
         },
         "http": {
            "current_open": 1,
            "total_opened": 616
         },
         "breakers": {
            "fielddata": {
               "limit_size_in_bytes": 2459172864,
               "limit_size": "2.2gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1.03,
               "tripped": 0
            },
            "request": {
               "limit_size_in_bytes": 1639448576,
               "limit_size": "1.5gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            },
            "parent": {
               "limit_size_in_bytes": 2869035008,
               "limit_size": "2.6gb",
               "estimated_size_in_bytes": 0,
               "estimated_size": "0b",
               "overhead": 1,
               "tripped": 0
            }
         }
      },
      "mVrbi9qtRcm5nyRPYi1Nkw": {
         "timestamp": 1428951789871,
         "name": "tqcilproesda02.cilos.sat.gob.mx",
         "transport_address": "inet[/10.57.140.78:9300]",
         "host": "tqcilproesda02",
         "ip": [
            "inet[/10.57.140.78:9300]",
            "NONE"
         ],
         "attributes": {
            "master": "false"
         },
         "indices": {
            "docs": {
               "count": 3228129,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 637435725,
               "throttle_time_in_millis": 36413
            },
            "indexing": {
               "index_total": 3624180,
               "index_time_in_millis": 511012,
               "index_current": 5311,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 2,
               "query_time_in_millis": 64,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 2,
               "current_docs": 5208421,
               "current_size_in_bytes": 1027976776,
               "total": 109,
               "total_time_in_millis": 155314,
               "total_docs": 7193491,
               "total_size_in_bytes": 1521304896
            },
            "refresh": {
               "total": 1023,
               "total_time_in_millis": 51176
            },
            "flush": {
               "total": 8,
               "total_time_in_millis": 2799
            },
            "warmer": {
               "current": 0,
               "total": 1922,
               "total_time_in_millis": 173
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 0
            },
            "segments": {
               "count": 30,
               "memory_in_bytes": 2220980,
               "index_writer_memory_in_bytes": 0,
               "index_writer_max_memory_in_bytes": 1024000,
               "version_map_memory_in_bytes": 0,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 0,
               "size_in_bytes": 17
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            },
            "recovery": {
               "current_as_source": 0,
               "current_as_target": 0,
               "throttle_time_in_millis": 29698
            }
         },
         "os": {
            "timestamp": 1428951789871,
            "uptime_in_millis": 2935581,
            "load_average": [
               0,
               0.01,
               0.05
            ],
            "cpu": {
               "sys": 0,
               "user": 0,
               "idle": 99,
               "usage": 0,
               "stolen": 0
            },
            "mem": {
               "free_in_bytes": 13554765824,
               "used_in_bytes": 3184865280,
               "free_percent": 90,
               "used_percent": 9,
               "actual_free_in_bytes": 15216439296,
               "actual_used_in_bytes": 1523191808
            },
            "swap": {
               "used_in_bytes": 0,
               "free_in_bytes": 12889092096
            }
         },
         "process": {
            "timestamp": 1428951789871,
            "open_file_descriptors": 469,
            "cpu": {
               "percent": 0,
               "sys_in_millis": 22750,
               "user_in_millis": 829970,
               "total_in_millis": 852720
            },
            "mem": {
               "resident_in_bytes": 897847296,
               "share_in_bytes": 15761408,
               "total_virtual_
</comment><comment author="clintongormley" created="2015-04-14T12:59:58Z" id="92814585">The output that you show is typical output when sigar is not loaded.

Please can you add the `logger.monitor: TRACE`, start a node, run `GET /_nodes/stats/fs` and upload the logs here.
</comment><comment author="iemem15" created="2015-04-14T20:41:20Z" id="93053047">Here it is
THis is the log https://gist.github.com/iemem15/d60fcee730cc31b19264
This is the stats output
https://gist.github.com/iemem15/2344b56f34a3e522f4ef

My logging.yml
https://gist.github.com/iemem15/6fb5a301e5d5190c42d8

Thanks,

2015-04-14 8:00 GMT-05:00 Clinton Gormley notifications@github.com:

&gt; The output that you show is typical output when sigar is not loaded.
&gt; 
&gt; Please can you add the logger.monitor: TRACE, start a node, run GET
&gt; /_nodes/stats/fs and upload the logs here.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10550#issuecomment-92814585
&gt; .

## 

Israel El&#237;as Morales
</comment><comment author="clintongormley" created="2015-04-23T10:38:01Z" id="95529491">Hi @iemem15 

What operating system and version of Java are you using?
</comment><comment author="iemem15" created="2015-04-23T14:32:30Z" id="95606348">JAva
java version "1.8.0_25"
Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)

SLES 12 64 bits

2015-04-23 5:38 GMT-05:00 Clinton Gormley notifications@github.com:

&gt; Hi @iemem15 https://github.com/iemem15
&gt; 
&gt; What operating system and version of Java are you using?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10550#issuecomment-95529491
&gt; .

## 

Israel El&#237;as Morales
</comment><comment author="clintongormley" created="2015-04-29T08:18:47Z" id="97348659">Hi @iemem15 

Could you try running the sigar shell from the ES directory:

```
java -jar lib/sigar/sigar-1.6.4.jar   
```

Then run the commands: `df`, `iostat`, and `sysinfo`, and paste the output here.

Also try running the above as root and see if it makes any difference.
</comment><comment author="clintongormley" created="2015-04-29T10:04:16Z" id="97376428">@iemem15 also, what kernel version do you have?  You can find out with `uname - r`.  

Apparently version 3+ is not yet supported by Sigar - see https://support.hyperic.com/display/SIGAR/Home
</comment><comment author="iemem15" created="2015-04-29T16:31:18Z" id="97491661">tqcilproescl02:/usr/share/elasticsearch/lib/sigar # java -jar
sigar-1.6.4.jar
sigar&gt; df

Filesystem      Size Used Avail Use% Mounted on      Type

rootfs           56G 7.3G   49G  14% /               rootfs/none

sysfs             0    0     0     - /sys            sysfs/none

proc              0    0     0     - /proc           proc/none

devtmpfs        3.8G   0   3.8G    - /dev            devtmpfs/none

securityfs        0    0     0     - /sys/kernel/security securityfs/none

tmpfs           3.8G   0   3.8G    - /dev/shm        tmpfs/none

devpts            0    0     0     - /dev/pts        devpts/none

tmpfs           3.8G 9.7M  3.8G   1% /run            tmpfs/none

tmpfs           3.8G   0   3.8G    - /sys/fs/cgroup  tmpfs/none

cgroup            0    0     0     - /sys/fs/cgroup/systemd cgroup/none

pstore            0    0     0     - /sys/fs/pstore  pstore/none
cgroup            0    0     0     - /sys/fs/cgroup/cpuset cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/cpu,cpuacct cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/memory cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/devices cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/freezer cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/blkio cgroup/none
cgroup            0    0     0     - /sys/fs/cgroup/perf_event cgroup/none
--More-- (Page 1 of 3)
cgroup            0    0     0     - /sys/fs/cgroup/hugetlb cgroup/none
/dev/mapper/system-root  56G 7.3G   49G  14% /               btrfs/none
systemd-1         0    0     0     - /proc/sys/fs/binfmt_misc autofs/none
mqueue            0    0     0     - /dev/mqueue     mqueue/none
debugfs           0    0     0     - /sys/kernel/debug debugfs/none
hugetlbfs         0    0     0     - /dev/hugepages  hugetlbfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /.snapshots     btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/tmp        btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/spool      btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/opt        btrfs/none
/dev/mapper/cilos--search--core--vg-cilos--search  50G  17M   48G   1%
/cilos          btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /usr/local      btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /tmp            btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/log        btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /opt            btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/lib/pgsql  btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/lib/named  btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /var/lib/mailman btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /home           btrfs/none
--More-- (Page 2 of 3)
/dev/mapper/system-root  56G 7.3G   49G  14% /var/crash      btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /srv            btrfs/none
/dev/sda1       485M  62M  394M  14% /boot           ext4/local
/dev/mapper/system-root  56G 7.3G   49G  14% /boot/grub2/x86_64-efi
btrfs/none
/dev/mapper/system-root  56G 7.3G   49G  14% /boot/grub2/i386-pc btrfs/none
sigar&gt; iostat
Filesystem      Mounted on           Reads     Writes R-bytes W-bytes Queue
Svctm
/dev/sda1       /boot                  320         10     18M     22K  0.00
 0.00
sigar&gt; sysinfo
Sigar version.......java=1.6.4.129, native=1.6.4.129
Build date..........java=04/28/2010 04:26 PM, native=04/28/2010 04:26 PM
SCM rev.............java=4b67f57, native=4b67f57
Archlib.............libsigar-amd64-linux.so
Current fqdn........10.57.140.75
Hostname............tqcilproescl02
Current user........root

OS description......SuSE 12
OS name.............Linux
OS arch.............x86_64
OS machine..........x86_64
OS version..........3.12.28-4-default
OS patch level......unknown
OS vendor...........SuSE
OS vendor version...12
OS code name........
OS data model.......64
OS cpu endian.......little
Java vm version.....25.25-b02
Java vm vendor......Oracle Corporation
Java home.........../usr/java/jdk1.8.0_25/jre
  11:30 AM  up 7 days, 15:58, load average: 0.06, 0.03, 0.05

File Systems.........[/, /sys, /proc, /dev, /sys/kernel/security, /dev/shm,
/dev/pts, /run, /sys/fs/cgroup, /sys/fs/cgroup/systemd, /sys/fs/pstore,
/sys/fs/cgroup/cpuset, /sys/fs/cgroup/cpu,cpuacct, /sys/fs/cgroup/memory,
/sys/fs/cgroup/devices, /sys/fs/cgroup/freezer, /sys/fs/cgroup/blkio,
/sys/fs/cgroup/perf_event, /sys/fs/cgroup/hugetlb, /,
/proc/sys/fs/binfmt_misc, /dev/mqueue, /sys/kernel/debug, /dev/hugepages,
/.snapshots, /var/tmp, /var/spool, /var/opt, /cilos, /usr/local, /tmp,
/var/log, /opt, /var/lib/pgsql, /var/lib/named, /var/lib/mailman, /home,
/var/crash, /srv, /boot, /boot/grub2/x86_64-efi, /boot/grub2/i386-pc,
/proc/sys/fs/binfmt_misc]

Network Interfaces...[lo, eth0, eth1]

System resource limits:
sigar&gt;

2015-04-29 3:19 GMT-05:00 Clinton Gormley notifications@github.com:

&gt; Hi @iemem15 https://github.com/iemem15
&gt; 
&gt; Could you try running the sigar shell from the ES directory:
&gt; 
&gt; java -jar lib/sigar/sigar-1.6.4.jar
&gt; 
&gt; Then run the commands: df, iostat, and sysinfo, and paste the output here.
&gt; 
&gt; Also try running the above as root and see if it makes any difference.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10550#issuecomment-97348659
&gt; .

## 

Israel El&#237;as Morales
</comment><comment author="clintongormley" created="2015-04-29T17:43:38Z" id="97515677">OK - sigar is only reporting iostats for /boot, not for the partition where your data lives.  This is because kernel v3 isn't supported yet.  We won't be able to do anything until sigar is updated.

thanks for the info, closing
</comment><comment author="iemem15" created="2015-04-29T18:48:22Z" id="97537764">Thanks

2015-04-29 12:44 GMT-05:00 Clinton Gormley notifications@github.com:

&gt; OK - sigar is only reporting iostats for /boot, not for the partition
&gt; where your data lives. This is because kernel v3 isn't supported yet. We
&gt; won't be able to do anything until sigar is updated.
&gt; 
&gt; thanks for the info, closing
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10550#issuecomment-97515677
&gt; .

## 

Israel El&#237;as Morales
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>extended_bounds does not work on date histogram when index name is missing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10549</link><project id="" key="" /><description>When using date histogram and extended_bounds feature, it doesn't prepend buckets with 0 count if queried without the index name. This doesn't work but still returns values that exist in db. For example, if we have 4 documents dated 2015-04-06, 2015-04-06, 2015-04-08, 2015-04-08

it would return only 4(2 with same date and 2 with zero counts) buckets instead of 4 + (2 months) of empty buckets.

```
GET /_search
{
  "query" : {
    "bool" : {
      "must" : [ {
        "match" : {
          "url" : {
            "query" : "someurl.com"
          }
        }
      }]
    }
  },
  "aggregations" : {
    "pageview_count" : {
      "date_histogram" : {
        "field" : "viewDate",
        "interval" : "1d",
        "min_doc_count" : 0,
        "format" : "yyyy-MM-dd",
        "extended_bounds" : {
          "min" : "2015-02-11",
          "max" : "2015-04-11"
        }
      }
    }
  }
}
```

this works:

```
GET /myindex/_search
{
  "query" : {
    "bool" : {
      "must" : [ {
        "match" : {
          "url" : {
            "query" : "someurl.com"
          }
        }
      }]
    }
  },
  "aggregations" : {
    "pageview_count" : {
      "date_histogram" : {
        "field" : "viewDate",
        "interval" : "1d",
        "min_doc_count" : 0,
        "format" : "yyyy-MM-dd",
        "extended_bounds" : {
          "min" : "2015-02-11",
          "max" : "2015-04-11"
        }
      }
    }
  }
}
```
</description><key id="67822371">10549</key><summary>extended_bounds does not work on date histogram when index name is missing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mason</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-04-11T19:14:59Z</created><updated>2016-01-17T17:10:55Z</updated><resolved>2016-01-17T17:10:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T17:10:55Z" id="172353998">This appears to have been fixed in 2.0 or before
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need a API to get exists script languages list of ES cluster.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10548</link><project id="" key="" /><description>https://github.com/elastic/kibana/pull/3558

Then we can select the language from the list in Kibana settings.
</description><key id="67778817">10548</key><summary>Need a API to get exists script languages list of ES cluster.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chenryn</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-04-11T12:09:54Z</created><updated>2016-05-13T17:51:22Z</updated><resolved>2016-05-13T17:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2016-03-14T18:59:07Z" id="196472776">@clintongormley with painless coming about soon it would be really helpful to get this.
</comment><comment author="clintongormley" created="2016-03-15T17:03:28Z" id="196924208">Once https://github.com/elastic/elasticsearch/issues/17114 is in, running the following command will provide the necessary info:

```
GET _cluster/settings?include_defaults&amp;filter_path=**.script.engine.*.inline
```
</comment><comment author="clintongormley" created="2016-05-13T17:51:22Z" id="219113504">Closed by #17114
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Data not being indexed after 2million</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10547</link><project id="" key="" /><description>We're having a problem with our system, new data aren't being index.

Anyone?
</description><key id="67749285">10547</key><summary>Data not being indexed after 2million</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jcricaro</reporter><labels /><created>2015-04-11T07:28:12Z</created><updated>2015-04-12T04:52:19Z</updated><resolved>2015-04-12T04:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mason" created="2015-04-11T19:18:42Z" id="91908728">after 2 million what? documents? megabytes?
</comment><comment author="markwalkom" created="2015-04-12T04:52:18Z" id="91992656">Please use the mailing list for these sorts of problems - https://groups.google.com/forum/#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gauss decay function gives unexpected results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10546</link><project id="" key="" /><description>Hi there,

We observed strange behavior when using the gauss decay function ([function score query](http://www.elastic.co/guide/en/elasticsearch/reference/1.x/query-dsl-function-score-query.html)) on a `date`field. It gives unexpected results (that no more look gaussian) when the date difference with `now` becomes too large, resulting into boosting docs that should not be. Here is a [graph](https://gist.github.com/ncolomer/99c2ac85d5a04d7d1fdf#file-results-20-days-png) of the issue.

You can find the detailed procedure, a shell script to reproduce and some results/observations in [this gist](https://gist.github.com/ncolomer/99c2ac85d5a04d7d1fdf).

Tests were run on elasticsearch `1.1.0` and `1.5.1`
</description><key id="67746373">10546</key><summary>Gauss decay function gives unexpected results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">ncolomer</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-04-11T07:02:28Z</created><updated>2015-05-29T11:46:06Z</updated><resolved>2015-05-29T11:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T10:44:21Z" id="92304834">@brwe could you take a look at this please
</comment><comment author="marcelkornblum" created="2015-05-15T15:00:37Z" id="102423446">I feel as though it's related to the date format - I'm using date_optional_time and saw this behaviour. My assumption is that the field value is being translated as milliseconds, while the scale and offset are seen as integers (i.e. date to millisecond discrepancy)...
</comment><comment author="brwe" created="2015-05-22T12:11:41Z" id="104645585">Thanks a lot for the script to reproduce and sorry for the late reply.
It seems to me that when you convert the result to output you neglect the exponents "E-...". A result score from your script like

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1.0325352E-4,
    "hits": [
      {
        "_index": "test",
        "_type": "test",
        "_id": "1",
        "_score": 1.0325352E-4
      }
    ]
  }
}
```

then becomes 1.0325352 while it should be 0.00010325352. Can you confirm?
</comment><comment author="ncolomer" created="2015-05-29T11:46:06Z" id="106781117">@brwe you're right! Sorry for the noise, it was a stupid mistake on my part. I corrected the related gist.
Thanks for your feedback!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Add prediction mode to moving_avg agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10545</link><project id="" key="" /><description>Small extension which adds a `predict` parameter to the `moving_avg` aggregation.  This is mostly useful for the double-exponential model (aka Holt-Winters), because it can provide semi-decent forecasts into the future based on the trend component.

![screen shot 2015-04-08 at 8 37 48 pm](https://cloud.githubusercontent.com/assets/1224228/7099252/563e70b4-dfbc-11e4-8f07-0990e6e24191.png)

The other moving average are a little less clever.  They basically emit the moving average, then add that to the window and emit the next avg.  Since these are just simple weightings based on time, the "predictions" converge on the mean of the series at the last data point and give a straight line:

_(Note: Kibana swapped the colors on the graph from the previous one)_
![screen shot 2015-04-08 at 8 43 06 pm](https://cloud.githubusercontent.com/assets/1224228/7099256/8842176e-dfbc-11e4-9046-4aef9fef297e.png)

This also includes a few minor fixups, such as a builder that was still named `smooth`, validating window size, refactor DoubleExpModel to use primitives instead of a List, and a buuuunch more tests.
### Todo
- Investigate adding error margins to the prediction.  I'm not positive, but I think there are ways to estimate how quickly the error grows
- Support `predict` based on percentage of buckets (e.g. `20%`) ?  I don't think this is a useful or necessary, but others may disagree :)
- More tests around prediction, gaps, etc
- Currently extrapolating the interval size...any way to extract this from the histo itself?
</description><key id="67708460">10545</key><summary>Aggregations: Add prediction mode to moving_avg agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>WIP</label></labels><created>2015-04-11T00:10:48Z</created><updated>2015-04-22T23:05:33Z</updated><resolved>2015-04-22T23:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Log sending translog operation batches to nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10544</link><project id="" key="" /><description>I'm not sure if this is too verbose, because it will log every 1000 translog operations or 512kb.

Thoughts?
</description><key id="67697645">10544</key><summary>Log sending translog operation batches to nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T22:40:09Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-13T21:06:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-11T13:23:33Z" id="91844242">LGTM, it's trace so I don't think we need to worry about volume. Why not push it to 1.6 as well?
</comment><comment author="kimchy" created="2015-04-11T14:20:42Z" id="91853083">should we also log the leftover that is sent at the end?
</comment><comment author="dakrone" created="2015-04-13T20:56:20Z" id="92495514">&gt; should we also log the leftover that is sent at the end?

Yep, I will add this and push since Boaz gave the LGTM (unless someone objects)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log Translog Recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10543</link><project id="" key="" /><description>In addition to the awesome changes in #10042, we should also put similar metrics into the logs at `DEBUG` or `TRACE` levels. Something like

```
[node1] replayed 50 translog ops. completed 150 / 13245 ops
```
</description><key id="67697609">10543</key><summary>Log Translog Recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>:Translog</label></labels><created>2015-04-10T22:39:53Z</created><updated>2015-04-10T22:44:59Z</updated><resolved>2015-04-10T22:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-04-10T22:44:59Z" id="91710772">Closed in favor of #10544
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/recovery should report translog replay progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10542</link><project id="" key="" /><description>`_cat/recovery` and `_recovery` should record the translog progress (percentage or otherwise) in terms of how much phase 3 of recovery has been processed.

For large shards, it can be painful watch automatic relocation's like:

```
my-index-2015-04-10 47    3053258  replica    translog NODE-SOURCE NODE-TARGET n/a        n/a      352   100.0%        20911663130 100.0%
```

The translog is ~20 GB in this example, so it's very hard to guess how long I will have to wait until it finishes replaying it on the target node.
</description><key id="67693520">10542</key><summary>_cat/recovery should report translog replay progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:CAT API</label><label>:Recovery</label><label>enhancement</label></labels><created>2015-04-10T22:08:59Z</created><updated>2015-04-10T22:35:29Z</updated><resolved>2015-04-10T22:35:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-04-10T22:17:38Z" id="91706894">May be related to https://github.com/elastic/elasticsearch/pull/10042
</comment><comment author="pickypg" created="2015-04-10T22:35:29Z" id="91709514">Yep! Didn't see that this made it into 1.5. The above was running 1.4.4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added an optimize_bbox option for geo_polygon filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10541</link><project id="" key="" /><description>This setting is analogous to the one for geo_distance filters, and defaults to "memory". Note
that the optimization will need to be updated when addressing #5968.

Closes #10356.
</description><key id="67690173">10541</key><summary>Added an optimize_bbox option for geo_polygon filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-04-10T21:44:42Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-07-27T11:31:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-12T05:38:32Z" id="91995070">@jtibshirani I left a couple comments.  I think we need some more tests, specifically we need a way to check the bounding box is actually being applied (the current test seems to only check the polygon filter still works, since the same results are expected even when bbox is none).
</comment><comment author="jtibshirani" created="2015-04-15T05:23:13Z" id="93194389">Thanks for the feedback, I'll add a couple more tests. I didn't see a mock testing framework -- were you thinking of something involving mock objects? Or just testing the filter parsing code works as expected?
</comment><comment author="nknize" created="2015-04-15T16:57:59Z" id="93486534">After investigating the behavior of this parameter I'm not sure this is going to be reliable.  (A few integration tests showed the pointInPolygon matchDoc occurring before bbox) 

@jpountz can give more detail, but at the moment the AndDocIdSet class uses heuristics to determine "slow" iterators and have them lead by the "fast" ones.  This is looking to be replaced by Lucene's two-phase iteration.  I don't know all of the details there but it looks like in some cases this optimization may actually be the opposite.  We might want to hold off and revisit this after two-phase iteration is available?
</comment><comment author="jtibshirani" created="2015-04-16T00:29:38Z" id="93605933">@nknize I'm having trouble reproducing the filter reordering -- would you mind pointing me to your test?

In any case, it doesn't seem hard to ensure the filters are executed in order (by using something like BitsFilteredDocIdSet instead of AndDocIdSet).
</comment><comment author="jtibshirani" created="2015-07-25T00:50:00Z" id="124780089">Really sorry it took so long to make changes -- I've rebased and responded to comments in the new PR #12457.
</comment><comment author="clintongormley" created="2015-07-27T11:31:51Z" id="125173346">Closing in favour of #12457
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add upgrade_only_ancient_segments option to upgrade API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10540</link><project id="" key="" /><description>PR is based on 1.x.

The new option defaults to false, because it's still important to upgrade old-but-same-Lucene-major-version to get improvements.

If it's set to true, only segments whose major version is older than current major version will be upgraded.

I also fixed the REST upgrade GET API to separately return bytes of ancient segments.

Does anyone know why UpgradeTest doesn't allow testing upgrading ancient segments...?  ElasticsearchBackwardsCompatibilityIntegrationTest.backwardsCompatibilityPath gets angry because e.g. 0.90.X is &lt; Version.V_1_6_0.minimumCompatibilityVersion() ... but I thought users are allowed to do such an upgrade?

Closes #10213
</description><key id="67674266">10540</key><summary>Add upgrade_only_ancient_segments option to upgrade API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Upgrade API</label><label>blocker</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T20:18:48Z</created><updated>2015-05-30T10:51:01Z</updated><resolved>2015-04-16T09:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-14T14:43:29Z" id="92885514">I don't have any complains with this though if @rjernst is happy this LGTM
</comment><comment author="mikemccand" created="2015-04-15T10:07:57Z" id="93301620">OK I pushed more changes.  I think this is ready.

I added a new test case with a static back compat index containing a mix of ancient (Lucene 3.x) and merely old (Lucene 4.x) segments, and confirmed if I upgrade with only_ancient then the "merely old" segments remain.

However, I saw this spooky-yet-apparently-harmless exception on upgrade:

```
 [2015-04-14 16:34:54,964][TRACE][index.gateway.local      ] [node_t0] [index-0.20.6-and-0.90.6][0] ignoring truncation exception, the translog is either empty or half-written
  1&gt; org.elasticsearch.index.translog.TruncatedTranslogException: translog header truncated
  1&gt; at org.elasticsearch.index.translog.ChecksummedTranslogStream.openInput(ChecksummedTranslogStream.java:120)
  1&gt; at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:246)
  1&gt; at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:112)
  1&gt; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt; at java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.io.EOFException
  1&gt; at org.apache.lucene.store.InputStreamDataInput.readByte(InputStreamDataInput.java:37)
  1&gt; at org.apache.lucene.store.DataInput.readInt(DataInput.java:98)
  1&gt; at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:134)
  1&gt; at org.elasticsearch.index.translog.ChecksummedTranslogStream.openInput(ChecksummedTranslogStream.java:116)
  1&gt; ... 5 more
```

It's caused by a 0-byte translog file that 0.20.6 left behind even after I flushed the index.  It doesn't cause the test to fail (ES really does just ignore it) but it still seems bad... though maybe it's just a known issue with 0.20.6?  The 0 byte translog that 0.20.6 wrote somehow survived after upgrade to 0.90.6, adding new docs and flushing.
</comment><comment author="rjernst" created="2015-04-15T18:29:15Z" id="93522827">LGTM
</comment><comment author="mikemccand" created="2015-04-15T18:29:58Z" id="93523136">&gt; maybe move this to a simple helper like shutdown_node(node) since it is exactly the same as above?

Good idea, I just pushed that.
</comment><comment author="mikemccand" created="2015-04-16T09:25:21Z" id="93689704">When I merged to master I dropped the static mixed ancient and "merely old" segments test because we cannot test that until we have a 2.0 release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct ShapeBuilder coordinate parser to ignore values in 3rd+ dimension</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10539</link><project id="" key="" /><description>ShapeBuilder's coordinate parser expected 2 double values for every coordinate array. If &gt; 2 doubles were provided the parser terminated parsing of the coordinate array. This resulted in an invalid Shape state leaving LineStrings, LinearRings, and Polygons with a single coordinate. An incorrect parse exception was thrown. This corrects the parser to ignore those values in the 3rd+ dimension, correctly parsing the rest of the coordinate array.  Unit tests have been updated to verify the fix.

closes #10510
</description><key id="67660182">10539</key><summary>Correct ShapeBuilder coordinate parser to ignore values in 3rd+ dimension</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.5</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T19:00:49Z</created><updated>2015-05-29T16:25:14Z</updated><resolved>2015-04-10T20:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-20T16:40:39Z" id="94504080">We should add this change to the breaking changes doc since it is a change of behaviour. IMO we should have an option in the mapping to decide whether to error on extra dimensions or not and should default to erroring so it is explicit that our geo implementaiton only actually supports 2 dimensions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide better visibility into cost of deeply nested aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10538</link><project id="" key="" /><description>As users start to leverage more complex and nested aggregations, they are more likely to run into performance problems due to the combinatorial explosion of nodes in the aggregation tree.

I'm unsure what exactly this would entail.  Perhaps the number of "operations" per layer in the hierarchy, or the number of buckets generated per layer (inclusive of lower layers).

It would be great to provide robust profiling at some point &lt;sub&gt;(*_cough_ \* #6699) &lt;/sub&gt;, but I think even simple metrics for now would be very helpful.  For example, I don't think users realize how many operations and buckets are involved in a 5-deep aggregation with a branching factor of 50-per-node.  Especially in situations where an entire "report" is being generated and pruning methods like `breadth_first` don't work.

I'm imagining these statistics either as a top-level element in the agg response, or as a per-layer response element.  It would also be ideal if these were enabled by default, which shouldn't be too difficult if they are just lightweight counters and nothing invasive like timings.
</description><key id="67655124">10538</key><summary>Provide better visibility into cost of deeply nested aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label></labels><created>2015-04-10T18:30:47Z</created><updated>2016-07-05T13:23:05Z</updated><resolved>2016-07-05T13:23:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T10:34:05Z" id="92303147">+1
</comment><comment author="colings86" created="2015-04-13T10:42:02Z" id="92304291">+1
</comment><comment author="smflorentino" created="2015-07-21T17:39:48Z" id="123412055">I've actually been tasked to to do this at work. I have counters for buckets generated as well as buckets pruned at each layer in the aggregation hierarchy, as well as the time taken for the combine phase of aggregation at each data node. Not sure if the time will be useful yet - I'm pretty new to ES and I'm working to get this tested on some production data this week. Still trying to tackle the operations count, too. Hoping to post results and code soon. 
</comment><comment author="mpereira" created="2015-08-27T22:18:41Z" id="135571888">+1
</comment><comment author="clintongormley" created="2016-01-17T17:04:44Z" id="172353652">@polyfractal @colings86 now that the profile API has been added for queries, we should look at extending it for aggregations.
</comment><comment author="polyfractal" created="2016-07-05T13:23:05Z" id="230476509">Closed by #18414

![emot-toot](https://cloud.githubusercontent.com/assets/1224228/16586086/1122b2ee-4292-11e6-9d38-8c64cd1ccc90.gif)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default suggestion mode to `popular`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10537</link><project id="" key="" /><description>The default suggest mode for Term and Phrase suggester is `missing`.  This can be problematic since typos in the index will prevent any suggestions will come back.  It's fine if you want that behavior, but as a default, it is a little user-unfriendly.  

`popular` would probably make a better default, since it will (usually) provide good suggestions despite the presence of typos in the index. 

Missing can also introduce strange artifacts due to shard-local information.  The nature of "suggest only if missing from the index" makes the shard-local abnormalities more apparent than the relative weighting of `popular`.
</description><key id="67652636">10537</key><summary>Change default suggestion mode to `popular`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Suggesters</label><label>discuss</label></labels><created>2015-04-10T18:18:23Z</created><updated>2017-07-06T20:28:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T10:33:22Z" id="92303036">+1
</comment><comment author="edudar" created="2017-07-06T20:28:08Z" id="313510017">On the other hand, `popular` brings up suggestions for correctly spelled terms simply because the other word is more 'popular'. In my case, it was `kia -&gt; ia`, `tesla -&gt; tells`, `oath -&gt; with`, etc. So I have to make various moves with multiple suggeters to make things OK, cause we have typos in the index (UGC) and `missing` does not work properly...</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve exception message when indexing a boolean value into a numeric field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10536</link><project id="" key="" /><description>Makes the exception consistent with other "incorrect data type" exceptions, instead of a cryptic internal exception due to trying to derive a long from VALUE_BOOLEAN.

New exception will look like this:

```
PUT /tweets/
{
   "mappings": {
      "tweet": {
         "properties": {
            "key": {
               "type": "integer"
            }
         }
      }
   }
}

PUT /tweets/tweet/1
{ "key" : true }
```

```
{
   "error": "MapperParsingException[failed to parse [key]]; nested: NumberFormatException[For input boolean value: true]; ",
   "status": 400
}
```

Instead of the existing message:

```
{
  "error": "MapperParsingException[failed to parse [key]]; nested: JsonParseException[Current token (VALUE_TRUE) not numeric, can not use numeric value accessors\n at [Source: [B@464291b8; line: 1, column: 15]]; ",
  "status": 400
}
```

Fixes #10056
</description><key id="67650292">10536</key><summary>Improve exception message when indexing a boolean value into a numeric field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>won't fix</label></labels><created>2015-04-10T18:05:55Z</created><updated>2015-07-09T19:21:19Z</updated><resolved>2015-07-09T19:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T20:11:15Z" id="91670305">The `NumberFormatException` is nice, but it kind of stinks that now we miss the line and column number in the exception. Is there a way we can keep the line numbering but have the better exception message?
</comment><comment author="polyfractal" created="2015-04-13T13:29:16Z" id="92351522">I had a look, and I don't think it will be (easily) fixable.

`JsonParser` has the line and column number, but that is only available in `JsonXContentParser`, which subclasses `AbstractXContentParser`.  We'd have to add some abstract methods so that `AbstractXContentParser` can access either the parser or the line/column ... but that rather defeats the abstraction of separating JSON parsing from the common functionality.

Then again, it appears `JsonXContentParser` is the only class that actually extends `AbstractXContentParser`.  So maybe this isn't a useful abstraction anyway?

It feels like a bigger change than this small PR should entail, and would turn into a quite a large project :)  It appears a PR like https://github.com/elastic/elasticsearch/pull/7891 better fits the bill too (although it does look stalled...)
</comment><comment author="s1monw" created="2015-07-09T11:44:10Z" id="119924224">@polyfractal given that #7891 is merged can we close this one?
</comment><comment author="polyfractal" created="2015-07-09T12:23:49Z" id="119942211">Eh, I think the root problem still exists: the exception text is inconsistent for booleans compared to the rest of the "mismatched" data types.  For example, trying to index a `string` into a `integer` field gives:

``` json
PUT /tweets/tweet/1
{ "key" : "abc" }

{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse [key]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "failed to parse [key]",
      "caused_by": {
         "type": "number_format_exception",
         "reason": "For input string: \"abc\""
      }
   },
   "status": 400
}
```

But if you try a boolean, you get what looks more like a stack trace exception than something user-friendly:

``` json
PUT /tweets/tweet/1
{ "key" : true }

{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse [key]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "failed to parse [key]",
      "caused_by": {
         "type": "json_parse_exception",
         "reason": "Current token (VALUE_TRUE) not numeric, can not use numeric value accessors\n at [Source: org.elasticsearch.common.io.stream.InputStreamStreamInput@492a7ac0; line: 1, column: 15]"
      }
   },
   "status": 400
}
```

It's easier to see what the problem is due to structured exceptions, but still inconsistent and confusing (I think?)

Also, sigh, another PR that I submitted and then promptly forgot.  Sorry for not following up on it. 
</comment><comment author="s1monw" created="2015-07-09T18:10:18Z" id="120090064">yeah honesty the parsing exception is on a different level I think the exceptions are ok as they are?
</comment><comment author="polyfractal" created="2015-07-09T19:14:07Z" id="120113850">&#175;\_(&#12484;)_/&#175;  I don't feel strongly either way tbh, this was just a quick fixit friday to a user's bug report:  https://github.com/elastic/elasticsearch/issues/10056

It was confusing enough that someone reported it, but perhaps not worth the code bloat since it isn't technically a bug?  I'm fine closing this if you think so.
</comment><comment author="s1monw" created="2015-07-09T19:21:11Z" id="120115916">I closed the issue... will do so here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fully support IPv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10535</link><project id="" key="" /><description>Currently it's possible to configure ES with IPv6 in some places, however, we don't do tests or qualification for ipv6 for things like multicast (see: #4989).

We should determine the best way to support a full ipv6 environment. One way we could do this is the addition of a `net.use_ipv6: true` setting so that a user has to fully opt-in to ipv6, rather than a mix of both ipv4 and ipv6 that is commonly configured out of the box on machines today.
</description><key id="67629918">10535</key><summary>Fully support IPv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Network</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-04-10T16:20:00Z</created><updated>2015-08-21T06:44:25Z</updated><resolved>2015-08-21T06:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-08-21T06:44:24Z" id="133306399">closed by and #12942 and #12999
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>pom.xml updates to allow m2e integration to work correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10534</link><project id="" key="" /><description>This adds another plugin to the lifecycle mapping in the pom.xml which was missed in https://github.com/elastic/elasticsearch/pull/10524.

It also changes the order of the preferences in the org.eclipse.core.resources.prefs and org.eclipse.jdt.core.prefs files to be the order which eclipse writes these files. This is so that these files are not marked as changed when eclipse updates the maven settings on the project since Eclipse re-writes these files every time.

After this change developers should not have any errors when importing the project as an existing maven project in Eclipse (tested on a fresh install of Eclipse Luna JavaEE)
</description><key id="67615767">10534</key><summary>pom.xml updates to allow m2e integration to work correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T15:06:03Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-10T15:56:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-10T15:56:38Z" id="91599945">This fix has been done in https://github.com/elastic/elasticsearch/pull/10495 using the following commit https://github.com/elastic/elasticsearch/commit/97a9b4ec2fb6691437c86b694b77d56e81efcaa1

Closing this PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add merge conflicts to GeoShapeFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10533</link><project id="" key="" /><description>Prevents the user from changing strategies, tree, tree_level or precision. distance_error_pct changes are allowed as they do not compromise the integrity of the index. A separate issue #10514 is open for allowing users to change tree_level or precision.

closes #10513 
</description><key id="67601788">10533</key><summary>Add merge conflicts to GeoShapeFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T13:58:30Z</created><updated>2015-08-07T10:07:48Z</updated><resolved>2015-04-10T19:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-10T15:35:23Z" id="91594273">LGTM, just a couple minor suggestions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the `limit` filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10532</link><project id="" key="" /><description>This is really a Collector instead of a filter. This commit deprecates the
`limit` filter, makes it a no-op and recommends to use the `terminate_after`
parameter instead that we introduced in the meantime.
</description><key id="67599053">10532</key><summary>Deprecate the `limit` filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T13:45:06Z</created><updated>2015-08-07T10:07:47Z</updated><resolved>2015-04-10T15:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-10T13:48:10Z" id="91563631">For the record, I have two main motivations to remove this filter:
- it is not cacheable per segment
- Lucene 5.1 got more sophisticated query execution, and a filter alone cannot be used to limit the total number of hits
</comment><comment author="rmuir" created="2015-04-10T14:06:27Z" id="91569956">If the user already has terminate_after as an alternative, can we issue the deprecation in 1.x?
</comment><comment author="jpountz" created="2015-04-10T14:17:28Z" id="91572318">OK, I'll do that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace deprecated filters with equivalent queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10531</link><project id="" key="" /><description>In Lucene 5.1 lots of filters got deprecated in favour of equivalent queries.
Additionally, random-access to filters are now replaced with approximations on
scorers. This commit
- replaces the deprecated NumericRangeFilter, PrefixFilter, TermFilter and
  TermsFilter with NumericRangeQuery, PrefixQuery, TermQuery and TermsQuery,
  wrapped in a QueryWrapperFilter
- replaces XBooleanFilter, AndFilter and OrFilter with a BooleanQuery in a
  QueryWrapperFilter
- removes DocIdSets.isBroken: the new two-phase iteration API will now help
  execute slow filters efficiently
- replaces FilterCachingPolicy with QueryCachingPolicy

There are some TODO and AwaitsFix left, but I would like to defer fixing them
to another iteration.

Close #8960
</description><key id="67593308">10531</key><summary>Replace deprecated filters with equivalent queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T13:18:03Z</created><updated>2015-06-08T15:44:22Z</updated><resolved>2015-04-21T13:40:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-20T16:48:54Z" id="94505935">@rmuir I rebased in order to have the fixes from the latest lucene snapshot, which fixed the `extractTerms` issue. About your comment to use `BooleanQuery` instead of `FilteredQuery` all the time, it makes some tests fail, eg. we have a test that ensures that script filters do not run the script more times than the number of documents that match the query, so I'm thinking of delaying it to another PR which would add approximations to scripts too.
</comment><comment author="rmuir" created="2015-04-20T16:59:07Z" id="94508822">&gt; About your comment to use BooleanQuery instead of FilteredQuery all the time, it makes some tests fail, eg. we have a test that ensures that script filters do not run the script more times than the number of documents that match the query, so I'm thinking of delaying it to another PR which would add approximations to scripts too.

I agree, lets defer this. We need it to fix NoCacheFilter as well. IMO we should make a simple Slow base class that takes Bits and checks deleted docs first and provides matchall-approximation or whatever. We can do it in lucene and then fix subclasses here in ES as well.

+1 to move forward with what is here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: replace empty index block checks with global block checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10530</link><project id="" key="" /><description>While working on #9203 we noticed (see [comment](https://github.com/elastic/elasticsearch/pull/9203#discussion_r27642154)) that many `checkBlock()` methods were checking blocks on an empty index like this:

``` java
protected ClusterBlockException checkBlock(PutIndexTemplateRequest request, ClusterState state) {
        return state.blocks().indexBlockedException(ClusterBlockLevel.METADATA, "");
}
```

I think most of this kind of calls can be replaced by a global block check like:

``` java
return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA);
```
</description><key id="67586447">10530</key><summary>Internal: replace empty index block checks with global block checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-10T12:42:34Z</created><updated>2016-01-17T16:57:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-10T12:43:55Z" id="91543405">+1 we might have to dig if that is indeed a 1:1 replacement, but for sure the different checkBlock variants could be clarified a bit. I think if it's about index blocks they shouldn't accept empty indices ideally.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Add sigar binaries when running unittests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10529</link><project id="" key="" /><description>the sigar binaries are not available when running tests today. This
commit adds the path to the test run.
</description><key id="67585574">10529</key><summary>[BUILD] Add sigar binaries when running unittests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>test</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T12:39:03Z</created><updated>2015-08-07T10:07:47Z</updated><resolved>2015-04-10T12:40:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-10T12:39:51Z" id="91541289">LGTM, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better highlighting fragment support for fields with list/array values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10528</link><project id="" key="" /><description>If I have a document like {"prop":["a...", "b..", "c...", ...]} and perform highlighting with max_fragments=n it is entirely possible for all those fragments to come from just the first list entry. It would be helpful if there was some setting/option to set the max fragment size per list entry.

Even better would be if the json response contained some indication of which list entry each fragment came from.
</description><key id="67585225">10528</key><summary>Better highlighting fragment support for fields with list/array values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pulquero</reporter><labels /><created>2015-04-10T12:37:19Z</created><updated>2015-04-13T15:30:15Z</updated><resolved>2015-04-13T09:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-13T09:49:40Z" id="92295608">Hi @pulquero 

An array of terms in Elasticsearch is indexed just like an analyzed string field, eg the text `The quick brown fox`, would get indexed in the same way as an array `[the, quick, brown, fox]`.

What you're asking for wouldn't make sense in the context of analyzed strings, as far as I understand. Realistically, I don't think there is any generic setting that we can support here.  Sorry.
</comment><comment author="pulquero" created="2015-04-13T10:10:25Z" id="92299582">That is not strictly true, because if I set num_fragments to 0, I get a fragment array the same length as my string field array. e.g. if I use highlight tags the following happens:

[the, quick, brown, fox] -&gt; [&lt;B&gt;the&lt;/B&gt;, &lt;B&gt;quick&lt;/B&gt;, &lt;B&gt;brown&lt;/B&gt;, &lt;B&gt;fox&lt;/B&gt;].[the quick brown fox] -&gt; [&lt;B&gt;the&lt;/B&gt; &lt;B&gt;quick&lt;/B&gt; &lt;B&gt;brown&lt;/B&gt; &lt;B&gt;fox&lt;/B&gt;].

So my current work-around is to have num_fragments=0, then post-process each fragment array element by splitting it on the highlight tags, and restricting it to say the first 2 highlights. Essentially, I'm looking for some setting to do that.

```
 On Monday, 13 April 2015, 10:50, Clinton Gormley &lt;notifications@github.com&gt; wrote:
```

 Hi @pulquero An array of terms in Elasticsearch is indexed just like an analyzed string field, eg the text The quick brown fox, would get indexed in the same way as an array [the, quick, brown, fox].What you're asking for wouldn't make sense in the context of analyzed strings, as far as I understand. Realistically, I don't think there is any generic setting that we can support here. Sorry.&#8212;
Reply to this email directly or view it on GitHub.   
</comment><comment author="nik9000" created="2015-04-13T15:30:15Z" id="92402090">&gt; It would be helpful if there was some setting/option to set the max fragment size per list entry.

Do you mean to say "it would be helpful if there was some option to set the maximum number of fragments per list entry"? As in "I only want one fragment per list entry but I want the top five fragments."  That wouldn't be inconceivable to implement using the [experimental highlighter](https://github.com/wikimedia/search-highlighter) but against the three built into Elasticsearch it'd be harder.  Partly it'd just be harder because you'd have to do it three times.  And partly harder because all but the plain highlighter don't load the field value in the order that you'd need it.

Upshot: if you want it you'll have to implement it yourself.  If you implement it against the experimental highlighter I linked above I'll code review it.

I figure its worth me saying this: I'm not part of the Elasticearch team. I just maintain the experimental highlighter. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log only a summary line of filesystem detail for all path.data on node startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10527</link><project id="" key="" /><description>This is a follow-on from #10502: I fixed the logging in NodeEnvironment so that if DEBUG is enabled you get (verbose) one line per path.data, e.g.:

```
  [2015-04-10 07:15:16,342][DEBUG][env                      ] [node_t2] node data locations details:
   -&gt; /l/es.fsstats/target/J0/./tests-20150410181516-239/0000 has-space/multi-path1/TEST-haswell-CHILD_VM=[0]-CLUSTER_SEED=[-5088446507314103482]-HASH=[B3A9F4276F5BF]/nodes/0, free_space [263.9gb], usable_space [259.4gb], total_space [465gb], spins? [no], mount [/ (/dev/mapper/haswell--vg-root)], type [btrfs]
   -&gt; /l/es.fsstats/target/J0/./tests-20150410181516-239/0000 has-space/multi-path2/TEST-haswell-CHILD_VM=[0]-CLUSTER_SEED=[-5088446507314103482]-HASH=[B3A9F4276F5BF]/nodes/0, free_space [263.9gb], usable_space [259.4gb], total_space [465gb], spins? [no], mount [/ (/dev/mapper/haswell--vg-root)], type [btrfs]
```

(I also removed the extra newline)

else if INFO is enabled (ES's default) you get a single summary line:

```
  1&gt; [2015-04-10 07:08:48,560][INFO ][env                      ] [node_t2] using [2] data paths, net usable_space [259.4gb], net total_space [465gb], types [btrfs], spins? [no]
```

I also added filesystem type and spins to FsStats.Info, and fixed JmxFsProbe to set them and to also set mount (I use FileStore.toString() for this). 

For SigarFsType, I used its FileSystem.getSysTypeName to get the filesystem type (not certain this is correct!), and also call Lucene's IOUtils.spins.

On backport to 1.x I'll fix the readFrom/writeTo to be back-compat if version &lt; 1.6.
</description><key id="67579479">10527</key><summary>Log only a summary line of filesystem detail for all path.data on node startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T11:59:53Z</created><updated>2015-06-06T19:10:51Z</updated><resolved>2015-04-14T08:57:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-10T12:34:23Z" id="91538515">&gt; I also added filesystem type and spins to FsStats.Info, and fixed JmxFsProbe to set them and to also set mount (I use FileStore.toString() for this).

when does this get called? These operations should not be assumed to be fast.
</comment><comment author="mikemccand" created="2015-04-10T12:42:20Z" id="91542481">&gt; when does this get called? These operations should not be assumed to be fast.

Hmm good point: they are called once per second (by default).

Maybe ... they should not be in FsProbe?  Maybe it should be in FsService.stats, so it's only done when stats are requested?  Or maybe we call this once and cache (in NodeEnv)?
</comment><comment author="rmuir" created="2015-04-10T12:45:54Z" id="91544442">IMO the code in question (IOUtils.spins) is too scary to call once per second. Today its only called once per writer on the first merge. It should be cached.
</comment><comment author="mikemccand" created="2015-04-10T17:20:01Z" id="91626827">I changed NodeEnvironment to cache the FileStore and IOUtils.spins result, in a new static NodePath class.

I left the separate Lock[] as a private parallel array: I didn't want callers to have access to the lock when they get the NodePath[].

Note that this made NodeEnv.nodeDataPaths and .indexPaths methods a bit more costly since they now build a new array on each call, but I reviewed places that call it and I don't think the added cost will matter.  If that's a problem I could pre-compute and cache that too.

Also, I noticed dead code in NodeEnv (findAllShardIds) and removed it.
</comment><comment author="mikemccand" created="2015-04-13T16:25:19Z" id="92416725">@rmuir any feedback here?
</comment><comment author="rmuir" created="2015-04-13T21:07:38Z" id="92499118">i added some minor comments. otherwise, looks good here, thanks for caching this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix updating templates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10526</link><project id="" key="" /><description>Closes #10397 

When putting new templates to an index they are added to the cache
of compiled templates as a side effect of the validate method. When
updating templates they are also validated but the scripts that are
already in the cache never get updated.

@javanna Assigning to you for review as you seem to have written most of the code this PR touches, feel free to re-assign.
</description><key id="67539338">10526</key><summary>Fix updating templates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T08:19:25Z</created><updated>2015-06-08T00:24:17Z</updated><resolved>2015-04-22T09:15:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-10T08:58:09Z" id="91483631">Hi @MaineC I don't think the problem stems from the validate method. `validate` properly caches using the script itself. The problem is that when we actually retrieve the script from the index as part of the search request, we actually look for and cache the script based on its id. That means it's never going to find the already cached version. Also, once a script is cached based on its id it is never going to be updated in the cache.

I would change the caching mechanism to use the retrieved script as cache key in case the script is indexed, so it will match with the one cached as part of `validate` and it will get cached again once it changes. makes sense?
</comment><comment author="javanna" created="2015-04-10T09:08:28Z" id="91488216">I did some research, this is a regression introduced with #10033 . `1.5.0` and `1.5.1` are affected, meaning that once you executed an indexed script for the first time, that first version is going to stick around in the internal cache although you update the script.  We better add some proper test coverage for this :) 
</comment><comment author="MaineC" created="2015-04-13T07:42:14Z" id="92254082">@javanna wrt. to your proposal - makes sense, changed the PR accordingly.

I also re-named the "script" parameter in compileInternal to "scriptOrId" to better reflect it's real content.

One thing I was wondering: For indexed scripts we could consistently use their id as part of the cache key. This would save retrieving the string version from the index later (assuming a cache hit). On the flip side it would make it understanding cache keys trickier. Not sure if this is worth the effort?
</comment><comment author="javanna" created="2015-04-16T09:42:12Z" id="93693881">looks good, left a couple of comments around testing.
</comment><comment author="MaineC" created="2015-04-16T09:54:48Z" id="93696207">@javanna Thanks for the review and comments - good suggestions, will update accordingly.
</comment><comment author="javanna" created="2015-04-21T09:56:04Z" id="94725628">I left a small comments around tests, LGTM besides that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in JVM checker user help.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10525</link><project id="" key="" /><description>When checking the JVM currently running ES we provide the user with
help on which environment variable to use to disable the check in
case the check fails. The variable we point to however is the wrong
one.

(As discussed by mail previously.)
</description><key id="67535130">10525</key><summary>Fix typo in JVM checker user help.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">MaineC</reporter><labels /><created>2015-04-10T07:47:45Z</created><updated>2015-04-20T17:35:41Z</updated><resolved>2015-04-20T17:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-10T08:15:29Z" id="91474025">LGTM
</comment><comment author="rmuir" created="2015-04-10T11:14:06Z" id="91519372">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix to pom.xml to allow eclipse maven integration using m2e</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10524</link><project id="" key="" /><description>Although eclipse currently works with the `mvn eclipse:eclipse` command, this will allow people to enable the m2e maven integration in eclipse.

The fix only changes the eclipse lifecycle mapping plugin so will not affect anything outside of the eclipse build. It instructs eclipse to execute the antrun plugin during an incremental or full build in the IDE and to ignore the maven-resources-plugin. The resources plugin is only used for the rpm and deb packaging so is not required for the IDE build.
</description><key id="67534239">10524</key><summary>Fix to pom.xml to allow eclipse maven integration using m2e</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T07:40:16Z</created><updated>2015-08-07T10:07:47Z</updated><resolved>2015-04-10T09:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-10T07:48:14Z" id="91469457">LGTM
</comment><comment author="clintongormley" created="2015-04-13T09:30:06Z" id="92290867">w00t
</comment><comment author="dadoonet" created="2015-04-13T09:39:10Z" id="92292986">@colings86 Do you think you can also apply this change here? https://github.com/elastic/elasticsearch-parent/blob/master/pom.xml#L719

(in master and 1.x and 1.5 branches)?
</comment><comment author="colings86" created="2015-04-13T11:45:38Z" id="92324327">@dadoonet done in https://github.com/elastic/elasticsearch-parent/commit/964f757fb8086961dc4f61c46a23df94b5096704 , https://github.com/elastic/elasticsearch-parent/commit/e4d7bb5ee083022495688b0d4b17edbc8fa26c39 , and https://github.com/elastic/elasticsearch-parent/commit/699763b3cf672126367509ac9d3a7e63191e1cd1
</comment><comment author="dadoonet" created="2015-04-13T12:53:31Z" id="92341509">Awesome! Thanks a lot @colings86!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add field stats api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10523</link><project id="" key="" /><description>The field stats api should return field level statistics such as lowest, highest values and number of documents that have at least one value for a field.

An api like this can be useful to explore a data set you don't know much about. For example you can figure at with the lowest and highest response times are, so that you can create a histogram or range aggregation with sane settings.

This api doesn't run a search to figure this statistics out, but rather use the Lucene index look these statics up (using `Terms` class in Lucene). So finding out these stats for fields is cheap and quick.

The min/max values are based on the type of the field. So for a numeric field min/max are numbers and date field the min/max date and other fields the min/max are term based. 

The api is quite straight forward it expects a list of fields and optionally an index can be provided:
(ran on a stack exchange dump)

```
curl -XGET "http://localhost:9200/_field_stats?fields=comment_count,view_count,answer_count,creation_date,display_name"
```

The response looks like this:

``` json
{
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "indices" : {
      "_all" : {
         "fields": {
            "comment_count": {
               "doc_count": 202940,
               "min_value": 0,
               "max_value": 98
            },
            "creation_date": {
               "doc_count": 564633,
               "min_value": "2008-08-01T16:37:51.513Z",
               "max_value": "2013-06-02T03:23:11.593Z"
            },
            "display_name": {
               "doc_count": 126741,
               "min_value": "0",
               "max_value": "&#51221;&#54812;&#49440;"
            },
            "answer_count": {
               "doc_count": 139885,
               "min_value": 0,
               "max_value": 160
            },
            "view_count": {
               "doc_count": 152357,
               "min_value": 1,
               "max_value": 506955
            }
         }
      }
   }
}
```

This is a work in progress. Things to be done are documentation and tests.
</description><key id="67530298">10523</key><summary>Add field stats api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>feature</label><label>release highlight</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T07:20:36Z</created><updated>2015-10-27T10:29:58Z</updated><resolved>2015-04-23T07:04:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-10T11:29:22Z" id="91524834">Should we expose the other stats in Terms too? I agree with leaving out size(), but these two are easily summed up just like docCount:
- sumDocFreq: number of postings
- sumTotalTermFreq: number of tokens
</comment><comment author="martijnvg" created="2015-04-10T11:55:30Z" id="91531503">@rmuir Thanks for taking a look at this! Agreed, it makes sense to expose `sumDocFreq` and `sumTotalTermFreq` as well, since it is trivial to add those. Adding `size()` is far from trivial, since we need to have the actual terms at reduce time...
</comment><comment author="rmuir" created="2015-04-10T12:00:07Z" id="91532544">Should we return max_doc as well (i know it comes from the IndexReader)? The reason is, its needed for some calculations (like determining the density/sparseness of a field) and it seems screwed up if the user has to use another api just to compute that. e.g. `displayname` field is populated 54% of the time (doc_count/max_doc)
</comment><comment author="martijnvg" created="2015-04-10T12:03:17Z" id="91532911">&gt; Should we return max_doc as well (i know it comes from the IndexReader)?

+1 Lets add that one too, it is very useful. I'll just pass `maxDoc` down with `Terms`.
</comment><comment author="martijnvg" created="2015-04-12T23:42:12Z" id="92152293">@rmuir I've updated the PR based on your feedback and added tests and docs.
</comment><comment author="rmuir" created="2015-04-13T03:50:22Z" id="92189319">Thanks for the update @martijnvg , a few questions:

For `field_ratio`, can we rename it? I think ratio is a little confusing (it makes me think of M:N). I would prefer something like `density`, and just document it is a percentage.

Along the same note, as `field_ratio` is a derived statistic, should we include other useful ones in a followup PR? Technically, that one a user could compute themselves, but I think its nice to have a plan.

Other potentially useful ones similar to that:
- `avg_length` (average number of words per doc): 
  
  sum_total_term_freq / doc_count

note: sum_total_term_freq will be -1 if frequencies are omitted, but in that case this one is easy, its 1 :)
- `avg_num_terms` (average number of unique words per doc): 
  
  sum_doc_freq / doc_count

note: both the above don't consider stopwords, they reflect what is indexed.
These can help you know how short/long the field is and so on.

Another interesting one:
- `multi_valued` (more than one indexed token in any doc): 
  
  sum_doc_freq != -1 &amp;&amp; sum_doc_freq &gt; doc_count

note: this currently is no good for numeric/date fields (will always say true, because of "extra tokens"), but works in the future with autoprefix.
This could maybe help with debugging fielddata or just understanding what is there.

I will take a look at the code tomorrow, thank you!
</comment><comment author="martijnvg" created="2015-04-13T06:23:20Z" id="92230088">&gt; For field_ratio, can we rename it? I think ratio is a little confusing (it makes me think of M:N). I would prefer something like density, and just document it is a percentage.

+1, it was just to best name I could come up with.

I like the idea of adding more derived statistics. In fact because of these derived statistics and the good use for debugging purposes, I think it makes sense to add a _cat api variant of this api too? This api can nicely be rendering in a tabular format in the console.

&gt; note: sum_total_term_freq will be -1 if frequencies are omitted, but in that case this one is easy, its 1 :)

Cool, that makes sense :)

&gt; note: both the above don't consider stopwords, they reflect what is indexed. These can help you know how short/long the field is and so on.

I think if in the documentation we emphasised on the fact that all stats are based on indexed tokens, this is less of an surprise.

&gt; note: this currently is no good for numeric/date fields (will always say true, because of "extra tokens"), but works in the future with autoprefix.

I like the `multi_valued` derived statistic. I'm just worried it will be misinterpret until autoprefix gets in, so maybe we should add that one later? (or at least not backport it to 1.6)

I also have been thinking about how to expose `Terms.size()` in a minimal way. I think it makes since to expose it as a per shard statistic, so it shows the number of terms for shard that holds the most terms? This way at least there is some insight in the number of terms per field.
</comment><comment author="rmuir" created="2015-04-13T11:48:14Z" id="92324641">&gt; I like the multi_valued derived statistic. I'm just worried it will be misinterpret until autoprefix gets in, so maybe we should add that one later? (or at least not backport it to 1.6)

Well we already know the type of field, so i was thinking to just make it unavailable for numeric fields for now. But we'd still have it for strings.

&gt; I also have been thinking about how to expose Terms.size() in a minimal way. I think it makes since to expose it as a per shard statistic, so it shows the number of terms for shard that holds the most terms? This way at least there is some insight in the number of terms per field.

I don't think we should do this. It would have to be the number of terms for a segment... Lets stay away from this one because the minute we go down this path, the API becomes slow.
</comment><comment author="martijnvg" created="2015-04-13T12:28:21Z" id="92334730">&gt; Well we already know the type of field, so i was thinking to just make it unavailable for numeric fields for now. But we'd still have it for strings.

That makes sense.

&gt; I don't think we should do this. It would have to be the number of terms for a segment... Lets stay away from this one because the minute we go down this path, the API becomes slow.

I totally missed that MultiTerms#size() isn't implemented, so yes doing that via the TermsEnum would be slow.
</comment><comment author="rmuir" created="2015-04-13T21:19:37Z" id="92502967">changes look good to me. datatypes for all stats/ -1 handling / etc is all good. Maybe we want to rename the `field_ratio` -&gt; `density` and push the change? We can open followups for the other stats ideas.
</comment><comment author="martijnvg" created="2015-04-13T21:32:19Z" id="92507229">I updated the PR and renamed `field_ratio` to `density`. But yes lets do the other derived measurements and _cat api variant in different issues.
</comment><comment author="rmuir" created="2015-04-13T21:51:56Z" id="92511364">+1 (just some more nitpicks). thanks martijn, this is great.
</comment><comment author="rjernst" created="2015-04-13T22:48:46Z" id="92522893">@martijnvg I left some comments, looks ok in general.  It is unfortunate so much boiler plate is necessary for a simple api :(
</comment><comment author="martijnvg" created="2015-04-14T06:42:05Z" id="92649762">@rjernst @rmuir  Thanks for the feedback. I'll update the PR later today. I agree adding a new api shouldn't add so much boilerplate code. This can definitely be done with less code. (Ideally we should have 3 classes per api: request, response and an operation class. I think for many broadcast apis this is doable. Hopefully this can done in the not so far future!)
</comment><comment author="rashidkpc" created="2015-04-17T00:56:43Z" id="93870575">Would it be possible to filter the fields/indices we get back based on a filter? Perhaps I want back all of the indices with `@timestamps` fields between 2014-01-01 and 2015-01-01:

```
curl -XGET "http://localhost:9200/_field_stats?fields=@timestamp&amp;min=2014-01-01&amp;max=2015-01-01"
```

This would be **incredibly** useful for Kibana and let us get rid of some serious bugs that occur for users with large numbers of indices. In fact, we could entirely get rid of our notion of timestamped indices as we'd be able to reliably sort an index list based on time. Users would only need to know their wildcard pattern! It would also allow for weird indexing strategies where maybe they're indexing weekly at one point and they need to step up to daily, something that isn't currently possible. Heck, you wouldn't even need timestamped indices, you could just increase a counter!

It would be so amazing, and so fast!

Martijn, I can't tell you how excited I am for this feature. Sorry for the late feedback! Amazing work!
</comment><comment author="martijnvg" created="2015-04-17T07:42:29Z" id="93936096">@rashidkpc Yes, I think we can add this filtering on the min value and max value in the reduce phase of this api. So we effectively omit shard level results with a `min_value` and `max_value` that fall outside of the defined `min` and `max`. First I'll make sure that this gets pushed and I'll create a followup issue for this enhancement.
</comment><comment author="martijnvg" created="2015-04-17T12:29:55Z" id="93976483">I updated the PR to the latest master and addressed @rjernst's comments. (latest commit) 
</comment><comment author="rjernst" created="2015-04-17T18:43:15Z" id="94049403">@martijnvg LGTM, I left a couple more minor comments. I also accidentally left most of them on the last commit itself...sorry.
</comment><comment author="martijnvg" created="2015-04-19T22:05:27Z" id="94318656">I updated the PR with the follow changes:
- Incorporated @rjernst feedback (thank you!)
- Added a `level` option to control whether the field stats of all indices are combined or field stats are only combined on a per index basis.
- Added better error handling in the case fields with the same are of a different type between indices.
</comment><comment author="martijnvg" created="2015-04-21T09:43:52Z" id="94722848">If there are no further comments about the additional commits then I'll push this tomorrow.
</comment><comment author="rjernst" created="2015-04-23T06:29:44Z" id="95458491">@martijnvg LGTM, i just left a couple minor suggestions. 
</comment><comment author="maharg101" created="2015-04-28T19:15:34Z" id="97173940">Very nice. Will elasticsearch core use this internally to optimise which indices to query for a given field value ?
</comment><comment author="pjcard" created="2015-10-26T09:05:55Z" id="151067757">Am I right in thinking that anyone wishing to replace their time based indices with this mechanism will need to perform two queries in order to do so? (The first field_api query to obtain the indicies to pass in the URI for the search query). Is it possible to add this as a filter for a search query, thus doing away for the need use patterns in the search URI completely?

Or is @maharg101 correct in postulating that elasticsearch will use this internally to remove the need for the query to be explicit about indicies at all?
</comment><comment author="maharg101" created="2015-10-26T09:53:12Z" id="151084698">@pjcard you have extrapolated what I meant slightly - the scenario I had in mind was that you would still be explicit about the time-based indices, and that the query execution would take advantage of the min / max values of fields to reduce the set of indices / shards to look in.
</comment><comment author="martijnvg" created="2015-10-27T02:52:10Z" id="151353890">@pjcard You would still need to send two requests to ES (1 field stats &amp; 1 search request). ES doesn't use the min and max internally to optimise the search (all though in theory it can). This is also how Kibana is going to use the field stats api to reduce the number of indices a search needs to be executed on: https://github.com/elastic/kibana/issues/4342
</comment><comment author="pjcard" created="2015-10-27T10:29:58Z" id="151446674">Hi @martijnvg, thanks for the information. I'll add a request for that feature in case it's of use for anybody else and/or feasible.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>put warmer api should check for data read blocks too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10522</link><project id="" key="" /><description>Put warmer api currently checks for metadata write blocks only. Given that it executes a search request before storing the warmer, to verify that the search request is valid, it should also check for data read blocks.

Will be solved with #9203
</description><key id="67528026">10522</key><summary>put warmer api should check for data read blocks too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T07:06:54Z</created><updated>2015-04-23T13:21:58Z</updated><resolved>2015-04-23T13:21:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-10T21:30:31Z" id="91696789">Isn't the fact warmer validation works via an actual search request a problem with validation? Will the query parsing refactoring allow fixing this? I don't think doing a metadata write should require data read...
</comment><comment author="javanna" created="2015-04-15T13:12:00Z" id="93388154">valid point @rjernst , I agree the inner search and requiring to read data as part of a metadata write operation might feel weird. That said this issue was meant to keep track of the missing check for blocks issue that was found in #9203, which we are fixing there. We can then open a new issue if we want to change how the put warmer api works internally.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>get field mapping api should check for metadata blocks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10521</link><project id="" key="" /><description>The get field mapping api currently checks for data read blocks rather than metadata blocks. Seems like a bug given that the api allows to retrieve mappings and not data. This can be seen by adding a metadata block to an index and then calling the get field mapping api, which works while it should be rejected.

The problem might stem from the fact that metadata block currently means read and write, causing different issues, which will be solved with #9203. Once that is in we will be able to only check for metadata read blocks instead.
</description><key id="67527379">10521</key><summary>get field mapping api should check for metadata blocks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-10T07:00:40Z</created><updated>2015-04-23T13:21:58Z</updated><resolved>2015-04-23T13:21:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>All shards failed to start!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10520</link><project id="" key="" /><description>Hi all,

Since I use multiple paths, this problem often occurs when I restart the cluster, but sometimes this error has inexplicably disappeared. Tried deleting segments.gen files, seem to have no effect.

This is the Debug mode's log when opening one of the index &#65306;    https://gist.github.com/SephenXu/66b92d51eaa63ac85579

jdk:1.6
elasticsearch version : v1.1.2
Really need your help!

Regards,
Sephen Xu
</description><key id="67491205">10520</key><summary>All shards failed to start!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SephenXu</reporter><labels /><created>2015-04-10T02:20:43Z</created><updated>2015-04-10T07:37:25Z</updated><resolved>2015-04-10T02:26:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-10T02:26:27Z" id="91405449">Please note that ES 1.1.X is not supported on Java 6. See https://www.elastic.co/subscriptions/matrix for more information.

Also the mailing list is your best option for these sorts of operational issues - https://groups.google.com/forum/#!forum/elasticsearch. Github is for code related problems or bugs.
</comment><comment author="SephenXu" created="2015-04-10T02:39:09Z" id="91408369">But the document said, for Java 6, ES versions &lt; 1.2.0 is ok! When did it change?
</comment><comment author="markwalkom" created="2015-04-10T02:41:50Z" id="91408546">Which document?
</comment><comment author="SephenXu" created="2015-04-10T03:05:16Z" id="91413344">https://www.elastic.co/blog/elasticsearch-1-2-0-released
"Elasticsearch now requires Java 7 and will no longer work with Java 6."
Isn't meant that since 1.2.0, es no longer work with java 6, but not contained 1.1.x? 
</comment><comment author="markwalkom" created="2015-04-10T03:08:17Z" id="91413905">Supported and required are different things though.
1.2 won't run with java 1.6 at all. 1.1 will but it's not officially supported.

Also Java 6 has been end of life for years now.
</comment><comment author="s1monw" created="2015-04-10T07:37:25Z" id="91468214">give your logs you data is not present where you started elasticsearch did you delete any data files / directories?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add multi data path testing to static bwc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10519</link><project id="" key="" /><description>This randomly chooses to run bwc indexes on single or multi data paths.
</description><key id="67430238">10519</key><summary>Tests: Add multi data path testing to static bwc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T19:07:38Z</created><updated>2015-04-09T21:33:17Z</updated><resolved>2015-04-09T21:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-09T21:24:29Z" id="91359942">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use DisableFsyncFS in tests to avoid fsync most of the time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10518</link><project id="" key="" /><description>Spinoff from #10516 where we fixed ElasticsearchMockDirectoryWrapper to skip fsync most of the time, but really the right way to do this is as Lucene did, at the filesystem level...
</description><key id="67424299">10518</key><summary>Use DisableFsyncFS in tests to avoid fsync most of the time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v2.3.0</label></labels><created>2015-04-09T18:34:36Z</created><updated>2016-01-17T18:54:34Z</updated><resolved>2016-01-17T18:54:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T16:56:16Z" id="172351820">@mikemccand is this a change you're still planning making?
</comment><comment author="mikemccand" created="2016-01-17T18:54:34Z" id="172365187">@clintongormley I'll close it for now ... but I do think we should eventually do this, once we cutover ES tests eventually to use Lucene's randomized MockFS infrastructure ... the change from #10516 was really a hacky workaround ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indexed script not properly updated when used for sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10517</link><project id="" key="" /><description>I index a groovy script and use it w/ script_score. Everything is fine.
I update the script (I used PUT below, but POST does the same) with a new script that should produce different values (constant, for the sake of demonstration). I get old values instead (values below are from an older atan2(1,1) version).

Same if I -XDELETE on the endpoint before POST/PUTing the new script.

Restarting elasticsearch makes the problem go away, until I change the script again.

```
 ~ &#10003; curl -XPUT localhost:9200/_scripts/groovy/indexedCalculateScore -d '{"script": "atan2(1, 2)"}'
{"_id":"indexedCalculateScore","_version":24,"created":false} ~ &#10003; 
 ~ &#10003; curl http://localhost:9200/twitter/tweet/_search?pretty=true -d '
{ "query": {
  "function_score": {
    "query": {
      "match": {"message": "tweet"}
    },
    "boost_mode": "replace",
    "script_score":{
      "scriptId": "indexedCalculateScore",
      "lang":"groovy"}
    }
  }
}
'
{
  "took" : 11,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.7853982,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 0.7853982,
      "_source":
{
    "user": "kimchy",
    "postDate": "2009-11-15T14:12:12",
    "message": "Another tweet, will it be indexed?"
}
    } ]
  }
}


 ~ &#10003; curl -XPUT localhost:9200/_scripts/groovy/indexedCalculateScore -d '{"script": "atan2(2, 1)"}'
{"_id":"indexedCalculateScore","_version":25,"created":false} ~ ~ &#10003; curl http://localhost:9200/twitter/tweet/_search?pretty=true -d '
{ "query": {
  "function_score": {
    "query": {
      "match": {"message": "tweet"}
    },
    "boost_mode": "replace",
    "script_score":{
      "scriptId": "indexedCalculateScore",
      "lang":"groovy"}
    }
  }
}
'
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.7853982,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 0.7853982,
      "_source":
{
    "user": "kimchy",
    "postDate": "2009-11-15T14:12:12",
    "message": "Another tweet, will it be indexed?"
}
    } ]
  }
}
```
</description><key id="67415192">10517</key><summary>indexed script not properly updated when used for sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schnittchen</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label></labels><created>2015-04-09T17:47:50Z</created><updated>2015-04-24T07:14:35Z</updated><resolved>2015-04-24T07:14:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-10T09:00:17Z" id="91483934">this is caused by the same reason as #10397 . We are addressing it in #10526 .
</comment><comment author="MaineC" created="2015-04-23T09:16:25Z" id="95503568">As #10526 was merged I think this can be closed?
</comment><comment author="javanna" created="2015-04-23T19:23:22Z" id="95691639">agreed @MaineC go for it ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't fsync so often in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10516</link><project id="" key="" /><description>This is a big speedup for some tests, e.g. OldIndexBackwardsCompatibilityTests goes from 159 sec  (master) down to 49 sec on my dev box with this change:

In Lucene, when tests need to fsync, we only actually do it rarely. This used to be done in MockDirWrapper.sync, but was recently moved it down to MockFileSystem (DisableFsyncFS).

ES hasn't cutover to MockFS yet, so we are now always doing fsync in master...

I think we can do it rarely() like Lucene?  I fixed ESMockDirectoryWrapper to do this ... but maybe instead we can cutover to MockFS?
</description><key id="67406553">10516</key><summary>Don't fsync so often in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T17:06:18Z</created><updated>2015-04-09T18:30:16Z</updated><resolved>2015-04-09T18:30:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-09T17:10:14Z" id="91294388">I am ok with the reflection hacks for now, lets get tests stable!

Can we open up a followup issue to integrate mockfs? I want to know these hacks will go away eventually.
</comment><comment author="mikemccand" created="2015-04-09T17:12:07Z" id="91295163">Thanks @rmuir, yeah I'll open a follow-on issue to add MockFS and then remove this...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] Documentation not clear about uniqueness of ids in an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10515</link><project id="" key="" /><description>A document is uniquely identified by index, type and id. Two documents in an index can have the same id if they have different types. However, the documentation does not seem to explicitly say so anywhere. It would be good to describe this somewhere, probably in http://www.elastic.co/guide/en/elasticsearch/reference/1.x/docs-index_.html
similar to how it is described in the guide:
http://www.elastic.co/guide/en/elasticsearch/guide/master/_document_metadata.html#_id
</description><key id="67402849">10515</key><summary>[doc] Documentation not clear about uniqueness of ids in an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">brwe</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-04-09T16:45:54Z</created><updated>2016-01-15T12:41:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] Modify GeoShapeFieldMapper to accept precision field updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10514</link><project id="" key="" /><description>Related to #10513  There are a couple enhancement discussion points here:
- GeoPointFieldMapper disallows any changes. This makes sense since changing any one of the parameters compromises the integrity of all existing commits.  Thus new commits require a full reindex.
- We should be able to allow the changing of precision on GeoShapeFieldMapper since each existing term is identified in the PrefixTree as either an internal or leaf node.  If we increase the precision, existing terms will terminate at the level of the leaves, so they will remain at the same precision when they were created.  The converse is true, decreasing precision will reduce the resolution of newly created shapes and the leaf nodes will prevent traversal beyond the shapes tree depth.  This will be a separate enhancement issue beyond the scope of fixing this bug.
</description><key id="67401424">10514</key><summary>[GEO] Modify GeoShapeFieldMapper to accept precision field updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label><label>enhancement</label></labels><created>2015-04-09T16:37:50Z</created><updated>2016-01-20T15:28:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T16:55:39Z" id="172351440">@nknize is this still relevant?
</comment><comment author="nknize" created="2016-01-20T15:28:09Z" id="173237903">This is still relevant for `geo_shape` - for now. I'll be revisiting shortly.  

Use case: a mapping is created with highest level of precision (without knowing an optimal precision a priori). Suddenly `geo_shape` queries and inserts become sluggish. This allows precision changes without requiring a full reindex.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] GeoShapeFieldMapper incorrectly acknowledges mapping changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10513</link><project id="" key="" /><description>GeoShapeFieldMapper does not overload `merge()`, so the following request produces 
 `acknowledged: "true"` from the `AbstractFieldMapper`

``` json
# update mapping after inserting a shape
PUT geo-shapes/doc/_mapping
{
  "doc": {
    "properties": {
        "geometry": {
          "type": "geo_shape",
          "tree": "quadtree",
          "tree_levels": 26,
          "distance_error_pct": 0
        }
    }
  }
}
```

This is misleading since changing field parameters has zero impact on the `GeoShapeFieldMapper` itself leading to a user thinking they're indexing at a higher precision when no change has occurred.  This is bad news for query expectations.

This issue will be corrected by overloading merge to add conflicts if certain parameters are changed.  
</description><key id="67401017">10513</key><summary>[GEO] GeoShapeFieldMapper incorrectly acknowledges mapping changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>discuss</label></labels><created>2015-04-09T16:35:48Z</created><updated>2015-04-10T19:40:23Z</updated><resolved>2015-04-10T19:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES doesn't decode JSON escapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10512</link><project id="" key="" /><description>JSON allows any unicode codepoint in Basic Multilingual Plane to be represented with \u escape mechanism. Instead of handling the escape, ES seems to save the data as is. This causes problems later when searching. For example if the original document contained a field with \u00e4, a search like _search?q=name:\u00e4 would find it, but _search?q=name:&#228; or _search?q=name:%e4 doesn't.

If this is by design, it needs to be documented at http://www.elastic.co/guide/en/elasticsearch/guide/current/_document_oriented.html#_json and other places which currently state that ES uses JSON (without any disclaimers).
</description><key id="67386743">10512</key><summary>ES doesn't decode JSON escapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hukka</reporter><labels /><created>2015-04-09T15:28:02Z</created><updated>2015-04-12T15:12:49Z</updated><resolved>2015-04-12T15:12:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-12T15:12:49Z" id="92078055">Hi @tpievila 

Elasticsearch does support the `\u` escape, and stores strings as UTF8:

```
DELETE test 

PUT test/test/1
{
  "text": "\u00e4"
}

GET /test/test/_search
{
  "aggs": {
    "indexed_terms": {
      "terms": {
        "field": "text"
      }
    }
  }
}
```

The above returns:

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "test",
            "_id": "1",
            "_score": 1,
            "_source": {
               "text": "&#228;"
            }
         }
      ]
   },
   "aggregations": {
      "indexed_terms": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "&#228;",
               "doc_count": 1
            }
         ]
      }
   }
}
```

However your query for `?q=%E4` is in UTF-16, not UTF-8.  This works:

```
GET /test/test/_search?q=%c3%a4
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate ids in index (without autogeneration)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10511</link><project id="" key="" /><description>Opening a new issue as requested by @brwe in the comments to #8788.

We recently saw an unexpectedly high count on an index we're feeding. It's the second time we've seen this; the first time was only off by 1, but this time it was off by over 16,000 in an index that should contain less than 300k docs. Since the dataset is so small I pulled down everything (`_search?pretty=true&amp;fields=&amp;size=400000`), scraped out the ids and ran them through `uniq`, which revealed a ton of duplicates. 

Notes/observations:
- We're using 1.3.4, doing bulk indexing with the Java API's `TransportClient` and `BulkProcessor`.
- We're **not** using autogenerated ids, so it doesn't look as if the fix for #8788 is likely to help us.
- No id appeared more than twice.
- The index has not been migrated from an earlier version. In fact, the index is deleted and recreated irregularly but fairly frequently, from every few days to every couple of weeks.
- Around the time the duplicates appeared, we saw problems in other (non-Elastic) parts of the system. I can't see any way that they could directly cause the duplication, but it's possible that network issues were the common cause of both.

Running

```
curl -s 'http://HOST:9200/std_physicalasset_alpha_a/_search?pretty&amp;q=_id:77309419847&amp;explain&amp;fields=_source,_routing' -d '{"version":true}'
```

produced

```
{
  "took" : 247,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "HRLe1R1iTzqzZ44ocSb4Sw",
      "_index" : "std_physicalasset_alpha_a",
      "_type" : "other",
      "_id" : "77309419847",
      "_version" : 2,
      "_score" : 1.0,
      "_source":{"DEBUGRUN":"2015-04-03T20:48:55.327Z"},
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:_default_#77309419847 _uid:generic#77309419847 _uid:other#77309419847 _uid:plant#77309419847 _uid:vessel#77309419847), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    }, {
      "_shard" : 3,
      "_node" : "HRLe1R1iTzqzZ44ocSb4Sw",
      "_index" : "std_physicalasset_alpha_a",
      "_type" : "vessel",
      "_id" : "77309419847",
      "_version" : 456,
      "_score" : 1.0,
      "_source":{"DEBUGRUN":"2015-04-08T16:12:52.903Z"},
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:_default_#77309419847 _uid:generic#77309419847 _uid:other#77309419847 _uid:plant#77309419847 _uid:vessel#77309419847), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```

I've removed most of `_source` as verbose and not relevant, but `DEBUGBATCH` is interesting; it's a manually-set human-readable equivalent of `_timestamp`, and shows that the two documents with this id were loaded almost 5 days apart. The second has been updated again since I captured that, although the feeder process has now been stopped.

```
curl -s 'http://HOST:9200/std_physicalasset_alpha_a/_segments?pretty'
```

produces around 100k of JSON, which seems a bit much to post in full. (That may be telling in itself; the full response contained a suspiciously-round 255 segment objects.) The following is just the portion for the shard and (non-primary) node containing the sample duplicate id above:

```
{
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "indices" : {
    "std_physicalasset_alpha_a" : {
      "shards" : {
====================
# Shards 0-2 omitted
====================
        "3" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : false,
            "node" : "HRLe1R1iTzqzZ44ocSb4Sw"
          },
          "num_committed_segments" : 26,
          "num_search_segments" : 16,
          "segments" : {
            "_2a5z" : {
              "generation" : 106487,
              "num_docs" : 43857,
              "deleted_docs" : 15215,
              "size_in_bytes" : 341737818,
              "memory_in_bytes" : 2035824,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a76" : {
              "generation" : 106530,
              "num_docs" : 1197,
              "deleted_docs" : 31,
              "size_in_bytes" : 9592426,
              "memory_in_bytes" : 418896,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a7t" : {
              "generation" : 106553,
              "num_docs" : 1382,
              "deleted_docs" : 35,
              "size_in_bytes" : 11067533,
              "memory_in_bytes" : 434208,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a82" : {
              "generation" : 106562,
              "num_docs" : 1629,
              "deleted_docs" : 48,
              "size_in_bytes" : 12533722,
              "memory_in_bytes" : 443952,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a8n" : {
              "generation" : 106583,
              "num_docs" : 2070,
              "deleted_docs" : 61,
              "size_in_bytes" : 15764916,
              "memory_in_bytes" : 447088,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a9e" : {
              "generation" : 106610,
              "num_docs" : 1480,
              "deleted_docs" : 28,
              "size_in_bytes" : 11471539,
              "memory_in_bytes" : 435432,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a9m" : {
              "generation" : 106618,
              "num_docs" : 151,
              "deleted_docs" : 3,
              "size_in_bytes" : 1529856,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2a9n" : {
              "generation" : 106619,
              "num_docs" : 6528,
              "deleted_docs" : 145,
              "size_in_bytes" : 45294050,
              "memory_in_bytes" : 632040,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2a9q" : {
              "generation" : 106622,
              "num_docs" : 186,
              "deleted_docs" : 4,
              "size_in_bytes" : 1872124,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2a9t" : {
              "generation" : 106625,
              "num_docs" : 220,
              "deleted_docs" : 9,
              "size_in_bytes" : 2276799,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2a9w" : {
              "generation" : 106628,
              "num_docs" : 654,
              "deleted_docs" : 18,
              "size_in_bytes" : 5614691,
              "memory_in_bytes" : 368520,
              "committed" : true,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2aaf" : {
              "generation" : 106647,
              "num_docs" : 101,
              "deleted_docs" : 80,
              "size_in_bytes" : 1858120,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2aaj" : {
              "generation" : 106651,
              "num_docs" : 499,
              "deleted_docs" : 269,
              "size_in_bytes" : 6309352,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : false
            },
            "_2aaq" : {
              "generation" : 106658,
              "num_docs" : 609,
              "deleted_docs" : 207,
              "size_in_bytes" : 6785525,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : false
            },
            "_2aas" : {
              "generation" : 106660,
              "num_docs" : 663,
              "deleted_docs" : 225,
              "size_in_bytes" : 7485334,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : false
            },
            "_2aat" : {
              "generation" : 106661,
              "num_docs" : 106,
              "deleted_docs" : 0,
              "size_in_bytes" : 1942039,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2aau" : {
              "generation" : 106662,
              "num_docs" : 123,
              "deleted_docs" : 0,
              "size_in_bytes" : 2226032,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2aaw" : {
              "generation" : 106664,
              "num_docs" : 75,
              "deleted_docs" : 0,
              "size_in_bytes" : 1489187,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2aaz" : {
              "generation" : 106667,
              "num_docs" : 93,
              "deleted_docs" : 0,
              "size_in_bytes" : 1683929,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2ab0" : {
              "generation" : 106668,
              "num_docs" : 74,
              "deleted_docs" : 0,
              "size_in_bytes" : 1368605,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2ab5" : {
              "generation" : 106673,
              "num_docs" : 68,
              "deleted_docs" : 0,
              "size_in_bytes" : 1346587,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2ab8" : {
              "generation" : 106676,
              "num_docs" : 31,
              "deleted_docs" : 0,
              "size_in_bytes" : 690748,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2ab9" : {
              "generation" : 106677,
              "num_docs" : 54,
              "deleted_docs" : 0,
              "size_in_bytes" : 1086051,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2aba" : {
              "generation" : 106678,
              "num_docs" : 121,
              "deleted_docs" : 0,
              "size_in_bytes" : 2119135,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2abb" : {
              "generation" : 106679,
              "num_docs" : 473,
              "deleted_docs" : 0,
              "size_in_bytes" : 6822800,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : false
            },
            "_2abc" : {
              "generation" : 106680,
              "num_docs" : 714,
              "deleted_docs" : 388,
              "size_in_bytes" : 12332118,
              "memory_in_bytes" : 417680,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2abd" : {
              "generation" : 106681,
              "num_docs" : 1,
              "deleted_docs" : 0,
              "size_in_bytes" : 49613,
              "memory_in_bytes" : 0,
              "committed" : true,
              "search" : false,
              "version" : "4.9",
              "compound" : true
            },
            "_2acb" : {
              "generation" : 106715,
              "num_docs" : 17,
              "deleted_docs" : 0,
              "size_in_bytes" : 398023,
              "memory_in_bytes" : 195496,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            },
            "_2acc" : {
              "generation" : 106716,
              "num_docs" : 36,
              "deleted_docs" : 0,
              "size_in_bytes" : 756378,
              "memory_in_bytes" : 228552,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            },
            "_2acd" : {
              "generation" : 106717,
              "num_docs" : 13,
              "deleted_docs" : 0,
              "size_in_bytes" : 316552,
              "memory_in_bytes" : 196488,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            },
            "_2ace" : {
              "generation" : 106718,
              "num_docs" : 17,
              "deleted_docs" : 0,
              "size_in_bytes" : 354373,
              "memory_in_bytes" : 189920,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            },
            "_2acf" : {
              "generation" : 106719,
              "num_docs" : 2118,
              "deleted_docs" : 96,
              "size_in_bytes" : 23788950,
              "memory_in_bytes" : 490448,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : false
            },
            "_2acg" : {
              "generation" : 106720,
              "num_docs" : 24,
              "deleted_docs" : 0,
              "size_in_bytes" : 516210,
              "memory_in_bytes" : 219592,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            },
            "_2ach" : {
              "generation" : 106721,
              "num_docs" : 67,
              "deleted_docs" : 0,
              "size_in_bytes" : 1265169,
              "memory_in_bytes" : 236688,
              "committed" : false,
              "search" : true,
              "version" : "4.9",
              "compound" : true
            }
          }
        }, {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "oyFoYqV1SHyLoJmOzRQrDQ"
          },
          "num_committed_segments" : 14,
          "num_search_segments" : 13,
          "segments" : {
==========================
20 segment objects omitted
==========================
        } ],
===============
Shard 4 omitted
===============
      }
    }
  }
}
```

It's highly unlikely that I'll be able to make the ES server logs available in full, but I can grep them if there's anything specific that might help. I can't find any mention of the `std_physicalasset_alpha_a` index in them except for a bunch of entries of the form

```
[2015-04-05 22:20:58,598][INFO ][index.engine.internal    ] [c115knppaes01-01] [std_physicalasset_alpha_a][2] now throttling indexing: numMergesInFlight=6, maxNumMerges=5
[2015-04-05 22:20:59,216][INFO ][index.engine.internal    ] [c115knppaes01-01] [std_physicalasset_alpha_a][2] stop throttling indexing: numMergesInFlight=4, maxNumMerges=5
```
</description><key id="67379954">10511</key><summary>Duplicate ids in index (without autogeneration)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrec</reporter><labels /><created>2015-04-09T14:56:48Z</created><updated>2015-04-09T23:03:58Z</updated><resolved>2015-04-09T16:52:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-04-09T16:52:01Z" id="91289069">@mrec thanks for these details! You have the same doc id in different types, one is "vessel", the other one is "other". It is actually fine to have two documents with the same id in different types in an index because documents are identified by type + id. 
However, the documentation does not seem to explicitly state that anywhere. I opened issue https://github.com/elastic/elasticsearch/issues/10515 for that. 
I will close this issue for now. Please feel free to reopen in case anything is unclear.
</comment><comment author="mrec" created="2015-04-09T23:03:58Z" id="91377219">Hmm, I hadn't noticed that. It might not be an issue on Elastic's side, but it's definitely an issue on ours; the ids we supply **are** supposed to be globally unique across **all** types. I'll check some other duplicates and see if the same discrepancy applies to all of them; if so, something has gone badly wrong in another part of the system.

Thanks for the keen observiness.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES barfs on 3D coordinates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10510</link><project id="" key="" /><description>GeoJSON allows for altitude/elevation data in coordinates: "A position is represented by an array of numbers. There must be at least two elements, and may be more. The order of elements must follow x, y, z order (easting, northing, altitude for coordinates in a projected coordinate reference system, or longitude, latitude, altitude for coordinates in a geographic coordinate reference system)."

Unfortunately ES does not like this at all. For example

```
"location": {"coordinates": [[21.758451554932154, 59.94981396923298, 4.606], [21.758562151951164, 59.94958900257331, 4.507], [21.758594573325485, 59.949384308291066, 5.845], [21.758506828193497, 59.94908418029604, 8.552], [21.75850445683433, 59.9489969477522, 8.753], [21.75851567477881, 59.94896506422581, 8.745], [21.758709217831672, 59.94875661966012, 7.749], [21.758812419950235, 59.94852881190606, 6.785], [21.758934003178023, 59.9481809000838, 5.878], [21.75894258819726, 59.948156920023145, 5.831]], "type": "LineString"}
```

returns

```
org.elasticsearch.ElasticsearchParseException: Invalid number of points in LineString (found 1 - must be &gt;= 2)
at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parseLineString(ShapeBuilder.java:838)
at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parse(ShapeBuilder.java:757)
at org.elasticsearch.common.geo.builders.ShapeBuilder.parse(ShapeBuilder.java:289)
at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:242)
... 12 more
```

(linenumbers do not match to master HEAD in github, but the problem is in parseCoordinates method).
The limitation is not mentioned in the documentation either (or at least it wasn't on the geo_shape page).
</description><key id="67366430">10510</key><summary>ES barfs on 3D coordinates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">hukka</reporter><labels><label>:Geo</label></labels><created>2015-04-09T13:49:49Z</created><updated>2015-04-10T20:36:07Z</updated><resolved>2015-04-10T20:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Removed aggregations from ReduceContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10509</link><project id="" key="" /><description>ReduceContext contains the list of aggregations to reduce but these aggregations are set as null half of the time. This change makes the reduce(ReduceContext) method changed to reduce(List&lt;InternalAggregation&gt;, ReduceContext) and ReduceContext now only holds the BigArrays and Script services.
</description><key id="67365847">10509</key><summary>Removed aggregations from ReduceContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T13:47:00Z</created><updated>2015-08-07T10:07:47Z</updated><resolved>2015-04-09T13:59:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-09T13:47:40Z" id="91236862">@jpountz could you review this when you have some time?
</comment><comment author="jpountz" created="2015-04-09T13:50:09Z" id="91237807">This looks great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added compatibility for java-8 (which we now recommend)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10508</link><project id="" key="" /><description /><key id="67364865">10508</key><summary>Added compatibility for java-8 (which we now recommend)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels /><created>2015-04-09T13:40:46Z</created><updated>2015-04-09T13:43:57Z</updated><resolved>2015-04-09T13:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Build: Update package repositories when creating a release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10507</link><project id="" key="" /><description>In order to automatically sign and and upload our debian and RPM
packages, this commit incorporates signing into the build process
and adds the necessary steps to the release process. In order to do this
the pom.xml has been adapted and the RPM and jdeb maven plugins have been
updated, so the packages are signed on build. However the repositories
need to signed as well.

Syncing the repos requires downloading the current repo, adding
the new packages and syncing it back.

The following environment variables are now required as part of the build
- GPG_KEY_ID - the key ID of the key used for signing
- GPG_PASSPHRASE - your GPG passphrase

The following environment variables are optional
- S3_BUCKET_SYNC_FROM: S3 bucket to get existing packages from
- S3_BUCKET_SYNC_TO: S3 bucket to sync new repo into
- GPG_KEYRING - home of gnupg, defaults to ~/.gnupg

The following command line tools are needed
- createrepo (creates RPM repositories)
- expect (used by the maven rpm plugin)
- apt-ftparchive (creates DEB repositories)
- gpg (signs packages and repo files)
- s3cmd (syncing between the different S3 buckets)

The current approach would also work for users who want to run their
own repositories, all they need to change are a couple of environment
variables.

Minor implementation detail: Right now the branch name is used as version
for the repositories (like 1.4/1.5/1.6) - if we ever change our branch naming
scheme, the script needs to be fixed.
</description><key id="67362273">10507</key><summary>Build: Update package repositories when creating a release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>v1.4.5</label><label>v1.5.3</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T13:29:23Z</created><updated>2015-06-08T15:24:36Z</updated><resolved>2015-04-26T17:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-10T07:12:28Z" id="91460173">I just skimmed through the patch and it looks to me that if you forget to set `GPG_KEY_ID` or don't have one of the required binaries installed, it will only fail at the moment when it is needed. Is it possible to make it fail when the script starts?
</comment><comment author="spinscale" created="2015-04-10T08:16:50Z" id="91474374">good catch, I changed the code to always check for gpg and expect and the gpg env variables, so signing is also done, when running in `dry_run` mode
</comment><comment author="clintongormley" created="2015-04-23T08:57:50Z" id="95496608">@spinscale is this ready to go in?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch arguments -Xms and -Xmx do not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10506</link><project id="" key="" /><description>The documentation details these:
http://www.elastic.co/guide/en/elasticsearch/guide/master/heap-sizing.html
so it looks like they worked once, and IMO they should work again (environment variables aren't always easy to set), but it looks like it can't work the way the script currently calls exec:
https://github.com/elastic/elasticsearch/blob/master/bin/elasticsearch#L154

&gt; /opt/elasticsearch/bin/elasticsearch -Xms4g -Xmx4g -Des.path.conf=/etc/elasticsearch

results in:

&gt; elastic+ 4122 171 1.7 6903540 269320 ? Sl 12:57 0:05 /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/opt/elasticsearch -cp :/opt/elasticsearch/lib/elasticsearch-1.5.0.jar:/opt/elasticsearch/lib/*:/opt/elasticsearch/lib/sigar... -Xms4g -Xmx4g -Des.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch
</description><key id="67359983">10506</key><summary>elasticsearch arguments -Xms and -Xmx do not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels /><created>2015-04-09T13:17:13Z</created><updated>2015-04-12T15:02:38Z</updated><resolved>2015-04-12T15:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-12T15:02:38Z" id="92076935">@robin13 you can see that your custom settings have been added to the end of the command line parameters, and if you do:

```
GET _nodes/jvm
```

you will see that your settings have taken effect
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only flush for checkindex if we have uncommitted changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10505</link><project id="" key="" /><description>Today we force a flush before check index to ensure we have an index
to check on. Yet if the index is large and the FS is slow this can have
significant impact on the index deletion performance. This commit introduces
a check if there are any uncommitted changes in order to skip the additional commit.
</description><key id="67355880">10505</key><summary>Only flush for checkindex if we have uncommitted changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>test</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T12:57:42Z</created><updated>2015-08-07T10:08:03Z</updated><resolved>2015-04-09T16:29:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-09T12:57:58Z" id="91223279">@mikemccand can you take a look
</comment><comment author="rjernst" created="2015-04-09T15:02:53Z" id="91256396">LGTM, one minor comment.
</comment><comment author="mikemccand" created="2015-04-09T15:20:40Z" id="91262432">LGTM, but I worry there is still a root cause here that we don't understand (why a flush that "does nothing" is so costly on Jenkins boxes but not our dev boxes).
</comment><comment author="rmuir" created="2015-04-09T15:24:44Z" id="91263395">By flush do you mean ES flush? (e.g. fsync). What is the filesystem type?

Flushes could be slower because MockDirectoryWrapper is passing down fsync() calls always to the inner directories these days. Omitting fsync calls at this level disguised several bugs.

On the other hand in lucene tests we use DisableFsyncFS around our test temporary directories which omits fsync calls to the filesystem (we do this 95% of the time, sometimes we just don't wrap the filesystem at all, so we don't mask anything with our testing layers). 
</comment><comment author="mikemccand" created="2015-04-09T15:33:16Z" id="91265866">Yeah ES flush (fsync).

The first flush is slow on my box (~16 seconds) because we had just done tons of tiny writes in the test, and the second one (there are no changes) is fast for me (~0.6 sec) ... but on Jenkins the second one is also very slow and hits a timeout in the "delete index" request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe using HTTP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10504</link><project id="" key="" /><description>Hi, is it possible to make Tribe connect to clusters using HTTP?

I want to use a single connection when querying the clusters. 
</description><key id="67353647">10504</key><summary>Tribe using HTTP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frecar</reporter><labels /><created>2015-04-09T12:45:07Z</created><updated>2015-04-12T14:58:46Z</updated><resolved>2015-04-09T13:05:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-09T13:05:49Z" id="91225503">Hi @frecar can you please ask your question on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch)? We'd rather use github issues for bugs and feature requests. Thanks!
</comment><comment author="clintongormley" created="2015-04-12T14:58:45Z" id="92076792">And for posterity: no, the tribe has to join the clusters as a client node, but you can send http requests to the tribe (which then forwards the requests over the transport layer)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate rivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10503</link><project id="" key="" /><description>- In code, we mark River class as deprecated
- We log that information when a cluster is still using it
- We add this information in the plugins list as well
</description><key id="67349015">10503</key><summary>Deprecate rivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>deprecation</label><label>docs</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T12:19:00Z</created><updated>2015-04-09T12:33:36Z</updated><resolved>2015-04-09T12:32:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-09T12:22:20Z" id="91213198">I think we should deprecate `RiverComponent` and `AbstractRiverComponent` too? Maybe also `RiverName`?
</comment><comment author="dadoonet" created="2015-04-09T12:25:30Z" id="91213937">@javanna Agreed with all your comments. I pushed another commit.
</comment><comment author="javanna" created="2015-04-09T12:28:40Z" id="91214506">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add INFO logging saying whether each path.data is on an SSD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10502</link><project id="" key="" /><description>Recently in Lucene we added an IOUtils.spins() method, which returns true for old-fashioned spinning magnets hard drives, and false for SSDs or RAM disks.

It's best effort, but should work well on Linux.

I think we should log this?  It can be helpful when looking at a node's log to see whether it's using SSDs for not for it shards...
</description><key id="67329607">10502</key><summary>Add INFO logging saying whether each path.data is on an SSD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T10:23:17Z</created><updated>2015-06-06T19:11:01Z</updated><resolved>2015-04-09T17:10:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-09T10:24:26Z" id="91185717">this is going to log it quite a bit for shards allocated and such, maybe we should do it on NodeEnvironment, @s1monw thoughts?
</comment><comment author="s1monw" created="2015-04-09T10:26:09Z" id="91187159">yeah lets put it on NodeEnvironmetn for now?
</comment><comment author="mikemccand" created="2015-04-09T10:26:44Z" id="91187356">OK I'll move to NodeEnvironment...
</comment><comment author="mikemccand" created="2015-04-09T10:41:28Z" id="91190551">OK I moved it to NodeEnvironment.  Note that this is not as "accurate" since before we checked the actual Directory instance that Lucene would be using, so if someone is creating some per-index symlink or something then we could log the wrong thing here ... but I agree it is less noisy.
</comment><comment author="s1monw" created="2015-04-09T12:34:06Z" id="91215919">LGTM
</comment><comment author="mikemccand" created="2015-04-09T16:31:37Z" id="91283552">OK I merged this with the existing (TRACE) logging we were already doing for free space, but upgraded to INFO logging, and added total space, spins, FileStore, FileStore.type() to the output.  So now e.g. on my box I see a line like this:

```
 -&gt; /l/es.logspins/target/J0/./tests-20150409230259-051/0000 has-space/TEST-haswell-CHILD_VM=[0]-CLUSTER_SEED=[2926863498862121027]-HASH=[AFBBDFEB30608]/nodes/0, free_space [260.5gb], usable_space [256.2gb], total_space [465gb], spins? [no], mount [/ (/dev/mapper/haswell--vg-root)], type [btrfs]
```
</comment><comment author="rjernst" created="2015-04-09T16:35:17Z" id="91284938">+1
</comment><comment author="rmuir" created="2015-04-09T16:46:32Z" id="91287913">looks great. we should try to get them to fix that FileStore bug one day!
</comment><comment author="mikemccand" created="2015-04-09T17:30:04Z" id="91302024">On 1.x backport I had to drop the "spins: " part because that's only in Lucene 5.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing hashCode method to RecoveryState#File</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10501</link><project id="" key="" /><description>it has an equals method - it also needs hashcode
</description><key id="67317634">10501</key><summary>Add missing hashCode method to RecoveryState#File</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T09:13:41Z</created><updated>2015-08-07T10:08:03Z</updated><resolved>2015-04-09T13:17:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-09T09:34:23Z" id="91174806">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot put percolator query on geo_shape field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10500</link><project id="" key="" /><description>Here is a script to reproduce the problem:

``` shell
ES_SERVER=http://localhost:9200
curl -XPUT $ES_SERVER/myindex?pretty

# Insert mapping with geo_shape type
curl -XPUT $ES_SERVER/myindex/_mapping/mytype?pretty -d '
{
    "mytype" : {
        "properties": {
            "location": {
                "type": "geo_shape",
                "tree": "quadtree",
                "precision": "1m"
            }
        }
    }
}
'

# Check that mapping is correct
curl -XGET $ES_SERVER/myindex/_mapping/?pretty

curl -XPUT $ES_SERVER/myindex/mytype/mydocument?pretty -d'
{
    "location" : {
        "type" : "point",
        "coordinates" : [1.44207, 43.59959]
    }
}
'

# Check that we can retrieve our document with a normal query
curl -XGET $ES_SERVER/myindex/mytype/_search?pretty -d '
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}
'
# Try to submit the same query to the percolator. FAIL!
curl -XPUT $ES_SERVER/myindex/.percolator/myquery?pretty -d '
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}
'

```

Everything goes as expected except the last request which returns:

``` json
{
  "error" : "PercolatorException[[myindex] failed to parse query [myquery]]; nested: QueryParsingException[[myindex] Field [location] is not a geo_shape]; ",
  "status" : 500
}
```

Following the documentation, it seems that it should have worked. I'll try to figure out the problem and submit a patch (whether it turns out to be a documentation patch or a code patch), but it would be great if someone could confirm if it's a bug or a misuse from my end.
</description><key id="67314895">10500</key><summary>Cannot put percolator query on geo_shape field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">amarandon</reporter><labels><label>:Percolator</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-04-09T09:04:49Z</created><updated>2016-01-29T10:57:22Z</updated><resolved>2016-01-29T10:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-12T14:39:06Z" id="92072113">Hi @amarandon 

I've just tried this on 1.5.0, and it works correctly. 

```
DELETE myindex

PUT /myindex?pretty

# Insert mapping with geo_shape type
PUT /myindex/_mapping/mytype?pretty
{
  "mytype": {
    "properties": {
      "location": {
        "type": "geo_shape",
        "tree": "quadtree",
        "precision": "1m"
      }
    }
  }
}

# Check that mapping is correct
GET /myindex/_mapping/?pretty


PUT /myindex/mytype/mydocument?pretty
{
  "location": {
    "type": "point",
    "coordinates": [
      1.44207,
      43.59959
    ]
  }
}

# Check that we can retrieve our document with a normal query

GET /myindex/mytype/_search?pretty
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}

# Try to submit the same query to the percolator. Works
PUT /myindex/.percolator/myquery?pretty
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}

# Percolate request works too
POST myindex/mytype/_percolate
{
  "doc": {
    "location": {
      "type": "point",
      "coordinates": [
        1.44207,
        43.59959
      ]
    }
  }
}
```
</comment><comment author="amarandon" created="2015-04-12T15:07:44Z" id="92077735">Hi @clintongormley Thanks for trying it out. I found out that the issue is triggered by having `index.percolator.map_unmapped_fields_as_string: true` in my config file. I'm in the process of migrating an app built against an earlier version of Elasticsearch and found out that I had to enable that option to keep it working because not all the percolator queries we record have corresponding mappings.
</comment><comment author="amarandon" created="2015-04-14T06:26:19Z" id="92642140">@clintongormley Does it still work for you with `index.percolator.map_unmapped_fields_as_string: true` in your config file?
</comment><comment author="clintongormley" created="2015-04-14T13:22:03Z" id="92827618">@amarandon if you're using that option, then I'm not surprised it fails... A geoshape query can't work on a string field.  You need to have the field specified in the mapping before you can create a percolator which uses it.  Otherwise (with that setting enabled) it will assume that the missing field is a string, and... fail
</comment><comment author="amarandon" created="2015-04-14T13:49:57Z" id="92855659">@clintongormley But I do have the field specified in the mapping before I create the percolator which uses it. In the test script I provided, we create that mapping explicitly and even check that it's been properly created with a GET request before trying to create a percolator query against it. In other words the geo_shape field is not unmapped and shouldn't be affected by that option.
</comment><comment author="clintongormley" created="2015-04-14T14:17:44Z" id="92868265">Gotcha!  And I can recreate, too.  Agreed, this is a bug.

@martijnvg please could you take a look

Here's the full recreation:

```
PUT /myindex?pretty
{
  "settings": {
    "index.percolator.map_unmapped_fields_as_string":true
  }
}

# Insert mapping with geo_shape type
PUT /myindex/_mapping/mytype?pretty
{
  "mytype": {
    "properties": {
      "location": {
        "type": "geo_shape",
        "tree": "quadtree",
        "precision": "1m"
      }
    }
  }
}

# Check that mapping is correct
GET /myindex/_mapping/?pretty


PUT /myindex/mytype/mydocument?pretty
{
  "location": {
    "type": "point",
    "coordinates": [
      1.44207,
      43.59959
    ]
  }
}

# Check that we can retrieve our document with a normal query

GET /myindex/mytype/_search?pretty
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}

# Try to submit the same query to the percolator. Works
PUT /myindex/.percolator/myquery?pretty
{
    "query": {
        "geo_shape": {
            "location": {
                "shape": {
                    "type": "envelope",
                    "coordinates": [[0, 50],[2, 40]]
                }
            }
        }
    }
}

# Percolate request works too
POST myindex/mytype/_percolate
{
  "doc": {
    "location": {
      "type": "point",
      "coordinates": [
        1.44207,
        43.59959
      ]
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-01-17T16:53:56Z" id="172350818">This still fails in 2.2 and master, but it fails when trying to PUT the percolator query with `location is not a geoshape`.

@martijnvg are percolator queries still parse-once, or are they parsed on each execution? If the latter, then could we just remove the `index.percolator.map_unmapped_fields_as_string` setting?
</comment><comment author="martijnvg" created="2016-01-17T21:15:08Z" id="172384041">@clintongormley Percolator queries are still parsed once. 

This issue was caused by a bug that if the 'map unmapped fields as strings' was enabled it would even substitute found fields with string fields! The `geo_shape` query has a hard check if what type of field is being returned from the mapping as therefor fails. Luckily this is easy to fix: #16043
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused code from TransportShardReplicationOperationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10499</link><project id="" key="" /><description>This code can only be executed if primary and replica were on the same node
in which case an exception or something of the likes would be more appropriate. 
I still find it a little tricky to figure out how replica requests end up in the indexing threadpool and will try to make a pr for that as well.
</description><key id="67312842">10499</key><summary>Remove unused code from TransportShardReplicationOperationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-04-09T08:56:08Z</created><updated>2015-04-14T10:59:43Z</updated><resolved>2015-04-14T07:46:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-04-14T07:46:15Z" id="92678993">closing in favour of #10582
</comment><comment author="s1monw" created="2015-04-14T10:59:43Z" id="92768501">this still applies to 1.x no?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose fine-grained metadata_read and metadata_write blocks settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10498</link><project id="" key="" /><description>The update indices settings api currently allows to add blocks to indices as follows:

`index.blocks.read` =&gt; block data reads
`index.blocks.write` =&gt; block data writes
`index.blocks.metadata` =&gt; block metadata reads and writes

We also have the `index.blocks.read_only` shortcut that allows to block data writes and metadata operations. As soon as #9203 is in the latter will only block data writes and metadata writes, meaning that metadata reads will be possible against a read_only index.

It might be beneficial to expose as setting the distinction between metadata read and write blocks made possible with #9203, as we do with data. The idea would be to deprecate the `index.blocks.metadata` and add `index.blocks.metadata_read` and `index.blocks.metadata_write`.

Opening this up for discussion.
</description><key id="67299369">10498</key><summary>Expose fine-grained metadata_read and metadata_write blocks settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>adoptme</label><label>enhancement</label></labels><created>2015-04-09T07:33:01Z</created><updated>2016-01-17T16:37:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-22T11:20:59Z" id="114071484">+1
</comment><comment author="clintongormley" created="2015-06-22T11:21:53Z" id="114071616">+1
</comment><comment author="clintongormley" created="2015-06-22T11:22:41Z" id="114071723">Also, change `read_only` to be a shortcut for setting `write: false` and `metadata_write: false` so that these settings don't interfere with each other
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use static logger name in Engine.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10497</link><project id="" key="" /><description>To ensure subclasses like MockInternalEngine which is in a different
package (test.engine) are logging under the same logger name this commit
moves to a static logger class to determin the logger name. This way
all subclasses of engine will log under `index.engine` which also plays
nicely with `@TestLogging` where log messages sometimes disappeared since
they were enabled for the `index.engine` package but not for `test.engine`
</description><key id="67281492">10497</key><summary>Use static logger name in Engine.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T05:02:38Z</created><updated>2015-08-07T10:08:03Z</updated><resolved>2015-04-09T08:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-09T05:03:21Z" id="91116170">LGTM, glad this will be fixed!
</comment><comment author="bleskes" created="2015-04-09T07:19:59Z" id="91134014">w00t! (i.e., LGTM)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting captures all fields regardless of a search query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10496</link><project id="" key="" /><description>When searching for document with highlighting turned on all found keywords are highlighted regardless of a field. This means that when searching for some document with a **text** field "africa" and a **name** field "return" ES will highlight "africa" and "return" tokens in both text and title:

```
...
"highlight": {
   "name": [
       "[+highlight+]Return[-highlight-] from [+highlight+]Africa[-highlight-]"
   ],
   "text": [
      "[+highlight+]Return[-highlight-] from [+highlight+]Africa[-highlight-]\r\nDate:\r\n"
   ]
}
...
```

However "return" token has to be highlighted within **name** and "africa" within **text** field only.

![highlighting_issue_red](https://cloud.githubusercontent.com/assets/1760979/7059101/45f99422-deae-11e4-9fb9-41bcaa8923b8.png)

```
[ES]: 
{
   "status": 200,
   "version": {
      "number": "1.4.2",
      "lucene_version": "4.10.2"
   },
   "tagline": "You Know, for Search"
}
```

```
[FULL QUERY]:
POST documents/_search
{
   "highlight": {
      "pre_tags": [
         "[+highlight+]"
      ],
      "post_tags": [
         "[-highlight-]"
      ],
      "number_of_fragments": 0,
      "fields": {
         "text": {},
         "name": {}
      }
   },
   "query": {
      "filtered": {
         "query": {
            "bool": {
               "must": [
                  {
                     "bool": {
                        "must": [
                           {
                              "bool": {
                                 "must": [
                                    {
                                       "query_string": {
                                          "query": "africa",
                                          "fields": [
                                             "text"
                                          ],
                                          "default_operator": "and",
                                          "analyze_wildcard": true
                                       }
                                    }
                                 ]
                              }
                           },
                           {
                              "bool": {
                                 "must": [
                                    {
                                       "query_string": {
                                          "query": "return",
                                          "fields": [
                                             "name"
                                          ],
                                          "default_operator": "and",
                                          "analyze_wildcard": true
                                       }
                                    }
                                 ]
                              }
                           }
                        ]
                     }
                  }
               ]
            }
         },
         "filter": {
            "ids": {
               "values": [
                  "cc1db8f5-80bd-48a1-9b3e-3256dd99b0db"
               ]
            }
         }
      }
   }
}
```
</description><key id="67260601">10496</key><summary>Highlighting captures all fields regardless of a search query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuriyfilonov</reporter><labels /><created>2015-04-09T01:57:15Z</created><updated>2016-10-27T07:24:12Z</updated><resolved>2015-04-09T07:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-09T07:37:29Z" id="91138351">Hi @Singletone what you describe is the default behaviour, you should be able to use the `require_field_match` option to enforce highlighting on the fields that hold query matches only. You can find that in the [documentation](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#field-match).
</comment><comment author="yuriyfilonov" created="2015-04-09T23:05:09Z" id="91377491">Hi @javanna Thanks for a tip. After turning require_field_match on everything works fine.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Eclipse fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10495</link><project id="" key="" /><description>Various minor fixes to make the project buildable in Eclipse.
</description><key id="67257755">10495</key><summary>Eclipse fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dpursehouse</reporter><labels><label>build</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-09T01:32:15Z</created><updated>2015-06-07T11:44:59Z</updated><resolved>2015-04-10T00:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-09T07:50:33Z" id="91142250">Thanks for proposing fixes @dpursehouse, I left some questions inline.
</comment><comment author="dpursehouse" created="2015-04-09T08:55:46Z" id="91157577">Note: I have signed the CLA (as an individual contributing under an existing company contributor agreement) and an email has been sent to get me added to the contributors list.
</comment><comment author="jpountz" created="2015-04-09T09:05:26Z" id="91163223">@colings86 @markharwood @rmuir I think you are using Eclipse, do you have similar issue as @dpursehouse (see in particular comments on the pom.xml file)
</comment><comment author="colings86" created="2015-04-09T10:39:13Z" id="91190190">@dpursehouse instead of modifying the plugins in the pom.xml directly to remove the errors in the maven integration it is often better to use the eclipse lifecycle mapping plugin to tell eclipse how to deal with these plugins. you can see an example of this [here](https://github.com/elastic/elasticsearch/pull/10495/files#diff-600376dffeb79835ede4a0b285078036R1547) where we are telling eclipse to execute the copy-dependencies goal of the maven-dependancy-plugin as part of a full or incremental build. You can also instruct eclipse to ignore a plugin by using `&lt;ignore/&gt;` instead of `&lt;execute/&gt;`.

The two plugins that are erroring I believe are the maven-antrun-plugin and the maven-resources-plugin. Is that what you are seeing? The ant-run plugin does some validation so might be useful to execute as part of the eclipse build but the resources plugin is only relevant to the creation of the deb and rpm packages so can be ignored for the eclipse build.

Maybe you could update your PR to use this approach since it does not involve changing the pom for non-eclipse builds?
</comment><comment author="rmuir" created="2015-04-09T11:25:26Z" id="91199415">@jpountz eclipse is working fine for me, and has been for months.
</comment><comment author="colings86" created="2015-04-10T15:54:38Z" id="91599317">@dpursehouse I ended up fixing this using some of your code is https://github.com/elastic/elasticsearch/commit/97a9b4ec2fb6691437c86b694b77d56e81efcaa1

This should mean that you can now import the project in eclipse using the 'import existing maven projects' option.

Thanks for raising this problem and for providing code for the fix.

This has been merged into master, 1.x, and 1.5
</comment><comment author="dpursehouse" created="2015-04-13T00:45:15Z" id="92157752">@colings86 Thanks.  It works perfectly now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException for some shards when the _name field is set for a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10494</link><project id="" key="" /><description>Here's the query:

```
{
    "query": {
        "filtered": {
            "query": {
                "bool": {
                    "should": [
                        {
                            "query_string": {
                                "query": "someField1:(someValue1 OR someValue2) AND someField2:(someValue3 OR someValue4)",
                                _name:"test"
                            }
                        }
                    ]
                }
            },
            "filter": {
                "bool": {
                    "must": [
                        {
                            "match_all": {}
                        }
                    ]
                }
            }
        }
    }
}
```

Here's the response:

```
{
  "took": 5,
  "timed_out": false,
  "_shards": {
    "total": 6,
    "successful": 4,
    "failed": 2,
    "failures": [
      {
        "index": "my_index",
        "shard": 1,
        "status": 500,
        "reason": "RemoteTransportException[[elasticsearch-node][inet[/0.0.0.0:9300]][indices:data/read/search[phase/fetch/id]]]; nested: NullPointerException; "
      },
      {
        "index": "my_index",
        "shard": 2,
        "status": 500,
        "reason": "RemoteTransportException[[elasticsearch-node][inet[/0.0.0.0:9300]][indices:data/read/search[phase/fetch/id]]]; nested: NullPointerException; "
      }
    ]
  }
}
```

There are no shard failures when I remove the _name attribute for my query.
</description><key id="67190315">10494</key><summary>NullPointerException for some shards when the _name field is set for a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Asimov4</reporter><labels><label>feedback_needed</label></labels><created>2015-04-08T18:30:11Z</created><updated>2015-08-27T07:48:11Z</updated><resolved>2015-08-26T19:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-10T18:14:08Z" id="91641464">@Asimov4 Can you provide your mappings?
</comment><comment author="clintongormley" created="2015-04-13T10:32:53Z" id="92302977">@Asimov4 and the stack trace in the elasticsearch logs please
</comment><comment author="Asimov4" created="2015-04-13T18:16:56Z" id="92449691">Here's our mapping.
I just saw that it looks like someone indexed a query by mistake.

```
{
  "_default_": {
    "dynamic_date_formats": [
      "yyyy-MM-dd HH:mm:ss",
      "dd-MM-yyyy",
      "dd-MMM-yy",
      "date_optional_time"
    ],
    "dynamic_templates": [
      {
        "text_template": {
          "mapping": {
            "analyzer": "english",
            "type": "string"
          },
          "match_mapping_type": "string",
          "match": "text"
        }
      },
      {
        "string_template": {
          "mapping": {
            "index": "not_analyzed",
            "type": "string"
          },
          "match_mapping_type": "string",
          "match": "*"
        }
      }
    ],
    "properties": {}
  },
  "DocType1": {
    "properties": {
      "field01": {
        "type": "string"
      },
      "suggest": {
        "max_input_length": 50,
        "payloads": false,
        "analyzer": "simple",
        "preserve_position_increments": true,
        "type": "completion",
        "preserve_separators": true
      }
    }
  },
  "DocType2": {
    "properties": {
      "field1": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field2": {
        "type": "long"
      },
      "field3": {
        "type": "long"
      },
      "field4": {
        "type": "long"
      },
      "field5": {
        "type": "long"
      },
      "field6": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field7": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field8": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field9": {
        "type": "string"
      },
      "field10": {
        "type": "long"
      },
      "field11": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field12": {
        "index": "not_analyzed",
        "type": "string"
      },
      "filtered": {
        "properties": {
          "filter": {
            "properties": {
              "bool": {
                "properties": {
                  "must": {
                    "properties": {
                      "bool": {
                        "properties": {
                          "should": {
                            "properties": {
                              "missing": {
                                "properties": {
                                  "field": {
                                    "index": "not_analyzed",
                                    "type": "string"
                                  }
                                }
                              },
                              "term": {
                                "properties": {
                                  "field21": {
                                    "index": "not_analyzed",
                                    "type": "string"
                                  }
                                }
                              }
                            }
                          }
                        }
                      },
                      "match_all": {
                        "type": "object"
                      }
                    }
                  }
                }
              }
            }
          },
          "query": {
            "properties": {
              "bool": {
                "properties": {
                  "should": {
                    "properties": {
                      "query_string": {
                        "properties": {
                          "_name": {
                            "index": "not_analyzed",
                            "type": "string"
                          },
                          "query": {
                            "index": "not_analyzed",
                            "type": "string"
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      },
      "field13": {
        "type": "long"
      },
      "field14": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field15": {
        "type": "string"
      },
      "field16": {
        "type": "string"
      },
      "field17": {
        "type": "long"
      },
      "field18": {
        "type": "long"
      },
      "field19": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field20": {
        "type": "string"
      },
      "field21": {
        "type": "nested",
        "properties": {
          "field211": {
            "type": "string"
          },
          "field212": {
            "type": "string"
          }
        }
      },
      "field22": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field23": {
        "type": "string"
      },
      "field24": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field25": {
        "properties": {
          "field251": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field252": {
            "type": "long"
          },
          "field253": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field254": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field255": {
            "index": "not_analyzed",
            "type": "string"
          }
        }
      },
      "field30": {
        "type": "long"
      },
      "field40": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field41": {
        "type": "nested",
        "properties": {
          "field42": {
            "type": "string"
          },
          "field43": {
            "type": "string"
          }
        }
      },
      "field44": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field45": {
        "analyzer": "english",
        "type": "string"
      },
      "field46": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field47": {
        "type": "string"
      },
      "field48": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field49": {
        "type": "long"
      },
      "field50": {
        "type": "string"
      },
      "field51": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field52": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field53": {
        "type": "string"
      },
      "field54": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field55": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field56": {
        "type": "long"
      },
      "field57": {
        "type": "long"
      },
      "field58": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field59": {
        "type": "string"
      },
      "field60": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field61": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field62": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field63": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field64": {
        "type": "long"
      },
      "field65": {
        "type": "string"
      },
      "field66": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field67": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field68": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field69": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field70": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field71": {
        "properties": {
          "field72": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field73": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field74": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field75": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field76": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field77": {
            "index": "not_analyzed",
            "type": "string"
          }
        }
      },
      "field78": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field79": {
        "type": "long"
      },
      "field80": {
        "properties": {
          "field81": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field82": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field83": {
            "index": "not_analyzed",
            "type": "string"
          }
        }
      },
      "field84": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field85": {
        "type": "string"
      },
      "field86": {
        "type": "long"
      },
      "field87": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field88": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field89": {
        "type": "long"
      },
      "field90": {
        "type": "string"
      },
      "field91": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field92": {
        "type": "long"
      },
      "field93": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field94": {
        "type": "string"
      },
      "field95": {
        "type": "string"
      },
      "field96": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field97": {
        "type": "string"
      },
      "field98": {
        "type": "long"
      },
      "field99": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field100": {
        "type": "string"
      },
      "field101": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field102": {
        "type": "long"
      },
      "field103": {
        "type": "string"
      },
      "field104": {
        "type": "string"
      },
      "field105": {
        "type": "long"
      },
      "field106": {
        "type": "long"
      },
      "field107": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field108": {
        "type": "long"
      },
      "field109": {
        "type": "long"
      },
      "field110": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field111": {
        "type": "long"
      },
      "field112": {
        "type": "string"
      },
      "field113": {
        "type": "long"
      },
      "field114": {
        "type": "long"
      },
      "field115": {
        "type": "long"
      },
      "field116": {
        "type": "long"
      },
      "field117": {
        "type": "long"
      },
      "field118": {
        "type": "string"
      },
      "field119": {
        "type": "string"
      },
      "field120": {
        "analyzer": "english",
        "type": "string"
      },
      "field121": {
        "type": "string"
      },
      "field122": {
        "type": "string"
      },
      "field123": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field124": {
        "type": "long"
      },
      "field125": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field126": {
        "type": "string"
      },
      "field127": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field128": {
        "properties": {
          "field129": {
            "index": "not_analyzed",
            "type": "string"
          },
          "field130": {
            "index": "not_analyzed",
            "type": "string"
          }
        }
      },
      "field131": {
        "index": "not_analyzed",
        "type": "string"
      },
      "field132": {
        "index": "not_analyzed",
        "type": "string"
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-04-13T18:25:50Z" id="92452022">@Asimov4 it's quite difficult to figure out what is going on when you've renamed all of the fields :)

Could you upload that stack trace, and possibly a simple but complete recreation?
</comment><comment author="Asimov4" created="2015-04-13T18:40:05Z" id="92455887">Sorry about that. I was trying to scrub the data. For the same reason, the stack trace is not published for security reasons in the stack I am troubleshooting.

I am also having a hard time writing an example that would highlight the problem. It looks like a tricky bug as the NullPointerException is triggered when some values are there or not.

For example:
`someField1:(someValue1 OR someValue2 OR someValue3) AND someField2:(someValue3 OR someValue4)`
will trigger it.

But 
`someField1:(someValue1 OR someValue3) AND someField2:(someValue3 OR someValue4)`
and
`someField1:(someValue1 OR someValue2 OR someValue3 OR someValue4) AND someField2:(someValue3 OR someValue4)`
will not.

I'll work on recreating the bug in a sample environment if I can.
</comment><comment author="clintongormley" created="2015-04-14T12:38:32Z" id="92796719">@Asimov4 weird :)  appreciate the effort on trying to recreate
</comment><comment author="adichad" created="2015-05-20T04:03:00Z" id="103748671">I've come up against the same issue. for _some_ queries (not all), when I set the queryName parameter via the java api, I get failures in the shards section of the response:-

&lt;code&gt;
"_shards": {
"total": 18,
"successful": 4,
"failed": 14,
"failures": [
{
"index": "b",
"shard": 0,
"status": 500,
"reason": "RemoteTransportException[[search04][inet[/172.30.102.220:9300]][indices:data/read/search[phase/fetch/id]]]; nested: NullPointerException; "
},
&lt;/code&gt;
also, I get no hits in the hits section when this happens. 

all works well when the query name is removed. I can share the mapping, index settings, and queries that fail (and ones that don't).

I want to actually try to use matched queries to do custom sort-comparators via native scripts, and this isn't helping.

While I haven't delved into this code at all, it could have something to do with certain subqueries not being evaluated at all by lucene under some circumstances. I'll look around in code and post whatever I can that's relevant.
</comment><comment author="clintongormley" created="2015-05-25T12:48:47Z" id="105227415">Hi @adichad 

If you could provide a recreation, it would be very helpful.  I've been unable to replicate this.
</comment><comment author="Asimov4" created="2015-08-04T17:35:44Z" id="127686959">@adichad any chance that you can share the mapping, index settings, and queries that fail (and ones that don't)?
</comment><comment author="jpountz" created="2015-08-26T19:25:26Z" id="135145779">Closing due to lack of feedback
</comment><comment author="javanna" created="2015-08-27T07:48:11Z" id="135325391">As a side note, in the query-refactoring branch we have improved how `_name` and `boost` work and also prevented any NPE from being thrown when either of them is set and the query ends up being null, which is most probably the case here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster.routing.allocation.enable = None is ignored during rolling restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10493</link><project id="" key="" /><description>We are going through the process of making some changes to our ES cluster (adding new plugins/minor config tweaks). Following the process outlined at:
http://www.elastic.co/guide/en/elasticsearch/guide/master/_rolling_restarts.html, we set `cluster.routing.allocation.enable` to `none` before restarting each node; in the past (&lt;1.4) this has worked a charm, and the shards from the restarted node have remained unassigned until re-enabled. However, at 1.5 this setting is seemingly ignored, and the shards are instantly re-assigned to the remaining nodes in the cluster.

The result is that restarting each node is taking ~30-60 minutes rather than the ~5 it used to take, because we now have to rebalance everything after all the shards are re-assigned, mostly on the wrong nodes.

Any help would be greatly appreciated!
</description><key id="67176771">10493</key><summary>cluster.routing.allocation.enable = None is ignored during rolling restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fizzadar</reporter><labels><label>feedback_needed</label></labels><created>2015-04-08T17:16:55Z</created><updated>2015-08-26T19:26:07Z</updated><resolved>2015-08-26T19:26:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rabidscorpio" created="2015-04-09T17:11:09Z" id="91294764">I've also seen something similar with 1.5.0, when I set `cluster.routing.allocation.enable` to `all` after a restart, there's inevitably at least one shard that's relocating and a number of shards that are in initializing state for a while.  At the same time, the shards that became unassigned while the node was restarting take a long time to be reassigned to the node.  What used to take a few mins per node now takes 30-60 mins per node.
</comment><comment author="clintongormley" created="2015-04-12T15:24:47Z" id="92079639">@Fizzadar I've just tried this on 1.5.0. I started 4 nodes, created 4 indices, and then ran this:

```
PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.enable": "none"
  }
}
```

Then I killed 3 nodes, which put the cluster into red state, then I restarted them and the cluster went yellow, as expected.  But it only went to green when I reenabled allocation.

What exactly are you doing, and what are you seeing that differs from what you expect?
</comment><comment author="jpountz" created="2015-08-26T19:26:07Z" id="135145925">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API produces invalid search request with AggregationBuilders.topHits("topHitsAgg").setNoFields()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10492</link><project id="" key="" /><description>The Java API allows to construct a top_hits aggregation without fields using the method setNoFields():

```
AggregationBuilders.topHits("topHitsAgg").setNoFields()
```

This is translated to

```
{"top_hits":{"fields":[]}}
```

Unfortunately, the top_hits aggregation does not support "fields". Executing a request built like that leads to a SearchPhaseExecutionException:

```
...Parse Failure [Unknown key for a START_ARRAY in [topHitsAgg]: [fields].]];
```

I think this is misleading. I see two options to avoid this confusion:
- the top_hits API allows setting "fields" (at least to an empty array)
- the Java API does not offer this method

It is obviously not a big problem, I just stumbled over this and thought it's worth mentioning. I tested this behavior with ES 1.4.1 and 1.5.0. Here is a gist to reproduce it:
https://gist.github.com/hkorte/64e630ec579c15697c6d
</description><key id="67175052">10492</key><summary>Java API produces invalid search request with AggregationBuilders.topHits("topHitsAgg").setNoFields()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hkorte</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-08T17:06:00Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2016-01-17T16:36:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wbowling" created="2015-11-09T12:56:17Z" id="155055266">This is probably fixed by #12962 as top hits now supports "fields".
</comment><comment author="clintongormley" created="2016-01-17T16:36:52Z" id="172347595">Closed by #12962
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default script language to expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10491</link><project id="" key="" /><description>Since we have non sandboxable dynamic scripts disabled by default, and the sandbox is being removed for groovy (#10480), I think we should change the default language to expression scripts.  This will at least be a better behavior out-of-the-box for users with simple scripts (e.g. #10474).

We should also consider moving groovy back to a separate plugin (separate issue, but just mentioning here). In order to make expressions the default, we will need to make the lucene expression deps non-optional (and possibly shade in asm so as not to guard against conflicts).  Having groovy and expressions in the default setup would then seem overkill (at least at some point in the future, when expressions become more powerful).
</description><key id="67170423">10491</key><summary>Change default script language to expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>breaking</label><label>discuss</label></labels><created>2015-04-08T16:40:28Z</created><updated>2016-01-17T16:35:54Z</updated><resolved>2016-01-17T16:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-08T16:57:31Z" id="90974598">+1 for moving to expressions by default

&gt; We should also consider moving groovy back to a separate plugin

I'm -1 on this, because it's still nice to have a "full-power" scripting language that doesn't require doing a node restart to use (just stick some scripts on disk and they are usable)
</comment><comment author="dakrone" created="2015-04-08T20:55:06Z" id="91034392">To clarify, I'm only -1 on removing Groovy if Expressions doesn't support everything we need, if we get it to the point that it does, I think removing Groovy to make it a plugin would be totally fine.
</comment><comment author="javanna" created="2015-04-09T07:40:07Z" id="91139218">I agree with @dakrone .  The only reason why I am not totally sold on making expressions the default is that it can't do updates, but I see how it would at least work out-of-the-box for other usecases without requiring to use file scripts.
</comment><comment author="rjernst" created="2015-04-11T01:03:19Z" id="91734693">I think making it work out of the box is more important than extra work (changing the default) for those that already have to do work to enable scripts (either by writing them on each host or enabling dynamic).
</comment><comment author="clintongormley" created="2015-08-16T11:08:22Z" id="131525279">I don't think we should make another change to the default scripting language until we have a proper replacement which works across the board.  Closing this for now.
</comment><comment author="rmuir" created="2015-09-10T17:27:05Z" id="139317737">I reopened the issue. Groovy is not even working with java 9. I think we should reconsider.
</comment><comment author="clintongormley" created="2015-09-19T15:08:04Z" id="141678570">Java 9 is due to be released in September 2016. I hope to have https://github.com/elastic/elasticsearch/issues/13084 in place long before that. Once merged, we can give it some time to iron out the issues, then switch to that as the default.  

I don't see the point in switching the default to expressions, and then switching again a couple of months later.
</comment><comment author="clintongormley" created="2015-09-19T15:09:23Z" id="141678645">@rmuir I think you said to me that you wanted to move groovy out to its own module, in order to clean up some security issues in core? That work can continue, without us changing the default scripting language for now, no?
</comment><comment author="rmuir" created="2015-09-19T15:12:36Z" id="141678775">I want to move all scripting engines (including expressions) out of the core into plugins. This is necessary due to the security issues around classloaders. 

Personally I think, if we want to bundle certain plugins with our distribution by default, that's what we should do (e.g. distro build will bin/plugin install ones before zipping or whatever), but we keep the separation this way.
</comment><comment author="dadoonet" created="2015-09-19T15:53:15Z" id="141682876">Once we will split core in modules it will be super easy to decide if a module is embedded or exposed as a plugin.
</comment><comment author="clintongormley" created="2015-09-19T16:00:15Z" id="141683265">@rmuir ++
</comment><comment author="rmuir" created="2015-09-19T16:09:35Z" id="141684000">I am not going to wait on some big modularization of the core. I am going to proceed with fixing the security issues around scripting in master.
</comment><comment author="clintongormley" created="2016-01-17T16:35:54Z" id="172347471">Now that we have a new safe scripting language merged into master, which will become the new default as soon as it is ready, we can close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>URI search not returning results when &amp;fields used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10490</link><project id="" key="" /><description>Running into an issue with URI searches not returning expected results after upgrading from 0.90.9 to 1.4.1

The issue is that subfields are not returned when specified with "&amp;fields".  I have confirmed that the data is there by doing the same search without the &amp;fields specifications and get the Meter.geometry.coordinates.  I am not seeing anything in the release notes for 1.4.1 that indicate any change in URI search functionality.

Hoping someone has run into this already and can tell me how to specify subfields for results sets other than the &amp;fields method or how to get the &amp;fields method to work again in 1.4.1+

Thanks in advance.

0.90.9 URI Search:
https://contoso.com/customer/v1/tapsandmeters?q=Meter.servicePointId:0865540026&amp;fields=Meter.geometry.coordinates

0.90.9 URI Search Results:
{
took: 2,
timed_out: false,
_shards: 
{
total: 3,
successful: 3,
failed: 0
},
hits: 
{
total: 1,
max_score: 11.723421,
hits: 
[
{
_index: "customer12345",
_type: "TapAndMeter",
_id: "086554009008655400270865540026",
_score: 11.723421,
fields: 
{
Meter.geometry.coordinates: 
{
lon: -104.959796299,
lat: 39.7702803882
}
}
}
]
}
}

1.4.1 URI Search:
https://newercontoso.comcustomer/v1/tapsandmeters?q=Meter.servicePointId:0865540026&amp;fields=Meter.geometry.coordinates

1.4.1 URI Search Results:
{
took: 5,
timed_out: false,
_shards: 
{
total: 3,
successful: 3,
failed: 0
},
hits: 
{
total: 1,
max_score: 11.856463,
hits: 
[
{
_index: "customer12345",
_type: "TapAndMeter",
_id: "086554009008655400270865540026",
_score: 11.856463
}
]
}
}
</description><key id="67165012">10490</key><summary>URI search not returning results when &amp;fields used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tnesavich</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-04-08T16:14:33Z</created><updated>2016-01-17T16:33:47Z</updated><resolved>2016-01-17T16:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tnesavich" created="2015-04-08T18:51:19Z" id="91002603">Interesting thing to note. ... This issue is only for fields of Geo Point Type (geo_point)
http://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-geo-point-type.html

Tests of other fields work.
</comment><comment author="clintongormley" created="2015-04-11T14:58:53Z" id="91867520">Hi @tnesavich 

These days we prefer the `_source` parameter to extract the original values from the `_source` field, eg:

```
https://newercontoso.comcustomer/v1/tapsandmeters?q=Meter.servicePointId:0865540026&amp;_source=Meter.geometry.coordinates
```

If you want to retrieve the actual coordinates, you could set the `coordinates` field to `store:true`, in which case it'll return an array of indexed values:

```
PUT test 
{
  "mappings": {
    "test": {
      "properties": {
        "loc": {
          "type": "geo_point",
          "store": true
        }
      }
    }
  }
}

PUT test/test/1
{
  "loc": {"lat":1, "lon": 2}
}
PUT test/test/2
{
  "loc": [1,2]
}

GET /test/test/_search?fields=loc
```

returns:

```
  "hits": [
     {
        "_index": "test",
        "_type": "test",
        "_id": "1",
        "_score": 1,
        "fields": {
           "loc": [
              "1.0,2.0"
           ]
        }
     },
     {
        "_index": "test",
        "_type": "test",
        "_id": "2",
        "_score": 1,
        "fields": {
           "loc": [
              "2.0,1.0"
           ]
        }
     }
  ]
```

That said, the `fields` parameter should fall back to extracting the values from the `_source` field if the field is not stored.  Thoughts?
</comment><comment author="clintongormley" created="2016-01-17T16:33:47Z" id="172347363">The `fields` parameter no longer falls back to extracting from source.  https://github.com/elastic/elasticsearch/pull/15017

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix tokenizer settings in SynonymTokenFilterFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10489</link><project id="" key="" /><description>Add test for synonym with tokenizer settings and fix.

Reported by Elasticsearch ML.

At Elasticsearch 1.3, the following setting works.  "bigramTokenizer" of synonymTest gets (min_gram/max_gram) settings.

```
curl -XPUT localhost:9200/test -d '{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "bigram_analyzer" : {
          "type" : "custom",
          "tokenizer" : "bigramTokenizer",
          "filter" : ["synonymTest"]
        }
      },
      "tokenizer" : {
          "bigramTokenizer" : {
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 2
        }
      },
      "filter" : {
        "synonymTest" : {
          "type" : "synonym",
          "synonyms_path" : "synonym.txt",
          "tokenizer" : "bigramTokenizer",
          "min_gram" : 2,
          "max_gram" : 2
        }
      }
    }
  }
}'
```

At Elasticsearch 1.5, it does not work. 
</description><key id="67161405">10489</key><summary>Fix tokenizer settings in SynonymTokenFilterFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T15:55:09Z</created><updated>2015-08-07T10:08:03Z</updated><resolved>2015-04-14T02:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-08T17:06:06Z" id="90976516">LGTM
</comment><comment author="rjernst" created="2015-04-08T17:06:28Z" id="90976580">This should have fix label 2.0.0 as well right?
</comment><comment author="johtani" created="2015-04-09T16:07:00Z" id="91275682">@rjernst Thanks for reviewing. Exactly, I add 2.0.0 label.
And I think we should have fix label 1.5.x. What do you think?
</comment><comment author="rjernst" created="2015-04-09T16:19:05Z" id="91279084">Sure @johtani I think this fix is harmless enough to backport to 1.5.2.
</comment><comment author="johtani" created="2015-04-14T02:58:50Z" id="92572894">merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible missing dependency: Apache HttpCore 4.3.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10488</link><project id="" key="" /><description>When upgrading our dependencies, we encountered this error in our stack trace:

```
Exception Thrown
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: org/apache/http/util/Args
  at org.elasticsearch.discovery.ec2.Ec2Discovery.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.discovery.ec2.Ec2Discovery
  while locating org.elasticsearch.discovery.Discovery
    for parameter 3 at org.elasticsearch.node.service.NodeService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.node.service.NodeService
Caused by: java.lang.NoClassDefFoundError: org/apache/http/util/Args
```

After checking through the pomfile for elasticsearch-cloud-aws, we didn't find any references to a specific version of Apache HTTPCore (so we weren't able to directly identify which version elasticsearch depends on).

We are using elasticsearch 1.5.0 and cloud-aws 2.5.0 (per the helpful README in cloud-aws!); we were using these in a previous release as well, but had another dependency that happened to provide HTTPCore 4.3.x as a transitive dependency. When that lib was upgraded, it specified HTTPCore 4.2.5, which appears not to contain the `org.apache.http.util.Args` class.

So when we were no longer lucky enough to have another dependency transitively satisfying cloud-aws's dependency on HTTPCore 4.3.x, since cloud-aws doesn't specify HTTPCore 4.3.x in its pomfile, we got a NoClassDefFound error.

It was something like this before:

```
                     OUR_APP
          /                         \
         /                           \
        /                             \
    DepA                     ElasticSearch
       |                               /  (dep not in pomfile)
 HTTPCore &lt;---------------------------/        
 (no version specified, so grabs 4.3.x)
```

And now we have this:

```
                     OUR_APP
          /                         \
         /                           \
        /                             \
    DepA                     ElasticSearch
       |                               /  (dep not in pomfile)
 HTTPCore &lt;---------------------------/        
   (4.2.5, breaks ElasticSearch)
```

Should this dependency be added to the pomfile?
</description><key id="67146903">10488</key><summary>Possible missing dependency: Apache HttpCore 4.3.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">spencerwi</reporter><labels /><created>2015-04-08T14:47:39Z</created><updated>2015-04-13T09:43:46Z</updated><resolved>2015-04-13T09:43:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-10T11:28:38Z" id="91524549">@dadoonet could you take a look at this please? (should probably move to https://github.com/elastic/elasticsearch-cloud-aws ?)
</comment><comment author="tlrx" created="2015-04-10T11:56:31Z" id="91531803">As far as I know, elasticsearch has a transitive dependency on `httpcore` with the scope `test`:

```
$ mvn dependency:tree | grep http
[INFO] ------------------------------------------------------------------------
[INFO] Building elasticsearch 1.5.0
[INFO] ------------------------------------------------------------------------
...
[INFO] +- org.apache.httpcomponents:httpclient:jar:4.3.5:test
[INFO] |  +- org.apache.httpcomponents:httpcore:jar:4.3.2:test
```

This dependency is not required at runtime, so I don't see why this dependency should be declared in the pom file.

This issue is a duplicate of https://github.com/elastic/elasticsearch-cloud-aws/issues/200, maybe we can close this one and continue on the cloud-aws issue?
</comment><comment author="clintongormley" created="2015-04-13T09:43:46Z" id="92293681">@tlrx agreed. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dangling indices may not be imported if found on elected master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10487</link><project id="" key="" /><description>Dangling indices are indices that are found on disk but do not exist in the cluster state. To be safe, those are imported into the cluster state instead of automatically deleting them. This is done by reading their meta data and sending the master node a request to add them to the current meta data. 

At the moment, this request may be lost if the dangling index is discovered on the master and the master only (which means the master imported it's cluster state from another node it deemed better, or index is copied into alive cluster, like our tests do ). This is because the dangling request import logic doesn't take into account that the local node might be the master and tries to send the request via the transport layer to itself. However, the local node may not be connected (we only connect if the local node is a unicast host target or after 10s have passed and `InternalClusterService.ReconnectToNodes` kicks in).  

A generic solution was added in #10350 but it had to be pulled out, we now need to fix this in the dangling index code.
</description><key id="67140614">10487</key><summary>Dangling indices may not be imported if found on elected master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>adoptme</label><label>bug</label></labels><created>2015-04-08T14:18:25Z</created><updated>2016-01-18T13:20:52Z</updated><resolved>2016-01-18T10:05:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-17T16:31:46Z" id="172347260">@ywelsch would this be of interest to you?
</comment><comment author="bleskes" created="2016-01-18T10:05:24Z" id="172484961">this can be closed now. #10350 was pulled out of 1.x and we opened this issue to track it (note the original 1.6.0 label). Now that 2.x is the release version this isn't an issue anymore. I'm closing it and adapting the labels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster state: return `routing_nodes` only when requested through specific flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10486</link><project id="" key="" /><description>For backwards compatibility reasons routing_nodes were previously printed out when routing_table was requested, together with the actual routing_table. Now they are printed out only when requested through `routing_nodes` flag.

 Relates to #10412
</description><key id="67135252">10486</key><summary>Cluster state: return `routing_nodes` only when requested through specific flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:REST</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T13:51:27Z</created><updated>2015-06-06T15:38:18Z</updated><resolved>2015-04-08T14:10:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-08T13:52:39Z" id="90923411">@clintongormley can you have a quick look please?
</comment><comment author="clintongormley" created="2015-04-08T14:00:22Z" id="90925545">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse alias filters at search time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10485</link><project id="" key="" /><description>Currently alias filters are parsed only when the alias is created.  This results in problems like:
- #8534 Needing to resolve `now` at search time
- #10135 Inability to support parent-child clauses

This would be solved by simply parsing alias filters at search time instead.
</description><key id="67126046">10485</key><summary>Parse alias filters at search time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aliases</label><label>adoptme</label><label>enhancement</label></labels><created>2015-04-08T13:09:16Z</created><updated>2015-07-01T19:21:53Z</updated><resolved>2015-07-01T19:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Sync translog before closing engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10484</link><project id="" key="" /><description>If the translog is buffered we must make sure everything is synced to disk
before we rollback the writer otherwise we open a window for potential dataloss due
to stupid errors preventing the translog from being closed.
</description><key id="67125119">10484</key><summary>Sync translog before closing engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>bug</label><label>resiliency</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T13:05:19Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-08T16:00:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-08T13:05:51Z" id="90908776">@bleskes can you take a look?
</comment><comment author="bleskes" created="2015-04-08T14:02:28Z" id="90926008">LGTM. Can we change the title to sync translog when closing engine?

Also, to be clear - on normal shutdown we flush so this is mostly relevant to closing an index right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add profile name to TransportChannel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10483</link><project id="" key="" /><description>The `NettyTransportChannel` has a profile name, which is the name of the profile that the channel belongs to. These profiles allow for binding to multiple addresses and/or ports. 

After #10350 there is a new `TransportChannel` implementation, `DirectResponseChannel`, used in the netty transport, which does not have the profile information. This implementation should provide a profile name to make it easier to deal with logic based on profiles in the netty transport. The name could be something like `__local__`.

Adding the `getProfileName` method to the `TransportChannel` interface may make sense here as other transport implementations may implement the profile concept and those that don't can just return a default name.
</description><key id="67123434">10483</key><summary>add profile name to TransportChannel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T12:56:37Z</created><updated>2015-05-21T16:43:55Z</updated><resolved>2015-05-21T16:43:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-08T14:27:19Z" id="90932500">One more note that whether we use `__local__` or something else, we need to make sure we don&#8217;t allow people to use that name when configuring profiles in their settings (`transport.profiles.*`). It also seems we use `.client` in one place. `.local` then perhaps?

&gt; On 08 Apr 2015, at 14:56, Jay Modi notifications@github.com wrote:
&gt; 
&gt; The NettyTransportChannel has a profile name, which is the name of the profile that the channel belongs to. These profiles allow for binding to multiple addresses and/or ports.
&gt; 
&gt; After #10350 there is a new TransportChannel implementation, DirectResponseChannel, used in the netty transport, which does not have the profile information. This implementation should provide a profile name to make it easier to deal with logic based on profiles in the netty transport. The name could be something like **local**.
&gt; 
&gt; Adding the getProfileName method to the TransportChannel interface may make sense here as other transport implementations may implement the profile concept and those that don't can just return a default name.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: move ScriptType outside of ScriptService, to its own class file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10482</link><project id="" key="" /><description>Opening this up for discussion. I find it very weird that java api users have to import some `ScriptService` inner class when using scripts. `ScriptType` should belong to its own class, at the same level of `ScriptContext` (`org.elasticsearch.script` package).

The only problem with this change is that it breaks the Java api and plugins, as `ScriptType` needs to passed in as argument when using scripts. That is why the change is targeted for 2.0.
</description><key id="67115611">10482</key><summary>Scripting: move ScriptType outside of ScriptService, to its own class file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label></labels><created>2015-04-08T12:15:02Z</created><updated>2015-04-28T08:47:12Z</updated><resolved>2015-04-16T15:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-04-08T13:43:02Z" id="90920693">@javanna agree... personally I think it ties to the discussion we had around having a `Script` construct... if we would have it, then it can be an inner enum in it (a la `Script.Type`)
</comment><comment author="javanna" created="2015-04-08T13:56:14Z" id="90924242">right @uboness , maybe it is worth to make that change directly then and introduce the `Script` class etc.
</comment><comment author="uboness" created="2015-04-08T13:56:51Z" id="90924558">+1
</comment><comment author="javanna" created="2015-04-16T15:59:45Z" id="93771511">I am closing this, we will introduce the `Script` class in one go with a different PR that will most likely move the `ScriptType` too. Don't want to move it twice.
</comment><comment author="javanna" created="2015-04-28T08:47:12Z" id="96979935">Superseded by #10649
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child applied on an alias does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10481</link><project id="" key="" /><description>I am using elasticsearch 1.5.0 and I have a filtered alias.
The index contains 2 types A and B. B is a child to A. The filter extracts from the index all the types applying some conditions. B gets soft deleted (marked as deleted in the document with a flag) and does not show in the documents returned when searching the alias. But when I query for A that do not have B (using a has_child filter), all A are returned regardless if they any children in the alias (they still have children in the index).

This is a workaround for this issue https://github.com/elastic/elasticsearch/issues/10135. I removed the has_child filter for the alias (see person in the issue ) tried to filter on search time with this simple filter:

```
{"filter":{"has_child":{"type":"message","filter":{"match_all":{}},"min_children":1}}}
```

Also I think that the children aggregation is affected in this scenario as any aggregation on A will take into consideration the deleted B.

Is this a bug?
</description><key id="67113638">10481</key><summary>has_child applied on an alias does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikiot</reporter><labels /><created>2015-04-08T12:02:16Z</created><updated>2015-04-08T13:05:34Z</updated><resolved>2015-04-08T12:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-08T12:31:03Z" id="90900236">Hi @mikiot 

The `filter` (now renamed to `post_filter`) is only applied after aggregations are calculated.  This is not a bug, it is by design.  You would need to include the `has_child` clause in your main `query` for it to be applied where you want it.
</comment><comment author="mikiot" created="2015-04-08T12:40:40Z" id="90902623">But I use it in my main query (not in alias filter) and it does not wok. And my main problem is not the aggregation, but the fact that if I do a simple filter on the alias it does not work and it should because this makes aliases pretty useless when using parent/child relationships. You can try it and see! :)
</comment><comment author="clintongormley" created="2015-04-08T13:05:34Z" id="90908709">Currently you have to include that filter clause anywhere that you use a `has_child` filter/query, otherwise it won't work.

Related: https://github.com/elastic/elasticsearch/issues/10058
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove groovy sandbox</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10480</link><project id="" key="" /><description>Closes #10156
</description><key id="67111669">10480</key><summary>Remove groovy sandbox</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T11:50:49Z</created><updated>2015-06-07T17:07:09Z</updated><resolved>2015-04-28T09:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-08T11:51:12Z" id="90889987">@dakrone can you have a look please?
</comment><comment author="dakrone" created="2015-04-08T16:30:43Z" id="90967231">@javanna there are still a lot of references to the sandbox in `docs/reference/modules/scripting.asciidoc` (near the beginning of the file)

Code-wise this looks good to me
</comment><comment author="javanna" created="2015-04-09T07:53:14Z" id="91143082">&gt; there are still a lot of references to the sandbox in docs/reference/modules/scripting.asciidoc (near the beginning of the file)

@dakrone I left those references on purpose at the beginning of the file, I thought we wanted to have the security alert on all branches. It mentions explicitly the versions that are vulnerable that require to turn off the sandbox.

@clintongormley shall we remove this part from the scripting docs on master?
</comment><comment author="clintongormley" created="2015-04-12T14:29:51Z" id="92070698">@javanna I'd remove that section from the scripting page and add a note to the migration page.
</comment><comment author="javanna" created="2015-04-16T16:06:37Z" id="93773377">@clintongormley I left the dyanmic scripts note aat the beginning of the page, but removed references to groovy sandbox and settings. I had already added a note to the migration page, is that not enough?
</comment><comment author="clintongormley" created="2015-04-23T18:11:12Z" id="95674263">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typos in example JSON data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10479</link><project id="" key="" /><description /><key id="67108332">10479</key><summary>fix typos in example JSON data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marko-asplund</reporter><labels /><created>2015-04-08T11:29:15Z</created><updated>2015-04-08T11:41:01Z</updated><resolved>2015-04-08T11:41:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-08T11:40:56Z" id="90888031">thanks @marko-asplund - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Groovy sandbox and related settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10478</link><project id="" key="" /><description>The groovy sandbox has proved not effective and it is currently turned off by default. This PR deprecates it in the docs so we can remove it later on.

Relates to #10156
</description><key id="67105317">10478</key><summary>Deprecate Groovy sandbox and related settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-04-08T11:08:52Z</created><updated>2015-05-30T10:48:31Z</updated><resolved>2015-04-08T11:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-08T11:10:22Z" id="90881849">@clintongormley mind having a look?
</comment><comment author="javanna" created="2015-04-08T11:48:36Z" id="90889635">Merged https://github.com/elastic/elasticsearch/commit/3ff8c90601a78ded7cbd0a86dbe4544dd857bfa7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query string time zone not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10477</link><project id="" key="" /><description>I have following code: 

``` bash
curl -XPUT 'http://localhost:9200/foo/?pretty' -d '{"mapping": {"tweets": {"properties": {"tweet_date": {"type": "date"}}}}}'
curl -XPOST 'http://localhost:9200/foo/tweets/1/' -d '{"tweet_date": "2015-04-05T23:00:00+0000"}'
curl -XPOST 'http://localhost:9200/foo/tweets/2/' -d '{"tweet_date": "2015-04-06T00:00:00+0000"}'
curl -XPOST 'http://localhost:9200/foo/_refresh?pretty'

curl -XGET 'http://localhost:9200/foo/tweets/_search?pretty' -d '{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00+0200 TO 2015-04-06T23:00:00+0200]"
        }
    }
}'

curl -XGET 'http://localhost:9200/foo/tweets/_search?pretty' -d '{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00 TO 2015-04-06T23:00:00]",
            "time_zone": "+0200"
        }
    }
}'

```

First query result:

```
{
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-06T00:00:00+0000"}
    }, {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-05T23:00:00+0000"}
    } ]
  }
}
```

Second query result:

```
{
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-06T00:00:00+0000"}
    } ]
  }
}
```

Shouldn't these results be the same?

(Copied from https://github.com/elastic/elasticsearch/pull/8164#issuecomment-90730355)
</description><key id="67103464">10477</key><summary>Query string time zone not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-04-08T10:55:35Z</created><updated>2015-06-23T17:10:29Z</updated><resolved>2015-04-29T16:10:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-08T10:56:00Z" id="90879653">@dadoonet Opened this issue for you
</comment><comment author="Asimov4" created="2015-06-23T17:10:29Z" id="114574909">Hi! I was just curious to know what release of Elasticsearch this fix would be in.
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated methods from ScriptService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10476</link><project id="" key="" /><description>Removed the following methods from `ScriptService`, which don't require the `ScriptContext` argument:

```
public CompiledScript compile(String lang,  String script, ScriptType scriptType)

public ExecutableScript executable(String lang, String script, ScriptType scriptType, Map&lt;String, Object&gt; vars)

public SearchScript search(SearchLookup lookup, String lang, String script, ScriptType scriptType, @Nullable Map&lt;String, Object&gt; vars)
```

Also removed the `ScriptContext.Standard.GENERIC_PLUGIN` enum value, as it was used only for backwards compatibility.

 Plugins that make use of scripts should declare their own script contexts through `ScriptModule#registerScriptContext` and use them when compiling/executing scripts.
</description><key id="67096555">10476</key><summary>Remove deprecated methods from ScriptService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T10:14:09Z</created><updated>2015-06-06T17:26:21Z</updated><resolved>2015-04-08T10:22:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-08T10:14:19Z" id="90870720">@uboness can you have a look please?
</comment><comment author="uboness" created="2015-04-08T10:19:01Z" id="90871799">LGTM
</comment><comment author="javanna" created="2015-04-08T10:23:20Z" id="90872705">Note: this change is breaking only for plugins that make use of scripts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting compilation error when parsing Timestamp from logs file </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10475</link><project id="" key="" /><description>Hi,
I am getting compilation error when trying to parse a timestamp in format '2013-02-03' i.e 'yyyy-MM-dd'. Actually i am trying to parse the timestamp in the logs file on 'http://grokdebug.herokuapp.com/' website with a input  : 
2015-02-02 11:00:58
and pattern :
 date {
    match =&gt; [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
 Whenever i use yyyy-MM-dd format i get compilation error. Attaching screen shot for the same 

![compilation error](https://cloud.githubusercontent.com/assets/11851015/7043159/345e9bbc-de05-11e4-8195-c3ee2e5d1b18.png)

If i give one space after the yyyy  like yyyy -mm the error goes but i am unable to match this input, could you please look into this issue or suggest me any solution.

Thanks and regards,
Anand J.kadhi
</description><key id="67095780">10475</key><summary>Getting compilation error when parsing Timestamp from logs file </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Anand-J-Kadhi</reporter><labels /><created>2015-04-08T10:10:22Z</created><updated>2015-04-08T11:50:53Z</updated><resolved>2015-04-08T10:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-08T10:59:16Z" id="90880260">Hi @Anand-J-Kadhi 

It sounds like you have a question with logstash, not Elasticsearch. Please ask your question on their mailing list instead.
</comment><comment author="Anand-J-Kadhi" created="2015-04-08T11:50:53Z" id="90889947">Sorry for the mispost, thanks i will post on this on Logstash.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ELK] Elasticsearch errors when i add a scripted field in Kibana 4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10474</link><project id="" key="" /><description>Hello, I'm using ELK with kibana 4.0.2 and elasticsearch 1.4.4 on ubuntu 14.04 and java version "1.8.0_40"
Java(TM) SE Runtime Environment (build 1.8.0_40-b25).

When I want to create a scripted field (example: `doc['status'].value+42`) the web ui displays the message: `Discover: An error occurred with your request. Reset your inputs and try again.`

And in `/var/log/elasticsearch/elasticsearch.log`:

```
[2015-04-08 11:59:16,294][DEBUG][action.search.type       ] [Moonstone] [logstash-2015.04.08][4], node[3hsQHmeVRo2hGHdFbiG_-Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@45f7b369] lastShard [true]
org.elasticsearch.search.SearchParseException: [logstash-2015.04.08][4]: query[ConstantScore(BooleanFilter(+QueryWrapperFilter(host:codemanager) +QueryWrapperFilter(ConstantScore(cache(_type:nginx))) +cache(@timestamp:[1428486256162 TO 1428487156162])))],from[-1],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@51d592b6&gt;!]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"query":{"match":{"host":{"query":"codemanager","type":"phrase"}}}},{"query":{"match":{"_type":{"query":"nginx","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1428486256162,"lte":1428487156162}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"size":500,"sort":{"@timestamp":"desc"},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+02:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1428486256161,"max":1428487156161}}}},"fields":["*","_source"],"script_fields":{"lala":{"script":"doc['status'].value+42","lang":"expression"}},"fielddata_fields":["@timestamp","time"]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [status] used in expression must be numeric
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:131)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:511)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:515)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:671)
    ... 9 more
[2015-04-08 11:59:16,295][DEBUG][action.search.type       ] [Moonstone] [logstash-2015.04.08][1], node[3hsQHmeVRo2hGHdFbiG_-Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@45f7b369]
org.elasticsearch.search.SearchParseException: [logstash-2015.04.08][1]: query[ConstantScore(BooleanFilter(+QueryWrapperFilter(host:codemanager) +QueryWrapperFilter(ConstantScore(cache(_type:nginx))) +cache(@timestamp:[1428486256162 TO 1428487156162])))],from[-1],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@210e49a&gt;!]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"query":{"match":{"host":{"query":"codemanager","type":"phrase"}}}},{"query":{"match":{"_type":{"query":"nginx","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1428486256162,"lte":1428487156162}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"size":500,"sort":{"@timestamp":"desc"},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+02:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1428486256161,"max":1428487156161}}}},"fields":["*","_source"],"script_fields":{"lala":{"script":"doc['status'].value+42","lang":"expression"}},"fielddata_fields":["@timestamp","time"]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [status] used in expression must be numeric
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:131)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:511)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:515)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:671)
    ... 9 more
[2015-04-08 11:59:16,295][DEBUG][action.search.type       ] [Moonstone] All shards failed for phase: [query]
[2015-04-08 11:59:16,296][DEBUG][action.search.type       ] [Moonstone] [logstash-2015.04.08][3], node[3hsQHmeVRo2hGHdFbiG_-Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@45f7b369] lastShard [true]
org.elasticsearch.search.SearchParseException: [logstash-2015.04.08][3]: query[ConstantScore(BooleanFilter(+QueryWrapperFilter(host:codemanager) +QueryWrapperFilter(ConstantScore(cache(_type:nginx))) +cache(@timestamp:[1428486256162 TO 1428487156162])))],from[-1],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@1598a69f&gt;!]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"query":{"match":{"host":{"query":"codemanager","type":"phrase"}}}},{"query":{"match":{"_type":{"query":"nginx","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1428486256162,"lte":1428487156162}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"size":500,"sort":{"@timestamp":"desc"},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+02:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1428486256161,"max":1428487156161}}}},"fields":["*","_source"],"script_fields":{"lala":{"script":"doc['status'].value+42","lang":"expression"}},"fielddata_fields":["@timestamp","time"]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [status] used in expression must be numeric
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:131)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:511)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:515)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:671)
    ... 9 more
[2015-04-08 11:59:16,295][DEBUG][action.search.type       ] [Moonstone] [logstash-2015.04.08][2], node[3hsQHmeVRo2hGHdFbiG_-Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@45f7b369] lastShard [true]
org.elasticsearch.search.SearchParseException: [logstash-2015.04.08][2]: query[ConstantScore(BooleanFilter(+QueryWrapperFilter(host:codemanager) +QueryWrapperFilter(ConstantScore(cache(_type:nginx))) +cache(@timestamp:[1428486256162 TO 1428487156162])))],from[-1],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@7d035eda&gt;!]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"query":{"match":{"host":{"query":"codemanager","type":"phrase"}}}},{"query":{"match":{"_type":{"query":"nginx","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1428486256162,"lte":1428487156162}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"size":500,"sort":{"@timestamp":"desc"},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+02:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1428486256161,"max":1428487156161}}}},"fields":["*","_source"],"script_fields":{"lala":{"script":"doc['status'].value+42","lang":"expression"}},"fielddata_fields":["@timestamp","time"]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [status] used in expression must be numeric
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:131)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:511)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:515)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:671)
    ... 9 more
[2015-04-08 11:59:16,296][DEBUG][action.search.type       ] [Moonstone] [logstash-2015.04.08][0], node[3hsQHmeVRo2hGHdFbiG_-Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@45f7b369] lastShard [true]
org.elasticsearch.search.SearchParseException: [logstash-2015.04.08][0]: query[ConstantScore(BooleanFilter(+QueryWrapperFilter(host:codemanager) +QueryWrapperFilter(ConstantScore(cache(_type:nginx))) +cache(@timestamp:[1428486256162 TO 1428487156162])))],from[-1],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@3ce90bce&gt;!]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"query":{"match":{"host":{"query":"codemanager","type":"phrase"}}}},{"query":{"match":{"_type":{"query":"nginx","type":"phrase"}}}},{"range":{"@timestamp":{"gte":1428486256162,"lte":1428487156162}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"size":500,"sort":{"@timestamp":"desc"},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+02:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1428486256161,"max":1428487156161}}}},"fields":["*","_source"],"script_fields":{"lala":{"script":"doc['status'].value+42","lang":"expression"}},"fielddata_fields":["@timestamp","time"]}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.expression.ExpressionScriptCompilationException: Field [status] used in expression must be numeric
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.search(ExpressionScriptEngineService.java:131)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:511)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:515)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:671)
    ... 9 more
```

What can I do please?
</description><key id="67095451">10474</key><summary>[ELK] Elasticsearch errors when i add a scripted field in Kibana 4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Nurza</reporter><labels /><created>2015-04-08T10:08:10Z</created><updated>2015-04-08T10:58:05Z</updated><resolved>2015-04-08T10:58:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-08T10:58:05Z" id="90879979">Hi @Nurza 

Please ask questions like these on the mailing list.  By default, the only scripting language enabled for dynamic scripts is "expressions", which doesn't handle non-numeric fields.  If you want to use groovy, you'll have to store your scripts in a file on the server.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatically wait for green / yellow if possible on index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10473</link><project id="" key="" /><description>today index creation returns soon / immediately without waiting for green / yellow. I think we should for a better user experience. We really only ready for indexing and durability if replicas are assigned and started. At least we should do best effort here.
</description><key id="67090184">10473</key><summary>Automatically wait for green / yellow if possible on index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-04-08T09:38:06Z</created><updated>2015-04-08T09:42:52Z</updated><resolved>2015-04-08T09:42:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-08T09:42:48Z" id="90863691">seems like I had the same idea a while ago :+1: 
</comment><comment author="s1monw" created="2015-04-08T09:42:52Z" id="90863701">https://github.com/elastic/elasticsearch/issues/9126
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: add an assertion to verify consistent serialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10472</link><project id="" key="" /><description>We recently run into two issues where mapping weren't serialized in a consistent manner (#10302 and #10318). We rely on this consistency to do a byte level checl that mappings we get from the master are indentical to the one we have locally. Mistakes here can cause endless refresh mapping loops.

This commit adds an assert that verifies this upon every update from the master.
</description><key id="67069349">10472</key><summary>Mapping: add an assertion to verify consistent serialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-08T07:35:37Z</created><updated>2015-05-29T19:43:16Z</updated><resolved>2015-05-29T09:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-08T08:24:52Z" id="90839976">@bleskes I think your note on concurrency might actually be relevant? what happens if during that time an indexing comes in and introduces a new field? If so, I wonder if we should just wait till mappings are immutable and do it then
</comment><comment author="bleskes" created="2015-04-08T09:04:39Z" id="90854541">@kimchy I think capturing the compressed string and checking it deserializes and serializes back to the same thing is useful? the compressed string it self is immutable? Feels like I'm missing something.
</comment><comment author="bleskes" created="2015-04-15T07:17:16Z" id="93232490">@kimchy ping?
</comment><comment author="rjernst" created="2015-04-23T16:14:02Z" id="95636882">LGTM
</comment><comment author="rjernst" created="2015-05-29T09:42:52Z" id="106759642">I rebased and pushed.
Master: 521f804c
1.x: 7828852
</comment><comment author="bleskes" created="2015-05-29T19:43:16Z" id="106912945">Thx @rjernst !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated slow query log to add alias information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10471</link><project id="" key="" /><description>prints Filter or XBooleanFilter ( if query uses more than one alias ) in
slow query logs

https://github.com/elastic/elasticsearch/issues/10044
</description><key id="67040242">10471</key><summary>Updated slow query log to add alias information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nirmalc</reporter><labels><label>:Logging</label><label>enhancement</label><label>stalled</label></labels><created>2015-04-08T03:28:13Z</created><updated>2016-03-08T19:25:58Z</updated><resolved>2016-03-08T19:25:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-08T08:37:24Z" id="90842921">Hi @nirmalc thanks for your PR!
The one thing I am not sure about is whether we want to print out this string representation of the lucene alias filter as you did, or the query dsl version of it instead, more understandable for users.

The problem with the query dsl version is that we don't have it available in the search context, especially given that alias filters get parsed at type creation time. We are discussing though if we should move to parsing alias filters at search time, which would require to carry the query dsl filter in the search context I believe. Then we could easily print it out in the slow log too.
</comment><comment author="javanna" created="2015-04-08T13:13:17Z" id="90910899">I think #10485 would  help here.
</comment><comment author="nirmalc" created="2015-04-08T13:25:44Z" id="90914577">Thanks for input - I thought same way , but resisted from making much downstream changes for logging as i wasnt sure of what you guys have in plan. Will try  , it would also make sense to carry alias name in a clean way , currently filteredAlias is available only if alias contains filter 

Logging query dsl will make debugging lot easier 
</comment><comment author="javanna" created="2015-04-08T14:00:29Z" id="90925567">I would wait for #10485 here, that should already make the filter available in the search context. I wouldn't make such a change just for logging, instead let it happen on #10485 and afterwards make use of it for logging too.
</comment><comment author="nirmalc" created="2015-04-08T14:06:39Z" id="90927559">cool , Thanks
</comment><comment author="javanna" created="2015-11-09T15:19:31Z" id="155093471">getting back to this now that we parse alias filters at search time. Seems technically possible to print out the original query representation, we would just need to keep track of it as part of the search context together with its lucene representation. @martijnvg what do you think?
</comment><comment author="clintongormley" created="2016-03-08T19:25:58Z" id="193931460">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include groovy community</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10470</link><project id="" key="" /><description>This change includes the a new community: groovy to point to the official client.
</description><key id="67026562">10470</key><summary>Include groovy community</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">blackorzar</reporter><labels><label>docs</label></labels><created>2015-04-08T01:31:21Z</created><updated>2015-04-08T15:28:32Z</updated><resolved>2015-04-08T09:46:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-04-08T03:47:39Z" id="90795437">Hi @blackorzar, this is a good addition. I've seen this page a couple of times, and I honestly never noticed the "official" clients getting listed!

This issue is definitely blocked by elastic/elasticsearch-groovy#24 though as I need to update the Groovy client documentation to get it hosted on the main website rather than the repo as it is today.
</comment><comment author="clintongormley" created="2015-04-08T09:46:26Z" id="90864509">thanks @blackorzar - merged!
</comment><comment author="clintongormley" created="2015-04-08T09:48:54Z" id="90865256">@pickypg sorry - just seen your comment about waiting on https://github.com/elastic/elasticsearch-groovy/issues/24
</comment><comment author="blackorzar" created="2015-04-08T14:28:20Z" id="90932812">Thanks guys, I will keep looking at this commit.

On Wed, Apr 8, 2015 at 3:49 AM, Clinton Gormley notifications@github.com
wrote:

&gt; @pickypg https://github.com/pickypg sorry - just seen your comment
&gt; about waiting on elastic/elasticsearch-groovy#24
&gt; https://github.com/elastic/elasticsearch-groovy/issues/24
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/10470#issuecomment-90865256
&gt; .
</comment><comment author="pickypg" created="2015-04-08T15:28:32Z" id="90950852">@clintongormley No problem. It just means that I need to get the Groovy docs updated sooner rather than later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow relocating primary shards on shared filesystems without failing the shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10469</link><project id="" key="" /><description>In the current implementation of replica shard when a primary shard is relocated to a different node the cluster enters red state for the duration of the shared storage handoff. That duration is proportional to the shard size. 
The desired behavior is that the cluster doesn't enter the red state at all. An acceptable constraint is that for the duration of the storage handoff no documents can be added to the shard.

Wojtek K. 
Microsoft
</description><key id="67002320">10469</key><summary>Allow relocating primary shards on shared filesystems without failing the shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">kozaczynski</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T22:15:28Z</created><updated>2015-08-13T15:31:38Z</updated><resolved>2015-05-05T10:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-08T09:51:59Z" id="90865993">I will look into this, yet the `That duration is proportional to the shard size.` is a property of the shared filesystem not of elasticsearch in this case.
</comment><comment author="kozaczynski" created="2015-04-08T16:04:17Z" id="90960548">The duration comment is about the current behavior that we need to change. The cumulative effect of that behavior is that when we restore a large index with 30+ shards into a cluster that already has number of large indexes we see the cluster go in-and-out of the red state for an extended period of time.
</comment><comment author="s1monw" created="2015-04-09T08:16:01Z" id="91146488">I think the issue with going in-and-out of the  red state here is mainly triggered by the restore operation. I think you should disable allocation for those indices or at least for the primary while you restore? Yet, I think it's a bug that we go red here or at least a not so nice impl detail. Yet, the only other downside (aside of a false-red kind of) of this is that you can't write to that index for the time being. Yet, I stated looking into it but it's not a trivial change so it might take some time to get it fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.1.0-snapshot-1671894.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10468</link><project id="" key="" /><description>This includes a fix for LUCENE-6406.
</description><key id="66955579">10468</key><summary>Upgrade to lucene-5.1.0-snapshot-1671894.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T18:06:19Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-07T18:07:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-07T18:06:44Z" id="90681471">+1
</comment><comment author="dadoonet" created="2015-04-09T12:41:25Z" id="91218650">Also pushed in parent project: https://github.com/elastic/elasticsearch-parent/commit/998793f4e066aaf7d90b83881f4fdf0dcc8432fd
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support ANT-like syntax in path_match for dynamic mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10467</link><project id="" key="" /><description>Use case:

I'm indexing documents with nested objects where some of the objects include unique ids (GUIDS). I want all such id fields to be "not_analyzed." The id fields always have an '_id' suffix and these fields can appear at arbitrary levels in the document hierarchy. I'm trying to come up with a dynamic mapping template to address this so that any field of the form "__id" regardless of the nesting depth will be marked as "not_analyzed." I don't think there's a way to specify this as a single path_match.  In practice, I suppose the nesting will never go deeper than let's say, 5.  So I could define 5 path_match patterns like *_id, *.__id, _._.*_id...etc. Although experience shows that the moment I do this, we'll find the need to go to 6 levels ;-). 

Ideally, you'd be able to specify a path in ANT-like syntax e.g., *_/__id.
</description><key id="66951370">10467</key><summary>Support ANT-like syntax in path_match for dynamic mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blevine</reporter><labels /><created>2015-04-07T17:51:17Z</created><updated>2015-04-07T18:16:22Z</updated><resolved>2015-04-07T18:00:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T18:00:36Z" id="90677927">Hi @blevine 

Just `match` should work for you:

```
DELETE test

PUT /test
{
  "mappings": {
    "test": {
      "dynamic_templates": [
        {
          "ids": {
            "match": "*_id",
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      ]
    }
  }
}

PUT test/test/1
{
  "foo_id": 123,
  "bar": {
    "foo": "some text",
    "bar_id": "123"
  }
}

GET test/_mapping
```
</comment><comment author="blevine" created="2015-04-07T18:16:22Z" id="90687095">Too obvious ;-)  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require units be specified for time configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10466</link><project id="" key="" /><description>Default units for attributes such as discovery.zen.ping_timeout are milliseconds, but the documentation often uses values in seconds when describing defaults.  Unless you specify the units when modifying one of these values, for instance using 30 vs. 30s you will wind up with 30 milliseconds, which was not intended.  Requiring a valid time unit when setting one of these should be required.  Alternatively, documentation could be expanded to flag this as a possible opportunity for misconfiguration.  Especially in areas where an incorrect setting is likely to result in instability such as discovery.zen.ping_timeout.
</description><key id="66943194">10466</key><summary>Require units be specified for time configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nemonster</reporter><labels><label>:Settings</label><label>enhancement</label></labels><created>2015-04-07T17:25:03Z</created><updated>2015-08-26T19:28:48Z</updated><resolved>2015-08-26T19:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T17:42:13Z" id="90666389">Related to #7633
</comment><comment author="ppf2" created="2015-04-08T03:42:46Z" id="90795066">It sounds like there are backwards compatibility concerns around requiring a time units.  Have we considered implementing smarter defaults when whole numbers are detected for settings in the meantime?  For example, there are settings out there that can be dangerous when mistakenly set to whole numbers (and we have seen this happen in production environments), things like zen discovery and fault detection timeouts, refresh and flush intervals, etc..  Instead of defaulting everything to milliseconds when whole numbers are detected, perhaps we can selectively default to milliseconds only for settings that makes sense?  For example, if refresh interval is set to a whole number and is &lt;1000, ignore the setting and use the system default value (eg, 1s)  and write a warning to the log file indicating that we are falling back to the system default as a safety measure, etc..
</comment><comment author="jpountz" created="2015-08-26T19:28:48Z" id="135146480">Fixed via https://github.com/elastic/elasticsearch/pull/11437
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify murmur3 type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10465</link><project id="" key="" /><description>I don't think we should allow trappy options like indexed=true or docvalues=false for this field. Cardinality and range of values is enormous by definition.
</description><key id="66935895">10465</key><summary>Simplify murmur3 type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T16:51:02Z</created><updated>2015-04-24T04:49:49Z</updated><resolved>2015-04-24T04:49:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-04-07T16:52:41Z" id="90643852">+1

I think it would be great to default:

``` yml
"index" : "no",
"doc_values" : true
```

for

``` yml
"type" : "murmur3"
```

Doc values _should_ be true in 2.0 already, but it makes a lot of sense to not index the field by default as well.
</comment><comment author="rjernst" created="2015-04-07T22:16:12Z" id="90747667">@pickypg I don't think it should be defaults, it should just be not allowed to have anything but `index: no` and `doc_values: true`. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Netty exceptionCaught method protected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10464</link><project id="" key="" /><description>The exceptionCaught method had default access, which imposes a requirement
for subclasses that need to override this method to be in a specific package. This
change simply makes the method protected, which removes the package requirement.
</description><key id="66930844">10464</key><summary>Make Netty exceptionCaught method protected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T16:31:03Z</created><updated>2015-06-07T16:30:20Z</updated><resolved>2015-04-07T17:11:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-07T16:56:48Z" id="90645449">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene merges should run on the target shard during recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10463</link><project id="" key="" /><description>This is already fixed on 2.0, since we let Lucene launch its own merges again.

But in 1.x, Lucene merges might not run on the target during recovery, causing segment explosion when there are many docs to replay and/or the index buffer is low.  This then makes recovery time O(N^2) and can cause issues like #9226.

I just moved launching of the mergeScheduleFuture out of startScheduledTasksIfNeeded (only called once recovery is done) and into createNewEngine.  This way whenever the engine is created we also start checking for merges.

I also renamed startScheduledTasksIfNeeded -&gt; startEngineRefresher, and cleaned up a couple unrelated things.
</description><key id="66919867">10463</key><summary>Lucene merges should run on the target shard during recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label></labels><created>2015-04-07T15:45:41Z</created><updated>2015-05-30T10:56:41Z</updated><resolved>2015-04-07T21:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-07T16:20:44Z" id="90629854">I moved the mergeScheduleFuture creation to ctor, so now we create it once when the IndexShard is created, not in newEngine.

And I fixed EngineMerge to use engineUnsafe and skip merging if engine is currently null...
</comment><comment author="bleskes" created="2015-04-07T19:32:46Z" id="90707708">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Explicitely opt-out for the Lucene query cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10462</link><project id="" key="" /><description>Even though the default query cache is not on by default, it might change in
the future so we should opt out as it would not play well with NoCacheFilter
(which we should aim at removing).
</description><key id="66910764">10462</key><summary>Internal: Explicitely opt-out for the Lucene query cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-04-07T15:06:13Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-07T15:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-07T15:13:44Z" id="90603688">don't think this catches all the places creating indexsearchers.

For example ChildrenConstantScoreQuery
</comment><comment author="jpountz" created="2015-04-07T15:16:33Z" id="90605469">Good point, it looks like we have lots of code that instantiate IndexSearcher instances actually. :(
</comment><comment author="jpountz" created="2015-04-07T15:27:25Z" id="90610222">Hmm, closing as a won't fix then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move to one data.path per shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10461</link><project id="" key="" /><description>This commit moves away from using stripe RAID-0 simumlation across multiple
data paths towards using a single path per shard. Multiple data paths are still
supported but shards and it's data is not striped across multiple paths / disks.
This will for instance prevent to loose all shards if a single disk is corrupted.

Indices that are using this features already will automatically upgraded to a single
datapath based on a simple diskspace based heuristic. In general there must be enough
diskspace to move a single shard at any time otherwise the upgrade will fail.

Closes #9498
</description><key id="66908089">10461</key><summary>Move to one data.path per shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T14:57:42Z</created><updated>2015-06-07T17:35:33Z</updated><resolved>2015-04-20T15:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-07T14:58:08Z" id="90596446">FYI this is a first cut at this feature so it's not perfect but it mirrors the direction pretty well..
</comment><comment author="mikemccand" created="2015-04-15T15:41:55Z" id="93454944">If ES crashes/is killed while a shard is being upgraded that shard is now corrupt right?  Because we have moved some but not all files?  Maybe in the upgrade release notes we strongly suggest taking snapshot before (maybe we do this already)?
</comment><comment author="s1monw" created="2015-04-15T15:56:22Z" id="93458495">&gt; If ES crashes/is killed while a shard is being upgraded that shard is now corrupt right? Because we have moved some but not all files? Maybe in the upgrade release notes we strongly suggest taking snapshot before (maybe we do this already)?

that is a great question, the answer is no IMO. Today if you have multiple datapaths and distributor directory you can stop upgradeing in the middle and still being able to open it with 1.x and you distributor dir. The distiributor doesn't care where the files are and we don't delete before we have successfully moved them so I think we are ok here?
</comment><comment author="mikemccand" created="2015-04-15T16:19:13Z" id="93471969">I love this change, I love all the stuff that's removed!  I left some minor comments...
</comment><comment author="mikemccand" created="2015-04-15T16:22:16Z" id="93473630">&gt; that is a great question, the answer is no IMO. 

Oh I see, because on restart, the distributor will just look around and find where the file resides and just open it there?  Good!

Hmm but what about the non-atomic move case?  Does Files.move behave well if JVM crashes while it's running?  E.g. copy to a temp file on the dest file store and then do an atomic rename in the end?
</comment><comment author="s1monw" created="2015-04-16T09:53:00Z" id="93695912">&gt; Hmm but what about the non-atomic move case? Does Files.move behave well if JVM crashes while it's running? E.g. copy to a temp file on the dest file store and then do an atomic rename in the end? 

yeah but when do you remove the source file, there is always a window here I guess?
</comment><comment author="s1monw" created="2015-04-16T10:00:16Z" id="93697350">@mikemccand I pushed changes according to your comments, and merged with master
</comment><comment author="mikemccand" created="2015-04-16T21:21:29Z" id="93842354">&gt; yeah but when do you remove the source file, there is always a window here I guess?

Or you could do the atomic rename and then the remove?  If the same file exists in two (hmm, or more, if it happens again on the next upgrade) places what will happen?
</comment><comment author="mikemccand" created="2015-04-16T21:22:05Z" id="93842451">LGTM thanks @s1monw 
</comment><comment author="bleskes" created="2015-04-17T10:46:51Z" id="93963207">I this is good. Left little concerns/comments/suggestions here and there.
</comment><comment author="s1monw" created="2015-04-20T08:49:06Z" id="94396104">@bleskes I pushed another commit adressing your comments. @mikemccand I moved to a more pessimistic model give your comments too by using atomic move etc. can you take another look
</comment><comment author="s1monw" created="2015-04-20T09:59:51Z" id="94412623">@mikemccand pushed a new commit
</comment><comment author="mikemccand" created="2015-04-20T10:01:21Z" id="94412900">LGTM, thanks @s1monw!
</comment><comment author="bleskes" created="2015-04-20T11:10:44Z" id="94425274">Thx @s1monw looks great. I left some very minor comment and replied to your question about https://github.com/elastic/elasticsearch/pull/10461#discussion_r28583800
</comment><comment author="s1monw" created="2015-04-20T13:18:52Z" id="94448990">@bleskes pushed one more commit - would you mind taking a look
</comment><comment author="bleskes" created="2015-04-20T13:49:29Z" id="94456018">LGTM. Left one comment we agreed to solve on a follow up issue.
</comment><comment author="s1monw" created="2015-04-20T13:56:11Z" id="94457447">@bleskes I opened #10677 for this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to set custom default analyzer by reference.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10460</link><project id="" key="" /><description>Feature request.

I want to be able to have a common `ToXContext` object that sets up a custom analyzer, and then be able to set it as a default by reference, e.g. the request would look something like this:

```
"settings":{"index":{
  "analysis": {
    "filter": {
      "doublemetaphone_filter": {
        "replace": "true",
        "type": "phonetic",
        "encoder": "doublemetaphone"
      }
    },
    "analyzer": {
      "doublemetaphone_analyzer": {
        "filter": [
          "standard",
          "lowercase",
          "stop",
          "doublemetaphone_filter"
        ],
        "tokenizer": "standard"
      },
      "default": {
        "type": "doublemetaphone_analyzer"
      }
    }
  }
}}
```

If I try to do this, ES complains that `doublemetaphone_analyzer` is an unknown type.

Currently, the only way to use a custom analyzer as a default one, is to define it with the name `default`.
</description><key id="66887422">10460</key><summary>Unable to set custom default analyzer by reference.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apogrebnyak</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2015-04-07T13:45:47Z</created><updated>2016-01-15T19:36:49Z</updated><resolved>2016-01-15T19:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apogrebnyak" created="2015-04-08T21:37:21Z" id="91043417">I think it's better to separate analyzer definition, and whether or not it is used by default.

For some indexes I want to enable custom analysis on a few fields and use something else as a default for everything, for others I always want to use a custom analyzer as default.

Having been able to define `default` analyzer in terms of already defined custom analyzer leads to a better code reuse and is in accordance with DRY principal.

-Alex
</comment><comment author="clintongormley" created="2016-01-15T19:36:48Z" id="172062999">Given that this issue doesn't seem to have garnered any support in the last 9 months, I'm going to close it.  Feel free to reopen if you feel it is really important.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Add relations to GeoShapeQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10459</link><project id="" key="" /><description>As of 1.5.0 and earlier spatial relation support is only available to `GeoShapeFilter`. `GeoShapeQuery` returns any geometry that `INTERSECTS || WITHIN || CONTAINS` the search geometry.  In order for a user to achieve a spatial relation search a post filter must be used which will result in a traversal over the candidate result set.  This enhancement adds relation support to `GeoShapeQuery` to avoid the need for post filtering.

This improvement is spawned from #10271
</description><key id="66884684">10459</key><summary>[GEO] Add relations to GeoShapeQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2015-04-07T13:31:25Z</created><updated>2016-01-20T15:20:46Z</updated><resolved>2016-01-20T15:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T19:34:55Z" id="172062564">Hi @nknize 

Is this already resolved?
</comment><comment author="nknize" created="2016-01-20T15:20:46Z" id="173235820">Indeed. Resolved by #14810
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO-JavaApi] ShapeBuilder.new{Geometry} should accept initialization parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10458</link><project id="" key="" /><description>Many of the static builder methods in ShapeBuilder are cumbersome to use. Below is an example for creating a newEnvelope

``` java
Shape shape = ShapeBuilder.newEnvelope().topLeft(0, 10).bottomRight(10, 0).build()
```

Java users should have the option to pass the coordinates as parameters:

``` java
Shape shape = ShapeBuilder.newEnvelope(0,10,10,0).build();
```

Other static initializers like this would make the java api more "user-friendly" simply by adding parameters or parameter lists. 
</description><key id="66881601">10458</key><summary>[GEO-JavaApi] ShapeBuilder.new{Geometry} should accept initialization parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Java API</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-07T13:21:32Z</created><updated>2016-03-29T09:58:05Z</updated><resolved>2016-01-17T16:31:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-04-07T13:22:21Z" id="90550385">Spawned from #10271 
</comment><comment author="clintongormley" created="2016-01-17T16:31:00Z" id="172347219">This appears to have been done in the search refactoring.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to t-digest 3.1.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10457</link><project id="" key="" /><description>This release fixes https://github.com/elastic/elasticsearch/issues/10216 through https://github.com/tdunning/t-digest/pull/44
</description><key id="66863617">10457</key><summary>Upgrade to t-digest 3.1.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>stalled</label><label>upgrade</label></labels><created>2015-04-07T11:47:28Z</created><updated>2016-04-07T10:09:43Z</updated><resolved>2016-04-07T07:23:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-04-07T12:36:33Z" id="90534074">LGTM
</comment><comment author="clintongormley" created="2015-09-15T09:56:27Z" id="140341848">This version of t-digest introduces new bugs which cause test failures.  bumping.
</comment><comment author="dakrone" created="2016-04-06T21:09:57Z" id="206569746">@jpountz any idea what the test failures are with 3.1? Should we close this until t-digest 3.2 is released?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch should raise a mapper exception when reserved "mapping fields" are used inside "properties"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10456</link><project id="" key="" /><description>Unfortunately it is possible to use ["top-level"/meta mapping properties](http://www.elastic.co/guide/en/elasticsearch/reference/1.x/mapping-fields.html), such as `_source`, `_all`, ... inside the `"properties": {}` portion of a mapping, without complaints. This allows for user errors that are easily missed.

Using the following mapping

``` json
{
  "t1": {
    "_source": {"enabled": false}, 
    "properties": {
      "_all": {"enabled": false},
      "field1": {"type": "string"},
      "field2": {"type": "long"},
      "_underscore1": {"type": "string"},
      "_underscore2": {"enabled": true}
    }
  }
}
```

will lead to:

``` json
{
  "mytest" : {
    "mappings" : {
      "t1" : {
        "_source" : {
          "enabled" : false
        },
        "properties" : {
          "_underscore1" : {
            "type" : "string"
          },
          "_underscore2" : {
            "type" : "object"
          },
          "field1" : {
            "type" : "string"
          },
          "field2" : {
            "type" : "long"
          }
        }
      }
    }
  }
}
```

Note, how the `_all`-"Field-Mapping" is just silently ignored.

Please see https://gist.github.com/konradkonrad/16220520eb9bacc1f6e6 for a complete reproduction.
</description><key id="66862430">10456</key><summary>Elasticsearch should raise a mapper exception when reserved "mapping fields" are used inside "properties"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">konradkonrad</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-04-07T11:43:05Z</created><updated>2015-12-23T16:34:24Z</updated><resolved>2015-12-23T16:34:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T12:42:07Z" id="90535298">Relates to #9059
</comment><comment author="konradkonrad" created="2015-04-07T13:29:54Z" id="90554971">@clintongormley thx for labelling this! 

While I totally agree on the relation to #9059, I believe this one should/could be addressed ahead of time, since atm there exists a set of should-be-reserved words: [Mapping &#187; Fields](http://www.elastic.co/guide/en/elasticsearch/reference/1.x/mapping-fields.html)*)

As the `_all` example seems to show, there is also some validation already in place. I cannot tell how many users use ambigous field names such as `_timestamp`, and I understand the BC concerns of #9059 -- however from _my_ POV I'd rather have users become aware of their misconfigured mappings, then serve the BC needs of mappings using questionable ambigous names.

*) SN: btw. pretty unfortunate naming in the docs imho, what about one of these?
- _Mapping &#187; DocType Configuration_
- _Mapping &#187; DocType Settings_
- _Mapping &#187; Mapping Configuration_
- _Mapping &#187; Top-Level Fields_
</comment><comment author="jpountz" created="2015-12-23T16:34:24Z" id="166938044">Fixed via #15243
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add list parse methods to XContentParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10455</link><project id="" key="" /><description>In XContentParser, maps can be parsed, but no lists. List parsing can be useful when non-ES JSON must be parsed, e.g. from external data input. This pull request adds some API methods to parse lists, in analogy to the methods for map parsing. Two private access modifiers are removed from readList() and readValue() in order to allow cleaner subclassing of AbstractXContentParser.
</description><key id="66860564">10455</key><summary>add list parse methods to XContentParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jprante</reporter><labels><label>:Plugins</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T11:36:14Z</created><updated>2015-06-24T10:25:24Z</updated><resolved>2015-06-24T10:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-24T10:25:11Z" id="114816644">Thanks @jprante , I merged the pull request manually, with the exception of the `*AndClose()` methods since we removed them for maps on master already.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor MatchAllQueryBuilder, TermQueryBuilder, IdsQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10454</link><project id="" key="" /><description>MatchAllQueryBuilder, TermQueryBuilder, IdsQueryBuilder now include the fromXContent, doXContent and toQuery methods suggested in #9901. Also implementing the Streamable interface and adding unit tests for the serialization and parsing.

NOTE: this PR is against the feature/query-parse-refactoring branch.
</description><key id="66850855">10454</key><summary>Refactor MatchAllQueryBuilder, TermQueryBuilder, IdsQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-07T10:39:21Z</created><updated>2015-04-20T13:05:54Z</updated><resolved>2015-04-20T13:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-10T14:33:43Z" id="91576380">I left a bunch of comments. I am pretty positive that we can trim down the code needed for testing, by just having a base test class with a few abstract methods that provide the query to test etc. we'll see how far we can take this. I am happy that we are going with unit testing rather than requiring to fire up a node.

My personal preference, I think a single PR per query would be easier to review, also then you don't have to adapt all of the other queries to the same comments :) but I see how this time it was better to touch a few of them rather than just one.
</comment><comment author="cbuescher" created="2015-04-10T16:24:26Z" id="91607673">I worked through your comments. When we got through the other PR with Lees query we can start identifying common issues and test setup that can be generalised. I'll wait with further improvements here till we decided what kind of Parser/Builder merge we want on the feature branch and if we have to rebase there.
</comment><comment author="javanna" created="2015-04-20T13:05:53Z" id="94446691">This got replaced by individual PRs, one per query: #10668 , #10669 &amp; #10670 . Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix maven-resources-plugin warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10453</link><project id="" key="" /><description>Commit 168238dab6f5cb081c1e919c0136c13a3c837b72 declared multiple maven-resources-plugin usages instead of declaring multiple executions for the same plugin... resulting to Maven warnings.

Closes #10433
</description><key id="66839871">10453</key><summary>Fix maven-resources-plugin warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>build</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T09:42:39Z</created><updated>2015-06-08T15:25:31Z</updated><resolved>2015-04-09T08:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-07T09:47:48Z" id="90489984">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integrate translog recovery into Engine / InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10452</link><project id="" key="" /><description>Today the engine writes the transaction log itself as well as manages
all the commit / translog mapping internally. Yet, if an engine is closed
and reopend it doesn't replay it's translog or does anything to be consistent
with it's latest state again.
This change moves the transaction log replay code into the Engine / InternalEngine
and adds unittests for replaying and consistency.
</description><key id="66833219">10452</key><summary>Integrate translog recovery into Engine / InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>blocker</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T09:15:06Z</created><updated>2015-06-07T17:40:25Z</updated><resolved>2015-04-13T14:43:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-07T09:58:17Z" id="90493237">@bleskes if you have a day or two to review ;)
</comment><comment author="bleskes" created="2015-04-13T08:25:07Z" id="92266928">I like the change. Left a bunch of comments. Most of them minor, with the exception of the recovery state stats handling.
</comment><comment author="s1monw" created="2015-04-13T09:42:18Z" id="92293501">pushed some updates
</comment><comment author="s1monw" created="2015-04-13T12:15:49Z" id="92331605">@bleskes pushed a new commit
</comment><comment author="s1monw" created="2015-04-13T13:14:51Z" id="92347230">@bleskes I pushed that stuff into the performer as you requested I hope it's worth all the added complexity
</comment><comment author="bleskes" created="2015-04-13T14:07:56Z" id="92368620">LGTM. Thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: Add link to blog post on shard vs. index level doc frequencies to cutoff_frequency documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10451</link><project id="" key="" /><description>Relates to #10154 and #10150

Adds link to additional information on how document frequencies are treated across shards to the cutoff_frequency parameter documentation.
</description><key id="66823354">10451</key><summary>Proposal: Add link to blog post on shard vs. index level doc frequencies to cutoff_frequency documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>docs</label></labels><created>2015-04-07T08:30:29Z</created><updated>2015-04-07T12:29:11Z</updated><resolved>2015-04-07T12:28:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T12:29:11Z" id="90532971">thanks @MaineC - merged

(I just added a :defguide: attribute to the reference docs to make it easier to link to the definitive guide)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there someway to get parent doc with hitted children docs when using "has_children" to search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10450</link><project id="" key="" /><description>Can I get return like this when using "has_children" to search :

``` java
/**search*/
curl -XGET  'http://localhost:9200/hermes/email/_search/?pretty=true' -d '{ "query": {
    "has_child": {
      "type": "email_owner",
      "query": {
        "bool": {
          "must": [
            { "term": { "owner": "shinyke@189.cn" } },
            {"term": {"labelId": "1"} }
          ]}
      }
    }
  }
}'
```

``` java
/**return */
{
    * "took": 5,
    * "timed_out": false,
    * "_shards": {
        * "total": 5,
        * "successful": 5,
        * "failed": 0
        },
    * "hits": {
        * "total": 1,
        * "max_score": 1,
        * "hits": [
            * {
                * "_index": "hermes",
                * "_type": "email",
                * "_id": "1",
                * "_score": 1,
                * "_source": {
                    * "subject": "&#24191;&#24030;&#24066;&#21313;&#22823;&#26032;&#38395;",
                    * "content": "1. &#38271;&#26149;&#24066;&#38271;&#26149;&#33647;&#24215;&#12290;2. &#19968;&#27425;&#24615;&#20132;&#26131;&#19968;&#30334;&#20803;&#12290;",
                    * "recepter": "shinyke@163.com",
                    * "sender": "13724100993@189.cn"
                               },
                                /** "matched_children" is what I need */
                                 "matched_children": [
                                        {"owner":"shinyke@189.cn","labelId":"1,2,3"}
                                 ]
                          }
                ]
         }
} 
```

"matched_children" is what I need. I hope to get parent docs with matched children docs. Is there someway to meet this requirement&#65311; 
</description><key id="66798623">10450</key><summary>Is there someway to get parent doc with hitted children docs when using "has_children" to search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shinyke</reporter><labels /><created>2015-04-07T06:56:20Z</created><updated>2015-04-08T02:23:07Z</updated><resolved>2015-04-07T12:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T12:21:35Z" id="90531856">See http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html#parent-child-inner-hits (new in 1.5.0)
</comment><comment author="shinyke" created="2015-04-08T02:23:07Z" id="90783383">@clintongormley  Thx!!!! "inner_hits"  is  what I want. It's very helpful~
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add --all flag to create-bwc script to regenerate all indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10449</link><project id="" key="" /><description>It is currently a pain to regenerate all the bwc indexes when making
a change to the generation script.  This makes it a single command.

Also, updated the script to run on python 3.
</description><key id="66774504">10449</key><summary>Tests: Add --all flag to create-bwc script to regenerate all indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-07T04:50:14Z</created><updated>2015-04-07T15:40:19Z</updated><resolved>2015-04-07T15:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-07T07:03:21Z" id="90406438">+1 this will be very useful!
</comment><comment author="mikemccand" created="2015-04-07T08:19:14Z" id="90454865">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mappings are unexpectedly modified if analysis plugin is removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10448</link><project id="" key="" /><description>Say you're using a custom analysis plugin (perhaps one that defines a "canadian" analyzer), and add a mapping that refers to this analyzer, plus a document:

https://gist.github.com/jtibshirani/1ac46d1624655ba1b694

If you bring down the cluster, remove the plugin from each 'plugins' directory, then restart the cluster, Elasticsearch will throw a MapperParsingException, but then do a dynamic mapping update using the contents of the document. The mappings are then permanently changed, which can often render the index useless. This is particularly bad because as far as I know, there's no way to force the mappings back to their original version (besides hacking the index metadata file).

This only seems to be an issue in ES 1.4.5 and earlier.
</description><key id="66744379">10448</key><summary>mappings are unexpectedly modified if analysis plugin is removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jtibshirani</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-04-07T01:15:45Z</created><updated>2016-01-15T19:32:35Z</updated><resolved>2016-01-15T19:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T19:32:35Z" id="172061975">This no longer seems to be the case - Elasticsearch keeps throwing exceptions but doesn't change the mapping any longer.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hanging transport connection thread on EC2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10447</link><project id="" key="" /><description>Observed on 1.4.4, 1.5.0.

To repro:  Set up four-node cluster on EC2, 1 master, 2 data, 1 client only.  Under certain load test conditions with load driving against the client node only, the client node reaches a state where it can no longer handle indexing or query operations, and many administrative commands (_cat) fail.

_cat/indices and _cat/shards will fail 100% of the time.  _cat/nodes will fail sometimes, succeed others.  This always occurs in a regular repeating pattern, with the same number of successes and failures each time.

Restarting the client node always clears the problem.

Restarting one of the data nodes may clear the problem.

The problem never occurs if there is only one data node.

The behavior around the restart of the data node suggests that this is caused by a hanging transport thread in the client.  When the data node connected to that thread is restarted, the transport thread is knocked loose and the client returns to normal operation.

A test case that can regularly cause the problem is available on a separate machine in the same EC2 zone.  Please contact me for access to the test system where this can be reproduced.
</description><key id="66706076">10447</key><summary>Hanging transport connection thread on EC2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>:Network</label><label>bug</label></labels><created>2015-04-06T21:08:06Z</created><updated>2015-04-08T17:37:38Z</updated><resolved>2015-04-08T17:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2015-04-06T21:10:22Z" id="90245390">The same tests performed by our customer on a non-EC2 cluster did not result in the same error.  This seems to be an edge case around EC2's networking and the load they are using to drive the client node.

Configuration is completely vanilla except for the use of unicast discovery.  No plugins installed.
</comment><comment author="nornagon" created="2015-04-06T22:22:31Z" id="90263060">I'm seeing what appears to be a similar issue. 1.4.4 and logstash 1.5.0rc2. I have 4 total nodes (m3.large), 3 data-only (data: true, master: false) and one client/master (data: false, master: false). Logstash (also hosted on ec2) is sending around 50 messages/sec via the "transport" protocol to the client node.

After a while, searches stop working and the client node no longer responds to `GET /_status` &#8212;&#160;the request just hangs. It's still responsive to `/_cluster/health`, though. Restarting the client node fixes it for a while.
</comment><comment author="nornagon" created="2015-04-06T22:39:57Z" id="90268550">The `/_cat/{nodes, shards, indices}` behaviour matches what @seang-es is seeing also (nodes succeeds sometimes, shards/indices fail always)
</comment><comment author="nornagon" created="2015-04-07T00:24:40Z" id="90295492">Possibly related: https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1317811 (I'm running 3.13.0-36 and seeing some 'rides the rocket' messages in syslog)
</comment><comment author="markwalkom" created="2015-04-07T01:16:58Z" id="90303768">Just to note, @nornagon mentioned he was seeing no disconnects after he upgraded his kernel;

&gt; nornagon
&gt; 11:01 warkolm: fwiw, updating my (ubuntu) kernel from 3.13.0-36 to 3.13.0-48 seems to have fixed the issue (at least, it hasn't happened in the last hour or so, where before it was happening every 10 minutes or so)
</comment><comment author="spinscale" created="2015-04-08T17:37:38Z" id="90983773">Hey,

indeed this is a kernel issue, which has been introduced in kernel 3.7. It is resolved upstream in 3.18 but I guess some vendors patched this in their own releases, ubuntu being one them, by fixing it in `3.13.0-46.75`

If you dont want to restart, you could also run `sudo ethtool -K eth0 sg off` - but this has a performance impact, as the kernel has to do things, that the NIC is supposed to do resulting in copying of more data.

thx to @nornagon for pointing into the right direction.

### Further Resources
- https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1317811
- https://bugs.launchpad.net/ubuntu/+source/linux-lts-raring/+bug/1195474
- http://ubuntu.5.x6.nabble.com/Trusty-Utopic-SRU-Avoid-quot-rides-the-rocket-quot-and-dropped-packages-td5091302.html
- http://packages.ubuntu.com/trusty/kernel/linux-image-3.13.0-48-generic and the [changelog](http://changelogs.ubuntu.com/changelogs/pool/main/l/linux/linux_3.13.0-48.80/changelog) mentioning it
- https://lkml.org/lkml/2014/8/11/416
- http://patchwork.ozlabs.org/patch/379108/
- https://www.kernel.org/pub/linux/kernel/v3.0/ChangeLog-3.18.1
- http://vger.kernel.org/~davem/skb.html
- http://www.brendangregg.com/blog/2014-09-11/perf-kernel-line-tracing.html
- https://github.com/elastic/elasticsearch/issues/10447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing 1 escape character in example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10446</link><project id="" key="" /><description /><key id="66680803">10446</key><summary>Missing 1 escape character in example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joelbourbon</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-04-06T19:05:10Z</created><updated>2015-04-07T12:14:03Z</updated><resolved>2015-04-07T12:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-07T11:41:20Z" id="90518478">Hi @joelbourbon 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
https://www.elastic.co/contributor-agreement
</comment><comment author="joelbourbon" created="2015-04-07T11:45:16Z" id="90520383">I did right after I submitted the pull request. I also received the e-mail validation.

Want me to do it again ?
</comment><comment author="clintongormley" created="2015-04-07T11:48:25Z" id="90521770">Hi @joelbourbon 

Sorry, for some reason I'm not seeing your name or github username in the list.  Please could you sign the individual contributor version?

sorry for the runaround

thanks
</comment><comment author="joelbourbon" created="2015-04-07T11:52:25Z" id="90523068">Hi @clintongormley,

I just signed the CLA again. Hope it works this time!
</comment><comment author="clintongormley" created="2015-04-07T12:10:32Z" id="90530243">thanks @joelbourbon - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Document `indices.recovery.concurrent_small_file_streams`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10445</link><project id="" key="" /><description /><key id="66657214">10445</key><summary>[DOCS] Document `indices.recovery.concurrent_small_file_streams`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>docs</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-06T17:17:42Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-06T17:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-06T17:34:58Z" id="90165806">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: don't allow index.merge.async_interval=0 to disable all segment merging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10444</link><project id="" key="" /><description>I think we currently allow 0 here to mean "no segments are ever merged" ... I think we should disallow that?
</description><key id="66655128">10444</key><summary>Core: don't allow index.merge.async_interval=0 to disable all segment merging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels /><created>2015-04-06T17:05:53Z</created><updated>2015-04-09T08:49:42Z</updated><resolved>2015-04-07T15:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-06T17:11:31Z" id="90159610">Woops, I removed 2.0.0 fix version here ... this only applies to 1.x since this option is gone in 2.x.
</comment><comment author="mikemccand" created="2015-04-07T15:48:23Z" id="90616328">Actually, I guess a value of 0 to disable async merges makes sense if you had set index.merge.force_async_merge to false (so Lucene can kick off its own merges)... and anyway this setting is gone in 2.0, so I don't think we should do anything for 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Revert delete-by-query deprecations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10443</link><project id="" key="" /><description>Un-deprecate delete-by-query since we will not remove it for 2.0 (#10288), but rather hopefully soon replace it with a less trappy implementation (#7052).
</description><key id="66640107">10443</key><summary>Revert delete-by-query deprecations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>non-issue</label><label>v1.5.1</label><label>v1.6.0</label></labels><created>2015-04-06T16:07:47Z</created><updated>2015-04-09T08:49:55Z</updated><resolved>2015-04-07T08:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-06T16:52:04Z" id="90152202">LGTM, just a couple very minor requests.
</comment><comment author="mikemccand" created="2015-04-06T17:43:04Z" id="90168280">Thanks @rjernst, I added the missing newlines, removed the empty javadocs...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CopyOnWriteHashMap/Set tests are extremely slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10442</link><project id="" key="" /><description>Suite: org.elasticsearch.common.collect.CopyOnWriteHashMapTests
Completed on J0 in 153.03s, 4 tests

Suite: org.elasticsearch.common.collect.CopyOnWriteHashSetTests
Completed on J2 in 180.31s, 3 tests

I do not understand why we have our own copy-on-write hashmaps/sets, nor why we would need tests for them that take minutes to run?
</description><key id="66584008">10442</key><summary>CopyOnWriteHashMap/Set tests are extremely slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-06T11:21:20Z</created><updated>2015-04-07T11:39:23Z</updated><resolved>2015-04-07T11:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-06T18:11:46Z" id="90179696">This is super slow indeed, I will look into it.

The history here is that we used to use immutable maps that we would copy whenever we would need to make changes but it did not play well with dynamic mappings (O(n) field introduction, see https://github.com/elastic/elasticsearch/pull/6707) so we finally used these new classes (see https://github.com/elastic/elasticsearch/pull/7486) which are immutable too but have faster modifications.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New Feature - sourceAsMap()/sourceAsString() for TermVectorsResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10441</link><project id="" key="" /><description>I recently tried to get the result of a TermVectorsRequest as a map/json string and found no suitable function to do so.

GetResponse already has that feature: elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java

I managed to do convert the result into a string with the following code:

``` java
TermVectorResponse resp = client.prepareTermVector().setIndex(index).setType(type).setId(id).execute().actionGet();
XContentBuilder builder = XContentFactory.jsonBuilder();
builder.startObject();
resp.toXContent(builder, ToXContent.EMPTY_PARAMS);
builder.endObject();
```

With:

``` java
Map&lt;String, Object&gt; map = XContentHelper.convertToMap(builder.bytes(), false).v2();
```

to convert it into a map

or

```
builder.string();
```

to convert it into a string

I would like to see this functionality as easy to use functions for TermVectorsResponse
(sourceAsMap() and sourceAsString())

I could also implement that in the TermVectorsResponse class if you also find it useful.
</description><key id="66581146">10441</key><summary>New Feature - sourceAsMap()/sourceAsString() for TermVectorsResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sleighsoft</reporter><labels><label>:Term Vectors</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-06T10:57:03Z</created><updated>2016-01-17T16:21:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-06T18:24:08Z" id="90184719">@alexksikes any thoughts?
</comment><comment author="alexksikes" created="2015-04-10T07:41:18Z" id="91468749">I agree that would be a useful feature, that is having the choice between string or map representation for term vectors. This would be similar to GetResponse#getSourceAsString or GetResponse#getSourceAsMap, but on the term vectors returned, not on the source, right?
</comment><comment author="sleighsoft" created="2015-04-10T07:46:53Z" id="91469308">Yes that's right.
I just mentioned it for better understanding.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with floating fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10440</link><project id="" key="" /><description>Have problem with Elastic 1.4.4. Query result without scripting:

```
{
    "took": 7,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 33,
        "max_score": null,
        "hits": [{
            "_index": "logstash-2015.03.13",
            "_type": "gelf_photo",
            "_id": "AUwSeR5Mf_J86mgo9hKZ",
            "_score": null,
            "_source": {
                "version": "1.0",
                "type": "gelf_photo",
                "time_done_full": "0.0009758472442627",
                "time_done": 0.0,
                "@version": "1",
                "@timestamp": "2015-03-13T09:31:09.067Z",
                "_class__": "",
                "_method__": "",
                "_extra__": "null",
                "_request_id__": "45e21fc5e190de6269567c57c5f2177f0009e188"
            },
            "sort": [1426239069067]
        }]
    }
}

```

Query result with scripting:

```
{
    "took": 11,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 33,
        "max_score": null,
        "hits": [{
            "_index": "logstash-2015.03.13",
            "_type": "gelf_photo",
            "_id": "AUwSeR5Mf_J86mgo9hKZ",
            "_score": null,
            "fields": {
                "s_time_done": [" \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"]
            },
            "sort": [1426239069067]
        }]
    }
}
```

Result of`http://logstash.test:9200/logstash-2015.03.13/_mappings`:

```
...
"time_done": {
"type": "double"
},
...
```
</description><key id="66519779">10440</key><summary>Problem with floating fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">UnderGreen</reporter><labels><label>feedback_needed</label></labels><created>2015-04-06T03:55:23Z</created><updated>2015-04-26T20:07:45Z</updated><resolved>2015-04-26T20:07:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-06T18:19:03Z" id="90182753">@UnderGreen you want to provide the queries as well?

Also, did you map the `time_done` field manually, or with dynamic mapping?
</comment><comment author="clintongormley" created="2015-04-26T20:07:45Z" id="96429456">No more info provided. Feel free to reopen if you can reproduce this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update all examples to use full path names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10439</link><project id="" key="" /><description>With #9670 in place, all doc examples which reference fields need to be updated to use full path names, as the short name variants will no longer work.

Should probably add a note to http://www.elastic.co/guide/en/elasticsearch/reference/master/api-conventions.html as well.
</description><key id="66437378">10439</key><summary>Update all examples to use full path names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2015-04-05T13:10:05Z</created><updated>2015-04-05T13:11:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T13:10:27Z" id="89769539">@debadair could you add this to your list please
</comment><comment author="clintongormley" created="2015-04-05T13:11:52Z" id="89769600">Also need to remove any mention of using a `type.` prefix when using the full path name, as this no longer works (see #9492)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>parameter script_file does not seem to work (but "script": "script_file_name" does) </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10438</link><project id="" key="" /><description>The documentation at http://www.elastic.co/guide/en/elasticsearch/reference/1.x/modules-scripting.html suggests that you can use a script stored in {{conf.dir}}/scripts/name.groovy by specifying "script_file": "name" in the query. However,  when I use "script_file" I get a NulllPointerException. However, using "script" and pointing to the script filename seems to work fine. So, it seems that the docs are inconsistent with the actual behaviour, at least with version 1.4.4

A "curl" test file and session transcript are available here: https://gist.github.com/vanatteveldt/59ff5730b025270e8035
</description><key id="66430659">10438</key><summary>parameter script_file does not seem to work (but "script": "script_file_name" does) </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vanatteveldt</reporter><labels /><created>2015-04-05T11:50:47Z</created><updated>2015-04-05T19:42:54Z</updated><resolved>2015-04-05T19:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:42:54Z" id="89838847">Hi @vanatteveldt 

Thanks for reporting. This should be fixed already in #7977
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch sometimes starts consuming a lot of CPU indefinitely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10437</link><project id="" key="" /><description>I am running a logs server on AWS. The total CPU usage is typically 3-5%.

Sometimes, elasticsearch starts taking up much more CPU and stays at that level until I SSH into the server and restart it.

This below image shows what typically happens. At an arbitrary time, CPU load dramatically increases (even though there is no corresponding increase in new data) and stays there until I log in to the server and restart elasticsearch.

![elasticsearch](https://cloud.githubusercontent.com/assets/321497/6995371/984c23c0-db11-11e4-80fa-a02122b58bd8.png)

I am running 1.5.0 on Ubuntu 14.04 using the official repository package.

Please let me know what additional information I should provide and how to get that information.
</description><key id="66387900">10437</key><summary>elasticsearch sometimes starts consuming a lot of CPU indefinitely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erjiang</reporter><labels><label>feedback_needed</label></labels><created>2015-04-05T01:30:31Z</created><updated>2017-04-21T09:44:29Z</updated><resolved>2015-08-26T19:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-05T08:45:36Z" id="89735923">when the server is under high CPU load, can you first check if its the ES process, and if so, can you issue the hot threads api and post back the response of it here? http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html
</comment><comment author="erjiang" created="2015-04-07T13:44:56Z" id="90558087">Here is the output of hot_threads after elasticsearch started consuming extra CPU again. It is now consistently using approx 40-60% CPU up from 4-8% CPU.

```
::: [MODAM][tB-KGWlsQ4KDt1PUMleoyQ][localhost][inet[/10.0.3.26:9300]]
   Hot threads at 2015-04-07T13:40:51.020Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

    0.0% (133.7micros out of 500ms) cpu usage by thread 'elasticsearch[MODAM][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
```

Running it again, I get:

```
::: [MODAM][tB-KGWlsQ4KDt1PUMleoyQ][localhost][inet[/10.0.3.26:9300]]
   Hot threads at 2015-04-07T13:43:43.961Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

```

Please let me know what other information I can provide.
</comment><comment author="clintongormley" created="2015-04-07T17:24:10Z" id="90654373">Hi @erjiang 

Are you seeing any logs in syslog about "riding the rocket"?  See https://github.com/elastic/elasticsearch/issues/10447#issuecomment-90295492
</comment><comment author="erjiang" created="2015-04-09T19:04:53Z" id="91329286">We didn't see the "riding the rocket" message.

So far our best guess is that elasticsearch used up its available Java heap space, causing GC to continually run and consume cycles. That would explain why the problem would occur after a day or two and go away after restarting elasticsearch.
</comment><comment author="clintongormley" created="2015-04-13T08:58:45Z" id="92277786">OK, so sounds like memory pressure.  Are you using a lot of fielddata (ie per-doc field values loaded for sorting, aggs, or scripting)?  You can check with:

```
GET /_nodes/stats/indices/fielddata?fields=*&amp;human&amp;pretty
```

If so, consider switching those fields to use `doc_values: true` (which you can only do on a new index) to shift the memory use from your heap to the file system cache.  This will be the new default in 2.0.
</comment><comment author="jpountz" created="2015-08-26T19:20:44Z" id="135144927">Closing due to lack of feedback
</comment><comment author="pjcard" created="2015-09-28T15:26:28Z" id="143777253">I too saw this issue, with HashedWheelTimer appearing as the only process in hot threads. I've restarted my server (rather than process), and I'll report back as to whether it recurs.
</comment><comment author="mausch" created="2015-09-28T15:33:08Z" id="143780081">In my case, something similar was happening because I had too many indices open. Closing old indices with Curator stopped that behaviour. https://www.elastic.co/blog/curator-tending-your-time-series-indices
</comment><comment author="andrenarchy" created="2016-05-15T12:31:22Z" id="219283162">I also experienced a constant cpu load without any activity on the ES cluster. Increasing ES_HEAP_SIZE to half of my system's memory fixed it. @erjiang: Thanks for the hint!

Is there a way how ES can detect such a situation? It would be very helpful if a warning would appear in the log messages.
</comment><comment author="jasontedor" created="2016-05-17T19:01:22Z" id="219819808">&gt; Is there a way how ES can detect such a situation? It would be very helpful if a warning would appear in the log messages.

I opened #18419.
</comment><comment author="maoxiajun" created="2017-04-21T09:44:29Z" id="296146097">`26.2% (131ms out of 500ms) cpu usage by thread 'elasticsearch[xxx][search][T#62]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)`

I can see this when use 'curl localhost:9200/_nodes/hot_threads'</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Template not getting applied with ES version 1.4.1 - Works for 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10436</link><project id="" key="" /><description>Hi, 

I have the following template

``` json
{
    "_source": {
        "enabled": false
    },
    "_all": {
        "enabled": false
    },
    "_timestamp": {
        "enabled": true,
        "store": true
    },
    "dynamic_templates": [
        {
            "template_timestamp": {
                "match": "timestamp",
                "mapping": {
                    "store": false,
                    "index": "not_analyzed",
                    "type": "date"
                }
            }
        },
        {
            "template_no_store_analyzed": {
                "match": "*",
                "match_mapping_type": "string",
                "mapping": {
                    "store": false,
                    "index": "not_analyzed",
                    "fields": {
                        "analyzed": {
                            "store": false,
                            "type": "string",
                            "index": "analyzed"
                        }
                    }
                }
            }
        },
        {
            "template_no_store": {
                "match_mapping_type": "date|boolean|double|long|integer",
                "match_pattern": "regex",
                "path_match": ".*",
                "mapping": {
                    "store": false,
                    "index": "not_analyzed"
                }
            }
        }
    ]
}
```

This template is applicable for all indexes and type - "document".

I am running an embedded instance in my java process for testing purposes.

When I am trying to index a document in the following method, above template is not getting applied - 

``` java
          connection.getClient()
                    .prepareIndex()
                    .setIndex("abc")
                    .setType("document")
                    .setId(document.getId())
                    .setTimestamp(Long.toString(timestamp))
                    .setSource(mapper.writeValueAsBytes(document.getData()))
                    .setConsistencyLevel(WriteConsistencyLevel.QUORUM)
                    .execute()
                    .get(2, TimeUnit.SECONDS);
```

However, when I am using BulkRequestBuilder and indexing same document, mappings are getting applied correctly. 

The same mapping works fine for ES 1.1.1, 1.2.4, 1.3.6 and 1.4.0. 

Am I missing something here ? Has any setting changed from 1.4.0 to 1.4.1 ?
</description><key id="66372100">10436</key><summary>Template not getting applied with ES version 1.4.1 - Works for 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">r0goyal</reporter><labels><label>:Java API</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v6.0.0</label></labels><created>2015-04-04T22:04:54Z</created><updated>2017-05-03T06:55:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:35:39Z" id="89838497">Hi @r0goyal 

You've given us incomplete information, eg how you set the template up, what the document contains, what you mean by "the mapping is not applied" (ie all of it? or just some field mappings?) etc.

A simple recreation of the problem would be appreciated.
</comment><comment author="r0goyal" created="2015-04-05T20:00:08Z" id="89843786">Hi @clintongormley 

Following is the code snippet

``` java
    public static void main(String[] args) throws IOException {
        ImmutableSettings.Builder elasticsearchSettings = ImmutableSettings.settingsBuilder()
                .put("http.enabled", "false")
                .put("path.data", "target/" + UUID.randomUUID().toString());

        Node esNode = nodeBuilder()
                .local(true)
                .settings(elasticsearchSettings.build())
                .node();
        Client client = esNode.client();

        PutIndexTemplateRequest templateRequest = getClusterTemplateMapping(client.admin().indices());
        client.admin().indices().putTemplate(templateRequest).actionGet();

        String index = "foxtrot-abcd";
        String type = "document";

        Map&lt;String, String&gt; data = Collections.singletonMap("abcd", "abcd");
        client.prepareIndex()
                .setIndex(index)
                .setType(type)
                .setId(UUID.randomUUID().toString())
                .setTimestamp(Long.toString(System.currentTimeMillis()))
                .setConsistencyLevel(WriteConsistencyLevel.QUORUM)
                .setSource(new ObjectMapper().writeValueAsString(data))
                .execute()
                .actionGet();

        GetMappingsResponse mappingsResponse = client.admin().indices()
                .prepareGetMappings(index)
                .execute().actionGet();
        MappingMetaData metaData = mappingsResponse.getMappings().get(index).get(type);
    }
    public static PutIndexTemplateRequest getClusterTemplateMapping(IndicesAdminClient indicesAdminClient) {
        PutIndexTemplateRequestBuilder builder = new PutIndexTemplateRequestBuilder(indicesAdminClient, "generic_template");
        builder.setTemplate("foxtrot-*");
        builder.addMapping("document", "{\n" +
                "            \"_source\" : { \"enabled\" : false },\n" +
                "            \"_all\" : { \"enabled\" : false },\n" +
                "            \"_timestamp\" : { \"enabled\" : true, \"store\" : true },\n" +
                "\n" +
                "            \"dynamic_templates\" : [\n" +
                "                {\n" +
                "                    \"template_timestamp\" : {\n" +
                "                        \"match\" : \"timestamp\",\n" +
                "                        \"mapping\" : {\n" +
                "                            \"store\" : false,\n" +
                "                            \"index\" : \"not_analyzed\",\n" +
                "                            \"type\" : \"date\"\n" +
                "                        }\n" +
                "                    }\n" +
                "                },\n" +
                "                {\n" +
                "                    \"template_no_store_analyzed\" : {\n" +
                "                        \"match\" : \"*\",\n" +
                "                        \"match_mapping_type\" : \"string\",\n" +
                "                        \"mapping\" : {\n" +
                "                            \"store\" : false,\n" +
                "                            \"index\" : \"not_analyzed\",\n" +
                "                            \"fields\" : {\n" +
                "                                \"analyzed\": {\n" +
                "                                    \"store\" : false,\n" +
                "                                    \"type\": \"string\",\n" +
                "                                    \"index\": \"analyzed\"\n" +
                "                                }\n" +
                "                            }\n" +
                "                        }\n" +
                "                    }\n" +
                "                },\n" +
                "                {\n" +
                "                    \"template_no_store\" : {\n" +
                "                        \"match_mapping_type\": \"date|boolean|double|long|integer\",\n" +
                "                        \"match_pattern\": \"regex\",\n" +
                "                        \"path_match\": \".*\",\n" +
                "                        \"mapping\" : {\n" +
                "                            \"store\" : false,\n" +
                "                            \"index\" : \"not_analyzed\"\n" +
                "                        }\n" +
                "                    }\n" +
                "                }\n" +
                "            ]\n" +
                "        }");
        return builder.request();
    }
```

With ES 1.1.1 to 1.4.0 in classpath, serializing metaData gives 

``` json
{
    "dynamic_templates": [
        {
            "template_timestamp": {
                "mapping": {
                    "index": "not_analyzed",
                    "store": false,
                    "type": "date"
                },
                "match": "timestamp"
            }
        },
        {
            "template_no_store_analyzed": {
                "mapping": {
                    "index": "not_analyzed",
                    "store": false,
                    "fields": {
                        "analyzed": {
                            "index": "analyzed",
                            "store": false,
                            "type": "string"
                        }
                    }
                },
                "match": "*",
                "match_mapping_type": "string"
            }
        },
        {
            "template_no_store": {
                "mapping": {
                    "index": "not_analyzed",
                    "store": false
                },
                "match_mapping_type": "date|boolean|double|long|integer",
                "match_pattern": "regex",
                "path_match": ".*"
            }
        }
    ],
    "_all": {
        "enabled": false
    },
    "_timestamp": {
        "enabled": true,
        "store": true
    },
    "_source": {
        "enabled": false
    },
    "properties": {
        "abcd": {
            "type": "string",
            "index": "not_analyzed",
            "fields": {
                "analyzed": {
                    "type": "string"
                }
            }
        }
    }
}
```

However with ES 1.4.1 and above, only following mapping is present - 

``` json
{"properties":{"abcd":{"type":"string"}}}
```

Please let me know if you need anything else as well.
</comment><comment author="r0goyal" created="2015-04-14T09:46:49Z" id="92725351">@clintongormley Any update on this ?
</comment><comment author="clintongormley" created="2015-04-14T14:00:46Z" id="92862304">Can anybody who can read Java give some feedback here?
</comment><comment author="javanna" created="2015-04-20T13:59:55Z" id="94458671">I had a look and can confirm this behaviour. You should wrap the whole mapping object, including `dynamic_templates` etc. into a `document` object in the json, which is what your type is called. Not sure why this used to work with 1.4.0 to be honest (I confirm it does), do you know @clintongormley ?
</comment><comment author="r0goyal" created="2015-04-21T18:11:25Z" id="94892589">@javanna I am already wrapping this whole mapping in "document" object in the Json.  Look at the getClusterTemplateMapping() method. 
</comment><comment author="javanna" created="2015-04-22T07:12:50Z" id="95055442">Hi @r0goyal I ran your code, I see you are passing in the type as an argument, but the mapping itself doesn't contain the document type: `builder.addMapping("document", "{\n" + \"_source\" : { \"enabled\" : false },...."` should be `builder.addMapping("document", "{ \"document\" : { \n" + \"_source\" : { \"enabled\" : false },....}"`, that is what I meant. If you do that you should work around the problem you are seeing.
</comment><comment author="clintongormley" created="2015-04-25T18:39:33Z" id="96258488">@javanna that seems very odd.  Is `addMapping` the equivalent of the update mapping REST API?  

In the REST layer, we support passing the mapping with the type name and without.  In fact, I thought we preferred the version without the type name, but I see the docs still show the example with type name: http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html

No idea what changed in 1.4
</comment><comment author="javanna" created="2015-04-28T08:27:36Z" id="96971942">agreed @clintongormley it is odd, I have no idea either what changed in 1.4 to be honest, only tested and acknowledged the problem.
</comment><comment author="r0goyal" created="2015-04-28T13:34:58Z" id="97066420">@javanna Yes it started working with the fix suggested by you. Thanks a lot.
</comment><comment author="clintongormley" created="2016-01-15T19:26:13Z" id="172060358">In summary, an `addMapping` request in the Java API requires the mapping to be wrapped in a JSON object with the type name as a key, while the REST API allows you to specify the mapping directly without this extra layer.

Should we change the Java API to work in the same way as the REST layer?
</comment><comment author="clintongormley" created="2016-04-20T16:35:28Z" id="212504215">Another person just bitten by this in #17886 

We should make the Java API consistent with the REST api.  Should be easy to do as using a type name at the top level will throw an exception (bad mapping)
</comment><comment author="billnbell" created="2017-04-29T22:45:08Z" id="298199111">I got bitten today on 5.3.1</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to Lucene 5.1 snapshot r1671277</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10435</link><project id="" key="" /><description>Upgrades to latest snapshot of lucene 5.1 release branch.

We actually have interesting stuff in 5.2, but I wanted to do this step first. It helps test the lucene release and allows us to deal with changes to the new PostingsEnum api in 5.1, without also having to tackle differences in Spans and other things in 5.2 at the same time.
</description><key id="66340766">10435</key><summary>Update to Lucene 5.1 snapshot r1671277</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-04-04T16:55:18Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-04-07T10:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-06T08:52:47Z" id="89980584">+1
</comment><comment author="rmuir" created="2015-04-06T20:12:26Z" id="90226730">this upgrade is stuck on the same tests as the last one got stuck on. These only test via scripts and still have the same problems, still timesinks.

i'm not gonna do awaitsfix, i don't want to do it again. i want git rm.
</comment><comment author="rjernst" created="2015-04-06T21:48:15Z" id="90255768">I fixed the test in question (actually it was a bug in the index lookup stuff since the change in behavior). I will run the full tests a couple more times. 
</comment><comment author="dadoonet" created="2015-04-08T15:15:10Z" id="90946920">Also pushed in parent project with this commit: https://github.com/elastic/elasticsearch-parent/commit/24bc54c6597934db60e8949bbc0236f2ef8fa416
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] Old index tests can take a long time to create replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10434</link><project id="" key="" /><description>This seems to happen just for 0.90.13, when replicas is set to 2 (from a very cursory checking of the recent failures). When I run with the same seed on my local box, the test finishes fine.

Examples:
http://build-us-00.elastic.co/job/es_core_master_regression/1730/testReport/org.elasticsearch.bwcompat/OldIndexBackwardsCompatibilityTests/testOldIndexes/
http://build-us-00.elastic.co/job/es_core_master_regression/1731/testReport/org.elasticsearch.bwcompat/OldIndexBackwardsCompatibilityTests/testOldIndexes/
http://build-us-00.elastic.co/job/es_core_master_debian/4710/testReport/junit/org.elasticsearch.bwcompat/OldIndexBackwardsCompatibilityTests/testOldIndexes/
</description><key id="66338011">10434</key><summary>[CI] Old index tests can take a long time to create replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2015-04-04T16:24:12Z</created><updated>2015-04-10T06:32:15Z</updated><resolved>2015-04-09T16:38:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-07T08:20:47Z" id="90455265">I think it's useful to have back compat indices with "many" segments: this uncovered one nasty lucene back-compat bug (now fixed), that was only tickled if merges kicked off immediately on upgrade.
</comment><comment author="bleskes" created="2015-04-07T08:24:56Z" id="90457109">@mikemccand agreed it's well worth the extra time. We just need to figure out what exactly is slow there. If it ends being sane given the number of files, we'll just bump up the timeout.
</comment><comment author="rjernst" created="2015-04-09T16:38:27Z" id="91285710">We fixed this by increasing the timeout for ensureGreen, as well as setting replicas to always be 1, instead of 1 or 2.
</comment><comment author="javanna" created="2015-04-09T16:48:42Z" id="91288423">I guess we should remove the `AwaitsFix` from 1.5 branch then (not sure why it was annotated that way on 1.5 only).
</comment><comment author="s1monw" created="2015-04-09T16:49:09Z" id="91288507">@javanna I did this like 60 sec ago :)
</comment><comment author="javanna" created="2015-04-10T06:32:15Z" id="91450348">great, thanks @s1monw ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>maven emits scary warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10433</link><project id="" key="" /><description>At least with master (did not check other branches) I see this:

[WARNING] Some problems were encountered while building the effective model for org.elasticsearch:elasticsearch:jar:2.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-resources-plugin @ line 1104, column 21
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-resources-plugin @ line 1367, column 21
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
</description><key id="66337362">10433</key><summary>maven emits scary warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-04-04T16:15:23Z</created><updated>2015-04-09T08:05:56Z</updated><resolved>2015-04-09T08:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-04T16:17:28Z" id="89608544">@rmuir this should be fixed when we will merge the PR about parent project.
Parent project defines all plugins correctly.
</comment><comment author="rmuir" created="2015-04-04T16:17:56Z" id="89608573">Ok, thanks for looking @dadoonet !
</comment><comment author="dadoonet" created="2015-04-04T16:18:32Z" id="89608616">We can also fix it directly without merging my PR though.
</comment><comment author="rmuir" created="2015-04-04T16:19:33Z" id="89608676">Yeah, I don't know the branches impacted or which ones you plan to merge your PR to. I just didn't want it to get missed.
</comment><comment author="dadoonet" created="2015-04-04T16:24:25Z" id="89609371">+1! The original plan was to merge the PR in 1.5, 1.x and master.
Now 1.5 has been released, I think it will may be go to 1.x and master?

For info, PR is waiting for a review here: #9735 :)
</comment><comment author="rmuir" created="2015-04-04T17:03:24Z" id="89614610">Well, for the record, that PR just looks fantastic to me. Consolidating that stuff should really make plugins easier and better tested. But I don't know enough about maven to really give you a competent review.
</comment><comment author="tlrx" created="2015-04-07T09:44:36Z" id="90488999">@dadoonet @rmuir my fault - I should have seen those warnings when merging #10330. By the way I'm not sure the parent project PR will fix thoses warnings so I created #10453.
</comment><comment author="dadoonet" created="2015-04-07T09:46:33Z" id="90489748">@tlrx I agree. My initial thought was that there was duplicated maven plugins. Something I fixed while creating the parent project.
But, as Tanguy said, I would not have fixed the issue reported by @rmuir.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch does not start due to NoClassDefFoundError with an older Marvel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10432</link><project id="" key="" /><description>I upgraded Elasticsearch to _1.5.0_ but had the nasty surprise to not be able to start it anymore, due to a class that has disappeared in the meantime:

```
[2015-04-04 15:46:40,478][ERROR][bootstrap                ] Exception
org.elasticsearch.common.util.concurrent.ExecutionError: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalI
ndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
        at org.elasticsearch.common.inject.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:50)
        at org.elasticsearch.common.inject.InjectorImpl.initializeBinding(InjectorImpl.java:372)
        at org.elasticsearch.common.inject.BindingProcessor$1$1.run(BindingProcessor.java:148)
        at org.elasticsearch.common.inject.BindingProcessor.initializeBindings(BindingProcessor.java:204)
        at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:119)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.MembersInjectorStore.get(MembersInjectorStore.java:68)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:67)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:37)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:33)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 19 more
Caused by: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at java.lang.Class.getDeclaredFields0(Native Method)
        at java.lang.Class.privateGetDeclaredFields(Class.java:2499)
        at java.lang.Class.getDeclaredFields(Class.java:1811)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:378)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:376)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectorsForMembers(InjectionPoint.java:351)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectionPoints(InjectionPoint.java:345)
        at org.elasticsearch.common.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:287)
        at org.elasticsearch.common.inject.MembersInjectorStore.createWithListeners(MembersInjectorStore.java:80)
        at org.elasticsearch.common.inject.MembersInjectorStore.access$000(MembersInjectorStore.java:36)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:45)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:41)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 33 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.indices.InternalIndicesService
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 50 more
```

This is perfectly reproduceable with Marvel 1.2.1. The latest Marvel 1.3.x works well. I'm more interested in knowing **why** than in getting this issue solved.
#### How come a site plugin (i.e. without a single trace of Java in it) induces a class not found error? Is there a compatibility layer that triggers this?
</description><key id="66336971">10432</key><summary>Elasticsearch does not start due to NoClassDefFoundError with an older Marvel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acarstoiu</reporter><labels /><created>2015-04-04T16:11:42Z</created><updated>2015-04-04T16:20:48Z</updated><resolved>2015-04-04T16:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-04T16:20:48Z" id="89608758">Marvel is not a site plugin. It contains also a jar.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document docvalues option for boolean field in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10431</link><project id="" key="" /><description>Similar to the doc fix for #9809, this got fixed for 2.0 in #7961
</description><key id="66336018">10431</key><summary>Document docvalues option for boolean field in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-04-04T16:03:46Z</created><updated>2015-04-29T14:00:28Z</updated><resolved>2015-04-29T14:00:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Deleting a mapping should not delete data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10430</link><project id="" key="" /><description>Hi, 

I am just deleting the mapping so that I can recreate the mapping with different core types for some fields (especially when "updating a mapping type" doesn't really do much good on changing core types at all). And even if we don't care about the past data that does not mean it should just delete the data. At the very least this should be more of a configurable option rather than an internal clean up in elastic search. I do not want to go about reindexing all my data just because I changed a core type. I just want to keep the past data as document storage and somehow mark them as "old data". But if I delete and recreate the mapping, I don't even have a choice to do that because the corresponding documents will be deleted!
</description><key id="66335856">10430</key><summary>Deleting a mapping should not delete data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shivangshah</reporter><labels /><created>2015-04-04T16:00:16Z</created><updated>2015-04-05T19:54:10Z</updated><resolved>2015-04-05T19:19:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:19:46Z" id="89835104">HI @shivangshah 

The ability to delete a mapping has been removed in master (see https://github.com/elastic/elasticsearch/issues/8877).  If you need to change the mapping, you need to reindex.
</comment><comment author="shivangshah" created="2015-04-05T19:22:56Z" id="89835814">@clintongormley but how do we update the mapping if there is no "delete" ? I understand the reindexing part. Right now if you don't delete and recreate you cannot update the core mapping of a field. How can you do it in master now?
</comment><comment author="clintongormley" created="2015-04-05T19:24:23Z" id="89836260">@shivangshah you can't.  you have to reindex.  changing a mapping means that your existing indexed data will now be incorrect.  the only way to correct that data is to reindex it all.
</comment><comment author="shivangshah" created="2015-04-05T19:27:51Z" id="89837259">@clintongormley So basically you can't delete, you can't update and you are stuck with whatever is created (by first document indexed or manually put mappings) for a given field mapping? 
</comment><comment author="clintongormley" created="2015-04-05T19:31:46Z" id="89838031">correct
</comment><comment author="shivangshah" created="2015-04-05T19:33:10Z" id="89838375">got it .. and this will be release in v2.0 I am guessing ? 
</comment><comment author="clintongormley" created="2015-04-05T19:48:49Z" id="89839662">Yes, but even with delete-mapping today, it is trappy.  you can still have vestiges of old fields in old segments, even though you have deleted the mapping.  So the same advice still applied: you want to change a field? reindex your data
</comment><comment author="shivangshah" created="2015-04-05T19:54:10Z" id="89841428">reindexing the data is not a problem. The problem is with reindexing the data in a whole different index by putting new mappings to it. That's just not a good solution. My feedback here would be let the clients update the mappings and just warn them to reindex the data .. I'd rather get all the data from the same index and reindex in the same one rather than going through the route of creating a new index, reindexing all the data there, update ALL the aliases that pointed to the old index and than drop the old index. The process is extremely cumbersome especially you know that your mappings are going to change a whole lot. I described a part of our usecase here: https://github.com/elastic/elasticsearch/issues/8870

I still believe that instead of throttling so much at the mapping level, make it more flexible for clients but make them aware of the risks (reindexing and stuff). You will have to reindex regardless when you update the mapping. The question is which is the easiest route for a client .. And my vote goes for flexible implementation + understanding of risks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove content thread safe from REST layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10429</link><project id="" key="" /><description>there is no need for this anymore, for some time, since in netty now we rely on copying over the buffer and reusing it
</description><key id="66323151">10429</key><summary>Remove content thread safe from REST layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-04T14:01:05Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-07T15:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-06T16:39:45Z" id="90139587">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing ID for an update/delete operation causes the entire bulk to fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10428</link><project id="" key="" /><description>If one of the actions from a bulk requires an ID (update/delete) but doesn't have it, the entire bulk fails. For example:

```
$ cat /tmp/test_bulk 
{"index":{}}
{"name":"test doc"}
{"delete":{}}

$ curl -XPOST localhost:9200/test/test/_bulk?pretty --data-binary @/tmp/test_bulk
{
  "error" : "ActionRequestValidationException[Validation Failed: 1: id is missing;]",
  "status" : 400
}
```

I would expect the index operation to succeed and the delete/update operation to fail. Or maybe there's a good reason for failing the whole bulk that I don't see?
</description><key id="66298089">10428</key><summary>Missing ID for an update/delete operation causes the entire bulk to fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels><label>:Bulk</label><label>discuss</label></labels><created>2015-04-04T09:35:28Z</created><updated>2016-01-27T11:52:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:12:39Z" id="89832790">Hmm. This is a parse validation which happens during the initial parse phase, as opposed to when the bulk request has been divided up into mini-bulks and forwarded to all nodes to succeed or fail as appropriate.

I can understand why it fails as it does today, but from a client perspective it makes it a bit harder to decided what to do here i suppose...
</comment><comment author="sherry-ger" created="2016-01-26T22:29:58Z" id="175268402">Version : ES 2.1.1

This also happens if the index does not exist. It seems all the bulk actions to the the index will fail instead of just the one that is malformed or with the incorrect syntax.  The error message to each operation regardless of correctness is the same.  Also, the index is not created.  For example,

```
POST _bulk
{ "index" : { "_index" : "my_index", "_type" : "test", "_id" : "5" } }
{ "user" : "user5" } 
{ "index" : { "_index" : "my_index", "_type" : "", "_id" : "2" } }
{ "user" : "user2" } 
{ "delete" : { "_index" : "my_index", "_type" : "test", "_id" : "5" } }
```

Here are the error messages for all items.

```
{
  ...
  "error": {
    "type": "mapper_parsing_exception",
    "reason": "Failed to parse mapping []: mapping type name is empty",
    "caused_by": {
      "type": "invalid_type_name_exception",
      "reason": "mapping type name is empty"
    }
  }
}
```

This behavior is inconsistent with the documentation.  Should the "correct" operations be successful?
</comment><comment author="clintongormley" created="2016-01-27T11:52:32Z" id="175580256">Yeah, this is tricky.  This is essentially a malformed request which is detected before the mini bulk requests are sent to the respective shards for processing.  But I agree that it'd be nicer if these were handled in the same way as other errors.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better migration path to new script parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10427</link><project id="" key="" /><description>For 2.0 we modified and unified our script parameters with #7977 . This was done in a backwards compatible manner in 1.x, but broke backwards compatibility on master, as only new parameters are read on master. A note has been added to our migration docs, but I fear that is not enough.

We should consider being more polite to our users and tell them what they have to change in their request if they send the old parameters, and fail gracefully. At the moment, if you send a request that includes the old script parameters, they get ignored, and we might end up throwing a NPE, or outputting an `AssertionError` with assertions enabled.

I saw this when sending a request referring to a file script using `file` instead of `script_file`. I'm sure there are other cases. Instead of assertions there we should have proper exceptions that get thrown whenever one of the mandatory arguments is not present.
</description><key id="66292445">10427</key><summary>Better migration path to new script parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2015-04-04T08:36:57Z</created><updated>2015-06-12T17:01:33Z</updated><resolved>2015-06-12T09:00:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-04T08:37:16Z" id="89528387">Assigning this to you @colings86 ;)
</comment><comment author="clintongormley" created="2015-06-07T17:06:29Z" id="109773752">@colings86 is this still relevant?
</comment><comment author="colings86" created="2015-06-12T08:34:22Z" id="111413802">@clintongormley I need to check. I'll update this issue when I have tested
</comment><comment author="colings86" created="2015-06-12T09:00:49Z" id="111419979">WIth the new script API updates, the error messages are a lot better when old parameters are passed in. Closing this issue, but feel free to open issues for specific scriptable APIs if necessary
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A VM pause (due to GC, high IO load, etc) can cause the loss of inserted documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10426</link><project id="" key="" /><description>Following up on #7572 and #10407, I've found that Elasticsearch will lose inserted documents even in the event of a node hiccup due to garbage collection, swapping, disk failure, IO panic, virtual machine pauses, VM migration, etc. https://gist.github.com/aphyr/b8c98e6149bc66a2d839 shows a log where we pause an elasticsearch primary via SIGSTOP and SIGCONT. Even though no operations can take place against the suspended node during this time, and a new primary for the cluster comes to power, it looks like the old primary is still capable of acking inserts which are not replicated to the new primary--somewhere right before or right after the pause. The result is the loss of ~10% of acknowledged inserts.

You can replicate these results with Jepsen (commit e331ff3578), by running `lein test :only elasticsearch.core-test/create-pause` in the `elasticsearch` directory.

Looking through the Elasticsearch cluster state code (which I am by no means qualified to understand or evaluate), I get the... really vague, probably incorrect impression that Elasticsearch might make a couple assumptions:
1. Primaries are considered authoritative "now", without a logical clock that identifies what "now" means.
2. Operations like "insert a document" don't... seem... to carry a logical clock with them allowing replicas to decide whether or not the operation supercedes their state, which means that messages delayed in flight can show up and cause interesting things to happen.

Are these at all correct? Have you considered looking in to an epoch/term/generation scheme? If primaries are elected uniquely for a certain epoch, you can tag each operation with that epoch and use it to reject invalid requests from the logical past--invariants around advancing the epoch, in turn, can enforce the logical monotonicity of operations. It might make it easier to tamp down race conditions like this.
</description><key id="66270787">10426</key><summary>A VM pause (due to GC, high IO load, etc) can cause the loss of inserted documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">aphyr</reporter><labels><label>bug</label><label>resiliency</label></labels><created>2015-04-04T03:49:11Z</created><updated>2015-04-28T14:58:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-04T06:11:18Z" id="89510312">Thx @aphyr . In general I can give a quick answer to, while we research the rest:

&gt; Have you considered looking in to an epoch/term/generation scheme?

This is indeed the current plan.
</comment><comment author="bleskes" created="2015-04-10T12:58:15Z" id="91546379">We have made some effort to reproduce this failure. In general, we see GC as just another disruption that can happen, the same way we view network issues and file corruptions. If anyone is interested in the work we do there, the [org.elasticsearch.test.disruption](https://github.com/elastic/elasticsearch/tree/master/src/test/java/org/elasticsearch/test/disruption) package and [DiscoveryWithServiceDisruptionsTests](https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsTests.java) are a good place to look.

In the Jepsen runs that failed for us, Jepsen created an index and have paused the master node's JVM where the primary of one of the index shards was allocated to that master node. At the time the JVM was paused, no other replica of this shard was fully initialized after initial creation. Because the master JVM was pause, other nodes elected another master but that cluster had no copies left for that specific shard. This left the cluster at a red state. When the node is unpaused it rejoins the cluster. The shard is not re-allocated because we require a qurom of copies to assign a primary (in order to make sure we do not reuse a dated copy). As such the cluster stays red and all the data previously indexed into this shard is not available for searches.

When we changed Jepsen to wait for all replicas to be assigned before starting the nemsis, the failure doesn't happen anymore. This change, and some other improvements are part this [PR](https://github.com/aphyr/jepsen/pull/51) to Jepsen.

That said, because of the similar nature between GC and an unresponsive network, there is still small window to loose documents which is captured by #7572 and documented [on the resiliency status page](http://www.elastic.co/guide/en/elasticsearch/resiliency/current/#_loss_of_documents_during_network_partition_status_ongoing)

@aphyr can you confirm that changes in the [PR](https://github.com/aphyr/jepsen/pull/51) offers the same behavior for you?
</comment><comment author="aphyr" created="2015-04-15T04:15:18Z" id="93182887">Thanks for this, @bleskes! I have been super busy with a few other issues but this is the last one I have to clear before talks go! I'll take a look tomorrow morning. :)
</comment><comment author="bleskes" created="2015-04-21T21:39:27Z" id="94950019">@aphyr re our previous discussion of:

&gt; &gt; Have you considered looking in to an epoch/term/generation scheme?
&gt; &gt; This is indeed the current plan.

If you're curious - I've open a (high level) issue describing our current thinking - see #10708 . 
</comment><comment author="aphyr" created="2015-04-28T05:35:14Z" id="96918655">I've merged your PR, and can confirm that ES still drops documents when a primary process is paused.

``` clj
{:valid? false,
 :lost "#{1761}",
 :recovered
 "#{0 2..3 8 30 51 73 97 119 141 165 187 211 233 257 279 302 324 348 371 394 436 457 482 504 527 550 572 597 619 642 664 688 711 734 758 781 804 827 850 894 911 934 957 979 1003 1025 1049 1071 1092 1117 1138 1163 1185 1208 1230 1253 1277 1299 1342 1344 1350 1372 1415 1439 1462 1485 1508 1553 1576 1599 1623 1645 1667 1690 1714 1736 1779 1803 1825 1848 1871 1893 1917 1939 1964 1985 2010 2031 2054 2077 2100 2123 2146 2169 2192}",
 :ok "#{0..1344 1346..1392 1394..1530 1532..1760 1762..2203}",
 :recovered-frac 24/551,
 :unexpected-frac 0,
 :unexpected "#{}",
 :lost-frac 1/2204,
 :ok-frac 550/551}
```
</comment><comment author="dakrone" created="2015-04-28T14:58:53Z" id="97094610">@aphyr thanks for running it! I think the PR helps remove the index not being in a green state before starting the test as a cause of document loss (not the only cause). I will keep running the test with additional logging to try and reproduce the failure you see.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mlt does not support the q parameter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10425</link><project id="" key="" /><description>More like this functionality completely ignores the _q_ parameter. The results are the same with or without it.

According to the documentation: [http://www.elastic.co/guide/en/elasticsearch/reference/1.5/search-more-like-this.html](http://www.elastic.co/guide/en/elasticsearch/reference/1.5/search-more-like-this.html)

_"The API simply results in executing a search request with moreLikeThis query (http parameters match the parameters to the more_like_this query). This means that the body of the request can optionally include all the request body options in the [search API](http://www.elastic.co/guide/en/elasticsearch/reference/1.5/search-search.html) (aggs, from/to and so on)."_

This means that the _q_ is a valid argument and that the expected behavior is to be able to filter the mlt results.

Here is an example of the url: http://localhost:9200/en_docs/document/1/_mlt?q=type:blog&amp;mlt_fields=title,summary&amp;pretty

The results I'm getting back don't meet the _q_ condition.
</description><key id="66254372">10425</key><summary>mlt does not support the q parameter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">harph</reporter><labels><label>:More Like This</label></labels><created>2015-04-04T00:12:56Z</created><updated>2015-07-06T14:53:13Z</updated><resolved>2015-07-06T14:53:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:02:56Z" id="89832247">@alexksikes please could you take a look
</comment><comment author="alexksikes" created="2015-04-07T10:09:52Z" id="90495439">For your use case I would recommend switching to the MLT Query which, as being part of the query DSL, would allow you to filter out the results. What is meant in the documentation is that you can use any  of request body options of the search API but excluding a query, as the MLT API is already performing a query.  
</comment><comment author="harph" created="2015-04-10T13:00:44Z" id="91547302">Thank you @alexksikes That's exactly what I ended up doing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>http.publish_address should allow for more fine-grained configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10424</link><project id="" key="" /><description>Our use case for this ticket involves an ES cluster with an Nginx proxy in front of the HTTP API, to enforce TLS mutual authentication. This works perfectly fine for simple use cases, however, when using the ruby client with some of its more advanced/robust features (specifically, reload_connections) we noticed that we would configure a client, it would make a request to determine cluster state, and then attempt to perform communications over http://$IPADDRESS:9200, which is firewalled off. I attempted to set http.publish_host and publish_port, but observed the following problems:

1) while http.publish_host had a visible impact on the output of the /_nodes endpoint, ultimately the client gem still wants to use the IP that the name I set resolves to, rather than the name I set itself.
2) setting http.publish_port had no visible impact
3) there is no apparent way to manipulate the protocol, to advertise https instead of http

A lot of these pain points could be resolved if a user were allowed to directly control http.publish_address, but I have been unsuccessful at doing so.

I realize that shield may solve this problem, but given that we have a functional TLS proxy, and a functional ES cluster, Shield seems a bit like using a jackhammer to drive a nail in this case.
</description><key id="66235707">10424</key><summary>http.publish_address should allow for more fine-grained configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">torrancew</reporter><labels /><created>2015-04-03T21:55:58Z</created><updated>2015-04-06T20:12:23Z</updated><resolved>2015-04-05T19:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:00:29Z" id="89832146">Hi @torrancew 

The ruby client should use the address provided in this output:

```
GET /_nodes/http
```

You should be able to set an `https` flag in the ruby client itself, which will automatically use https:// for sniffed nodes instead of http.  eg see https://github.com/elastic/elasticsearch-ruby/tree/master/elasticsearch-transport#authentication

Either way, Elasticsearch doesn't know about https, and it shouldn't.  
</comment><comment author="torrancew" created="2015-04-06T17:36:31Z" id="90166550">@clintongormley Thanks, that clears up the https matter, but I'm still having trouble setting an http publish port that differs from the port Elasticsearch is listening on. What is the proper config key for that? Currently, we set `http.publish_port` in elasticsearch.yml, with no visible effect.
</comment><comment author="clintongormley" created="2015-04-06T18:36:19Z" id="90190560">Hi @torrancew 

If I start Elasticsearch as follows:

```
 ./bin/elasticsearch --http.publish_port 9506
```

then it listens to localhost:9200 but the HTTP port is set to :9506

```
GET /_nodes/http
```

Returns:

```
{
   "cluster_name": "elasticsearch",
   "nodes": {
      "OrGQUNFjQe6BWmPPE7_1XA": {
         "name": "Tyrannosaur",
         "transport_address": "inet[/192.168.2.184:9300]",
         "host": "Slim-2.local",
         "ip": "192.168.2.184",
         "version": "1.5.0",
         "build": "5448160",
         "http_address": "inet[/192.168.2.184:9506]",
         "http": {
            "bound_address": "inet[/0:0:0:0:0:0:0:0:9200]",
            "publish_address": "inet[/192.168.2.184:9506]",
            "max_content_length_in_bytes": 104857600
         }
      }
   }
}
```
</comment><comment author="torrancew" created="2015-04-06T18:48:45Z" id="90194538">Thanks, this helps quite a bit, but I still have a few problems left to solve (or perhaps I'm doomed to not be able to use cluster enumeration?)

Primarily, I need to advertise a hostname, not an IP, for certificate validation to pass.
Past that, I see that I can set a scheme per-host when instantiating the Elasticsearch::Client instance, but I don't understand how that would carry over to hosts discovered after connecting. Is there a setting at a higher scope that I'm missing?
</comment><comment author="clintongormley" created="2015-04-06T19:04:04Z" id="90199920">&gt; Primarily, I need to advertise a hostname, not an IP, for certificate validation to pass.

Elasticsearch internally deals only with IP addresses, not hosts, so I think you're out of luck there.  Sniffing is used when the client is part of the same network as the clusters.  This sounds like you have a determined list of proxies in front of Elasticsearch, which you could list specificially rather than relying on sniffing.
</comment><comment author="torrancew" created="2015-04-06T20:12:23Z" id="90226721">@clintongormley thanks. In this case, our automation installs a local proxy per Elasticsearch cluster node, so the list is inherently as dynamic as the cluster itself. That said, we can probably make due with specifying the list, and maintaining it separately - we were just hoping that we could make the cluster do the work for us.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warning in documentation for deprecation of rivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10423</link><project id="" key="" /><description>Adding a warning in the documentation on the wiki to inform users that rivers are being deprecated.

Related to #10345
</description><key id="66226187">10423</key><summary>Warning in documentation for deprecation of rivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crohling</reporter><labels><label>deprecation</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T20:54:47Z</created><updated>2015-04-09T08:36:44Z</updated><resolved>2015-04-05T18:55:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="crohling" created="2015-04-03T21:01:27Z" id="89418473">I have now signed the CLA
</comment><comment author="jpountz" created="2015-04-03T21:24:40Z" id="89424341">Related to #10345
</comment><comment author="clintongormley" created="2015-04-05T18:55:34Z" id="89831931">thanks @crohling - i reformatted a bit to use the deprecation[] tag and pushed
</comment><comment author="crohling" created="2015-04-06T14:13:01Z" id="90078447">Thank you @clintongormley much appreciated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update script documentation examples to remove MVEL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10422</link><project id="" key="" /><description>MVEL isn't supported by default in recent ES versions, but the documentation still uses MVEL script examples.
</description><key id="66220462">10422</key><summary>update script documentation examples to remove MVEL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">jayswan</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-04-03T20:13:15Z</created><updated>2016-09-27T14:27:05Z</updated><resolved>2016-09-27T14:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T14:27:05Z" id="249881185">I believe all scripting documentation has been updated to be either Groovy (which needs to be removed now), or Painless.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logger warning using Elasticsearch Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10421</link><project id="" key="" /><description>Hi,

We have a java application that indexes documents to an elasticsearch setup.
The application uses log4j2 as its' logging library.

At startup, when _Client_ is initialized, the following warning messages occur:

`log4j:WARN No appenders could be found for logger (org.elasticsearch.plugins).`
`log4j:WARN Please initialize the log4j system properly.`
`log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.`

The easy solution would be to hold log4j.properties file but I try to avoid this, since I already use log4j2.properties I would prefer to "bridge" over elastic's log4j logs to my log4j2.

I tried creating a logger at my log4j2 configuration named `org.elasticseach.plugins` but it didn't help.

Googling this problem found [this stackoverflow question](http://stackoverflow.com/questions/24543787/logger-warning-using-elasticsearch-java-api) that was posted a year ago and was not answered.
I email the guy asked the question and he said in the end they didn't solve it, just left it.

Anyone got an idea?

Thanks.
</description><key id="66217078">10421</key><summary>Logger warning using Elasticsearch Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Ghost93</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-04-03T19:46:07Z</created><updated>2016-05-20T21:08:59Z</updated><resolved>2016-05-20T21:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snowwolf007cn" created="2015-09-09T05:24:57Z" id="138789212">Oops, I have the same problem
</comment><comment author="jmcomets" created="2015-09-14T10:12:58Z" id="140027915">+1, same problem here. This is really cluttering my logs/nerves.
</comment><comment author="rubenve" created="2015-09-18T23:10:57Z" id="141590869">+1 I'm the Stackoverflow guy. :)
</comment><comment author="Ghost93" created="2015-12-05T11:56:37Z" id="162175853">@clintongormley 
any news on this issue?
</comment><comment author="oubenali" created="2016-01-23T19:29:33Z" id="174214364">Has anyone found a solution ? I received the same warnings
</comment><comment author="jmoney8080" created="2016-01-24T14:46:32Z" id="174305078">Do you use the slf4j-apis with the log4j2 bindings or do you use the log4j2 apis directly?  

The "future proof" way of doing this is use the slf4j-apis with an slf4j implementation of the apis(in this case log4j2). Then, you can use the log4j12-over-slf4j library that routes all log4j to slf4j which should then proxy to log4j2 in your case. 

That "should" work. It's how I migrated everything to logback a year ago in some of my projects and I encountered similar errors. I assume the same process would work for log4j2(just have not tried it yet) as this problem is exactly why slf4j came into existence in the first place. 
</comment><comment author="Ghost93" created="2016-01-24T15:15:46Z" id="174308463">@jmoney8080 could you please provide a detailed sample?
Thanks!
</comment><comment author="jmoney8080" created="2016-01-24T20:49:40Z" id="174340130">https://github.com/jmoney8080/slf4j-example

you can download that repo and run mvn clean -U package and it should compile and show the logging.  There is a README explaining the setup.

Basically, 
1) log4j is ONLY a log4j project
2) log4j2 is ONLY a log4j project
3) slf4j-log4j2 depends on (1) and (2) but uses slf4j with log4j implementation.  It then delegates all log4j classes to slf4j so that logging from all three classes continues to work.
</comment><comment author="jasontedor" created="2016-05-20T21:08:58Z" id="220719060">Closing in favor of #17697.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bring back `numeric_resolution`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10420</link><project id="" key="" /><description>We had an undocumented parameter called `numeric_resolution` which allows to
configure how to deal with dates when provided as a number. The default is to
handle them as milliseconds, but you can also opt-in for eg. seconds.

Close #10072
</description><key id="66198845">10420</key><summary>Bring back `numeric_resolution`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T17:56:50Z</created><updated>2015-06-09T11:55:00Z</updated><resolved>2015-04-09T10:28:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-05T10:19:57Z" id="89748917">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow plugins to define custom operations that they use scripts for</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10419</link><project id="" key="" /><description>Plugins can now define multiple operations/contexts that they use scripts for. Fine-grained settings can then be used to enable/disable scripts based on each single registered context.

Also added a new generic category called `plugins`, which will be used as a default when the context is not specified. This allows us to restore backwards compatibility for plugins on `ScriptService` by restoring the old methods that don't require the script context and making them internally use the `plugins` context, as they can only be called from plugins.

Closes #10347
</description><key id="66184538">10419</key><summary>Allow plugins to define custom operations that they use scripts for</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T16:52:35Z</created><updated>2015-06-07T17:07:18Z</updated><resolved>2015-04-08T09:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-03T16:56:28Z" id="89353438">@uboness assigning this to you for review
</comment><comment author="uboness" created="2015-04-06T08:19:12Z" id="89972624">@javanna (don't kill me) thinking out load.. going back to the previous enum... would it be better to change that enum to an interface, such that the contract `ScriptService`'s methods will be clearer/"safer".. yet the context will still be based on an opaque string. Something like:

``` java
public interface ScriptContext {

    enum Std implements ScriptContext {
        MAPPING() {
            public String key() {
                return "mapping";
            }
        },
        SEARCH() {
            public String key() {
                return "search";
            }
        },
        UPDATE() {
            public String key() {
                return "update";
            }
        },
        AGGS() {
            public String key() {
                return "aggs";
            }
        };
    }

    String key();

    class Plugin implements ScriptContext {

        private String key;

        public Plugin(String pluginName, String context) {
            this.key = pluginName + "_" + context;
        }

        public String key() {
            return key;
        }
    }
}
```

the `Plugin` class is there just to promote plugins to implement the context in a consistent manner. (also, not sure if it will, but we open up the option for the context to hold more info in the future, without breaking the contract of the service methods)
</comment><comment author="javanna" created="2015-04-07T11:38:19Z" id="90516992">@uboness makes sense to me, just pushed a new commit.
</comment><comment author="uboness" created="2015-04-07T11:53:49Z" id="90523310">left a couple of comments
</comment><comment author="javanna" created="2015-04-07T15:36:20Z" id="90612264">Pushed another commit. Wondering: shall we restrict `ScriptModule#registerScriptContext(ScriptContext)` to `ScriptModule#registerScriptContext(ScriptContext.Plugin)` and make `ScriptContext.Plugin` final? That would allow us to actually enforce conventions on context keys etc. rather than only semi-enforcing them. Would that be too restrictive? Can you think of cases where implementing `ScriptContext` directly would be useful over using `Plugin`?
</comment><comment author="uboness" created="2015-04-07T16:16:35Z" id="90628009">I like it... hmm... I can't think of any scenario where you'd want to register custom contexts outside of a plugin. outside of a plugin is only within the core codebase and for that we can always extend the std enum. It's easier to start restrictive and open it up if we find the need to.. rather than open it up now and later try to restrict it. So I'd say go for it... 
</comment><comment author="javanna" created="2015-04-07T16:55:25Z" id="90644955">pushed another commit, restricted the custom script context to `ScriptContext.Plugin` instances only. We still rely on plugins providing the right info, there can still be interaction between different plugins (think of a plugin registering context plugin1_test and plugin2 using it), but I think this as far as we can take it (at least there's a single way to plug in custom contexts providing plugin name and operation).

Are we happy about using the `_` operator between plugin name and operation. That will need to be used as part of fine-grained settings e.g. `script.engine.groovy.inline.plugin1_feature1: on` . I think we could potentially replace `_` with `.` too, shouldn't hurt but maybe more confusing? Thoughts @clintongormley ?
</comment><comment author="uboness" created="2015-04-07T16:59:28Z" id="90646187">&gt; pushed another commit, restricted the custom script context to ScriptContext.Plugin instances only. We still rely on plugins providing the right info, there can still be interaction between different plugins (think of a plugin registering context plugin1_test and plugin2 using it), but I think this as far as we can take it (at least there's a single way to plug in custom contexts providing plugin name and operation).

agreed

&gt; Are we happy about using the _ operator 

yea... I chose this one as I wasn't sure how it'd effect 1) the parsing of the settings, 2) would confuse when reading the settings as `.` is used to separate other elements there.
</comment><comment author="javanna" created="2015-04-07T17:02:23Z" id="90646852">cool @uboness can you go over it one last time please? should be ready
</comment><comment author="clintongormley" created="2015-04-07T17:41:15Z" id="90665637">&gt; &gt; Are we happy about using the _ operator
&gt; &gt; yea... I chose this one as I wasn't sure how it'd effect 1) the parsing of the settings, 2) would confuse when reading the settings as . is used to separate other elements there.

Agreed. I'm happy with `_`
</comment><comment author="uboness" created="2015-04-07T20:26:42Z" id="90721508">left a small comment on docs (not sure if we want to mention plugin as we don't really mention plugins anywhere else AFAIK)... other than that LGTM
</comment><comment author="javanna" created="2015-04-08T10:01:40Z" id="90868101">ping @dadoonet you might want to update plugins to use the plugin category, or even define their own script context.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up include/exclude in terms aggregations with regexps, using Lucene regular expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10418</link><project id="" key="" /><description>Today we check every regular expression eagerly against every possible term.
This can be very slow if you have lots of unique terms, and even the bottleneck
if your query is selective.

This commit switches to Lucene regular expressions instead of Java (not exactly
the same syntax yet most existing regular expressions should keep working) and
uses the same logic as RegExpQuery to intersect the regular expression with the
terms dictionary. I wrote a quick benchmark (in the PR) to make sure it made
things faster and the same request that took 750ms on master now takes 74ms with
this change.

Close #7526
Close #9848
</description><key id="66183512">10418</key><summary>Speed up include/exclude in terms aggregations with regexps, using Lucene regular expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T16:47:38Z</created><updated>2015-12-24T17:56:11Z</updated><resolved>2015-04-09T10:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-03T16:49:19Z" id="89350375">Note for self: @rashidkpc asked to create an issue in Kibana it this change makes it in so that Kibana can be changed too.
</comment><comment author="jpountz" created="2015-04-03T17:01:36Z" id="89355258">@rmuir @mikemccand @s1monw I am not familiar with the Automaton/RegExp/CompiledAutomaton/ByteRunAutomaton API so if one of you could check that I did not misuse these APIs in this change, that would be great!
</comment><comment author="rmuir" created="2015-04-03T17:25:01Z" id="89367567">I added some comments. In general I am a little confused as to what is going on in all cases. 

Doing this kind of filtering seems costly, since it will intersect against the entire termsenum, potentially dereferencing many global ordinals to byte[] to do the matching, in order to finally get the bitset. 

Can we use a more efficient bitset in some cases? We will be populating it in ordinal order...

Do we cache these bitsets anywhere in case the same filtering is repeated over and over?
</comment><comment author="rmuir" created="2015-04-03T18:15:11Z" id="89379146">I thought this over more and brainstormed with @jpountz . I think we should always build one 'automaton' based on includes, excludes, includeValues, excludeValues, whatever. This can be Operations.minus(includes, excludes) basically.

Then, we can always simply use terms.intersect to make the bitset and not enumerate terms, doing ord-BytesRef resolution. Making it completely optimal for this case is interesting, its different than the techniques we would use for scoring documents, depending on the regex.

For dense cases, prefix cases (`ab*`), or similar regexps, it could be a win to be tricky in some cases. If someone does `ab*`, we could just do two reverse lookups (`ab` and `ac`) and fill the bitset range in between, instead of reading all the ordinals and deref'ing them to terms in `ab*` .

 But intersect() is probably much better already than what we do today.
</comment><comment author="jpountz" created="2015-04-08T10:13:49Z" id="90870646">I pushed a new commit in order to fold in exclusions in the automaton. It is still far from great (we should better reuse what we build when several aggregators reuse the same include/exclude rules for instance) but better than what we have today?
</comment><comment author="rmuir" created="2015-04-08T16:11:10Z" id="90962016">looks great to me. I added comments but only asking for code comments
</comment><comment author="dawi" created="2015-12-24T16:24:42Z" id="167132840">@jpountz I am wondering if it would be possible to support both, lucene and java regexps.

On the one hand I greatly appreciate the performance improvements achieved by using lucene regexps but on the other hand I need the power and flexibility of java regexps.

In a perfect world we would get both, speed AND flexibility, but in the meantime it would be great if the user could make the decision which regexp engine to use.

**Background information:**

It is currently not possible to include/exclude terms on a caseinsensitive basis.

I can think of some possible workarounds, but they are not as simple and straight forward as this implementation:

```
.addAggregation(
    terms("fieldAgg")
        .field(field + ".original")
        .include(regex, Pattern.CASE_INSENSITIVE) // supported by elasticsearch 1.7
        .size(maxResults)
        .order(Terms.Order.count(false))
);
```
</comment><comment author="rmuir" created="2015-12-24T17:13:23Z" id="167138925">Just index your content correctly. If you want case sensitive, that means indexing with lowercasefilter. its a search engine!
</comment><comment author="dawi" created="2015-12-24T17:56:11Z" id="167141651">I don't know how this would help me in this particular issue. I already have a multi field mapping to preserve the original field value. So it already is possible to build the aggregation over the lowercase field. But in this case the returned field values would also be lowercase, but I am interested in the exact original value.

I know that the decision to only use Lucenes Regexps is carefully chosen for various reasons.

But I am note sure if this goal is reached in the end if users have to implement complexer mappings or client side logic to achieve the same result. As I said, this usecase could be implemented perfectly fine just using the snippet above using Elasticsearch 1.7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[es 1.4.4] Scroll query returns 0 results before fully done </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10417</link><project id="" key="" /><description>We're on ES 1.4.4, running a 16 node cluster +  3 dedicated client nodes (c4.xlarge) which runs all our queries.

We're seeing intermittent failures on scroll queries on larger data sets of ~1 million documents. This has happened while we were on 1.3.2 as well.

Today, it failed after fetching 5000 documents (5 requests, each fetch is 1000 documents), then started returning 0. Retrying the scroll query again worked until completion. Each fetch took 1-2 seconds before failure and then started returning almost immediately with 0 results.

This index in particular has 5 shards and 26gb of data total.

Scroll size per shard: 200
Scroll time: 4 minutes

Let me know if there's other information I can provide to help diagnose
</description><key id="66180091">10417</key><summary>[es 1.4.4] Scroll query returns 0 results before fully done </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hjz</reporter><labels><label>feedback_needed</label></labels><created>2015-04-03T16:28:30Z</created><updated>2016-01-18T10:36:16Z</updated><resolved>2016-01-15T19:17:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-03T22:02:37Z" id="89433808">When the query returns 0 results prematurely, do you have shard failures in the response?
</comment><comment author="hjz" created="2015-04-03T22:29:25Z" id="89438456">We've seen shard failures before where 1 or 2 shards would fail and instead of 1000 results, it'll return 900 or 800. 

Didn't log the shard failures this time, suppose all 5 shards failed, any idea why that would happen? I'll add logging for that and circle back.
</comment><comment author="clintongormley" created="2015-04-05T19:02:08Z" id="89832211">@hjz can you provide the actual code you're using for scroll requests, plus any exceptions?
</comment><comment author="hjz" created="2015-04-06T07:27:31Z" id="89958896">Here's the query to start the scroll

``` scala
  def startScrollQueryGetTotalHitsAndScrollId(index: String, filter: BaseFilterBuilder): (Long, String) = {
    val r = ES.client.prepareSearch(index)
      .setTypes(User.eventType)
      .setSearchType(SearchType.SCAN)
      .setFrom(0)
      .setSize(config.scrollSizePerShard)
      .setScroll(scrollTime)

     r.setQuery(QueryBuilders.filteredQuery(QueryBuilders.matchAllQuery(), filter))

    val resp = r.execute().actionGet(config.scrollTimeoutSecs, TimeUnit.SECONDS)

    (resp.getHits.getTotalHits, resp.getScrollId)
  }
```

Then subsequent looks up are done via:

``` scala
      val scrollResp = ES.client
        .prepareSearchScroll(scrollId)
        .setScroll(scrollTime)
        .execute()
        .actionGet(config.scrollTimeoutSecs, TimeUnit.SECONDS)

      if (scrollResp.getFailedShards &gt; 0) {
        val shards = scrollResp.getShardFailures
        shards.foreach { s =&gt;
          val msg = s"Shard failed scroll query: ${s.index()}, shardId: ${s.shardId()} reason: ${s.reason()}}"
          hipChatService.errorMessage(msg, createAlert = false)
        }
      }

      (scrollResp.getHits.getHits.map { sh =&gt;
        ApiUser.fromSource(sh.getId, sh.source() )
      }.toSeq,  scrollResp.getScrollId)
```

I'll follow up with exceptions
</comment><comment author="clintongormley" created="2015-04-06T18:22:37Z" id="90184115">@hjz I don't see where you are getting the scroll_id from the previous scroll request and using it for the next scroll request.

Normally there would be some `while` loop which does a scroll request, checks if there are any hits, then issues a new scroll request, using the `scroll_id` returned by the previous scroll request.
</comment><comment author="hjz" created="2015-04-06T20:08:59Z" id="90225966">Yes that what we do. I omittied some code there. Essentially

``` scala
def getNextScroll() = {
      val scrollResp = ES.client
        .prepareSearchScroll(scrollId)
        .setScroll(scrollTime)
        .execute()
        .actionGet(config.scrollTimeoutSecs, TimeUnit.SECONDS)

      if (scrollResp.getFailedShards &gt; 0) {
        val shards = scrollResp.getShardFailures
        shards.foreach { s =&gt;
          val msg = s"Shard failed scroll query: ${s.index()}, shardId: ${s.shardId()} reason: ${s.reason()}}"
          hipChatService.errorMessage(msg, createAlert = false)
        }
      }

      (scrollResp.getHits.getHits.map { sh =&gt;
        ApiUser.fromSource(sh.getId, sh.source() )
      }.toSeq,  scrollResp.getScrollId)
}
```

``` scala
val (totalHits, startScrollId) = startScrollQueryGetTotalHitsAndScrollId
var currentScrollId = startScrollId
var batch = 1
var remaining = totalHits
var retries = 0
var batchHits = 0L

do {
      val startTime = DateTime.now

     getNextScroll(currentScrollId) match {
        case Success((users, nextScrollId)) =&gt;
          currentScrollId = nextScrollId
          batch += 1

        case Failure(e) =&gt;
          batchHits = 0
          hipChatService.sendMessage(s"$logHdr scroll query error. ${e.getMessage}. retry $retries", color = HipChatNotificationColor.Red)
      }

      // If hits was 0 but there are users remaining, something fubared, so sleep for BatchRetrySeconds and retry
      if (batchHits == 0 &amp;&amp; remaining &gt; 0) {
        retries += 1
        Logger.error(s"$logHdr Elasticsearch scroll request failed, retries: $retries. Sleeping for ${config.scrollRetrySleepDuration.seconds} seconds")
        Thread.sleep(config.scrollRetrySleepDuration.millis)
      }

      remaining -= batchHits
} while (remaining &gt; 0 &amp;&amp; retries &lt; config.scrollMaxRetries)
```
</comment><comment author="pickypg" created="2015-04-09T18:30:14Z" id="91319788">@hjz Have you gotten any logs for this issue yet?
</comment><comment author="hjz" created="2015-04-20T18:48:48Z" id="94538026">// Starts just after the initial scroll query

2015-04-20 17:34:11,281 [ERROR] application - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[datanode4][inet[/x.x.x.248:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 1000) on org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@1cf09828]; }

2015-04-20 17:34:11,282 [ERROR] application - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[datanode4][inet[/x.x.x.248:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 1000) on org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@57767fad]; }

2015-04-20 17:34:11,283 [ERROR] application - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[datanode4][inet[/x.x.x.248:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 1000) on org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@779578ef]; }

2015-04-20 17:34:11,305 [ERROR] application - [xxx Batch 3 - 14531 remaining] [200 DONE] Fetching: 56.319 secs. Queuing: 0.015 secs. 3.550253843149785 msgs/sec

2015-04-20 17:34:51,414 [ERROR] application - [xxx Batch 4 - 14331 remaining] [200 DONE] Fetching: 40.094 secs. Queuing: 0.013 secs. 4.986660682673848 msgs/sec

2015-04-20 17:34:59,458 [ERROR] application - [xxx Batch 5 - 14131 remaining] [200 DONE] Fetching: 8.029 secs. Queuing: 0.013 secs. 24.869435463814973 msgs/sec

2015-04-20 17:34:59,574 [ERROR] application - [xxx Batch 6 - 13931 remaining] [200 DONE] Fetching: 0.099 secs. Queuing: 0.015 secs. 1754.3859649122805 msgs/sec

2015-04-20 17:34:59,646 [ERROR] application - [xxx Batch 7 - 13731 remaining] [200 DONE] Fetching: 0.057 secs. Queuing: 0.012 secs. 2898.550724637681 msgs/sec

2015-04-20 17:34:59,865 [ERROR] application - [xxx Batch 9 - 13331 remaining] [200 DONE] Fetching: 0.098 secs. Queuing: 0.027 secs. 1600.0 msgs/sec

2015-04-20 17:34:59,994 [ERROR] application - [xxx Batch 10 - 13131 remaining] [200 DONE] Fetching: 0.083 secs. Queuing: 0.045 secs. 1562.5 msgs/sec

2015-04-20 17:35:00,009 [ERROR] application - [xxx Batch 11 - 12931 remaining] [9 DONE] Fetching: 0.011 secs. Queuing: 0.002 secs. 692.3076923076924 msgs/sec

2015-04-20 17:35:00,012 [ERROR] application - [xxx Batch 12 - 12922 remaining] [0 DONE] Fetching: 0.0 secs. Queuing: 0.0 secs. NaN msgs/sec

2015-04-20 17:35:00,015 [ERROR] application - [xxx Batch 13 - 12922 remaining] Elasticsearch scroll request failed, retries: 1. Sleeping for 5 seconds

2015-04-20 17:35:00,071 [WARN ] n.k.r.connection.AbstractConnection - Lockdown ended.

2015-04-20 17:35:05,017 [ERROR] application - [xxx Batch 13 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.0 secs. 0.0 msgs/sec

2015-04-20 17:35:05,018 [ERROR] application - [xxx Batch 14 - 12922 remaining] Elasticsearch scroll request failed, retries: 2. Sleeping for 5 seconds

2015-04-20 17:35:x.x.x [ERROR] application - [xxx Batch 14 - 12922 remaining] [0 DONE] Fetching: 0.0 secs. Queuing: 0.001 secs. 0.0 msgs/sec

2015-04-20 17:35:10,021 [ERROR] application - [xxx Batch 15 - 12922 remaining] Elasticsearch scroll request failed, retries: 3. Sleeping for 5 seconds

2015-04-20 17:35:15,023 [ERROR] application - [xxx Batch 15 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.0 secs. 0.0 msgs/sec

2015-04-20 17:35:15,025 [ERROR] application - [xxx Batch 16 - 12922 remaining] Elasticsearch scroll request failed, retries: 4. Sleeping for 5 seconds

2015-04-20 17:35:20,027 [ERROR] application - [xxx Batch 16 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.0 secs. 0.0 msgs/sec

2015-04-20 17:35:20,030 [ERROR] application - [xxx Batch 17 - 12922 remaining] Elasticsearch scroll request failed, retries: 5. Sleeping for 5 seconds

2015-04-20 17:35:25,032 [ERROR] application - [xxx Batch 17 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.001 secs. 0.0 msgs/sec

2015-04-20 17:35:25,034 [ERROR] application - [xxx Batch 18 - 12922 remaining] Elasticsearch scroll request failed, retries: 6. Sleeping for 5 seconds

2015-04-20 17:35:30,036 [ERROR] application - [xxx Batch 18 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.0 secs. 0.0 msgs/sec

2015-04-20 17:35:30,038 [ERROR] application - [xxx Batch 19 - 12922 remaining] Elasticsearch scroll request failed, retries: 7. Sleeping for 5 seconds

2015-04-20 17:35:30,350 [WARN ] n.k.r.connection.AbstractConnection - Lockdown ended.

2015-04-20 17:35:35,040 [ERROR] application - [xxx Batch 19 - 12922 remaining] [0 DONE] Fetching: 0.0 secs. Queuing: 0.001 secs. 0.0 msgs/sec

2015-04-20 17:35:35,042 [ERROR] application - [xxx Batch 20 - 12922 remaining] Elasticsearch scroll request failed, retries: 8. Sleeping for 5 seconds

2015-04-20 17:35:40,044 [ERROR] application - [xxx Batch 20 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.001 secs. 0.0 msgs/sec

2015-04-20 17:35:40,046 [ERROR] application - [xxx Batch 21 - 12922 remaining] Elasticsearch scroll request failed, retries: 9. Sleeping for 5 seconds

2015-04-20 17:35:45,048 [ERROR] application - [xxx Batch 21 - 12922 remaining] [0 DONE] Fetching: 0.001 secs. Queuing: 0.0 secs. 0.0 msgs/sec

2015-04-20 17:35:45,050 [ERROR] application - [xxx Batch 22 - 12922 remaining] Elasticsearch scroll request failed, retries: 10. Sleeping for 5 seconds

2015-04-20 17:35:45,415 [WARN ] n.k.r.connection.AbstractConnection - Lockdown ended.

2015-04-20 17:35:50,051 [ERROR] application - [xxx Batch 22 - 12922 remaining] Some users failed to fetch: remaining, 12922 retries: 10

// We stop trying here
</comment><comment author="pickypg" created="2015-04-21T15:17:33Z" id="94836023">@hjz For your value of `index`, are you using `"_all"` (or equivalent) by chance? How many shards are actually associated with that request?
</comment><comment author="hjz" created="2015-04-22T01:44:53Z" id="94995715">We have _all turned off. This is hitting 15 shards that has 2 replicas. No doc value formats.
</comment><comment author="l15k4" created="2015-06-19T10:09:49Z" id="113459700">I seem to have this issue too... I'm scanning a little more than 100 indices with `keepAlive=10s` : 

```
            "number_of_replicas": "1",
            "number_of_shards": "5",
```

on : 

```
   "number_of_nodes": 4,
   "number_of_data_nodes": 4,
   "active_primary_shards": 530,
   "active_shards": 1060,
```

I'm using pattern `mi_*` for scanning those 100+ indices.

I haven't noticed it would happen with sorted `QueryThenFetch` scroll because I couldn't scroll as much as I can `Scan`. But when scanning it returns 0 results after ~ 100M documents from 350M. I don't check `SearchResponse#getShardFailures` though. What should I do when there is a failure? Repeat the request with the same `scroll_id` ?
</comment><comment author="l15k4" created="2015-06-19T15:47:50Z" id="113553972">I figured out why this happened to me, the reason is that scanning slows down so much that it gets out of `10s` keepAlive period which leads to `SearchResponse#shardFailures`. These logs document it : 

```
100320 elements processed in 17 seconds
100320 elements processed in 13 seconds
100320 elements processed in 20 seconds
100320 elements processed in 20 seconds
100320 elements processed in 13 seconds
100320 elements processed in 32 seconds
100320 elements processed in 17 seconds
100320 elements processed in 53 seconds
100320 elements processed in 37 seconds
100320 elements processed in 38 seconds
100320 elements processed in 64 seconds
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208081]]
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208084]]
 - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[High Evolutionary][inet[/172.31.47.53:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: SearchContextMissingException[No search context found for id [221366]];
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208092]]
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208086]]
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208093]]
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208089]]
 - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[High Evolutionary][inet[/172.31.47.53:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: SearchContextMissingException[No search context found for id [221369]];
 - Shard failed scroll query: null, shardId: -1 reason: SearchContextMissingException[No search context found for id [208087]]
 - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[High Evolutionary][inet[/172.31.47.53:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: SearchContextMissingException[No search context found for id [221371]];
 - Shard failed scroll query: null, shardId: -1 reason: RemoteTransportException[[High Evolutionary][inet[/172.31.47.53:9300]][indices:data/read/search[phase/scan/scroll]]]; nested: SearchContextMissingException[No search context found for id [221372]];
 - ScanSource just completed...

```
</comment><comment author="vvcephei" created="2015-08-19T13:58:42Z" id="132607543">I have had some trouble with this as well. Sadly, it's been long enough that I won't be able to remember the details for debugging. (I just happened to see this issue while looking for something else.)

What I can say is that I've been able to (as far as I can tell) eliminate the possibility of missing data during a scroll by counting the number of documents I've seen and checking at the end of the scroll that it _exactly_ matches the total_hits reported by the initial query.

This seems like a good sanity check for anyone doing a scan&amp;scroll, as there's no reason for the hits to differ from the total_hits.
</comment><comment author="clintongormley" created="2016-01-15T19:17:08Z" id="172057998">The original issue looks like the cluster was overloaded. I don't think there is anything to do here.  Closing
</comment><comment author="vvcephei" created="2016-01-15T21:53:49Z" id="172104904">Hey Clinton,

Yeah, our clusters are generally pretty heavily loaded, but I don't know
how to quantify it being overloaded.

This error actually happens to us all the time, and we simply detect it and
start the scroll again.

Thanks,
John

On Friday, January 15, 2016, Clinton Gormley notifications@github.com
wrote:

&gt; Closed #10417.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.&lt;
&gt; https://ci5.googleusercontent.com/proxy/D3fUvDCaH68UfTcyrC_41gEBYmFovoX9yBs4r5lq1Laf4bxsDbQSWk6_se-Oi4oNSPW8HJJdyOFXj7r0ZdCsnJNsF2dhmKquDRV0JrTGA0sxdrSwRx6x1s1ATQgnOXp5fiJiFvulaNuD4TduirJrHx3lOHzZMA=s0-d-e1-ft#https://github.com/notifications/beacon/AAy1E4J4EKL3Uyv3_42vkojmEbfE-RTrks5paT2FgaJpZM4D537e.gif
</comment><comment author="clintongormley" created="2016-01-18T10:36:16Z" id="172493108">@vvcephei see these in the logs from the https://github.com/elastic/elasticsearch/issues/10417#issuecomment-94538026:

&gt;  [indices:data/read/search[phase/scan/scroll]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 1000)

You can keep an eye on your search thread pool queue size.  If you're regularly filling it up then either you need (a) more efficient queries or (b) more hardware.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch number matching does not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10416</link><project id="" key="" /><description>elasticsearch does not work with exactly matching with number. if i wanted to search price=53.it retrieve all prices of 53,531,5394,............so on ........ 
</description><key id="66171297">10416</key><summary>elasticsearch number matching does not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bairaginath</reporter><labels /><created>2015-04-03T15:42:49Z</created><updated>2015-04-03T15:52:33Z</updated><resolved>2015-04-03T15:52:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-03T15:52:33Z" id="89335785">Hi @bairaginath may I ask you to move your question to our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch) please? Please describe your problem thoroughly there and you'll get help for sure. We use github issues for bugs and feature requests, if it turns out that you found a bug you can still open a new issue including a complete recreation for it. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be lenient with "has_child" filter against a missing type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10415</link><project id="" key="" /><description>Currently a search request or query containing a filter like:

```
       "has_child": {
          "type": "missingtype",
          "query": { "term": { "text": "yay" } }
       }
```

Will fail with a QueryParsingException ("No mapping for for type [missingtype]") if the mapping type it refers to is not defined.

I think (and propose) that it would be useful to be able to at least instruct ES to be lenient in that case, and just evaluate the filter to false.

I think this would make sense, moreover, it would be consistent with the behavior of search on a missing type, i.e. currently:

```
curl http://localhost:9200/my-index/missingtype/_search?q=sometext
```

Doesn't fail, it returns 0 hits, despite of "missingtype" being (obviously) missing.

Any reason why something similar shouldn't be done for the "has_child" filter?
## 

Supporting use case:

We've got an app that indexes the full text of office documents that are attached to another type of entity, whose contents we also index in ES. We designed this as a parent-child relationship, indexing full text in a child type of the main entity type.

Currently, we are in the process of allowing end users to tweak their ES mappings, that means multiple (per user) parent mappings, each with their corresponding child mapping for indexed documents.

We would find it very clean and convenient to just create all these mappings lazily, and create the children type just when that user pushes an office document. The problem is that, unless we proactively make sure that the children types exist, our text search requests will break.
</description><key id="66162122">10415</key><summary>Be lenient with "has_child" filter against a missing type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuxpiper</reporter><labels><label>:Parent/Child</label><label>discuss</label></labels><created>2015-04-03T15:02:33Z</created><updated>2016-01-15T19:07:32Z</updated><resolved>2016-01-15T19:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T18:19:18Z" id="89825513">@martijnvg what's your take on this? 
</comment><comment author="martijnvg" created="2015-04-05T19:01:42Z" id="89832189">@tuxpiper I agree we should be able to be lenient towards a missing parent type.

@clintongormley The idea I had was adding a `_unmapped_fields` option to the search dsl. Which can have the: `ignore`, `warn` and `error` (default). This option shouldn't just apply for `has_child` and has_parent, but also `nested` and other queries rely on mapping settings.

This idea came from: https://github.com/elastic/elasticsearch/pull/9521#issuecomment-73956623
</comment><comment author="tuxpiper" created="2015-04-06T07:19:13Z" id="89956915">That sounds good @martijnvg , having such option in the search dsl would work great for us.

Thanks for reviewing this!
</comment><comment author="tuxpiper" created="2015-04-07T11:19:26Z" id="90510118">@clintongormley @martijnvg  since the reception of the idea seems to be positive, we are looking forward to having a notion of when this behavior could be incorporated in ElasticSearch. Thanks!
</comment><comment author="kausalyb" created="2015-11-02T05:14:17Z" id="152917238">@clintongormley @martijnvg we would like to know the possible timeline of this feature will be into elastic search. 
 My issue is same as the one that was mentioned this link https://github.com/elastic/elasticsearch/issues/10882.
Until then , It would be helpful if you can suggest any workaround for this.
</comment><comment author="clintongormley" created="2016-01-15T19:07:31Z" id="172055752">Closing in favour of https://github.com/elastic/elasticsearch/issues/12016
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search results missing after upgrade from 1.1.2 to 1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10414</link><project id="" key="" /><description>We're experience strange behaviour with search results after upgrading from 1.1.2 to 1.5.0. We were indexing documents that had nested objects and had to use queries to separate results for different pages (really a separate index should be used, but changing requirements, etc. etc.). This is an example object and an example of the query used for one page (index, type, field names changed for confidentiality):

``` bash
curl -XPOST http://localhost:9200/example-index/example-type/1234567890 -d '{
   "fieldA": true,
   "fieldB": {
      "nestedFieldA": "value",
      "nestedFieldB": "value"
   }
}'
```

``` bash
curl -XPOST http://localhost:9200/example-index/example-type/_search -d '{
      "query": {
         "bool": {
            "must_not": {
               "filtered": {
                  "query": {
                     "match_all": {}
                  },
                  "filter": {
                     "missing": {
                        "field": "fieldB.nestedFieldA",
                        "existence": true,
                        "null_value": false
                     }
                  }
               }
            }
         }
      }
}'
```

This worked before and returned the correct results. Once we upgraded to 1.5.0 it returned no results. However, if we index new documents, only the new documents are returned in the results.  I can see the previous documents are still there by running a match_all query

I thought it might have to do with running an _upgrade against the index so I did that but it did not resolve the issue.
</description><key id="66148594">10414</key><summary>Search results missing after upgrade from 1.1.2 to 1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tomcashman</reporter><labels /><created>2015-04-03T13:41:18Z</created><updated>2015-04-23T16:50:19Z</updated><resolved>2015-04-03T13:49:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-03T13:49:52Z" id="89294822">This is due to a bug in 1.5.0: https://github.com/elastic/elasticsearch/pull/10268 which will be fixed in 1.5.1. Sorry for the incovenience.
</comment><comment author="tomcashman" created="2015-04-03T14:00:38Z" id="89298037">No problem, thanks for the quick response.
</comment><comment author="HenleyChiu" created="2015-04-21T14:13:14Z" id="94810222">FYI,

I don't think this was ever fixed in 1.5.1 as I've ran into the same problem with the latest version.

I assume this is going to be in a future release?
</comment><comment author="tdoman" created="2015-04-23T16:25:33Z" id="95642076">We've also experienced the same problem with 1.5.1.  Will newly indexed documents be effected or the just old data?  If just the old data is effected, is a re-index of the old data a valid temporary work around?
</comment><comment author="rjernst" created="2015-04-23T16:33:10Z" id="95646021">@tdoman If it is the same problem as this issue then it would affect any _indices_ created before 1.3.0. Reindexing the same docs into an old index will not fix the problem, but creating a new index should fix the issue.  If you are seeing this in 1.5.1, can you please describe your setup and the problem on #10590?
</comment><comment author="tdoman" created="2015-04-23T16:50:19Z" id="95651285">We had been running 1.1 until about a week ago when we upgraded to 1.5.1.  That's a lot of work to execute the work around but we can create new indexes and then update our aliases if that's the only way to go.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Found 1 record, but cannot see data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10413</link><project id="" key="" /><description>Request:
POST property_v2/property/_search
{
  "from": 1,
  "size": 20,
  "sort": [
    {
      "Models.MinPrice": {
        "order": "asc"
      }
    }
  ],
  "filter": {
    "and": {
      "filters": [
        {
          "ids": {
            "values": [1726485],
            "type": "property"
          }
        }
      ]
    }
  }}
Here is response. 
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": []
   }
}

But when you use: http://localhost:9200/property_v2/property/1726485, it return data.
</description><key id="66136495">10413</key><summary>Found 1 record, but cannot see data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thoaingo</reporter><labels /><created>2015-04-03T12:22:56Z</created><updated>2015-04-03T12:32:12Z</updated><resolved>2015-04-03T12:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-03T12:32:12Z" id="89274552">Hi @thoaingo this type of question would be better handled if it was sent to our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch). I suspect what you see is due to `from: 1` in your request, in any case can you move the discussion to the mailing list please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `routing_nodes` an independent metric option in cluster state api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10412</link><project id="" key="" /><description>closes #10352
</description><key id="66123973">10412</key><summary>Make `routing_nodes` an independent metric option in cluster state api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T10:44:31Z</created><updated>2015-06-06T19:07:13Z</updated><resolved>2015-04-08T13:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-03T12:41:24Z" id="89277108">I like it @lmenezes , thanks! Makes sense to keep bw comp in 1.x and still return `routing_nodes` too when asking for `routing_table`, I would change that in master probably as a follow-up. @clintongormley want to double check?
</comment><comment author="clintongormley" created="2015-04-05T14:09:17Z" id="89777383">+1 to this change, and yes i'd make it 2.0 only
</comment><comment author="lmenezes" created="2015-04-05T23:27:32Z" id="89868975">@clintongormley great! but for 2.0 only you mean making routing_table routing_table only, right? the routing_nodes as an independent metric could go on 1.X, right? :)
</comment><comment author="clintongormley" created="2015-04-06T18:16:09Z" id="90181289">@lmenezes thinking about it again... it's such an easy change for users to make in 1.6, that i'd be tempted to just go ahead and do it, ie make the change completely in 1.6.
</comment><comment author="lmenezes" created="2015-04-06T19:44:24Z" id="90214105">@clintongormley perfect for me. just thought it could be useful to have a smooth transition, where you could upgrade your nodes and have old behaviour, and then adapt your code on a cluster that already has this change.

anyway, any of the alternatives work for me :)
</comment><comment author="javanna" created="2015-04-07T12:01:26Z" id="90526341">Hi @lmenezes I gave a final look at this, I wonder if we should add the same options to the Java api as well. We would need to add a `routingNodes` flag to `ClusterStateRequest` and handle bw comp on the wire format using version checks. Thoughts?
</comment><comment author="lmenezes" created="2015-04-07T12:15:15Z" id="90530909">@javanna makes sense. Will update the PR with this :)
</comment><comment author="lmenezes" created="2015-04-08T11:28:54Z" id="90884537">@javanna so, just had time to look into this... Does it really make sense to separate that for the Java API? 

On the REST API, routing_nodes and routing_table are 2 completely different entities(even though both are rendered from the same data) and returning them or not might make difference(bytes sent through the wire, and the process of iterating the routing table to built the json representation for each of them).

On the Java API, the RoutingTable object has to be sent over the wire anyway, and the RoutingNodes is actually volatile and built on demand from the RoutingTable(https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/ClusterState.java#L134 and https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/ClusterState.java#L226-L232). 

So, unless I'm missing something, it would make no difference at all(except from offering the same options on both Java/REST API) being able to control that individually on the Java API, and likely also make it confusing since you cannot really offer RoutingNodes without offering RoutingTable.

Makes sense?
</comment><comment author="javanna" created="2015-04-08T12:59:55Z" id="90906862">hey @lmenezes you are right, this makes perfect sense to me. The routing table is what gets serialized over the wire, routing nodes are built on demand when requested first based on the routing table. I will merge this as-is, thanks a lot for looking into this! 
</comment><comment author="lmenezes" created="2015-04-08T13:10:37Z" id="90910266">@javanna thank you for taking the time to merge this :+1: 
</comment><comment author="javanna" created="2015-04-08T13:37:09Z" id="90918428">Merged, thanks a lot @lmenezes !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix multi-level breadth-first aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10411</link><project id="" key="" /><description>The refactoring in #9544 introduced a regression that broke multi-level
aggregations using breadth-first. This was due to sub-aggregators creating
deferred collectors before their parent aggregator and then the parent
aggregator trying to collect sub aggregators directly instead of going through
the deferred wrapper.

This commit fixes the issue but we should try to simplify all the pre/post
collection logic that we have.

Also `breadth_first` is now automatically ignored if the sub aggregators need
scores (just like we ignore `execution_mode` when the value does not make sense
like using ordinals on a script).

Close #9823
</description><key id="66119008">10411</key><summary>Fix multi-level breadth-first aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T10:04:36Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-09T10:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-08T13:26:22Z" id="90914829">@colings86 Would you mind reviewing this change?
</comment><comment author="colings86" created="2015-04-08T13:40:53Z" id="90919582">@jpountz left some comments
</comment><comment author="jpountz" created="2015-04-08T15:50:07Z" id="90957329">@colings86 Pushed a new commit
</comment><comment author="colings86" created="2015-04-09T08:59:30Z" id="91159748">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update core-types.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10410</link><project id="" key="" /><description /><key id="66117071">10410</key><summary>Update core-types.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">GuillaumeDievart</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-03T09:44:38Z</created><updated>2015-04-03T12:13:39Z</updated><resolved>2015-04-03T12:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GuillaumeDievart" created="2015-04-03T09:48:16Z" id="89238385">I signed CLA.
</comment><comment author="javanna" created="2015-04-03T12:13:39Z" id="89268587">Thanks @GuillaumeDievart merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Poor indexing performance with elasticsearch 1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10409</link><project id="" key="" /><description>Hi again,

I migrated my cluster to elasticsearch 1.5.0 yesterday and I do have performance indexing problem again.
I just lost my logs so I cannot show you any graph to demonstrate it but indexing is about 2x slower.
As I already have this problem (https://github.com/elastic/elasticsearch/issues/8553) I run a GET /_nodes/hot_threads.

Notice that the request is always the same.

Here is the result :

::: [S3DEV-BI-ES05][3YlS8B4dRxaFaqSSzyOxew][S3DEV-BI-ES05][inet[/10.199.31.19:9300]]{master=false}
   Hot threads at 2015-04-03T09:20:20.490Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   45.8% (229ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES05][bulk][T#2]'
     5/10 snapshots sharing following 17 elements
       org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:663)
       org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
       org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
       org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
       org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
       org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
       org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
       org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
       org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 16 elements
       org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
       org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
       org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
       org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
       org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
       org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
       org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
       org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 11 elements
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:552)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
       org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
       org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
       org.elasticsearch.common.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:179)
       org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
       org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:613)
       org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:547)
       org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:236)
       org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:400)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:685)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
       org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   45.7% (228.7ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES05][bulk][T#1]'
     2/10 snapshots sharing following 29 elements
       org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
       org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
       org.elasticsearch.common.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:179)
       org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
       org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:613)
       org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:547)
       org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:236)
       org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:400)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:685)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
       org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     5/10 snapshots sharing following 18 elements
       org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:151)
       org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:663)
       org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
       org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
       org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
       org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
       org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
       org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
       org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
       org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 7 elements
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       org.elasticsearch.common.jackson.core.json.UTF8JsonGenerator.writeString(UTF8JsonGenerator.java:436)
       org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeString(JsonXContentGenerator.java:99)
       org.elasticsearch.common.xcontent.XContentBuilder.writeValue(XContentBuilder.java:1176)
       org.elasticsearch.common.xcontent.XContentBuilder.writeMap(XContentBuilder.java:1163)
       org.elasticsearch.common.xcontent.XContentBuilder.writeValue(XContentBuilder.java:1197)
       org.elasticsearch.common.xcontent.XContentBuilder.writeValue(XContentBuilder.java:1201)
       org.elasticsearch.common.xcontent.XContentBuilder.writeMap(XContentBuilder.java:1163)
       org.elasticsearch.common.xcontent.XContentBuilder.map(XContentBuilder.java:1072)
       org.elasticsearch.action.index.IndexRequest.source(IndexRequest.java:379)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:228)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:523)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: [S3DEV-BI-ES06][rF5XW0dVRz689Fbl0Cuh-w][s3dev-bi-es06][inet[/10.199.31.20:9300]]{master=false}
   Hot threads at 2015-04-03T09:20:20.295Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

::: [S3DEV-BI-ES07][8v1UTkWwTsOPFS-aBjMb0w][S3DEV-BI-ES07][inet[/10.199.31.21:9300]]{master=false}
   Hot threads at 2015-04-03T09:20:20.283Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

::: [S3DEV-BI-ES01][lbFh9XQeTyGU-WEBeyMQHA][s3dev-bi-es01][inet[/10.199.31.15:9300]]{data=false, master=true}
   Hot threads at 2015-04-03T09:20:20.149Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

::: [S3DEV-BI-ES04][bjy958SyTjG3-oSqryU9Pw][S3DEV-BI-ES04][inet[/10.199.31.18:9300]]{master=false}
   Hot threads at 2015-04-03T09:20:20.349Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

```
7.6% (38.1ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][[operations][2]: Lucene Merge Thread #563]'
 10/10 snapshots sharing following 15 elements
   java.lang.Thread.sleep(Native Method)
   java.lang.Thread.sleep(Thread.java:340)
   org.apache.lucene.store.RateLimiter$SimpleRateLimiter.pause(RateLimiter.java:151)
   org.apache.lucene.store.RateLimitedFSDirectory$RateLimiterWrapper.pause(RateLimitedFSDirectory.java:96)
   org.apache.lucene.store.RateLimitedIndexOutput.checkRate(RateLimitedIndexOutput.java:76)
   org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:70)
   org.apache.lucene.store.CompoundFileWriter$DirectCFSIndexOutput.writeBytes(CompoundFileWriter.java:356)
   org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:281)
   org.apache.lucene.store.Directory.copy(Directory.java:194)
   org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4785)
   org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4266)
   org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3811)
   org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:409)
   org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
   org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:486)

7.5% (37.6ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][bulk][T#1]'
 3/10 snapshots sharing following 15 elements
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:685)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
   org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
   org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 5/10 snapshots sharing following 16 elements
   org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
   org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
   org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
   org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
   org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 14 elements
   org.apache.lucene.index.SegmentReader.document(SegmentReader.java:335)
   org.elasticsearch.search.lookup.SourceLookup.loadSourceIfNeeded(SourceLookup.java:70)
   org.elasticsearch.search.lookup.SourceLookup.extractRawValues(SourceLookup.java:145)
   org.elasticsearch.index.get.ShardGetService.innerGetLoadFromStoredFields(ShardGetService.java:378)
   org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:210)
   org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:104)
   org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:77)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:523)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)

7.1% (35.2ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][bulk][T#2]'
 3/10 snapshots sharing following 17 elements
   org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:663)
   org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
   org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
   org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
   org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
   org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 33 elements
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
   org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
   org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
   org.elasticsearch.common.joda.time.format.InternalParserDateTimeParser.parseInto(InternalParserDateTimeParser.java:52)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2556)
   org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2462)
   org.elasticsearch.common.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:179)
   org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
   org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:613)
   org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:547)
   org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:236)
   org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:400)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
   org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:685)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
   org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
   org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 17 elements
   org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:668)
   org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
   org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
   org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
   org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
   org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 7 elements
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:523)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 unique snapshot
   org.apache.lucene.index.TermVectorsConsumerPerField.start(TermVectorsConsumerPerField.java:131)
   org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:297)
   org.apache.lucene.index.FreqProxTermsWriterPerField.start(FreqProxTermsWriterPerField.java:72)
   org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:616)
   org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:359)
   org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
   org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
   org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
   org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
```

::: [S3DEV-BI-ES03][a2rQ95QFTYWF-_f0a9ABVg][S3DEV-BI-ES03][inet[/10.199.31.17:9300]]{master=true}
   Hot threads at 2015-04-03T09:20:20.355Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

```
3.6% (18ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES03][bulk][T#2]'
 5/10 snapshots sharing following 15 elements
   org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:318)
   org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:284)
   org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:424)
   org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1353)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:438)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 13 elements
   org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:400)
   org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper.postParse(FieldNamesFieldMapper.java:183)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:552)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
   org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 2/10 snapshots sharing following 15 elements
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeNonDynamicArray(ObjectMapper.java:685)
   org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)
   org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
   org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
   org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:465)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:423)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
 unique snapshot
   org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.scanToTermLeaf(SegmentTermsEnumFrame.java:566)
   org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.scanToTerm(SegmentTermsEnumFrame.java:469)
   org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:508)
   org.elasticsearch.common.lucene.uid.PerThreadIDAndVersionLookup.lookup(PerThreadIDAndVersionLookup.java:104)
   org.elasticsearch.common.lucene.uid.Versions.loadDocIdAndVersion(Versions.java:150)
   org.elasticsearch.common.lucene.uid.Versions.loadVersion(Versions.java:161)
   org.elasticsearch.index.engine.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1002)
   org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:404)
   org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:362)
   org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:483)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:427)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:529)
   org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:239)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:515)
   org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:422)
   java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   java.lang.Thread.run(Thread.java:745)
```

::: [Hardcase/t1][u2qP36hdQfawXLGhhYx3Jw][s3dev-bi-es02][inet[/10.199.31.16:9301]]{data=false, client=true}
   Hot threads at 2015-04-03T09:20:20.146Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

I'm not good enough to read those logs so if someone see something I should do ...

Thank you.
</description><key id="66115411">10409</key><summary>Poor indexing performance with elasticsearch 1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">ZeAleks</reporter><labels><label>feedback_needed</label></labels><created>2015-04-03T09:29:20Z</created><updated>2016-01-15T19:00:23Z</updated><resolved>2016-01-15T19:00:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-04-03T09:37:08Z" id="89236734">Do you see any "now throttling indexing" messages in your node console logs?

Maybe try increasing the store throttling (defaults to 20 MB/sec): http://www.elastic.co/guide/en/elasticsearch/reference/1.4/index-modules-store.html
</comment><comment author="ZeAleks" created="2015-04-03T09:45:11Z" id="89238098">No.
There is only info about "updating refresh_interval".
No errors, no warnings.
</comment><comment author="jhansen-tt" created="2015-04-17T19:56:11Z" id="94062446">Why is more feedback needed?  I am seeing ~5x performance decrease in ES 1.5.1, with geoshape documents.
</comment><comment author="mikemccand" created="2015-04-17T23:41:02Z" id="94098403">&gt; Why is more feedback needed?

I was wondering whether increasing the default 20 MB/sec throttle helps.

@jhansen-tt which version of ES did you upgrade from?

&gt;  I am seeing ~5x performance decrease in ES 1.5.1, with geoshape documents.

Hmm, @nknize did anything change in how we index geoshape docs?
</comment><comment author="nknize" created="2015-04-18T18:02:07Z" id="94187960">@mikemccand No, geo_shape indexing has not changed in 1.5.1.  I'm also curious which version of ES was used previously.  As of 1.4.3 the ShapeBuilder enforces OGC ordering, but that will only affect the unwinding for shapes crossing the dateline.  @jhansen-tt can you provide your geo_shape mapping?
</comment><comment author="jhansen-tt" created="2015-04-21T20:10:26Z" id="94923521">The mapping is simply

&#8220;myIndex&#8221;: {
  &#8220;mappings&#8221;: {
    &#8230; other non-geo_shape properties
    &#8220;data&#8221;: {
      &#8220;properties&#8221;: {
        &#8220;geometry&#8221;: {
          &#8220;type&#8221;: &#8220;geo_shape&#8221;
        }
      }
    }
  }
}

&gt; On Apr 18, 2015, at 12:02 PM, Nick Knize notifications@github.com wrote:
&gt; 
&gt; @mikemccand https://github.com/mikemccand No, geo_shape indexing has not changed in 1.5.1. I'm also curious which version of ES was used previously. As of 1.4.3 the ShapeBuilder enforces OGC ordering, but that will only affect the unwinding for shapes crossing the dateline. @jhansen-tt https://github.com/jhansen-tt can you provide your geo_shape mapping?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/10409#issuecomment-94187960.
</comment><comment author="clintongormley" created="2016-01-15T19:00:23Z" id="172054061">So much has changed since 1.5, I'm going to close this issue now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Prefix Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10408</link><project id="" key="" /><description>There is Prefix Query, Prefix Filter, but no **Prefix Aggregation**. It should be fairly easy to add since GeoHash Aggregation should internally work the same way (I guess).

Currently it can be kind of simulated by creating Filters Aggregation and specifying one Prefix Filter per each desired prefix-bucket, but it's not quite that and there might be some performance issues.

A Prefix Aggregation with a prefix length parameter would be really handy in many cases.
</description><key id="66111982">10408</key><summary>Add Prefix Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JanJakes</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-04-03T08:59:17Z</created><updated>2017-05-11T13:15:35Z</updated><resolved>2016-01-15T18:59:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-03T22:05:05Z" id="89434350">Can you give more information about your use-case? I would be interested to know more about the big picture to see how we can improve elasticsearch to address your needs.
</comment><comment author="JanJakes" created="2015-04-04T08:29:54Z" id="89528011">Our use case might be a little bit special - we use it to store quadkeys (GeoHash is not suitable for us) and to spread items evenly on map tiles by Prefix Aggregation (currently simulated by filters). However, aggregating items by a prefix of certain length seems to be quite a legit and general use case - one might want to group items to buckets by their prefixes (see http://stackoverflow.com/questions/23067983/term-aggregation-consider-only-the-prefix-to-aggregate) which can be quite useful for any string codes or hashes that have some defined structure. A more general way to look at it is that one could use strings as any type of tree structures (binary 0110..., quadtree 01032..., etc.) and aggregate items in subtrees by their prefix. GeoHash Aggregation is a special case of a more general "string prefix aggregation".

**Some more details about our use case:**
We use custom quadkeys instead of GeoHash since it allows as to query items on a map per-tile (as they are really displayed on the map). A simple query to get top items for a particular tile would be something like "_get N items with quadkey prefix P sorted by rating R with limit L_".

However, this could return many items in one particular region of the tile (they have all high rating) an we would not be able to display them all anyway because they would overlap (while other regions would be empty). To spread the items on a tile we want to group them by a prefix longer than the quadkey of the tile and get some Top Hits from each of the groups (each tile is thus partitioned in 4, 16, 64, etc. subregions depending on the prefix length). This is equivalent to GeoHash Aggregation, but it is much more general.
</comment><comment author="sciphilo" created="2015-04-07T14:04:52Z" id="90570455">I'd find this very useful too.  Right now I'd like to be able to aggregate on hierarchical paths such as /1/2/3 by specifying a prefix or a depth (for example a depth of 2 would be /1/2).
</comment><comment author="JanJakes" created="2015-06-16T13:56:26Z" id="112441264">Any updates/ideas on this feature?
</comment><comment author="costasovo" created="2015-07-03T07:52:34Z" id="118267640">This would be useful. +1
</comment><comment author="wuranbo" created="2015-07-17T11:28:41Z" id="122250904">badly need. 
Now the RESTful url always contain the resource id, so the analyse of a kind of user action become impossible.

Considering most people [recommending best practices of url](http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api#restful) which has been widely deployed in many projects. For example:

```
/api/v0/user/456/setting
/user/458
/blog/7788
/commit/4578
```

@JanJakes @jpountz 
When we want to a term-agg by path.The result should looks like 

```
{
     buckets: [
    {
       "key": "/blog",
       "doc_count": 1000
    } ,
    {
       "key": "/user",
       "doc_count": 100
    }
  ]
}
```

 We only find out to use a complex script do this without additional index now.

Future more,  `/album/3434/track/2324` this kind of url is more harder..... 
</comment><comment author="wuranbo" created="2015-07-18T03:06:51Z" id="122477240">@sciphilo 
How to specifying a depth? I did not find.
</comment><comment author="sciphilo" created="2015-07-18T08:46:50Z" id="122514477">@wuranbo Depth was a theoretical suggestion , that functionality doesn't exist yet in his context afaik. 
</comment><comment author="sciphilo" created="2015-07-18T08:48:40Z" id="122514623">@wuranbo in the end I just loaded each path segment (path to root) flat into the index. Then my aggregation counts worked nicely. So for 1/2/3 I loaded 1/, 1/2/,1/2/3/
</comment><comment author="whythecode" created="2015-09-21T09:10:03Z" id="141919242">+1
</comment><comment author="clintongormley" created="2016-01-15T18:59:34Z" id="172053873">As with most things in Elasticsearch, it is better to use an index time solution instead of a query time solution.  The `terms`  aggregation is more efficient because it uses global ordinals rather than the raw string term, while a runtime `prefix` agg would need to work with raw strings.

Preparing your data at index time can be done with the right analysis chain, eg for paths like `/foo/bar/baz` you could use a path-hierarchy tokenizer.  For instance:

```
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "paths": {
          "tokenizer": "path_hierarchy"
        }
      }
    }
  },
  "mappings": {
    "my_type": {
      "properties": {
        "path": {
          "type": "string",
          "analyzer": "paths",
          "search_analyzer": "keyword"
        },
        "depth": {
          "type": "integer"
        }
      }
    }
  }
}
```

The `path` field will tokenize `/foo/bar/baz` as `[/foo, /foo/bar /foo/bar/baz]`.  The `depth` field is easy to precalculate and can be used for various filters if needed (see below).  Index some data:

```
POST my_index/my_type/_bulk
{"index":{"_id":1}}
{"path":"/foo","depth":1}
{"index":{"_id":2}}
{"path":"/foo/bar","depth":2}
{"index":{"_id":3}}
{"path":"/foo/baz","depth":2}
{"index":{"_id":4}}
{"path":"/foo/bar/one","depth":3}
{"index":{"_id":5}}
{"path":"/foo/bar/two","depth":3}
```

Get all documents anywhere in the tree beginning with `/foo`:

```
GET _search
{
  "query": {
    "match": {
      "path": "/foo"
    }
  }
}
```

Get direct children of `/foo` only:

```
GET _search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "path": "/foo"
          }
        },
        {
          "term": {
            "depth": 2
          }
        }
      ]
    }
  }
}
```

Return the 10 paths under `/foo` with the most descendants:

```
GET _search?size=0
{
  "query": {
    "match": {
      "path": "/foo"
    }
  },
  "aggs": {
    "popular_descendants": {
      "terms": {
        "field": "path"
      }
    }
  }
}
```

Return the 10 direct children of `foo`with counts of all their descendants:

```
GET _search?size=0
{
  "query": {
    "match": {
      "path": "/foo"
    }
  },
  "aggs": {
    "popular_children": {
      "terms": {
        "field": "path",
        "include": "(/[^/]+){2}"
      }
    }
  }
}
```

Return the 10 direct children of `foo`with counts of all their descendants AND counts of their direct children (ie grandchildren):

```
GET _search?size=0
{
  "query": {
    "match": {
      "path": "/foo"
    }
  },
  "aggs": {
    "popular_children": {
      "terms": {
        "field": "path",
        "include": "(/[^/]+){2}"
      },
      "aggs": {
        "grandchildren_counts": {
          "filter": {
            "term": {
              "depth": 3
            }
          }
        }
      }
    }
  }
}
```

Note: the `include` parameter still requires some raw text crunching.  It would be more efficient still if you know the maximum number of levels you can have in your tree, and could index these docs as:

```
{
  "level_1": "/foo",
  "level_2": "/foo/bar",
  "level_3": "/foo/bar/baz"
}
```

In the Elasticsearch API, we try to direct users towards the most efficient way to do things.  I'm against adding an arbitrary `prefix` aggregation because it hides the complexity of the search operation, in other words it makes it seem like the search operation should be light when really it is doing a lot of manual work behind the scenes, which could be done more efficiently at index time.
</comment><comment author="yavuzmester" created="2017-05-11T13:10:53Z" id="300784339">@clintongormley how to do this for quadtrees and isn't it better to provide a quadtree_grid aggregation like geohash_grid aggregation?

Example use case is drawing heatmaps with Leaflet:

- For each visible tile (256 px):
     - I convert the tile to its path equivalent in the quadtree
     - I dive to the path with 8 levels to get an aggregated result pixel by pixel (where in the response the "key"s will be the paths to those pixels)
     - Then I assign colors for each pixel value and draw the tile canvas.

- I also have a configuration parameter to adjust resolution so that i may decrease resolution by 1 level and that means I have to dive 7 levels instead of 8. Then the squares will be 4 times big (2 px by 2 px instead of a single pixel). I can decrease resolution one by one until I dive 0 levels and the squares will get bigger the same way. Geohash aggregation is not that predictable, i.e. sometimes the aggregation units are squares sometimes rectangles.

Thanks</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A network partition isolating a primary can cause the loss of inserted documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10407</link><project id="" key="" /><description>This is tough for me to reliably reproduce, but I've seen it a half-dozen times and think it needs an issue. Elasticsearch can still lose inserted documents when a single primary node is isolated by a simple network partition. Nothing fancy, no overlapping partitions required. Sometimes it's only a few documents, from immediately following the partition:

``` clj
{:valid? false,
 :lost "#{640 643 645 649}",
 :recovered
 "#{140..142 144 146 148..152 155 157 159..162 164..168 171 174..175 177 179..185 187 190..192 194 197 199..200 202 205..208 214 224 234 238 240..247 249..254 256..264 266..282 285 296 317 342 363 388 431}",
 :ok
 "#{0..137 140..142 144 146 148..152 155 157 159..162 164..168 171 174..175 177 179..185 187 190..192 194 197 199..200 202 205..208 210..212 214 224 234 238 240..247 249..254 256..264 266..639 641..642 644 646..648 650..654}",
 :recovered-frac 96/655,
 :unexpected-frac 0,
 :unexpected "#{}",
 :lost-frac 4/655,
 :ok-frac 598/655}
```

Other times, the cluster seems to get really confused, and loses over half of the acknowledged records, distributed throughout the history:

``` clj
{:valid? false,
 :lost
 "#{53..55 126..129 131..142 144..167 169..189 191..212 214..235 237..260 262..281 283..305 307..327 329..352 354..367 374 376}",
 :recovered
 "#{46..47 49 52 60 428..430 432..435 437..440 442..445 447..450 452..455 457..473}",
 :ok
 "#{0..47 49 52 60 428..430 432..435 437..440 442..445 447..450 452..455 457..473}",
 :recovered-frac 15/158,
 :unexpected-frac 0,
 :unexpected "#{}",
 :lost-frac 118/237,
 :ok-frac 91/474}
```

The logs aren't particularly informative here, but I have noticed some interesting messages which seem correlated with data loss: for instance:

```
[2015-04-02][WARN ][index.shard] [n4] [jepsen-index][0] suspect illegal state: trying to move shard from primary mode to replica mode
```

and

```
[2015-04-02 17:19:54,370][DEBUG][action.index             ] [n2] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
```

which I think is a hardcoded message.

Again, these are tough to reproduce--only 1 in 10 runs or so--so I'll keep working on finding a failure schedule that reliably triggers the fault.
</description><key id="66041777">10407</key><summary>A network partition isolating a primary can cause the loss of inserted documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aphyr</reporter><labels><label>bug</label><label>resiliency</label></labels><created>2015-04-03T00:46:02Z</created><updated>2016-05-12T11:55:12Z</updated><resolved>2016-05-12T11:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aphyr" created="2015-04-03T01:06:21Z" id="89102489">Potentially related: the worst data-loss incidents seem related to long-lasting split-brain, where after a 120-second partition isolating a single primary is resolved, the original node _and_ a newly elected node both believe they're the primary--even after exchanging traffic.
</comment><comment author="aphyr" created="2015-04-03T01:33:42Z" id="89107918">Here's a particularly rough case, losing 25% of documents with only primary-isolating partitions:

``` clj
{:valid? false,
 :lost
 "#{50..53 125..128 130..149 151..170 172..194 196..217 219..239 241..263 265..286 288..309 311..331 333..355 357..378 380..392 394}",
 :recovered
 "#{400 448..449 451 453..454 456..459 461..464 466..469 471..474 476..484 486..489 491..494 496..498 516 619 624 629 634 639 644 647..649 651..654 656..659 661..665 667..669 671..675 677..680 682..684 686..689 691 877..879 882..884 887..889 892..894 897..899 902..904 907..909 912..914 917..919}",
 :ok
 "#{0..49 400 448..449 451 453..454 456..459 461..464 466..469 471..474 476..484 486..489 491..494 496..499 501..537 539..561 563..583 585..606 608..617 619 624 629 634 639 644 647..649 651..654 656..659 661..665 667..669 671..675 677..680 682..684 686..689 691..694 696..713 715..737 739..758 760..783 785..804 806..829 831..848 850 877..879 882..884 887..889 892..894 897..899 902..904 907..909 912..914 917..919 922..924 927..938 940..942 944..956 958..960 962..975 977..979 981..993 995..998 1000..1012 1014..1016 1018..1030 1032..1035 1037..1039}",
 :recovered-frac 7/65,
 :unexpected-frac 0,
 :unexpected "#{}",
 :lost-frac 261/1040,
 :ok-frac 53/104}
```

This test is 600 seconds long, where the network is stable for 10 seconds, then all nodes considering themselves the primary are isolated for 120 seconds, then the network heals and the cycle repeats. We give the cluster 20 seconds of guaranteed healthy network, then wait for the health endpoint to return green on all nodes, then wait a while just in case, and do a final read.

``` clj
            :generator (gen/phases
                         (-&gt;&gt; (range)
                              (map (fn [x] {:type  :invoke
                                            :f     :add
                                            :value x}))
                              gen/seq
                              (gen/stagger 1/10)
                              (gen/delay 1)
                              (gen/nemesis
                                (gen/seq (cycle
                                           [(gen/sleep 10)
                                            {:type :info :f :start}
                                            (gen/sleep 120)
                                            {:type :info :f :stop}])))
                              (gen/time-limit 600))
                         (gen/nemesis
                           (gen/phases
                             (gen/once {:type :info :f :stop})
                             (gen/sleep 20)))
                         (gen/clients
                           (gen/once {:type :invoke :f :read})))}))
```

In this test, we isolate [n4], then [n5], then [n4], and then something interesting happens: both n5 _and_ n4 consider themselves primaries, even after 120 seconds of n4 being isolated. On the next cycle round, n2 and n4 both consider themselves primaries!

In an attempt to speed up fault detection (this network is virtual with negligible latency and no contention except during simulated partitions), I've lowered the ping timeouts to the order of 2-3 seconds--the cluster seems to detect failures within ~10 seconds normally, but sometimes gets stuck!

``` yaml
discovery.zen.ping.timeout: 3s

# After joining, send a join request with this timeout? Docs are kinda unclear
# here.
discovery.zen.join_timeout: 5s

# Fault detection timeouts. Our network is super reliable and uncongested
# except when we explicitly simulate failure, so I'm gonna lower these to speed
# up cluster convergence.
discovery.zen.fd.ping_interval: 1s
discovery.zen.fd.ping_timeout: 1s
discovery.zen.fd.ping_retries: 2

# Publish cluster state updates more frequently
discovery.zen.publish_timeout: 5s
```

I've uploaded the Jepsen history, set analysis, and elasticsearch logs (timestamps are perfectly coordinated--they're LXC containers) to https://aphyr.com/media/es-create-loss.tar.gz, if you'd like to take a look. :)
</comment><comment author="bleskes" created="2015-04-03T06:59:43Z" id="89195569">Thx @aphyr 

&gt; Elasticsearch can still lose inserted documents when a single primary node is isolated by a simple network partition

This might be #7572 , which is also documented here: http://www.elastic.co/guide/en/elasticsearch/resiliency/current/#_loss_of_documents_during_network_partition_status_ongoing 

&gt; Other times, the cluster seems to get really confused

This one feels like something else, but it will require some research. Can you share the exact commit of Jepsen you used? That will help understand what exactly happened.  I'm also curious to see what effect @dakrone 's [suggestion to use scan scroll](https://twitter.com/thnetos/status/583387248759971840) will have. 
</comment><comment author="aphyr" created="2015-04-03T18:11:27Z" id="89378714">Hi @bleskes! This is from Jepsen 4fdf509d82620ac59b4daf4e6a21f9495c73e68a--just `cd elasticsearch; lein test`. I've been breaking up the test suites in hopes of actually having Jepsen As A Library instead of a big monolithic hairball, haha. Slow going so far. ;-)

I've also switched to using scan scroll, but it doesn't seem to have any effect on observed data loss--I can reproduce it with both a `size: n` query or a scrolling one.
</comment><comment author="dakrone" created="2015-04-07T16:02:48Z" id="90621695">Hi @aphyr , I've been looking through the https://aphyr.com/media/es-create-loss.tar.gz logs that you provided, however, it looks like the history.edn timestamps are nanoseconds relative to the time that the test was started.

Could you add or modify the timestamps to be absolute instead of relative so we can correlate the logs from the ES instances to the Jepsen history?
</comment><comment author="aphyr" created="2015-04-08T17:31:39Z" id="90982197">Sorry, I've got a huge backlog at the moment and have to get slides prepped, but you can just flip https://github.com/aphyr/jepsen/blob/master/jepsen/src/jepsen/core.clj#L114 to use (System/currentTimeMillis) if you want unix timestamps. Be advised that Jepsen has facilities for lying to the database about what time it is--those aren't currently in play on the Elasticsearch tests, but I wouldn't rely on the log timestamps going forward. ;-)
</comment><comment author="bleskes" created="2015-04-28T11:14:09Z" id="97022642">A quick update on this one - we have put some time into reproducing this and it looks more and more like #7572 . I will post an update once this becomes definite (or if anything else comes up).
</comment><comment author="clintongormley" created="2016-05-12T11:55:11Z" id="218735617">Closed as duplicate of #7572
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester incorrect sorting by weight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10406</link><project id="" key="" /><description>```
"version" : {
    "number" : "1.4.4",
    "lucene_version" : "4.10.3"
  }
```

I have about **20m** database of keywords such as:
"**car**" (weight is 100)
"**buy car**" (weight is 50) 
"**buy used car**" (weight is 10)
...
each keyword having weight equal popularity. Initial mapping is:

```
PUT /suggestions/search/_mapping
{
  "search": {
            "properties": {
               "suggest": {
                  "type": "completion",
                  "analyzer": "standard"
               }
            }
         }
}
```

And indexing is simple:

```
PUT /suggestions/search/1
{
   "name": "car"
   "suggest": {
      "input": "car",
      "weight": 100
   }
}
```

So, considering the above list, when I type "car" I will only get "**car**" query. It worked fine as long as I did not want to look up words in middle of phrase. The solution I have found is split keyword to words and add them ti suggestion input as array:

```
PUT /suggestions/search/1
{
   "name": "buy used car"
   "suggest": {
      "input": ["buy", "used", "car"],
      "weight": "10"
   }
}
```

Now when I type "car" I can find all of three keywords correctly sorted by weight. The solution was very rapid, much faster then any other elasticsearch autocompletion features. But then there was a big problem.

When I reached 200k different keywords related to "car" (indexing arrays like [best, car, buy], [car, rental], [used,car,sale], etc...), I have problem with sorting by weight. When I make query:

```
GET /suggestions/_suggest
{
   "text" : "car",
   "completion": {
      "field": "suggest"
   }
}
```

The results are sorted in wrong order. The weight (imagine that top keyword "car" has 200k weight and bottom is 1) is shifted randomly and first result has a weight of 100-180k instead of 200k.

You can reproduce this by creating index in loop (pseudo-code):

```
for ($i=1;$i&lt;200000;$i++)
{

   PUT /suggestions/search/$i
   {
      "name": "$i"
      "suggest": {
         "input": ["test", "$i"], // $i as second word is needed otherwise results will be merged
         "weight": $i
      }
   }

}
```

Then try to search "test" and you can see something like this:

```
{
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "suggest": [
      {
         "text": "test",
         "offset": 0,
         "length": 4,
         "options": [
            {
               "text": "179992",
               "score": 179992
            },
            {
               "text": "179940",
               "score": 179940
            },
            {
               "text": "179868",
               "score": 179868
            },
            {
               "text": "179820",
               "score": 179820
            },
            {
               "text": "179696",
               "score": 179696
            }
         ]
      }
   ]
}
```

As you can see, first result is 179992, not 200000. Of course the top weight is 200k and index contains 200k elements. What could be the problem? This is feature of FST working or just a bug?
</description><key id="66008365">10406</key><summary>Completion Suggester incorrect sorting by weight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">ravlio</reporter><labels /><created>2015-04-02T20:44:19Z</created><updated>2017-04-27T16:43:21Z</updated><resolved>2015-04-03T02:41:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-04-03T02:41:32Z" id="89131271">Thanks for the detailed report, @ravlio!
Currently, there is a hard limit of at most 256 duplicated analyzed form per (lucene) segment, after which the suggestion entries are ignored. It seems like you have hit that limit.
In this case, the analyzed form of "car" must have exceeded that limit in some segments and hence you see missing suggestions. 
A work-around might be to use multiple surface forms to represent "car", s.t. none of the surface forms hit this limit. 
This is a known issue, we are working towards removing this limitation.  
</comment><comment author="ravlio" created="2015-04-12T09:17:02Z" id="92022019">Finally, I have found a trick to solve problem with limitations. Simply we append each input word with set of random characters (5 chars length is enough to for getting unique words). Another hint is to make first character of appending numeric. 

So the indexing now looks like:

```
"suggest": {
         "input": ["best card to buy", "best1dricj", "car1vjtiw", "buy1clskq"], // "to" is a stopword and omitted
         "output" : "best car to buy", // output is required to get whole phrase in results instead of searchable words 
         "weight": 123
      }
```

The "**1**" digit prevents cases when word and random string form other words. For example, in some case  we can get randomly word "**cardhsqoe**" which besides "**car**" will be also searched by "**card**" giving inaccurate results. Maybe is is possible to more unique character like "**#**", need to try.

This solution words perfectly in 20m index. 
</comment><comment author="trompx" created="2017-04-27T16:03:12Z" id="297759564">Hello,

ES version: 5.2

Thanks a lot for the solution @ravlio. Unfortunatly, it is not fully working in my case. I have less data so I don't really know what may be the bug.

I have 15 strings composed of multiple words : `word1 word2 word3 word4 ...`
word1 is common to all of those 15 strings.
I index as input all permutation of those words (I have between 30-1000 inputs for each indexed documents).
```
"suggest": {
         "input": ["word1", "word2", "word3", "word4", "word1 word2", "word1 word3" and so on...],
         "output" : "word1 word2 word3 word4",
         "weight": 1000
}
```
Searching for word1 was only returning 2 strings.

I tried adding a random string at the end of each input:
```
"suggest": {
         "input": ["word1$rnd1", "word2$rnd2", "word3$rnd3", "word4$rnd4", "word1 word2$rnd5", "word1 word3$rnd6" and so on...],
         "output" : "word1 word2 word3 word4",
         "weight": 1000
}
```
Now I get more results, but only 5 out of 15.

Did you find a better solution?

@areek Did you come up with something to remove that limitation?
@ravlio did you use anything not default for the mapping?

Thank you

**Update** : the problem happens when not setting weight, or setting a common weight for all strings. When setting weight per input, I get all the strings. Is this expected behaviour?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor SimpleQueryStringBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10405</link><project id="" key="" /><description>SimpleQueryStringBuilder now includes the `fromXContent`, `doXContent`,
and `toQuery` methods as well as implementing the Streamable interface.

It includes unit tests for the serialization and parsing.

Because of testing, I also implemented `hashCode` and `equals` similar
to the way that Lucene queries implement these.

_NOTE_: this PR is against the `feature/query-parse-refactoring` branch.
</description><key id="66003507">10405</key><summary>Refactor SimpleQueryStringBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query Refactoring</label></labels><created>2015-04-02T20:17:52Z</created><updated>2015-04-24T19:35:31Z</updated><resolved>2015-04-24T19:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-10T14:46:40Z" id="91580630">looks good, I have some comments that are mainly around testing, same as in #10454. I think we should go for a test base class so we trim down the test code required. Define a bunch of tests that need to be present for each query, try and write them in the base class and define abstract methods that we have to go and extend in each specific query test. How does this sound?
</comment><comment author="dakrone" created="2015-04-24T19:35:30Z" id="96043161">Closing this, I will re-create based on the new feature branch since we went a different direction.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent value types for node status/info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10404</link><project id="" key="" /><description>When viewing the following URL, http://localhost:9200/_nodes/_local, queue sizes are not consistent in types, for example:

```
merge: {
type: "scaling",
min: 1,
max: 4,
keep_alive: "5m",
queue_size: -1
}
```

versus:

```
get: {
type: "fixed",
min: 8,
max: 8,
queue_size: "1k"
}
```

The queue_size value should always be an integer, fully expanded out, ie, 1000 and not "1k". This will make monitoring these queues much simpler.
</description><key id="65998488">10404</key><summary>Inconsistent value types for node status/info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Cidan</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-02T19:48:12Z</created><updated>2015-08-24T08:06:38Z</updated><resolved>2015-08-24T08:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:47:12Z" id="89818565">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhancement: hint user on node version mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10403</link><project id="" key="" /><description>When comissioning new nodes with a lower ES version (_e.g._ 1.4.0 _vs._ 1.4.2), obviously only new shards get allocated to the new nodes, while older shards (using newer lucene version) never reallocate to new nodes. While this is fine, there should be a hint in the logs, something in the likes of:

```
[cluster.routing.allocation.decider] [somenode] old ES version detected [oE5QByo8RVSVqVDerxB-7A][comissionednode] existing shards will not be reassigned to this node"
```

Would have saved me some time today ;-)
</description><key id="65995691">10403</key><summary>Enhancement: hint user on node version mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-04-02T19:31:50Z</created><updated>2017-05-26T15:29:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:45:30Z" id="89818193">@faxm0dem nice idea, but I'm not sure how doable it is.  The allocation decider makes lots and lots of decisions, which ones do we choose to log?

@dakrone what's your take on this?
</comment><comment author="javanna" created="2017-05-05T16:09:46Z" id="299507096">@dakrone thoughts on this?</comment><comment author="dakrone" created="2017-05-08T17:44:55Z" id="299938005">I think this is something we could pursue, it'd be nice to tell people about version mismatches.

&gt; The allocation decider makes lots and lots of decisions, which ones do we choose to log?

I don't think we should do this in the allocation decider, rather I think we should do this when the node joins via zen discover, log some sort of warning about node version mismatch. I think in all cases the mismatch should be short-lived (rolling restart, or a mistake), so it'd be good to let people know as soon as the nodes are joined.

Do you think this is something we should add to zen @ywelsch?
</comment><comment author="ywelsch" created="2017-05-08T19:54:25Z" id="299972736">@dakrone I think that the allocation explain API in ES 5 should be sufficient for this purpose. We can extend the message that is given by the `NodeVersionAllocationDecider` to state the reason why we won't allow recovery from a new to an old-version node (currently it only states "target node version [%s] is older than source node version [%s]").</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkRequest with listener waiting for response fails assertion 'Expected current thread [...] to not be a transport thread'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10402</link><project id="" key="" /><description>Starting with 1.5.0, creating a bulk request then calling **bulkRequest.execute().addListener(...)** results in

&gt; java.lang.AssertionError: Expected current thread [...] to not be a transport thread. Reason: 

full stack trace : https://gist.github.com/nmunro-cvt/ea14d625629692ef2773

Started happening in 1.5.0, same code works fine pre-1.5.0.

Noticed some new assertions were introduced in 1.5.0

&gt; https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java#L117

However, I don't see what I need to do differently to pass the assertions.

Simplified example to reproduce error: https://gist.github.com/nmunro-cvt/0be42236e9abf4359a44
</description><key id="65994900">10402</key><summary>BulkRequest with listener waiting for response fails assertion 'Expected current thread [...] to not be a transport thread'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nmunro-cvt</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-04-02T19:27:25Z</created><updated>2015-06-04T20:36:58Z</updated><resolved>2015-06-04T20:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:44:08Z" id="89818072">@jpountz any ideas what should be changed here?
</comment><comment author="jpountz" created="2015-04-09T09:37:51Z" id="91175364">It looks to me like an actual bug that we only find about now thanks to #9164. The transport thread waits for the action to be executed so that it can run the listeners, but my understanding is that we should never wait in transport threads?

@bleskes @s1monw Since you helped reviewing on #9164 could you have a look at this issue and confirm/infirm this is a real bug?
</comment><comment author="clintongormley" created="2015-04-12T14:54:55Z" id="92076650">@bleskes assigning to you
</comment><comment author="bleskes" created="2015-06-04T20:36:58Z" id="109041685">this should be fixed now with https://github.com/elastic/elasticsearch/pull/10573 . @nmunro-cvt thx for reporting!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException when indexing document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10401</link><project id="" key="" /><description>I'm trying to index a document using the following procedure:

``` java
byte[] source = myDocument;
final IndexResponse response =  client
        .prepareIndex(index, type, id)
        .setSource(source)
        .execute()
        .actionGet();
```

When I'm doing so, I get a `MapperParsingException` saying that it can't parse an input string within my document. However, when I attempt to index the exact document through a REST client with the Index API, it completes without any issue. It would be nice if I could use Elasticsearch's Java API to index my documents, but if there isn't a solution, I'll have to make a REST call.
</description><key id="65980195">10401</key><summary>MapperParsingException when indexing document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cashgl</reporter><labels /><created>2015-04-02T18:04:09Z</created><updated>2015-04-02T18:06:52Z</updated><resolved>2015-04-02T18:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-02T18:06:52Z" id="88994883">Please ask questions like this on the mailing list and provide details about what you have in `source` field.
May be your `byte[]` is not correct?

try with a String first.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Repeatedly segfaults on Ubuntu 14.04</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10400</link><project id="" key="" /><description>Hi,
After upgrading our (super-tiny) instance from 0.90 to 1.5 ES has started to crash. Here are some of the java dump files: https://gist.github.com/alexmorozov/b49f1c8c755236d237b6
It crashes randomly, there`s no specific query that causes segfault.
Can it be a memory issue?
We run at default settings (max 1G heap), but we have only about 50 documents in the index (with many fields though).
</description><key id="65971057">10400</key><summary>Repeatedly segfaults on Ubuntu 14.04</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexmorozov</reporter><labels /><created>2015-04-02T17:16:56Z</created><updated>2016-01-14T14:16:47Z</updated><resolved>2016-01-14T14:16:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T20:04:34Z" id="89027768">The crash happens while compiling a groovy script. Can you share the groovy script that you are using in your search request?
</comment><comment author="alexmorozov" created="2015-04-03T06:44:05Z" id="89191313">Sure. The script is generated dynamically, here's the typical one: https://gist.github.com/alexmorozov/b4aa8aabc8df69731aab . Note that this script doesn't crash the server alone, ES segfaults after many of similar requests.
And I'm not sure it`s in Groovy, as far as we created it in the 0.90 (mvel) days.
</comment><comment author="jpountz" created="2015-04-06T18:21:29Z" id="90183808">&gt; And I'm not sure it`s in Groovy

The reason why I thought so is that it seems to apply while Groovy is compiling the script (`org.codehaus.groovy.control.CompilationUnit.compile(I)`)

Can you maybe check if you can reproduce the error if you use mvel? ( available as a plugin at https://github.com/elastic/elasticsearch-lang-mvel)
</comment><comment author="clintongormley" created="2016-01-14T14:16:47Z" id="171654814">No more feedback in 9 months. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explanation with null description causes NullPointerException in distributed query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10399</link><project id="" key="" /><description>I'm writing a custom score function that returns a deeply-nested Explanation object. When a description in one of the nested Explanations is null I get a NullPointerException from the node. This doesn't happen when I run things on a single node, so it's likely some issue with the serialization of Explanation objects.
</description><key id="65962619">10399</key><summary>Explanation with null description causes NullPointerException in distributed query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mausch</reporter><labels><label>feedback_needed</label></labels><created>2015-04-02T16:25:06Z</created><updated>2015-04-21T07:51:05Z</updated><resolved>2015-04-21T07:51:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:32:32Z" id="89816256">Hi @mausch 

Any chance you can provide some simple code which replicates the problem?  Without more to go on, I doubt anybody will pick this one up...

thanks
</comment><comment author="mausch" created="2015-04-20T15:54:57Z" id="94492160">Due to the nature of the issue unfortunately this is not easy to reproduce with some simple code.

From my point of view it means setting up a new plugin project with maven and so on, create a module class that registers a custom ScoreFunctionParser which in turn instantiates a custom ScoreFunction that returns an Explanation with a null description. And then run this on many instances of ES in a cluster and run a query with `"explain": true`. Sadly I can't afford to spend so much time on a non-blocking issue.

But perhaps someone with better knowledge of ES internals could directly unit-test the serialization of `Explanation` and see if that's really what's causing the `NullPointerException` I see.
</comment><comment author="jpountz" created="2015-04-20T16:31:20Z" id="94501482">@mausch Indeed our serialization logic treats descriptions as non-null strings while you can create Lucene explanations with `null` descriptions. However an explanation with a `null` description feels useless to me, I'm tempted to consider that the bug is that the explanation API does not require the description field. Is there a reason why you don't set a description on your explanations?
</comment><comment author="mausch" created="2015-04-20T16:47:35Z" id="94505482">I simply forgot to call `setDescription` on some `Explanation` instance and got this NullPointerException which was not helpful at all to diagnose the cause of the problem. That's why I'm reporting it, things like waste people's time.

If it was up to me I'd absolutely make `Explanation` immutable and check for null in the constructor. But I don't think you can change this by now, it seems like it would be a pretty important breaking change in Lucene. 

A less breaking change would be marking the no-arg constructor and setters with `@Deprecated` but I doubt people will go for even that.

Instead, ES could simply throw a more specific exception so that people at least don't have to guess what the problem is.
</comment><comment author="jpountz" created="2015-04-20T16:53:03Z" id="94507094">&gt; it would be a pretty important breaking change in Lucene.

In general we try to weigh the pros/cons of API breaks: breaking an API is unpleasant but leaving API traps is probably worse. I'll open an issue.

&gt; Instead, ES could simply throw a more specific exception so that people at least don't have to guess what the problem is.

I can do that.
</comment><comment author="mausch" created="2015-04-20T16:55:14Z" id="94507507">Cool, thanks!
</comment><comment author="jpountz" created="2015-04-20T22:06:37Z" id="94580504">I opened https://issues.apache.org/jira/browse/LUCENE-6446 and #10689
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Pes Plugin to the misc page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10398</link><project id="" key="" /><description>I do not know normally whether this section is the right place or not, please let me know.

Here is the pes plugin's GitHub site :https://github.com/kodcu/pes
</description><key id="65959797">10398</key><summary>Add Pes Plugin to the misc page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ozlerhakan</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-04-02T16:09:00Z</created><updated>2015-04-05T18:35:46Z</updated><resolved>2015-04-05T18:15:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-03T12:33:02Z" id="89274937">HI @ozlerhakan thaks for your PR. Trying to find the best place for your project. I believe it is not what we'd call a plugin, hence I would lean towards adding it to the community page. How about adding it [here](https://github.com/elastic/elasticsearch/blob/master/docs/community/misc.asciidoc)?
</comment><comment author="ozlerhakan" created="2015-04-03T12:37:53Z" id="89276335">Hi @javanna , I actually do not truly call it as a plug-in either, your suggestion seems fine by me :+1: 
</comment><comment author="javanna" created="2015-04-03T12:38:23Z" id="89276469">cool, do you feel like updating your PR then?
</comment><comment author="ozlerhakan" created="2015-04-03T12:39:50Z" id="89276908">Yep, I will add it to that page instead of the plugin 
</comment><comment author="clintongormley" created="2015-04-05T18:15:01Z" id="89823976">thanks @ozlerhakan - merged
</comment><comment author="ozlerhakan" created="2015-04-05T18:35:46Z" id="89830134">You're welcome @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search templates created with restapi   are not deleteing completely </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10397</link><project id="" key="" /><description>We created a search templates with rest api  will adds to .scripts index , when we delete the templete it says success and when we explore the index the  templete  won't appear .
  When we recreate the search template with same name  it will create , when we execute the template search it will take the old  deleted  template code . we  updated to latest version 1.5  from 1.4.
You can easily replicate this issue ween u created a template with some wrong syntax , and delete  that template and recreate with same name with valid syntax , then execute it wont work .  I tried to flush cache on that index reorganize too 
</description><key id="65959441">10397</key><summary>search templates created with restapi   are not deleteing completely </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">kumar007007</reporter><labels><label>:Search Templates</label><label>bug</label></labels><created>2015-04-02T16:07:03Z</created><updated>2015-04-22T09:15:13Z</updated><resolved>2015-04-22T09:15:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:30:25Z" id="89815850">@kumar007007 could you provide a simple curl recreation which demonstrates the issue please?
</comment><comment author="kumar007007" created="2015-04-06T18:28:00Z" id="90187073">// new index 

POST testindex/test
{
  "searchtext": "dev1"
}

// creating template with  wrong type to create error 

POST _search/template/git01
{
    "query": {
                 "match_all": {
                       "searchtext": {
                              "query": "{{P_Keyword1}}",
                              "type": "oooophrase_prefix"
                                      }
                              }
              }
}

//   calling template  it should error ,  it will give error 

GET testindex/_search/template
{
 "template": {"id": "git01"}
,
"params": {
"P_Keyword": "dev"
}
}

//  fixing the template error by updating right type 
POST _search/template/git01
{
    "query": {
                 "match_all": {
                       "searchtext": {
                              "query": "{{P_Keyword1}}",
                              "type": "phrase_prefix"
                                      }
                              }
              }
}

//  fixing the template error by deleting  and recreating  template 
DELETE  _search/template/git01

POST _search/template/git01
{
    "query": {
                 "match_all": {
                       "searchtext": {
                              "query": "{{P_Keyword1}}",
                              "type": "phrase_prefix"
                                      }
                              }
              }
}
//  using template still will give error 

GET testindex/_search/template
{
 "template": {"id": "git01"}
,
"params": {
"P_Keyword": "dev"
}
}
</comment><comment author="clintongormley" created="2015-04-06T18:56:37Z" id="90197401">Hi @kumar007007 

Your example didn't actually work, but it gave me enough to demonstrate the problem: existing templates never get reparsed, even when they are overwritten:

```
 # create test doc
POST testindex/test
{
  "searchtext": "dev1"
}

 # Create bad template
PUT _search/template/git01
{
  "query": {
    "match": {
      "searchtext": {
        "query": "{{P_Keyword1}}",
        "type": "ooophrase_prefix"
      }
    }
  }
}

 # Throws exception
GET testindex/_search/template
{
  "template": {
    "id": "git01"
  },
  "params": {
    "P_Keyword1": "dev"
  }
}

 # Overwrite with correct template
PUT _search/template/git01
{
  "query": {
    "match": {
      "searchtext": {
        "query": "{{P_Keyword1}}",
        "type": "phrase_prefix"
      }
    }
  }
}

 # Search fails with old error
GET testindex/_search/template
{
  "template": {
    "id": "git01"
  },
  "params": {
    "P_Keyword1": "dev"
  }
}

 # Create template with new ID
PUT _search/template/git02
{
  "query": {
    "match": {
      "searchtext": {
        "query": "{{P_Keyword1}}",
        "type": "phrase_prefix"
      }
    }
  }
}

 # Now it works correctly
GET testindex/_search/template
{
  "template": {
    "id": "git02"
  },
  "params": {
    "P_Keyword1": "dev"
  }
}
```
</comment><comment author="clintongormley" created="2015-04-06T18:57:50Z" id="90197909">@MaineC coule you look at this one too please?
</comment><comment author="ericbernal" created="2015-04-07T20:56:52Z" id="90730382">i have the same issue.. forcing us to resort to only managing templates from file and not index.  
@MaineC show us some magic
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed scripts/templates: return response body when script is not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10396</link><project id="" key="" /><description>Align get indexed scripts and get search template apis to our get api, which returns a response body when the document is not found, with a found boolean flag. Also, return metadata info all the time too.

Closes #7325
</description><key id="65943589">10396</key><summary>Indexed scripts/templates: return response body when script is not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-02T14:51:03Z</created><updated>2015-04-08T09:02:32Z</updated><resolved>2015-04-08T09:02:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-07T16:58:30Z" id="90645930">@clintongormley can you please review?
</comment><comment author="clintongormley" created="2015-04-07T17:38:55Z" id="90663867">@javanna REST tests look good to me
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: implement delete-by-query as search + delete-by-id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10395</link><project id="" key="" /><description>Here's a first cut, not yet ready (some nocommits), switching InternalEngine's DBQ impl to "run a query and delete those docs" instead of using IndexWriter's delete-by-query.  Tests pass...

I just run the query on the current NRT searcher and for each hit I pull the current version and pass that to the Delete request, but I think this means we must add VersionType param to DBQRequest (I put a nocommit), and I think it needs back-compat logic for older indices that stored version as a payload?

I also put a TODO to optimize this impl: we can use IndexWriter.tryDeleteDocument, since we already know which segment + docID we want to delete.  But I think this should come later...

There are two behavior changes here.  First, DBQ will now only delete those docs that the current NRT searcher "sees" matching the query. This means apps that want to be sure the delete "covers" anything they've indexed, must do a refresh themselves before the DBQ.  Second, they must also do a refresh after, to see the deletes reflected in searches, since DBQ no longer does so.
</description><key id="65939214">10395</key><summary>Core: implement delete-by-query as search + delete-by-id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">mikemccand</reporter><labels /><created>2015-04-02T14:27:57Z</created><updated>2015-06-04T19:28:51Z</updated><resolved>2015-06-04T19:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-02T14:29:18Z" id="88928622">@mikemccand I think this needs to be implemented on the action level doing scan search and then using bulk delete, as this will also solve our replication problems with delete by query. @martijnvg did something similar with update by query
</comment><comment author="clintongormley" created="2015-06-04T19:28:51Z" id="109019855">Closing as DbQ will be implemented as a plugin for 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiword query time synonyms and match queries with the "and" operator may not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10394</link><project id="" key="" /><description>This issue is very similar to what is documented on the entry about [multiword synonyms and phrase queries](http://www.elastic.co/guide/en/elasticsearch/guide/current/multi-word-synonyms.htm) . What is different is that the same kind of problems, when the synonyms are of different lengths (in number of terms), occur when using the standard [`match`](http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html) query with the `operator` flag set to `and`. In this case too, some documents may unexpectedly not match.

Here is an example that reproduces the behavior:

``` bash
# delete old index if exists
curl -XDELETE 'http://localhost:9200/multiwordsyns?pretty'

# create index with synonym analyzer and mapping
curl -XPUT 'http://localhost:9200/multiwordsyns?pretty' -d '{
    "settings" : {
        "number_of_replicas": 0,
        "number_of_shards": 1,
        "index": {
            "analysis": {
                "analyzer": {
                    "index_time": {
                        "tokenizer": "standard",
                        "filter": ["standard", "lowercase"]
                    },
                    "synonym": {
                        "tokenizer": "standard",
                        "filter": ["standard", "lowercase", "stop", "synonym"]
                    }
                },
                "filter": {
                    "synonym": {
                        "type": "synonym",
                        "synonyms": [
                            "spider man, spiderman"
                        ]
                    }
                }
            }
        }
    },
    "mappings": {
        "test": {
            "properties": {
                "text": {"type": "string", "index_analyzer": "index_time", "search_analyzer": "synonym"}
            }
        }
    }
}'

# index the test documents
curl -XPUT 'http://localhost:9200/multiwordsyns/test/1?pretty' -d '{"text": "the adventures of spiderman"}'
curl -XPUT 'http://localhost:9200/multiwordsyns/test/2?pretty' -d '{"text": "what hath man wrought?"}'
curl -XPUT 'http://localhost:9200/multiwordsyns/test/3?pretty' -d '{"text": "that spider is the size of a man"}'
curl -XPUT 'http://localhost:9200/multiwordsyns/test/4?pretty&amp;refresh=true' -d '{"text": "spiders eat insects"}'

# WRONG! finds only #1, should find #1 &amp; #3
curl -XPOST 'http://localhost:9200/multiwordsyns/test/_search?pretty' -d '{"query": {"match": {"text": {"query": "spiderman", "operator": "and"}}}}'

# Also WRONG! finds only #1, should find #1 &amp; #3
curl -XPOST 'http://localhost:9200/multiwordsyns/test/_search?pretty' -d '{"query": {"match": {"text": {"query": "spider man", "operator": "and"}}}}'
```

Also available as a gist: https://gist.github.com/ianribas/f76d20c21bb9f5c0df2f

If the synonyms are applied at index time, the example above works. This can be used as a workaround, but is a choice that has other impacts, as described on the documentation.

It took us a while to identify this problem, so I thought it was important to at least write it down so it could maybe help others.
</description><key id="65935801">10394</key><summary>Multiword query time synonyms and match queries with the "and" operator may not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ianribas</reporter><labels><label>:Query DSL</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-04-02T14:12:37Z</created><updated>2017-04-26T11:07:58Z</updated><resolved>2017-04-26T11:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T17:07:53Z" id="89813161">Hi @ianribas 

Thanks for writing this up.  @mikemccand is there any way we could improve multi-word synonym queries with the TermAutomatonQuery? 

http://blog.mikemccandless.com/2014/08/a-new-proximity-query-for-lucene-using.html
</comment><comment author="ianribas" created="2015-04-06T13:08:14Z" id="90055724">It seems the problem is the same stated on the "Limitations" sections of http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html (linked on the post referenced above): both the SynonymFilter and the QueryBuilder are not able to correctly handle the situation where the synonym and the original term have different lengths on "AND" queries.

I think this issue in Lucene relates to this problem: [LUCENE-3843](https://issues.apache.org/jira/browse/LUCENE-3843).
</comment><comment author="rmuir" created="2015-04-06T13:28:06Z" id="90064567">Thanks for the reminder @ianribas , I had completely forgotten about this issue! As a start maybe we can fix QueryBuilder to make use of positionLength where available and formulate the queries better. The logic in that thing is kind of scary and hairy, but I will take a look.
</comment><comment author="ianribas" created="2015-04-06T13:34:33Z" id="90066940">You're welcome, @rmuir. I looked around QueryBuilder a bit and the logic is really complex already. And I fear this situation will only add more special cases. Please let me know if I can be of any assistance. 
</comment><comment author="rmuir" created="2015-04-06T13:42:13Z" id="90069809">You are correct @ianribas that the code is unapproachable. But we can't give up on it and just let it stagnate. 

A lot of the complexity is because the tokenstream api (used to consume the analysis chain) is awkward to use here (additional state must be kept because it can only be consumed "forward-only'). 

We could consider another approach, such as converting the tokenstream to an automaton (we have a TokenstreamToAutomaton somewhere), and then consuming that. Maybe it would simplify all the code around this thing.

I just want to think about all cases involved first: its not just AND/OR but also impacts the phrase operator. If you have this same situation in quotes, I think we should be using something like @mikemccand 's TermAutomatonQuery. I am not sure what state its in, mike put it in the sandbox i am sure for good reasons, and i'm not sure it supports slop yet.
</comment><comment author="mikemccand" created="2015-04-06T15:20:01Z" id="90101235">TermAutomatonQuery is in sandbox just because it's so new and likely quite slow (missing optos like https://issues.apache.org/jira/browse/LUCENE-6396 that @rmuir just opened), but it can run arbitrary token-level automata (each transition is a token), including ones with cycles I think (which our token streams cannot produce).

But I think the original issue here is not about positional querying but rather about consume the multiple tokens ("spider" and "man") created by the synonym filter yet not doing the right thing when the operator is "and", i.e. the query should effectively rewrite to +spider +man (except the impl in QueryBuilder.createFieldQuery seems to mix these cases)...
</comment><comment author="rmuir" created="2015-04-06T15:26:04Z" id="90103917">&gt; except the impl in QueryBuilder.createFieldQuery seems to mix these cases

That is exactly what it makes it difficult, to fix the non-positional case (AND/OR) and still defer the proximity case until we have better solutions. I think its still possible, but not without adding another specialized case there (e.g. "multiple positions and lengths"). I will investigate this as an intermediate solution, maybe its not so bad.
</comment><comment author="rmuir" created="2015-04-07T02:24:17Z" id="90320026">Here is some progress:
- https://issues.apache.org/jira/browse/LUCENE-6400 SolrSynonymParser (its the syntax used here by ES) doesn't really construct the map correctly. positionsLengths are really not even available today in your situation due to this.  
- https://issues.apache.org/jira/browse/LUCENE-6401 Refactor the big method :)

Even after these, there is more work for synonyms before it can do the right thing in all circumstances. And the position case needs some work on something like TermAutomatonQuery before things in double-quotes will work correctly.

Before changing the logic in QueryBuilder, we need to also consider CJK/shingle/n-grams/commongrams/kuromoji that are setting positionLength and make sure everything makes sense too.
</comment><comment author="atuljangra" created="2016-03-07T08:02:23Z" id="193145894">Any update on this?
I was using Solr 4.7.2, upgraded to 5.5.0. Faced the problem of having multiword synonyms and operators(AND OR etc). See http://stackoverflow.com/questions/35823263/complex-queries-using-multiword-synonyms-in-solr-lucene

I thought I should move to ES because this is where the party is going on. But I'm not sure that multiword synonyms and operators would work well here too after seeing this bug.
Can someone update me on this?
</comment><comment author="clintongormley" created="2017-04-26T11:07:58Z" id="297360484">This now works correctly, if you switch from using the `synonym` token filter to the `synonym_graph` token filter (as a `search_analyzer` - to use `synonym_graph` as an index-time `analyzer`, you also need to add the [`flatten_graph` token filter](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/analysis-flatten-graph-tokenfilter.html)

```
# delete old index if exists
DELETE /multiwordsyns?pretty

# create index with synonym analyzer and mapping
PUT /multiwordsyns?pretty
{
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1,
    "index": {
      "analysis": {
        "analyzer": {
          "index_time": {
            "tokenizer": "standard",
            "filter": [
              "standard",
              "lowercase"
            ]
          },
          "synonym": {
            "tokenizer": "standard",
            "filter": [
              "standard",
              "lowercase",
              "stop",
              "synonym"
            ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "synonyms": [
              "spider man, spiderman"
            ]
          }
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "text": {
          "type": "text",
          "analyzer": "index_time",
          "search_analyzer": "synonym"
        }
      }
    }
  }
}

# index the test documents
PUT /multiwordsyns/test/1?pretty
{"text": "the adventures of spiderman"}

PUT /multiwordsyns/test/2?pretty
{"text": "what hath man wrought?"}

PUT /multiwordsyns/test/3?pretty
{"text": "that spider is the size of a man"}

PUT /multiwordsyns/test/4?pretty&amp;refresh=true
{"text": "spiders eat insects"}

# Works - finds #1 &amp; #3
POST /multiwordsyns/test/_search?pretty
{"query": {"match": {"text": {"query": "spiderman", "operator": "and"}}}}

# Works - finds #1 &amp; #3
POST /multiwordsyns/test/_search?pretty
{"query": {"match": {"text": {"query": "spider man", "operator": "and"}}}}
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Fix snapshot status of version 1.1.2.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10393</link><project id="" key="" /><description>org.elasticsearch.Version mistakenly tags 1.1.2 as a snapshot version while it
has been released.
</description><key id="65926564">10393</key><summary>Internal: Fix snapshot status of version 1.1.2.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2015-04-02T13:36:03Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-07T15:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-05T10:21:46Z" id="89748974">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es 1.5.0 count doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10392</link><project id="" key="" /><description>Here is my script:

```
GET /engzo/topic/_search?pretty=true
{
  "query": {
    "bool": {
      "must": {
        "term": {
          "isDeleted": false
        }
      },
      "should": [
        {
          "match": {
            "title": {
              "query": "test",
              "type": "boolean",
              "operator": "AND"
            }
          }
        },
        {
          "match": {
            "body": {
              "query": "test",
              "type": "boolean",
              "operator": "AND"
            }
          }
        },
        {
          "query_string": {
            "query": "test",
            "fields": [
              "title",
              "body"
            ]
          }
        }
      ]
    }
  }
}
```

and I got this:

```
{
  "took": 13,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 414736,
    "max_score": 14.740633,
    "hits": [
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "9566",
        "_score": 14.740633,
        "_source": {
          "topicId": 9566,
          "title": "test",
          "body": "test",
          "createdAt": "2014-03-03T08:03:26.000+08:00",
          "updatedAt": "2014-03-03T08:03:26.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "106554",
        "_score": 14.740633,
        "_source": {
          "topicId": 106554,
          "title": "test",
          "body": "test",
          "createdAt": "2014-06-10T04:21:46.000+08:00",
          "updatedAt": "2014-07-01T10:04:05.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "369577",
        "_score": 14.740633,
        "_source": {
          "topicId": 369577,
          "title": "Test",
          "body": "Test",
          "createdAt": "2014-11-06T11:38:28.000+08:00",
          "updatedAt": "2014-11-06T13:56:55.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "401588",
        "_score": 14.740633,
        "_source": {
          "topicId": 401588,
          "title": "test",
          "body": "test",
          "createdAt": "2014-11-24T09:11:57.000+08:00",
          "updatedAt": "2014-11-24T09:11:57.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "45838",
        "_score": 14.455909,
        "_source": {
          "topicId": 45838,
          "title": "test",
          "body": "test",
          "createdAt": "2014-04-28T12:49:04.000+08:00",
          "updatedAt": "2014-04-29T05:13:04.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "99753",
        "_score": 14.455909,
        "_source": {
          "topicId": 99753,
          "title": "test",
          "body": "test",
          "createdAt": "2014-06-05T03:30:54.000+08:00",
          "updatedAt": "2014-08-17T05:39:08.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "91185",
        "_score": 14.423123,
        "_source": {
          "topicId": 91185,
          "title": "test",
          "body": "test",
          "createdAt": "2014-05-28T13:41:30.000+08:00",
          "updatedAt": "2014-05-28T13:45:33.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "281450",
        "_score": 14.423123,
        "_source": {
          "topicId": 281450,
          "title": "test",
          "body": "Test",
          "createdAt": "2014-09-04T10:35:50.000+08:00",
          "updatedAt": "2014-09-04T11:42:12.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "35181",
        "_score": 14.368841,
        "_source": {
          "topicId": 35181,
          "title": "test",
          "body": "test",
          "createdAt": "2014-04-20T20:19:43.000+08:00",
          "updatedAt": "2014-04-20T20:19:43.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      },
      {
        "_index": "engzo",
        "_type": "topic",
        "_id": "74362",
        "_score": 14.368841,
        "_source": {
          "topicId": 74362,
          "title": "test",
          "body": "test",
          "createdAt": "2014-05-16T08:48:20.000+08:00",
          "updatedAt": "2014-08-26T06:34:25.000+08:00",
          "replyCounts": 0,
          "starCounts": 0,
          "isDeleted": 0
        }
      }
    ]
  }
}
```

but when I try to get the count by the following script:

```
GET /engzo/topic/_count?pretty=true
{
  "query": {
    "bool": {
      "must": {
        "term": {
          "isDeleted": false
        }
      },
      "should": [
        {
          "match": {
            "title": {
              "query": "test",
              "type": "boolean",
              "operator": "AND"
            }
          }
        },
        {
          "match": {
            "body": {
              "query": "test",
              "type": "boolean",
              "operator": "AND"
            }
          }
        },
        {
          "query_string": {
            "query": "test",
            "fields": [
              "title",
              "body"
            ]
          }
        }
      ]
    }
  }
}
```

it said:

```
GET /
{
  "count": 414736,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  }
}
```

actually It is the total number of the index type. Is this a bug?
</description><key id="65914575">10392</key><summary>es 1.5.0 count doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffreyji666</reporter><labels /><created>2015-04-02T12:21:30Z</created><updated>2015-04-06T04:31:08Z</updated><resolved>2015-04-02T12:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-02T12:27:30Z" id="88882799">The `count` matches the `hits.total` value returned from your search request.  So apparently that is the number of documents that your query matched.  The search request just returns the first 10 results by default.
</comment><comment author="jeffreyji666" created="2015-04-02T12:31:31Z" id="88884022">so How could I get the count that meets up the search condition?
</comment><comment author="clintongormley" created="2015-04-02T18:27:49Z" id="89001507">@jeffreyji666 search defaults to a `size` of 10, so if the count is greater than 10, use 10 instead...

Not sure why you would want to do that though
</comment><comment author="jeffreyji666" created="2015-04-03T01:55:59Z" id="89117757">I mean I want to get the number of the record which meets the search condition, so I can do the pagination.
</comment><comment author="clintongormley" created="2015-04-05T17:55:56Z" id="89820798">@jeffreyji666 that's what the `count` represents.  What do you think is wrong?

btw, deep pagination is problematic in a distributed system.  there's a good reason that google only returns 100 pages of results.  
</comment><comment author="jeffreyji666" created="2015-04-06T04:31:08Z" id="89915266">I got it wrong. I will close this issue. thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `fuzzy_like_this` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10391</link><project id="" key="" /><description>The fuzzy-like-this query builds very expensive queries and only serves esoteric
use-cases.
</description><key id="65894184">10391</key><summary>Remove `fuzzy_like_this` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-04-02T09:52:52Z</created><updated>2015-06-06T16:08:55Z</updated><resolved>2015-04-10T15:17:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T09:53:10Z" id="88852897">The plan is to mark it as deprecated in 1.x.
</comment><comment author="s1monw" created="2015-04-02T09:56:20Z" id="88853320">+1
</comment><comment author="s1monw" created="2015-04-02T10:43:52Z" id="88863482">LGTM yet the only thing that I am always wondering about is what are we doing if somebody has this thing stuck in the the translog as a delete by query? It's not sovleable to be honest unless we say `f*** it`
</comment><comment author="jpountz" created="2015-04-02T12:52:57Z" id="88886906">It's a valid concern. However I don't think this query is used much for delete by query, so few users would be impacted, and if they are it would still be possible to downgrade, restore a snapshot, flush and upgrade again? I'm personally more concerned about the percolator, but this is what it is: if we can't have such breaks on rarely used queries then we have no opportunities to clean things up?
</comment><comment author="markharwood" created="2015-04-13T12:55:05Z" id="92341733">&gt; only serves esoteric use-cases.

One non-esoteric use case is as a work-around to Lucene's [fuzzy bug](https://issues.apache.org/jira/browse/LUCENE-329) which I never managed to get acknowledged as an issue.
This [came up recently](https://groups.google.com/d/msg/elasticsearch/dLdT90j1x74/zqJQiSlgHv8J) with one of our users.

Agreed FLT is not a long-term solution but it was always a work-around I'd reach for when hitting this issue with Lucene.

Maybe the removal of this functionality should coincide with a proper fix for core Lucene?
</comment><comment author="clintongormley" created="2015-04-13T12:58:38Z" id="92342314">@markharwood well volunteered!
</comment><comment author="markharwood" created="2015-05-01T11:20:09Z" id="98104620">I've taken the guts of our BlendedTermQuery and working it back into Lucene's FuzzyQuery rewrite capabilities.
The use case is not exactly the same - BTQ includes important tie-breaker logic to avoid "favouring the wrong field" when querying &gt;1 field.
FuzzyQuery is single-field only and so we don't have to give terms different DFs - they can all share the same DF and therefore be ranked internally purely on similarity (via the TQ boosts for edit distance). The only DF manipulation required is that they are all rewarded with the highest-DF of any term in the set as this represents the "most likely" interpretation of what all these high-similarity terms are hoped to represent. 

I toyed with the idea of retaining some sense of the original terms' DF to blend popularity with similarity scores (to help tune out all those pesky typos). There are 2 issues with this:
1) It would be hard to blend edit-distance similarity with notions of popularity in a way that scores well for all cases.
2) It happens too late - I think FQ's term-selection process is pure edit-distance-based and so may have already thrown away popular terms in favour of similar one-off typos by the time rewrite methods get called to tweak any DF settings.
I think the right answer here is to a add a minDF setting as a filter to FuzzyQuery's term-selection logic to remove the typos.
</comment><comment author="cedric-marcone" created="2015-05-07T13:36:20Z" id="99867835">At least, in the documentation, you should provide some migration tips for the users in the "esoteric" clan.
</comment><comment author="jeantil" created="2015-05-07T18:52:19Z" id="99977538">We stumbled on the thread mentionned by @markharwood a while ago when encountering the exact same issue. We also decided to use FLT to work around #9103. 
We have been monitoring #9103 ever since we went with this workaround, as far as I can tell it is not even considered for inclusion in 2.0

My current understanding is that removing FLT without fixing #9103 would effectively render serious uses of fuzzy queries impossible as they would favor very rare fuzzy matches way over frequent exact matches. Unfortunately we lack the detailed knowledge to be able to contribute a pull request.
</comment><comment author="clintongormley" created="2015-05-08T07:52:23Z" id="100139443">@markharwood would you be able to look at #9103?  
</comment><comment author="jeantil" created="2015-05-11T09:40:39Z" id="100834058">#9103 is dependant on [LUCENE-329](https://issues.apache.org/jira/browse/LUCENE-329), hopefully the patch will be accepted soon and from what i can tell, it would solve our problem. Do you think it would be possible to keep ftl around until LUCENE-329 is closed and elasticsearch has upgraded ?
</comment><comment author="jeantil" created="2015-05-26T09:39:38Z" id="105466848">@markharwood @clintongormley Looks like [LUCENE-329](https://issues.apache.org/jira/browse/LUCENE-329) has been merged into Lucene 5.x which should improve things. 

In the mean time we have been trying to replace FLT with a multi-match fuzzy query (ignoring relevance until the Lucene move and #9103 are available in a release). 

We query on 6 fields in a index with ~4000 documents, with queries ranging from 2 to 5 terms
On the same machine on the same index, we see a **massive** performance drop.

Given a query of 3 tokens:"audi a 3"
- With FLT the results are consistently delivered in less than 150 ms 
- With multi-match fuzzy results are consistently delivered in over 10 seconds !! (including when repeating that same query over and over)

If we move to a slightly bigger query with 4 terms of avout 6 letters each, the time scale moves to minutes instead of seconds for the multi-match while the FLT remains in the milliseconds.

best_fields and most_fields both suffer of the performance issue. cross_fields and phrase bring the performances back to about what FLT was (we yet have to study what the relevance implications are as this is much harder to analyse)

Is there a guide or something in the documentation  (or planned to be written) on how to properly configure multi-match fuzzy to get results comparable to FLT ? is it even possible ? 
</comment><comment author="clintongormley" created="2015-05-26T09:52:10Z" id="105472244">@jeantil 

&gt; cross_fields and phrase bring the performances back to about what FLT was (we yet have to study what the relevance implications are as this is much harder to analyse)

These types don't use fuzziness, which explains the difference :)

&gt; Given a query of 3 tokens:"audi a 3"
&gt; - With FLT the results are consistently delivered in less than 150 ms
&gt; - With multi-match fuzzy results are consistently delivered in over 10 seconds !! (including when repeating that same query over and over)

That's crazy - I'm really surprised you're seeing performance like this.  FLT uses the old-style fuzzy queries (see annotation "L" and before here http://people.apache.org/~mikemccand/lucenebench/Fuzzy2.html) while multi-match uses the new FST based fuzzy queries (see http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html).

Two questions:
- are you using `"fuzziness": "AUTO"` (which takes word length into account)
- FSTs use memory so do you have enough heap space available?
</comment><comment author="clintongormley" created="2015-05-26T09:54:55Z" id="105473516">perhaps @markharwood can shed some light on this?
</comment><comment author="markharwood" created="2015-05-26T10:03:31Z" id="105475691">The number of terms you search on (regardless of how they are scored/derived) has a direct impact on performance as they each require a disk seek to get at the postings list. It may be that FLT vs other forms of Fuzzy are just generating different volumes of search terms in these cases?
We'd need example queries and possibly data to dig deeper.
</comment><comment author="markharwood" created="2015-05-26T10:45:38Z" id="105485395">I tried various multi_match and fuzzy settings on wikipedia index with fields containing &gt;10m unique values and couldn't get anything approaching 10 second response times, let alone minutes. This was on SSD.

The question about heap space seems like a good next step.
</comment><comment author="jeantil" created="2015-05-26T10:46:58Z" id="105485562">@clintongormley yes we use `"fuzziness": "AUTO"` in both cases. Both tests are executed with the same max heap (1GB as it is) I can increase the max heap but ulitmately wouldn't that mean that on the same hardware our scalability will be severely reduced ? 

@markharwood 
On a index with 3561 structured documents representing consumer cars hosted on a SSD laptop in an ES instance with Xmx1g.
I won't be able to be much more precise in a public discussion as I am bound by non disclosure agreements, but we can arrange for private conversation if needed.

The query for `Audi A3 Sportback Quattro` using FLT, `"took": 121` and looks like 

```
GET vehicle/cartype/_search
{
  "size" : 100,
  "query" : {
    "bool" : {
      "should" : [ {
        "function_score" : {
          "query" : {
            "flt" : {
              "fields" : [ "field1_search", "field1_keywords", "field2_search", "field2_keywords", "field3_search", "field3_keywords" ],
              "like_text" : "Audi A3 Sportback Quattro",
              "fuzziness" : "AUTO"
            }
          },
          "functions" : [ {
            "field_value_factor" : {
              "field" : "preferred"
            }
          } ]
        }
      }, {
        "function_score" : {
          "query" : {
            "multi_match" : {
              "query" : "Audi A3 Sportback Quattro",
              "fields" : [ "field1_search", "field1_keywords", "field2_search", "field2_keywords", "field3_search", "field3_keywords" ],
              "boost" : 3.0
            }
          },
          "functions" : [ {
            "field_value_factor" : {
              "field" : "preferred"
            }
          } ]
        }
      } ]
    }
  },
  "highlight" : {
    "pre_tags" : [ "{" ],
    "post_tags" : [ "}" ],
    "require_field_match" : false,
    "fields" : {
      "field1_search" : { },
      "field1_keywords" : { },
      "field2_search" : { },
      "field2_keywords" : { },
      "field3_search" : { },
      "field3_keywords" : { }
    }
  }
}
```

The same query using multi-match fuzzy, `"took": 94032` and looks like 

```
GET vehicle/cartype/_search
{
  "size" : 100,
  "query" : {
    "bool" : {
      "should" : [ {       
            "multi_match" : {
              "fields" : [ "field1_search", "field1_keywords", "field2_search", "field2_keywords", "field3_search", "field3_keywords" ],
              "query" : "Audi A3 Sportback Quattro",
              "fuzziness" : "AUTO"
            }
      }, {
            "multi_match" : {
              "query" : "Audi A3 Sportback Quattro",
              "fields" : [ "field1_search", "field1_keywords", "field2_search", "field2_keywords", "field3_search", "field3_keywords" ],
              "boost" : 3.0
            }
      } ]
    }
  },
  "highlight" : {
    "pre_tags" : [ "{" ],
    "post_tags" : [ "}" ],
    "require_field_match" : false,
    "fields" : {
      "field1_search" : { },
      "field1_keywords" : { },
      "field2_search" : { },
      "field2_keywords" : { },
      "field3_search" : { },
      "field3_keywords" : { }
    }
  }
}
```
</comment><comment author="jeantil" created="2015-05-26T10:57:59Z" id="105488046">@markharwood  Boosting the heap to Xmx4g 

```
 -Xms512m -Xmx4g -Djava.awt.headless=true -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -Dfile.encoding=UTF-8 -Xss200000 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/local/Cellar/elasticsearch/1.5.1 -Des.config=/usr/local/Cellar/elasticsearch/1.5.1/config/elasticsearch.yml
```

On the same query, repeated a few times just to be sure, I get `"took": 70884`. It is a bit better but nowhere near what we used to get with FLT ... 

Forcing the instance to run that same query 5-6 times in // makes ES consume 90% of my quadcore CPU  where FLT never rose above backgound CPU usage.
</comment><comment author="clintongormley" created="2015-05-26T11:23:41Z" id="105493279">@jeantil are there any messages about slow GC in your logs? Or anything else that is interesting?  how much free space do you have on your heap after the fuzzy queries?  And could you run `GET /_nodes/hot_threads` while running the queries?
</comment><comment author="jeantil" created="2015-05-26T11:30:37Z" id="105494282">@clintongormley 

I don't get anything in the logs (homebrew's default configuration seems to log at INFO)  I restarted my elastic search at 10:28 (GMT) to apply the 4GB Xmx 

```
[2015-05-26 10:28:04,530][INFO ][http                     ] [Helio] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.4.202:9200]}
[2015-05-26 10:28:04,604][INFO ][node                     ] [Helio] started
[2015-05-26 10:28:30,216][INFO ][gateway                  ] [Helio] recovered [79] indices into cluster_state
[2015-05-26 10:28:30,318][INFO ][cluster.metadata         ] [Helio] [.marvel-2015.05.26] creating index, cause [auto(bulk api)], templates [marvel], shards [1]/[1], mappings [_default_, shard_event, index_event, index_stats, node_event, routing_event, cluster_event, cluster_state, cluster_stats, node_stats, indices_stats]
[2015-05-26 10:29:13,719][ERROR][marvel.agent.exporter    ] [Helio] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.05.26/_bulk]: SocketTimeoutException[Read timed out]
[2015-05-26 10:29:35,360][DEBUG][action.bulk              ] [Helio] observer timed out. notifying listener. timeout setting [1m], time since start [1m]
[2015-05-26 10:30:14,312][ERROR][marvel.agent.exporter    ] [Helio] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.05.26/_bulk]: SocketTimeoutException[Read timed out]
[2015-05-26 10:30:14,710][DEBUG][action.bulk              ] [Helio] observer timed out. notifying listener. timeout setting [1m], time since start [1m]
[2015-05-26 10:30:25,778][INFO ][cluster.metadata         ] [Helio] [.marvel-2015.05.26] update_mapping [index_stats] (dynamic)
[2015-05-26 10:30:26,135][INFO ][cluster.metadata         ] [Helio] [.marvel-2015.05.26] update_mapping [indices_stats] (dynamic)
[2015-05-26 10:30:28,859][INFO ][cluster.metadata         ] [Helio] [.marvel-2015.05.26] update_mapping [cluster_stats] (dynamic)
```

Here is the GET /_nodes/hot_threads while running one multimatch fuzzy querry 

```
::: [Halloween Jack][DVCm9wTyQWO0m-zus6thUQ][byjean][inet[192.168.4.214/192.168.4.214:9300]]
   Hot threads at 2015-05-26T11:25:10.499Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   113.4% (567.1ms out of 500ms) cpu usage by thread 'elasticsearch[Halloween Jack][search][T#3]'
     2/10 snapshots sharing following 38 elements
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:40)
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:40)
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:40)
       org.apache.lucene.util.InPlaceMergeSorter.sort(InPlaceMergeSorter.java:32)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:734)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 19 elements
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 34 elements
       org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:152)
       org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:135)
       org.apache.lucene.util.automaton.LevenshteinAutomata.toAutomaton(LevenshteinAutomata.java:199)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:174)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 39 elements
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:41)
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:40)
       org.apache.lucene.util.InPlaceMergeSorter.mergeSort(InPlaceMergeSorter.java:41)
       org.apache.lucene.util.InPlaceMergeSorter.sort(InPlaceMergeSorter.java:32)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:734)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:152)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:737)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```

Here is the GET /_nodes/hot_threads while running 6 multimatch fuzzy querry 

```
::: [Halloween Jack][DVCm9wTyQWO0m-zus6thUQ][byjean][inet[192.168.4.214/192.168.4.214:9300]]
   Hot threads at 2015-05-26T11:29:24.436Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   113.3% (566.5ms out of 500ms) cpu usage by thread 'elasticsearch[Halloween Jack][search][T#15]'
     2/10 snapshots sharing following 36 elements
       org.apache.lucene.util.InPlaceMergeSorter.sort(InPlaceMergeSorter.java:32)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:734)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 30 elements
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 37 elements
       java.util.HashMap.put(HashMap.java:611)
       java.util.HashSet.add(HashSet.java:219)
       org.apache.lucene.util.automaton.Automaton.getStartPoints(Automaton.java:575)
       org.apache.lucene.util.automaton.RunAutomaton.&lt;init&gt;(RunAutomaton.java:140)
       org.apache.lucene.util.automaton.ByteRunAutomaton.&lt;init&gt;(ByteRunAutomaton.java:32)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:203)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 35 elements
       org.apache.lucene.util.InPlaceMergeSorter.sort(InPlaceMergeSorter.java:32)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:734)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   112.8% (563.8ms out of 500ms) cpu usage by thread 'elasticsearch[Halloween Jack][search][T#11]'
     4/10 snapshots sharing following 34 elements
       org.apache.lucene.util.automaton.ByteRunAutomaton.&lt;init&gt;(ByteRunAutomaton.java:32)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:203)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 35 elements
       org.apache.lucene.util.InPlaceMergeSorter.sort(InPlaceMergeSorter.java:32)
       org.apache.lucene.util.automaton.Automaton$Builder.finish(Automaton.java:734)
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 14 elements
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   112.7% (563.4ms out of 500ms) cpu usage by thread 'elasticsearch[Halloween Jack][search][T#6]'
     2/10 snapshots sharing following 32 elements
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 33 elements
       org.apache.lucene.util.automaton.UTF32ToUTF8.convert(UTF32ToUTF8.java:311)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:197)
       org.apache.lucene.util.automaton.CompiledAutomaton.&lt;init&gt;(CompiledAutomaton.java:104)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:176)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 34 elements
       org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:152)
       org.apache.lucene.util.automaton.Automaton.addTransition(Automaton.java:135)
       org.apache.lucene.util.automaton.LevenshteinAutomata.toAutomaton(LevenshteinAutomata.java:199)
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:174)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:155)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99)
       org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 30 elements
       org.apache.lucene.search.FuzzyTermsEnum.initAutomata(FuzzyTermsEnum.java:174)
       org.apache.lucene.search.FuzzyTermsEnum.getAutomatonEnum(FuzzyTermsEnum.java:152)
       org.apache.lucene.search.FuzzyTermsEnum.maxEditDistanceChanged(FuzzyTermsEnum.java:211)
       org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:205)
       org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:143)
       org.apache.lucene.search.FuzzyQuery.getTermsEnum(FuzzyQuery.java:155)
       org.apache.lucene.search.MultiTermQuery$RewriteMethod.getTermsEnum(MultiTermQuery.java:76)
       org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:64)
       org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:67)
       org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.apache.lucene.search.DisjunctionMaxQuery.rewrite(DisjunctionMaxQuery.java:222)
       org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:554)
       org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:97)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217)
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 14 elements
       org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474)
       org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217)
       org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
       org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197)
       org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:118)
       org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:128)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:192)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:386)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
       org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
       org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```

and here is a jmap -heap of the VM while running the aforementionned 6 queries

```
jmap -heap 1298
Attaching to process ID 1298, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.40-b25

using thread-local object allocation.
Garbage-First (G1) GC with 8 thread(s)

Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 4294967296 (4096.0MB)
   NewSize                  = 1363144 (1.2999954223632812MB)
   MaxNewSize               = 2576351232 (2457.0MB)
   OldSize                  = 5452592 (5.1999969482421875MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 1048576 (1.0MB)

Heap Usage:
G1 Heap:
   regions  = 4096
   capacity = 4294967296 (4096.0MB)
   used     = 1371856408 (1308.3042221069336MB)
   free     = 2923110888 (2787.6957778930664MB)
   31.94102104753256% used
G1 Young Generation:
Eden Space:
   regions  = 560
   capacity = 2275409920 (2170.0MB)
   used     = 587202560 (560.0MB)
   free     = 1688207360 (1610.0MB)
   25.806451612903224% used
Survivor Space:
   regions  = 5
   capacity = 5242880 (5.0MB)
   used     = 5242880 (5.0MB)
   free     = 0 (0.0MB)
   100.0% used
G1 Old Generation:
   regions  = 745
   capacity = 1340080128 (1278.0MB)
   used     = 779410968 (743.3042221069336MB)
   free     = 560669160 (534.6957778930664MB)
   58.16151972667712% used

16084 interned Strings occupying 2325440 bytes.
```
</comment><comment author="clintongormley" created="2015-05-26T11:33:53Z" id="105494667">Hmm could you compare the results without G1GC, and (separately) without highlighting?
</comment><comment author="jeantil" created="2015-05-26T11:56:35Z" id="105499093">Reverting G1GC doesn't improve things (meaning the time scale is still the minute, the exact timings )
here is the corresponding jmap when running 6 queries (warmed up by running the query twice before)

```
jmap -heap 2689
Attaching to process ID 2689, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.40-b25

using parallel threads in the new generation.
using thread-local object allocation.
Concurrent Mark-Sweep GC

Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 4294967296 (4096.0MB)
   NewSize                  = 178913280 (170.625MB)
   MaxNewSize               = 697892864 (665.5625MB)
   OldSize                  = 357957632 (341.375MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 0 (0.0MB)

Heap Usage:
New Generation (Eden + 1 Survivor Space):
   capacity = 161021952 (153.5625MB)
   used     = 141274096 (134.72947692871094MB)
   free     = 19747856 (18.833023071289062MB)
   87.73592311189968% used
Eden Space:
   capacity = 143130624 (136.5MB)
   used     = 140167176 (133.67383575439453MB)
   free     = 2963448 (2.8261642456054688MB)
   97.92955000321943% used
From Space:
   capacity = 17891328 (17.0625MB)
   used     = 1106920 (1.0556411743164062MB)
   free     = 16784408 (16.006858825683594MB)
   6.186907981341575% used
To Space:
   capacity = 17891328 (17.0625MB)
   used     = 0 (0.0MB)
   free     = 17891328 (17.0625MB)
   0.0% used
concurrent mark-sweep generation:
   capacity = 1103278080 (1052.16796875MB)
   used     = 4068346332679738616 (3.8798774077222236E12MB)
   free     = 13712308637745 MB
   3.6875076251671185E11% used

15542 interned Strings occupying 2273448 bytes.
```
</comment><comment author="markharwood" created="2015-05-26T11:57:52Z" id="105499536">&gt; On a index with 3561 structured documents

Given the tiny number of documents and the time taken to query I wonder if you have some documents with massive numbers of unique terms? Try the `cardinality` aggregation on the various fields using a match_all query to get some sense of number of unique terms in your index
</comment><comment author="jeantil" created="2015-05-26T12:03:58Z" id="105500338">@clintongormley good call : the highlighting seems to be the culprit. removing the highlighting query time drops back to the 100 milliseconds scale. This is a big problem for us though as we use highlighting extensively to further process the results.

@markharwood 
cardinalities :
field1_search : 1631
field1_keywords : 3401
field2_search : 950
field2_keywords : 983
field3_search : 93
field3_keywords : 88
</comment><comment author="clintongormley" created="2015-05-26T12:08:29Z" id="105501257">@jeantil good to know. which highlighter are you using?
</comment><comment author="markharwood" created="2015-05-26T12:12:14Z" id="105502177">Stack trace says PlainHighlighter
</comment><comment author="jeantil" created="2015-05-26T12:13:27Z" id="105502348">I was searching and it seems we haven't configured anything specific pertaining to the highlighter. I guess `plain` is the default one.
</comment><comment author="clintongormley" created="2015-05-26T12:13:37Z" id="105502385">I wonder if it is worth trying the postings highlighter or the FVH?  Just to see if it makes a difference?
</comment><comment author="clintongormley" created="2015-05-26T12:29:53Z" id="105509065">@jeantil i think the other big difference is that FLT limits the number of fuzzy terms quite drastically.  Try using `max_expansions` with the multi-match query and see if that helps with the highlighting too.

In the current implementation, it'd just be the first `max_expansions` terms that get added, but in ES 2, it'll be the top-scoring `max_expansions` terms  (if i read the patch correctly)
</comment><comment author="jeantil" created="2015-05-26T12:32:04Z" id="105509377">what would be the value of  max_expansions equivalent to FLT ? (just as a starting point for our investigation) 

(i am reindexing with index_options: offsets to see if the postings higlighter behaves differently) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: Add note about not having sources in repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10390</link><project id="" key="" /><description>also fixed some minor things, like upper/lowercase and space between colons
</description><key id="65888021">10390</key><summary>Documentation: Add note about not having sources in repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>docs</label></labels><created>2015-04-02T09:29:37Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-05T16:10:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-02T10:44:26Z" id="88863543">LGTM
</comment><comment author="clintongormley" created="2015-04-05T16:10:37Z" id="89797992">thanks @spinscale - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make APIs work per-segment like Lucene's Collector.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10389</link><project id="" key="" /><description>We still have a lot of APIs that use setNextReader in order to change the
current segment that should be considered. This commit moves such APIs to
getLeafXXX() instead to be more in-line with Lucene 5's collector API.

I also renamed setDocId to setDocument to be more in-line with the doc values
APIs.
</description><key id="65884442">10389</key><summary>Make APIs work per-segment like Lucene's Collector.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-04-02T09:03:59Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-07T16:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T09:15:35Z" id="88837466">Hmm, I had not realized this PR was getting so large... If you need any guidance for the reviews please let me know. There is one place where the refactoring was not straightforward: `ExpressionScript`. The reason is that if the script references the score, you need a Scorer object to be in the context, but with this new API, the scorer can only be set AFTER the per-segment script has been created. So what I did is that I re-instanciate the values in setScorer. /cc  @rjernst 

For the record, this PR will break existing script plugins as it splits SearchScript into SearchScript and LeafSearchScript, the former being only responsible for creating per-leaf LeafSearchScript objects. I will fix them when this PR gets in. /cc @dadoonet 
</comment><comment author="dadoonet" created="2015-04-02T14:56:37Z" id="88935744">Thank you so much @jpountz! 
</comment><comment author="jpountz" created="2015-04-03T08:22:02Z" id="89216875">@rjernst FYI I assigned you for the review like we discussed.
</comment><comment author="rjernst" created="2015-04-04T01:55:37Z" id="89482099">LGTM, just a couple minor comments.
</comment><comment author="jpountz" created="2015-04-07T15:49:41Z" id="90616902">I pushed a new commit that addresses comments. Will merge soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure size=0 works on the inner_hits level.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10388</link><project id="" key="" /><description>PR for #10358
</description><key id="65862089">10388</key><summary>Make sure size=0 works on the inner_hits level.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-02T07:20:09Z</created><updated>2015-05-29T16:27:18Z</updated><resolved>2015-04-02T13:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T07:23:51Z" id="88788277">Left one minor comment, otherwise LGTM. Feel free to push without further review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update health.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10387</link><project id="" key="" /><description>Added documentation for 'wait_for_active_shards'
</description><key id="65861258">10387</key><summary>Update health.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">wittyameta</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-02T07:14:40Z</created><updated>2015-04-02T08:49:12Z</updated><resolved>2015-04-02T07:34:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-02T07:34:46Z" id="88794411">Thanks @wittyameta merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtering aggregation values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10386</link><project id="" key="" /><description>Hi,

in our project we need to filter the values of an aggregation. A possible solution is to use the filter param:

GET /sample/document/_search
{
  "aggs": {
    "value_facet": {
      "terms": {
        "field": "value_id",
        "include": "a|b"
      }}}}

I think a better way would be, to use a value param, like this:

GET /sample/document/_search
{
  "aggs": {
    "value_facet": {
      "terms": {
        "field": "value_id",
        "values": ["a", "b"]
      }}}}

to restrict the values. Can this be implemented or is there another possibility to implemented this?

Greetings,
Martin
</description><key id="65847042">10386</key><summary>Filtering aggregation values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">krueger</reporter><labels /><created>2015-04-02T06:00:41Z</created><updated>2015-04-02T16:02:17Z</updated><resolved>2015-04-02T13:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T13:15:51Z" id="88892322">This is already supposed to work: if you supply an array of strings it will be interpreted as the list of values you are interested in but if you supply a string it will be interpreted as a regexp. The documentation mentions this but I agree it would be nice to make things clearer by also giving an example (PRs welcome ;-)).
</comment><comment author="krueger" created="2015-04-02T16:02:17Z" id="88958997">Ok, thx for you answer. I used an old version of es, with the latest it works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Please support vietnamese language mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10385</link><project id="" key="" /><description>Please add vietnamese to list of support language analyzer
</description><key id="65834038">10385</key><summary>Please support vietnamese language mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jindov</reporter><labels /><created>2015-04-02T04:28:30Z</created><updated>2015-04-05T14:56:59Z</updated><resolved>2015-04-05T14:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T14:56:59Z" id="89788875">Duplicate of #6647
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Will allow_primary delete existing shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10384</link><project id="" key="" /><description>Hi
we had a outage on out es cluster, so we stop all incomming traffic.
But there are some unassigned shards hanging around. 

Will turning allow_primary on delete the existing shards?
</description><key id="65828290">10384</key><summary>Will allow_primary delete existing shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yangou</reporter><labels /><created>2015-04-02T03:33:02Z</created><updated>2015-04-02T03:34:17Z</updated><resolved>2015-04-02T03:34:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-04-02T03:34:17Z" id="88715378">These sorts of questions are best for the google group - https://groups.google.com/forum/#!forum/elasticsearch

Github is reserved for code bugs and changes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a way to drop norms for already optimized/older indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10383</link><project id="" key="" /><description>Here is a use case:
- There are a lot of optimized/older indices that are not getting written to (and no further deletions) but all of them have a great number of fields with norms enabled
- The user realized that they don't really need norms for these fields after the fact.

In order for norms to be dropped, a segment merge has to occur.  Currently, in order to force a segment merge for these indices (with just 1 segment), we can modify the index mapping to disable norms for these indices, add a dummy document with these fields (now with norms disabled), and optimize again.

It will be nice to provide a way to drop norms for these optimized indices without having to ingest a dummy document.  I think there was an attempt to provide a force option (https://github.com/elastic/elasticsearch/pull/5294) but was subsequently deprecated in 1.4 (http://www.elastic.co/guide/en/elasticsearch/reference/1.4/indices-optimize.html)
</description><key id="65805532">10383</key><summary>Provide a way to drop norms for already optimized/older indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>enhancement</label></labels><created>2015-04-02T00:23:29Z</created><updated>2016-01-15T00:28:16Z</updated><resolved>2016-01-14T14:16:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-02T00:43:59Z" id="88675801">I agree, this is too tricky and we should make it easy and not error-prone.

There are a couple possibilities (or combination of) we could use here with 2.0:
- ElasticSearchMergePolicy could check mappings and apply omitNorms to the FieldInfos it presents at merge, so there need not be any dummy document. It means the setting would kick in for a corresponding forceMerge, and also kick in even when indexing is happening, for fields that are "no longer being indexed to" .
- ElasticSearchDirectoryReader could also apply the same FilterCodecReader at read time when segments have norms but mappings has them turned off. This means you could issue a close/open index and the norms are disabled instantly (they are still on disk, but will not be loaded into RAM).
- We could try to fix/add back the force option in some way.
- (other simpler or more complicated solutions).
</comment><comment author="mikemccand" created="2015-04-02T14:20:41Z" id="88924797">+1

I like the ESDirectoryReader solution?  Then, you don't even need merges to run to make the norms effectively go away?  (They would still be on disk, but that's OK: the real horror happens when they are loaded into RAM).
</comment><comment author="jpountz" created="2015-04-02T14:23:25Z" id="88926525">+1 this sounds like a nice solution to me
</comment><comment author="jpountz" created="2015-08-26T19:17:25Z" id="135144272">Norms are going to be disk-based as of Lucene 5.3 (probably Elasticsearch 2.1) soI'm wondering that being able to drop norms is not that useful anymore, and probably not worth the complexity?
</comment><comment author="clintongormley" created="2016-01-14T14:16:04Z" id="171654661">Norms are now disk based. Closing
</comment><comment author="ppf2" created="2016-01-15T00:28:16Z" id="171828417">You guys are awesome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>There is no boolean_detection: false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10382</link><project id="" key="" /><description>There is no option `boolean_detection: false`
Is there way to disallow detection of boolean type?
</description><key id="65801301">10382</key><summary>There is no boolean_detection: false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bogdanovich</reporter><labels /><created>2015-04-01T23:43:53Z</created><updated>2015-04-02T07:15:39Z</updated><resolved>2015-04-02T07:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-02T07:15:39Z" id="88784801">Hi @bogdanovich would you mind asking this question on our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch)? We'd rather use github issues for actual bugs or feature requests. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>recovery state: fix concurrent access to file list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10381</link><project id="" key="" /><description>A shard recovery response might serialize a shard state at the same time that it is
modified by the recovery process. The test
RelocationTests.testMoveShardsWhileRelocation
failed because of this with a ConcurrentModificationException.
</description><key id="65772893">10381</key><summary>recovery state: fix concurrent access to file list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-04-01T20:30:14Z</created><updated>2015-04-02T10:37:57Z</updated><resolved>2015-04-02T10:37:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-01T20:37:44Z" id="88624471">good catch! left one comment about the solution, and a question
</comment><comment author="brwe" created="2015-04-01T21:50:05Z" id="88642490">Thanks for the review! Addressed all comments. I also changed the test now to use the Streamer class that was already there instead of a custom thread. 
</comment><comment author="bleskes" created="2015-04-02T08:41:45Z" id="88827705">LGTM. in my opinion this should go to 1.5.1
</comment><comment author="s1monw" created="2015-04-02T08:52:08Z" id="88832265">&gt; LGTM. in my opinion this should go to 1.5.1

+1
</comment><comment author="brwe" created="2015-04-02T10:35:29Z" id="88862175">This is already fixed on 1.5 and 1.x by 3a9a33ec4c7f4daffe26b4b1dc39f156108fce39 the commit to master (8c695355807be445d0322ecffb28377c1117c7a9) was just missing the synchronized. I will just push the test to 1.5 and 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The cluster keeps increasing unassigned_shards shards?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10380</link><project id="" key="" /><description>We are running ES 1.4.2
Suddenly, we noticed that the cluster state became red last night, although all indexing, query, search are working fine.

The reason why the cluster became red but service is still running fine is that the ES cluster has the unassigned_shards kept increasing. We don't know where those shards are from and which index do those shards belong to?

Are we hitting a but in ES?

Can anyone at least help point out why ES keeps increasing number of shards?

Thanks
</description><key id="65763348">10380</key><summary>The cluster keeps increasing unassigned_shards shards?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yang-bsft</reporter><labels /><created>2015-04-01T19:41:30Z</created><updated>2015-04-02T03:32:52Z</updated><resolved>2015-04-02T03:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yangou" created="2015-04-02T03:31:51Z" id="88714705">It turns out to be our issue. Please help close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a logger that reports a breakdown of the indexing time </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10379</link><project id="" key="" /><description>It will be helpful if we have a logger that can be set to DEBUG or TRACE level that reports a breakdown of the time spent on each step of an indexing request, eg. time spent indexing to primary, time spent indexing to replica, time spent in bulk/index queue (if applicable), etc.. 
</description><key id="65755535">10379</key><summary>Provide a logger that reports a breakdown of the indexing time </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-04-01T19:00:06Z</created><updated>2016-01-15T03:28:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T14:15:29Z" id="171654545">This could be very heavy to have for every indexing request. Would it still be useful if it were implemented like the Profile API?  ie on a per request basis?
</comment><comment author="GlenRSmith" created="2016-01-15T01:03:20Z" id="171833554">Agreed it would be heavy for every indexing request. Moreover, there would be diminishing return of value, I suspect, for profiling query after query after query. Better to be able to _sample_ on a timed or counted basis.

A profile API - or perhaps a keyword arg, could facilitate that. Whatever the indexing agent is could be configured to post to this endpoint on a either the time or count periodic basis.

If you _really_ want to profile every. single. document. then go for it.

Yeah, the API approach puts really nice granularity into the hands of the user.

It could conceivably accept some URL params to indicate desired verbosity or operational granularity (if such a thing is even feasible, breaking down further into the time spent in each analysis phase for that document, or, on another axis, breaking it down per field).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New Index creation time increases with more and more indexes in the cluster.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10378</link><project id="" key="" /><description>I am trying to test index creation performance in the cluster. I noticed as total number of indexes increases in the cluster, creating a new index takes more and more time. (no data loading at all and no other activity in cluster other than new index creations).

First 1000 index created in  approx 10 minutes
Next 1000 index creation took close to 40 minutes and it keeps degrading. Resources on the nodes (master &amp; data)  are not bottleneck.

Thanks
Ajay
</description><key id="65754361">10378</key><summary>New Index creation time increases with more and more indexes in the cluster.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels><label>feedback_needed</label></labels><created>2015-04-01T18:53:39Z</created><updated>2015-09-13T21:36:27Z</updated><resolved>2015-04-26T20:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajaybhatnagar" created="2015-04-01T18:54:46Z" id="88593952">ES version is 1.4.4, java  1.8.0_40
</comment><comment author="jpountz" created="2015-04-02T20:12:07Z" id="89030316">Are you testing it purely for experimental purposes or is index creation time actually a bottleneck for your use-case? In the latter case can you explain why you need to create so many indices?

Managing thousands of indices is something that will raise issues for a variety of reasons, I'm afraid index creation time is only one of the symptoms. If you can, I would advise to model your data in such a way that you don't need so many indices.
</comment><comment author="clintongormley" created="2015-04-26T20:06:18Z" id="96429402">No more info. Closing
</comment><comment author="ubershmekel" created="2015-09-13T01:04:28Z" id="139833077">This may be affecting my project. It took me a week to finish indexing 400 indices and the remaining 100 seem to be taking much longer to create (~5 minutes per index at the start and ~10-30 minutes now). Why does the amount of existing indices affect new index creation? E.g. you wouldn't expect writing to a file to be slower based on the amount of folders on the drive (unless there's fragmentation which seems like it shouldn't start to be a problem at 1K indices).

Degraded search warm-up is perfectly reasonable, but creating the index I expected to perform consistently. Perhaps these limitations can be explicitly mentioned at

https://www.elastic.co/blog/found-sizing-elasticsearch
https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing

Even the great answer here seems oblivious to this limitation http://stackoverflow.com/questions/14465668/elastic-search-multiple-indexes-vs-one-index-and-types-for-different-data-sets
</comment><comment author="dadoonet" created="2015-09-13T06:38:00Z" id="139847036">@ubershmekel join us on discuss.elastic.co 

And provide more info about your setup (nodes, memory...)
</comment><comment author="markwalkom" created="2015-09-13T21:36:27Z" id="139919428">FYI this is also at https://discuss.elastic.co/t/why-are-indices-slower-to-create-the-more-indices-you-have/29205
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Add autocorrelation agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10377</link><project id="" key="" /><description>WIP, putting up for discussion.

Depends on the `SiblingReducer` functionality introduced in @colings86's ["Max Aggregator" PR](https://github.com/elastic/elasticsearch/pull/10250), so any changes in that PR will need to be reflected here.   

No need for a review yet, this is largely just to test the sibling functionality.
#### Autocorrelation

Autocorrelation shows the similarity between a time series and a "lagged" version of itself at different intervals of time.  This can be used to determine if a signal has periodic elements hidden by noise.  If there is a periodic element (repeating every `n` elements), there will be a peak in the Autocorrelation every `n` lags.  This is because the original time series will "line up" with the lagged version and display a high degree of similarity, even in the presence of noise.

As an example, this "Lemmings Population" is a very noisy sine wave with a 30-day period. If you squint hard enough, you can see the sine wave.  The ACF of the series, however, clearly shows periodic elements.  The peaks are spaced ~27 days, which is very close to the actual 30-day period

![screen shot 2015-04-01 at 1 53 43 pm](https://cloud.githubusercontent.com/assets/1224228/6947964/bd57d93e-d878-11e4-94b8-80b82e5f301f.png)
#### Request

ACF is a sibling reducer, which accepts histogram or datehistogram input.

``` json
GET /test/test/_search?search_type=count
{
   "aggs": {
      "my_date_histo": {
         "date_histogram": {
            "field": "timestamp",
            "interval": "day",
            "min_doc_count": 0
         },
         "aggs": {
            "the_sum": {
               "sum": {
                  "field": "price"
               }
            }
         }
      },
      "the_acf": {
         "acf": {
            "bucketsPath": "my_date_histo.the_sum",
            "window" : 50
         }
      }
   }
}
```
##### Parameters
- `bucketsPath`: required
- `window`: size of time series to perform ACF on.  If series is length `n`, the ACF will be performed on `n - window .. n` values.  E.g. the most recent values.  Optional, defaults to `5`
- `zero_mean`: "centers" the ACF by removing the mean from the time series.  Optional, defaults to `true`
- `zero_pad`: pads the input data with zeros, up to the nearest power of two. FFTs are faster on powers of 2, and padding converts the ACF from a circular convolution to a linear convolution.  Linear are more useful for "real world" use-cases. Optional, defaults to `true`
- `normalize`: Divides all ACF values by variance, which normalizes the ACF to roughly -1..1. Optional, defaults to `true`
#### Response

``` json
{
   "took": 14,
   "timed_out": false,
   "_shards": { ... },
   "hits": { ... },
   "aggregations": {
      "my_date_histo": {
         "buckets": [ ... ]
      },
      "the_acf": {
         "values": [
            1,
            0.37343470483005364,
            -0.360763267740012,
            0.17441860465116257,
            0.5277280858676209
         ]
      }
   }
}
```
#### Todo
- Benchmark the padding vs non-padding speed, particularly if power of two matters.  JTransforms can accept any length input, but uses different radix algos which are theoretically slower.  OTOH, padding to power of two could mean more memory usage, and a bigger power-2 FFT might be slower than smaller non-power-2 FFT.  
- Add setting to disable linear convolution correction when using padding?  If you don't care about the correction, it'll save some CPU cycles by eliminating the mask (forward FFT, PSD computation, inverse FFT)
- Settings configuration is a bit wonky, talk to Colin about a better way?
- Talk to Colin about how sibling builders need to extend `AbstractAggregationBuilder`, and the need for `InternalAcfBuilder` which is registered as an aggregation (due to siblings potentially being "top level" aggs).  Unsure if this was the correct approach?
- Validate the agg structure, since this can only accept histo siblings
- More tests.  Tests are hard since the "brute force" approaches are also approximations...so we are comparing approximations against approximations.  Unsure how much we can randomize these tests for that reason.  Needs more thinking.
</description><key id="65750763">10377</key><summary>Aggregations: Add autocorrelation agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">polyfractal</reporter><labels /><created>2015-04-01T18:34:43Z</created><updated>2015-05-26T14:23:40Z</updated><resolved>2015-05-19T14:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-01T21:52:42Z" id="88643663">This is exciting. :) I am not familiar with the theory so please excuse me if my questions are silly.

&gt; window: size of time series to perform ACF on. If series is length n, the ACF will be performed on n - window .. n values. E.g. the most recent values. Optional, defaults to 5

Is there any particular reason to only apply the analysis to the last elements, could we apply it to all histogram buckets by default instead of the last 5 ones?

&gt; Unsure how much we can randomize these tests for that reason. Needs more thinking.

Not sure how applicable it is in your case but in such cases, I tend to like having both randomized tests and only check invariants in the output, and static tests when it comes to assessing that the algorithm actually works. Otherwise it can quickly become a nightmare to debug failures.
</comment><comment author="polyfractal" created="2015-04-01T22:05:49Z" id="88646831">&gt; Is there any particular reason to only apply the analysis to the last elements, could we apply it to all histogram buckets by default instead of the last 5 ones?

No particular reason, I was mostly just thinking about performance (e.g. if you want accidentally ask for an autocorrelation of 100k of points).  Perhaps we should default it to everything, but provide window as an option if you don't want the complete autocorrelation history?

Practically speaking, ACF becomes less useful (I think) the farther back in time you go.  And the higher order lags have more approximation error that accumulates.

&gt; Not sure how applicable it is in your case but in such cases, I tend to like having both randomized tests and only check invariants in the output, and static tests when it comes to assessing that the algorithm actually works. Otherwise it can quickly become a nightmare to debug failures.

Ahh, this makes sense.  I'll see what I can do to split the tests into those two categories.
</comment><comment author="jpountz" created="2015-04-01T22:15:07Z" id="88650273">&gt; Perhaps we should default it to everything, but provide window as an option if you don't want the complete autocorrelation history?

This makes sense to me. Since you mentioned performance, this got me curious: what is the runtime complexity of this reduction and do you know eg. how much time does it take in practice to process N data points?
</comment><comment author="polyfractal" created="2015-04-01T22:54:55Z" id="88656633">&gt; This makes sense to me. Since you mentioned performance, this got me curious: what is the runtime complexity of this reduction and do you know eg. how much time does it take in practice to process N data points?

I have no idea :D  Real benchmarks are on the top of my to-do list...I'm curious to see where this breaks.

Classical radix-2 FFTs have complexity of O(n log n).  I'm not sure what optimizations JTransforms is using, it may be better than that.  JTransform's has some [benchmark results](https://sites.google.com/site/piotrwendykier/software/jtransforms) which claim an FFT on 1m datapoints takes 10ms.  FFT on 23m values takes 700ms.  Timings are a bit slower if you include "construction" of the FFT plan (e.g. when you instantiate the object).

For non-padded ACF: two FFTs, one O(n) loop over the data compute magnitudes, potentially an extra O(n) loop to normalize.  _Note the FFTs will be non radix-2, so may be slower._

For padded ACF: four FFTs, two O(n) loops for magnitudes, and potentially an extra O(n) loop to normalize.

The brute-force, non-FFT ACF functions are O(n&lt;sup&gt;2&lt;/sup&gt;)
</comment><comment author="clintongormley" created="2015-05-25T11:57:32Z" id="105219446">@colings86 why did you close this, was it merged?
</comment><comment author="colings86" created="2015-05-25T12:01:14Z" id="105220046">@clintongormley it auto-closed because the feature/aggs_2_0 got deleted (since it's no longer needed). @polyfractal said it's an old PR anyway and needs to be updated onto the current pipeline aggs so it Ms probably ok to stay closed
</comment><comment author="polyfractal" created="2015-05-26T14:23:30Z" id="105541853">Yep, this needs to be rebased against current master.  I'll resubmit it soonish.

I'll pull tags from this PR so it doesn't confuse anyone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Miscellaneous additional logging and cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10376</link><project id="" key="" /><description>This fixes an issue where this was logged:

```
[node_t1] [test][0] flush with org.elasticsearch.action.admin.indices.flush.FlushRequest@65f6f1e
```

by adding a .toString() method to FlushRequest.

It also changes:

```
creating Index [test], shards [1]/[2]
```

to:

```
creating Index [test], shards [1]/[2s]
```

If shadow replicas are being used.
</description><key id="65742086">10376</key><summary>Miscellaneous additional logging and cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T17:53:12Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-02T10:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-01T21:55:25Z" id="88644366">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update get.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10375</link><project id="" key="" /><description>Updated to not mislead the reader that the data is actually gone when a document is updated. For example if you have 100GB of docs and update each one you'll only be able to access 100GB of the data, but there would theoretically be 200GB of doc data.
</description><key id="65741713">10375</key><summary>Update get.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">djschny</reporter><labels><label>docs</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T17:51:34Z</created><updated>2015-05-27T08:20:48Z</updated><resolved>2015-05-27T08:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="debadair" created="2015-04-01T17:54:02Z" id="88574883">I'd just say "are unreachable", rather than "are treated as unreachable". 
</comment><comment author="djschny" created="2015-04-01T17:59:03Z" id="88576580">Sure, that is more proper, thanks.
</comment><comment author="javanna" created="2015-05-27T08:20:48Z" id="105817254">Merged, sorry for the delay, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: add plugins category to supported scripted ops and related fine-grained settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10374</link><project id="" key="" /><description>Plugins can make use of scripts and expose custom scripting features. Fine-grained settings introduced with #10116 don't support any custom use of scripts, hence plugins are forced to choose between update, mapping, search and aggs. This commit introduces a new generic plugins category that can be used by plugins.

Fine-grained settings apply as well: `script.plugins: off` disables any script type, for any language, only when scripts are used from plugins. `script.engine.groovy.inline.plugins: off` disables scripts when used as part of plugins, only for inline scripts and groovy language.

Relates to #10347
</description><key id="65715947">10374</key><summary>Scripting: add plugins category to supported scripted ops and related fine-grained settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2015-04-01T15:48:18Z</created><updated>2015-04-03T16:56:13Z</updated><resolved>2015-04-02T12:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-02T10:24:49Z" id="88859677">LGTM
</comment><comment author="javanna" created="2015-04-02T10:31:51Z" id="88861692">@uboness can you have a look too please?
</comment><comment author="uboness" created="2015-04-02T11:48:38Z" id="88875257">but if the intention is to be able to extend it later (for plugins to define their own context)... why not change the search context from enum to an opaque string?
</comment><comment author="javanna" created="2015-04-02T12:12:42Z" id="88880323">My goal was restoring backwards compatibility on `ScriptService` and this change allows for just that as a follow-up. I figured we move away from the enum when we add extensibility from plugins? Why now?
</comment><comment author="uboness" created="2015-04-02T12:53:57Z" id="88887084">because if plugins already want to be compatible with 1.6, they should have the chance to define their own context already now. Otherwise, they'll need to start with the "plugin" context and then later on change their code to a string and manage multiple versions compatible with different es versions.... why?

so we now just change it to an opaque string and still have the 5 constants (including the `plugins` one which will be the default when the the context-less version of the methods are used)
</comment><comment author="javanna" created="2015-04-02T12:55:33Z" id="88887701">then why are we doing this in small steps, you are asking for the big change in a single PR, which is not what this is about. Closing.
</comment><comment author="uboness" created="2015-04-02T13:12:47Z" id="88891848">&gt;  you are asking for the big change in a single PR

smaller PR than the one originally introduced the bwc breakage.

&gt; which is not what this is about

this is about bwc... and introducing an option for more bwc issues is very much related.

&gt; Closing.

@javanna you meant to say "Closing in favour of another PR", right?
</comment><comment author="javanna" created="2015-04-02T20:44:42Z" id="89040534">Here is what I meant: 1.6 is not released yet, plugins that depend on 1.5 will have to change their code to adapt if we don't fix the bw comp breakage before releasing 1.6 (`ScriptService` methods signatures require the `ScriptContext` argument). Adding the `plugins` category allows us to revert the bw breakage by using it as default one so that plugins don't have to change anything in their code to adapt to 1.6 when it will be released. 

Maybe I am missing something, but I don't see how adding a value to an existing enum would make things worse, given that the new value will be used transparently as a default, so that plugins don't have to do anything to adapt. I thought we were solving the bw issue with this change.

My understanding was that [#10347](https://github.com/elastic/elasticsearch/issues/10347#issuecomment-88479402) implied two steps: 1) restoring bw comp by introducing the default `plugins` category 2) making the mechanism extensible. This PR was meant to address 1). 

Switching to strings for categories unfortunately implies a bigger change, actually 2) as a whole. In fact  plugins will need to declare upfront which contexts they support as we read and expand settings at startup based on all of the supported categories. We can't just move to strings by itself, we need to support the extensible infra if we do that.
</comment><comment author="uboness" created="2015-04-02T21:57:12Z" id="89057275">&gt; Here is what I meant: 1.6 is not released yet, plugins that depend on 1.5 will have to change their code to adapt if we don't fix the bw comp breakage before releasing 1.6 (ScriptService methods signatures require the ScriptContext argument). Adding the plugins category allows us to revert the bw breakage by using it as default one so that plugins don't have to change anything in their code to adapt to 1.6 when it will be released.

that's great... so the backwards compatibility is fixed. But lets also think about the future compatibilities.

&gt; Maybe I am missing something, but I don't see how adding a value to an existing enum would make things worse, given that the new value will be used transparently as a default, so that plugins don't have to do anything to adapt. I thought we were solving the bw issue with this change.

Do you agree with me that ideally, plugins will register their own context? so if there are multiple plugins, one can allow plugin A to execute scripts and plugin B not to?

if so, you'd want to provide this functionality sooner rather than later. Having the enum in 1.6 will prevent plugins from using it. So why not just enabling it already in 1.6. Maybe I'm missing something here.. I really don't see the issue with it.

&gt; Switching to strings for categories unfortunately implies a bigger change, actually 2) as a whole. In fact plugins will need to declare upfront which contexts they support as we read and expand settings at startup based on all of the supported categories. We can't just move to strings by itself, we need to support the extensible infra if we do that.

Sure.. it requires more work, but IMO it's better done right for 1.6 and for plugins to take advantage of that asap. Also, making the extension point can be achieved by turning the `ScriptContext` to a `ScriptContextRegistry` and expose a method in the `ScriptModule` to register additional contexts. `ScriptModes#buildScriptModeSettingsMap` can accept this registry in addition to the passed in `settings` and and script engines.
</comment><comment author="javanna" created="2015-04-03T06:40:31Z" id="89189537">I see your point @uboness just saying we are not making this change in two steps anymore, hence I closed this PR which was the first step only. I meant to get the second step in 1.6 too anyways, that is probably the reason behind the misunderstanding.
</comment><comment author="uboness" created="2015-04-03T10:30:21Z" id="89249585">OK... It just wasn't obvious to me that a new pr will follow with both changes. Thx @javanna 
</comment><comment author="javanna" created="2015-04-03T16:56:13Z" id="89353367">Superseded by #10419
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to specify a SizeBasedTriggeringPolicy for log configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10373</link><project id="" key="" /><description>Closes #10371
</description><key id="65714299">10373</key><summary>Add ability to specify a SizeBasedTriggeringPolicy for log configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">djschny</reporter><labels><label>:Logging</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T15:40:40Z</created><updated>2015-05-29T17:58:07Z</updated><resolved>2015-04-02T09:17:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-01T21:59:07Z" id="88645154">I am not familiar with log4j but this sounds good to me. I will merge the change tomorrow unless someone objects.
</comment><comment author="jpountz" created="2015-04-02T09:37:06Z" id="88843978">Thanks @djschny ! This will be in Elasticsearch 1.6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ability to specify a SizeBasedTriggeringPolicy for log configuration...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10372</link><project id="" key="" /><description>... #10371
</description><key id="65712940">10372</key><summary>add ability to specify a SizeBasedTriggeringPolicy for log configuration...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2015-04-01T15:34:11Z</created><updated>2015-04-01T15:39:46Z</updated><resolved>2015-04-01T15:39:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add ability to specify a SizeBasedTriggeringPolicy for log configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10371</link><project id="" key="" /><description>Currently only "timeBased" (org.apache.log4j.rolling.TimeBasedRollingPolicy) is allowed. Add ability to specify a sizeBased policy to ensure that disk usage of logs in the event of noisy logs/events.
</description><key id="65710688">10371</key><summary>add ability to specify a SizeBasedTriggeringPolicy for log configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2015-04-01T15:22:03Z</created><updated>2015-04-02T09:17:15Z</updated><resolved>2015-04-02T09:17:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unneccesary mapping refreshes caused by unordered fielddata settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10370</link><project id="" key="" /><description>Fixes #10318

Happy to hear feedback, e.g. suggestions to make the test simpler.
</description><key id="65703572">10370</key><summary>Unneccesary mapping refreshes caused by unordered fielddata settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">EikeDehling</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T14:46:15Z</created><updated>2015-06-07T18:17:44Z</updated><resolved>2015-04-02T20:01:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-02T14:03:08Z" id="88915734">Thx @EikeDehling . I think the fix is good. I made some comments on improving the test. Once that's done I can pull this in.
</comment><comment author="EikeDehling" created="2015-04-02T14:36:02Z" id="88930824">@bleskes Thanks for the tips, i adjusted as you suggested. 
</comment><comment author="bleskes" created="2015-04-02T15:43:20Z" id="88951172">looks good. I'll pull it in. Thx!
</comment><comment author="bleskes" created="2015-04-02T20:07:04Z" id="89028203">Merged. Thx Eike.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to change scripts directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10369</link><project id="" key="" /><description>At the moment the scripts directory is hard coded to `scriptsDirectory = new File(env.configFile(), "scripts");` but it would be handy to make this configurable.
Main reason is that with the Puppet module we allow multiple instances, and it would be handy that all instances on a single node can share the same directory where the scripts are located.
</description><key id="65702103">10369</key><summary>Ability to change scripts directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels /><created>2015-04-01T14:39:24Z</created><updated>2015-04-01T14:44:26Z</updated><resolved>2015-04-01T14:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-01T14:41:12Z" id="88509273">This seems like a duplicate of #6766 ?
</comment><comment author="electrical" created="2015-04-01T14:43:55Z" id="88510276">@javanna oops you are right. must have overlooked it. Closing this one. thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Keep elasticsearch core small</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10368</link><project id="" key="" /><description>Over time we accumulated a lot of features in elasticsearch core which have different status. Some of them are very important building blocks while other ones are only useful for esoteric use-cases or have known issues. Keeping the core small is important for a variety of reasons so maybe we should consider creating a new elasticsearch-extra plugin, with a best-effort only backward compatibility policy, which would contain such esoteric features.

Some ideas of what could move from core to such a plugin:
- the `token_count` field mapper (esoteric)
- the `murmur3` field mapper (not really needed)
- the `fuzzy_like_this` query (super costly + esoteric)
- the `top_children` query (questionable approximation)
- the `significant_terms` aggregation (scalability issues on large data)
- the `scripted_metric` aggregation (useful for experimentation but hard to use and slow)
</description><key id="65689415">10368</key><summary>Keep elasticsearch core small</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-04-01T13:54:52Z</created><updated>2016-01-14T14:13:47Z</updated><resolved>2016-01-14T14:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-01T13:55:18Z" id="88488625">+100 I just read the title :)
</comment><comment author="rmuir" created="2015-04-01T13:56:33Z" id="88489145">+1
</comment><comment author="mikemccand" created="2015-04-01T13:56:59Z" id="88489366">+1
</comment><comment author="kimchy" created="2015-04-01T14:05:58Z" id="88496646">Talking specifically on the list, I think we can simply remove `top_children`  and `fuzzy_like_this`, I don't think they are that helpful and would be a shame to even have a plugin that tries to maintain them.

I think we should keep `sig_terms` in core, its an important aggregation that needs to meet the level of "core".

`token_count` and `murmur` I think its ok to move it to a plugin, I think its interesting to know how many people use the murmur option in conjunction with cardinality, if a lot, then it might be a core level feature?
</comment><comment author="s1monw" created="2015-04-01T14:06:55Z" id="88497204">IMO this is less about what but rather about a direction IMO. just my $0.05 
</comment><comment author="clintongormley" created="2016-01-14T14:13:46Z" id="171654179">These changes have been made. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't reuse source index UUID on restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10367</link><project id="" key="" /><description>Today we reuse the UUID of the source index on restore. This can create conflicts
with existing shard state on disk. This also causes multiple indices with the same
UUID. This commit preserves the UUID of an existing index or creates a new UUID for
a newly created index.
</description><key id="65676106">10367</key><summary>Don't reuse source index UUID on restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T12:45:59Z</created><updated>2015-05-29T17:41:22Z</updated><resolved>2015-04-01T13:56:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-01T12:49:07Z" id="88466893">@imotov can you please review this?
</comment><comment author="imotov" created="2015-04-01T13:45:22Z" id="88486632">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AbstractBlobContainer.deleteByPrefix() should not list all blobs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10366</link><project id="" key="" /><description>The current implementation of AbstractBlobContainer.deleteByPrefix() calls AbstractBlobContainer.deleteBlobsByFilter() which calls BlobContainer.listBlobs() for deleting files, resulting in loading all files in order to delete few of them. This can be improved by calling BlobContainer.listBlobsByPrefix() directly.

This problem happened in #10344 when the repository verification process tries to delete a blob prefixed by "tests-" to ensure that the repository is accessible for the node. When doing so we have the following calling graph: BlobStoreRepository.endVerification() -&gt; BlobContainer.deleteByPrefix() -&gt; AbstractBlobContainer.deleteByPrefix() -&gt; AbstractBlobContainer.deleteBlobsByFilter() -&gt; BlobContainer.listBlobs()... and boom.

Also, AbstractBlobContainer.listBlobsByPrefix() and BlobContainer.deleteBlobsByFilter() can be removed because it has the same drawbacks as AbstractBlobContainer.deleteByPrefix() and also lists all blobs. Listing blobs by prefix can be done at the FsBlobContainer level.

The commit c623d90 will go in 1.5, 1.x and master, the commit 6bcc6e7 in 1.x and master only because it will break plugins.

Related to #10344 
</description><key id="65671953">10366</key><summary>AbstractBlobContainer.deleteByPrefix() should not list all blobs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T12:25:41Z</created><updated>2015-04-09T08:52:13Z</updated><resolved>2015-04-07T09:06:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-01T12:54:05Z" id="88467882">left one comment
</comment><comment author="tlrx" created="2015-04-02T12:19:46Z" id="88881232">@imotov Can you review it please?
</comment><comment author="imotov" created="2015-04-03T01:05:44Z" id="89102319">Left one comment. Otherwise LGTM.
</comment><comment author="s1monw" created="2015-04-07T08:25:49Z" id="90457699">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Add verify parameter to snapshot documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10365</link><project id="" key="" /><description>The `verify` parameter is not documented.
</description><key id="65630424">10365</key><summary>[DOCS] Add verify parameter to snapshot documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>docs</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T08:20:08Z</created><updated>2015-04-02T13:09:14Z</updated><resolved>2015-04-02T12:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-04-01T08:22:29Z" id="88390207">Should we remove it from here then? 

http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository
</comment><comment author="dadoonet" created="2015-04-01T08:24:11Z" id="88391164">Ignore my comment. I think it's OK to have it on both places.
May be a link from the FS part to the section you just updated?
</comment><comment author="tlrx" created="2015-04-01T08:35:24Z" id="88393755">@dadoonet no you're right, the `verify` setting in FS repository must be removed because it's not supported.
</comment><comment author="dadoonet" created="2015-04-01T09:06:02Z" id="88404270">LGTM. +1 to push
</comment><comment author="s1monw" created="2015-04-02T10:26:25Z" id="88860191">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not an Issue, just looking for some valuable feedbacks and help...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10364</link><project id="" key="" /><description>I wish to do the following operation using elasticsearch 1.4 and Kibana 4:
1. Aggregate field x with time for every 10 mins
2. Aggregate field y with time for every 10 mins
3. Take a ratio of the two fields i.e  Ratio (aggregated field x)/(aggregated field y)

Can this be done using scripted scripts in Scripted fields in Kibana 4....  
</description><key id="65628359">10364</key><summary>Not an Issue, just looking for some valuable feedbacks and help...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dev-shubh</reporter><labels /><created>2015-04-01T08:07:44Z</created><updated>2015-04-02T20:13:59Z</updated><resolved>2015-04-02T20:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-02T20:13:59Z" id="89031327">Please use the mailing list to ask questions, we use Github issues only for bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>I wish to do the following operation in Kibana 4:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10363</link><project id="" key="" /><description /><key id="65627432">10363</key><summary>I wish to do the following operation in Kibana 4:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dev-shubh</reporter><labels /><created>2015-04-01T07:59:31Z</created><updated>2015-04-01T07:59:45Z</updated><resolved>2015-04-01T07:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Kibana appears to break-down when I use scripted fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10362</link><project id="" key="" /><description>I am new to elasticsearch and Kibana. I was trying to use scripted fields in Kibana 4. 
![scripted_fileds](https://cloud.githubusercontent.com/assets/11183147/6936697/8dec0ebe-d872-11e4-9dae-731bac392fdc.png)
I tried some very simple arithmetic operations(division) on two fields in scripted fields and it worked just fine. Then I was trying to use aggregations under scripted fields script and my kibana 4 worked working. 
![kibana_error_snap](https://cloud.githubusercontent.com/assets/11183147/6936714/bab90a32-d872-11e4-8869-2c07dbd618d5.png)

Any help would be great. Also let me know if its possible to write script to do aggregation in scripted field script section. 
</description><key id="65627186">10362</key><summary>Kibana appears to break-down when I use scripted fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dev-shubh</reporter><labels /><created>2015-04-01T07:57:12Z</created><updated>2015-04-05T14:38:31Z</updated><resolved>2015-04-05T14:38:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T14:38:31Z" id="89783399">Hi @dev-shubh 

I suggest you ask this on the kibana list instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_ttl counting down faster than expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10361</link><project id="" key="" /><description>Hi,

I believe _ttl is counting down faster than it should.  

I have the following resultset for an index. 

{
_index: some_index
_type: some_type
_id: some_id
_score: 1
fields: {
my_id: some_Id
my_timestamp: [
2015-03-26 16:05:00+0530
]
_ttl: 1581673831
}

Since today is 01/04/15, the document is around 5 days old. However, if we were to check _ttl (1581673831 milliseconds = 18.3 days left). How is that possible? It should be around 25 days to go The index was mapped with the _template mapping given below. I have also given the _mapping that was set up dynamically by Elastic when data was fed into it. Please note that _ttl is set correctly as 2592000000 milliseconds = 30 days as per _template mapping.

For the following template mapping,

 "my_template" : {
    "order" : 1,
    "template" : "my_index*",
    "settings" : {
      "index.mapping.allow_type_wrapper" : "true",
      "index.refresh_interval" : "5s",
      "index.store.compress.stored" : "true",
      "index.cache.field.type" : "soft"
    },
    "mappings" : {
      "_default_" : {
        "_timestamp" : {
          "enabled" : true,
          "path" : "my_timestamp",
          "format" : "YYYY-MM-dd HH:mm:ssZ"
        },
        "_ttl" : {
          "enabled" : true,
          "default" : "30d"
        },
        "properties" : {
          "my_timestamp" : {
            "format" : "YYYY-MM-dd HH:mm:ssZ",
            "type" : "date"
          },
          "my_id" : {
            "index" : "not_analyzed",
            "type" : "string"
          }
        },
        "_all" : {
          "enabled" : false
        }
      }
    },
    "aliases" : { }
  }
}

It has the following index mapping,

 "my_index_2015.03.26" : {
    "mappings" : {
      "mytype1" : {
        "_all" : {
          "enabled" : false
        },
        "_timestamp" : {
          "enabled" : true,
          "path" : "my_timestamp",
          "format" : "YYYY-MM-dd HH:mm:ssZ"
        },
        "_ttl" : {
          "enabled" : true,
          "default" : 2592000000
        },
        "properties" : {
          "my_id" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "my_timestamp" : {
            "type" : "date",
            "format" : "YYYY-MM-dd HH:mm:ssZ"
          }
        }
      }
    }
  }
}
</description><key id="65622530">10361</key><summary>_ttl counting down faster than expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nithyanv</reporter><labels><label>:Mapping</label><label>bug</label><label>discuss</label></labels><created>2015-04-01T07:27:08Z</created><updated>2016-05-12T12:56:09Z</updated><resolved>2016-05-12T12:56:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T14:37:36Z" id="89782997">Hi @nithyanv 

When I try out your examples, the `_ttl` looks correct.  I wonder if you ran an update request on this document?
</comment><comment author="nithyanv" created="2015-04-06T12:48:33Z" id="90048615">We are populating using bulk input. Sometimes, we see overwrites (i.e) version is incremented some of our records may have same _id in the test data. But timestamp is the same even in these records. How does update affect _ttl?
</comment><comment author="clintongormley" created="2015-04-06T18:26:57Z" id="90186418">@nithyanv an `update` request could mess with the TTL, depending on what you set.

Perhaps check the `version` of the document with the bad TTL value? See if it is `1` or higher.  Also try inserting a new document with the same timestamp, and see if it is also reflecting the wrong TTL?

It seems to work for me, so I'm trying to figure out what is different about your setup.
</comment><comment author="nithyanv" created="2015-04-21T11:37:42Z" id="94756497">@clintongormley I believe it is related to Update. We were also getting _version conflict as multiple threads were updating the same record over and over again. That was resolved with setting retryOnconflict(). But now we still get "org.elasticsearch.index.mapper.MapperParsingException: failed to parse [_ttl]" when writing over the same record. This is even though document is well within the ttl.
</comment><comment author="clintongormley" created="2015-06-23T18:18:10Z" id="114596064">Closing in favour of #11809
</comment><comment author="clintongormley" created="2015-06-26T13:25:33Z" id="115683942">I was incorrect - #11809 doesn't fix this issue.  Reopening.

Related to #11802 
</comment><comment author="clintongormley" created="2016-05-12T12:56:09Z" id="218748552">Closing in favour of #18280
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unsafe options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10360</link><project id="" key="" /><description>For quite some time now, our networking layer makes sure to create safe messages as in not using the shared buffers. This is great, and we should remove the old support for "unsafe" notion in our codebase.

Note, this will break java code that provided the unsafe flag, it needs to be removed.
</description><key id="65575958">10360</key><summary>Remove unsafe options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>breaking</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T00:39:15Z</created><updated>2015-05-29T14:59:20Z</updated><resolved>2015-04-01T12:29:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-01T07:20:18Z" id="88376821">LGTM
</comment><comment author="jpountz" created="2015-04-01T07:27:15Z" id="88377792">LGTM
</comment><comment author="s1monw" created="2015-04-01T07:51:16Z" id="88383852">this made my day +1
</comment><comment author="s1monw" created="2015-04-01T12:54:39Z" id="88467983">btw. not sure if we need to do this on `1.6` unless it's simple?
</comment><comment author="kimchy" created="2015-04-01T13:04:34Z" id="88472951">@s1monw looking at it now for 1.6, so far looks like almost a clean merge, will update
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed an equality check in StringFieldMapper.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10359</link><project id="" key="" /><description>The check was ineffective and was causing search_quote_analyzer to be added to the mapping unnecessarily.

Closes #10357
</description><key id="65573418">10359</key><summary>Fixed an equality check in StringFieldMapper.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-04-01T00:16:32Z</created><updated>2015-04-17T20:53:57Z</updated><resolved>2015-04-17T20:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-04-01T07:54:57Z" id="88384444">Hi @jtibshirani thanks a lot for your PR! Would it be possible to have a test along with your fix? Let me know if you need help with that.
</comment><comment author="jtibshirani" created="2015-04-01T21:13:20Z" id="88633387">I took a stab at a test, thanks!
</comment><comment author="javanna" created="2015-04-02T09:00:46Z" id="88835012">@rjernst would you mind taking this over? I had a look and the fix seems ok but the test fails when backporting to 1.x, not sure why, I am not too familiar with mappings code.
</comment><comment author="rjernst" created="2015-04-02T20:12:56Z" id="89030755">This change looks good, I will merge later today.
</comment><comment author="rjernst" created="2015-04-02T20:15:20Z" id="89031928">@javanna I'm sure the test fails because 1.x has different behavior than master for specifying analyzers (how analyzer/index_analyzer/search_analyzer relate to each other).  I'll get it working on the backport.
</comment><comment author="javanna" created="2015-04-03T06:33:31Z" id="89186835">thanks @rjernst ! 
</comment><comment author="jtibshirani" created="2015-04-14T03:40:55Z" id="92589378">Just wanted to check the status on this and see if there's anything more I can do!
</comment><comment author="rjernst" created="2015-04-17T20:24:40Z" id="94067535">Thanks @jtibshirani! Sorry it took so long to get around to this. The backport required updating the tests to use `index_analyzer` instead of `analyzer`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner_hits - possible to have `search_type` set to `count`?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10358</link><project id="" key="" /><description>I tried setting the `inner_hits` `size` option to 0, but this causes an exception. Will it be possible to ONLY get back total matching inner hits, like we can with normal queries with http://www.elastic.co/guide/en/elasticsearch/reference/1.4/search-request-search-type.html#count?
</description><key id="65567956">10358</key><summary>inner_hits - possible to have `search_type` set to `count`?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaddison</reporter><labels /><created>2015-03-31T23:27:17Z</created><updated>2015-04-02T13:23:58Z</updated><resolved>2015-04-02T13:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-04-01T22:16:20Z" id="88650857">@jaddison This looks like a bug, you should be able to set `size` to 0 on the inner_hit definition. Can you share the exception and possible also a reproduction?

Inner hits is a fetch phase feature. When setting `search_type` to count the fetch phase doesn't get executed, so that is why inner_hits don't work with that search_type. 
</comment><comment author="jaddison" created="2015-04-01T22:26:45Z" id="88652839">With this query:

```
POST /activities/location/_search
{
    "query": {
        "has_child": {
            "type": "classes",
            "query": {"match_all": {}},
            "inner_hits": {"size": 0}
        }
    }
}
```

Here's the result:

```
{
   "took": 7,
   "timed_out": false,
   "_shards": {
      "total": 9,
      "successful": 8,
      "failed": 1,
      "failures": [
         {
            "index": "activities-20150326-154425",
            "shard": 0,
            "status": 500,
            "reason": "IllegalArgumentException[numHits must be &gt; 0; please use TotalHitCountCollector if you just need the total hit count]"
         }
      ]
   },
   "hits": {
      "total": 586,
      "max_score": 1,
      "hits": []
   }
}
```
</comment><comment author="martijnvg" created="2015-04-01T22:42:34Z" id="88654976">This is odd, inner_hits itself doesn't use TotalHitCountCollector...
On what ES version are you running this query? (snapshot?)
</comment><comment author="jaddison" created="2015-04-01T22:59:16Z" id="88657569">```
{
   "status": 200,
   "name": "Miek",
   "cluster_name": "elasticsearch",
   "version": {
      "number": "1.5.0",
      "build_hash": "544816042d40151d3ce4ba4f95399d7860dc2e92",
      "build_timestamp": "2015-03-23T14:30:58Z",
      "build_snapshot": false,
      "lucene_version": "4.10.4"
   },
   "tagline": "You Know, for Search"
}
```
</comment><comment author="martijnvg" created="2015-04-02T07:21:58Z" id="88787131">@jaddison I must have overlooked something last night... I opened a PR: #10388
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search_quote_analyzer always added to mapping when position_offset_gap is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10357</link><project id="" key="" /><description>If position_offset_gap is specified in a field's mapping, search_quote_analyzer is always added, even if no analyzer was specified:

https://gist.github.com/jtibshirani/f4acab68e7e505fb1778

For consistency, search_quote_analyzer should only be in the mapping if it has been specified and differs from search_analyzer.
</description><key id="65567253">10357</key><summary>search_quote_analyzer always added to mapping when position_offset_gap is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-03-31T23:21:29Z</created><updated>2015-04-17T20:20:11Z</updated><resolved>2015-04-17T20:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] add bounding box optimization for geo_polygon filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10356</link><project id="" key="" /><description>Currently, geo_distance filters have a built-in bounding box optimization, configurable through the optimize_bbox option. It would be great if geo_polygon filters also had this optimization, as it can be quite expensive to check every point against the query polygon.
</description><key id="65558916">10356</key><summary>[GEO] add bounding box optimization for geo_polygon filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Geo</label></labels><created>2015-03-31T22:14:57Z</created><updated>2016-01-20T15:18:51Z</updated><resolved>2016-01-20T15:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jtibshirani" created="2015-04-07T01:07:53Z" id="90301567">If the request sounds reasonable, would you mind if I took a stab at this? I would plan to stick with the current behavior for interpreting orientation (since #5968 is still open).
</comment><comment author="nknize" created="2015-04-07T12:42:55Z" id="90535523">@jtibshirani Absolutely. This is a good issue for getting started if you have not already contributed. Thanks for being sensitive to #5968. Just a heads up (although you shouldn't have this problem if you're mirroring the optimization from GeoDistanceFilter) be careful not to use any JTS or S4J functionality in the GeoPolygonFilter logic. This filter is used for `geo_point` types and does not require those dependencies. That's the only gotcha holding up the PR for #5968.
</comment><comment author="clintongormley" created="2016-01-14T14:12:54Z" id="171653992">@nknize I'm assuming this is no longer required given the new geopoints?  Can this be closed, along with the linked PR?
</comment><comment author="nknize" created="2016-01-20T15:18:51Z" id="173235296">Closed the PR. Closing issue in favor of #14537
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>issues with alias/routing name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10355</link><project id="" key="" /><description>Hi,

I created the following alias/routing (where **"users"** is the index name and **"user"** is the type name):

```
{
  "users": {
    "aliases": {
      "acme": {
        "filter": {
          "term": {
            "tenant": "acme"
          }
        },
        "index_routing": "acme",
        "search_routing": "acme"
      }
    }
  }
}
```

And everything works OK:

```
curl -XGET 'http://localhost:9200/acme/user/_count'
{"count":8, ... }

curl -XGET 'http://localhost:9200/users/user/_count?q=tenant:"acme"'
{"count":8, ... }
```

But if the alias name has a '-' char:

```
{
  "users": {
    "aliases": {
      "foo-bar": {
        "filter": {
          "term": {
            "tenant": "foo-bar"
          }
        },
        "index_routing": "foo-bar",
        "search_routing": "foo-bar"
      }
    }
  }
}
```

The routing is not working:

```
curl -XGET 'http://localhost:9200/foo-bar/user/_count'
{"count":0, ... }
```

But I have documents with "foo-bar" as tenant:

```
curl -XGET 'http://localhost:9200/users/user/_count?q=tenant:"foo-bar"'
{"count":15, ... }
```

Any clues?
</description><key id="65550940">10355</key><summary>issues with alias/routing name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">siacomuzzi</reporter><labels /><created>2015-03-31T21:27:25Z</created><updated>2015-04-01T17:23:00Z</updated><resolved>2015-04-01T17:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="siacomuzzi" created="2015-04-01T16:48:51Z" id="88551107">I fixed it adding `not_analyzed` flag:

```
"tenant": {
  "type": "string",
  "index": "not_analyzed"
}
```
</comment><comment author="javanna" created="2015-04-01T17:23:00Z" id="88564437">Thanks for getting back to us @siacomuzzi , the problem was not around the routing value nor the alias name, more about how your field was indexed, hence how the alias filter wasn't able to return the expected results. Glad you solved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separate repository registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10354</link><project id="" key="" /><description>Separate repository registration to make sure that failure in registering one repository doesn't cause failures to register other repositories.

Closes #10351
</description><key id="65550331">10354</key><summary>Separate repository registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T21:22:50Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-03T03:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-04-01T07:12:21Z" id="88374806">LGTM
</comment><comment author="s1monw" created="2015-04-02T08:40:14Z" id="88827244">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure inner hits also works for nested fields defined in object field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10353</link><project id="" key="" /><description>PR for #10334
</description><key id="65541204">10353</key><summary>Make sure inner hits also works for nested fields defined in object field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T20:43:57Z</created><updated>2015-06-07T18:13:45Z</updated><resolved>2015-04-03T07:19:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-02T10:28:15Z" id="88860666">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>separate /_cluster/state routing_table into routing_table and routing_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10352</link><project id="" key="" /><description>currently, specifying routing_table will return both routing_table and routing_nodes. Not sure if this was a thought decision, or something that started of this way and just lingered since then.

Anyway, for me would be awesome being able to control individually if I want routing_table or routing_nodes(or both). This is simply because depending on how many shards you have, the response starts to be pretty big(and being honest, the info given is also redundant, but grouped differently).

I imagine for backwards compatibility, routing_table should cover both values... but would be possible having routing_nodes as an independent metric? Say, extending this Enum here https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/ClusterState.java#L263 to include routing_nodes and accepting both routing_table and routing_nodes on the check here https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/ClusterState.java#L468 ??

If this is something you guys would be willing to take, I'd also be more than willing to attempt a PR at this.
</description><key id="65535529">10352</key><summary>separate /_cluster/state routing_table into routing_table and routing_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T20:11:45Z</created><updated>2015-04-08T13:35:30Z</updated><resolved>2015-04-08T13:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Failure to register of one misconfigured repository might prevent future repository registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10351</link><project id="" key="" /><description>If a repository cannot be registered on one of non-master nodes because of configuration issue (for example if the repository plugin is not installed on the node) it might prevent other properly configured repositories from registering on the node.
</description><key id="65532478">10351</key><summary>Failure to register of one misconfigured repository might prevent future repository registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T19:53:31Z</created><updated>2015-04-03T03:21:34Z</updated><resolved>2015-04-03T03:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Transport: shortcut local execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10350</link><project id="" key="" /><description>In several places in the code we need to notify a node it needs to do something (typically the master). When that node is the local node, we have an optimization in serveral places that runs the execution code immediately instead of sending the request through the wire to itself. This is a shame as we need to implement the same pattern again and again. On top of that we may forget (see note bellow) to do so and we might have to write some craft if the code need to run under another thread pool.

This commit folds the optimization in the TrasnportService, shortcutting wire serliazition if the target node is local.

Note: this was discovered by #10247 which tries to import a dangling index quickly after the cluster forms. When sending an import dangling request to master, the code didn't take into account that fact that the local node may master. If this happens quickly enough, one would get a NodeNotConnected exception causing the dangling indices not to be imported. This will succeed after 10s where InternalClusterService.ReconnectToNodes runs and actively connects the local node to itself (which is not needed), potentially after another cluster state update.
</description><key id="65530198">10350</key><summary>Transport: shortcut local execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T19:40:59Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-08T07:22:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-04-02T09:11:44Z" id="88836914">@bleskes added a few comments
</comment><comment author="bleskes" created="2015-04-02T14:24:27Z" id="88927157">@kimchy thx. pushed a minor commit plus responded. 
</comment><comment author="kimchy" created="2015-04-02T14:27:45Z" id="88928297">@bleskes somehow my comment about disconnect from node got lost, I am not sure we should throw an unsupported exception if its from the local node, it breaks the contract of not doing anything for connect if its a local node (and what happens with unicast disco where the existing host is provided as well?)
</comment><comment author="bleskes" created="2015-04-02T15:56:49Z" id="88957295">@kimchy your comment is folded up here. Here is my response:

re this being a noop - yeah, that's how I originally implemented it as well. Then I thought that strictly speaking we don't honor the disconnect (you can still ask nodeConnected(localNode) and get true), so we should throw an exception. I think your point valid regarding connectToNodeLight , but in that case, at least currently, we never use a known node id. Doubting which one is the lesser evil.
</comment><comment author="bleskes" created="2015-04-02T16:32:49Z" id="88967670">@kimchy I pushed another commit. I was hesitant to add the optimization where we check that the response is returned on the same executor as the request. The API doesn't guarantee us that the channel was not handed off to another thread. I would prefer not to do that one. Feels dangerous.
</comment><comment author="bleskes" created="2015-04-02T16:58:36Z" id="88974537">@kimchy pushed the noop disconnect thing.
</comment><comment author="kimchy" created="2015-04-05T10:40:40Z" id="89750918">@bleskes left another small comment, other than that LGTM. There are several other places where we check on local node that we can remove and simplify the code, `SearchServiceTransportAction` is a great example :). We can push this and keep it small, and then go and cleanup the places to keep the changes manageable?
</comment><comment author="bleskes" created="2015-04-08T07:47:00Z" id="90833507">@kimchy thx. I committed. Agreed on cleaning up more places as a second iteration.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos for gateway.recover_after_time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10349</link><project id="" key="" /><description>There were a few references to the setting `gateway.recovery_after_time`, which should instead be `gateway.recover_after_time`.

I'm new to contributing to Elasticsearch, so I'm not sure if I need to submit similar changes to the elasticsearch/docs repository as well. Let me know if so and I'll be happy to make them there as well.
</description><key id="65524421">10349</key><summary>Fix typos for gateway.recover_after_time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DavidWittman</reporter><labels><label>docs</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T19:11:30Z</created><updated>2015-03-31T20:11:21Z</updated><resolved>2015-03-31T20:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-31T19:45:38Z" id="88223805">Good catch! LGTM. I will pull it in shortly. The docs repo updates it self. No need to do anything.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a query profiling api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10348</link><project id="" key="" /><description>It would be useful to get a query profiling api to help users understand which part of the query/filter/aggregation takes time.
## Context

When preparing queries that will be run frequently, it is hard to predict query performance on large datasets.
Causes I can think of:
- filters caches already created or not
- data being indexed (invalidating caches at each merge?)
- field cardinality
- custom parameters defined in mapping (e.g precision for longs)
- query/filter operand orders

Current solution, involves testing each query/filter part (each operand) independently (at least) 3 times to warm caches, with and without data being written. This gives hints but does not help building complex queries/filters/aggregations. 
## Suggestion

Having a query profiling api could help users find:
- whether query/filter is slow or fast compared to fetch
- whether a filter is properly cached or if cache has to be created
- whether order has impact on query time
- if given filter/query operand improves the query time (or not)
- if times is spent on "shard phase"  or on "merge phase" (maybe not using the correct terms)

It seems to have been asked [before](http://elasticsearch-users.115913.n3.nabble.com/Tracing-query-execution-td3648329.html).
Several databases provide this kind of api: [sql server](https://technet.microsoft.com/en-us/library/ms178071%28v=sql.105%29.aspx) (through execution plan), [solr](https://wiki.apache.org/solr/CommonQueryParameters) (debug=timing/query), [mongodb](http://docs.mongodb.org/manual/reference/method/cursor.explain/)

I understand than ES query model is different from solr, but I think there is a possible improvement to help testing thoughfully instead of following the numerous advices ("term" query is faster than xxx, "filter x" is better because it can be cached) that we currently find on the net.

I'd be happy to provide more details on my use case if necessary (target is to store time series in ES).

What do you think?
</description><key id="65522402">10348</key><summary>Provide a query profiling api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">kamaradclimber</reporter><labels /><created>2015-03-31T18:58:53Z</created><updated>2015-04-05T14:41:14Z</updated><resolved>2015-04-05T14:41:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kamaradclimber" created="2015-03-31T19:05:32Z" id="88211025">cc @dpanth3r 
</comment><comment author="dakrone" created="2015-03-31T19:31:54Z" id="88219210">@kamaradclimber I believe @polyfractal is currently working on this
</comment><comment author="polyfractal" created="2015-03-31T19:42:23Z" id="88222615">@kamaradclimber Absolutely agree with you!  This information is very important, and we don't have a good way to expose (or even collect it) today.  

I have a WIP PR here:  https://github.com/elastic/elasticsearch/pull/6699

It was stalled for a while due to technical reasons, but we've recently rebooted it with a new approach (details here https://github.com/elastic/elasticsearch/pull/6699#issuecomment-85234304).
</comment><comment author="kamaradclimber" created="2015-04-01T09:48:04Z" id="88418315">@polyfractal this is a very great step towards what I am looking for, I'd really like this to be merged.

Improvements I can think of once your patch is in master:
- display if the query used a cache or not
- display merge time (time to merge the results of subqueries/filters)
</comment><comment author="clintongormley" created="2015-04-05T14:41:14Z" id="89784327">Closing in favour of #6699
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better categories for scripted operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10347</link><project id="" key="" /><description>With #10116 we introduced fine-grained script settings. Each time a script is run, it is associated with a context, which holds the operation/api that is making use of the script, provided by the caller.

We currently support 4 operations: `search`, `mapping`, `update` and `aggs`. We folded suggester and percolator under search though, which might be confusing for users, hence we might want to create proper distinct categories for them. 

Even more importantly, plugins may use our `ScriptService` to expose custom scripted operations, which may not fall into these pre-defined categories. I wonder if we should introduce a new generic category of operations for plugins, which can be enabled/disabled via settings too. This way we may also be able to remove the breaking aspect of this new feature, the problem being that each caller needs to specify which operation it's using the script for, and that is a required argument. Given that we updated all of our internal calls to provide that argument, we could now restore the original methods without the `ScriptContext` argument and assume that those calls come from plugins, by assigning them the default plugins category.
</description><key id="65516728">10347</key><summary>Better categories for scripted operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T18:31:52Z</created><updated>2015-04-08T09:57:33Z</updated><resolved>2015-04-08T09:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T18:33:20Z" id="88200710">hey @uboness @clintongormley can you comment on this please?
</comment><comment author="dadoonet" created="2015-04-01T12:14:57Z" id="88455399">@javanna for now, plugins I know basically use scripts to modify documents before injection. That was mostly the case for some rivers.
So `update` works well here. All official plugins have now been updated for 1.6.

Not sure we need something else.
</comment><comment author="javanna" created="2015-04-01T12:23:37Z" id="88457691">@dadoonet yea it makes sense to use update for now but it's ambiguous because it's not really about the update api, but pre-processing of document before they get indexed. I am for making it explicit that plugins may use scripts and we just don't know exactly for what :) 
</comment><comment author="uboness" created="2015-04-01T12:57:47Z" id="88468938">@javanna thinking a few things:
- would be nice of the script context would be extensible... so some plugins will also be able to define their context in more fine grained fashion than a simple "capture all" `plugin` context. Right now the context is an enum, perhaps it should be based on opaque strings (internally we'll sill use the same constants that the enum already captures.
- we can still have a "capture all" context (say `other`) that can be configured like all the others and have compile/execute methods overloaded so we'll also have those without the passed in context (in which case the ctx will default to `other`).... we can perhaps mark it as deprecated and remove it in 2.0 (so from 2.0 ctx will be a required param)?
</comment><comment author="javanna" created="2015-04-01T13:12:09Z" id="88474531">I agree with @uboness on making things more flexible, maybe we could just use the plugin name though instead of having plugins declare potentially multiple additional contexts (think of conflicting ones as well). Let's see what others think about this aspect.

That said I think we should move on with the first step of introducing an additional category that is specific for plugins, which would also have the advantage of allowing us to restore backwards compatibility on `ScriptService`. Will work on this.
</comment><comment author="uboness" created="2015-04-01T13:25:37Z" id="88479402">&gt; maybe we could just use the plugin name though instead of having plugins declare potentially multiple additional contexts (think of conflicting ones as well). Let's see what others think about this aspect.

I can see multiple categories used within a single plugin as well... so maybe then we'll have the category be prefixed with the plugin name... as a convention at least?

&gt; That said I think we should move on with the first step of introducing an additional category that is specific for plugins, which would also have the advantage of allowing us to restore backwards compatibility on ScriptService. Will work on this.

+1
</comment><comment author="clintongormley" created="2015-04-02T07:45:40Z" id="88798890">+1 to https://github.com/elastic/elasticsearch/issues/10347#issuecomment-88479402
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cross_fields multi match with dfs_query_then_fetch type uses field level idf</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10346</link><project id="" key="" /><description>I tested this on **Elasticsearch 1.5.0** (and on ES 1.4.2 and on 1.3.0)

The documentation for the `multi_match` query of type `cross_fields` states that 

&gt; The problem of differing term frequencies is solved by blending the term frequencies for all fields in order to even out the differences.

This holds true when executing a query of this type with search type `query_then_fetch`: in this case, the same (approximated, shard-level) idf is used for all fields. When using search type `query_dfs_then_fetch`, on the other hand, the specific field idf is used.

I would expect the scatter phase to provide to the multi match query the right merged idf, and not to completely overwrite the (approximated but still more correct) shard level idf with global field-specific idfs.

An obvious test is provided in the following gist
https://gist.github.com/micpalmia/c812200617307d78d495

A series of documents are inserted in one shard only, and when a `cross_field` query is executed with `query_dfs_then_fetch`, unmerged idfs are used for the two fields.
</description><key id="65495091">10346</key><summary>cross_fields multi match with dfs_query_then_fetch type uses field level idf</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2015-03-31T16:40:26Z</created><updated>2017-04-24T19:17:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T14:04:58Z" id="89776631">Nice demonstration. I confirm that this is a bug.
</comment><comment author="vharitonsky" created="2016-08-25T14:44:28Z" id="242413646">+
</comment><comment author="mkrakovyan" created="2016-08-25T15:25:51Z" id="242428446">@clintongormley 
Hi, couldn't U please update on the status of this issue, we've run into it as well, running the query on multiple shards, using dfs_query_then_fetch.
Also, while trying to understand the cause I've encountered this post on elastic discuss:
[question](https://discuss.elastic.co/t/why-is-idf-different-for-same-term-in-same-field-in-same-shard/52383)
</comment><comment author="gjbh-idematica" created="2017-04-24T19:17:40Z" id="296794848">I also ran into this bug. It would be great if it could be solved.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecating Rivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10345</link><project id="" key="" /><description>The reasoning behind it is captured in this blog post: https://www.elastic.co/blog/deprecating_rivers

We need to `@Deprecate` the River classes, and deprecate them in the docs, as well as add a deprecation notice in the rivers that are under elastic.
</description><key id="65478015">10345</key><summary>Deprecating Rivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>deprecation</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T15:27:24Z</created><updated>2017-05-31T06:31:00Z</updated><resolved>2015-06-03T11:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-09T08:37:10Z" id="91150985">Docs have been added to indicate that rivers are deprecated.  Still need to add deprecation annotations to the code
</comment><comment author="dharshanr" created="2015-04-13T18:05:33Z" id="92447595">Is there a place to have a discussion on this? Looks like the problem has been punted rather than fixed. From a customer perspective it doesn't matter if the import code runs inside elastic or not - its more about importing data. 

If stability was really an issue then a better solution would be to provide a standalone daemon that will be able to able to host the rivers and allow customers to import the data instead of leaving them to fend for themselves. 
</comment><comment author="carljohnstone" created="2015-04-15T10:10:16Z" id="93302316">Sorry I'm not hugely familiar with the code-base itself, but would it be possible to write a non-river plugin, that pulled in data from an external source than indexed it into Elasticsearch using the bulk-data APIs? Would you expect that to be any more stable than a plugin coded against the River classes? Eg is the actual problem the fact that these are plugins running within the ES server itself, rather than rivers.
</comment><comment author="kimchy" created="2015-04-18T11:08:15Z" id="94155968">@dharshanr / @carljohnstone in those cases, there really isn't any benefit in "Elasticsearch" itself having a framework for it, and its better left to be done by projects that are fully geared towards ETL such workloads, with native integration to Elasticsearch (which is where I think spending time would provide the best value). Logstash is a great example, but there are many more.
</comment><comment author="nkavian" created="2015-04-28T18:27:43Z" id="97162956">I agree with @dharshanr this is punting the problem.  We were enjoying using the Couchbase plugin to replicate into ES bypassing the app layer.  Now what I'm understanding is the app layer now needs to copy into both places.. 
</comment><comment author="dadoonet" created="2015-04-28T18:31:07Z" id="97163826">Couchbase plugin is a transport plugin not a river plugin so not concerned by this change.
</comment><comment author="nik9000" created="2015-04-28T18:31:48Z" id="97163958">&gt; @dharshanr / @carljohnstone in those cases, there really isn't any benefit in "Elasticsearch" itself having a framework for it

I figure the advantage of rivers is that you're already deploying Elasticsearch and they are already written. As much trouble as they are, they exist.

I'm still +1 on deprecating them.
</comment><comment author="bgiromini" created="2015-05-12T19:48:00Z" id="101399020">The biggest hole in my opinion is getting data from a relational database into ES. I know that some work is being down on the logstash front but would be nice if it was officially supported by Elastic.
</comment><comment author="dadoonet" created="2015-05-12T20:15:46Z" id="101407637">@bgiromini in case you missed it, have a look at https://github.com/logstash-plugins/logstash-input-jdbc
</comment><comment author="hash-include" created="2015-05-21T06:27:27Z" id="104152589">One quick question, If I am using river-mongodb and most of the company code is in Java. Can I push data into elasticsearch from Java API removing the river? 
</comment><comment author="dadoonet" created="2015-05-21T06:29:14Z" id="104152764">@hash-include This is indeed my favorite way for doing that! Pushing from the application directly :)
I wrote a blog post about it though it's based on RDBMS and not NoSQL db: http://david.pilato.fr/blog/2015/05/09/advanced-search-for-your-legacy-application/
</comment><comment author="s1monw" created="2015-06-03T10:30:58Z" id="108291477">@dadoonet do you know what is missing here to close this?
</comment><comment author="dadoonet" created="2015-06-03T11:02:56Z" id="108298925">@s1monw I think we can close it. It has been deprecated in 1.6.0 in docs and in 2.0.0 in code.
Note that we are not using the deprecation logger in master though: https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/RiversService.java#L129

Closing. If I'm missing anything, feel free to reopen.
</comment><comment author="tchcxp" created="2015-06-22T00:40:42Z" id="113973599">@kimchy 

I have been using mongodb river for a while. So the alternative solution is to update data in the application or other services with elastic API calls?

Thanks.
</comment><comment author="dadoonet" created="2015-06-22T06:32:52Z" id="114020389">@tchcxp I answered the same question a month ago: https://github.com/elastic/elasticsearch/issues/10345#issuecomment-104152764

HTH
</comment><comment author="tchcxp" created="2015-06-22T06:37:57Z" id="114021095">@dadoonet 

Cool! Great help. Thanks. :+1: 
</comment><comment author="syllogismos" created="2015-11-09T14:44:41Z" id="155082841">@tchcxp how well did updating data in elasticsearch from app layer work, instead of relying on mongodb river.
</comment><comment author="monikamaheshwari" created="2017-05-31T06:27:48Z" id="305096424">@kimchy I want to replicate same set of data from mongoDB server to elasticSearch server and vice
versa. What should i use?</comment><comment author="dadoonet" created="2017-05-31T06:31:00Z" id="305096970">@monikamaheshwari please ask your question on discuss.elastic.co. I'm sure many users will be able to share their experience with you. Or better, search on discuss for previous discussions regarding this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot repository registration api call causing "out of memory" errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10344</link><project id="" key="" /><description>We are running an elasticsearch cluster with 27 nodes and we create about 200 new indices per day. 
We keep data for the last 5 days, so in total we have about 1000 indices.  Every day we take a snapshot of yesterday's 200 indices in S3 (so, no incremental backups). 

After updating to 1.4.2 we've noticed that when we try to register a snapshots repository we end up with the master node running out of heap space and the cluster going into an unresponsive state. I'm attaching a screenshot where you can see the heap space usage on the master node after making a PUT request to register a snapshots repository.

![jmx-master](https://cloud.githubusercontent.com/assets/2852737/6919795/eb962f3e-d7b1-11e4-8a6f-89763fd6b7f7.png)

After inspecting a heap dump taken from the master node we realised that it's trying to list the contents of our s3 repository. At the moment we keep all previous snapshots in our s3 repository which means that it's impossible (in terms of time and resources) to list everything. I'm attaching a screenshot from Memory Analyser where you can see that 51% of the heap space (800 MB) is occupied by a Map storing 5.000.000 entries with PlainBlobMetadata objects as values and s3 locations as keys.

![memoryanalyser_tree](https://cloud.githubusercontent.com/assets/2852737/6919797/f114a09e-d7b1-11e4-90b4-fea6d402a3b0.png)

We recently updated to 1.4.2. (from 1.1.2) and we don't think we've seen a similar behaviour (i.e. listing s3 repository contents) in 1.1.2. Could be an issue that needs further investigation on your side or could be the way we are using the snapshots service at the moment that is causing us problems?

Any suggestions/feedback would be welcome.

Thank you
</description><key id="65450265">10344</key><summary>Snapshot repository registration api call causing "out of memory" errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicktgr15</reporter><labels /><created>2015-03-31T13:27:18Z</created><updated>2015-04-02T12:19:00Z</updated><resolved>2015-04-02T12:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-03-31T13:56:51Z" id="88096471">@nicktgr15 could you post the stack trace for the OOM error?
</comment><comment author="tlrx" created="2015-03-31T14:00:52Z" id="88098535">&gt; we create about 200 new indices per day

How many number of shards represent those 200 indices? What's the total size?
</comment><comment author="nicktgr15" created="2015-03-31T14:34:59Z" id="88112169">Thanks for the responses!

@imotov I'm currently reproducing the issue in order to get a full stack trace which I will attach to the ticket as soon as I have it

@tlrx There are 7 primary shards and 7 replicas per index, so 200 indices are represented by 2800 shards. The total number of shards (for the 5 days retention policy) is 14000. The total size in bytes for the 200 indices created per day is close to 400GB (including replicas).
</comment><comment author="tlrx" created="2015-03-31T15:02:25Z" id="88123013">@nicktgr15 thanks for your quick response

We are suspecting that the verification process failed in your case. Can you please try to register the repository but without verification? You have to set `verify: false` when registering the repo, see the documentation [here](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html#_shared_file_system_repository)
</comment><comment author="nicktgr15" created="2015-03-31T16:16:58Z" id="88151090">I'm attaching a screenshot from jConsole showing the heap space utilisation in both cases with `verify: true` (default) and `verify: false`. For the two cases we got the following output:

**verify: true** (default)

```
curl -XPUT 'http://localhost:9200/_snapshot/s3_repository' -d '{
&gt;     "type": "s3",
&gt;     "settings":{"region":"eu-west-1","base_path":"pyxis/test/snapshots","max_restore_bytes_per_sec":"100mb","bucket":"prod-s3-common-storagebucket-sl9vplg1o48d"}
&gt; }'
{"error":"OutOfMemoryError[Java heap space]","status":500}
```

```
[2015-03-31 15:43:25,265][INFO ][repositories             ] [Master Menace] update repository [s3_repository]
[2015-03-31 15:53:11,375][WARN ][repositories             ] [Master Menace] [s3_repository] failed to finish repository verification
```

**verify: false** 

```
curl -XPUT 'http://localhost:9200/_snapshot/s3_repository' -d '{
&gt;     "type": "s3",
&gt;     "settings":{"region":"eu-west-1","base_path":"pyxis/test/snapshots","max_restore_bytes_per_sec":"100mb","bucket":"prod-s3-common-storagebucket-sl9vplg1o48d", "verify": false}
&gt; }'
{"error":"AmazonClientException[Unable to unmarshall response (Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler). Response Code: 200, Response Text: OK]; nested: AmazonClientException[Failed to sanitize XML document destined for handler c
```

```
[2015-03-31 15:56:21,785][INFO ][repositories             ] [Master Menace] update repository [s3_repository]
[2015-03-31 16:05:33,932][WARN ][repositories             ] [Master Menace] [s3_repository] failed to finish repository verification
```

![repository-verify](https://cloud.githubusercontent.com/assets/2852737/6923334/efc902fe-d7c8-11e4-8508-bd8da584e422.png)

We reduced the heap space to 300m in order to trigger the out of memory error sooner. As you can see in the first block there is an "out of memory" error but there was no stack trace.

This was not reproduced on Live environment and load conditions were not the same.
</comment><comment author="tlrx" created="2015-04-01T12:41:40Z" id="88465144">@nicktgr15 thanks for the information. I think I can now imagine what happened in your cluster.

In elasticsearch 1.4 we added repository verification. When registering a repository, each node of the cluster tries to write a file in the repository before the repository registration is acknowledged. 

The current implementation of the verification process lists all the files at the root level of the repository... and in your case it represents too much data for the Java XML API so the node hits an OOM. 

First thing, I've created the pull request #10366 to avoid listing all files when verifying a repository.

Second thing, the is an error in the documentation. To disable repository verification you need to add the URL parameter `verify=false` when registering the repository. I've created the pull request #10365 to update the doc, but you can now try to register the repository with this parameter.

Last thing: in your case there are good chances that disabling the repository verification will just differ the OOM. The snapshot process needs to list all files of the repository to be able to make an incremental backup. The upcoming #8969 will improve the creation and deletion of snapshot in use cases like yours, but you should consider to create less indices (200 indices with 7 shards = 1400 shards, 400GB / 1400 =~142MB per shard where a shard could handle GBs of data)

You can also consider to create multiple repositories, like snapshotting newly indices in a new repo every day.
</comment><comment author="nicktgr15" created="2015-04-02T09:50:01Z" id="88852292">Thanks for the update @tlrx. As a temporary workaround we will be rotating the snapshots repository on a monthly basis. We didn't have a chance to try again with `verify: false` as a url parameter, if we do, I will provide an update.

Regarding your suggestion about the number of shards in the cluster, is there a performance impact on the snapshotting process because of this? 

In general, we know that there is a suggested 10GB upper limit for the shard size and based on what you describe, we should aim for a minimum of 1GB. Does this sound like the right strategy to optimise the number of shards we currently have in the cluster? At this point we can't reduce the number of indices as it is a requirement the system should satisfy.
</comment><comment author="tlrx" created="2015-04-02T10:07:59Z" id="88856426">&gt; Regarding your suggestion about the number of shards in the cluster, is there a performance impact on the snapshotting process because of this?

Definitely. The snapshot process iterates over each file of the shards to process and upload them. The more shards you have, the more files need to be snapshotted.

&gt; Does this sound like the right strategy to optimise the number of shards we currently have in the cluster?

Hard to tell if it's the right strategy without knowing exactly what is your use case. But for what I know of your cluster I feel like you should benefit of creating less but larger indices.

&gt; At this point we can't reduce the number of indices as it is a requirement the system should satisfy.

I'm curious to understand why the number of indices is a requirement for your system?
</comment><comment author="nicktgr15" created="2015-04-02T10:23:18Z" id="88859313">Was a requirement when the system was designed as it was simplifying the way we managed clients data (e.g. backups per index, permissions). However, we are going to review our current approach as the high number of indices seems to be an overhead. Thank you for you help. 
</comment><comment author="tlrx" created="2015-04-02T12:19:00Z" id="88881107">@nicktgr15 ok thanks, that's what I suspected. 

You may be interested in the chapter ["Designing for Scale"](http://www.elastic.co/guide/en/elasticsearch/guide/current/scale.html) of the book Elasticsearch Definitive Guide. It's a great chapter that deals with use case like yours.

I'm closing this issue since many pull requests are already engaged. Feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[STATE] Refactor state format to use incremental state IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10343</link><project id="" key="" /><description>Today there is a chance that the state version for shard, index or cluster
state goes backwards or is reset on a full restart etc. depending on
several factors not related to the state. To prevent any collisions
with already existing state files and to maintain write-once properties
this change introductes an incremental state ID instead of using the plain
state version. This also fixes a bug when the previous legacy state had a
greater version than the current state which causes an exception on node
startup or if left-over files are present.

Note: this is the backport of #10316 to 1.x but since it was tricky I really want @bleskes to take another look
</description><key id="65448286">10343</key><summary>[STATE] Refactor state format to use incremental state IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-03-31T13:19:06Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-01T08:05:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-31T21:02:55Z" id="88245582">Left one comment about a potential left over / forgotten usage. LGTM o.w. (although I'm not an expert of the File vs Path apis).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add check to MetaData#concreteIndices to prevent NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10342</link><project id="" key="" /><description>There is a current edge case in resolving concrete aliases or index names in MetaData that can lead to NullPointerException when the IndicesOptions don't allow wildcard expansion and the method is called with `aliasesOrIndices` argument `null` or emtpy list. This adds checks and randomized tests that catch this.

Closes #10339
</description><key id="65441648">10342</key><summary>Add check to MetaData#concreteIndices to prevent NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T12:42:41Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-01T09:13:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T12:46:51Z" id="88071022">Left  a couple of minor comments, looks good besides that
</comment><comment author="javanna" created="2015-03-31T15:36:45Z" id="88136602">I left a coding style comment on the tests. LGTM though other than that. Go ahead and push when you have addressed it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding irrelevant document change order of result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10341</link><project id="" key="" /><description>I started with a clean instance of elasticsearch
and add these documents:

```
POST index_for_test/business/201778816
{
               "name": "pizza perperook",
               "categories": "food italian"
}
POST index_for_test/business/126949399
{
               "name": "bono italian resturant",
               "categories": "pizza"
}
POST _refresh
```

and run this query:

```
GET index_for_test/business/_search
{

    "query": {
            "multi_match": {
            "query":       "italian",
            "type":        "most_fields",
            "fields":      [ "name^2", "categories" ]
          }
    }
}
```

result is this (as expected):

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0.04012554,
      "hits": [
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "126949399",
            "_score": 0.04012554,
            "_source": {
               "name": "bono italian resturant",
               "categories": "pizza"
            }
         },
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "201778816",
            "_score": 0.014542127,
            "_source": {
               "name": "pizza perperook",
               "categories": "food italian"
            }
         }
      ]
   }
}
```

after that I add another document:

```
POST index_for_test/business/125697759
{
               "name": "valid id",
               "categories": "pizza"
}
POST _refresh
```

then I run again the query:

```
GET index_for_test/business/_search
{

    "query": {
            "multi_match": {
            "query":       "italian",
            "type":        "most_fields",
            "fields":      [ "name^2", "categories" ]
          }
    }
}
```

and result is : (as expected)

```
{
   "took": 5,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0.04012554,
      "hits": [
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "126949399",
            "_score": 0.04012554,
            "_source": {
               "name": "bono italian resturant",
               "categories": "pizza"
            }
         },
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "201778816",
            "_score": 0.014542127,
            "_source": {
               "name": "pizza perperook",
               "categories": "food italian"
            }
         }
      ]
   }
}
```

then I add another document (id is important)

```
POST index_for_test/business/108493874
{
               "name": "invalid id",
               "categories": "pizza"
}

POST _refresh
```

and run the query again:

```
GET index_for_test/business/_search
{

    "query": {
            "multi_match": {
            "query":       "italian",
            "type":        "most_fields",
            "fields":      [ "name^2", "categories" ]
          }
    }
}
```

and result is :(order and scores is not as expected)

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0.08850529,
      "hits": [
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "201778816",
            "_score": 0.08850529,
            "_source": {
               "name": "pizza perperook",
               "categories": "food italian"
            }
         },
         {
            "_index": "index_for_test",
            "_type": "business",
            "_id": "126949399",
            "_score": 0.04012554,
            "_source": {
               "name": "bono italian resturant",
               "categories": "pizza"
            }
         }
      ]
   }
}
```

it is completely magical :)
it seems if I add a document with these id it corrupts index of elasticsearch:
120365673
407572830
108493874
</description><key id="65439713">10341</key><summary>adding irrelevant document change order of result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">esmaeilzadeh</reporter><labels /><created>2015-03-31T12:34:13Z</created><updated>2015-04-05T13:35:35Z</updated><resolved>2015-03-31T12:42:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T12:42:53Z" id="88070218">This is all about how scoring works in elasticsearch and lucene, as scores of matching documents are affected by the distribution of terms throughout the entire index That is in order to define which terms (matches) are more important than others. To know more about scoring in lucene, have a look [here](https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html).  What I briefly described is the inverse document frequency factor in the scoring formula. Have also a look at the explain api to know why the score changes.

I am closing this, I think for next time it would be better to first ask these questions on our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch), as we tend to use github issues for actual bugs and feature requests.
</comment><comment author="esmaeilzadeh" created="2015-03-31T12:58:36Z" id="88074253">I read this article. there is not anything in that article that explain id of document affect scoring of result. I offer to take a look on the question thoroughly.
</comment><comment author="javanna" created="2015-03-31T13:00:44Z" id="88074863">Hi @esmaeilzadeh we can definitely have another look at your question if you ask it on the maling list ;)
</comment><comment author="esmaeilzadeh" created="2015-03-31T14:16:48Z" id="88104173">it caused by a default configuration that was setup for large data and it was inaccurate:
https://www.elastic.co/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch
</comment><comment author="clintongormley" created="2015-04-05T13:35:35Z" id="89770839">@esmaeilzadeh also see http://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible bug with recovery and gateway.expected_nodes after process restart on ES 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10340</link><project id="" key="" /><description>Hi,

We are running `Elasticsearch` `1.4.4` on `Java(TM) SE Runtime Environment (build 1.7.0_60-b19)`

We have a cluster with 15 nodes: 3 master, 3 client, 9 data.

All the config files have:

``` yaml
gateway.expected_nodes: 15
```

Based on the reading of my documentation this should actually be 12, as expected nodes only counts `master` and `data` nodes. So in our case it should never be hitting expected nodes. As we would only have 12 total for the sake of this setting.

So when we restart a `data` node process, the cluster should be at 11 or 14 of the expected nodes. Either way less than 15.

The documentation states that: `Setting gateway.expected_nodes also defaults gateway.recovery_after_time to 5m`. And we don't define a value for this in our config file

So if I am not mistaken when we reboot a node it should never hit the 15 expected nodes and it should wait 5 minutes for recovery. However what we are noticing is that reallocation of the shards is happening immediately.

eg. the moment data1 stops, its shards are being being assigned to other nodes in the cluster, it is not waiting for 5 minutes, or for the expected number of nodes.

I believe this is a bug and thought I should submit.
</description><key id="65430254">10340</key><summary>Possible bug with recovery and gateway.expected_nodes after process restart on ES 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mbarrin</reporter><labels /><created>2015-03-31T11:25:01Z</created><updated>2015-04-01T15:03:30Z</updated><resolved>2015-04-01T15:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-04-01T07:49:51Z" id="88383626">You are correct in saying that expected nodes only relates to data + master nodes. I think the source of the confusing is that it also _only_ holds for the initial cluster forming after a full cluster restart or an outage. In those cases the master waits until it sees enough nodes to start assigning shards.

For a single node restart, I believe you expect something like delaying the re-assignment of a shard for a couple of minutes with the hope the node will come back. I think there is an issue for it somewhere but can't find it now. If you confirm I'll dig it up and/or open another issue.
</comment><comment author="mbarrin" created="2015-04-01T10:49:17Z" id="88437429">Ah thanks for the clarification. I assume disabling shard allocation for the purpose of the reboot is the best idea then?

Assuming it is at least a planned reboot.
</comment><comment author="bleskes" created="2015-04-01T12:52:36Z" id="88467696">if it's a controlled reboot yes. See http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#shards-allocation .

Let me know if we can close this...
</comment><comment author="mbarrin" created="2015-04-01T15:03:30Z" id="88516166">Yes I will close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Checks for preventing NPEs in MetaData#concreteIndices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10339</link><project id="" key="" /><description>This is a follow issue pulled out of #10148. The current implementation of MetaData.concreteIndices() leaves some edge cases where NPEs can occurr. This can happen when wildcard expansion is switched off for both open and closed indices in the IndicesOptions and the method is calles with `null`or empty list for the `aliasOrIndices` argument.

This PR adds a check and randomized tests for this method.
</description><key id="65425558">10339</key><summary>Checks for preventing NPEs in MetaData#concreteIndices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T11:01:08Z</created><updated>2015-04-01T09:13:07Z</updated><resolved>2015-04-01T09:13:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Setting the slowlog log level has no effect with 1.4.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10338</link><project id="" key="" /><description>On a ES 1.4.2 cluster, I am trying to transiently set the log level of search slowlogs via the API:

``` bash
curl -s -XPUT 'http://localhost:9200/_cluster/settings' -d '
{
    "transient" : {
        "logger.index.search.slowlog.query" : "INFO"
    }
}'
```

I confirmed that the transient setting is stored in the cluster with a GET on the same path.

However I still have many DEBUG log events in my slowlogs. What am I doing wrong? Is this a known issue? 
</description><key id="65412778">10338</key><summary>Setting the slowlog log level has no effect with 1.4.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cmeury</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label></labels><created>2015-03-31T09:58:51Z</created><updated>2016-01-14T14:11:38Z</updated><resolved>2016-01-14T14:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T13:25:26Z" id="89770170">@cmeury I wonder if the debug events are really fetch events?

Try setting `logger.index.search.slowlog` instead?
</comment><comment author="cmeury" created="2015-04-07T14:18:39Z" id="90580685">No, the log entries are shown with ".query". I configured like this:

``` bash
curl -s -XPUT 'http://localhost:9200/_cluster/settings' -d '
{
    "transient" : {
        "logger.index.search.slowlog.query" : "INFO"
    }
}'
```

It was acknowledged and showed up when GETting the settings, but I still have entries like this in the slowlog file:

```
[2015-04-07 13:45:39,948][DEBUG][index.search.slowlog.query] [hostname] [posts][1] took[762.7ms], took_millis[762], types[post], ...
```

I tried setting `logger.index.search.slowlog` to `INFO` as well, did not change anything. Isn't it hierarchical anyway?
</comment><comment author="clintongormley" created="2016-01-14T14:11:38Z" id="171653712">Duplicate of #7461
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Networking: Node did not recover from recovered networking issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10337</link><project id="" key="" /><description>I have one node, that continouosly spit a "NoRouteToHostException" until the JVM is restarted, then everything is fine.

This exception floods the logs, even though the network connectivity has been restored in the meantime (about 5 min outage). Maybe the JVM is caching something here, but not sure where to look.

```
[2015-XX-YY AA:BB:07,790][WARN ][transport.netty          ] [NODENAME] exception caught on transport layer [[id: 0x9634070f]], closing connection
java.net.NoRouteToHostException: No route to host
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Happened on elasticsearch 1.4.x
</description><key id="65397178">10337</key><summary>Networking: Node did not recover from recovered networking issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label></labels><created>2015-03-31T08:30:27Z</created><updated>2016-01-14T14:09:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-02T19:41:30Z" id="89021021">There is a 5 minute cache in the JDK for localhost.
</comment><comment author="rmuir" created="2015-04-02T19:47:55Z" id="89023037">Sorry, i meant 5 second :) Look @ InetAddress.java
</comment><comment author="kimchy" created="2015-04-02T20:00:18Z" id="89026157">this is really weird, since you can see we do full connection all the way to the SocketImpl every time... .
</comment><comment author="spinscale" created="2015-04-03T08:59:14Z" id="89228436">@rmuir how do you draw the connection to localhost? This cluster was running on several hosts, no localhost involved in discovery... could the cache still be an issue?
</comment><comment author="rmuir" created="2015-04-03T11:29:57Z" id="89259944">You mentioned a JVM cache, so I gave you one. I wouldn't dismiss the problem: i see ES calling InetAddress.getLocalHost in inconsistent and buggy ways. In one place it even caches it in a static block as if it can never change. broken...
</comment><comment author="kimchy" created="2015-04-04T13:22:12Z" id="89576926">@spinscale here is some reference to the caching I was referring to, it revolves around DNS caching done in the JVM: 
- http://wiki.apache.org/hadoop/NoRouteToHost
- http://docs.oracle.com/javase/8/docs/technotes/guides/net/properties.html
- http://serverfault.com/questions/459814/clearing-tomcats-dns-cache
- http://docs.aws.amazon.com/AWSSdkDocsJava/latest//DeveloperGuide/java-dg-jvm-ttl.html

We might want to consider setting different ttl in our config, unsure yet though, but at least it can explain whats going on.

Regarding the local address caching, its mainly used during startup and for logging. I think we can clean this up, for startup, its ok to simply not use the cache, and conceptually we can do the same for logging, since we set all the loggers anyhow.
</comment><comment author="kimchy" created="2015-04-04T13:30:46Z" id="89578303">Btw, another reason why this can happen, and this is more connected to how nodes are serialized over the network. When a node starts up, it gets initialized with an address, and when the cluster state is published it gets streamed to the other node, and it uses the actual IP address to construct back the inet address, and not perform another lookup based on host name. So the IP address of a node is set on startup, and then published so it can be connected to.

This can result in this failure as well, and to be honest, its a tricky tradeoff here... . There is a way to force serialization to serialize host name, and then do lookup on each deserialization, its in `InetSocketTransportAddress`. 

Most other systems I have looked at when investigating this were doing the same thing, which is serializing the IP address itself, I think thats why what you see when a change happens is the common recommendation is to restart the JVM (the combination of the JVM cache and this serialization mode for networked systems)
</comment><comment author="clintongormley" created="2016-01-14T14:09:06Z" id="171653232">Has this been resolved by recent changes?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use of javascript in mapping `transform` fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10336</link><project id="" key="" /><description>Below reproduction shows that while it is possible to use `groovy` in mapping `transform` the same operation fails when `javascript` is used.

Tested with Elasticsearch `1.4.1`, `1.4.4` and `1.5.0`.
### Reproduction

Download fresh Elasticsearch (`1.4.1`) and install lang-javascript plugin (`2.4.1`):

``` bash
    wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.1.zip
    unzip elasticsearch-1.4.1.zip
    cd elasticsearch-1.4.1
    ./bin/plugin install elasticsearch/elasticsearch-lang-javascript/2.4.1
```

Enable scripting:

``` bash
    echo script.disable_dynamic: false | tee -a config/elasticsearch.yml &gt; /dev/null
    cat config/elasticsearch.yml | grep script
    # script.disable_dynamic: false
```

Start node:

``` bash
    ./bin/elasticsearch -d
```

After node is ready check we have a fresh empty cluster with javascript plugin only:

``` bash
    curl 'http://localhost:9200/_cat/health?v'
    # epoch      timestamp cluster       status node.total node.data shards pri relo init unassign 
    #1427789340 10:09:00  elasticsearch green           1         1      0   0    0    0        0 

    curl 'localhost:9200/_cat/plugins?v'
    # name      component       version type url
    # Maur-Konn lang-javascript 2.4.1   j
```

Create index with `transform` script:

``` bash
    curl -X PUT 'localhost:9200/test_javascript' -d '{
      "mappings": {
        "test": {
          "transform": {
            "script":"true",
            "lang": "javascript"
          }
        }
      }
    }'
    # {"acknowledged":true}

    curl -X PUT 'localhost:9200/test_groovy' -d '{
      "mappings": {
        "test": {
          "transform": {
            "script":"true",
            "lang": "groovy"
          }
        }
      }
    }'
    # {"acknowledged":true}

    curl -X GET 'localhost:9200/test*/_mapping?pretty'
    #{
    #  "test_groovy" : {
    #    "mappings" : {
    #      "test" : {
    #        "transform" : {
    #          "script" : "true",
    #          "lang" : "groovy"
    #        },
    #        "properties" : { }
    #      }
    #    }
    #  },
    #  "test_javascript" : {
    #    "mappings" : {
    #      "test" : {
    #        "transform" : {
    #          "script" : "true",
    #          "lang" : "javascript"
    #        },
    #        "properties" : { }
    #      }
    #    }
    #  }
    #}
```

Index document into test indices:

``` bash
    curl -X POST 'localhost:9200/test_groovy/test' -d '{ "foo": "bar" }'
    # {"_index":"test_groovy","_type":"test","_id":"AUxu4fLdiRC7ona2R0WV","_version":1,"created":true}

    curl -X POST 'localhost:9200/test_javascript/test' -d '{ "foo": "bar" }'
    # {"error":"MapperParsingException[failed to parse]; nested: ElasticsearchIllegalArgumentException[failed to execute script]; nested: NullPointerException; ","status":400}
```

Check log file:

```
less logs/elasticsearch.log

[2015-03-31 10:10:59,993][DEBUG][action.index             ] [Maur-Konn] [test_javascript][2], node[KF0FaHAPSAybJxQpwMPCow], [P], s[STARTED]: Failed to execute [index {[test_javascript][test][AUxu4jHGiRC7ona2R0WW], source[{ "foo": "bar" }]}]
org.elasticsearch.index.mapper.MapperParsingException: failed to parse
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:562)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:198)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: failed to execute script
        at org.elasticsearch.index.mapper.DocumentMapper$ScriptTransform.transformSourceAsMap(DocumentMapper.java:836)
        at org.elasticsearch.index.mapper.DocumentMapper.transformSourceAsMap(DocumentMapper.java:606)
        at org.elasticsearch.index.mapper.DocumentMapper.transform(DocumentMapper.java:612)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:507)
        ... 8 more
Caused by: java.lang.NullPointerException
        at org.elasticsearch.script.javascript.JavaScriptScriptEngineService.executable(JavaScriptScriptEngineService.java:113)
        at org.elasticsearch.script.ScriptService.executable(ScriptService.java:467)
        at org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
        at org.elasticsearch.index.mapper.DocumentMapper$ScriptTransform.transformSourceAsMap(DocumentMapper.java:828)
        ... 11 more

```
</description><key id="65395450">10336</key><summary>Use of javascript in mapping `transform` fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2015-03-31T08:19:29Z</created><updated>2016-01-14T14:02:05Z</updated><resolved>2016-01-14T14:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-31T08:44:01Z" id="87997363">@lukas-vlcek I wonder if you should not better open it in javascript lang plugin instead of here. `org.elasticsearch.script.javascript.JavaScriptScriptEngineService` is in this plugin.
</comment><comment author="lukas-vlcek" created="2015-03-31T08:50:39Z" id="87998764">@dadoonet To be honest I am not sure where exactly the problem is. Feel free to move it to [lang-javascript](https://github.com/elastic/elasticsearch-lang-javascript) if the problem is there.
</comment><comment author="yesoreyeram" created="2015-04-22T08:56:50Z" id="95081753">Any workaround to run javascript in transform?
</comment><comment author="lukas-vlcek" created="2015-04-22T09:19:55Z" id="95088459">@yesoreyeram regarding workaround I switched to `"lang": "groovy"`.
</comment><comment author="clintongormley" created="2016-01-14T14:02:04Z" id="171651621">Mapping transform has been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logging: Allow to change log level of single node all the time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10335</link><project id="" key="" /><description>In order to improve debug capabilities, we should allow for changing the node level of a single node all the time, regardless of its state. Reason for this is, that it is impossible to increase logging, in case the node is not part of the cluster, which makes it hard to debug network issues/outages.

The the Cluster Update Settings API only works if the node is joined into a cluster.

Not sure about the right solution here, but could be
- Parse the cluster update settings API and apply logging settings locally all the time?
- Have an own endpoint?
- Support files?
</description><key id="65394354">10335</key><summary>Logging: Allow to change log level of single node all the time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Logging</label><label>discuss</label><label>enhancement</label></labels><created>2015-03-31T08:11:07Z</created><updated>2015-08-20T20:44:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T09:35:59Z" id="88014847">Hey @spinscale by "if the node is not part of a cluster" you mean that the node doesn't have a master, thus we would return a master not discovered exception? 
</comment><comment author="spinscale" created="2015-03-31T09:40:32Z" id="88016279">@javanna yes, think of a network segregation of the single node, which returns in a "master not discovered" error when running the cluster update settings API
</comment><comment author="javanna" created="2015-03-31T09:42:20Z" id="88016634">@spinscale why can't we restart the node then to change log settings then? The node doesn't do anything in such situation. Maybe there are other usecases for this, but in general I wonder if it's really needed. Marking for discussion.
</comment><comment author="spinscale" created="2015-03-31T09:44:08Z" id="88016901">restarting means loosing the current state which could be debugged and analyzed further with more logging output.. if restarting solves the problem, that is good, but the possibility for further debugging should remain in that state IMO
</comment><comment author="javanna" created="2015-03-31T09:45:37Z" id="88017120">I see your point...I was just assuming that restarting wouldn't fix the issue. I think having a more concrete example here would help. I bet you are referring to #10337 ?
</comment><comment author="spinscale" created="2015-03-31T10:56:14Z" id="88041699">@javanna exactly, intended for cases like that. But totally up for discussion, IMO it would be useful
</comment><comment author="dakrone" created="2015-03-31T15:04:22Z" id="88123885">What about implementing this feature: #1990? We already have the infrastructure for it, and then if one node needed to have the log level changed you could edit `logging.yml` and that particular node would pick it up without incrementing the cluster state.
</comment><comment author="ppf2" created="2015-08-20T20:44:02Z" id="133164761">This is helpful not just for troubleshooting the networking use cases Alex mentioned.  Imaging debugging query execution where the query is isolated to a specific index and will only hit a shard on a specific node, it will be helpful to be able to _dynamically_ enable debug/trace loggers for just this one node without having to turn it on for all nodes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch inner_hits query results in ArrayOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10334</link><project id="" key="" /><description>Hi! Please have a look at the following. Elastic is throwing an ArrayOutOfBoundsException when requesting inner_hits in my search query. The query works fine if inner_hits is not included. I feel this is a bug in elasticsearch. To  reproduce:

&lt;pre&gt;&lt;code&gt;curl -XPOST 'http://localhost:9200/twitter'

curl -XPOST 'http://localhost:9200/twitter/_mapping/tweet' -d '
{
    "tweet": {
        "properties": {
            "comments": {
                "properties": {
                    "messages": {
                        "type": "nested",
                        "properties": {
                            "message": {
                                "type" : "string", 
                                "index": "not_analyzed"
                            }   
                        }
                    } 
                }
            }
        }
    }
}'

curl -XPOST 'http://localhost:9200/twitter/tweet' -d '
{
    "comments": {
        "messages": [
            {"message": "Nice website"},
            {"message": "Worst ever"}
        ]
    }
}'

curl -XGET 'http://localhost:9200/twitter/tweet/_search' -d '
{
    "query": {
        "nested": {
            "path": "comments.messages",
            "query": {
                "match": {"comments.messages.message": "Nice website"}
            },
            "inner_hits" : {}
        }
    }
}'

Response:

{"took":54,"timed_out":false,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"twitter","shard":4,"status":500,"reason":"ArrayIndexOutOfBoundsException[-1]"}]},"hits":{"total":1,"max_score":1.4054651,"hits":[]}}

&lt;/code&gt;&lt;/pre&gt;


Should the document not have been returned with the "Nice website" comment in the inner_hits array?
</description><key id="65382224">10334</key><summary>Elasticsearch inner_hits query results in ArrayOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mariusdw</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-03-31T06:59:16Z</created><updated>2015-04-03T07:19:31Z</updated><resolved>2015-04-03T07:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-31T07:28:36Z" id="87975283">@mariusdw This is a bug. Elasticsearch mistakes the `comments` to be a nested object field and in your example it is just an object field.

This error will be fixed, but your mapping design (an object field that has a nested object field) raises a question. Elasticsearch indexes the `messages` json objects in a special way so that it can be used by nested query, nested sorting, inner hits etc. But the parent field is an object field and no special indexing happens there, so the nested features only work on the `messages` nested level. Is there a special reason why you chose this? It only makes sense if you have a single comment per field, otherwise I sugegst that you change the `comments` field to be of type `nested` too.
</comment><comment author="mariusdw" created="2015-03-31T08:54:15Z" id="88000395">Hi Martijn. Thanks for your reply. The example was just to reproduce the issue that I am experiencing with my real data structure. I kind of modified the example given in the elastic documentation to achieve this and maybe in the process of trying to simplify, used an example that isn't "practical" :)

Maybe I should rather explain my real data structure. In our system, we store configuration for various hardware devices. There are two levels of settings that can be configured: 
1. Individual settings - settings that apply to an individual device only.
2. Group settings - settings that apply to all devices unless overridden by an individual setting.

Each configuration server reports these settings (together with some other data) to a central server that stores it as JSON documents in elasticsearch. We then do interesting things like check which percentage of devices that has a certain setting is online etc.

As there will only ever be one "group settings" for all devices of a certain type, I have decided to store this as a simple singular object inside my document. The individual settings for each device is then stored inside this single object as nested documents. 

Looking at your answer I think a simple workaround for me for now would be to change the "group settings" to also be a nested document. 
</comment><comment author="martijnvg" created="2015-03-31T20:55:01Z" id="88241504">@mariusdw That decision to use object field makes perfect sense. When PR #10353 gets in inner hits will work again with your mapping. 

Changing the group settings to nested field will make it work for now, but does increase memory usage (due the fact that you have a nested `nested` field). I suggest that you move back to object field when 1.5.1 gets out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid NPE during query parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10333</link><project id="" key="" /><description>Workaround until Jackson is upgraded to 2.6.x.
</description><key id="65380109">10333</key><summary>Avoid NPE during query parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T06:42:50Z</created><updated>2015-06-08T00:17:45Z</updated><resolved>2015-04-06T03:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="masaruh" created="2015-03-31T06:48:01Z" id="87966308">In case #10124 isn't feasible...
</comment><comment author="s1monw" created="2015-04-02T10:31:12Z" id="88861487">I think this looks good! Can you please sign the CLA for this contribution? @kimchy can you please take another look at this too?
</comment><comment author="kimchy" created="2015-04-02T14:04:59Z" id="88917617">I made a small comment here, I think it would be cleaner not to rely on null value for the char array, but use the length and check if its 0. If agreed, +1 on pushing it, thanks @masaruh!
</comment><comment author="masaruh" created="2015-04-03T05:51:22Z" id="89175853">Thanks @kimchy! Addressed your comment along with a few changes to make tests pass (rebase, include license header, extend ElasticsearchTestCase).
</comment><comment author="kimchy" created="2015-04-05T10:48:26Z" id="89752251">@masaruh left another really small comment, other than that, LGTM
</comment><comment author="masaruh" created="2015-04-06T03:59:03Z" id="89911481">Thanks @kimchy. Fixed and merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix hole intersection at tangential coordinate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10332</link><project id="" key="" /><description>OGC SFA 2.1.10 assertion 3 (https://portal.opengeospatial.org/files/?artifact_id=829 ) allows interior boundaries to touch exterior boundaries provided they intersect at a single point. Issue #9511 provides an example where a valid shape is incorrectly interpreted as invalid (a false violation of assertion 3).  When the intersecting point appears as the first and last coordinate of the interior boundary in a polygon, the ShapeBuilder incorrectly counted this as multiple intersecting vertices. 

The fix required a little more than just a logic check. Passing the duplicate vertices resulted in a connected component in the edge graph causing a false invalid self crossing polygon. This required additional logic to the edge assignment in order to correctly segment the connected components. Finally, an additional hole validation has been added along with proper unit tests for testing valid and invalid conditions (including dateline crossing polys).

closes #9511
</description><key id="65343343">10332</key><summary>Fix hole intersection at tangential coordinate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.5</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-31T01:54:46Z</created><updated>2015-05-29T16:25:21Z</updated><resolved>2015-04-10T17:46:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-04-01T14:38:54Z" id="88508654">Thanks for the feedback @rjernst.  You're pretty busy with field mappings so maybe @colings86 has some feedback?
</comment><comment author="s1monw" created="2015-04-02T10:32:04Z" id="88861759">@colings86 I assigned you here
</comment><comment author="nknize" created="2015-04-02T19:06:48Z" id="89012752">Many thanks for the catch @clement-tourriere!  I suppose this is a good time for us to re-enable our random shape testing.

I've added a commit that catches self-loops during edge creation and throws an InvalidShapeException if found.  This does add more rigor to the shape validation, but it also complies with OGC SFA 6.1.6.1 while staying true to the "fail early" design.
</comment><comment author="clement-tourriere" created="2015-04-03T15:12:16Z" id="89315911">This kind of validation could be problematic. JTS library considers these kind of shapes as valid. So we can't easily know, before sending shapes to ES cluster, which ones will be valid. I really think that ignoring duplicate sequential coordinates (excluding first and last) in ring could be a better solution.
</comment><comment author="nknize" created="2015-04-03T18:55:17Z" id="89391376">You raise an interesting point re: pre-validation.  JTS violates other OGC specs (e.g., orientation) and handles dateline/pole wrapping differently, so I wouldn't necessarily suggest it as ground truth for pre-validation.  Many of the JTS issues will be refactored and corrected.  In the meantime you can pre-validate using the ES java API prior to sending it to the cluster.  Its an extra step but more cost effective than validating with JTS - and you'll get ground truth w.r.t ES expectations.
</comment><comment author="colings86" created="2015-04-10T15:29:20Z" id="91593109">@nknize left one more error message comment but otherwise LGTM
</comment><comment author="nknize" created="2015-04-10T17:46:12Z" id="91634341">merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10331</link><project id="" key="" /><description /><key id="65306665">10331</key><summary>typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">olistik</reporter><labels><label>docs</label></labels><created>2015-03-30T21:32:06Z</created><updated>2015-04-05T13:15:12Z</updated><resolved>2015-04-05T13:15:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T07:23:27Z" id="87973208">@clintongormley can you have a look? I'm not sure why this page is only on our master branch. Maybe it should not even be under core repo?
</comment><comment author="clintongormley" created="2015-04-05T13:15:04Z" id="89769693">@javanna the reason it is only on master is that we don't need versioned docs.

@olistik merged - thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add properties files to configure startup and installation scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10330</link><project id="" key="" /><description>Many scripts are used to start/stop and install/uninstall elasticsearch. These scripts share a lot of configuration properties like directory paths, max value for a setting, default user etc. Most of the values are identical but some of them are different depending of the platform (Debian-based or Redhat-based OS), depending of the way elasticsearch is started (shell script, systemd, sysv-init...) or the way it is installed (zip, rpm, deb...). Today the values are duplicated in multiple places, making it difficult to maintain the scripts or to update a value.

This pull request make this more uniform: values used in scripts must be defined in a common `packaging.properties` file. Each value can be overridden in another specific `packaging.properties` file for Debian or Redhat. All startup and installation scripts are filtered with the common then the custom `packaging.properties` files before being packaged as a zip/tar.gz/rpm/dpkf archive.

Also, this functionality will be very helpful when mutualizing scripts between rpm and debian packages (work in progress in another pull request)
</description><key id="65284726">10330</key><summary>Add properties files to configure startup and installation scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T19:35:48Z</created><updated>2015-05-29T18:18:44Z</updated><resolved>2015-04-02T16:11:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-30T19:38:21Z" id="87802511">@spinscale we talked about this so I assign it to you :) I'd be glad if you have time to review it, thanks!
</comment><comment author="spinscale" created="2015-04-02T09:19:23Z" id="88838033">tested the debian package, which looks good/like before. Like the changes

LGTM
</comment><comment author="electrical" created="2015-04-02T14:11:04Z" id="88920024">Did a test with the puppet module and ran into an issue at package removal of the debian package.

```
rmdir: failed to remove `/var/lib/elasticsearch': No such file or directory
```
</comment><comment author="tlrx" created="2015-04-02T14:19:17Z" id="88924155">@electrical I don't think this is due to the changes in this PR. Do you have the same message with a RPM built from master?
</comment><comment author="electrical" created="2015-04-02T14:19:54Z" id="88924296">Rpm package works fine.
Don't have the issue with the current ( released ) debian packages.
</comment><comment author="electrical" created="2015-04-02T15:29:32Z" id="88947805">LGTM. error is gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update readme with direct link to the upgrade section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10329</link><project id="" key="" /><description>I was looking this over and thought that it would be slightly nicer to include a direct link to the "Upgrading" section. This also changes the domain from `elasticsearch.org` to `elastic.co`.
</description><key id="65283757">10329</key><summary>Update readme with direct link to the upgrade section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lchi</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T19:30:55Z</created><updated>2015-04-01T08:43:23Z</updated><resolved>2015-04-01T08:43:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T07:28:55Z" id="87975489">Thanks for your PR @lchi , may I ask you to sign our [CLA](https://www.elastic.co/contributor-agreement) so we can merge this in?
</comment><comment author="lchi" created="2015-03-31T15:06:19Z" id="88124734">Hey @javanna I signed the agreement w/ this Github username but I think that the author and committer emails didn't match the one I used in Echosign. I've updated the commit to use my personal email address and also re-signed the agreement. Let me know if there's anything else you need!
</comment><comment author="javanna" created="2015-04-01T08:43:08Z" id="88396127">thanks @lchi merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Alias information to Slow query logs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10328</link><project id="" key="" /><description>Looks like currently we dont add alias name to slow query logs, Also I doubt if it adds the filter used in alias as well .

May be we need to update -&gt; https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/search/slowlog/ShardSlowLogSearchService.java#L185 and  add alias name + Context.aliasFilter ?
</description><key id="65277657">10328</key><summary>Add Alias information to Slow query logs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirmalc</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-03-30T18:54:59Z</created><updated>2015-04-05T13:31:28Z</updated><resolved>2015-04-05T13:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T07:49:57Z" id="87983398">Marking this for discussion. Might make sense to print out the alias filter, a little less to print out the alias name as an alias might point to multiple indices and in the slowlog we want to know the concrete index that the search executes against.
</comment><comment author="nirmalc" created="2015-03-31T12:27:18Z" id="88065788">It's useful to know which app fires most of slow queries if we create separate aliases per app - however filters are more useful
</comment><comment author="clintongormley" created="2015-04-05T13:31:28Z" id="89770557">Closing as duplicate of #10044
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added context and examples to text analysis topics.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10327</link><project id="" key="" /><description>This is the first batch of changes to the analysis topic in the ES reference. In addition to reviewing for correctness, I'd like feedback on the overall approach. Is this the right level of contextual information, the right format to present the options, are the examples useful, and so on. 

Also, I have a few more-specific questions:
- Why would you want to define multiple aliases for an analyzer?
- Do the language-specific analyzers use the same language-specific defaults as the Snowball analyzer?
- What about settings that aren't currently documented? For example, the custom analyzer has offset_gap, and position_offset_gap settings that are not documented. Do we want to document all settings or address them on a case-by-case basis? (Personally, I'd prefer to document all the options and point out the ones that are "advanced options" or that we advise against using for xyz reason.)

I've added Sense examples for each of the analyzers. To be able to launch them in Sense, you need to serve the docs up from a webserver by building the docs with the --web option. For example:

```
./build_docs.pl --doc &lt;pathtolocalrepo&gt;/elasticsearch/docs/reference/index.asciidoc --chunk 1 --open --web
```

@clintongormley @palecur @rjernst 
</description><key id="65274770">10327</key><summary>Added context and examples to text analysis topics.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">debadair</reporter><labels><label>docs</label></labels><created>2015-03-30T18:41:18Z</created><updated>2016-03-08T19:32:12Z</updated><resolved>2016-03-08T19:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-31T06:20:24Z" id="87956875">Hi @debadair 

It seems you're missing a snippet:

```
classic-tokenizer.asciidoc: line 30: include file not found: /Users/clinton/workspace/elasticsearch/docs/reference/snippets/analysis/classic-tokenizer.json
```
</comment><comment author="debadair" created="2015-03-31T16:55:22Z" id="88170557">Thanks, Clint. Updated with the missing snippet &amp; fixed an error in the sample output from the simple analyzer.
</comment><comment author="clintongormley" created="2015-04-14T19:51:25Z" id="93039831">Looking good so far. I've just finished the analyzers section. Will need to continue with the rest later :)
</comment><comment author="debadair" created="2015-04-17T00:24:24Z" id="93867267">@clintongormley Thanks for the thorough review--very helpful! If you've made it through all of the analyzers, that's as far as I've gotten. I'm currently working on the tokenizers. (Well, except I'm mostly working on the Watcher docs ATM.)
</comment><comment author="clintongormley" created="2015-05-03T20:05:10Z" id="98533923">@debadair let's do this in steps then, ie improve the analyzer docs and push, then open a separate PR for tokenizers, token filters, and char filters.  Otherwise it is way too much to review in one sitting :)

Also, just noticed in some of the changes that you're using tables for parameter layouts in some places (which is often what was there before). Instead I'd recommend using horizontal definition lists (which get rendered as tables in HTML but render better than tables in eg PDF), eg:

```
[horizontal]
`some_param`::  A param which does something.
`other_param`::
+
--
A longer description of what other param does, plus, an embedded code snippet:

[source,js]
---------
some code
---------

Still more text, ended by `--`
--
`third_option`::  etc
```
</comment><comment author="clintongormley" created="2016-03-08T19:32:12Z" id="193934732">Closing as very out of date
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score: Apply `min_score` to sub query score if no function provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10326</link><project id="" key="" /><description>For optimization purposes a function score query with an empty function
will just result in the original sub query. However, sometimes one might
want to use function_score query to actually filter out docs within for example
bool clauses by using the min_score functionality.
Therefore the sub query should only be used without wrapping inside
a function_score query if min_score was also not set.

closes #10253
</description><key id="65268302">10326</key><summary>Function score: Apply `min_score` to sub query score if no function provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T18:05:26Z</created><updated>2015-06-08T00:18:01Z</updated><resolved>2015-03-31T14:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-31T08:02:14Z" id="87988171">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test]&#160;awaitBusy: add a ceiling to max sleep time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10325</link><project id="" key="" /><description>[test] awaitBusy: add a ceiling to max sleep time
When using `awaitBusy`, sometimes, you might not want to double time between two runs in an infinitive manner.

For example, let's say it will probably take 30 seconds to run a test.
When doubling all the time, you will most likely wait for a bigger time than needed:

| iteration | ms | s | duration (ms) | duration (s) |
| --- | --- | --- | --- | --- |
| 1 | 1 | 0,001 | 1 | 0,001 |
| 2 | 2 | 0,002 | 3 | 0,003 |
| 3 | 4 | 0,004 | 7 | 0,007 |
| 4 | 8 | 0,008 | 15 | 0,015 |
| 5 | 16 | 0,016 | 31 | 0,031 |
| 6 | 32 | 0,032 | 63 | 0,063 |
| 7 | 64 | 0,064 | 127 | 0,127 |
| 8 | 128 | 0,128 | 255 | 0,255 |
| 9 | 256 | 0,256 | 511 | 0,511 |
| 10 | 512 | 0,512 | 1023 | 1,023 |
| 11 | 1024 | 1,024 | 2047 | 2,047 |
| 12 | 2048 | 2,048 | 4095 | 4,095 |
| 13 | 4096 | 4,096 | 8191 | 8,191 |
| 14 | 8192 | 8,192 | 16383 | 16,383 |
| 15 | 16384 | 16,384 | 32767 | 32,767 |
| 16 | 32768 | 32,768 | 65535 | 65,535 |
| 17 | 65536 | 65,536 | 131071 | 131,071 |
| 18 | 131072 | 131,072 | 262143 | 262,143 |
| 19 | 262144 | 262,144 | 524287 | 524,287 |
| 20 | 524288 | 524,288 | 1048575 | 1048,575 |
| 21 | 1048576 | 1048,576 | 2097151 | 2097,151 |

For example here, if the task is successful after 35 seconds, we will most likely have to wait for 32s more before the Predicate is run again.

With this patch, the maximum sleep time is now set to 1 second.
</description><key id="65248691">10325</key><summary>[test]&#160;awaitBusy: add a ceiling to max sleep time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T16:22:18Z</created><updated>2015-07-07T10:38:08Z</updated><resolved>2015-07-07T10:37:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-30T18:14:04Z" id="87777822">@dadoonet is it an issue that we are actually hitting, and if yes in which tests?
</comment><comment author="dadoonet" created="2015-03-30T19:44:02Z" id="87803536">Not really. Just that Twitter river tests take a loooooong time. Around 1.5 minute per test. :(
</comment><comment author="jpountz" created="2015-04-23T20:20:10Z" id="95708001">@dadoonet Any plans to iterate on this change?
</comment><comment author="dadoonet" created="2015-04-23T21:11:14Z" id="95719945">That's on my radar. Though I can't tell when I will move that forward.
Thanks for the heads up @jpountz!
</comment><comment author="dadoonet" created="2015-07-04T21:10:37Z" id="118555834">@jpountz I applied your proposal. Wondering if we want that change to be merged in.
I mean that as rivers are now removed, we won't really need to wait for a looooooong time in tests.

May be at some point if we run more integration tests...

WDYT? Merge ? Or close ?
</comment><comment author="jpountz" created="2015-07-07T09:49:05Z" id="119144237">LGTM

I just looked and this method is still used in lots of tests, so let's keep it for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: Merging Parser and Builder classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10324</link><project id="" key="" /><description>A first intermediate step in the query parsing refactoring proposed in #10217. It merges the current parser code from *QueryParser into the corresponding Builder &amp; deleting the former.

In some cases the current implementation of the Parsers require a no-arg constructor for the guice injection. In cases where the original Builder class has `final` fields these are set to either `null` or a default value where possible.
</description><key id="65244332">10324</key><summary>Query Refactoring: Merging Parser and Builder classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>non-issue</label></labels><created>2015-03-30T15:57:18Z</created><updated>2015-05-21T12:04:13Z</updated><resolved>2015-03-31T16:50:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-30T20:26:38Z" id="87821730">Part of this has me wondering if, since this is a large change, we should go ahead and do the rename similar to what Simon did in #9901 and go ahead and rename all the `FooQueryBuilder`s to `FooQuery`.

Some potential side effects:
- There can be name conflicts between Lucene queries and ES queries (like TermQuery)
- Potential confusion if we remove the Builder pattern completely

On the other hand, this does allow us to move away from the Builder pattern (if that is something we want to do) and `FooQuery` is a clearer and more succinct way of naming the queries.

I think the name conflicts issue is not particularly pressing, since end users are not expected to ever have to deal with Lucene queries, so I think that's fine to deal with.

@javanna @cbuescher what do you guys think? I'd like to get the big name-change PRs out of the way first so we can start working on individual queries.
</comment><comment author="cbuescher" created="2015-03-31T09:46:32Z" id="88017371">We could also rename FooQueryBuilder to FooQuery and add a deprecated
"FooQueryBuilder extends FooQuery" class with all old style constructors
delegating to the renamed class. Introduces lots of new classes though,
maybe not necessary if breaking the client API is not an issue here.

On Mon, Mar 30, 2015 at 10:27 PM, Lee Hinman notifications@github.com
wrote:

&gt; Part of this has me wondering if, since this is a large change, we should
&gt; go ahead and do the rename similar to what Simon did in #9901
&gt; https://github.com/elastic/elasticsearch/pull/9901 and go ahead and
&gt; rename all the FooQueryBuilders to FooQuery.
&gt; 
&gt; Some potential side effects:
&gt; - There can be name conflicts between Lucene queries and ES queries
&gt;   (like TermQuery)
&gt; - Potential confusion if we remove the Builder pattern completely
&gt; 
&gt; On the other hand, this does allow us to move away from the Builder
&gt; pattern (if that is something we want to do) and FooQuery is a clearer
&gt; and more succinct way of naming the queries.
&gt; 
&gt; I think the name conflicts issue is not particularly pressing, since end
&gt; users are not expected to ever have to deal with Lucene queries, so I think
&gt; that's fine to deal with.
&gt; 
&gt; @javanna https://github.com/javanna @cbuescher
&gt; https://github.com/cbuescher what do you guys think? I'd like to get
&gt; the big name-change PRs out of the way first so we can start working on
&gt; individual queries.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/10324#issuecomment-87821730
&gt; .

## 

Christoph B&#252;scher
</comment><comment author="dakrone" created="2015-03-31T14:59:37Z" id="88121302">I think I'm +1 on renaming to `FooQuery` and adding a deprecated `FooQueryBuilder` that can be removed at a later point. I think it might help ease the transition for the tests also.
</comment><comment author="javanna" created="2015-03-31T16:09:36Z" id="88146502">I think the main concern we have at this stage is backwards compatibility, others are secondary aspects. These classes are not going to be pure builders anyways... I am either for renaming or keeping names as they are, wouldn't rename and keep builder classes only for bw comp. We can definitely postpone this change/decision.
</comment><comment author="dakrone" created="2015-03-31T16:11:57Z" id="88148565">+1 to postpone a rename
</comment><comment author="dakrone" created="2015-03-31T16:13:48Z" id="88149456">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] move FakeRestRequest to org.elasticsearch.test.rest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10323</link><project id="" key="" /><description>FakeRestRequest is used by a few tests and can also be leveraged by
tests outside of elasticsearch. Moving the package will mean the class
gets exported as part of the test jar.
</description><key id="65219355">10323</key><summary>[TEST] move FakeRestRequest to org.elasticsearch.test.rest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T14:05:42Z</created><updated>2015-03-30T16:46:11Z</updated><resolved>2015-03-30T16:14:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-30T14:10:29Z" id="87693673">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide default paths in systemd service unit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10322</link><project id="" key="" /><description>Those defaults are enforced by the initscript in the sysvinit world.
Without them, in a systemd system, elasticsearch starts with a broken
configuration, trying to store data in /usr/share/elasticsearch/data
etc.

Also mark /etc/default/elasticsearch optional since the above
`Environment` definitions are sufficient to run elasticsearch.

cc @abravorus @t-lo @jpountz 
</description><key id="65219326">10322</key><summary>Provide default paths in systemd service unit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ctrochalakis</reporter><labels><label>:Packaging</label></labels><created>2015-03-30T14:05:33Z</created><updated>2015-05-05T13:37:03Z</updated><resolved>2015-05-05T13:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-02T10:35:37Z" id="88862192">@spinscale can you take a look?
</comment><comment author="spinscale" created="2015-04-02T10:38:41Z" id="88862548">this looks good, thanks a lot for the contribution!

@tlrx can we incorporate this into #10330 to write this file using the new properties as well (or change this PR after you got #10330 in)?
</comment><comment author="tlrx" created="2015-04-02T10:49:02Z" id="88864079">@spinscale I made something similar yesterday using #10330 (prevent ES from starting on Debian Jessie). I'll try to see how incorporate this.
</comment><comment author="tlrx" created="2015-04-20T14:13:58Z" id="94464220">@ctrochalakis Can you please rebase this? The file has moved to `src/packaging/deb/systemd/elasticsearch.service`. Thanks a lot :)
</comment><comment author="ctrochalakis" created="2015-04-21T08:06:48Z" id="94693873">@tlrx The proposed changes were applied in 0dad33f17f3a3dc62e8226cdeced5e17b6976459, it should be working now!
</comment><comment author="tlrx" created="2015-04-21T08:10:16Z" id="94696273">@ctrochalakis yeah, I did it before you can rebase, sorry for that. Note that the SystemD files will be merged soon.
</comment><comment author="tlrx" created="2015-05-05T13:37:02Z" id="99079584">Merged in https://github.com/elastic/elasticsearch/commit/0dad33f17f3a3dc62e8226cdeced5e17b6976459
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>exclude._ip transient cluster setting not working in ES 1.4 or 1.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10321</link><project id="" key="" /><description>We use the exclude._ip cluster setting to push data from nodes when updating our cluster and this doesn't seem to be working in ES 1.4 or 1.5. This is what our cluster settings look like, but shards on the node with the excluded ip do not get pushed from the node:

``` json
{
    persistent: { },
    transient: {
        cluster: {
            routing: {
                allocation: {
                    exclude: {
                        _ip: "x.x.x.x"
                    }
                }
            }
        }
    }
}
```
</description><key id="65218603">10321</key><summary>exclude._ip transient cluster setting not working in ES 1.4 or 1.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dhaworth</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2015-03-30T14:00:57Z</created><updated>2015-04-29T08:27:24Z</updated><resolved>2015-04-29T08:27:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T12:54:42Z" id="89766771">Hi @dhaworth 

Are you sure that your shards have somewhere to go?  Try using the cluster reroute API with explain to understand why your shards are not being moved.  
</comment><comment author="clintongormley" created="2015-04-26T20:04:47Z" id="96429334">Any more info?
</comment><comment author="dhaworth" created="2015-04-27T18:26:05Z" id="96770580">Apologies for the delay. I currently have a 3 node cluster in our staging environment where all nodes are both master and data nodes. If I exclude one of the nodes, the shards on that node should get pushed to the other two, but they aren't.

This is the explanation I get from the cluster reroute API, which seems like it may be suspect, but I'm not exactly sure how to interpret it.

``` json
"explanations" : [ {
  "command" : "move",
  "parameters" : {
    "index" : "decide",
    "shard" : 0,
    "from_node" : "i-xxxxxxxx",
    "to_node" : "i-yyyyyyyy"
  },
  "decisions" : [ {
    "decider" : "move_allocation_command",
    "decision" : "NO",
    "explanation" : "shard [decide][0] not found"
  } ]
} ]
```

Let me know if you need additional info. Thanks.
</comment><comment author="clintongormley" created="2015-04-28T08:49:38Z" id="96980290">It seems it is not finding shard zero of index `decide` on the `from_node`.  Check the location of your shards with:

```
GET _cluster/state/routing_table/decide
```
</comment><comment author="dhaworth" created="2015-04-28T13:06:04Z" id="97054180">That's interesting because I don't have an index named `decide` on this cluster, or any other cluster I've had this problem on. That routing table api call returns this:

``` json
{
    cluster_name: "Analytics15",
    routing_table: {
        indices: { }
    },
    routing_nodes: {
        unassigned: [ ],
        nodes: { }
    },
    allocations: [ ]
}
```

I don't see the word `decide` anywhere in my elasticsearch.yml or in `GET _cluster/state`. I do have the following plugins installed in case they may somehow be at fault:
- kopf
- HQ
- bigdesk
- kibana (which isn't technically a plugin anymore, but I do have installed on the same instance)

Any thoughts on where this may be coming from?
</comment><comment author="clintongormley" created="2015-04-28T17:36:35Z" id="97145861">Could you paste the output of the cluster-reroute command that you issue? (assuming that the output is still the same?)
</comment><comment author="dhaworth" created="2015-04-28T18:00:36Z" id="97154212">Ugh, sorry. That `decide` index thing was my fault. I tried it with an actual index and node and got the following:

``` json
"explanations" : [ {
  "command" : "move",
  "parameters" : {
    "index" : "analytics-2014-01",
    "shard" : 4,
    "from_node" : "i-xxxxxxxx",
    "to_node" : "i-yyyyyyyy"
  },
  "decisions" : [ {
    "decider" : "same_shard",
    "decision" : "YES",
    "explanation" : "shard is not allocated to same node or host"
  }, {
    "decider" : "filter",
    "decision" : "YES",
    "explanation" : "node passes include/exclude/require filters"
  }, {
    "decider" : "replica_after_primary_active",
    "decision" : "YES",
    "explanation" : "shard is primary"
  }, {
    "decider" : "throttling",
    "decision" : "YES",
    "explanation" : "below shard recovery limit of [2]"
  }, {
    "decider" : "enable",
    "decision" : "YES",
    "explanation" : "allocation disabling is ignored"
  }, {
    "decider" : "disable",
    "decision" : "YES",
    "explanation" : "allocation disabling is ignored"
  }, {
    "decider" : "awareness",
    "decision" : "YES",
    "explanation" : "no allocation awareness enabled"
  }, {
    "decider" : "shards_limit",
    "decision" : "YES",
    "explanation" : "total shard limit disabled: [-1] &lt;= 0"
  }, {
    "decider" : "node_version",
    "decision" : "YES",
    "explanation" : "target node version [1.5.0] is same or newer than source node version [1.5.0]"
  }, {
    "decider" : "disk_threshold",
    "decision" : "YES",
    "explanation" : "enough disk for shard on node, free: [138.7gb]"
  }, {
    "decider" : "snapshot_in_progress",
    "decision" : "YES",
    "explanation" : "no snapshots are currently running"
  } ]
```

The ip address of the `to_node` in this case is excluded:

``` json
{
    persistent: { },
    transient: {
        cluster: {
            routing: {
                allocation: {
                    exclude: {
                        _ip: "y.y.y.y"
                    }
                }
            }
        }
    }
}
```
</comment><comment author="clintongormley" created="2015-04-28T18:18:23Z" id="97160874">It says:

```
{
    "decider" : "filter",
    "decision" : "YES",
    "explanation" : "node passes include/exclude/require filters"
  }, 
```

which makes me think that the IP you're using is not the same as the IP returned by `getHostAddress()` (see https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeFilters.java#L73). This may not be the same as the bind or publish address.  Could you check for this?
</comment><comment author="dhaworth" created="2015-04-28T19:55:24Z" id="97187069">It looks like you're right. We recently switched from Amazon Linux (CentOS) to Ubuntu and it looks like the ip address of every node according to ES is now 127.0.1.1. This certainly explains the exclude not working. It looks like I have a configuration issue, either in `/etc/hosts` or with ES itself.

If you know a quick fix for this I'd appreciate any advice, otherwise I'll figure it out on my own. Thanks for helping me track this down.
</comment><comment author="clintongormley" created="2015-04-29T08:27:23Z" id="97351280">I don't know of a quick trick, but glad we could get to the bottom of this.  thanks for reporting back
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10320</link><project id="" key="" /><description>change description to better fit the flag name
</description><key id="65214748">10320</key><summary>Update scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T13:39:59Z</created><updated>2015-03-31T07:29:56Z</updated><resolved>2015-03-31T07:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T07:29:55Z" id="87976148">thanks @peschlowp merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add getter for channel in NettyTransportChannel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10319</link><project id="" key="" /><description>Adds a getter for the actual netty channel in NettyTransportChannel. The
channel can be used by plugins that need access into netty when processing
requests.
</description><key id="65211875">10319</key><summary>Add getter for channel in NettyTransportChannel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T13:27:34Z</created><updated>2015-06-07T16:30:34Z</updated><resolved>2015-03-30T16:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-30T14:17:27Z" id="87696467">LGTM - can you add a doc string
</comment><comment author="jaymode" created="2015-03-30T15:59:10Z" id="87734939">will add doc and push. thanks @s1monw 
</comment><comment author="s1monw" created="2015-03-30T16:06:22Z" id="87736649">thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Endless mapping refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10318</link><project id="" key="" /><description>- We're running ES 1.4.2
- Can sometimes be solved by closing/re-opening affected indices, but the issue usually returns prettys soon
- This "clogs" up pending tasks and some tasks get stuck / cluster actions will now always timeout (e.g. loading/removing warmers)
- This consumes all network capacity on the ES nodes

https://gist.github.com/EikeDehling/a015a5137ac5d99dc850

[buzzcapture@hermes ~]$ curl http://artemis3:9200/_cat/pending_tasks
18149502    1s HIGH   refresh-mapping [postings-5360000000][[posting]] 
18149503 745ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149509 736ms HIGH   refresh-mapping [postings-5360000000][[posting]] 
18149504 737ms HIGH   refresh-mapping [postings-4180000000][[posting]] 
18149510 735ms HIGH   refresh-mapping [postings-5190000000][[posting]] 
18149512 735ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149506 736ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149505 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149511 735ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149515 732ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149519 290ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149521 289ms HIGH   refresh-mapping [postings-5190000000][[posting]] 
18149513 734ms HIGH   refresh-mapping [postings-5100000000][[posting]] 
18149525 287ms HIGH   refresh-mapping [postings-5100000000][[posting]] 
18149507 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149508 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149514 733ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149516 299ms HIGH   refresh-mapping [postings-4180000000][[posting]] 
18149517 298ms HIGH   refresh-mapping [postings-5360000000][[posting]] 
18149518 291ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
12674966  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  
18149520 290ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
12676681  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  
18149522 289ms HIGH   refresh-mapping [postings-5190000000][[posting]] 
18149523 288ms HIGH   refresh-mapping [postings-5100000000][[posting]] 
18149524 288ms HIGH   refresh-mapping [postings-4180000000][[posting]] 
12678378  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  
18149526 286ms HIGH   refresh-mapping [postings-5430000000][[posting]] 
18149527 286ms HIGH   refresh-mapping [postings-5190000000][[posting]] 
18149528 286ms HIGH   refresh-mapping [postings-4180000000][[posting]] 
18149529 286ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149530 284ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149531 284ms HIGH   refresh-mapping [postings-5360000000][[posting]] 
18149532 284ms HIGH   refresh-mapping [postings-5100000000][[posting]] 
18149533 284ms HIGH   refresh-mapping [postings-5500000000][[posting]] 
18149534 281ms HIGH   refresh-mapping [postings-5360000000][[posting]]
</description><key id="65210200">10318</key><summary>Endless mapping refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EikeDehling</reporter><labels><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T13:22:04Z</created><updated>2015-04-02T20:01:47Z</updated><resolved>2015-04-02T20:01:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-30T19:53:17Z" id="87807286">@EikeDehling can you enable debug logging for `indices.cluster` and grep the logs for the output of this line? I want to see what the difference is between the two sources . A gist will be great.

```
logger.debug("[{}] parsed mapping [{}], and got different sources\noriginal:\n{}\nparsed:\n{}", index, mappingType, mappingSource, mapperService.documentMapper(mappingType).mappingSource());
```
</comment><comment author="EikeDehling" created="2015-03-31T08:43:42Z" id="87997308">@bleskes Thanks for the quick reponse!

Gist here: https://gist.github.com/EikeDehling/129aa3f8213ad8552f49

The difference in mapping appears to be in nested elements, apparently they are not ordered alphbetically? The difference in serialisation is under posting.properties.body.fields._text_.fielddata , there entries there are ordered differently in the original/parsed version.
</comment><comment author="EikeDehling" created="2015-03-31T17:52:13Z" id="88187224">This gist is a bit easier to read:

https://gist.github.com/EikeDehling/fc1289cc443b7acdc3f4

The issue is under the key `posting.properties.body.fields._text_.fielddata` : Ordering is different for the original/parsed mapping.
</comment><comment author="bleskes" created="2015-03-31T19:27:29Z" id="88218410">@EikeDehling thx for that. It's accurate. The problem lies in the way the field data settings are rendered:

https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java#L756

```
builder.field("fielddata", (Map) fieldDataType.getSettings().getAsMap());
```

The order of the keys in that map is arbitrary (practically). It may be different between master and nodes causing this endless loop.

To work around this, you can set `indices.cluster.send_refresh_mapping` to false (requires node restart). This will disable the sending of mapping refresh instructions. You must remember to remove this settings before you upgrade to the next ES version, which will have a fix for this.
</comment><comment author="bleskes" created="2015-03-31T19:36:04Z" id="88220290">@EikeDehling do you run on Java8 by any chance? (wondering to better understand how frequently this can happen)
</comment><comment author="EikeDehling" created="2015-04-01T08:25:01Z" id="88391557">We're running Java7, 1.7.0_45

Thanks for the tip about settings.

I also found that line of code indeed, i'll try and make a patch/test.
</comment><comment author="bleskes" created="2015-04-01T08:32:54Z" id="88393428">Cool. If you wait an hour or two, I&#8217;ll probably make a PR with a fix.

&gt; On 01 Apr 2015, at 10:25, EikeDehling notifications@github.com wrote:
&gt; 
&gt; We're running Java7, 1.7.0_45
&gt; 
&gt; Thanks for the tip about settings.
&gt; 
&gt; I also found that line of code indeed, i'll try and make a patch/test.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="EikeDehling" created="2015-04-01T08:45:12Z" id="88397289">This is my initial patch+unit test, happy to compare to what you're producing. Sorry, i'm not that handy with github/PR's yet.

https://gist.github.com/EikeDehling/2e34a78a54de646b71ca

Any chance there will also be a 1.4 release with a fix?
</comment><comment author="wkoot" created="2015-04-01T09:24:24Z" id="88410704">@bleskes You said that the `indices.cluster.send_refresh_mapping` requires node restart, what would the effect be if you only have a few (say half of) the nodes which have this setting set to false?
</comment><comment author="bleskes" created="2015-04-01T09:28:23Z" id="88412352">Then the other half might still send refresh mapping to the master. You need it on all data nodes. But you can do a rolling restart, one by one.

&gt; On 01 Apr 2015, at 11:24, wkoot notifications@github.com wrote:
&gt; 
&gt; @bleskes You said that the indices.cluster.send_refresh_mapping requires node restart, what would the effect be if you only have a few (say half of) the nodes which have this setting set to false?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="EikeDehling" created="2015-04-01T09:47:52Z" id="88418239">I am trying this fix in our staging environment, i'll let you know if that fixes the issue.

https://github.com/EikeDehling/elasticsearch/commit/cc79d71bbc4d55cb12a50df2acc67ca6ba4ac5dc
</comment><comment author="bleskes" created="2015-04-01T11:43:51Z" id="88448217">@EikeDehling looks good can you make a PR ? see https://www.elastic.co/contributing-to-elasticsearch . Also it would be great if you simplify the test and add random keys (but we can iterate on the PR). 
</comment><comment author="EikeDehling" created="2015-04-01T14:52:53Z" id="88512526">I made a PR, and afterwards signed the contributor license, i hope that's ok.

I randomized the test and simplified a bit, happy to hear suggestions for improvements.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch exceptions from cluster state listeners when they are notified?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10317</link><project id="" key="" /><description>When the cluster state service is closed then all listeners are notified. We currently do not handle unchecked Exceptions that are thrown when `ObserverClusterStateListener.onClose()` is called, see for example https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L167 . This might be trappy because it means that all listeners that come after one that caused an exception will not be notified and code after listener notification will not be executed as well. I ran into this in https://github.com/elastic/elasticsearch/pull/10172 : custom implementation of `onClose()` caused a `EsRejectedExecutionException` which then caused thread leaks in tests because the exception was not caught. 

Should we always handle unchecked exceptions in general when we notify listeners?
</description><key id="65207118">10317</key><summary>Catch exceptions from cluster state listeners when they are notified?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2015-03-30T13:05:41Z</created><updated>2015-12-08T17:15:47Z</updated><resolved>2015-12-08T17:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T22:03:18Z" id="162251076">@brwe is this still relevant?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor state format to use incremental state IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10316</link><project id="" key="" /><description>Today there is a chance that the state version for shard, index or cluster
state goes backwards or is reset on a full restart etc. depending on
several factors not related to the state. To prevent any collisions
with already existing state files and to maintain write-once properties
this change introductes an incremental state ID instead of using the plain
state version. This also fixes a bug when the previous legacy state had a
greater version than the current state which causes an exception on node
startup or if left-over files are present.
</description><key id="65192866">10316</key><summary>Refactor state format to use incremental state IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T11:46:06Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-03-31T12:18:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-30T16:49:21Z" id="87747047">@bleskes pushed a new commit
</comment><comment author="bleskes" created="2015-03-30T19:43:13Z" id="87803387">I left one comment regarding the use of versions in determining the counter. Another thing I was wondering about is how we deal with the scenario 1.5.0 can live us in - a higher id legacy file, with a lower id and non legacy file.
</comment><comment author="s1monw" created="2015-03-30T19:46:32Z" id="87804366">&gt; I left one comment regarding the use of versions in determining the counter. Another thing I was wondering about is how we deal with the scenario 1.5.0 can live us in - a higher id legacy file, with a lower id and non legacy file.

we don't deal with that at all. IMO this requires user interaction - no way to resolve it automatically and I don't think we should unless it's evident that there is a real problem here that happens regularly.
</comment><comment author="bleskes" created="2015-03-30T19:48:36Z" id="87805043">&gt;  I don't think we should unless it's evident that there is a real problem here that happens regularly.

Fair enough. Let's wait and see.
</comment><comment author="s1monw" created="2015-03-31T10:50:42Z" id="88040380">@bleskes can you take another look?
</comment><comment author="bleskes" created="2015-03-31T11:15:13Z" id="88048647">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>grand parent/parent/child aggregation (NullPointerException)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10315</link><project id="" key="" /><description>Hi,

I have tested against 1.4.3 and 1.5.0 and I am getting NPE during the aggreagation below:

MAPPINGS:

&lt;PRE&gt;
    "user": {
      "dynamic": "false",
      "_timestamp": {
        "enabled": false,
        "format": "date_optional_time"
      },
      "_index": {
        "enabled": false
      },
      "_ttl": {
        "enabled": false
      },
      "_size": {
        "enabled": false
      },
      "properties": {
        "tags": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        },
        "revenue_t": {
          "type": "double"
        },
        "revenue_avgd": {
          "type": "double"
        },
        "segments": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        },
        "install_date": {
          "format": "dateOptionalTime",
          "type": "date"
        },
        "revenue_avgm": {
          "type": "double"
        },
        "user_id": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        },
        "slot": {
          "type": "long"
        },
        "profile": {
          "properties": {
            "age": {
              "type": "long"
            },
            "team": {
              "index": "not_analyzed",
              "postings_format": "default",
              "similarity": "default",
              "fielddata": {
                "loading": "lazy"
              },
              "type": "string"
            }
          }
        }
      },
      "_all": {
        "enabled": false
      }
    },


    "session": {
      "dynamic": "false",
      "_source": {
        "enabled": false
      },
      "_routing": {
        "required": true
      },
      "_timestamp": {
        "enabled": false,
        "format": "date_optional_time"
      },
      "_index": {
        "enabled": false
      },
      "_ttl": {
        "enabled": false
      },
      "_size": {
        "enabled": false
      },
      "properties": {
        "spid": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        },
        "sts": {
          "format": "dateOptionalTime",
          "type": "date"
        }
      },
      "_all": {
        "enabled": false
      },
      "_parent": {
        "type": "user"
      }
    },
    

    "event": {
      "dynamic": "false",
      "_source": {
        "enabled": false
      },
      "_routing": {
        "required": true
      },
      "_timestamp": {
        "enabled": false,
        "format": "date_optional_time"
      },
      "_index": {
        "enabled": false
      },
      "_ttl": {
        "enabled": false
      },
      "_size": {
        "enabled": false
      },
      "properties": {
        "con": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        },
        "ets": {
          "format": "dateOptionalTime",
          "type": "date"
        },
        "data": {
          "properties": {
            "balance": {
              "type": "double"
            },
            "level": {
              "type": "long"
            },
            "page": {
              "index": "not_analyzed",
              "postings_format": "default",
              "similarity": "default",
              "fielddata": {
                "loading": "lazy"
              },
              "type": "string"
            },
            "invoiceID": {
              "index": "not_analyzed",
              "postings_format": "default",
              "similarity": "default",
              "fielddata": {
                "loading": "lazy"
              },
              "type": "string"
            },
            "tas": {
              "type": "long"
            },
            "pid": {
              "index": "not_analyzed",
              "postings_format": "default",
              "similarity": "default",
              "fielddata": {
                "loading": "lazy"
              },
              "type": "string"
            }
          }
        },
        "et": {
          "index": "not_analyzed",
          "postings_format": "default",
          "similarity": "default",
          "fielddata": {
            "loading": "lazy"
          },
          "type": "string"
        }
      },
      "_all": {
        "enabled": false
      },
      "_parent": {
        "type": "session"
      }
    }

&lt;/PRE&gt;


SAMPLE DATA:

USER:

&lt;PRE&gt;
{
  "_index": "testapp3",
  "_type": "user",
  "_id": "bD0CGpdVOAbF",
  "_version": 1,
  "_score": 1,
  "_source": {
    "user_id": "bD0CGpdVOAbF",
    "slot": 0,
    "segments": [
      "yandas trol",
      "dayisi bakan"
    ],
    "tags": [
      "asia"
    ],
    "install_date": "2015-04-12T06:16:54.787Z",
    "revenue_t": 300.6897,
    "revenue_avgd": 0,
    "revenue_avgm": 0,
    "profile": {
      "team": "karabukspor",
      "age": 34
    }
  }
}
&lt;/PRE&gt;


SESSION

&lt;PRE&gt;
{
  "_index": "testapp3",
  "_type": "session",
  "_id": "nKyJHGgCBl9t5oG7",
  "_version": 1,
  "_score": 1,
  "_source": {
    "spid": "4Jn7mOG6dtkrimdKQFUETmCD",
    "sts": "2015-04-20T20:11:20.209Z"
  },
  "fields": {
    "_parent": "bD0CGpdVOAbF"
  }
}
&lt;/PRE&gt;

EVENT:

&lt;PRE&gt;
{
  "_index": "testapp3",
  "_type": "event",
  "_id": "AUxqJOcTwN0lu_3PUVp2",
  "_version": 1,
  "_score": 1,
  "_source": {
    "et": "UserBalance",
    "ets": "2015-04-20T20:23:20.209Z",
    "con": "cellular",
    "data": {
      "balance": 8.722086988665563
    }
  },
  "fields": {
    "_parent": "nKyJHGgCBl9t5oG7"
  }
}
&lt;/PRE&gt;


QUERY:

&lt;PRE&gt;
{
  "query": {
    "match_all": {
      
    }
  },
  "from": 0,
  "size": 10,
  "explain": false,
  "version": false,
  "aggs": {
    "segments": {
      "terms": {
        "field": "segments",
        "size": 20
      },
      "aggs": {
        "to_session": {
          "children": {
            "type": "session"
          },
          "aggs": {
            "to_event": {
              "children": {
                "type": "event"
              },
              "aggs": {
                "filtered": {
                  "filter": {
                    "term": {
                      "et": "UserBalance"
                    }
                  },
                  "aggs": {
                    "average_balance": {
                      "avg": {
                        "field": "data.balance"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

&lt;/PRE&gt;


ERROR:

&lt;PRE&gt;
[2015-03-30 13:40:45,265][DEBUG][action.search.type       ] [Tagak the Leopard Lord] All shards failed for phase: [query]
org.elasticsearch.search.query.QueryPhaseExecutionException: [testapp3][3]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:286)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:297)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.doPostCollection(ParentToChildrenAggregator.java:149)
        at org.elasticsearch.search.aggregations.Aggregator.postCollection(Aggregator.java:335)
        at org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.doPostCollection(ParentToChildrenAggregator.java:186)
        at org.elasticsearch.search.aggregations.Aggregator.postCollection(Aggregator.java:335)
        at org.elasticsearch.search.aggregations.Aggregator.postCollection(Aggregator.java:334)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.postCollection(AggregationPhase.java:178)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:202)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 8 more

&lt;/PRE&gt;

</description><key id="65188055">10315</key><summary>grand parent/parent/child aggregation (NullPointerException)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkarakaya</reporter><labels /><created>2015-03-30T11:07:29Z</created><updated>2015-03-30T11:54:49Z</updated><resolved>2015-03-30T11:38:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-30T11:38:17Z" id="87641285">@rkarakaya Thanks for reporting. The bug has been fixed via #10263 and will be included in the upcoming 1.5.1 release.
</comment><comment author="rkarakaya" created="2015-03-30T11:54:49Z" id="87648652">thanks a lot..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update prefix-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10314</link><project id="" key="" /><description>text said phrase instead of prefix, probably due to copy-paste
</description><key id="65183554">10314</key><summary>Update prefix-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T10:36:06Z</created><updated>2015-03-31T07:29:47Z</updated><resolved>2015-03-31T07:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-31T07:29:47Z" id="87976104">Thanks @peschlowp merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Attachment search not working in Elasticsearch 1.5.0 if search string contains more than three characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10313</link><project id="" key="" /><description>I am trying to search for a given string in the content of the file. Say I have a file abc.docx and it contains the word "shark" in it. If I search for "sha", it returns the file abc.docx. However, if I search for "shar", it does not return that file i.e it seems to process only 3 character search strings while doing content search. I checked this in the latest version(1.5.0) and version 1.4.3 as well and it din't work either. This seems to be working in Elasticsearch 1.4.2. Please let me know if you need further inputs on this.
</description><key id="65165149">10313</key><summary>Attachment search not working in Elasticsearch 1.5.0 if search string contains more than three characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rhdip</reporter><labels /><created>2015-03-30T08:54:03Z</created><updated>2015-04-01T12:42:14Z</updated><resolved>2015-03-30T12:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-30T09:02:24Z" id="87601489">When you write "attachment search", do you mean that you are using the mapper attachment plugin or is it something else?
Can you post what is your mapping and how does a document which is supposed to match actually look like?
</comment><comment author="rhdip" created="2015-03-30T10:34:27Z" id="87629795">Yes I am using the mapper-attachments plugin. The file I am trying to search can be any file word, excel etc. 

I am providing the C# code. The UpdateIndex() method indexes the file name and its contents and the Search() method searches within the indexes.

The mapper class:

```
public class AttachmentES
{
    /// &lt;summary&gt;
    /// Index for Attachments table
    /// &lt;/summary&gt;    
   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public ICollection&lt;string&gt; title { get; internal set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public string name { get; internal set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public ICollection&lt;string&gt; text { get; set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public string content { get; internal set; }

   public string Id { get; internal set; }

   [ElasticProperty(Type = Nest.FieldType.Attachment, Store = false, TermVector = Nest.TermVectorOption.WithPositionsOffsets)]
   public ESAttachment File { get; set; }
}
```

The UpdateIndex method:

```
public void UpdateIndex(int attachmentID)
{
    //get the attachment details from database
    var atta = from a in dbES.GetAttachmentES(attachmentID).ToList()
               select new AttachmentES
               {
                   name = a.FileName,                      
                   Id = a.AttachmentID,                     
               };

    AttachmentES objAttachmentES = new AttachmentES();
    objAttachmentES = atta.FirstOrDefault();

    client.CreateIndex("attachment", c =&gt; c.AddMapping&lt;AttachmentES&gt;(m =&gt; m.MapFromAttributes()));

    setting.SetDefaultIndex("attachment");
    foreach (AttachmentES itm in atta)
    {

        int id;
        id = Convert.ToInt32(itm.Id.Split('_')[0]);
        var rootPath = (CommonClass.AttachmentRootPath());


        if (string.IsNullOrEmpty(itm.name) == false)
        {
            string filePath = rootPath + "\\" + id;// +new FileInfo(itm.name).Extension;


            var attachment = new ESAttachment();

            string fileExt = Path.GetExtension(itm.name);

            if (File.Exists(filePath))
                attachment._content = Convert.ToBase64String(File.ReadAllBytes(filePath));

            else if (File.Exists(filePath + fileExt))
                attachment._content = Convert.ToBase64String(File.ReadAllBytes(filePath + new FileInfo(itm.name).Extension));

            switch (new FileInfo(itm.name).Extension)
            {
                case ".txt":
                    attachment._content_type = "text/plain";
                    break;
                case ".doc":
                    attachment._content_type = "application/msword";
                    break;
                case ".docx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.wordprocessingml.document";
                    break;
                case ".xls":
                    attachment._content_type = "application/vnd.ms-excel";
                    break;
                case ".xlsx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet";
                    break;
                case ".ppt":
                    attachment._content_type = "application/vnd.ms-powerpoint";
                    break;
                case ".pptx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.presentationml.presentation";
                    break;
                case ".pdf":
                    attachment._content_type = "application/pdf";
                    break;

                //Images
                case ".png":
                    attachment._content_type = "image/png";
                    break;
                case ".jpeg":
                case ".jpg":
                    attachment._content_type = "image/jpeg";
                    break;
                case ".bmp":
                    attachment._content_type = "image/bmp";
                    break;
                case ".gif":
                    attachment._content_type = "image/gif";
                    break;

                //Videos
                case ".wmv":
                    attachment._content_type = "video/x-ms-wmv";
                    break;
                case ".avi":
                    attachment._content_type = "video/x-msvideo";
                    break;
                case ".flv":
                    attachment._content_type = "video/x-flv";
                    break;
                case ".mp4":
                    attachment._content_type = "video/mp4";
                    break;


                //zip
                case ".zip":
                    attachment._content_type = "application/zip";
                    break;

                //xml
                case ".xml":
                    attachment._content_type = "application/xml";
                    break;
            }

            attachment._name = itm.name;
            itm.File = attachment;
            client.Index&lt;AttachmentES&gt;(itm);
        }
    }
}
```

The Search method:

```
public List&lt;ESSearchResult&gt; Search(string searchString, int userId = 0)
    {
    List&lt;ESSearchResult&gt; lstIDs = new List&lt;ESSearchResult&gt;();
    ESSearchResult objItem = new ESSearchResult();

    string ElasticSearchPath = ConfigurationManager.AppSettings["ElasticSearchURL"].ToString();
    Uri o = new Uri(ElasticSearchPath);
    setting = new ConnectionSettings(o);
    client = new ElasticClient(setting);

    setting.SetDefaultIndex("attachment");

    string q1 = "*" + searchString + "*";

    var result12 = client.Search&lt;AttachmentES&gt;(body1 =&gt;
                 body1.Index("attachment").Query(query12 =&gt;
 query12.QueryString(qs1 =&gt; qs1.Query(q1))
 ).Fields("id"));

    List&lt;string&gt; lst = (from p in result12.FieldSelections
                        select p.FieldValues(item =&gt; item.Id).ElementAt(0).ToString()).ToList();

    List&lt;String&gt; lstAttachments = new List&lt;String&gt;();
    foreach (var i in lst)
    {                        
        lstAttachments.Add(i);            
    }

    objItem = new ESSearchResult();
    objItem.ObjectEntity = Enums.ESEntity.AttachmentES;
    objItem.ObjectRowIDs = lstAttachments.Distinct().ToList&lt;string&gt;();


    lstIDs.Add(objItem);

    return lstIDs;
}
```
</comment><comment author="dadoonet" created="2015-03-30T12:48:19Z" id="87664431">As far as I understand your code and mapping, you are using default analyzer for mapper attachment sub fields.
It means that if you index `shark`, you won't be able to find it if you search for `sha` term because there are obviously different.

You could define another analyzer for your attachment field though I have no idea on how you can do it with Nest. May be @Mpdreamz has an idea here.

Here is a link to the documentation which shows you how to do it using REST API: https://github.com/elastic/elasticsearch-mapper-attachments#using-mapper-attachments

May be you could use a edge n gram tokenizer. But note that it will generate a lot of sub tokens so it will consume more disk space.

The fact that `sha` matches is because I think you somehow have `sha` somewhere in your text. I don't think it's related to elasticsearch version 1.4.2. From the top of my head, I don't think also about a change in mapper attachment plugin which could cause that.

Closing as I don't see any issue here.
Feel free to reopen if you think it is or open an issue in mapper attachment project itself.
</comment><comment author="rhdip" created="2015-04-01T12:35:01Z" id="88462604">When I try to index using "**client.Index(itm);**" I get the following error:

"System.Net.WebException was caught
  HResult=-2146233079
  Message=The operation has timed out
  Source=Elasticsearch.Net
  StackTrace:
       at Elasticsearch.Net.Connection.RequestHandlers.RequestHandler.ReturnStreamOrVoidResponse[T](TransportRequestState`1 requestState, ElasticsearchResponse`1 streamResponse)
       at Elasticsearch.Net.Connection.RequestHandlers.RequestHandler.CoordinateRequest[T](TransportRequestState`1 requestState, Int32 maxRetries, Int32 retried, Boolean&amp; aliveResponse)
       at Elasticsearch.Net.Connection.RequestHandlers.RequestHandler.DoRequest[T](TransportRequestState`1 requestState)
       at Elasticsearch.Net.Connection.RequestHandlers.RequestHandler.Request[T](TransportRequestState`1 requestState, Object data)
       at Elasticsearch.Net.Connection.Transport.DoRequest[T](String method, String path, Object data, IRequestParameters requestParameters)
       at Elasticsearch.Net.ElasticsearchClient.DoRequest[T](String method, String path, Object data, IRequestParameters requestParameters)
       at Elasticsearch.Net.ElasticsearchClient.IndexPut[T](String index, String type, String id, Object body, Func`2 requestParameters)
       at Nest.RawDispatch.IndexDispatch[T](ElasticsearchPathInfo`1 pathInfo, Object body)
       at Nest.ElasticClient.&lt;&gt;c__DisplayClass671`1.&lt;Index&gt;b__675(ElasticsearchPathInfo`1 p, IndexDescriptor`1 d)
       at Nest.ElasticClient.Dispatch[D,Q,R](D descriptor, Func`3 dispatch)
       at Nest.ElasticClient.Index[T](T object, Func`2 indexSelector)
       at EmailNotificationLibrary.SOLRSearch.RefreshElasticSearchindex(String connectionString, Int32 communityID)"

This happens when I install the service ElasticSearch 1.5.0. But when I revert to ElasticSearch 1.4.2, the same line of code works fine.

P.S. I cannot find the option to re-open this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refresh if many deletes in a row use up too much version map RAM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10312</link><project id="" key="" /><description>Digging into #7052 I noticed that if an app does zillions of "delete by IDs" in a row, we fail to trigger a refresh when the version map is using too much RAM...
</description><key id="65163636">10312</key><summary>Refresh if many deletes in a row use up too much version map RAM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T08:48:49Z</created><updated>2015-06-06T19:11:39Z</updated><resolved>2015-03-30T14:25:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-30T08:49:41Z" id="87594928">LGTM in general - any change we can get a unittest triggering this? maybe in `InternalEngineTests`
</comment><comment author="mikemccand" created="2015-03-30T13:58:01Z" id="87689832">@s1monw I added a test case ... I think it's ready.
</comment><comment author="s1monw" created="2015-03-30T14:04:13Z" id="87691219">LGTM thanks mike
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't try to send a mapping refresh if there is no master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10311</link><project id="" key="" /><description>After processing mapping updates from the master, we compare the resulting binary representation of them and compare it the one cluster state has. If different, we send a refresh mapping request to master, asking it to reparse the mapping and serialize them again. This mechanism is used to update the mapping after a format change caused by a version upgrade.

The very same process can also be triggered when an old master leaves the cluster, triggering a local cluster state update. If that update contains old mapping format, the local node will again signal the need to refresh, but this time there is no master to accept the request. Instead of failing (which we now do because of #10283, we should just skip the notification and wait for the next elected master to publish a new mapping (triggering another refresh if needed).

This issue cause the failure in http://build-us-00.elastic.co/job/es_bwc_1x/9074/CHECK_BRANCH=origin%2F1.4,jdk=JDK7,label=bwc/testReport/junit/org.elasticsearch.gateway.local/RecoveryBackwardsCompatibilityTests/testReusePeerRecovery/
</description><key id="65155965">10311</key><summary>Don't try to send a mapping refresh if there is no master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T07:55:45Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-03-30T13:27:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-30T08:26:52Z" id="87589924">LGTM
</comment><comment author="kimchy" created="2015-03-30T08:34:34Z" id="87591382">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryState.File.toXContent reports file length as recovered bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10310</link><project id="" key="" /><description> Also fixes a BWC issue in setting the reused flag.
</description><key id="65155478">10310</key><summary>RecoveryState.File.toXContent reports file length as recovered bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-30T07:50:59Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-03-30T14:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-30T08:37:44Z" id="87591943">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix bug where parse error is thrown if a inner filter is used in a nested filter/query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10309</link><project id="" key="" /><description>PR for #10308
</description><key id="65100788">10309</key><summary>Fix bug where parse error is thrown if a inner filter is used in a nested filter/query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-29T22:01:14Z</created><updated>2015-05-29T16:28:12Z</updated><resolved>2015-03-31T20:45:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-30T09:51:17Z" id="87617878">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner_hits does not work for nested filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10308</link><project id="" key="" /><description>I'm updating a .NET client to support inner_hits for nested, has_child and has_parent queries and filters.

The inner_hits work for all queries and filters except nested filters. Is this a bug or a documentation error?

```
{
    "filter": {
        "nested": {
            "path": "skillchildren",
            "filter": {
                "match_all": { }
            },
            "inner_hits": { }
        }
    }
}
```

Greetings Damien
</description><key id="65086482">10308</key><summary>inner_hits does not work for nested filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels><label>bug</label></labels><created>2015-03-29T19:14:11Z</created><updated>2015-03-31T20:45:50Z</updated><resolved>2015-03-31T20:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-29T20:29:16Z" id="87470224">@damienbod inner_hits in a nested filter should work. I quickly tried to verified that `inner_hits` works in a `nested` filter. If it doesn't work in your case can you share your reproduction of the issue?
</comment><comment author="damienbod" created="2015-03-29T21:05:03Z" id="87476101">Hi Thanks for your reply. Here's my data:

Mapping:

```
PUT http://localhost:9200/nestedcollectiontests/nestedcollectiontest/_mappings HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 616
Expect: 100-continue

{"nestedcollectiontest":{"properties":{"id":{ "type" : "long" },"nameskillparent":{ "type" : "string" },"descriptionskillparent":{ "type" : "string" },"createdskillparent":{ "type" : "date", "format": "dateOptionalTime"},"updatedskillparent":{ "type" : "date", "format": "dateOptionalTime"},"skillchildren":{"type":"nested","include_in_parent":true,"properties":{"id":{ "type" : "long" },"nameskillchild":{ "type" : "string" },"descriptionskillchild":{ "type" : "string" },"createdskillchild":{ "type" : "date", "format": "dateOptionalTime"},"updatedskillchild":{ "type" : "date", "format": "dateOptionalTime"}}}}}}

```

Test Data:

```
POST http://localhost:9200/_bulk HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 906
Expect: 100-continue

{"index":{"_index":"nestedcollectiontests","_type":"nestedcollectiontest","_id":8}}
{"id":8,"nameskillparent":"cool","descriptionskillparent":"A test entity description","createdskillparent":"2015-03-29T20:59:51.0179334+00:00","updatedskillparent":"2015-03-29T20:59:51.0179334+00:00","skillchildren":[{"id":0,"nameskillchild":"cool","descriptionskillchild":"A test SkillChild description","createdskillchild":"2015-03-29T20:59:51.0159335+00:00","updatedskillchild":"2015-03-29T20:59:51.0159335+00:00"},{"id":1,"nameskillchild":"cool","descriptionskillchild":"A test SkillChild description","createdskillchild":"2015-03-29T20:59:51.0159335+00:00","updatedskillchild":"2015-03-29T20:59:51.0159335+00:00"},{"id":2,"nameskillchild":"cool","descriptionskillchild":"A test SkillChild description","createdskillchild":"2015-03-29T20:59:51.0159335+00:00","updatedskillchild":"2015-03-29T20:59:51.0159335+00:00"}]}
```

Search Request:

```
POST http://localhost:9200/nestedcollectiontests/nestedcollectiontest/_search HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 88
Expect: 100-continue

{"filter":{"nested":{"path":"skillchildren","filter":{"match_all":{}},"inner_hits":{}}}}
```

Response:

```
HTTP/1.1 400 Bad Request
Content-Type: application/json; charset=UTF-8
Content-Length: 1997

{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[g6ANIIX0SieBnVvi3jhjJA][nestedcollectiontests][0]: SearchParseException[[nestedcollectiontests][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filter\":{\"nested\":{\"path\":\"skillchildren\",\"filter\":{\"match_all\":{}},\"inner_hits\":{}}}}]]]; nested: QueryParsingException[[nestedcollectiontests] [nested] requires either 'query' or 'filter' field]; }{[g6ANIIX0SieBnVvi3jhjJA][nestedcollectiontests][1]: SearchParseException[[nestedcollectiontests][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filter\":{\"nested\":{\"path\":\"skillchildren\",\"filter\":{\"match_all\":{}},\"inner_hits\":{}}}}]]]; nested: QueryParsingException[[nestedcollectiontests] [nested] requires either 'query' or 'filter' field]; }{[g6ANIIX0SieBnVvi3jhjJA][nestedcollectiontests][2]: SearchParseException[[nestedcollectiontests][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filter\":{\"nested\":{\"path\":\"skillchildren\",\"filter\":{\"match_all\":{}},\"inner_hits\":{}}}}]]]; nested: QueryParsingException[[nestedcollectiontests] [nested] requires either 'query' or 'filter' field]; }{[g6ANIIX0SieBnVvi3jhjJA][nestedcollectiontests][3]: SearchParseException[[nestedcollectiontests][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filter\":{\"nested\":{\"path\":\"skillchildren\",\"filter\":{\"match_all\":{}},\"inner_hits\":{}}}}]]]; nested: QueryParsingException[[nestedcollectiontests] [nested] requires either 'query' or 'filter' field]; }{[g6ANIIX0SieBnVvi3jhjJA][nestedcollectiontests][4]: SearchParseException[[nestedcollectiontests][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filter\":{\"nested\":{\"path\":\"skillchildren\",\"filter\":{\"match_all\":{}},\"inner_hits\":{}}}}]]]; nested: QueryParsingException[[nestedcollectiontests] [nested] requires either 'query' or 'filter' field]; }]","status":400}
```

Elasticsearch version:

```
{

    "status": 200,
    "name": "Shen Kuei",
    "cluster_name": "elasticsearch",
    "version": 

    {
        "number": "1.5.0",
        "build_hash": "544816042d40151d3ce4ba4f95399d7860dc2e92",
        "build_timestamp": "2015-03-23T14:30:58Z",
        "build_snapshot": false,
        "lucene_version": "4.10.4"
    },
    "tagline": "You Know, for Search"

}
```

Thanks for your help.

Greetings Damien
</comment><comment author="martijnvg" created="2015-03-29T21:58:21Z" id="87486692">Hey @damienbod this is a bug. Thanks for sharing your data!

It only manifest if a inner filter is used in the nested filter. If a inner query is used instead then the issue doesn't occur, so the following works:

```
{
    "filter": {
        "nested": {
            "path": "skillchildren",
            "query": {
                "match_all": { }
            },
            "inner_hits": { }
        }
    }
}
```
</comment><comment author="damienbod" created="2015-03-30T04:14:50Z" id="87536466">No problem
Thanks for looking 
greetings Damien
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix wrong use of currentFieldName outside of a parsing loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10307</link><project id="" key="" /><description>Parsing a multipercolate request copies the content of the JSON to a Map and then iterates on the entrySet, but for the fields `ignore_unavailable`, `allow_no_indices` and `expand_wildcards` it is still using the currentFieldName variable and not entry.getKey().
This cause the parsing to fail if the last fieldName of the JSON was one of these fields.

example : 

```
curl -XPOST "http://localhost:9200/twitter/_mpercolate" -d'
{"percolate":{"index":"the_index","type":"the_type", "realtime": true,"expand_wildcards":"open"}}
```

will fail with error :

```
{
   "error": "ElasticsearchIllegalArgumentException[No valid expand wildcard value [true]]",
   "status": 400
}
```

This PR fixes the parsing and also extracts the parsing to a map to a dedicated method to avoid having the parsing vars wrongly reused in the future.
</description><key id="65076378">10307</key><summary>Fix wrong use of currentFieldName outside of a parsing loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obourgain</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-29T16:43:47Z</created><updated>2015-06-08T00:15:37Z</updated><resolved>2015-03-30T08:08:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-29T19:54:31Z" id="87463524">The fix looks good. Thanks for extracting the parsing logic to another method in order to make sure we don't make the same mistake again if we add a new parameter! Will merge soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix multi percolate response sample in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10306</link><project id="" key="" /><description>The sample multipercolate response in the documentation is wrong, here is a little PR to fix that.
</description><key id="65069451">10306</key><summary>Fix multi percolate response sample in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-29T15:09:44Z</created><updated>2015-03-30T09:34:33Z</updated><resolved>2015-03-30T09:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-30T09:34:33Z" id="87613747">Thanks @obourgain merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update for clarification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10305</link><project id="" key="" /><description>Make it clear which nodes in the cluster should have `http.enabled` set to `false`.
</description><key id="65018596">10305</key><summary>Update for clarification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Dshiv</reporter><labels><label>docs</label></labels><created>2015-03-29T03:42:27Z</created><updated>2015-04-05T16:05:40Z</updated><resolved>2015-04-05T16:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-03-29T07:44:18Z" id="87367485">You need to sign the CLA if you'd like this merged :) See https://www.elastic.co/contributor-agreement/

Also, why do you think this only makes sense to apply to a data only node? If may make sense to disable it on any node that isn't a client to lock things down.
</comment><comment author="Dshiv" created="2015-03-29T19:12:27Z" id="87456721">I have signed the CLA! :)

While reading the docs, the cluster I had in mind was a simple cluster of data nodes, and a single client node. Upon reading this portion of the docs, the grammatical structure implies that one should set http.enabled to false, _when_ creating non data nodes. Which is why I was confused. The example the docs present is that of data node(s) and client node(s), which is why I refer only to data nodes. I agree though that it may not only be data nodes where one would set the http.enabled flag to false on.
</comment><comment author="s1monw" created="2015-04-02T10:37:12Z" id="88862370">@clintongormley can you take a look at this?
</comment><comment author="clintongormley" created="2015-04-05T16:05:37Z" id="89796940">I've tidied up the wording a bit and pushed. Thanks @Dshiv 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>key_as_string is always showing up as 1970-01-01 in Date Histogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10304</link><project id="" key="" /><description>Using ElasticSearch 1.5.0 and datehistogram, the key_as_string is always showing up as 1970-01-01. I am using publicly available IMDB movie DB to test ES. Please advice if I am doing something wrong.

``` js
GET /movies/movie/_search?search_type=count
{
    "aggs" : {
        "movie-releases-by-year" : {
            "date_histogram" : {
                "field" : "fields.year",
                "interval" : "50",
                "min_doc_count": 0,
                "format" : "yyyy-MM-dd" 
            }
        }
    }
}
```

``` js
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 4903,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "movie-releases-by-year": {
         "buckets": [
            {
               "key_as_string": "1970-01-01",
               "key": 1900,
               "doc_count": 51
            },
            {
               "key_as_string": "1970-01-01",
               "key": 1950,
               "doc_count": 1480
            },
            {
               "key_as_string": "1970-01-01",
               "key": 2000,
               "doc_count": 3309
            }
         ]
      }
   }
}
```
</description><key id="65013017">10304</key><summary>key_as_string is always showing up as 1970-01-01 in Date Histogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devnull52</reporter><labels><label>feedback_needed</label></labels><created>2015-03-29T02:13:32Z</created><updated>2015-04-05T19:15:10Z</updated><resolved>2015-04-05T19:15:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-29T04:34:31Z" id="87357238">Can you send your mapping please?
</comment><comment author="devnull52" created="2015-03-29T21:07:37Z" id="87477471">Hi David,

Thanks for getting back. I am a novice on ElasticSearch, so apologies in
anticipation if I might be doing something incorrectly. I am basically
trying to play around with publicly available IMDB Movie data to understand
search concepts.

What I was hoping to get is a date histogram by Release date:-

Code, Log and Mapping attached. Environment : I am running ES 1.5.0 ( MBP,
Java 8 ) via Home Brew. Code is compiled using Maven.

1970 (Release year in my mind maps to a Date): 10 ( Number of movies
released in the year ).
1971 : 1
1972 : 2 etc

On Sun, Mar 29, 2015 at 12:35 AM, David Pilato notifications@github.com
wrote:

&gt; Can you send your mapping please?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10304#issuecomment-87357238
&gt; .

  {
      "fields" : {
         "directors" : [
            "Roland Emmerich"
         ],
         "release_date" : "2013-06-26T00:00:00Z",
         "rating" : 6.4,
         "genres" : [
            "Action",
            "Drama",
            "Thriller"
         ],
         "image_url" : "http://ia.media-imdb.com/images/M/MV5BMTAyNzQyNTcwNjVeQTJeQWpwZ15BbWU3MDAwOTQ4Nzk@._V1_SX400_.jpg",
         "plot" : "While on a tour of the White House with his young daughter, a Capitol policeman springs into action to save his child and protect the president from a heavily armed group of paramilitary invaders.",
         "title" : "White House Down",
         "rank" : 60,
         "running_time_secs" : 7860,
         "actors" : [
            "Channing Tatum",
            "Jamie Foxx",
            "Maggie Gyllenhaal"
         ],
         "year" : 2013
      },
      "id" : "tt2334879",
      "type" : "add"
   },
   {
      "fields" : {
         "directors" : [
            "Michael Bay"
         ],
         "release_date" : "2013-04-11T00:00:00Z",
         "rating" : 6.5,
         "genres" : [
            "Comedy",
            "Crime",
            "Drama"
         ],
         "image_url" : "http://ia.media-imdb.com/images/M/MV5BMTU0NDE5NTU0OV5BMl5BanBnXkFtZTcwMzI1OTMzOQ@@._V1_SX400_.jpg",
         "plot" : "A trio of bodybuilders in Florida get caught up in an extortion ring and a kidnapping scheme that goes terribly wrong.",
         "title" : "Pain &amp; Gain",
         "rank" : 61,
         "running_time_secs" : 7740,
         "actors" : [
            "Mark Wahlberg",
            "Dwayne Johnson",
            "Anthony Mackie"
         ],
         "year" : 2013
      },
      "id" : "tt1980209",
      "type" : "add"
   },
   {
      "fields" : {
         "directors" : [
            "Baltasar Korm&#225;kur"
         ],
         "release_date" : "2013-07-30T00:00:00Z",
         "rating" : 7,
         "genres" : [
            "Action",
            "Comedy",
            "Crime",
            "Drama",
            "Thriller"
         ],
         "image_url" : "http://ia.media-imdb.com/images/M/MV5BNTQ5MTgzNDg4OF5BMl5BanBnXkFtZTcwMjAyODEzOQ@@._V1_SX400_.jpg",
         "plot" : "A DEA agent and a naval intelligence officer find themselves on the run after a botched attempt to infiltrate a drug cartel. While fleeing, they learn the secret of their shaky alliance: Neither knew that the other was an undercover agent.",
         "title" : "2 Guns",
         "rank" : 62,
         "running_time_secs" : 6540,
         "actors" : [
            "Denzel Washington",
            "Mark Wahlberg",
            "Paula Patton"
         ],
         "year" : 2013
      },
      "id" : "tt1272878",
      "type" : "add"
   }
curl -XPOST localhost:9200/movies/ -d '{
    "settings" : {
        "number_of_shards" : 1,
        "number_of_replicas" : 0
    },
    "mappings" : {

```
   "movie": {

        "_all" : {
            "enabled" : "false"
        },
        "_id" : {
            "path" : "fields.title"
        },
        "properties": {
            "fields": {
                "type": "object",
                "properties": {
                    "directors" : { "type" : "string" },
                    "release_date" : { "type": "date" },
                    "rating" : { "type": "float" }, 
                    "genres" : { "type" : "string" },
                    "image_url" : { "type": "string" },
                    "plot" : { "type": "string" },
                    "title" : { "type": "string" },
                    "rank" : { "type": "long" },
                    "running_time_secs" : { "type": "long" },   
                    "actors" : { "type" : "string" },
                    "year" : { "type" : "date", "format" : "yyyy" }
                }
            },
            "id": {
                "type": "string"

            },
            "type": {
                "type": "string"
            }
        }

    }
},
"aliases" : {
    "all_movies" : {},
    "recent_movies" : {
        "filter" : {
            "term" : {"year" : 2014 }
        }
    },
    "eighties_nineties" : {
        "filter" : {
                "range" : {
                    "year" : {
                        "gt" : 1979,
                        "lt" : 2000
                    }
                }
            }
        }
} 
```

}'
</comment><comment author="rjernst" created="2015-03-30T17:12:52Z" id="87754410">@devnull52 What you posted there is just a sample document.  I assume then you are using dynamic mappings? Can you run the following command and post the output?

```
curl -XGET 'http://localhost:9200/movies/_mapping/movie'
```
</comment><comment author="patricksurry" created="2015-04-01T13:54:37Z" id="88488495">It looks like it's interpreting fields.year as an timestamp (millis since epoch) rather than a year value like you expect.  So all your keys are a few seconds appear 1/1/1970 and hence your key-as-string is constant.
</comment><comment author="devnull52" created="2015-04-03T01:24:58Z" id="89107034">Hi Ryan,

Please see below. I created mappings during index creation time.
Please let me know if there is anything else I can provide.

{
  "movies" : {
    "mappings" : {
      "movie" : {
        "_all" : {
          "enabled" : false
        },
        "_id" : {
          "path" : "fields.title"
        },
        "properties" : {
          "facets" : {
            "properties" : {
              "date_histogram" : {
                "properties" : {
                  "field" : {
                    "type" : "string"
                  },
                  "interval" : {
                    "type" : "string"
                  }
                }
              }
            }
          },
          "fields" : {
            "properties" : {
              "actors" : {
                "type" : "string"
              },
              "directors" : {
                "type" : "string"
              },
              "genres" : {
                "type" : "string"
              },
              "image_url" : {
                "type" : "string"
              },
              "plot" : {
                "type" : "string"
              },
              "rank" : {
                "type" : "long"
              },
              "rating" : {
                "type" : "float"
              },
              "release_date" : {
                "type" : "date",
                "format" : "dateOptionalTime"
              },
              "running_time_secs" : {
                "type" : "long"
              },
              "title" : {
                "type" : "string"
              },
              "year" : {
                "type" : "date",
                "format" : "yyyy"
              }
            }
          },
          "id" : {
            "type" : "string"
          },
          "query" : {
            "properties" : {
              "match_all" : {
                "type" : "object"
              }
            }
          },
          "type" : {
            "type" : "string"
          }
        }
      }
    }
  }
}

On 3/30/15, Ryan Ernst notifications@github.com wrote:

&gt; @devnull52 What you posted there is just a sample document.  I assume then
&gt; you are using dynamic mappings? Can you run the following command and post
&gt; the output?
&gt; 
&gt; ```
&gt; curl -XGET 'http://localhost:9200/movies/_mapping/movie'
&gt; ```
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/10304#issuecomment-87754410
</comment><comment author="rjernst" created="2015-04-04T01:04:10Z" id="89476252">@devnull52, @patricksurry is correct (thanks!). Date fields internally store their value as an epoch based timestamp (in milliseconds). The fact that "key" comes back as an int in your first posted response shows the actual value is far to low (it should be much large to be a timestamp representing e.g. the year 1950).

I see your mappings have a format set for the field. IIRC, format won't work unless you pass the value as a string (otherwise it interprets as a timestamp).  Your example documents show you are passing year as an int.
</comment><comment author="devnull52" created="2015-04-04T12:49:27Z" id="89567565">Ryan, Patrick and David -- thanks for picking this up and responding
patiently. I did not read the documentation. I changed the mapping and
it worked like a charm.

On 4/3/15, Ryan Ernst notifications@github.com wrote:

&gt; @devnull52, @patricksurry is correct (thanks!). Date fields internally store
&gt; their value as an epoch based timestamp (in milliseconds). The fact that
&gt; "key" comes back as an int in your first posted response shows the actual
&gt; value is far to low (it should be much large to be a timestamp representing
&gt; e.g. the year 1950).
&gt; 
&gt; I see your mappings have a format set for the field. IIRC, format won't work
&gt; unless you pass the value as a string (otherwise it interprets as a
&gt; timestamp).  Your example documents show you are passing year as an int.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/10304#issuecomment-89476252
</comment><comment author="clintongormley" created="2015-04-05T19:15:10Z" id="89833321">Great to hear it is resolved. Closing this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] enable scripts on demand via annotation and randomize default script settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10303</link><project id="" key="" /><description>All of our tests have been running with dynamic scripts always on for a long while now, mainly for simplicity and historical reasons.

This commit introduces a new annotation that let each test declare which script features it requires. The infra will make sure that those features will be on, and will randomize the remaining settings. Same logic gets applied to bw comp tests, with the only difference that the new fine-grained settings are used only when supported on external nodes (otherwise the previous `script.disable_dynamic` is set).

`ElasticsearchSingleNodeTest` stays with all inline scripts enabled for simplicity.
</description><key id="64964140">10303</key><summary>[TEST] enable scripts on demand via annotation and randomize default script settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-28T16:50:30Z</created><updated>2015-06-07T11:45:04Z</updated><resolved>2015-05-27T08:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-28T18:09:55Z" id="87275194">Do we really need an annotation? Why wouldn't you just override nodeSettings when necessary and turn on the settings needed? I don't like the idea of having a bunch of annotations (this being precedent for future ones) which tweak node settings, when there is already a way to tweak node settings.
</comment><comment author="javanna" created="2015-03-28T18:50:37Z" id="87282148">That was the original implementation (override nodeSettings) but it requires more code, the annotation is more lightweight, especially given that there are quite some tests needing scripts. Also, there are multiple ways to set those settings, this way we separate what the test needs from which settings get injected (randomized behaviour) and we can randomize what the test doesn't need. I don't see this one being a precedent for future cases to be honest, it's a different usecase compared to the ones that we had up until now.
</comment><comment author="s1monw" created="2015-03-31T08:49:46Z" id="87998337">I tend to agree with ryan there why don't we just use node settings. We don't have a global cluster anymore anyway to this should be very simple?
</comment><comment author="javanna" created="2015-03-31T09:32:10Z" id="88013938">I didn't use node settings for the reasons stated above. We can do nodeSettings, it's just that we need to give up some randomization on those settings (other than randomizing the default settings, that can be overridden by tests via specific settings). Also that will require more lines of code. I thought in the past we talked about moving more to annotations even for number of shards etc. and have less methods to override things, seems like that's not the case anymore.
</comment><comment author="javanna" created="2015-04-01T13:15:57Z" id="88476437">General question: beyond the annotation discussion, do people think this is a nice to have improvement for our test infra? To be honest this requires some more work if I have to move back to `nodeSettings` and drop the annotation, I really want to make sure that we want to do this. If we are fine with having scripts enabled all the time in our test infra, I can just close and be done with it.
</comment><comment author="s1monw" created="2015-04-02T10:38:56Z" id="88862604">&gt; I didn't use node settings for the reasons stated above. We can do nodeSettings, it's just that we need to give up some randomization on those settings (other than randomizing the default settings, that can be overridden by tests via specific settings). Also that will require more lines of code. I thought in the past we talked about moving more to annotations even for number of shards etc. and have less methods to override things, seems like that's not the case anymore.

I don't get this why can't we randomize and tests that need it explicity can override node settings?
</comment><comment author="javanna" created="2015-04-02T11:02:18Z" id="88865951">@s1monw it's not that we cannot randomize, we can just do it less.

Lemme explain with an example, code is [here](https://github.com/elastic/elasticsearch/pull/10303/files#diff-82ae4161d8716f6fb433071d55f4a41aR51). Right now if we find `@RequiresScript(context=ScriptContext.AGGS, lang="groovy", type=ScriptType.INLINE)` we might either 

1) set `script.inline: on`

2) set  `script.aggs: on` and randomize `script.$scriptType` value (as `script.aggs` has the precedence)

3) set `script.engine.groovy.inline.aggs: on` and randomize both `script.$scriptType` value and `script.$scriptContext` value (as the engine specific setting has the precedence)

If we switch to relying on `nodeSettings` we should probably require using the last option only for settings in tests (engine specific one), which is the one that always has the precedence, so that we can randomize `script.$scriptType` and `script.$scriptContext`, but e.g. these last two will never be used to enable scripts for a tests that need them. 

Using the annotation decouples the test requirements from what gets set and what can get randomized. Also, maybe not that important, but when I gave try at the nodeSettings option, it felt like much more lines of code were required in many testts, while the annotation is a bit more lightweight.

If you still think you prefer to simplify the randomization logic and use nodeSettings only, I can work on that.
</comment><comment author="javanna" created="2015-05-27T08:23:28Z" id="105819563">This has pretty much stalled, given the size of the PR and the not so crucial importance I would tend to close it, unless somebody wants me to revive it and get it in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix doc values representation to always serliaze if explicitly set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10302</link><project id="" key="" /><description>When doc values are explicitly set to the default value serialization
is skipped. This means the alternate way of specifying doc values,
through `fielddata.format: doc_values`, will take precedence if
present.

This change fixes doc values to always be serialized when an explicit value
was passed, so that it continues to take precedence over
`fielddata.format`.

closes #10297
</description><key id="64904144">10302</key><summary>Fix doc values representation to always serliaze if explicitly set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-03-28T05:56:54Z</created><updated>2015-06-08T09:02:48Z</updated><resolved>2015-04-04T02:38:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-28T13:47:49Z" id="87231785">LGTM
</comment><comment author="brwe" created="2015-03-29T21:36:51Z" id="87484858">Could we have unit tests for the serialization? A SingleNodeTest like TimestampMappingTests maybe?
</comment><comment author="rjernst" created="2015-03-30T19:14:56Z" id="87796062">Sure, I pushed a new commit with a unit test.
</comment><comment author="brwe" created="2015-04-02T10:12:37Z" id="88857238">Test looks good. 
</comment><comment author="s1monw" created="2015-04-02T10:13:46Z" id="88857426">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add non existing settings in cluster settings in  elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10301</link><project id="" key="" /><description>Hi there,

Recently i have built elasticsearch with fluentd and Kibana and all is working fine, but we are facing high cpu load while performing search caused by java.

I have 60 GB Ram and 16 processors and each processor has 2 cores.

ES_HEAP_SIZE=32g
-find below more details

 curl "localhost:9200/_cat/thread_pool?v&amp;h=search.rejected"
search.rejected
            387

curl "localhost:9200/_cat/thread_pool?v&amp;h=index.rejected"
index.rejected
             0
- snapshot from elasticsearch.log

[DEBUG][action.search.type       ] [Hulk 2099] [logstash-2015.03.14][4], node[qxcAN3lURs65Lf1GMhB_qg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7c71025f] lastShard [true]
org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (queue capacity 1000) on org.elasticsearch.search.action.SearchServiceTransportAction$23@1d7c9f0f
        at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
        at org.elasticsearch.search.action.SearchServiceTransportAction.execute(SearchServiceTransportAction.java:551)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:228)
        at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:71)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:171)

Now i read that i have to update the threadpool settings in ES and i have now 
 curl -XGET localhost:9200/_cluster/settings?pretty
{
  "persistent" : { },
  "transient" : { }
}

i'm trying to update the settings with the below commands 

curl -XPUT localhost:9200/_cluster/settings -d '{

"threadpool" : {
  "index": {
    "type": "fixed",
    "size": 32,
    "queue_size": 1000
  },
  "bulk": {
    "type": "fixed",
    "size": 32,
    "queue_size": 1000
  },
  "search": {
    "type": "fixed",
    "size": 96,
    "queue_size": 1000
 }
}
}'

but i keep getting {"error":"ActionRequestValidationException[Validation Failed: 1: no settings to update;]","status":400}

What's wrong with this command? is it the right solution for my issue ?

Please advise

Thanks.

Ayman
</description><key id="64889806">10301</key><summary>Add non existing settings in cluster settings in  elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shorman88</reporter><labels /><created>2015-03-28T02:43:48Z</created><updated>2015-03-29T10:45:35Z</updated><resolved>2015-03-29T10:45:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-29T10:45:35Z" id="87389513">Hi @shorman88 I think the best way to get help is asking your question on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch). We generally use github issues for bugs and feature requests.

This doesn't look like a bug, your cluster update settings request is malformed. Also, tweaking your threadppol settings doesn't seem like a great idea to me. If you can move this to the mailing list this can be better discussed so you can get some advice.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to get mapping based on document type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10300</link><project id="" key="" /><description>I am using the python 'elasticsearch' library to interact with a elasticsearch cluster. One of the indices stores 2 different types of documents under it and i am trying to extract the mapping of each of these documents separately, i get a very confusing error message. 

```
In [14]: es.indices.get_mapping(index='es_index', doc_type='doc1')
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-14-00f02e55aadb&gt; in &lt;module&gt;()
----&gt; 1 es.indices.get_mapping(index='es_index', doc_type='doc1')

/Library/Python/2.7/site-packages/elasticsearch/client/utils.pyc in _wrapped(*args, **kwargs)
     66                 if p in kwargs:
     67                     params[p] = kwargs.pop(p)
---&gt; 68             return func(*args, params=params, **kwargs)
     69         return _wrapped
     70     return _wrapper

/Library/Python/2.7/site-packages/elasticsearch/client/indices.pyc in get_mapping(self, index, doc_type, params)
    297         """
    298         _, data = self.transport.perform_request('GET', _make_path(index, '_mapping', doc_type),
--&gt; 299             params=params)
    300         return data
    301

/Library/Python/2.7/site-packages/elasticsearch/transport.pyc in perform_request(self, method, url, params, body)
    305
    306             try:
--&gt; 307                 status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
    308
    309             except TransportError as e:

/Library/Python/2.7/site-packages/elasticsearch/connection/http_urllib3.pyc in perform_request(self, method, url, params, body, timeout, ignore)
     84         if not (200 &lt;= response.status &lt; 300) and response.status not in ignore:
     85             self.log_request_fail(method, url, body, duration, response.status)
---&gt; 86             self._raise_error(response.status, raw_data)
     87
     88         self.log_request_success(method, full_url, url, body, response.status,

/Library/Python/2.7/site-packages/elasticsearch/connection/base.pyc in _raise_error(self, status_code, raw_data)
    100             pass
    101
--&gt; 102         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
    103
    104

NotFoundError: TransportError(404, u'{"_index":"es_index","_type":"_mapping","_id":"doc1","exists":false}')
```

I am not completely sure why the argument for `doc_type` is getting translated to `_id` in the request. Below is the code i am using,

```
from elasticsearch import Elasticsearch

ES  = Elasticsearch('http://localhost:1111')
print ES.indices.get_mapping(index='es_index', doc_type='doc1')
```

Although i am able to get mappings for all indexes and their respective document types as a whole when i try 

```
print ES.indices.get_mapping()
```

Can you please help in this regard. 

Thanks in advance
--Manoj
</description><key id="64876728">10300</key><summary>Unable to get mapping based on document type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manoj-v</reporter><labels /><created>2015-03-28T00:20:32Z</created><updated>2015-03-28T00:28:13Z</updated><resolved>2015-03-28T00:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="manoj-v" created="2015-03-28T00:28:13Z" id="87130549">Apologies, wrong repo under elasticsearch. Closing here and creating another ticket under elasticsearch-py with the same content.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings of index request are incorrectly merged with index template mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10299</link><project id="" key="" /><description>Hey,  
steps to reproduce
1) create template `mi_*` (this is a elastic4s version): 

``` scala
create template "resource_template" pattern "mi_*" mappings (
   "resource" as (
      "name"      typed StringType index NotAnalyzed,
      "host"      typed StringType index NotAnalyzed,
      "campaign"  typed StringType index NotAnalyzed,
      "created"   typed LongType
    ),

    "record" as (
      "gwid"      typed StringType index NotAnalyzed,
      "mid"       typed StringType index NotAnalyzed,
      "memberid"  typed StringType index NotAnalyzed,
      "respid"    typed StringType index NotAnalyzed,
      "token"     typed StringType index NotAnalyzed,
      "weighting" typed DoubleType,
      "timestamp" typed LongType
    ) numericDetection false,

    "error" as (
      "fileName" typed StringType index NotAnalyzed,
      "headerRow" typed StringType index NotAnalyzed,
      "dataRow" typed StringType index NotAnalyzed,
      "headerFields" typed StringType index NotAnalyzed,
      "dataFields" typed StringType index NotAnalyzed
    )
)
```

results to : 

``` json
"template_mi": {
      "order": 0,
      "template": "mi_*",
      "settings": {},
      "mappings": {
         "error": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "dataFields": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "dataRow": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "headerRow": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "fileName": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "headerFields": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         },
         "resource": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "host": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "created": {
                  "type": "long"
               },
               "name": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "campaign": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         },
         "record": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "timestamp": {
                  "type": "long"
               },
               "memberid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "token": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "respid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "weighting": {
                  "type": "double"
               },
               "mid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "gwid": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         }
      }
```

2) 
![crazy_bug](https://cloud.githubusercontent.com/assets/518855/6877850/97e09f8e-d4d5-11e4-9870-aeb9fca9a551.png)

Elastic search  [error](http://pastebin.com/raw.php?i=KCUxqz5C) too
</description><key id="64862375">10299</key><summary>Mappings of index request are incorrectly merged with index template mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">l15k4</reporter><labels /><created>2015-03-27T22:23:10Z</created><updated>2015-04-02T08:09:13Z</updated><resolved>2015-04-01T14:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="l15k4" created="2015-03-30T18:11:05Z" id="87777115">Weird thing is that if I create exactly the same template in Sense, it works! And by "exactly" I mean  (Compare with clipboard -&gt; contents are identical) ... So that the template created by Java client is somehow "corrupted" on the binary level or something like that
</comment><comment author="brwe" created="2015-03-30T21:30:41Z" id="87836802">The error could come from a template that looks like the one I pasted below. If the "template_mi" that you showed had caused it, then the fields inside the "resource" mapping would be printed as well so I assume a different template is responsible. To check if you have a different template like the one below, could you do
`
 curl -XGET "http://localhost:9200/_template?pretty"
`
after you got the error message and paste the result and the document you tried to index here?

Faulty template that can cause this exception:

```
 {
   "template_mi": {
      "order": 0,
      "template": "mi_*",
      "settings": {},
      "mappings": {
         "resource": {
            "resource": {
               "properties": {}
            }
         }
      },
      "aliases": {}
   }
}
```
</comment><comment author="l15k4" created="2015-03-30T21:40:53Z" id="87840138">Hi,

the existing template `GET _template?pretty` is this, nothing else is there except for `marvel` template : 

``` json
 "template_mi": {
      "order": 0,
      "template": "mi_*",
      "settings": {},
      "mappings": {
         "error": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "dataFields": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "dataRow": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "headerRow": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "fileName": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "headerFields": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         },
         "resource": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "host": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "recIds": {
                  "type": "integer"
               },
               "created": {
                  "type": "long"
               },
               "name": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "campaign": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         },
         "record": {
            "dynamic": "dynamic",
            "_source": {
               "enabled": true
            },
            "properties": {
               "timestamp": {
                  "type": "long"
               },
               "memberid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "token": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "respid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "weighting": {
                  "type": "double"
               },
               "mid": {
                  "index": "not_analyzed",
                  "type": "string"
               },
               "gwid": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            },
            "_all": {
               "enabled": true
            }
         }
      },
      "aliases": {}
   }
```

And the request : 

``` json
POST /mi_124/resource/
{
    "name" : "abc.log.gz",
    "host" : "whatever",
    "campaign" : "0001",
    "created" : 1234567890
}
```

Error: 

```
MapperParsingException[mapping [resource]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [resource : {properties={}}]]; 
```

```
org.elasticsearch.index.mapper.MapperParsingException: mapping [resource]
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:401)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:365)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [resource : {properties={}}]
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:278)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:192)
        at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:434)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:307)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:398)
        ... 6 more
```

Btw if I create this template using `curl` or `Sense`, it works, but if I create this template (absolutely identically, meaning that the `_tempalte` outputs are identical) using Java client, then I get this error... 
</comment><comment author="brwe" created="2015-03-30T22:17:21Z" id="87856029">Do you use the java client or elastic4s? If you use plain java, can you add the code that reproduces this?
</comment><comment author="brwe" created="2015-03-30T22:32:47Z" id="87860335">Can you paste the java code here?
</comment><comment author="l15k4" created="2015-03-30T22:39:57Z" id="87862186">I will, I'm just debugging the index request and this is what it looks like at MetaDataCreateIndexService, you can see the weird object model...
![es-bug](https://cloud.githubusercontent.com/assets/518855/6908594/722d48a8-d73e-11e4-9780-0e3ec19509b9.png)
</comment><comment author="l15k4" created="2015-03-30T23:16:35Z" id="87874158">Uff, I'll have to come back to it tomorrow, it's 1:15AM ... The XContentBuilder#string() for `resource` mapping from the code I pasted yields : 

``` json
{
  "_all": {
    "enabled": true
  },
  "_source": {
    "enabled": true
  },
  "dynamic": "dynamic",
  "_ttl": {
    "enabled": false
  },
  "properties": {
    "name": {
      "type": "string",
      "index": "not_analyzed"
    },
    "host": {
      "type": "string",
      "index": "not_analyzed"
    },
    "campaign": {
      "type": "string",
      "index": "not_analyzed"
    },
    "created": {
      "type": "long"
    },
    "recIds": {
      "type": "integer"
    }
  }
}
```

I don't think the problem is in the template creation... Everything looks as it should on this part. So far it really looks like the bug is on the `MetaDataCreateIndexService` side
</comment><comment author="l15k4" created="2015-03-30T23:40:07Z" id="87877562">![bug-es](https://cloud.githubusercontent.com/assets/518855/6909516/353ff7b6-d747-11e4-8304-1d35da506209.png)
The empty resource comes from here https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L250

And it is then merged with the template mappings into the resulting invalid template mapping that I have showed you on the screenshot
</comment><comment author="l15k4" created="2015-03-30T23:48:07Z" id="87878764">This is code for indexing : 

```
      client
        .client
        .prepareIndex("mi_2015_02", "resource")
        .setSource(
         jsonBuilder()
          .startObject()
            .field("name", fileName)
            .field("host", host)
            .field("campaign", campaign)
            .field("time", date.getTime)
          .endObject())
        .execute()
        .actionGet()
```

Any idea why this happens? Why is the empty resource mapping in the request?
</comment><comment author="l15k4" created="2015-03-31T07:55:20Z" id="87986508">It looks like createIndexRequest I pasted above has an empty mapping autogenerated for it's type as you can see on the screenshot and it also triggers template index creation and these 2 mappings are merged together, unfortunately incorrectly
![es-bug](https://cloud.githubusercontent.com/assets/518855/6914545/084fd876-d78c-11e4-8acc-127d867e516b.png)

So I was happy I found that I needed to set `action.auto_create_index: false` and it will solve my problem, but guess what, `action.auto_create_index: false` disables index templates, so that I'm still stuck here....
</comment><comment author="l15k4" created="2015-03-31T09:03:36Z" id="88003956">This is the java version and it works, so the problem seems to be on my part or in elastic4s

``` java
client.admin.indices.preparePutTemplate(name).setTemplate(pattern).setOrder(0)
        .addMapping("resource",
          jsonBuilder().
            startObject.
              startObject("resource").
                field("numeric_detection", false).
                startObject("properties").
                  startObject("name").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("host").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("campaign").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("created").
                    field("type", "long").
                  endObject().
                  startObject("recIds").
                    field("type", "integer").
                  endObject().
                endObject().
              endObject().
            endObject()
        ).addMapping("record",
          jsonBuilder().
            startObject().
              startObject("record").
                field("numeric_detection", false).
                startObject("properties").
                  startObject("gwid").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("mid").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("memberid").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("respid").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("token").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("weighting").
                    field("type", "double").
                  endObject().
                  startObject("timestamp").
                    field("type", "long").
                  endObject().
                endObject().
              endObject().
            endObject()
        ).addMapping("error",
          jsonBuilder().
            startObject().
              startObject("error").
                startObject("properties").
                  startObject("fileName").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("headerRow").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("dataRow").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("headerFields").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                  startObject("dataFields").
                    field("type", "string").field("index", "not_analyzed").
                  endObject().
                endObject().
              endObject().
            endObject()
        ).get()
```
</comment><comment author="l15k4" created="2015-04-01T14:26:51Z" id="88504666">Closing as it is either my fault or it is elastic4s problem
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Having trouble creating a query that queries by a parent query's value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10298</link><project id="" key="" /><description>I'm using the Elastic Search query DSL. I have documents that all have unique ids labeled "request-id". The request id, identifies related sets of log documents. For example, a start request would have the same `request-id` as it's end request counter part, and all logs in between.

I need to find end requests that have a status of 500, then query the documents by the `request-id` and find their related logs and show this in the results instead of only the end requests with status of 500.

I've been reading about parent-child queries, but I'm having trouble formulating a proper query that yields the results I'm seeking. Does anyone have any insight on how to make this happen?

To summarize, the query would be something like this:
1) `Parent`: find all documents with statusCode = 500
2) find all documents with `request-id == Parent.request-id`, show results.
</description><key id="64803232">10298</key><summary>Having trouble creating a query that queries by a parent query's value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jrthib</reporter><labels /><created>2015-03-27T16:51:29Z</created><updated>2015-03-27T17:41:30Z</updated><resolved>2015-03-27T17:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-27T17:40:05Z" id="87027280">Hi @jrthib can you please ask your question on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch) ? We generally use github issues for bugs or feature requests. Thanks!
</comment><comment author="jrthib" created="2015-03-27T17:41:30Z" id="87027808">No problem, will do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] UpdateMappingOnClusterTests.testTimestampMergingConflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10297</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_window-2008/1351/consoleFull

reproducible with

```
 mvn clean test -Dtests.seed=18576849DC7E894A -Dtests.class=org.elasticsearch.index.mapper.update.UpdateMappingOnClusterTests -Dtests.method="testTimestampMergingConflicts" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.assertion.disabled=org.elasticsearch -Dtests.security.manager=true -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:-UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=ar_BH -Dtests.timezone=Pacific/Nauru -Dtests.processors=4

```

on master.

`doc_values: true` is lost for _timestamp. Probably related to https://github.com/elastic/elasticsearch/pull/10209 although not sure if the commit caused this or just exposed an existing bug.
</description><key id="64799755">10297</key><summary>[CI] UpdateMappingOnClusterTests.testTimestampMergingConflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-27T16:35:02Z</created><updated>2015-04-04T02:38:17Z</updated><resolved>2015-04-04T02:38:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-28T05:18:43Z" id="87162493">Here is what happens:
1. The randomized default template is setup with `_timestamp` having `doc_values: false` explicitly.  However, this is serialized _without_ any `doc_values` setting because `false` is the default value for `_timestamp`.
2. The first mapping is added on index creation. It contains `fielddata.format: doc_values`. Since the default template was serialized without any doc values settings, this takes precedence and doc values is enabled, and part of the mappings serialization.
3. The mappings update is made. This has `fieldata.format: array`.  This causes `_timestamp` to use the default for doc values, which is `false`. Again, since this is the default, it is skipped during serialization.
4. The test fails when comparing the serialized mappings from (2) and (3), the first having `doc_values: true` and the second having no doc values setting.

The fix is to change how the doc values setting is stored in `AbstractFieldMapper`.  Instead of using a `boolean`, we instead need `Boolean`, so that a `null` values means the default was used, and a non-null value indicates we should _always_ write out the `doc_values` setting (it was explicitly set).
</comment><comment author="rjernst" created="2015-03-28T05:22:46Z" id="87162633">Note that this is an edge case because there are 2 ways to specify doc values.  If there were just one, skipping serialization of the default value would be fine (there would be nothing else to potentially supersede it).
</comment><comment author="brwe" created="2015-03-29T21:10:32Z" id="87479506">Actually, it seems to me this is some sort of race condition. If I add a sleep of 1s here: https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java#L230 before the first mapping is requested, then the test passes. 

In addition, the mapping that changes the fielddata format to `fieldata.format: array` is never applied (point 3. in the description) because there is a mapping conflict for this update and so all mappings should be untouched by the update. `doc_values: true` should remain too. If something changed then I would assume this is a bug. Also, is it even possible to switch from `doc_values` from `true` to `false`? I thought not?
</comment><comment author="rjernst" created="2015-03-30T18:16:07Z" id="87778332">I agree it looks like there is a race condition.  However, I think the race condition is only there because with the bug I fix in #10302, the mappings are serialized differently than when they are read.  So any refresh-mapping action would cause the parsed mappings to change.

While I don't know what triggers refresh-mapping actions, when I add your 1 second sleep with a printout, I notice this:

```
2015-03-30 11:00:58,899][DEBUG][org.elasticsearch.cluster.service] [node_s0] processing [refresh-mapping [index][[type]]]: execute
[2015-03-30 11:00:58,899][INFO ][org.elasticsearch.index.mapper.update] --&gt; Sleeping for 1 sec
...
[2015-03-30 11:00:58,923][DEBUG][org.elasticsearch.cluster.service] [node_s0] cluster state updated, version [10], source [refresh-mapping [index][[type]]]
```

So, a refresh-mapping is kicked off just before the sleep, and the sleep gives it about enough time to finish (so that by the time the get mapping is executed, it has the refreshed view, which now has `doc_values: true`.  Also, the log message from the failure is actually reversed.  The "got" is what we got before, and the "expected" is the second retrieval at the end of the test.  The `assertThat` call has the arguments in the wrong order, which I think has led to part of the confusion. At first when the index template is applied, the in memory version has `doc_values: false`.  Once the refresh-mapping action is complete, we have `doc_values: true` because the explicit doc values setting has been lost.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: GeoDistanceTests.testDistanceSortingNestedFields runs into NPE on fetch phase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10296</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_15_centos/168/testReport/junit/org.elasticsearch.search.geo/GeoDistanceTests/testDistanceSortingNestedFields/

Doesn't look like a geo related issue:

```
java.lang.AssertionError: Expected different hit count.  Total shards: 9 Successful shards: 8 &amp; 1 shard failures:
 shard [[UJboLng4Sbmce5B3_MvRnQ][companies][0]], reason [RemoteTransportException[[node_s1][local[274]][indices:data/read/search[phase/fetch/id]]]; nested: NullPointerException; ]
Expected: &lt;4&gt;
     but: was &lt;3&gt;
    at __randomizedtesting.SeedInfo.seed([2294217C90EA15A7:F1CD59C9510B38EC]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
```
</description><key id="64786192">10296</key><summary>CI: GeoDistanceTests.testDistanceSortingNestedFields runs into NPE on fetch phase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-03-27T15:35:45Z</created><updated>2015-12-07T09:02:40Z</updated><resolved>2015-12-07T09:02:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-04-03T08:30:38Z" id="89219119">This is the actual error:

```
org.elasticsearch.transport.RemoteTransportException: [node_s1][local[274]][indices:data/read/search[phase/fetch/id]]
  1&gt; Caused by: java.lang.NullPointerException
  1&gt;    at org.elasticsearch.common.bytes.BytesReference$Helper.hashCode(BytesReference.java:71)
  1&gt;    at org.elasticsearch.common.bytes.BytesReference$Helper.bytesHashCode(BytesReference.java:61)
  1&gt;    at org.elasticsearch.common.bytes.BytesArray.hashCode(BytesArray.java:171)
  1&gt;    at org.elasticsearch.common.text.StringAndBytesText.hashCode(StringAndBytesText.java:94)
  1&gt;    at com.carrotsearch.hppc.Internals.rehash(Internals.java:10)
  1&gt;    at com.carrotsearch.hppc.ObjectIntOpenHashMap.containsKey(ObjectIntOpenHashMap.java:714)
  1&gt;    at org.elasticsearch.common.io.stream.HandlesStreamOutput.writeSharedText(HandlesStreamOutput.java:60)
  1&gt;    at org.elasticsearch.search.internal.InternalSearchHit.writeTo(InternalSearchHit.java:709)
  1&gt;    at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:259)
  1&gt;    at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:104)
  1&gt;    at org.elasticsearch.transport.local.LocalTransportChannel.sendResponse(LocalTransportChannel.java:74)
  1&gt;    at org.elasticsearch.transport.local.LocalTransportChannel.sendResponse(LocalTransportChannel.java:62)
  1&gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:869)
  1&gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:862)
  1&gt;    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:279)
  1&gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

I'm not sure yet why the type of a hit is null.
</comment><comment author="clintongormley" created="2015-12-05T22:02:58Z" id="162250983">@martijnvg can this be closed?
</comment><comment author="martijnvg" created="2015-12-07T09:02:40Z" id="162453374">This can be closed. I can't find the fix, but it stopped failing a long time ago (20th April).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Batching of snapshot state updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10295</link><project id="" key="" /><description>Similar to the batching of "shards-started" actions, this PR implements batching of snapshot status updates. This is useful when backing up many indices as the cluster state does not need to be republished as many times.
</description><key id="64742886">10295</key><summary>Batching of snapshot state updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ywelsch-t</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-27T11:28:43Z</created><updated>2015-05-21T00:52:59Z</updated><resolved>2015-05-21T00:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-03-27T17:47:01Z" id="87030588">@ywelsch-t have you signed [CLA](https://github.com/elastic/elasticsearch/blob/9646083c5ae00466cb4f68318f24609d2d696333/CONTRIBUTING.md#submitting-your-changes)? I cannot find your name there for some reason.
</comment><comment author="ywelsch-t" created="2015-04-01T12:14:40Z" id="88455372">@imotov I'm clearing things with our legal department. Will probably take a week. Sorry for that.
</comment><comment author="imotov" created="2015-05-08T00:51:32Z" id="100056874">@ywelsch-t any news?
</comment><comment author="ywelsch-t" created="2015-05-11T05:56:56Z" id="100776731">The CLA has been signed last Friday.
</comment><comment author="imotov" created="2015-05-13T16:50:07Z" id="101743620">@ywelsch-t Indeed the CLA was signed. Thanks a lot for PR. I have reviewed it and left a few comments.  Could you also add a test that would verify that shard updates are indeed batching?
</comment><comment author="ywelsch-t" created="2015-05-15T12:43:03Z" id="102388650">@imotov Thanks for the feedback. I have incorporated all your suggestions. I do not know however how to test that batching indeed occurs (only validated it manually). I looked at the batching of "shard started" actions (which inspired this PR), but could not find a test case to draw inspiration from.
</comment><comment author="imotov" created="2015-05-15T23:27:12Z" id="102546582">@ywelsch-t I think we are getting there. I left a few minor comments. I think for the test we can do something like this: https://gist.github.com/imotov/6460fe90ffd3678573d7 What do you think?
</comment><comment author="ywelsch-t" created="2015-05-18T08:18:36Z" id="102966346">Test looks good to me (thanks for the help!). If all is good, I can squash the commits and add a proper commit message.
</comment><comment author="imotov" created="2015-05-18T19:31:39Z" id="103183272">@ywelsch-t great, please also rebase against current master after squashing. 
</comment><comment author="ywelsch-t" created="2015-05-19T11:41:48Z" id="103455191">@imotov done
</comment><comment author="imotov" created="2015-05-20T03:57:40Z" id="103746751">@ywelsch-t thanks! I was about to push it to the master, but then I noticed that the test fails from time to time. I will try to figure out and fix it tomorrow and then push.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary joda-convert dependency (issue #10290)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10294</link><project id="" key="" /><description /><key id="64724757">10294</key><summary>Remove unnecessary joda-convert dependency (issue #10290)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marko-asplund</reporter><labels /><created>2015-03-27T09:14:10Z</created><updated>2016-03-08T19:24:45Z</updated><resolved>2016-03-08T19:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-27T09:22:37Z" id="86878592">thx @marko-asplund . It looks like this was put in on purpose to solve another issue (#4660) . This problem might be better fixed in the shading side of things. Still researching..
</comment><comment author="marko-asplund" created="2015-03-27T10:41:03Z" id="86898042">We're actually building custom ES plugins using Scala and bundling custom code with ES code in one jar. This worked well up-to ES 1.4.4, but because we're using joda-convert there's a naming clash when I tried upgrading to ES 1.5.0. If I build ES 1.5.0 (in ES sandbox) without the joda-convert dependency ES compiles. So, it seems that joda-convert is not required at compile-time. Also,  I don't get any additional test failures when I run the tests. With the ES 1.5.0 jar built this way, I'm also able to build and run our customized ES bundle.

I'm using Scala 2.11.6, Java 8 and sbt 0.13.7.
</comment><comment author="kimchy" created="2015-03-27T14:56:57Z" id="86964147">I mused admit I am confused now :), we added this and shaded it to fix a bug in Scala compiler, and now it complains again and we need to take it out? Won't it cause the break that it was first added to fix in the first place?
</comment><comment author="marko-asplund" created="2015-03-27T15:49:15Z" id="86982030">Just to clarify: The issue I'm seeing is not a Scala compiler error, it's an error with the sbt assembly plugin. The plugin gets confused when classes mentioned in #10290 are available from two different sources). This could be worked around in our sbt build config, but I think including the classes in es.jar will result in classpath / namespace collision issues for other developers when custom plugin code needs to use joda-convert directly or indirectly through a dependency. From the perspective of this issue (though I'm not familiar with issue #4660), I think a good solution would be to place the repackaged joda-convert classes in a different namespace (such as org.elasticsearch).
</comment><comment author="marko-asplund" created="2015-03-27T21:54:26Z" id="87102907">Added detailed description on how to reproduce this problem on issue #10290.

Also, I tried to reproduce issue #4660 by changing making the following changes to the demo project:
- Elasticsearch 1.0.0.Beta2, Scala 2.10.1, OpenJDK 1.7.0_76 (Zulu) (build.sbt)
- removed the joda-convert dependency from build.sbt + FooRiver.scala

but the demo project the codebase still gets built successfully and I'm unable to reproduce issue #4660.
</comment><comment author="l15k4" created="2015-04-07T20:54:10Z" id="90729771">Possible fix until this is resolved : 

``` scala
      assemblyExcludedJars in assembly := {
        val cp = (fullClasspath in assembly).value
        cp filter {_.data.getName == "joda-convert-1.7.jar"}
      }
```

This is the only way I found to work this around...
</comment><comment author="marko-asplund" created="2015-05-26T08:04:03Z" id="105434827">What's the status with this issue?

Just tried it with the latest version and it still affects ES 1.5.2 preventing us from upgrading.
To re-iterate: this is a regression introduced after ES 1.4.4 and it's reproducible with code found in #10290.

Has someone else been able to reproduce issue #4660? Where can I find instructions or code for reproducing it?
</comment><comment author="fcofdez" created="2015-10-07T15:01:38Z" id="146222334">I'm having the same problem with ES 1.7.1
</comment><comment author="emmekappa" created="2015-12-21T14:45:03Z" id="166320726">Any updates?
</comment><comment author="clintongormley" created="2016-03-08T19:24:45Z" id="193930930">Closing in favour of #12829
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Skip Version.CURRENT for static bwc indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10293</link><project id="" key="" /><description>The current version is normally a snapshot while in development.
However, when the release process changes the snapshot flag to false,
this causes the static bwc tests to fail because they cannot
find an index for the current version.  Instead, this change
skips the current version, because there is no need to test
a verion's bwc against itself.

closes #10292
</description><key id="64722465">10293</key><summary>Tests: Skip Version.CURRENT for static bwc indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-27T08:55:51Z</created><updated>2015-04-04T04:57:58Z</updated><resolved>2015-04-04T02:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-27T09:08:18Z" id="86874564">LGTM. Left one minor comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests fail when building ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10292</link><project id="" key="" /><description>I've checked out ES source with v1.5.0 tag and when I try to build tests fail with the following tests:
- org.elasticsearch.bwcompat.RestoreBackwardsCompatTests.restoreOldSnapshots
- org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityTests.testAllVersionsTested

This happens on Mac OS X with Java 8 + Maven 3.2.5

The README advises to skip tests, but it's bad practice to routinely do this since this way contributors won't be able to get any confirmation that their code actually works apart from it being syntactically correct.

``````
mvn clean package
...
Tests with failures:
  - org.elasticsearch.bwcompat.RestoreBackwardsCompatTests.restoreOldSnapshots
  - org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityTests.testAllVersionsTested


[INFO] JVM J0:     0.67 ..   541.62 =   540.95s
[INFO] JVM J1:     0.67 ..   541.47 =   540.80s
[INFO] JVM J2:     0.67 ..   558.75 =   558.08s
[INFO] JVM J3:     0.67 ..   541.54 =   540.87s
[INFO] Execution time total: 9 minutes 18 seconds
[INFO] Tests summary: 708 suites, 4699 tests, 2 failures, 70 ignored (59 assumptions)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 09:55 min
[INFO] Finished at: 2015-03-27T10:26:36+02:00
[INFO] Final Memory: 31M/1331M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.2:junit4 (tests) on project elasticsearch: Execution tests of goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.2:junit4 failed: /Users/marko/projects/3rd-party/elasticsearch/target/junit4-ant-7275726970577951678.xml:16: There were test failures: 708 suites, 4699 tests, 2 failures, 70 ignored (59 assumptions) -&gt; [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
~/projects/3rd-party/elasticsearch (5448160 &#10004;) &#5125; java -version
java version "1.8.0_40"
Java(TM) SE Runtime Environment (build 1.8.0_40-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)```
``````
</description><key id="64719907">10292</key><summary>Tests fail when building ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marko-asplund</reporter><labels /><created>2015-03-27T08:34:55Z</created><updated>2015-04-04T02:27:30Z</updated><resolved>2015-04-04T02:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-27T08:57:28Z" id="86872972">@marko-asplund Thanks for the bug report. This is an issue with how the release script changes code (from a snapshot to release) after we run the tests. I've proposed a fix in #10293.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch overload when handle many connections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10291</link><project id="" key="" /><description>Hi all

I have installed ElasticSearch (ES) version 1.5 in our server (only one server)
I set ulimit ~ 64K (max open file).
Our server: 4 virtual CPUs, 7.5 GB RAM (Amazon EC2)

after I start ES, many users can search with reponse respectively. But after 30 seconds to 1 minute, our ES can not response anymore connect ffrom client (PHP library, just implement curl command)

I try to get PID of ES, and run command: ls /proc/ES_ID/fd | wc -l
the result is increase every second (~ 2 connect), and the CPU that ES process take about ~ 100%, RAM free about 60%

I try to view log, here is some logs:
_[2015-03-27 07:57:33,512][DEBUG][http.netty               ] [Impossible Man] Caught exception while handling client http traffic, closing connection [id: 0xbd4fa29e, /10.0.0.166:43963 :&gt; /10.0.0.230:9200]
java.nio.channels.ClosedChannelException
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:433)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromUserCode(AbstractNioWorker.java:128)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:99)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:199)
        at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
        at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:149)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2015-03-27 07:57:38,286][DEBUG][monitor.jvm              ] [Impossible Man] [gc][old][207][165] duration [2.7s], collections [1]/[2.8s], total [2.7s]/[5.1m], memory [811.9mb]-&gt;[836mb]/[990.7mb], all_pools {[young] [120.7mb]-&gt;[144.9mb]/[266.2mb]}{[survivor] [0b]-&gt;[0b]/[33.2mb]}{[old] [691.2mb]-&gt;[691.2mb]/[691.2mb]}_
# 

Here is some configurations I added:
script.groovy.sandbox.enabled: true

bootstrap.mlockall: true

threadpool.index.type: fixed
threadpool.index.size: 4
threadpool.index.queue_size: 400
threadpool.search.queue_size: 1000
threadpool.search.type: cached
threadpool.bulk.type: fixed
threadpool.bulk.size: 4                 # availableProcessors
threadpool.bulk.queue_size: 1000

And the remaining configurations are default.

With those connections (many connections), when I use version 0.9x, everything still OK - work normally. I must upgrade because critical security of ES :(

Could you help me solve that problem :( :(((((((((((((((((((((
</description><key id="64717040">10291</key><summary>ElasticSearch overload when handle many connections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quydo</reporter><labels><label>feedback_needed</label></labels><created>2015-03-27T08:07:39Z</created><updated>2015-08-05T11:14:21Z</updated><resolved>2015-04-05T12:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="quydo" created="2015-03-27T08:20:29Z" id="86865378">When the ES service started, I try to run, curl 'localhost:9200/_cluster/stats?network=true&amp;transport=true&amp;http=true&amp;thread_pool=true&amp;indices=false&amp;pretty=true'

And I get the result:
      "open_file_descriptors" : {
        "min" : 211,
        "max" : 211,
        "avg" : 211
      }

why open_file_descriptors are very large :(
</comment><comment author="quydo" created="2015-03-27T09:29:09Z" id="86879807">When I switch to version 0.9.x again, here is command "mpstat -P ALL 1" response output:

09:27:39     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
09:27:40     all    0,25    0,00    0,00    0,00    0,00    0,00    0,00    0,00   99,75
09:27:41     all    0,50    0,00    0,00    0,00    0,00    0,00    0,00    0,00   99,50
09:27:42     all    0,63    0,00    0,00    0,13    0,00    0,00    0,00    0,00   99,25
09:27:43     all    0,25    0,00    0,13    0,13    0,00    0,00    0,00    0,00   99,50
09:27:44     all    0,13    0,00    0,00    0,00    0,00    0,00    0,00    0,00   99,87
09:27:45     all    1,88    0,00    0,13    0,50    0,00    0,00    0,00    0,00   97,49

who think that problem is version 1.5 take many CPU?
</comment><comment author="clintongormley" created="2015-03-29T11:56:04Z" id="87403342">Hiya

That network error occurs when the client network connection disconnects, perhaps because you set a request timeout?

It looks like your Elasticsearch is under severe memory pressure.  You've given it less than one GB of heap, and it needs more to cope with your load.

Also, you say you had to upgrade ES because of security, but then you have reenabled dynamic scripting, so you are allowing anybody with access to your box to run whatever scripts they want, defeating the point of the upgrade.

The number of open file descriptors you have is not large at all.  Elasticsearch can easily use many more file handles than you are currently using.

Also check (look at GET /_nodes) to ensure that mlockall is being applied correctly.
</comment><comment author="clintongormley" created="2015-03-29T11:56:39Z" id="87403368">Also you can check what is using all the CPU with the hot threads API, but I think you'll find that it is mostly garbage collection.
</comment><comment author="quydo" created="2015-03-29T15:38:32Z" id="87428564">Hi clintongormley

what is the best value of HEAP size should I assign, my server RAM is 4GB (maybe 8GB)

currently, the mlockall value is true, is that ok? And what is the hots API.

Thank you very much.
</comment><comment author="quydo" created="2015-03-30T04:56:19Z" id="87547121">Hi guy,

Here is some variables in init script

es_heap_size:  2G
es_heap_newsize: 
es_min_mem:  512M
es_max_mem:  1G
max_locked_memory: unlimited

I try to add ES_HEAP_SIZE to 2G, and ElasticSearch server run normally for 3 hours, and after that, ES will not response request anymore (like the issue I created).

Here is the lastest log:

[root@ip-10-0-0-230 quydo]# tailf /var/log/elasticsearch/elasticsearch.log  
[2015-03-30 04:47:41,217][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6235][112] duration [3.5s], collections [1$
/[4.2s], total [3.5s]/[1.7m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [394.5kb]-&gt;[270.9kb]/[266.2mb]}{[survivor] [0b$
-&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
[2015-03-30 04:47:47,218][INFO ][monitor.jvm              ] [Karolina Dean] [gc][old][6236][113] duration [5.5s], collections [1$
/[6s], total [5.5s]/[1.8m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [270.9kb]-&gt;[27.3mb]/[266.2mb]}{[survivor] [0b]-&gt;$
0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
[2015-03-30 04:47:53,708][INFO ][monitor.jvm              ] [Karolina Dean] [gc][old][6238][114] duration [5.4s], collections [1$
/[5.4s], total [5.4s]/[1.9m], memory [1.7gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [31.9mb]-&gt;[167.3kb]/[266.2mb]}{[survivor] [29m$
]-&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
[2015-03-30 04:47:58,135][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6239][115] duration [3.4s], collections [1$
/[4.4s], total [3.4s]/[2m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [167.3kb]-&gt;[15.2mb]/[266.2mb]}{[survivor] [0b]-&gt;$
12.1mb]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
[2015-03-30 04:48:02,313][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6240][116] duration [3.6s], collections [1$
/[4.1s], total [3.6s]/[2m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [15.2mb]-&gt;[1005kb]/[266.2mb]}{[survivor] [12.1mb$
-&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
[2015-03-30 04:48:05,667][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6241][117] duration [3.2s], collections [1$
/[3.3s], total [3.2s]/[2.1m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [1005kb]-&gt;[372.4kb]/[266.2mb]}{[survivor] [0b]$

&gt; [0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
&gt; [2015-03-30 04:48:09,417][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6242][118] duration [3.6s], collections [1$
&gt; /[3.7s], total [3.6s]/[2.2m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [372.4kb]-&gt;[45.1kb]/[266.2mb]}{[survivor] [0b]$
&gt; [0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
&gt; [2015-03-30 04:48:13,156][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6243][119] duration [3.6s], collections [1$
&gt; /[3.7s], total [3.6s]/[2.2m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [45.1kb]-&gt;[410.4kb]/[266.2mb]}{[survivor] [0b]$
&gt; [0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
&gt; [2015-03-30 04:48:16,903][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6244][120] duration [3.6s], collections [1$
&gt; /[3.7s], total [3.6s]/[2.3m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [410.4kb]-&gt;[501.1kb]/[266.2mb]}{[survivor] [0b$
&gt; -&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
&gt; [2015-03-30 04:48:20,369][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6245][121] duration [3.3s], collections [1]
&gt; /[3.4s], total [3.3s]/[2.3m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [501.1kb]-&gt;[422.1kb]/[266.2mb]}{[survivor] [0b]
&gt; -&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}
&gt; [2015-03-30 04:48:23,694][DEBUG][monitor.jvm              ] [Karolina Dean] [gc][old][6246][122] duration [3.2s], collections [1]
&gt; /[3.3s], total [3.2s]/[2.4m], memory [1.6gb]-&gt;[1.6gb]/[1.9gb], all_pools {[young] [422.1kb]-&gt;[322.5kb]/[266.2mb]}{[survivor] [0b]
&gt; -&gt;[0b]/[33.2mb]}{[old] [1.6gb]-&gt;[1.6gb]/[1.6gb]}

What should I do to solve that problem
</comment><comment author="clintongormley" created="2015-04-05T12:44:45Z" id="89764657">It looks like you need more memory.  Your heap is full.
</comment><comment author="splashx" created="2015-07-31T14:18:40Z" id="126706842">@clintongormley if the heap is full shouldn't it use disk to store? I thought so if it doesn't have index.store.type set to memory.
</comment><comment author="clintongormley" created="2015-08-05T11:14:21Z" id="127957275">@splashx that would simply kill performance
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Naming clash with Joda convert classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10290</link><project id="" key="" /><description>elasticsearch.jar (v1.5.0) re-packages Joda convert classes:
- org.joda.convert.FromString
- org.joda.convert.ToString

this will create a naming clash in projects that use Joda convert.
</description><key id="64713400">10290</key><summary>Naming clash with Joda convert classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marko-asplund</reporter><labels><label>:Packaging</label><label>bug</label><label>discuss</label></labels><created>2015-03-27T07:39:25Z</created><updated>2015-09-15T14:04:30Z</updated><resolved>2015-09-15T14:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Pyppe" created="2015-03-27T08:28:18Z" id="86867819">Same issue here. Cannot create our own packaged application, because elasticsearch.jar contains the above-mentioned joda-classes.
</comment><comment author="marko-asplund" created="2015-03-27T21:52:38Z" id="87102596">here's how to reproduce this issue:

```
wget https://dl.bintray.com/sbt/native-packages/sbt/0.13.8/sbt-0.13.8.zip
unzip sbt-0.13.8.zip
git clone https://github.com/marko-asplund/es-repro
cd es-repro
../sbt/bin/sbt assembly
```

Assembly fails with ES 1.5.0. However, if you edit `build.sbt` in the repository to replace the ES 1.5.0 dependency with ES 1.4.4 assembly is performed successfully.
</comment><comment author="fuwaneko" created="2015-06-04T14:23:55Z" id="108913200">Very annoying bug effectively preventing us from creating fat jar in our project. Looking back to original issue #4660 that introduced monkey-patched files from joda-convert into ES I can only sigh at how developers sometimes do things purely wrong and forget about it until it blows up someday.

I can't even use usual sbt/sbt-assembly stuff to exclude these files during build. That's unacceptable.
</comment><comment author="pheaver" created="2015-08-29T16:21:43Z" id="136006814">Yup, I'm also unable to build a package because of this.
</comment><comment author="s1monw" created="2015-08-29T16:32:47Z" id="136007640">can you guys use the shaded jar? 

``` xml
 &lt;groupId&gt;org.elasticsearch.distribution.shaded&lt;/groupId&gt;
 &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
```
</comment><comment author="pheaver" created="2015-08-29T16:41:06Z" id="136008447">Oh, I'm not familiar with that. Do you know how I would use that in sbt?
</comment><comment author="s1monw" created="2015-08-29T16:41:45Z" id="136008480">I don't know sbt but how do you use elasticsearch?
</comment><comment author="s1monw" created="2015-08-29T16:42:55Z" id="136008526">`libraryDependencies += "org.elasticsearch.distribution.shaded" % "elasticsearch" % "2.0.0-beta1"` according to http://search.maven.org/#artifactdetails%7Corg.elasticsearch.distribution.shaded%7Celasticsearch%7C2.0.0-beta1%7Cjar
</comment><comment author="pheaver" created="2015-08-29T17:30:43Z" id="136015068">So the shaded package is only available as 2.x? I'm not sure if that will work.

Also, I'm using https://github.com/sksamuel/elastic4s, which maybe has a dependency on the non-shaded elasticsearch, so even when I try the shaded package I still get errors about conflicting versions of elasticsearch.
</comment><comment author="s1monw" created="2015-08-29T17:59:45Z" id="136019612">&gt; So the shaded package is only available as 2.x? I'm not sure if that will work.

yeah the 1.x dev branch is closed we are mainly focusing on 2.x now
</comment><comment author="m4dc4p" created="2015-09-02T22:44:11Z" id="137265426">2.0.0 isn't even out of beta. Please fix this issue. It's super annoying. Just do a point release! (1.7.2)
</comment><comment author="clintongormley" created="2015-09-15T14:04:21Z" id="140403819">Closed by #13358
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Watcher should not try to compile .swp (or other .*) files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10289</link><project id="" key="" /><description>I was editing a file in `config/scripts` when this showed up in my logs:

```
[2015-03-26 23:42:25,022][WARN ][script] [Madcap] no script engine found for [swp]
[2015-03-26 23:42:50,023][INFO ][script] [Madcap] removing script file [/etc/elasticsearch/scripts/.myScript.groovy.swp]
[2015-03-26 23:42:50,023][INFO ][script] [Madcap] compiling script file [/etc/elasticsearch/scripts/myScript.groovy]
```

I'm thinking the watcher should ignore `.swp` files.  Better yet, it should ignore `.*`
</description><key id="64657968">10289</key><summary>Watcher should not try to compile .swp (or other .*) files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsnod</reporter><labels><label>:Scripting</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-03-26T23:48:13Z</created><updated>2017-05-28T05:40:36Z</updated><resolved>2017-05-28T05:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-04-06T18:30:09Z" id="90187989">I think logging a warning is the right thing to do here? What if someone creates a plugin to add a scripting engine called `swp`? I also like the simplicity of the way it works now, if we start adding complex exclusion patterns I'm afraid it would become harder to reason about what gets compiled and what not.
</comment><comment author="clintongormley" created="2015-04-06T19:01:03Z" id="90198892">@jpountz how about only trying to compile recognised extensions? Makes more sense to me than compiling anything in the folder.
</comment><comment author="dadoonet" created="2015-04-06T19:08:29Z" id="90201761">+1 we should only compile expected extensions. My 0.05 cents.
</comment><comment author="rmuir" created="2015-04-06T19:46:53Z" id="90216136">I see other places in the code that ignore hidden files here (using isHidden). I think we should be consistent about this stuff, if we are going to look for only certain extensions here, then we should nuke the isHidden checks in the other places.
</comment><comment author="jpountz" created="2015-04-06T19:47:59Z" id="90217436">@clintongormley @dadoonet  What if you install elasticsearch on a new machine, move your scripts to `config/scripts` but forget to install the scripting plugin. You would get errors that the scripts could not be found while they are in the expected folder. I think I could easily get confused while the warning that we have in the logs today would clearly explain what the issue is?
</comment><comment author="jsnod" created="2015-04-06T21:55:49Z" id="90257052">Sorry my request wasn't clear -- it is not the extension that is the issue, but the fact that it is a hidden file prefixed with the `.` character.  If your scripting engine was called `swp`, I think it would be fine to load `myScript.swp`.  But I think it is problematic to load `.myScript.swp` (notice the dot in the front).  Watcher shouldn't attempt to load dot-files files regardless of their extension.
</comment><comment author="jsnod" created="2015-04-06T21:59:07Z" id="90257509">Although, now thinking about it some more, I could see an situation where someone copies an existing script to `myScript.groovy.bak` or `myScript.groovy.20150406` and then creates a new version of `myScript.groovy`.  Does that mean watcher will compile both files?  What if we have 50 backups in the directory, will watcher compile them all?  Perhaps that is an argument for compiling only expected extensions.
</comment><comment author="clintongormley" created="2015-04-07T12:13:30Z" id="90530662">&gt; @clintongormley @dadoonet What if you install elasticsearch on a new machine, move your scripts to config/scripts but forget to install the scripting plugin. You would get errors that the scripts could not be found while they are in the expected folder. I think I could easily get confused while the warning that we have in the logs today would clearly explain what the issue is?

Surely the error would be "unsupported language XXX" instead?

&gt; Although, now thinking about it some more, I could see an situation where someone copies an existing script to myScript.groovy.bak or myScript.groovy.20150406 and then creates a new version of myScript.groovy. Does that mean watcher will compile both files?

I think so.  We have had similar issues with config files, eg https://github.com/elastic/elasticsearch/issues/8040
</comment><comment author="rjernst" created="2017-05-28T05:40:36Z" id="304494287">File scripts have been removed for 6.0 so I am closing this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove current delete-by-query implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10288</link><project id="" key="" /><description>This is master only, and will close #10067.

I removed DBQ everywhere except Engine/Translog/IndexShard so that on upgrade a DBQ in the translog will still apply (the back-compat tests now check this as of #10266).

Once we do #7052 for 2.0 we'll add back delete-by-query, but implemented as a sugar API to run a scan query and scroll through all hits, doing bulk delete for them.
</description><key id="64649884">10288</key><summary>Remove current delete-by-query implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>breaking</label></labels><created>2015-03-26T22:42:48Z</created><updated>2015-05-29T18:10:06Z</updated><resolved>2015-03-30T08:50:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-27T10:10:26Z" id="86888649">man the only concern I have is, if we add this back later we have to re-add all the API here. I really hate to say it but it might make sense to implement the entire thing as a pure client side simple method for now that really just uses scan/scroll in the java client that way we can get rid of all the internal code, keep the tests and have the API still?
</comment><comment author="mikemccand" created="2015-03-27T18:08:06Z" id="87036541">&gt;  implement the entire thing as a pure client side simple method for now 

I agree this should really be "client side", somehow.

The problem is, every client would need to implement it (I am partial to the Python client so I would naturally do that one first :smirk: )?  Maybe that's OK?

Or can we do it as server side sugar, so that whichever node receives this request, would (somehow) open a client (or maybe rest/http) connection and issue the request?  Does every ES node already have a [Java] client node instance it can use to talk to other nodes?  (I am way out of my comfort zone here....).

Maybe there is an example where we already do something similar to this?  I could use that for inspiration/starting point ... e.g. when we cast attachments to plain text using Tika (the mapper-attachment plugin) do we do this once per document and then index that result on primary + replicas as if we were a client...?
</comment><comment author="s1monw" created="2015-03-27T20:37:55Z" id="87081645">we could implement this inside the Java client API as an implementation detail of `AbstractClient.java` and make those methods final? That way we can keep all the api and only trash the transport part and the engine internals. We might even be able to port this to 1.6 which would be great too.
</comment><comment author="TwP" created="2015-03-29T05:35:05Z" id="87360453">Is it not possible to keep the current ES API and implement delete-by-query in the server itself as an internal scan/scroll + bulk delete? That seems like the best solution here as it would avoid implementing this logic in all language clients.
</comment><comment author="kimchy" created="2015-03-29T07:36:42Z" id="87366711">I am leaning towards @TwP idea, I think its common enough API that ES should implement internally. I am using similar logic here into why, for example, reindex is better implemented in ES itself, even though the client libs implement it currently.
</comment><comment author="kimchy" created="2015-03-29T07:39:58Z" id="87366774">@s1monw yea, if we do scan search + bulk delete we can trash the engine part for sure. If we keep the transport action part, we can broadcast and do the scan search collocated with the primary. If we remove it, then we will need to scan search across, which might not be the end of the world.
</comment><comment author="s1monw" created="2015-03-30T08:31:25Z" id="87590728">@mikemccand yeah let's do a simple scroll/scan in the transport action so we can do it per shard?
</comment><comment author="mikemccand" created="2015-03-30T08:36:08Z" id="87591711">&gt; we can broadcast and do the scan search collocated with the primary

I agree this would be more efficient.  It would also mean the DBQ runs concurrently on each shard I think...

So this means the primary receives the DBQ, runs the scan/scroll search (locally), then issues (replicated) bulk delete-by-id ops within its shard.

I wonder if I can somehow do this in the existing TransportShardDeleteByQueryAction class... seems like it's not quite flexible enough (assumes the op is done on primary then done on replicas).  Maybe we need specialized code in TransportShardReplicationOperationAction for DBQ?  Or am I looking in entirely the wrong place :)

Separately, I noticed we currently fail to trigger a refresh if version map RAM is too much due to only deletions ... I'll open a separate PR.
</comment><comment author="mikemccand" created="2015-03-30T08:50:39Z" id="87595264">I'm closing this PR as "won't fix": let's have further discussions about the approach on #7052.

However I will say I'm doing so under protest: I think if we have suitably dangerous features in ES, we should be free to simply remove them outright, even if we don't have an alternative implementation immediately in mind / at hand.  Coupling removal with "you must also build a better replacement" puts up a big barrier to correcting past mistakes...

I will work towards the new implementation under #7052.
</comment><comment author="s1monw" created="2015-03-30T08:51:47Z" id="87595672">@mikemccand I am more than with you. IMO you cam push it as it is and we add the relevant stuff back later!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to launch bootstrapped ES using maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10287</link><project id="" key="" /><description>This adds the exec-maven-plugin that allows a developer to run:

```
mvn exec:exec
```

To launch the `Bootstrap` process similar to the way that a Java IDE
would. All the logs go to logs/elasticsearch.log (or wherever
configured)
</description><key id="64645707">10287</key><summary>Add ability to launch bootstrapped ES using maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>build</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T22:16:27Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-03-30T17:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-26T22:17:43Z" id="86737052">This bumps into the (admittedly very low) ceiling of my Maven knowledge, so maybe @dadoonet can take a look?
</comment><comment author="jpountz" created="2015-03-26T23:19:26Z" id="86751290">This looks great! Maybe add a note about it to the README? This would definitely have helped me a lot when I started digging elasticsearch.
</comment><comment author="dakrone" created="2015-03-30T17:01:10Z" id="87751282">@jpountz added a note to TESTING.asciidoc (it seems to fit better there), thanks for the suggestion!
</comment><comment author="jpountz" created="2015-03-30T17:03:05Z" id="87752223">LGTM. I see you tagged it with 1.5.1, I don't think this is required as this is a new feature?
</comment><comment author="dakrone" created="2015-03-30T17:06:16Z" id="87752827">@jpountz whoops, you're right, removed that, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for `script.disable_dynamic` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10286</link><project id="" key="" /><description>Now that fine-grained script settings are supported (#10116) we can remove support for the `script.disable_dynamic` setting, which is deprecated in our docs as of `1.6.0`. Also, from `1.6.0` a deprecation warning will be printed out at startup in the logs whenever using the `'script.disable_dynamic` param in `elasticsearch.yml`.

Note that the same result as `script.disable_dynamic: false` can be obtained as follows:

```
script.inline: on
script.indexed: on
```
</description><key id="64620957">10286</key><summary>Remove support for `script.disable_dynamic` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T20:06:30Z</created><updated>2015-06-06T17:27:38Z</updated><resolved>2015-03-31T12:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-31T08:53:42Z" id="88000074">we have to at least add this to `docs/reference/migration/migrate_2_0.asciidoc` or even throw an exception if somebody still uses this setting?
</comment><comment author="javanna" created="2015-03-31T09:15:11Z" id="88009140">good point @s1monw I addressed your comments
</comment><comment author="s1monw" created="2015-03-31T10:47:18Z" id="88038735">LGTM
</comment><comment author="mrsolo" created="2015-04-03T16:14:20Z" id="89340543">Comment.  

This is the current main branch behavior when encountering script.disable_dynamic and on.. pretty substantial log spam  

```
2015-04-03 17:56:59,616][INFO ][node                     ] [Abomination] version[2.0.0-SNAPSHOT], pid[30537], build[d3b1567/2015-04-03T14:33:31Z]
[2015-04-03 17:56:59,616][INFO ][node                     ] [Abomination] initializing ...
[2015-04-03 17:56:59,619][INFO ][plugins                  ] [Abomination] loaded [], sites []
[2015-04-03 17:57:06,121][ERROR][bootstrap                ] Exception
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.ElasticsearchIllegalArgumentException: script.disable_dynamic is not a supported setting, replace with fine-grained script settings. 
Dynamic scripts can be enabled for all languages and all operations by replacing `script.disable_dynamic: false` with `script.inline: on` and `script.indexed: on` in elasticsearch.yml
  at org.elasticsearch.script.ScriptService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.script.ScriptService
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: script.disable_dynamic is not a supported setting, replace with fine-grained script settings. 
Dynamic scripts can be enabled for all languages and all operations by replacing `script.disable_dynamic: false` with `script.inline: on` and `script.indexed: on` in elasticsearch.yml
    at org.elasticsearch.script.ScriptService.&lt;init&gt;(ScriptService.java:120)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:158)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:66)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:200)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch results using JAVA</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10285</link><project id="" key="" /><description>HI all, 
I am trying to use ElasticSearch within the JAVA API. As a JSON File (named Google.json) I am using the following structure:
{
 "markers": [
     {
              "point":"new GLatLng(40.266044,-74.718479)", 
      "homeTeam":"Lawrence Library",
              "awayTeam":"LUGip",
              "markerImage":"images/red.png",
              "information": "Linux users group meets second Wednesday of each month.",
              "fixture":"Wednesday 7pm",
              "capacity":"",
             },
             {
              "point":"new GLatLng(40.266044,-74.75022)", 
      "homeTeam":"Hamilton Library",
              "awayTeam":"LUGip HW SIG",
              "markerImage":"images/white.png",
              "information": "Linux users group meets second Tuesday of each month.",
              "fixture":"Tuesday 7pm",
              "capacity":"",
             }
           ]
 }

Using Jackson I am converting this file into a JAVA Hashmap. Then I try to search in field "fixture" for the Word "Tuesday". I am finding this, but as a result I am getting the complete "markers" List back instead of only the second element. I am now wondering what the problem is with my approach.

Here is my JAVA code:

JSONParser parser= new JSONParser();
String path="d:\Google.json";

Node node = nodeBuilder().local(true).node();
Client client = node.client();

/\* Jackson mapper*/  
ObjectMapper mapper = new ObjectMapper();
File jsonFile=new File(path);

try {
            /_Read in file using Jackson into HashMap_/
            Map&lt;String, Object&gt; mapObject=new HashMap&lt;String, Object&gt;();
            mapObject = mapper.readValue(jsonFile, new TypeReference&lt;Map&lt;String, Object&gt;&gt;(){});

```
        /*Create index*/            
        IndexResponse response=null;
        response = client.prepareIndex(index, type)
                .setSource(mapObject)               
                .execute()
                .actionGet();


        } catch (JsonParseException e1) {
     e1.printStackTrace();
} catch (JsonMappingException e1) {
    e1.printStackTrace();
} catch (IOException e1) {
    e1.printStackTrace();
}

    QueryBuilder qb=QueryBuilders.matchQuery("fixture","Tuesday");

    SearchResponse response= client.prepareSearch(index)
            .setTypes(type)
            .setSearchType(SearchType.QUERY_AND_FETCH)
            .setQuery(qb)
            .setFrom(0)
            .setSize(100)
            .setExplain(true)
            .execute().actionGet();

    SearchHit[] results = response.getHits().getHits();

    System.out.println("Current results: " + results.length);

    for (SearchHit hit : results) {
        System.out.println("------------------------------");
        Map&lt;String,Object&gt; result = hit.getSource();   
        System.out.println(result);
    }
```

It would be great to get any hint/help on this!

Cheers,
Andi
</description><key id="64615684">10285</key><summary>ElasticSearch results using JAVA</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AGrosserHH</reporter><labels /><created>2015-03-26T19:35:44Z</created><updated>2015-03-27T17:04:59Z</updated><resolved>2015-03-27T17:04:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-27T17:04:59Z" id="87011802">HI @AndiG1985 could you please ask your question on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch)? We generally use github issues for bugs and feature requests. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: formatted string values are always returned in 1.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10284</link><project id="" key="" /><description>In ES 1.5, many aggregations now return a formatted `&lt;foo&gt;_as_string` version of the return values by default, even without specifying a `format` in the request, or regardless of the field type.

For instance:

``` json
"aggregations": {
      "my_stats": {
         "count": 54,
         "min": 10148,
         "max": 19602,
         "avg": 15151.611111111111,
         "sum": 818187,
         "min_as_string": "10148.0",
         "max_as_string": "19602.0",
         "avg_as_string": "15151.611111111111",
         "sum_as_string": "818187.0"
      }
   }
```

Aggregations where I've observed this:

`as_string` always present:
- min
- max
- sum
- avg
- stats
- extended_stats
- value_count
- percentiles
- percentiles_ranks
- cardinality

`as_string` only present if the field is a date, regardless if `format` is specified or not:
- terms
- histogram
- date_histogram

Does it ever make sense to return a formatted version of a numeric field?  For dates, should we only return a formatted version if `format` is specified in the request?
</description><key id="64605381">10284</key><summary>Aggregations: formatted string values are always returned in 1.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.5.2</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T18:47:33Z</created><updated>2015-04-14T15:41:29Z</updated><resolved>2015-04-14T15:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T12:07:15Z" id="89757436">@colings86 was this from one of your changes?
</comment><comment author="colings86" created="2015-04-07T08:05:06Z" id="90445136">@clintongormley yes, I think it might be due to https://github.com/elastic/elasticsearch/pull/9032. I'll assign it to myself and hopefully get a fix soon

@gmarz formatting numeric results can be useful to do things such as padding and rounding when required. The `format` parameter should work on terms, and histogram when the field is numeric. I will look into that as well as fixing this issue so the `as_string` field is only present in the result when `format` is specified (or the field is a date).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail shard when index service/mappings fails to instantiate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10283</link><project id="" key="" /><description>When the index service (which holds shards) fails to be created as a result of a shard being allocated on a node, we should fail the relevant shard, otherwise, it will remain stuck.
Same goes when there is a failure to process updated mappings form the master.

Note, both failures typically happen when the node is misconfigured (i.e. missing plugins, ...), since they get created and processed on the master node before being published.
</description><key id="64601570">10283</key><summary>Fail shard when index service/mappings fails to instantiate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T18:29:15Z</created><updated>2015-04-09T08:47:41Z</updated><resolved>2015-03-27T15:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-26T18:47:31Z" id="86665593">Left one comment about grammar, but LGTM otherwise
</comment><comment author="kimchy" created="2015-03-26T20:58:12Z" id="86712614">@dakrone thanks for the review, updated the log message, will wait for another review
</comment><comment author="bleskes" created="2015-03-27T13:54:54Z" id="86946113">Change LGTM2 - left some comments regarding logging and error reporting
</comment><comment author="bleskes" created="2015-03-27T13:55:36Z" id="86946321">+1 on getting this into 1.5.1
</comment><comment author="kimchy" created="2015-03-27T14:54:34Z" id="86963547">@bleskes pushed another round, review?
</comment><comment author="bleskes" created="2015-03-27T15:28:08Z" id="86974561">LGTM again. nice clean up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] enable inline scripts on demand in bw comp tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10282</link><project id="" key="" /><description>We currently have a single bw comp test (FunctionScoreBackwardCompatibilityTests) that requires inline scripts on.
After introducing fine-grained script settings, we moved the internal cluster to use the newer settings, but they are not supported by older nodes started as part of the bw comp tests. Moved script settings out of the default settings, so they won't be part of the ordinary settings when running bw comp tests.
Added logic in FunctionScoreBackwardCompatibilityTests to enable dynamic scripts using the proper setting, depending on the version of the node.
</description><key id="64594939">10282</key><summary>[TEST] enable inline scripts on demand in bw comp tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T17:57:17Z</created><updated>2015-06-07T11:46:38Z</updated><resolved>2015-03-26T18:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-26T18:01:25Z" id="86648200">LGTM
</comment><comment author="javanna" created="2015-03-26T18:25:15Z" id="86656349">Pushed https://github.com/elastic/elasticsearch/commit/59affb579816f47dab157a4d2d6c8e4684525ff2
</comment><comment author="javanna" created="2015-03-26T18:58:26Z" id="86670704">and ported to master too https://github.com/elastic/elasticsearch/commit/18b02d58a4d0caf3496eaadd40d3832533afddfa
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard Allocation Problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10281</link><project id="" key="" /><description>Our cluster has 7 data nodes. Last week one of them was turned off. We restarted the node and then the cluster started rebalancing. The problem is it nerver finishes rebalancing.

Node logs shows an update coming from master node during the rebalancing, seconds later the rebalancing is canceled and then it starts again.

I will put the log in the next msg.
</description><key id="64542981">10281</key><summary>Shard Allocation Problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcelopaesrech</reporter><labels /><created>2015-03-26T14:41:22Z</created><updated>2015-03-31T12:19:04Z</updated><resolved>2015-03-31T07:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcelopaesrech" created="2015-03-26T14:41:56Z" id="86544069">```
[2015-03-24 15:26:05,244][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][0] creating shard
[2015-03-24 15:26:05,244][DEBUG][index.service            ] [THE_CHOSEN_ONE] [my_index] creating shard_id [0]
[2015-03-24 15:26:05,274][DEBUG][index.deletionpolicy     ] [THE_CHOSEN_ONE] [my_index][0] Using [keep_only_last] deletion policy
[2015-03-24 15:26:05,275][DEBUG][index.merge.policy       ] [THE_CHOSEN_ONE] [my_index][0] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]
[2015-03-24 15:26:05,276][DEBUG][index.merge.scheduler    ] [THE_CHOSEN_ONE] [my_index][0] using [concurrent] merge scheduler with max_thread_count[3]
[2015-03-24 15:26:05,278][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [CREATED]
[2015-03-24 15:26:05,278][DEBUG][index.translog           ] [THE_CHOSEN_ONE] [my_index][0] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2015-03-24 15:26:05,280][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [CREATED]-&gt;[RECOVERING], reason [from [VEROIA][Fto2cgJ-RB-IjV3EfLm1Sw][es2.mydomain][inet[/172.31.234.165:9300]]{master=false}]
[2015-03-24 15:26:05,281][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][2] creating shard
[2015-03-24 15:26:05,281][DEBUG][index.service            ] [THE_CHOSEN_ONE] [my_index] creating shard_id [2]
[2015-03-24 15:26:05,358][DEBUG][index.deletionpolicy     ] [THE_CHOSEN_ONE] [my_index][2] Using [keep_only_last] deletion policy
[2015-03-24 15:26:05,361][DEBUG][index.merge.policy       ] [THE_CHOSEN_ONE] [my_index][2] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]
[2015-03-24 15:26:05,363][DEBUG][index.merge.scheduler    ] [THE_CHOSEN_ONE] [my_index][2] using [concurrent] merge scheduler with max_thread_count[3]
[2015-03-24 15:26:05,371][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [CREATED]
[2015-03-24 15:26:05,373][DEBUG][index.translog           ] [THE_CHOSEN_ONE] [my_index][2] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2015-03-24 15:26:05,378][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [CREATED]-&gt;[RECOVERING], reason [from [PERAIA][bOUh4S-fSmW8vcD8Gw0sQw][es4.mydomain][inet[/172.31.234.167:9300]]{master=false}]
[2015-03-24 15:26:05,380][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: done applying updated cluster_state (version: 116348)
[2015-03-24 15:26:05,682][DEBUG][discovery.zen.publish    ] [THE_CHOSEN_ONE] received cluster state version 116349
[2015-03-24 15:26:05,684][DEBUG][discovery.zen            ] [THE_CHOSEN_ONE] received cluster state from [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}] which is also master but with cluster name [Cluster [gvtmusic]]
[2015-03-24 15:26:05,685][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: execute
[2015-03-24 15:26:05,685][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] cluster state updated, version [116349], source [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]
[2015-03-24 15:26:05,686][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] set local cluster state to version 116349
[2015-03-24 15:26:05,687][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][0] removing shard (different instance of it allocated on this node, current [[my_index][0], node[OI3VgsmPSBekiVF7WhSm4Q], relocating [gBhKyL1gQ1WnHLUFvR_BPQ], [R], s[INITIALIZING]], global [[my_index][0], node[OI3VgsmPSBekiVF7WhSm4Q], [R], s[INITIALIZING]])
[2015-03-24 15:26:05,807][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [RECOVERING]-&gt;[CLOSED], reason [removing shard (different instance of it allocated on this node)]
[2015-03-24 15:26:05,808][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][0] creating shard
[2015-03-24 15:26:05,808][DEBUG][index.service            ] [THE_CHOSEN_ONE] [my_index] creating shard_id [0]
[2015-03-24 15:26:05,856][DEBUG][index.deletionpolicy     ] [THE_CHOSEN_ONE] [my_index][0] Using [keep_only_last] deletion policy
[2015-03-24 15:26:05,857][DEBUG][index.merge.policy       ] [THE_CHOSEN_ONE] [my_index][0] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]
[2015-03-24 15:26:05,858][DEBUG][index.merge.scheduler    ] [THE_CHOSEN_ONE] [my_index][0] using [concurrent] merge scheduler with max_thread_count[3]
[2015-03-24 15:26:05,861][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [CREATED]
[2015-03-24 15:26:05,862][DEBUG][index.translog           ] [THE_CHOSEN_ONE] [my_index][0] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2015-03-24 15:26:05,865][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [CREATED]-&gt;[RECOVERING], reason [from [VEROIA][Fto2cgJ-RB-IjV3EfLm1Sw][es2.mydomain][inet[/172.31.234.165:9300]]{master=false}]
[2015-03-24 15:26:05,868][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: done applying updated cluster_state (version: 116349)
[2015-03-24 15:26:06,514][DEBUG][discovery.zen.publish    ] [THE_CHOSEN_ONE] received cluster state version 116350
[2015-03-24 15:26:06,514][DEBUG][discovery.zen            ] [THE_CHOSEN_ONE] received cluster state from [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}] which is also master but with cluster name [Cluster [gvtmusic]]
[2015-03-24 15:26:06,515][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: execute
[2015-03-24 15:26:06,515][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] cluster state updated, version [116350], source [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]
[2015-03-24 15:26:06,516][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] set local cluster state to version 116350
[2015-03-24 15:26:06,518][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][2] removing shard (different instance of it allocated on this node, current [[my_index][2], node[OI3VgsmPSBekiVF7WhSm4Q], relocating [Fto2cgJ-RB-IjV3EfLm1Sw], [R], s[INITIALIZING]], global [[my_index][2], node[OI3VgsmPSBekiVF7WhSm4Q], [R], s[INITIALIZING]])
[2015-03-24 15:26:06,622][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [RECOVERING]-&gt;[CLOSED], reason [removing shard (different instance of it allocated on this node)]
[2015-03-24 15:26:06,623][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][2] creating shard
[2015-03-24 15:26:06,623][DEBUG][index.service            ] [THE_CHOSEN_ONE] [my_index] creating shard_id [2]
[2015-03-24 15:26:06,687][DEBUG][index.deletionpolicy     ] [THE_CHOSEN_ONE] [my_index][2] Using [keep_only_last] deletion policy
[2015-03-24 15:26:06,688][DEBUG][index.merge.policy       ] [THE_CHOSEN_ONE] [my_index][2] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0]
[2015-03-24 15:26:06,690][DEBUG][index.merge.scheduler    ] [THE_CHOSEN_ONE] [my_index][2] using [concurrent] merge scheduler with max_thread_count[3]
[2015-03-24 15:26:06,692][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [CREATED]
[2015-03-24 15:26:06,692][DEBUG][index.translog           ] [THE_CHOSEN_ONE] [my_index][2] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2015-03-24 15:26:06,695][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [CREATED]-&gt;[RECOVERING], reason [from [PERAIA][bOUh4S-fSmW8vcD8Gw0sQw][es4.mydomain][inet[/172.31.234.167:9300]]{master=false}]
[2015-03-24 15:26:06,697][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: done applying updated cluster_state (version: 116350)
[2015-03-24 15:26:08,215][DEBUG][discovery.zen.publish    ] [THE_CHOSEN_ONE] received cluster state version 116351
[2015-03-24 15:26:08,215][DEBUG][discovery.zen            ] [THE_CHOSEN_ONE] received cluster state from [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}] which is also master but with cluster name [Cluster [gvtmusic]]
[2015-03-24 15:26:08,216][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: execute
[2015-03-24 15:26:08,217][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] cluster state updated, version [116351], source [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]
[2015-03-24 15:26:08,218][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] set local cluster state to version 116351
[2015-03-24 15:26:08,224][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][0] removing shard (not allocated)
[2015-03-24 15:26:08,297][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][0] state: [RECOVERING]-&gt;[CLOSED], reason [removing shard (not allocated)]
[2015-03-24 15:26:08,304][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: done applying updated cluster_state (version: 116351)
[2015-03-24 15:26:08,812][DEBUG][discovery.zen.publish    ] [THE_CHOSEN_ONE] received cluster state version 116352
[2015-03-24 15:26:08,813][DEBUG][discovery.zen            ] [THE_CHOSEN_ONE] received cluster state from [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}] which is also master but with cluster name [Cluster [gvtmusic]]
[2015-03-24 15:26:08,813][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: execute
[2015-03-24 15:26:08,814][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] cluster state updated, version [116352], source [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]
[2015-03-24 15:26:08,814][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] set local cluster state to version 116352
[2015-03-24 15:26:08,816][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index][2] removing shard (not allocated)
[2015-03-24 15:26:08,820][DEBUG][index.shard.service      ] [THE_CHOSEN_ONE] [my_index][2] state: [RECOVERING]-&gt;[CLOSED], reason [removing shard (not allocated)]
[2015-03-24 15:26:08,821][DEBUG][indices.cluster          ] [THE_CHOSEN_ONE] [my_index] cleaning index (no shards allocated)
[2015-03-24 15:26:08,821][DEBUG][index.cache.filter.weighted] [THE_CHOSEN_ONE] [my_index] full cache clear, reason [close]
[2015-03-24 15:26:08,827][DEBUG][cluster.service          ] [THE_CHOSEN_ONE] processing [zen-disco-receive(from master [[MR3][4LssjGYPTX6EL7Kv-ylgMQ][master3.mydomain][inet[/172.31.234.181:9300]]{data=false, master=true}])]: done applying updated cluster_state (version: 116352)
```
</comment><comment author="marcelopaesrech" created="2015-03-26T18:29:21Z" id="86658534">Another log extracted from Master node:

```
[2015-03-26 15:23:08,468][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,506][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,516][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,523][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,534][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,545][DEBUG][cluster.service          ] [MR3] set local cluster state to version 142212
[2015-03-26 15:23:08,545][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: execute
[2015-03-26 15:23:08,545][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-03-26 15:23:08,547][DEBUG][cluster.service          ] [MR3] processing [shard-started ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], [R], s[INITIALIZING]), reason [after recovery (replica) from node [[KOZANI][_dLxnGwlTt6P4i7PqzNEUQ][pves1-6.popvono][inet[/172.31.234.187:9300]]{master=false}]]]: done applying updated cluster_state (version: 142212)
[2015-03-26 15:23:08,547][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,547][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,548][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,549][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,549][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,564][DEBUG][cluster.service          ] [MR3] cluster state updated, version [142213], source [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]
[2015-03-26 15:23:08,564][DEBUG][cluster.service          ] [MR3] publishing cluster state version 142213
[2015-03-26 15:23:08,601][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,619][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,668][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,724][DEBUG][cluster.service          ] [MR3] set local cluster state to version 142213
[2015-03-26 15:23:08,725][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: execute
[2015-03-26 15:23:08,725][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-03-26 15:23:08,725][WARN ][cluster.action.shard     ] [MR3] [my_index][1] received shard failed for [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: done applying updated cluster_state (version: 142213)
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,727][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,728][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [indices/index/b_shard/delete] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,729][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,730][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,730][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,730][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,730][DEBUG][cluster.action.shard     ] [MR3] [my_index][1] will apply shard failed [my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING], indexUUID [evIJHzdjQP6x0vr3Tve2-w], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2015-03-26 15:23:08,739][DEBUG][gateway.local            ] [MR3] [my_index][1]: allocating [[my_index][1], node[null], [R], s[UNASSIGNED]] to [[PERAIA][bOUh4S-fSmW8vcD8Gw0sQw][pves1-4.popvono][inet[/172.31.234.167:9300]]{master=false}] in order to reuse its unallocated persistent store with total_size [7gb]
[2015-03-26 15:23:08,746][DEBUG][cluster.service          ] [MR3] cluster state updated, version [142214], source [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]
[2015-03-26 15:23:08,746][DEBUG][cluster.service          ] [MR3] publishing cluster state version 142214
[2015-03-26 15:23:08,775][DEBUG][cluster.service          ] [MR3] set local cluster state to version 142214
[2015-03-26 15:23:08,776][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: execute
[2015-03-26 15:23:08,776][DEBUG][river.cluster            ] [MR3] processing [reroute_rivers_node_changed]: no change in cluster_state
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[bOUh4S-fSmW8vcD8Gw0sQw], relocating [4UkjJGtNQdCvJH3OuXEyUQ], [R], s[RELOCATING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: done applying updated cluster_state (version: 142214)
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: execute
[2015-03-26 15:23:08,777][DEBUG][cluster.service          ] [MR3] processing [shard-failed ([my_index][1], node[4UkjJGtNQdCvJH3OuXEyUQ], [R], s[INITIALIZING]), reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]]: no change in cluster_state
```
</comment><comment author="marcelopaesrech" created="2015-03-27T13:08:05Z" id="86935264">Environment set:
ElasticSearch 1.1.1
7 Data Nodes;
4 Load Balancers (no data, no master);
4 Master Nodes (no data).
</comment><comment author="markwalkom" created="2015-03-31T02:39:49Z" id="87905929">These sorts of issues are best sent to the mailing list - https://groups.google.com/forum/#!forum/elasticsearch

Issues here are usually reserved for code problems :)
</comment><comment author="marcelopaesrech" created="2015-03-31T12:19:04Z" id="88064682">Yeah, do you know if it isn't a bug in ES 1.1.1? Because this userd to work till two weeks ago.
I didn't change the server configuration, so what did the cluster stop working?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>boost_factor deprecated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10280</link><project id="" key="" /><description>this word is not present as it is in source code from last ES versions
</description><key id="64537488">10280</key><summary>boost_factor deprecated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">HonzaMac</reporter><labels /><created>2015-03-26T14:17:35Z</created><updated>2015-04-02T11:13:48Z</updated><resolved>2015-04-02T11:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-02T10:42:15Z" id="88863319">@clintongormley can you take a look 
</comment><comment author="clintongormley" created="2015-04-02T11:13:48Z" id="88867241">thanks @HonzaMac - I've changed the syntax slightly so that the deprecation is rendered as a block, but includes the term `boost_factor`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: testOldIndexes fails to find docs after sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10279</link><project id="" key="" /><description>See: http://build-us-00.elastic.co/job/es_core_1x_window-2008/1258/testReport/junit/org.elasticsearch.bwcompat/OldIndexBackwardsCompatibilityTests/testOldIndexes/

(note that the message is confusing, expected is what the search returned)

```
java.lang.AssertionError: 
Expected: &lt;0L&gt;
     got: &lt;2858L&gt;

    at __randomizedtesting.SeedInfo.seed([6135E006EB9CCAB2:46EFA250AC715F05]:0)
    at org.junit.Assert.assertThat(Assert.java:780)
    at org.junit.Assert.assertThat(Assert.java:738)
```
</description><key id="64521641">10279</key><summary>CI: testOldIndexes fails to find docs after sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-03-26T13:04:14Z</created><updated>2015-03-27T01:03:06Z</updated><resolved>2015-03-27T01:03:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-26T13:17:33Z" id="86508990">another example: http://build-us-00.elastic.co/job/es_core_1x_strong/3200/testReport/junit/org.elasticsearch.bwcompat/OldIndexBackwardsCompatibilityTests/testOldIndexes/
</comment><comment author="rjernst" created="2015-03-27T01:03:06Z" id="86777846">This was fixed in a follow up commit yesterday:
314ee9c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: BigArraysTests.testLongArrayFill raises ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10278</link><project id="" key="" /><description>see http://build-us-00.elastic.co/job/es_g1gc_master_metal/4810/testReport/junit/org.elasticsearch.common.util/BigArraysTests/testLongArrayFill/

```
java.lang.ArrayIndexOutOfBoundsException: -1
    at org.elasticsearch.common.util.BigLongArray.fill(BigLongArray.java:106)
    at org.elasticsearch.test.cache.recycler.MockBigArrays$LongArrayWrapper.fill(MockBigArrays.java:435)
    at org.elasticsearch.common.util.BigArraysTests.testLongArrayFill(BigArraysTests.java:229)
```

The seed reproduces:

```
mvn clean test -Dtests.seed=7E0265B0C4E00A96 -Dtests.class=org.elasticsearch.common.util.BigArraysTests -Dtests.method="testLongArrayFill" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=false -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=ms_MY -Dtests.timezone=America/Cuiaba -Dtests.processors=8
```
</description><key id="64510845">10278</key><summary>CI: BigArraysTests.testLongArrayFill raises ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-03-26T12:09:57Z</created><updated>2015-03-27T07:48:54Z</updated><resolved>2015-03-27T07:48:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-27T07:48:54Z" id="86859809">The fill function did not account for an empty range of [from,to).  This only "mattered" when `from == to == 0`.

I've pushed a fix.
master: 5b0512f
1.x: aa1d0f2
1.5: a008fc1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simple query string accepting "field:" syntax in the query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10277</link><project id="" key="" /><description>Why does the simple query string parser not accept "title:foobar" in the query like the normal query string parser does?

It would be nice to combine the fault tolerance of the simple query string parser with the "specify field in the query" syntax of the normal query parser.
</description><key id="64483850">10277</key><summary>Simple query string accepting "field:" syntax in the query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ferdynice</reporter><labels /><created>2015-03-26T09:51:46Z</created><updated>2015-04-05T12:17:49Z</updated><resolved>2015-04-05T11:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T11:59:26Z" id="89757134">The simple query string does not support field syntax by design, as it allows users to query any field that they like
</comment><comment author="ferdynice" created="2015-04-05T12:13:32Z" id="89757690">Fair enough, fields can be supplied next to the query, but still then it's
not possible to query something like

pants:blue AND shirt:red

because all fields are thrown together in query. In this example I only
want people with blue pants and red shirts, which is not possible in a
single simple query. (According to what you are saying).

Please correct me if I'm wrong. In any case, thank you for your time.
On 5 Apr 2015 14:00, "Clinton Gormley" notifications@github.com wrote:

&gt; Closed #10277 https://github.com/elastic/elasticsearch/issues/10277.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10277#event-273577494.
</comment><comment author="clintongormley" created="2015-04-05T12:17:49Z" id="89757821">No, you're correct.  But perhaps at this level, you want to build your own query parser that allows for the level of control that you're after.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to install lang-groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10276</link><project id="" key="" /><description>follow the documents, I execute the command 
./plugin --install elasticsearch/elasticsearch-lang-groovy/3.0.0

but it shows 

Message:
   Error while installing plugin, reason: IllegalArgumentException: Plugin installation assumed to be site plugin, but contains source code, aborting installation.

anyone help me?
</description><key id="64483315">10276</key><summary>failed to install lang-groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hyhsoftware</reporter><labels /><created>2015-03-26T09:49:28Z</created><updated>2015-03-27T05:43:25Z</updated><resolved>2015-03-27T05:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-26T10:52:48Z" id="86450635">This version does not exist.
Which elasticsearch version are you using BTW?
</comment><comment author="pickypg" created="2015-03-27T02:40:17Z" id="86792248">@hyhsoftware The Groovy language plugin is installed by default with Elasticsearch starting with Elasticsearch 1.3 and later.
</comment><comment author="dadoonet" created="2015-03-27T05:43:25Z" id="86834521">Yeah. I need to update groovy plugin README to make sure users are aware of it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can not start ES after upgrade to 1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10275</link><project id="" key="" /><description>We had to revert to 1.4.4 because 1.5.0 won't start. What might be causing it?

```
[2015-03-26 08:40:36,558][INFO ][node                     ] [lepton001] version[1.5.0], pid[13791], build[5448160/2015-03-23T14:30:58Z]
[2015-03-26 08:40:36,558][INFO ][node                     ] [lepton001] initializing ...
[2015-03-26 08:40:36,624][ERROR][plugins                  ] [lepton001] cannot start plugin due to incorrect Lucene version: plugin [4.10.2], node [4.10.4].
[2015-03-26 08:40:36,625][WARN ][plugins                  ] [lepton001] failed to load plugin from [jar:file:/usr/share/elasticsearch/plugins/experimental-highlighter-elasticsearch-plugin/experimental-highlighter-elasticsearch-plugin-1.4.1
.jar!/es-plugin.properties]
org.elasticsearch.ElasticsearchException: Failed to load plugin class [org.wikimedia.highlighter.experimental.elasticsearch.plugin.ExperimentalHighlighterPlugin]
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:533)
        at org.elasticsearch.plugins.PluginsService.loadPluginsFromClasspath(PluginsService.java:408)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:116)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:152)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.ElasticsearchException: Plugin is incompatible with the current node
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:526)
        ... 7 more
[2015-03-26 08:40:36,629][INFO ][plugins                  ] [lepton001] loaded [analysis-smartcn, analysis-kuromoji, marvel, analysis-combo, analysis-icu], sites [marvel, head, kopf, paramedic, HQ]
[2015-03-26 08:40:37,451][ERROR][bootstrap                ] Exception
org.elasticsearch.common.util.concurrent.ExecutionError: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
        at org.elasticsearch.common.inject.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:50)
        at org.elasticsearch.common.inject.InjectorImpl.initializeBinding(InjectorImpl.java:372)
        at org.elasticsearch.common.inject.BindingProcessor$1$1.run(BindingProcessor.java:148)
        at org.elasticsearch.common.inject.BindingProcessor.initializeBindings(BindingProcessor.java:204)
        at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:119)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.MembersInjectorStore.get(MembersInjectorStore.java:68)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:67)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:37)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:33)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 19 more
Caused by: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at java.lang.Class.getDeclaredFields0(Native Method)
        at java.lang.Class.privateGetDeclaredFields(Class.java:2499)
        at java.lang.Class.getDeclaredFields(Class.java:1811)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:378)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:376)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectorsForMembers(InjectionPoint.java:351)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectionPoints(InjectionPoint.java:345)
        at org.elasticsearch.common.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:287)
        at org.elasticsearch.common.inject.MembersInjectorStore.createWithListeners(MembersInjectorStore.java:80)
        at org.elasticsearch.common.inject.MembersInjectorStore.access$000(MembersInjectorStore.java:36)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:45)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:41)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 33 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.indices.InternalIndicesService
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 50 more
```

ES is installed from the debian repo:

```
$ apt-cache policy elasticsearch
elasticsearch:
  Installed: 1.5.0
  Candidate: 1.5.0
  Version table:
 *** 1.5.0 0
        990 http://packages.elasticsearch.org/elasticsearch/1.5/debian/ stable/main amd64 Packages
        100 /var/lib/dpkg/status
     1.0.3+dfsg-5 0
        500 http://debian.mirrors.ovh.net/debian/ jessie/main amd64 Packages
```

We are using Java7 from Oracle:

```
java version "1.7.0_76"
Java(TM) SE Runtime Environment (build 1.7.0_76-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)
```

Let me know if more information is needed.
Thanks.
</description><key id="64479488">10275</key><summary>Can not start ES after upgrade to 1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cyberhuman</reporter><labels><label>non-issue</label></labels><created>2015-03-26T09:29:10Z</created><updated>2015-06-01T07:12:07Z</updated><resolved>2015-03-26T10:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-26T09:33:04Z" id="86420454">It looks like this plugin is not compatible with ES 1.5. You have to use a version of the plugin which is compatible with elasticsearch 1.5.
</comment><comment author="cyberhuman" created="2015-03-26T09:41:43Z" id="86423282">Hi Tanguy,
I removed ExperimentalHighlighterPlugin, that was not compatible, but the main issue (`ClassNotFoundException: org.elasticsearch.indices.InternalIndicesService`) still remains.
</comment><comment author="tlrx" created="2015-03-26T09:43:31Z" id="86424258">Can you please post again the output without the ExperimentalHighlighterPlugin?
</comment><comment author="cyberhuman" created="2015-03-26T09:46:25Z" id="86425768">Here you are:

```
[2015-03-26 09:38:58,645][INFO ][node                     ] [lepton001] version[1.5.0], pid[5926], build[5448160/2015-03-23T14:30:58Z]
[2015-03-26 09:38:58,646][INFO ][node                     ] [lepton001] initializing ...
[2015-03-26 09:38:58,706][INFO ][plugins                  ] [lepton001] loaded [analysis-smartcn, analysis-kuromoji, marvel, analysis-combo, analysis-icu], sites [marvel, head, kopf, paramedic, HQ]
[2015-03-26 09:38:59,532][ERROR][bootstrap                ] Exception
org.elasticsearch.common.util.concurrent.ExecutionError: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
        at org.elasticsearch.common.inject.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:50)
        at org.elasticsearch.common.inject.InjectorImpl.initializeBinding(InjectorImpl.java:372)
        at org.elasticsearch.common.inject.BindingProcessor$1$1.run(BindingProcessor.java:148)
        at org.elasticsearch.common.inject.BindingProcessor.initializeBindings(BindingProcessor.java:204)
        at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:119)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.MembersInjectorStore.get(MembersInjectorStore.java:68)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:67)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:37)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:33)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 19 more
Caused by: java.lang.NoClassDefFoundError: Lorg/elasticsearch/indices/InternalIndicesService;
        at java.lang.Class.getDeclaredFields0(Native Method)
        at java.lang.Class.privateGetDeclaredFields(Class.java:2499)
        at java.lang.Class.getDeclaredFields(Class.java:1811)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:378)
        at org.elasticsearch.common.inject.spi.InjectionPoint$Factory$1.getMembers(InjectionPoint.java:376)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectorsForMembers(InjectionPoint.java:351)
        at org.elasticsearch.common.inject.spi.InjectionPoint.addInjectionPoints(InjectionPoint.java:345)
        at org.elasticsearch.common.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:287)
        at org.elasticsearch.common.inject.MembersInjectorStore.createWithListeners(MembersInjectorStore.java:80)
        at org.elasticsearch.common.inject.MembersInjectorStore.access$000(MembersInjectorStore.java:36)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:45)
        at org.elasticsearch.common.inject.MembersInjectorStore$1.create(MembersInjectorStore.java:41)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 33 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.indices.InternalIndicesService
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 50 more
```
</comment><comment author="tlrx" created="2015-03-26T10:08:13Z" id="86433997">@cyberhuman thanks :) It's possible that some installed plugins are not compatible with the version of elasticsearch you use. One thing you can do is to test if elasticsearch starts correctly without plugins. If it starts correctly, then try to install each plugin one by one, restarting elasticsearch after each plugin installation. This should indicates the potential plugin that fails.

Also, note that few plugins have been updated for ES 1.5: analysis-icu, analysis-kuromoji etc. So be sure to use updated plugins.
</comment><comment author="cyberhuman" created="2015-03-26T10:23:43Z" id="86440629">I was able to nail it down to the marvel plugin. Too bad it does not complain about being incompatible with ES. Thanks for the help anyway!
</comment><comment author="tlrx" created="2015-03-26T10:24:40Z" id="86441130">@cyberhuman thanks for letting us know :)
</comment><comment author="bleskes" created="2015-03-26T11:36:25Z" id="86477356">@cyberhuman marvel 1.3.1 should work with 1.5 . Can you double check which version you use(d) ? if not, can you install the lastest marvel and give it a spin? (works for me :))
</comment><comment author="cyberhuman" created="2015-03-26T11:40:48Z" id="86478045">@bleskes, honestly I don't know. The official download page recommends /latest, and I have already reinstalled it, so there is no way to see know what was the previous version. But the latest version works fine, thanks.
</comment><comment author="bleskes" created="2015-03-26T11:54:33Z" id="86481129">@cyberhuman if it's works now I'm happy. The `latest` is a redirect link to the latest version compatible with your ES.  You can check the marvel version by looking at the plugins/marvel folder . it will be part of the jar name. Also `GET /_cat/plugins`  will give you versions as well.
</comment><comment author="chbrown" created="2015-05-31T00:02:39Z" id="107102503">I hit the same issue using Homebrew. After removing the `marvel` plugin it works just fine, but the error message says nothing about plugin incompatibility, which would have been helpful. Or even better, skip loading plugins that are incompatible, instead of completely failing to start.
</comment><comment author="tlrx" created="2015-06-01T07:12:07Z" id="107334791">@chbrown Agreed, there's a room for improvement here. Be sure that we have it in mind :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate script.disable_dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10274</link><project id="" key="" /><description>With the introduction of fine-grained script settings as requested in #6418 and implemented in #10116, we should deprecate the `script.disable_dynamic` setting which currently disables both indexed and inline scripts (without affecting native and mustache scripts).

It is possible to achieve the same as `script.disable_dynamic: false` through fine-grained generic settings as follows:

```
script.inline: on
script.indexed: on
```

Note that native scripts won't be affected by these generic settings and will always be on as they are static by definition (they require a plugin to be installed). Mustache scripts though, used in search templates, will be affected by these generic settings meaning. That said they will be on by default as their engine is sandboxed, but they can be disabled if needed. The defaults are:

```
script.inline: sandbox
script.indexed: sandbox
script.file: on
```
</description><key id="64477026">10274</key><summary>Deprecate script.disable_dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>deprecation</label><label>v1.6.0</label></labels><created>2015-03-26T09:10:12Z</created><updated>2015-03-26T10:14:33Z</updated><resolved>2015-03-26T10:14:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-26T10:14:33Z" id="86436876">Done in https://github.com/elastic/elasticsearch/commit/8c2cd66ff3f503826afa0b53aff7a71bb82cbb46 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.5 response syntax error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10273</link><project id="" key="" /><description>I just downloaded elasticsearch 1.5. After having successfully indexed a document that includes an attachment file , I tried to get my index data in order to test some queries . When I make a simple /_search query i get a not valide Json response.
![es1 5error](https://cloud.githubusercontent.com/assets/3472300/6842729/925447b8-d39b-11e4-8ebd-455d6e735d19.PNG)
</description><key id="64470202">10273</key><summary>ES 1.5 response syntax error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ramzy27</reporter><labels /><created>2015-03-26T08:36:52Z</created><updated>2015-03-26T08:55:55Z</updated><resolved>2015-03-26T08:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-26T08:39:10Z" id="86404670">Hi @ramzy27 could you please post a recreation of what you did, especially the complete search response so we can double check?
</comment><comment author="ramzy27" created="2015-03-26T08:48:00Z" id="86407102">1) I created the following mapping {
    "rfp": {
        "properties": {
            "file": {
                "type": "attachment"
            },
            "filename": {
                "type": "string"
            },
            "filedate": {
                "type": "date"
            },
            "tags": {
                "type": "string",
                "index_name": "tag"
            },
            "author": {
                "type": "string"
            },
            "couple": {
                "type": "nested",
                "include_in_parent": true,
                "properties": {
                    "question": {
                        "type": "string",
                        "analyzer": "english"
                    },
                    "response": {
                        "type": "string",
                        "analyzer": "english"
                    }
                }
            }
        }
    }
}
2)I indexed one document
3)Made a search query 
![es1 5error2](https://cloud.githubusercontent.com/assets/3472300/6842869/1b0b19be-d39d-11e4-8cd3-9877eb664148.PNG)
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"searchtest","_type":"rfp","_id":"1","_score":1.0,"_source":{"file":"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","filename":"avancement.xlsx","filedate":"2015-02-12T08:22:01.000Z","author":"Ramzi","couple":[{"question":"Duree estimee","response":"Commentaires"},{"question":"1h","response":"TutoOpenclass:create,remove,use,find"},{"question":"2h","response":"Tutozenk:clusternode,Headplugin, mapping du livre cookbook"},{"question":"2h,5","response":"Rien"},{"question":"1h","response":"Livre Cookbook"},{"question":"2h,5","response":"Exemple de la saisie autocomplete"},{"question":"2h","response":"Rien"},{"question":"35 min","response":"Travail &#224; faire : use case et mod&#232;le de donn&#233;es"},{"question":"3h","response":"http://satishgandham.com/2012/09/a-complete-guide-to-integrating-mongodb-with-elastic-search/"},{"question":"Initiation &#224; Spring Data avec les technologies existantes","response":"Initiation &#224; Spring Data avec les technologies existantes"},{}],"tags":["java","elasticsearch"]}}}]}}
</comment><comment author="javanna" created="2015-03-26T08:55:33Z" id="86408975">I'm afraid the problem is in your document. elasticsearch doesn't validate json, and just returns what you sent to it, which is why the search response is not a valid json. Have a look [here](https://github.com/elastic/elasticsearch/issues/9415). We've been discussing how we could improve this on #2315 but we haven't reached an agreement yet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to create custom mapping for the same field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10272</link><project id="" key="" /><description>When I put the following JSON to elasticsearch. It get error about nested: NumberFormatException
{  
   "id":"eee",
   "crosstab":{  
      "dims":[  
         {  
            "id":1
         },
         {  
            "id":"asd"
         }
      ]
   }
}

I  knew dims.id have different type, what should I do to avoid create index dim.id or create custom mapping.
</description><key id="64458188">10272</key><summary>how to create custom mapping for the same field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hordemark</reporter><labels /><created>2015-03-26T07:32:53Z</created><updated>2015-03-26T07:54:46Z</updated><resolved>2015-03-26T07:54:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-26T07:54:46Z" id="86381491">Hi @hordemark can you please ask your question on the [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch)?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geoShapeQuery relation error?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10271</link><project id="" key="" /><description>// Import Spatial4J shapes
import com.spatial4j.core.context.SpatialContext;
import com.spatial4j.core.shape.Shape;
import com.spatial4j.core.shape.impl.RectangleImpl;

// Also import ShapeRelation
import org.elasticsearch.common.geo.ShapeRelation;// Shape within another
QueryBuilder qb = geoShapeQuery(
        "location",  
        new RectangleImpl(0,10,0,10,SpatialContext.GEO) 
    )
    .relation(ShapeRelation.WITHIN);       

I find  relation function is not implement in geoShapeQuery ,but It's suggested with link: http://www.elastic.co/guide/en/elasticsearch/client/java-api/current/query-dsl-queries.html
</description><key id="64440126">10271</key><summary>geoShapeQuery relation error?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">gucasbrg</reporter><labels><label>:Geo</label><label>non-issue</label></labels><created>2015-03-26T04:40:51Z</created><updated>2015-12-06T14:35:47Z</updated><resolved>2015-12-06T14:35:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T11:57:58Z" id="89756859">@nknize any ideas?
</comment><comment author="nknize" created="2015-04-07T13:09:21Z" id="90543389">I'm not sure why the docs have an example like this.  This is describing how to use a FilterBuilder not a QueryBuilder.  Here's the example using the FilterBuilder:

``` java
FilterBuilder fb = geoShapeFilter("location", 
                                   ShapeBuilder.newEnvelope().topLeft(0,10).bottomRight(10,0), 
                                   ShapeRelation.WITHIN)
```

At the moment GeoShapeQueryBuilder does not accept a relation (everything boils down to `CONTAINS` || `WITHIN` || `INTERSECTS`). This is primarily due to spatial query complexities in Lucene that haven't been cleaned up until recently. For the time being, to achieve more complex queries you can `match_all` and set a post filter, or you can combine a `geoShapeQuery` and post filter using the `geoShapeFilter`.  

There are a few low hanging fruit issues and optimizations here that I need to write up.
</comment><comment author="clintongormley" created="2015-12-05T21:00:20Z" id="162245368">@nknize can this be closed now?
</comment><comment author="nknize" created="2015-12-06T14:35:47Z" id="162321285">++. Closing as non-issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with nested filter, sub field and "path"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10270</link><project id="" key="" /><description>It seems like when using a nested filter to filter a sub field, the "path" parameter gets ignored.

Considering the following index mapping

```
{
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": {
            "filter": {
                "ngram_filter": {
                    "type": "nGram",
                    "min_gram": 1,
                    "max_gram": 20
                },
                "lowercase_filter": {
                    "type": "lowercase"
                }
            },
            "analyzer": {
                "lowercase_ngram_index_analyzer": {
                    "type": "custom",
                    "tokenizer": "keyword",
                    "filter": [
                        "lowercase_filter",
                        "ngram_filter"
                    ]
                },
                "lowercase_ngram_search_analyzer": {
                    "type": "custom",
                    "tokenizer": "keyword",
                    "filter": [
                        "lowercase_filter"
                    ]
                }
            }
        }
    },
    "mappings": {
        "test": {
            "properties": {
                "fields": {
                    "type": "nested",
                    "properties": {
                        "name": {
                            "type": "string",
                            "index": "not_analyzed"
                        },
                        "value": {
                            "type": "string",
                            "index": "not_analyzed",
                            "fields": {
                                "ngram": {
                                    "type": "string",
                                    "index": "analyzed",
                                    "norms": {
                                        "enabled": false
                                    },
                                    "index_analyzer": "lowercase_ngram_index_analyzer",
                                    "search_analyzer": "lowercase_ngram_search_analyzer"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

And the following data:

```
{
    "fields": [
        {
            "name": "field1",
            "value": "value1"
        },
        {
            "name": "field2",
            "value": "value2"
        }
    ]
}
```

The following query does not work:

```
{
    "query": {
        "filtered": {
            "filter": {
                "nested": {
                    "path": "fields",
                    "filter": {
                        "bool": {
                            "must": [
                                {
                                    "term": {
                                        "name": "field1"
                                    }
                                },
                                {
                                    "term": {
                                        "value.ngram": "val"
                                    }
                                }
                            ]
                        }
                    }
                }
            }
        }
    }
}
```

To make it work, you need to use the fully qualified name of the field (`fields.value.ngram` vs `value.ngram`) in the term filter like so :

```
{
    "query": {
        "filtered": {
            "filter": {
                "nested": {
                    "path": "fields",
                    "filter": {
                        "bool": {
                            "must": [
                                {
                                    "term": {
                                        "name": "field1"
                                    }
                                },
                                {
                                    "term": {
                                        "fields.value.ngram": "val"
                                    }
                                }
                            ]
                        }
                    }
                }
            }
        }
    }
}
```

This seems like a bug to me.
</description><key id="64414787">10270</key><summary>Issue with nested filter, sub field and "path"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kutchar</reporter><labels /><created>2015-03-26T01:41:39Z</created><updated>2015-04-05T13:12:27Z</updated><resolved>2015-04-05T13:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-26T06:35:22Z" id="86360397">@kutchar I believe this should have been fixed with #9490. Have you tried this on 1.5.0?
</comment><comment author="kutchar" created="2015-03-26T18:08:06Z" id="86650716">@rjernst yes, the problem is with 1.5.0. I haven't tried it with the older versions.

```
"version" : {
    "number" : "1.5.0",
    "build_hash" : "544816042d40151d3ce4ba4f95399d7860dc2e92",
    "build_timestamp" : "2015-03-23T14:30:58Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
}
```
</comment><comment author="rjernst" created="2015-03-27T05:41:06Z" id="86834212">@kutchar Sorry I had this mixed up with specifying types for parent/child.  What you are seeing here is in fact a difficult limitation to overcome because of the eager binding of the query parser.  When the inner term query is parsed, it looks up this field in the mappings immediately, and creates a lucene TermQuery.  It has no idea that it is running within a nested context.  It might be possible to tweak the parsing context to "know" about the nested context, but it is possible the `path` may not be specified until after the `filter` or `query` parameters to the nested filter.

So while I agree it would be better to have the nested context be taken into account, I don't think it is possible without some major refactoring of the query parsers.
</comment><comment author="kutchar" created="2015-03-30T22:56:59Z" id="87869081">@rjernst would it make sense to mention this in the documentation?
</comment><comment author="rjernst" created="2015-03-31T02:01:07Z" id="87898017">@clintongormley might have an idea how/where best to document this?
</comment><comment author="clintongormley" created="2015-04-05T13:12:27Z" id="89769621">@rjernst actually, with #9670 in place, we need to use the full path name for all field references, not just nested queries.  I've opened #10439 for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatically add "index." prefix to the settings are changed on restore if the prefix is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10269</link><project id="" key="" /><description>Closes #10133
</description><key id="64403839">10269</key><summary>Automatically add "index." prefix to the settings are changed on restore if the prefix is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-26T00:33:17Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-03-30T23:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-26T13:12:35Z" id="86507926">LGTM. I'm not to familiar with the snapshot and restore API, so I'd love a second reviewer to validate the context.

One thing I was wondering is how a settings which isn't prefixed by "index." could end up in a snapshot. It seems that at least the current code changes any incoming requests to use the full syntax.
</comment><comment author="imotov" created="2015-03-26T13:32:58Z" id="86514694">@bleskes it's not for the settings in the snapshot, it's for the settings that you would like to modify during restore. For example, it's possible to create an index with `index.refresh_interval` setting as well as simply `refresh_interval`. The result is the same. However, without this change if you will try to change the `refresh_interval` in the later way (without `index.`) this change will be simply ignored. Basically [this test](https://github.com/elastic/elasticsearch/pull/10269/files#diff-0073fea52aa3281043bc830493d899b1R1560) fails.
</comment><comment author="dakrone" created="2015-03-30T17:58:14Z" id="87773051">Can you move all of the hardcoded `"index."` strings into a static `IndexMetaData.INDEX_SETTING_PREFIX`? Makes it much less likely that we will typo something in the future.

Other than that and the question I asked, LGTM.
</comment><comment author="dakrone" created="2015-03-30T17:59:48Z" id="87773870">@imotov I also think it may be good to backport this to 1.5 for the 1.5.1 release
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `_field_names` to be disabled on pre 1.3.0 indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10268</link><project id="" key="" /><description>In #9893, an enabled flag was added for _field_names.  However,
backcompat for indexes created before 1.3.0 (when _field_names
was added) was lost. This change corrects the mapper
to always be disabled when used with older indexes that
cannot have _field_names.
</description><key id="64375161">10268</key><summary>Fix `_field_names` to be disabled on pre 1.3.0 indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T21:17:18Z</created><updated>2015-06-07T18:34:30Z</updated><resolved>2015-03-25T21:28:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-03-25T21:25:16Z" id="86223244">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Concurrent recoveries setting applies only to target nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10267</link><project id="" key="" /><description>Test case is to use shard allocation filtering to exclude a single node (node2) from the cluster:

Concurrent recoveries set to 1, &#8220;target&#8221; nodes are throttled because only 1 shard is initializing on each node due to relocation from node 2.  But this does not seem to apply to the &#8220;source&#8221; node (node2) where it is relocating 3 at a time.

![image](https://cloud.githubusercontent.com/assets/7216393/6834867/16835a86-d2f4-11e4-978b-8b0dab7118e4.png)

Compare this to concurrent recoveries set to 2 (default), &#8220;target&#8221; nodes have up to 2 shards relocating to each node (consistent with the concurrent recoveries setting).  Which equates to 6 concurrent relocations from node 2 occurring. 

![image](https://cloud.githubusercontent.com/assets/7216393/6834874/21052bb0-d2f4-11e4-98fb-dddcff20e248.png)

Have the impression that concurrent recoveries apply to both source and target nodes.  Is there a way to control the concurrent recoveries for the source node?  Or is this is a special case (for exclusions) where the idea is to get the shards off the node to be decommissioned as fast as possible so we are only applying concurrent recoveries to the &#8220;target&#8221; nodes and not the source ?  
</description><key id="64367559">10267</key><summary>Concurrent recoveries setting applies only to target nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-03-25T20:41:00Z</created><updated>2015-12-06T08:00:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-26T12:34:03Z" id="86490909">&gt; Have the impression that concurrent recoveries apply to both source and target nodes

The `cluster.routing.allocation.node_concurrent_recoveries` settings concerns initializng shards, which implicitly mean target nodes. 

For the source node we have the `indices.recovery.concurrent_streams` which control how many concurrent out going file copy requests we can have. That effectively throttles recoveries on the source.  

Was this causing any issue?
</comment><comment author="ppf2" created="2015-03-26T21:50:40Z" id="86731036">Thanks @bleskes .  Tried `indices.recovery.concurrent_streams` but it does not appear to have an effect.  The following are the settings:

```
{
   "persistent": {
      "indices": {
         "recovery": {
            "concurrent_streams": "1"
         }
      },
      "cluster": {
         "routing": {
            "allocation": {
               "cluster_concurrent_rebalance": "2",
               "node_concurrent_recoveries": "2"
            }
         }
      }
   },
   "transient": {}
}
```

And the source is still showing 6 out going file copy requests.

![image](https://cloud.githubusercontent.com/assets/7216393/6857863/0b920fea-d3c7-11e4-89d0-75f1dfe44a9e.png)

For a ~100 node cluster with a node getting excluded, this is causing many out going file copy requests from the source node.
</comment><comment author="bleskes" created="2015-03-29T18:10:45Z" id="87451678">@ppf2  yeah. I agree it's confusing. `indices.recovery.concurrent_streams` doesn't limit the number of source recoveries that can go on but rather how big the pull of out going requests they can have. The default of 3 means that all the recoveries copying files of a node can only have 3 "inflight" requests _together_. We also have a separate pool (and setting) for very small files to make sure those are not blocked by 3 large files. I'm wondering if we can just make do with the same concurrent recoveries as we do for the target side.
</comment><comment author="clintongormley" created="2015-12-05T20:59:27Z" id="162245324">@bleskes anything to do here?
</comment><comment author="bleskes" created="2015-12-06T08:00:05Z" id="162285592">yeah, I think this is still an open confusion. There were some extra discussion https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161227518 and  https://github.com/elastic/elasticsearch/issues/15161#issuecomment-161755836 

The gist of the concern is that we need to find a way not to throttle recovery of replicas (from the primary)  for new indices. The reason is that two long ongoing relocations doesn't throttle getting to green when creating new indices (who's shard are small/empty so they don't matter).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: add delete-by-query into translog in OldIndexBackwardsCompatibilityTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10266</link><project id="" key="" /><description>This PR just improves our back compat tests in preparation for #10067.  It's a standalone test improvement and I think we should push it first since when I deleted DBQ entirely in #10067, no tests failed when I broke back compat.

I fixed create-bwc-index.py to add a delete-by-query into the translog so that on upgrade the translog must be replayed, and I also fixed OldIndexBackwardsCompatibilityTests to confirm the expected documents are in fact deleted on upgrade.

I confirmed that if I intentionally break the translog replay of DBQ in master, OldIndexBackwardsCompatibilityTests in fact fails (good).

However, I hit a pre-existing back-compat bug caused long ago by #4074.  I opened #10262 for this but I'm not sure how to fix it... for now I worked around it here by avoiding DBQ in translog for version &lt;= 1.0.0 Beta2.

I also fixed get-bwc-version.py to special case 1.2.0 and pull that from maven instead of download.elasticsearch.org.
</description><key id="64311407">10266</key><summary>Tests: add delete-by-query into translog in OldIndexBackwardsCompatibilityTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T16:22:25Z</created><updated>2015-04-09T08:51:01Z</updated><resolved>2015-03-26T14:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-26T02:46:11Z" id="86308302">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_ttl calculation appears to be incorrect when _timestamp extracted from external value via path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10265</link><project id="" key="" /><description>on 1.5.0
Creating a document using the below

```
DELETE my_index

PUT my_index

PUT my_index/_mapping/doc
{
  "_timestamp": {
    "enabled": true,
    "store": "yes",
    "path": "my_external_timestamp"
  },
  "_ttl": {
    "enabled": true,
    "default": "1s"
  },
  "properties": {
    "title": {
      "type": "string"
    },
    "my_external_timestamp":{
      "type": "date"
    }
  }
}
```

index the doc at around 2015-03-25T16:03:00.000 

```
PUT my_index/doc/1
{"title":"test",
 "my_external_timestamp":"2015-03-25T16:05:00.000"}
```

Immediately ask for current _ttl 

```
GET my_index/_search
{
  "fields": [
    "_timestamp",
    "_ttl",
    "_source"
  ],
  "query": {
    "match_all": {}
  }
}
```

returns

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "my_index",
            "_type": "doc",
            "_id": "1",
            "_score": 1,
            "_source": {
               "title": "test",
               "my_external_timestamp": "2015-03-25T16:05:00.000"
            },
            "fields": {
               "_ttl": 3699935,
               "_timestamp": 1427299500000
            }
         }
      ]
   }
}
```

_ttl is way bigger than it should be ( should be about 3 minutes )
_timestamp is correct

for 1427299500000
Convert epoch to human readable date and vice versa
 Timestamp to Human date  [batch convert timestamps to human dates]
Assuming that this timestamp is in milliseconds:
GMT: Wed, 25 Mar 2015 16:05:00 GMT

Exact same test works as expected when _timestamp is not extracted via path and I let elasticsearch assign default value of now().
</description><key id="64294337">10265</key><summary>_ttl calculation appears to be incorrect when _timestamp extracted from external value via path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>bug</label></labels><created>2015-03-25T15:10:49Z</created><updated>2015-03-25T15:56:06Z</updated><resolved>2015-03-25T15:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2015-03-25T15:52:53Z" id="86093043">seems to be off roughly by one hour?
am I missing anything re-timezone?
</comment><comment author="nellicus" created="2015-03-25T15:56:05Z" id="86093956">on same system when I let elasticsearch assign now() that equals to now-1h.
closing until I'll do some more test 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>zen unicast discovery not detecting nodes when protected by shield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10264</link><project id="" key="" /><description>Hello, when installed shield on nodes which are present on different VPS to protect ELK , the hosts are failing to discover each other. The log lines point to shield exception :

org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:178)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:130)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.InvalidClassException: failed to read class descriptor
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1606)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1520)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1776)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1999)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:503)
    at java.lang.Throwable.readObject(Throwable.java:913)
    at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:502)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1898)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1803)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:176)
    ... 23 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.shield.authc.AuthenticationException
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:262)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.loadClass(ThrowableObjectInputStream.java:93)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:67)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1604)
    ... 38 more
[2015-03-25 13:22:49,971][WARN ][transport.netty          ] [remotenode] Message not fully read (response) for [63272] handler org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$4@404ce7aa, error [true], resetting
[2015-03-25 13:22:51,471][WARN ][discovery.zen.ping.unicast] [remotenode] failed to send ping to [[#zen_unicast_1#][ip-][inet[/&lt;node_ip_address&gt;:9300]]]
</description><key id="64294266">10264</key><summary>zen unicast discovery not detecting nodes when protected by shield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jagadeeshv16</reporter><labels><label>feedback_needed</label></labels><created>2015-03-25T15:10:37Z</created><updated>2015-04-26T19:55:35Z</updated><resolved>2015-04-26T19:55:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-03-26T10:18:01Z" id="86438891">@jagadeeshv16 can you confirm that you have Shield installed on all of your nodes? The exception looks like a ping request was sent from a node without Shield to a node with Shield installed.
</comment><comment author="clintongormley" created="2015-04-26T19:55:34Z" id="96428526">No more info provided.  Closing, but feel free to reopen if you have more info.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix 2 bugs in `children` agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10263</link><project id="" key="" /><description>PR for #10158

1) Multiple nested children aggs result in a NPE. This only occurs in 1.x and not in master, since post collection works there in the correct order.
2) Fixed a counting bug where the same readers where post collected twice. Possible fixes #9958

Note this PR is against 1.x b/c bug 1 is fixed in master already. This bug fix was part of a refactoring that happened as part of #9544
</description><key id="64276548">10263</key><summary>Fix 2 bugs in `children` agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T14:01:35Z</created><updated>2015-06-08T00:14:13Z</updated><resolved>2015-03-28T07:50:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-26T23:23:11Z" id="86751842">LGTM
</comment><comment author="martijnvg" created="2015-03-28T07:50:33Z" id="87188410">pushed to master, 1.x and 1.5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: delete-by-query fails to replay from translog &lt; 1.0.0 Beta2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10262</link><project id="" key="" /><description>Working on #10067, improving our back-compat test indices so that the translog has a delete-by-query on upgrade, I hit this pre-existing back-compat bug where on upgrade of an index &lt;= 1.0.0 Beta2 that has a DBQ in its translog, this exception shows up:

```
  1&gt; [2015-03-25 08:57:35,714][INFO ][index.gateway            ] [node_t3] [test][0] ignoring recovery of a corrupt translog entry
  1&gt; org.elasticsearch.index.query.QueryParsingException: [test] request does not support [range]
  1&gt;    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:362)
  1&gt;    at org.elasticsearch.index.shard.IndexShard.prepareDeleteByQuery(IndexShard.java:537)
  1&gt;    at org.elasticsearch.index.shard.IndexShard.performRecoveryOperation(IndexShard.java:864)
  1&gt;    at org.elasticsearch.index.gateway.IndexShardGateway.recover(IndexShardGateway.java:235)
  1&gt;    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:114)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
```

This is happening because of #4074 when we required that the top-level "query" is present to delete-by-query requests, but prior to that we required that it is not present.  So the translog has a DBQ without "query" and when we try to parse it we hit this exception.

I have changes to create-bwc-index.py that shows the bug ... but I'm not sure how to cleanly fix it.  Somehow on parsing a translog entry from an old enough version of ES we need to insert "query" at the top...
</description><key id="64276052">10262</key><summary>Core: delete-by-query fails to replay from translog &lt; 1.0.0 Beta2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T13:59:36Z</created><updated>2015-06-03T18:38:23Z</updated><resolved>2015-06-03T18:38:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove existing _es090* files since Bloom filters are now deprecated?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10261</link><project id="" key="" /><description>After upgrading to 1.5.0 which includes #8571 I was hoping I could free up some diskspace from our nodes. Is there a way to remove the *_es090 files (since they are unused) now that the functionality is deprecated or do I need to wait for the entire functionality to be removed as well?
</description><key id="64259893">10261</key><summary>Remove existing _es090* files since Bloom filters are now deprecated?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SjonHortensius</reporter><labels /><created>2015-03-25T12:40:23Z</created><updated>2015-03-25T15:37:43Z</updated><resolved>2015-03-25T15:37:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-25T13:42:58Z" id="86030550">You cannot [easily] remove these files from existing shards, however natural merging over time will remove them since the newly merged segment will not write any bloom filters.
</comment><comment author="SjonHortensius" created="2015-03-25T15:03:10Z" id="86070534">Okay so the only way to get rid of them would be to ex&amp;import existing data to a new index on e/s 1.5 which will then not write the bloom filters? The es090\*  consumes roughly 40% of our disk-space so this might actually be an interesting option for us
</comment><comment author="mikemccand" created="2015-03-25T15:34:29Z" id="86084404">Hang on: it's only the .blm files that will go away, and this almost certainly should not be 40% of your index...
</comment><comment author="SjonHortensius" created="2015-03-25T15:37:43Z" id="86085373">Ah, you're right; that is actually less then 1%. So maybe this isn't as interesting as I thought. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: new `frequencies` aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10260</link><project id="" key="" /><description>When working on time-based data, a farily common question is to ask about the frequency of certain field values. For instance imagine that you are indexing Apache logs, you might wonder how many visitors are new visitors vs. recurring visitors. Or more generally, how many visitors have visited your website more than 5, 50 or 500 times.

It is possible to do it right now using the terms aggregation and doing the analysis on client-side. But this doesn't scale on high-cardinality fields as you would essentially need to retrieve the frequency of all the terms in the index. If we are speaking about unique ips that visited a high-traffic website, this just won't work. The other alternative is to implement [entity-centric indexing](https://www.elastic.co/elasticon/2015/sf/building-entity-centric-indexes) but it is not always easy/efficient (but to be fair also allows to answer many more questions).

We could make this information available by implementing a new aggregation based on the [count-min sketch](http://en.wikipedia.org/wiki/Count%E2%80%93min_sketch) algorithm.

In terms of API, this aggregation would take a list of frequencies and return (an approximation of) how many unique values have greater frequencies.

One challenge with this aggregation is to pick a good name. I'm currently thinking of `frequencies` or `count-min` (like the algorithm name) but there might be better options.

Note: this issue is mostly a reminder for myself since I've seen this kind of questions coming more often, I don't actually know yet how well or even if it will work.
</description><key id="64243058">10260</key><summary>Aggregations: new `frequencies` aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label></labels><created>2015-03-25T11:04:24Z</created><updated>2015-08-26T19:07:19Z</updated><resolved>2015-08-26T19:07:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-25T15:11:28Z" id="86076217">Here is a paper that gives a nice explanation of the algorithm as well as error bounds: http://dimacs.rutgers.edu/~graham/pubs/papers/cmsoft.pdf
</comment><comment author="jpountz" created="2015-08-26T19:07:19Z" id="135142247">Closing: this is harder than I expected.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10259</link><project id="" key="" /><description>Hello, I downloaded elasticsearch 1.5.0, I tried to run it but I got this error:

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.Elasticsearch
   at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
   at java.security.AccessController.doPrivileged(Native Method)
   at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
   at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
   at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
```

What's the problem?
</description><key id="64238657">10259</key><summary>Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mohamed70</reporter><labels><label>feedback_needed</label></labels><created>2015-03-25T10:36:07Z</created><updated>2015-04-30T09:06:48Z</updated><resolved>2015-04-26T20:08:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-25T17:11:25Z" id="86125462">Which exact file did you download?
How did you install it?
On which OS?
</comment><comment author="clintongormley" created="2015-04-26T20:08:01Z" id="96429475">No more info provided. Closing
</comment><comment author="Mohamed70" created="2015-04-29T09:10:02Z" id="97362548">Sorry, I was busy that's why I didn't answer...
So, I tested ZIP and TAR, and it doesn't works, it only works with the DEB archive.
When I run with zip or tar I get this:

```
root@debian:/home/user/elasticsearch-1.5.2# ./bin/elasticsearch
Exception in thread "main" java.lang.NoClassDefFoundError:     org/elasticsearch/bootstrap/Elasticsearch
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.Elasticsearch
at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
```

And I want to test the Marvel plugin but I also get a similar error:

```
root@debian:/home/user/elasticsearch-1.5.2# ./bin/plugin -i elasticsearch/marvel/latest
Exception in thread "main" java.lang.UnsupportedClassVersionError:      org/elasticsearch/plugins/PluginManager : Unsupported major.minor version 51.0
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.plugins.PluginManager. Program will exit.
```
</comment><comment author="dadoonet" created="2015-04-29T09:41:17Z" id="97371321">The second trace is different than the first one. But it shows something interesting which is that you are using an old JVM.
</comment><comment author="dadoonet" created="2015-04-29T09:43:04Z" id="97371929">Could you run also `ls -l lib` and paste the result here?
</comment><comment author="Mohamed70" created="2015-04-29T09:50:45Z" id="97374167">Ok.
I'm using a virtual machine with VMPlayer where Debian is installed.
The result of ls -l lib is:

```
root@debian:/home/mohamed/T&#233;l&#233;chargements/elasticsearch-1.5.2# ls -l lib
total 28864
-rw-rw-r-- 1 root root   167735 avril 27 09:36 antlr-runtime-3.5.jar
-rw-rw-r-- 1 root root   448794 avril 27 11:04 apache-log4j-extras-1.2.17.jar
-rw-rw-r-- 1 root root    47359 avril 27 09:36 asm-4.1.jar
-rw-rw-r-- 1 root root    38090 avril 27 09:36 asm-commons-4.1.jar
-rw-rw-r-- 1 root root 13774099 avril 27 11:22 elasticsearch-1.5.2.jar
-rw-rw-r-- 1 root root  6919921 avril 27 11:04 groovy-all-2.4.0.jar
-rw-rw-r-- 1 root root   914597 avril 27 09:36 jna-4.1.0.jar
-rw-rw-r-- 1 root root   794991 avril 27 09:36 jts-1.13.jar
-rw-rw-r-- 1 root root   489884 avril 27 09:36 log4j-1.2.17.jar
-rw-rw-r-- 1 root root  1699875 avril 27 09:36 lucene-analyzers-common-4.10.4.jar
-rw-rw-r-- 1 root root  2563490 avril 27 09:36 lucene-core-4.10.4.jar
-rw-rw-r-- 1 root root    75491 avril 27 09:36 lucene-expressions-4.10.4.jar
-rw-rw-r-- 1 root root   107784 avril 27 09:36 lucene-grouping-4.10.4.jar
-rw-rw-r-- 1 root root   138282 avril 27 09:36 lucene-highlighter-4.10.4.jar
-rw-rw-r-- 1 root root    64021 avril 27 09:36 lucene-join-4.10.4.jar
-rw-rw-r-- 1 root root    36078 avril 27 09:36 lucene-memory-4.10.4.jar
-rw-rw-r-- 1 root root    97173 avril 27 09:36 lucene-misc-4.10.4.jar
-rw-rw-r-- 1 root root   213029 avril 27 09:36 lucene-queries-4.10.4.jar
-rw-rw-r-- 1 root root   391515 avril 27 09:36 lucene-queryparser-4.10.4.jar
-rw-rw-r-- 1 root root   119449 avril 27 09:36 lucene-sandbox-4.10.4.jar
-rw-rw-r-- 1 root root   126794 avril 27 09:36 lucene-spatial-4.10.4.jar
-rw-rw-r-- 1 root root   179011 avril 27 09:36 lucene-suggest-4.10.4.jar
drwxr-xr-x 2 root root     4096 avril 29 11:00 sigar
-rw-rw-r-- 1 root root   102177 avril 27 09:36 spatial4j-0.4.1.jar
```
</comment><comment author="dadoonet" created="2015-04-29T09:55:38Z" id="97375048">Wondering if something could be wrong with your path which contains french characters: `T&#233;l&#233;chargements`. Could you try to move your elasticsearch installation in another dir like `/home/mohamed/work/`? Also it sounds like you have downloaded and extracted elasticsearch as `root`. Are you running your terminal as `root`?

If so, may be you should try to launch elasticsearch with `sudo`?
</comment><comment author="Mohamed70" created="2015-04-29T10:09:53Z" id="97377479">I tried to move elasticsearch to another dir and to extract it without root privilege but I also have the same problem.
</comment><comment author="dadoonet" created="2015-04-29T10:22:46Z" id="97379384">what gives `java -version`?
</comment><comment author="Mohamed70" created="2015-04-29T11:37:26Z" id="97396581">```
user@debian:~/Documents/elasticsearch-1.5.2$ java -version
java version "1.6.0_34"
OpenJDK Runtime Environment (IcedTea6 1.13.6) (6b34-1.13.6-1~deb7u1)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)
```
</comment><comment author="dadoonet" created="2015-04-29T11:38:24Z" id="97396961">Not sure it's your issue here but for sure elasticsearch 1.5 won't work with Java 6.
It requires at least Java7
</comment><comment author="Mohamed70" created="2015-04-29T12:50:57Z" id="97417165">Now I installed java 1.7 and it works!! Thanks.
</comment><comment author="Mohamed70" created="2015-04-29T15:39:38Z" id="97474176">I installed elasticsearch with deb package, then how I can install the marvel plugin?
</comment><comment author="dadoonet" created="2015-04-29T15:51:29Z" id="97477413">Please use the mailing list for your questions.
</comment><comment author="Mohamed70" created="2015-04-30T08:10:53Z" id="97701853">Which mailing list?
</comment><comment author="dadoonet" created="2015-04-30T08:11:30Z" id="97702038">https://groups.google.com/forum/?hl=fr&amp;fromgroups#!forum/elasticsearch
</comment><comment author="Mohamed70" created="2015-04-30T09:06:48Z" id="97713297">Ok.

2015-04-30 10:12 GMT+02:00 David Pilato notifications@github.com:

&gt; https://groups.google.com/forum/?hl=fr&amp;fromgroups#!forum/elasticsearch
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/10259#issuecomment-97702038
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[KIBANA 4] Unable to use other aggregation than count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10258</link><project id="" key="" /><description>As I got now answer in the forums, I announce this as a possible bug:

I want to create a View which shows an average request time in Kibana (which worked in Kibana 3). I have configured the conversion of the according Integer-String in Logstash as follows:

```
mutate {
    convert =&gt; ["request_time", "integer"]
}
```

In Kibana, I see 'request_time' marked with an exclamation mark in a triangle (no idea what that means exactly, I found no information about this).
The I try to create a Visualization: When I want to do an Y-Axis Aggregation and select 'Average' as the Addregation, I get an empty Field-DDLB where I would expet at least 'request_time' to select as the base to build the calculation. 
</description><key id="64236580">10258</key><summary>[KIBANA 4] Unable to use other aggregation than count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pueffl</reporter><labels /><created>2015-03-25T10:21:52Z</created><updated>2015-03-25T12:06:50Z</updated><resolved>2015-03-25T10:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-25T10:59:05Z" id="85982198">@pueffl I'm sorry to send you to yet-another-place, but I think your best chance for a good answer is at the Kibana github repo (https://github.com/elastic/kibana ). Do you mind opening the issue there? it is watched but the Kibana devs. 

I'm closing it here for now as it is not an ES issue (as far as I can tell). 
</comment><comment author="pueffl" created="2015-03-25T12:06:50Z" id="86000192">No problem, I opened the issue there :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 1.5 initialize error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10257</link><project id="" key="" /><description>I have already install the lastest sheild from https://download.elasticsearch.org/elasticsearch/shield/shield-1.1.0.zip

```
[2015-03-25 17:04:54,234][INFO ][node                     ] [Wind Dancer] version[1.5.0], pid[50884], build[5448160/2015-03-23T14:30:58Z]
[2015-03-25 17:04:54,234][INFO ][node                     ] [Wind Dancer] initializing ...
[2015-03-25 17:04:54,288][INFO ][plugins                  ] [Wind Dancer] loaded [shield, license], sites []
[2015-03-25 17:04:54,827][INFO ][transport                ] [Wind Dancer] Using [org.elasticsearch.shield.transport.ShieldServerTransportService] as transport service, overridden by [shield]
[2015-03-25 17:04:54,827][INFO ][transport                ] [Wind Dancer] Using [org.elasticsearch.shield.transport.netty.ShieldNettyTransport] as transport, overridden by [shield]
[2015-03-25 17:04:54,827][INFO ][http                     ] [Wind Dancer] Using [org.elasticsearch.shield.transport.netty.ShieldNettyHttpServerTransport] as http transport, overridden by [shield]
[2015-03-25 17:04:57,781][INFO ][node                     ] [Wind Dancer] initialized
[2015-03-25 17:04:57,782][INFO ][node                     ] [Wind Dancer] starting ...
[2015-03-25 17:04:59,043][INFO ][shield.transport         ] [Wind Dancer] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/11.12.205.204:9300]}
[2015-03-25 17:04:59,066][INFO ][discovery                ] [Wind Dancer] elasticsearch/glJ3SVj_RDqKTzD301RTgw
[2015-03-25 17:04:59,090][WARN ][discovery.zen.ping.unicast] [Wind Dancer] [1] failed send ping to [Wind Dancer][glJ3SVj_RDqKTzD301RTgw][linux][inet[/11.12.205.204:9300]]
java.lang.NoSuchMethodError: org.elasticsearch.transport.netty.MessageChannelHandler.&lt;init&gt;(Lorg/elasticsearch/transport/netty/NettyTransport;Lorg/elasticsearch/common/logging/ESLogger;)V
    at org.elasticsearch.shield.transport.netty.ShieldMessageChannelHandler.&lt;init&gt;(ShieldMessageChannelHandler.java:43)
    at org.elasticsearch.shield.transport.netty.ShieldNettyTransport$SslClientChannelPipelineFactory.getPipeline(ShieldNettyTransport.java:122)
    at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:206)
    at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:778)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:731)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:704)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:216)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:376)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="64223997">10257</key><summary>elasticsearch 1.5 initialize error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhufeizzz</reporter><labels /><created>2015-03-25T09:19:27Z</created><updated>2015-03-25T11:19:36Z</updated><resolved>2015-03-25T10:18:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:18:32Z" id="85972679">Hi @zhufeizzz shield 1.1 works against 1.4.x, while shield 1.2 has same features but works against 1.5. I'm afraid you installed the wrong version. Once you move to shield 1.2 everything should be fine.
</comment><comment author="javanna" created="2015-03-25T10:28:14Z" id="85975117">One more thing, if you point the plugin script to `elasticsearch/shield/latest` that should already download the correct version depending on your elasticsearch version. Not sure, but seems like it went wrong for you because you have pointed to `elasticsearch/shield/1.1.0` directly, or did you use the `latest` shortcut?
</comment><comment author="uboness" created="2015-03-25T10:31:37Z" id="85975543">indeed `elasticsearch/shield/latest` should work... but we seem to have an issue there at the moment... checking it out
</comment><comment author="uboness" created="2015-03-25T10:36:24Z" id="85976197">oops.. my bad... seems to work after all... in any case.. see http://www.elastic.co/guide/en/shield/current/quick-getting-started.html
</comment><comment author="zhufeizzz" created="2015-03-25T11:11:03Z" id="85985146">@javanna I download shield manual from this url https://download.elasticsearch.org/elasticsearch/shield/shield-latest.zip

The server return back shield-1.1.0.zip to me, can you try this url?
</comment><comment author="javanna" created="2015-03-25T11:19:36Z" id="85987596">@zhufeizzz correct if you download manually `latest` points to `1.1`, we only have a 50% chance to give you the correct version. We are discussing how we could improve this for manual downloads. For now the behaviour you see is expected, you should download directly the 1.2.0 version [here](https://download.elasticsearch.org/elasticsearch/shield/shield-1.2.0.zip) .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Partials not working in Mustache template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10256</link><project id="" key="" /><description>There is feature in Mustache templates called [Partials](http://mustache.github.io/mustache.5.html) which allows to include one template into another.

This could be very useful feature (especially while constructing more complex queries) but it doesn't work correctly in ES [pre-registered templates](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#pre-registered-templates)

**Steps to reproduce:**
1. create file _child.mustache_ file in _config/scripts_ folder with content
   `"match_all": {}`
2. wait until ES compiles this file what is indicated by following text in log
   `compiling script file [...\config\scripts\child.mustache]`
3. create file _parent.mustache_ file in _config/scripts_ folder with content

```
{
  "query": {
    {{&gt;child}}
  }
}
```
1. When ES tries to compile parent template, following error occurs

```
failed to load/compile script [parent]
org.elasticsearch.common.mustache.MustacheException: Template 'child' not found
```

It looks like ES understands partial syntax but is unable to find existing template.

I'm facing this problem in ES 1.4.4 but [the same problem was in 1.2.2](https://groups.google.com/forum/#!searchin/elasticsearch/mustache/elasticsearch/ihTskQ2RheA/vxTNlOC_Z5wJ) too
</description><key id="64223360">10256</key><summary>Partials not working in Mustache template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdeseb</reporter><labels><label>:Search Templates</label><label>discuss</label><label>enhancement</label></labels><created>2015-03-25T09:16:45Z</created><updated>2015-12-05T20:58:32Z</updated><resolved>2015-12-05T20:58:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T11:18:43Z" id="89755146">Related to #10047
</comment><comment author="clintongormley" created="2015-12-05T20:58:32Z" id="162245280">No plans to implement partials, given then outcome of #10047 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>python API es.explain on range query : inconsistent int size constraints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10255</link><project id="" key="" /><description>I'm trying to use explain on a range query through the python API : 

(Pdb) es.explain("test8_ref05", doc_type="document", id="www.orange.fr/", body = """{"query": {"range": {"meta_description_ic": {"gte": 2147483648}}}}""")
{'_index': 'test8_ref05', '_type': 'document', '_id': 'www.orange.fr/', 'explanation': {'value': 1.0, 'details': [{'value': 1.0, 'description': 'boost'}, {'value': 1.0, 'description': 'queryNorm'}], 'description': 'ConstantScore(meta_description_ic:[-2147483648 TO *]), product of:'}, 'matched': True}

My lower bound, 2147483648 (=2^32), does not fit into a java int, and apparently gets cast to -2147483648

This does not happen if I use the ES API directly through Sense : 

GET test/document/www.orange.fr/_explain
  {"query": {"range": {"meta_description_ic": {"gte": 2147483648}}}}

{
   "_index": "test",
   "_type": "document",
   "_id": "www.orange.fr",
   "matched": false,
   "explanation": {
      "value": 0,
      "description": "ConstantScore() doesn't match id 0"
   }
}
which is what I expect, since my document has
"meta_description_ic": 1590728576
</description><key id="64218052">10255</key><summary>python API es.explain on range query : inconsistent int size constraints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alix-Martin</reporter><labels /><created>2015-03-25T08:54:05Z</created><updated>2015-04-05T11:15:56Z</updated><resolved>2015-04-05T11:15:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T11:15:56Z" id="89755077">Hi @Alix-Martin 

Please reopen this issue on https://github.com/elastic/elasticsearch-py/issues

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin manager should handle path `elastic/plugin/version` for installing plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10254</link><project id="" key="" /><description>The repository was renamed from `elasticsearch` to `elastic` but for installing plugins from this repository the path still needs to be `elasticsearch/plugin/version`. This is confusing.
</description><key id="64215418">10254</key><summary>Plugin manager should handle path `elastic/plugin/version` for installing plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Plugins</label></labels><created>2015-03-25T08:41:24Z</created><updated>2015-09-22T14:07:10Z</updated><resolved>2015-09-22T14:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-03-25T09:00:47Z" id="85937855">we should keep the installation path `elasticsearch/...` as these are elasticsearch plugins.. the repo name should not play a factor in the usage
</comment><comment author="brwe" created="2015-03-25T09:12:42Z" id="85945037">could we support both?
</comment><comment author="bleskes" created="2015-03-25T09:33:39Z" id="85955127">+1 to support both as it is confusing. We promote the notion of the first part being a github user as well, which refers to the source of the plugin (same goes for maven), not the fact that is an ES plugin:

```
bin/plugin --install github_user/repo
```

so you could read `elasticsearch/marvel/latest` as get the marvel plugin, supplied by elasticsearch (the org/company) and I want the latest version. In this interpretation people can totally expect elastic to work. Bonus points for being shorter... 
</comment><comment author="uboness" created="2015-03-25T09:46:47Z" id="85961087">I'd say this is an attempt to put patches on a broken functionality to begin with... IMO we need to drop this "magic" support for github repo and maven... we can still support them but that can come with an additional explicit arg (e.g. `--github` and `--maven`)... for the rest all downloads should go to our downloads service (or the url defined by the `--url` arg)
</comment><comment author="rjernst" created="2015-03-25T23:22:15Z" id="86250689">+1 to what @uboness said.
</comment><comment author="tlrx" created="2015-03-26T08:39:51Z" id="86404899">I don't think we need the support for 'elastic' but I like the github and maven support. I think most users don't know if a community plugin is available on github or maven or something else. 

Here's what I've done in #9998: 

I updated the plugin manager to add a notion of [plugins repository](https://github.com/elastic/elasticsearch/pull/9998/files#diff-196560d7540091985eaa960c6472daafR27). Each repository can have multiple URL patterns (the defaults are [here](https://github.com/elastic/elasticsearch/pull/9998/files#diff-9030409020becbc44b2a241180305d0fR528)). An URL pattern is used to convert the user input like `elasticsearch/marvel/latest', 'elastic/marvel' or 'marvel' into a final URL that will be tested. If the URL works, the plugin is downloaded and installed. Otherwise it will try the next pattern. When all patterns are tested and failed, it can try another plugins repo or fails the install.

Because URL patterns are tested in order and plugins repositories can be chained, and because default URL patterns for our download service are:

```
http://download.elasticsearch.org/[organisation]/[name]/[name]-[version].zip
http://download.elasticsearch.org/[organisation]/elasticsearch-[name]/elasticsearch-[name]-[version].zip
http://download.elasticsearch.org/elasticsearch/[name]/[name]-latest.zip
```

the following commands will work:

```
bin/plugin install marvel                                          (match the 3rd pattern)
bin/plugin install elasticsearch/marvel/latest          (match the 1st pattern)
bin/plugin install elastic/marvel/latest                     (match the 3rd pattern)
```

There are default plugins repositories defined for sonatype, github and maven. A fixed URL or an URL pattern can explicitly be defined with the --url option. 

Another nice thing I'd like to add once the PR is merged is the ability for users to define their own plugins repository in configuration like bintray, jcenter or a shared filesystem (with a pattern like file:///path/to/my/plugins/[name]-[version].zip). It could help a lot with deployments where there's no external access.
</comment><comment author="bleskes" created="2015-03-26T09:32:54Z" id="86420393">@tlrx sounds great. if these all work:

```
bin/plugin install marvel                       
bin/plugin install elasticsearch/marvel/latest
bin/plugin install elastic/marvel/latest  
```

I'm happy :)
</comment><comment author="tlrx" created="2015-03-26T09:33:39Z" id="86420701">@bleskes if you're happy, I'm happy. :)
</comment><comment author="brwe" created="2015-03-26T09:56:14Z" id="86430164">@tlrx I agree, once #9998 is merged this issue can be closed as far as I am concerned
</comment><comment author="brwe" created="2015-09-22T14:07:10Z" id="142299164">#9998 was merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`min_score` not working in `function_score` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10253</link><project id="" key="" /><description>The new `min_score` functionality in the `function_score` query doesn't seem to to work as advertised.  For instance:

```
DELETE t

PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "name": {
          "type": "string"
        },
        "foo": {
          "type": "nested",
          "properties": {
            "key": {
              "type": "string",
              "index": "not_analyzed"
            },
            "value": {
              "type": "long"
            }
          }
        }
      }
    }
  }
}

PUT /t/t/1
{
  "name": "test",
  "foo": [
    {
      "key": "bar",
      "value": 10
    },
    {
      "key": "bar",
      "value": 20
    },
    {
      "key": "bar",
      "value": 30
    },
    {
      "key": "bar",
      "value": 40
    }
  ]
}
```

This query returns a score of 4 for the above document, which means that it should be filtered out by the `min_score` of `10`:

```
GET /_search
{
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "min_score": 10,
      "query": {
        "nested": {
          "path": "foo",
          "score_mode": "sum",
          "query": {
            "constant_score": {
              "filter": {
                "match_all": {}
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="64182138">10253</key><summary>`min_score` not working in `function_score` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T05:36:26Z</created><updated>2015-03-31T14:21:12Z</updated><resolved>2015-03-31T14:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-03-25T09:10:42Z" id="85943650">This is a bug indeed and it seems to me for two reasons: 
1) when the score is 4 the document should not be returned 
2) the score should not be 4 in the first place. `"boost_mode": "replace"` should cause the score to be 1 (default if no functions and no query given).
I'll work on  a fix. For now the workaround would be to define a 

`"weight": 1,` 

if you really want to score all matches with 1 (which does not make much sense imo but should still work) or

```
 "script_score": {
        "script": "_score"
}, 
```

in case one actually wants the score of the query to be used.
</comment><comment author="pickypg" created="2015-03-27T02:42:53Z" id="86792897">To be clear, `min_score` as a child of `function_score` is new in 1.5.0.

`min_score` at the _root_ document level works locally running 1.4.4.
</comment><comment author="brwe" created="2015-03-30T15:02:49Z" id="87715299">Currently the way it works is this: If function_score encounters a query without a function, then the query is just executed as is without wrapping in a function score query (https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java#L161) so min_score has no effect in this case. This was implemented so on purpose but it seems to me that now with the min_score there is a usecase where you would want to use function_score without a function. I'll change that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent results </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10252</link><project id="" key="" /><description>Hi Elastic Users,
  We have 6 shards 2 replicas(3 nodes). While am searching in node1 it returns one result, then pass the query to node2 &amp; 3 it returns different order.

I tried preference = _primary &amp; _primary_first It will returns correct order in all the nodes.

But I tried preference = userId(003fdaa6-c5e7-4578-a70e-1c010d358960). This will also returns the first results(Inconsistent results).

Could you please help me how to resolve the issue.

Thanks in advance
</description><key id="64176489">10252</key><summary>Inconsistent results </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andavarlaxman</reporter><labels /><created>2015-03-25T05:07:31Z</created><updated>2015-03-25T05:15:39Z</updated><resolved>2015-03-25T05:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-03-25T05:15:39Z" id="85843010">Please use the mailing list for these sorts of problems - https://groups.google.com/forum/#!forum/elasticsearch

Github issues are for problems relating to code issues/bugs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rivers: Add back deletion of river content on river removal.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10251</link><project id="" key="" /><description>In #8877, the deletion of the type associated with a river was
removed. This change adds back the removal using a scan search
along with bulk delete requests.
</description><key id="64174168">10251</key><summary>Rivers: Add back deletion of river content on river removal.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T04:57:02Z</created><updated>2015-06-20T06:41:47Z</updated><resolved>2015-06-20T06:41:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-25T04:57:29Z" id="85835493">@kimchy Is this what you were thinking? Can you suggest how to test? The tests for rivers seem...light.
</comment><comment author="s1monw" created="2015-04-02T10:47:37Z" id="88863908">@rjernst take a look at `IndicesTTLService.PurgerThread#purgeShards` I think we should use the bulk to drive the deletion?
</comment><comment author="dadoonet" created="2015-04-25T16:59:12Z" id="96236174">Ha! I think I now understand why I can see a test failing in couchdb river in master.
As the river is not really removed when we remove `_meta` document, test is failing.

Related code in couchdb-river is https://github.com/elastic/elasticsearch-river-couchdb/blob/master/src/test/java/org/elasticsearch/river/couchdb/CouchdbRiverIntegrationTest.java#L456-457

I guess the test will be fix automagically when this PR will be merged.
</comment><comment author="rjernst" created="2015-06-20T06:41:45Z" id="113716845">Closing since rivers have been removed from master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maximum Bucket reducer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10250</link><project id="" key="" /><description>This reducer calculates the bucket(s) in a sibling aggregation which contains the maximum value of a particular metric. The request is of the form:

``` javascript
GET test/_search?search_type=count
{
  "aggs": {
    "my_date_histo": {
      "date_histogram": {
        "field": "d",
        "interval": "day",
        "min_doc_count": 0
      },
      "aggs": {
        "avg_i": {
          "avg": {
            "field": "i"
          }
        }
      }
    },
    "max_avg_i": {
      "max_bucket": {
        "bucketsPath": "my_date_histo&gt;avg_i"
      }
    }
  }
}
```

Which will produce the following response:

``` javascript
{
   "took": 256,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 10,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "my_date_histo": {
         "buckets": [
            {
               "key_as_string": "2014-01-01T00:00:00.000Z",
               "key": 1388534400000,
               "doc_count": 1,
               "avg_i": {
                  "value": 1,
                  "value_as_string": "1.0"
               }
            },
            ...
            {
               "key_as_string": "2014-01-10T00:00:00.000Z",
               "key": 1389312000000,
               "doc_count": 1,
               "avg_i": {
                  "value": 10,
                  "value_as_string": "10.0"
               }
            }
         ]
      },
      "max_avg_i": {
         "value": 10,
         "keys": [
            "2014-01-10T00:00:00.000Z"
         ]
      }
   }
}
```
</description><key id="64162072">10250</key><summary>Maximum Bucket reducer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label></labels><created>2015-03-25T03:23:30Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-04-09T11:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-04-02T16:39:30Z" id="88969071">Just two general things I ran into.

First, when I started to implement tests for the ACF agg, I discovered that `ReducerBuilder` needs to extend `AbstractAggregationBuilder`, or else it cannot be used as a top-level aggregation in Java queries.  E.g. this code is not possible without extending:

``` java
SearchResponse response = client()
                .prepareSearch("idx").setTypes("linear")
                .addAggregation(histogram("histo").field(INTERVAL_FIELD).interval(interval))        
                .addAggregation(acf("acf").setBucketsPaths("histo&gt;the_max"))  // &lt;-- this won't compile
                .execute().actionGet();
```

In my branch I extended `ReducerBuilder` and the change seemed fairly minor, and does not appear to break things.  But I'm not sure if it's a good idea or not, if things should be structured differently, etc.

The second issue was also related to SiblingReducers being potentially "top level aggregations".  I had to implement an `InternalACF` class so that agg responses could be serialized back to the client, as well as register the stream as an "aggregation".  So the agg has two registered streams (reducer + "aggregation").

Also doesn't seem like a big deal, but thought I should raise the issue.
</comment><comment author="colings86" created="2015-04-07T10:29:38Z" id="90500722">@polyfractal yep the first issue is my fault, I forgot to update the search builder to accept ReducerBuilders, I'll update that in this PR.

You are correct with the second issue, you will need to register two streams (reducer and aggregation). this is because the reducer has to be serialised to get it back from the shards (where the parsing of the request is currently done) for the reduce phase to have it and the aggregation has to be serialised because this is the response that is needed for the Java API. This should be helped once https://github.com/elastic/elasticsearch/issues/10217 has been merged as we should hopefully be able to get rid of the serialising of the reducers, but we will have to see once it's merged in.
</comment><comment author="polyfractal" created="2015-04-07T14:05:35Z" id="90571063">@colings86 great, thanks for clarifying those issues.  Other than that, the new sibling stuff worked great, was very happy how smooth implementing the ACF went.
</comment><comment author="colings86" created="2015-04-08T09:53:44Z" id="90866301">@polyfractal I have pushed a commit which should address your comments
</comment><comment author="colings86" created="2015-04-08T13:24:50Z" id="90914171">@jpountz would you mind re-reviewing this when you have the time? I have rebased the commits from the feature/aggs_2_0 branch since it was getting quite out of date. I also added a simple test. It will obviously need more tests before it is considered feature complete but I am keen to get this merged into the feature/aggs_2_0 branch ASAP.
</comment><comment author="jpountz" created="2015-04-08T14:54:00Z" id="90939533">Sure @colings86 I left some comments.

I think we should merge the AggregationBuilder and ReducerBuilder classes. Although we deal with them differently on server-side, they are the same from a client perspective? (but we could still keep the abstract impls that we have in order not to have to duplicate the subAggregation or bucketsPath methods everywhere)

Also this PR made me realize the ReduceContext API is not very friendly as sometimes you need to pass `null` to the wrapped aggregations. So maybe we should remove aggregations from ReduceContext and change InternalAggregation.reduce to take a list of aggregations in addition to the ReduceContext. (can be done in a different PR, just thinking out loud)
</comment><comment author="colings86" created="2015-04-08T15:01:22Z" id="90942607">@jpountz are you talking about making ReducerBuilder extend AggregationBuilder. The problem I see with that is that Aggregations are allowed sub-aggregations but reducers are not. The advantage with having the ReducerBuilder separate is that it is not possible in the Java API to create a reducer with sub-aggregations/reducers. I suppose we could override those methods in ReducerBuilder and have them throw an UnsupportedOperationExeception or leave it to the parser to detect the illegal behaviour?

I was actually going to make that change to the ReduceContext already as I saw the same thing. Even before we introduce the reducers framework there are many instances in the code where we pass in null for the aggregations when constructing the ReduceContext because we don't need it at that point. I will do that in a separate PR after this is merged (if that works for you?).
</comment><comment author="jpountz" created="2015-04-08T15:18:52Z" id="90947721">&gt; @jpountz are you talking about making ReducerBuilder extend AggregationBuilder.

Oh, I just noticed that we already have a base class: AbstractAggregationBuilder. So maybe we just need to make ReducerBuilder extend it and then we don't need to add a new method to eg. PercolateRequestBuilder?

&gt; I was actually going to make that change to the ReduceContext already as I saw the same thing. Even before we introduce the reducers framework there are many instances in the code where we pass in null for the aggregations when constructing the ReduceContext because we don't need it at that point. I will do that in a separate PR after this is merged (if that works for you?).

Awesome. :)
</comment><comment author="colings86" created="2015-04-09T10:00:51Z" id="91180359">@jpountz I pushed a new commit with the builder changes we discussed
</comment><comment author="jpountz" created="2015-04-09T10:04:39Z" id="91181153">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change output of _cat/fielddata to be row-centric</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10249</link><project id="" key="" /><description>Currently `_cat/fielddata` adds a column per field:

```
id                     host        ip         node          total title disambiguation 
lFPaYAUQTUGKOIKiRYZOeg hotel.local 10.0.1.200 raphael          0b    0b             0b 
qfsKN3rAQ0q9Ef1eUF-fvA hotel.local 10.0.1.200 splinter         0b    0b             0b 
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo 9.7kb 8.2kb          1.4kb 
OsBOGm9UQFypK10Kx4r99A hotel.local 10.0.1.200 donatello        0b    0b             0b 
3YEw8s0KTTaLrn3HVNkXCg hotel.local 10.0.1.200 leonardo         0b    0b             0b 
```

which isn't really the spirit of the cat api.  In order to find out how many fields are in the cache on `michaelangelo`, eg, you'd have to count the columns. In order to sort the fields in descending order by bytes, you'd have to do some significant processing (read: something more than counting some lines) outside the api, rather than just a `?bytes=b | sort -rnk999`.

I propose something like:

```
id                     host        ip         node          field           size
lFPaYAUQTUGKOIKiRYZOeg hotel.local 10.0.1.200 raphael       total             0b
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo total          9.7kb
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo title          8.2kb
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo disambiguation 1.4kb
```

Then you can ask questions like:

```
curl -s 'localhost:9200/_cat/fielddata?h=node,field,size&amp;bytes=b' | fgrep -v ' total' | sort -rnk3 | head
```

to get a snapshot of the biggest offenders and where they live.  This denormalization is the heart of the API, like with `segments` and `shards`.
</description><key id="64157810">10249</key><summary>Change output of _cat/fielddata to be row-centric</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha3</label></labels><created>2015-03-25T02:57:49Z</created><updated>2016-05-10T14:28:00Z</updated><resolved>2016-05-10T14:28:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2015-04-08T03:44:06Z" id="90795211">Also filtering by fields by url param might be good addition  
</comment><comment author="szroland" created="2015-06-17T15:38:31Z" id="112852843">What if you have a field called `total` or have total in the name? Is it a good idea to mix the aggregate size with individual field sizes in the same table?

Maybe there could be a flag that would allow you to switch between totals or individual fields:

```
_cat/fielddata
```

```
id                     host        ip         node          field           size
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo title          8.2kb
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo disambiguation 1.4kb
```

But if you want totals:

```
_cat/fielddata?sum
```

```
id                     host        ip         node          size
lFPaYAUQTUGKOIKiRYZOeg hotel.local 10.0.1.200 raphael       0b
djZlGw6qQcGBNCqW5iFoIA hotel.local 10.0.1.200 michaelangelo 9.7kb
```
</comment><comment author="szroland" created="2015-06-21T19:58:01Z" id="113950196">I created a pull request that implements this behavior, see just above. Instead of the idea I described previously, it seemed cleaner to use `_cat/fielddata` for totals, and `_cat/fielddata/{fields]` for field level details.

E.g.:

`curl '192.168.56.10:9200/_cat/fielddata'`

```
id                     host    ip            node          total
c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb
waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary     435.2kb
yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip     284.6kb
```

Whereas if you want field-level data, you can do either
`curl '192.168.56.10:9200/_cat/fielddata?fields=body,text'` or 
`curl '192.168.56.10:9200/_cat/fielddata/*'` to get:

```
id                     host    ip            node          total   field size
c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb body  159.8kb
c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb text  225.7kb
waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary     435.2kb body  159.8kb
waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary     435.2kb text  275.3kb
yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip     284.6kb body  109.2kb
yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip     284.6kb text  175.3kb
```

Let me know what you think.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `validate_*` and `normalize_*`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10248</link><project id="" key="" /><description>No need to carry 3 validate and 3 normalize options in GeoPointFieldMapping. This is just causing confusion. This change deprecates the validate_lat, validate_lon, normalize_lat, and normalize_lon options on the `geo_point` field mapping and simplifies the options by using just `validate` and `normalize`, respectively. Geo filters have been updated to include a `validate` and `normalize` option to provide consistency with the field mapping defaults. Unit tests and documentation have also been updated.

closes #10170
</description><key id="64154632">10248</key><summary>Deprecate `validate_*` and `normalize_*`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>deprecation</label><label>v1.7.1</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T02:38:10Z</created><updated>2015-08-07T10:08:02Z</updated><resolved>2015-07-16T21:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-25T03:56:31Z" id="85819061">@nknize I left a couple comments.

@clintongormley Maybe you have some thoughts on whether the breaking change should happen for 1.6 or 2.0?
</comment><comment author="dakrone" created="2015-05-04T20:27:04Z" id="98840554">@clintongormley I think deprecate in 1.6.0 and remove in 2.0.0, what do you think?
</comment><comment author="clintongormley" created="2015-05-05T08:15:16Z" id="98990164">Wondering if we should make the whole change (including removing `validate`) in 2.0.  So just mark the options as deprecated in 1.6, eg:

```
|`validate` | deprecated[1.6.0, This parameter will be removed in 2.0] Set to `false` to accept geo points with invalid latitude or
longitude (default is `true`). *Note*: Validation only works when
normalization has been disabled. This option will be deprecated and removed
```
</comment><comment author="clintongormley" created="2015-06-04T19:27:52Z" id="109019693">@nknize any movement here?
</comment><comment author="nknize" created="2015-06-09T15:11:14Z" id="110396133">@clintongormley looks like my issue notification was eaten by build failures. I'll deprecate in 1.6.1 and completely remove in 2.0. This one has hung around long enough.
</comment><comment author="clintongormley" created="2015-06-09T17:37:50Z" id="110442561">@nknize perhaps do something similar to the suggestion here: https://github.com/elastic/elasticsearch/pull/11161#issuecomment-110442105
</comment><comment author="martijnvg" created="2015-07-16T09:55:48Z" id="121912645">Bumping the version up to 1.7.1 for the today's release.
</comment><comment author="nknize" created="2015-07-16T21:44:26Z" id="122112388">closing this PR in favor of #12300 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Revamp static bwc test framework to use dangling indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10247</link><project id="" key="" /><description>The static old index tests currently take a long time to run because
each index version essentially recreates the cluster, and spins up
new nodes.  This PR instead loads each old version into the existing
cluster as a dangling index. It also removes the intermediate
"StaticIndexBackwardCompatibilityTest" which was an extra layer
with no purpose, and moves a shared version of a commonly found
function to get an http client.

The test now takes between 40 and 60 seconds for me. I also ran it
"under stress" by running all ES tests in one shell, while
simultaneously running 10 iterations of the old index tests. Each
iteration took on average about 90 seconds, which is much better
than the 20+ minutes we see in master on jenkins.
</description><key id="64141160">10247</key><summary>Tests: Revamp static bwc test framework to use dangling indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-25T01:13:43Z</created><updated>2015-04-04T06:23:34Z</updated><resolved>2015-04-04T06:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-25T09:22:22Z" id="85949998">This is much cleaner! thx . Left some comments.
</comment><comment author="rjernst" created="2015-03-26T07:15:57Z" id="86366742">@bleskes I pushed another commit with some changes based on your feedback.
</comment><comment author="bleskes" created="2015-03-26T15:30:49Z" id="86571067">Thx @rjernst . I replied to the comments.
</comment><comment author="rjernst" created="2015-03-31T06:49:26Z" id="87966540">@bleskes I pushed a new commit.  I believe this addresses your concern over using hard coded paths.
</comment><comment author="bleskes" created="2015-03-31T06:54:42Z" id="87967329">Awesome. LGTM. I will work on the dangling request issue today, so we can get this in.
</comment><comment author="bleskes" created="2015-03-31T06:55:48Z" id="87967421">scratch the waiting for the dangling indices request issue. missed the master:false on the loading node. No need to wait indeed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add node id to _cat apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10246</link><project id="" key="" /><description>this addresses https://github.com/elastic/elasticsearch/issues/10226 and also adds node id to _cat/segments and _cat/plugins.

/cc @drewr @javanna 
</description><key id="64128488">10246</key><summary>Add node id to _cat apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-24T23:43:40Z</created><updated>2015-06-06T19:06:14Z</updated><resolved>2015-03-25T11:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2015-03-24T23:45:46Z" id="85742027">new CLA?
</comment><comment author="javanna" created="2015-03-25T09:56:26Z" id="85965424">looks good @lmenezes I left a small comment, if you can address that I will get this in.
</comment><comment author="lmenezes" created="2015-03-25T10:42:20Z" id="85977889">@javanna done. if you want me to rebase and squash it, let me know :)
</comment><comment author="javanna" created="2015-03-25T11:23:58Z" id="85988633">Thanks @lmenezes merged!
</comment><comment author="lmenezes" created="2015-03-25T11:30:51Z" id="85990790">@javanna great, thanks :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write to ES works from Pig results in success (DocCount looks good), but the mappings aren't there</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10245</link><project id="" key="" /><description>ES:

curl -XDELETE 'http://localhost:9200/ztmp_inventory_tool_sample'

curl -XPOST localhost:9200/ztmp_inventory_tool_sample -d '{
  "settings" : {
    "term_index_interval" : 256,
    "term_index_divisor" : 5
  },
  "mappings" : {
    "invData" : {
      "_source" : { "enabled" : true },
      "properties" : {
        "ekv_raw" : { "type" : "byte" },
        "ekv_flight" : { "type" : "byte" },
        "event_id" : { "type" : "long" },
        "cookie_id" : { "type" : "long" },
        "dpId" : { "type" : "short" },
        "vertical" : { "type" : "string" },
        "activity_group" : { "type" : "string" },
        "activity" : { "type" : "string" },
        "eventDateTime" : {"type":"date", "format":"YYYY-MM-dd'"'T'"'HH:mm:ss.SSSZ"},
        "departureDate" : { "type" : "date", "format":"YYYY-MM-dd", "ignore_malformed" : true},
        "returnDate" : { "type" : "string" },
        "origin" : { "type" : "string" },
        "destination" : { "type" : "string" },
        "destination_country_code" : { "type" : "string" },
        "destination_state" : { "type" : "string" },
        "destination_city" : { "type" : "string" },
        "carrier" : { "type" : "string" },
        "cabinClassGroup" : { "type" : "string" },
        "currency" : { "type" : "string" },
        "travelers" : { "type" : "short" },
        "duration" : { "type" : "short" },
        "bookedDate" : { "type" : "date", "format":"YYYY-MM-dd", "ignore_malformed" : true},
        "airFare" : { "type" : "float" }}
      }}}'

REGISTER elasticsearch-hadoop-2.1.0.Beta3/dist/elasticsearch-hadoop-2.1.0.Beta3.jar

DEFINE EsStorage org.elasticsearch.hadoop.pig.EsStorage('es.http.timeout=1m',
                        'es.resource=ztmp_inventory_tool_sample/invData',
                        'es.mapping.pig.tuple.use.field.names = true',
                        'es.http.timeout = 5s',
                        'es.index.auto.create = false',
                        'es.nodes = 68.67.141.63',
                        'es.port = 9200');

testEs = LOAD '/user/bganapathy/flightPKeys' USING PigStorage('|') AS (ekv_raw: int, ekv_flight: int, eventId: long, cookie_id: long, dpId: int, vertical: chararray, activity_group: chararray, activity: chararray, eventDateTime: chararray, departureDate: chararray, returnDate: chararray, origin: chararray, destination: chararray, destination_country_code: chararray, destination_state: chararray, destination_city: chararray, carrier: chararray, cabinClassGroup: chararray, currency: chararray, travelers: chararray, duration: chararray, bookedDate: chararray, airFare: chararray);

testEs = LIMIT testEs 100000;
STORE testEs INTO 'ztmp_inventory_tool_sample/invData' USING EsStorage();

Logs:

2015-03-24 22:10:34,544 [JobControl] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2015-03-24 22:10:34,755 [JobControl] INFO  org.elasticsearch.hadoop.mr.EsOutputFormat - Writing to [ztmp_inventory_tool_sample/invData]
2015-03-24 22:10:34,774 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1
2015-03-24 22:10:34,774 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
2015-03-24 22:10:34,776 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1
2015-03-24 22:10:35,971 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201408291723_27582
2015-03-24 22:10:35,971 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases testEs
2015-03-24 22:10:35,971 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M:  C:  R: testEs[-1,-1]
2015-03-24 22:10:35,971 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://hnn101-lax1:50030/jobdetails.jsp?jobid=job_201408291723_27582
2015-03-24 22:10:47,051 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 75% complete
2015-03-24 22:10:59,619 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 92% complete
2015-03-24 22:11:15,720 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2015-03-24 22:11:15,721 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 

HadoopVersion   PigVersion  UserId  StartedAt   FinishedAt  Features
2.0.0-cdh4.4.0  0.11.0-cdh4.4.0 bganapathy  2015-03-24 22:09:46 2015-03-24 22:11:15 LIMIT

Success!

Job Stats (time in seconds):
JobId   Maps    Reduces MaxMapTime  MinMapTIme  AvgMapTime  MedianMapTime   MaxReduceTime   MinReduceTime   AvgReduceTime   MedianReducetime    Alias   FeatureOutputs
job_201408291723_27581  12  1   8   5   7   7   15  15  15  15  testEs  
job_201408291723_27582  1   1   5   5   5   5   17  17  17  17  testEs      ztmp_inventory_tool_sample/invData,

Input(s):
Successfully read 1200000 records (19469816 bytes) from: "/user/bganapathy/flightPKeys"

Output(s):
Successfully stored 100000 records in: "ztmp_inventory_tool_sample/invData"
</description><key id="64113922">10245</key><summary>Write to ES works from Pig results in success (DocCount looks good), but the mappings aren't there</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">BalachanderGS</reporter><labels /><created>2015-03-24T22:19:04Z</created><updated>2015-03-26T17:18:23Z</updated><resolved>2015-03-26T15:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BalachanderGS" created="2015-03-24T22:19:50Z" id="85716917">Not sure how to debug this as there isn't any error in the Pig logs and I don't see any indexing logs in ES as well
</comment><comment author="costin" created="2015-03-26T15:13:45Z" id="86562983">@BalachanderGS This is an es-hadoop issue and should have been posted under `elastic/elasticsearch-hadoop` issue tracker.

The read/write operation succeed so likely the issue is that the data hits an ES cluster but not the one you are expecting. In the past we've seen issue with `DEFINE` command which might ignore the given configuration - in other words, while you do `EsStorage` to point to `'es.nodes = 68.67.141.63` Pig might disregard that and use the default parameter, namely the `localhost`.
You can double check this in several ways:
1. make sure there's no cluster on your localhost. If you are running the Pig script against a remote cluster, double check your elasticsearch ip.
2. turn on logging (see the docs) to double check to what IP/node the connection is made to
3. Make sure you are looking at the right cluster when checking the mapping.

I'm closing the issue for now - if you still have issues, please open a new one in the es-hadoop project.

Thanks,
</comment><comment author="BalachanderGS" created="2015-03-26T17:18:23Z" id="86630299">I think it is going to the write IP as I am able to see the indexing through Marvel (on the desired cluster). We are on Pig 0.11 and CDH 4.4. Do you think there is some version issue here ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scroll broke after upgrade to ES 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10244</link><project id="" key="" /><description>We have a script that uses the elasticsearch JS lib to pull sorted results and write log files. It worked great against a cluster running ES 1.4.1 (I think? possibly 1.4.3), but after an upgrade to 1.4.4 it appears to be returning the same results over and over.

Initial request and first result:

```
Elasticsearch DEBUG: 2015-03-24T21:27:39Z
  starting request { method: 'POST',
    path: '/streammachine-hls-sessions-2015-03-22/session/_search',
    body: 
     { query: { filtered: [Object] },
       sort: [ [Object] ],
       size: 1000,
       from: 0 },
    query: { scroll: '10s' } }
...
Elasticsearch TRACE: 2015-03-24T21:27:39Z
  -&gt; POST http://es-scpr-logstash.service.consul:9200/streammachine-hls-sessions-2015-03-22/session/_search?scroll=10s
...
  &lt;- 200
  {
    "_scroll_id": "cXVlcnlUaGVuRmV0Y2g7MzsxNDEwNTgyMTp5dzVSLXFEUVNxbXRXMHAwMFFvZV9nOzE0MTA1ODIyOnl3NVItcURRU3FtdFcwcDAwUW9lX2c7MTg1MTU5MTg6TjVmR3BlcnhRaXF0VzZKQUVRTmNCdzswOw==",
    "took": 19,
    "timed_out": false,
    "_shards": {
      "total": 3,
      "successful": 3,
      "failed": 0
    },
    "hits": {
      "total": 26316,
      "max_score": null,
      "hits": [
        {
          "_index": "streammachine-hls-sessions-2015-03-22",
          "_type": "session",
          "_id": "AUw-zBWy6JcMmvzTK2ST",
          "_score": null,
          "_source": {
            "session_id": "7F888D3A-4196-4920-B60E-820AB75D729D",
            "stream": "kpcclive",
            "time": "2015-03-22T00:00:01.800Z",
            "start_time": "2015-03-21T22:57:31.504Z",
            "client": {
              "output": "shoutcast",
              "ip": "68.105.116.228",
              "path": "/kpcclive?ua=SCPRIPAD",
              "ua": "SCPRIPAD | AppleCoreMedia/1.0.0.11D257 (iPad; U; CPU OS 7_1_2 like Mac OS X; en_us)",
              "user_id": "e55be008-82b1-4b44-a862-a4918aa690af",
              "pass_session": false,
              "session_id": "7F888D3A-4196-4920-B60E-820AB75D729D",
              "offsetSecs": -1,
              "meta_int": 32768
            },
            "kbytes": 784,
            "duration": 100.57142925262451,
            "connected": 3750.296
          },
          "sort": [
            1426982401800
          ]
        },
...
```

First scroll and first result:

```
Elasticsearch TRACE: 2015-03-24T21:27:39Z
  -&gt; POST http://es-scpr-logstash.service.consul:9200/_search/scroll?scroll=10s
  cXVlcnlUaGVuRmV0Y2g7MzsxNDEwNTgyMTp5dzVSLXFEUVNxbXRXMHAwMFFvZV9nOzE0MTA1ODIyOnl3NVItcURRU3FtdFcwcDAwUW9lX2c7MTg1MTU5MTg6TjVmR3BlcnhRaXF0VzZKQUVRTmNCdzswOw==
  &lt;- 200
  {
    "_scroll_id": "cXVlcnlUaGVuRmV0Y2g7MzsxNDEwNTgyMTp5dzVSLXFEUVNxbXRXMHAwMFFvZV9nOzE0MTA1ODIyOnl3NVItcURRU3FtdFcwcDAwUW9lX2c7MTg1MTU5MTg6TjVmR3BlcnhRaXF0VzZKQUVRTmNCdzswOw==",
    "took": 56,
    "timed_out": false,
    "_shards": {
      "total": 3,
      "successful": 3,
      "failed": 0
    },
    "hits": {
      "total": 26316,
      "max_score": null,
      "hits": [
        {
          "_index": "streammachine-hls-sessions-2015-03-22",
          "_type": "session",
          "_id": "AUw-zBWy6JcMmvzTK2ST",
          "_score": null,
          "_source": {
            "session_id": "7F888D3A-4196-4920-B60E-820AB75D729D",
            "stream": "kpcclive",
            "time": "2015-03-22T00:00:01.800Z",
            "start_time": "2015-03-21T22:57:31.504Z",
            "client": {
              "output": "shoutcast",
              "ip": "68.105.116.228",
              "path": "/kpcclive?ua=SCPRIPAD",
              "ua": "SCPRIPAD | AppleCoreMedia/1.0.0.11D257 (iPad; U; CPU OS 7_1_2 like Mac OS X; en_us)",
              "user_id": "e55be008-82b1-4b44-a862-a4918aa690af",
              "pass_session": false,
              "session_id": "7F888D3A-4196-4920-B60E-820AB75D729D",
              "offsetSecs": -1,
              "meta_int": 32768
            },
```

Importantly, it's the same scroll id and the same result set. That repeats each time we try to scroll.
</description><key id="64108165">10244</key><summary>Scroll broke after upgrade to ES 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ewr</reporter><labels><label>bug</label></labels><created>2015-03-24T21:46:25Z</created><updated>2015-12-05T20:54:51Z</updated><resolved>2015-12-05T20:54:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-25T14:15:43Z" id="86045814">@ewr No changes were made to scroll between 1.4.1 and 1.4.4. Also 1.4.4 was a release that was mainly done to fix the RPM and DEB packages. Maybe it is an issue that has been around for longer.

Are there by any chance nodes of different version in your cluster? The easiest way to see this is by running a cluster stats request: http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html
</comment><comment author="clintongormley" created="2015-03-25T14:17:43Z" id="86046538">@martijnvg I wonder if there is another bug similar to this one: https://github.com/elastic/elasticsearch/pull/9157 when sorting on null values during scroll

@jpountz you looked at the last issue

Also see similar bug report here: https://github.com/elastic/elasticsearch-perl/issues/60
</comment><comment author="ewr" created="2015-03-25T15:19:16Z" id="86079706">@martijnvg: I think you nailed it on the versions. Our data nodes were all 1.4.4, but we had logstash connecting via the node protocol and it was registering as 1.1.1. Just for the fun of it I switched that back to using the http protocol (which it had been until sometime around the time this issue started) and our log script started exporting good numbers again.
</comment><comment author="martijnvg" created="2015-03-25T19:53:52Z" id="86192372">Good to hear that things are working again. So this is just means that there is a back in the bwc scroll logic.
</comment><comment author="clintongormley" created="2015-04-02T12:12:54Z" id="88880342">Switching the Logstash output to use `protocol: transport` works as well.
</comment><comment author="clintongormley" created="2015-04-05T11:53:02Z" id="89756681">@jpountz this was the scrolling issue that I mentioned to you. Any way we can fix this without requiring logstash to upgrade it's version of ES?
</comment><comment author="clintongormley" created="2015-12-05T20:54:51Z" id="162244913">Old issue, no longer relevant. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added "lang" field to documentation of script score definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10243</link><project id="" key="" /><description>Fixes #9659 
</description><key id="64097599">10243</key><summary>Added "lang" field to documentation of script score definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">GlenRSmith</reporter><labels><label>docs</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-24T20:57:26Z</created><updated>2015-03-25T10:00:35Z</updated><resolved>2015-03-25T10:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:00:35Z" id="85966389">Thanks @GlenRSmith merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat indices with pattern does not list closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10242</link><project id="" key="" /><description>I am on ES 1.4.4 and I want to list indices with some prefix

```
curl -XPUT 'localhost:9200/quick'
curl -XPOST 'localhost:9200/quick/_close'
```

Then, there is difference between querying exact index and using prefix with wildcard

```
curl 'localhost:9200/_cat/indices/quick'
close quick

curl 'localhost:9200/_cat/indices/quic*'
  --- nothing ---
```

On open index, the output is identical.
</description><key id="64056135">10242</key><summary>_cat indices with pattern does not list closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinhynar</reporter><labels><label>:CAT API</label><label>enhancement</label></labels><created>2015-03-24T17:40:26Z</created><updated>2016-08-04T08:22:58Z</updated><resolved>2016-08-04T08:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GlenRSmith" created="2015-03-24T19:51:05Z" id="85666374">:+1: 
</comment><comment author="javanna" created="2015-03-25T09:25:37Z" id="85951220">I confirm, referring to an index explicitly makes it show up, while when using wildcards it all depends on the indices options (expand_wildcards etc.) which are hardcoded in the cat api though. The problem is [here](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java#L75), you can never get back closed indices through cat indices unless you refer to them explicitly. @drewr can you comment on this? This is probably the only cat api where showing closed indices might make sense, then we might want to change how we resolve wildcards internally?
</comment><comment author="javanna" created="2015-04-02T11:12:32Z" id="88867125">ping @drew @dakrone what do you think?
</comment><comment author="javanna" created="2016-08-04T08:22:50Z" id="237486153">This was solved by #18545 in version 2.3.4 and 5.0.0-alpha3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable query unless user specifies index and a field&#8207;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10241</link><project id="" key="" /><description>In my org, each team is creating ES index daily so the number of index grows very quickly.  Usually users are interested only in their 'team' data.  Currently, some users execute query using Kibana w/o specifying index and field name(it takes _all').   Such queries at times, bring our cluster to halt/ crash etc.  I would like queries to be restricted such that only well defined queries can be executed on ES.  A well defined query is a query which specifies index and field name.  I could not find a server level config param to enforce such constraint on user queries.  I posted question on ES forum but did not get a clear answer.

This bug/ ER filed to have ES server level parameter such as to enforce user defined queries to have index name and/ or field name specified.
</description><key id="64048114">10241</key><summary>Disable query unless user specifies index and a field&#8207;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abordia</reporter><labels /><created>2015-03-24T17:03:20Z</created><updated>2015-03-25T10:22:23Z</updated><resolved>2015-03-25T10:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:22:23Z" id="85973985">I think this is a duplicate of #6470. You can disable wildcard expansion at the moment, but only per request, which is not what you need. We would need to make this a global setting as described in #6470. Closing as a duplicate, feel free to reopen if I missed something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[API] changed all the links in json specs to the new site</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10240</link><project id="" key="" /><description>I am not sure if the link in README should point to just elastic.co or elastic.co/products/elasticsearch. Not 100% happy with either...
</description><key id="64037772">10240</key><summary>[API] changed all the links in json specs to the new site</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-24T16:10:01Z</created><updated>2015-03-24T20:46:02Z</updated><resolved>2015-03-24T20:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-24T16:12:31Z" id="85585293">LGTM
</comment><comment author="s1monw" created="2015-03-24T20:45:47Z" id="85685332">this has been pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate delete-by-query in client/transport/action APIs too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10239</link><project id="" key="" /><description>In #10082 we deprecated delete-by-query, only in the docs.  This PR also deprecates Java client APIs, and action/transport internal APIs too.
</description><key id="64006830">10239</key><summary>Deprecate delete-by-query in client/transport/action APIs too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Search</label><label>deprecation</label><label>v1.5.1</label><label>v1.6.0</label></labels><created>2015-03-24T14:22:17Z</created><updated>2015-05-29T18:11:14Z</updated><resolved>2015-03-24T16:06:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-24T14:24:39Z" id="85520126">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integration test fails after upgrade to 1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10238</link><project id="" key="" /><description>I have bunch of integration tests which works fine with elasticsearch  version 1.4.4 and  lucene version 4.10.3. When I upgrade to elasticsearch version 1.5.0 and lucene 4.10.4 tests does not work anymore. 
They are failing with "java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler" exception. 

I am running tests on Centos 6.5 with java 1.7.0_75
</description><key id="63987117">10238</key><summary>Integration test fails after upgrade to 1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaroslawZawila</reporter><labels /><created>2015-03-24T12:44:22Z</created><updated>2015-03-25T08:01:24Z</updated><resolved>2015-03-25T07:18:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-24T12:49:16Z" id="85483185">Hi @jaroslawZawila can you please share some more details about your tests? What do they test exactly and how? Anymore info on the error that you get back (e.g. the whole stacktrace)?
</comment><comment author="jaroslawZawila" created="2015-03-25T07:17:59Z" id="85890699">I have got to bottom of my problem. It is caused by PowerMock not by ElasticSearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian init script exits with $?=1 when ES already started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10237</link><project id="" key="" /><description>Running `/etc/init.d/elasticsearch start` when ES is running gives an error (`$?` is 1).

According to the [LSB guidelines](http://refspecs.linuxbase.org/LSB_3.0.0/LSB-generic/LSB-generic/iniscrptact.html), an init script should exit with 0 when starting an already running service:

&gt; In the case of init script commands other than "status" (i.e., "start", "stop", "restart", "try-restart", "reload", and "force-reload"), the init script shall return an exit status of zero if the action described by the argument has been successful. Otherwise, the exit status shall be non-zero, as defined below.  
&gt; In addition to straightforward success, **the following situations are also to be considered successful**:  
&gt; - restarting a service (instead of reloading it) with the "force-reload" argument
&gt; - **running "start" on a service already running**
&gt; - running "stop" on a service already stopped or not running
&gt; - running "restart" on a service already stopped or not running
&gt; - running "try-restart" on a service already stopped or not running
</description><key id="63979664">10237</key><summary>Debian init script exits with $?=1 when ES already started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">andsens</reporter><labels><label>:Packaging</label></labels><created>2015-03-24T12:02:56Z</created><updated>2015-12-05T20:52:42Z</updated><resolved>2015-12-05T20:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-24T12:48:51Z" id="85483120">@andsens thanks for reporting. Yes, we have to make the return codes conform with LSB guidelines. I'm currently working on it ;)
</comment><comment author="clintongormley" created="2015-12-05T20:52:41Z" id="162244669">I believe this has been fixed. Please reopen if that is not the case
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to add documents to existing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10236</link><project id="" key="" /><description>Hi,
Am using Elasticsearch 1.4. My requirement is I will have data every hour and that needs to be uploaded. So the approach that I have taken is to create an index - demo_&lt;date&gt; and upload the data. Now, my question is how to append the subsequent hours data into this index.
Request you to throw light on this.
</description><key id="63973455">10236</key><summary>How to add documents to existing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Anaghasri</reporter><labels /><created>2015-03-24T11:33:27Z</created><updated>2015-03-24T11:36:17Z</updated><resolved>2015-03-24T11:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-24T11:36:17Z" id="85458482">please use the mailing list for questions like this. The issue tracker is only for bugs / features and development.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix nested stored field support.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10235</link><project id="" key="" /><description>This also fixes a NPE when the nested part has been filtered out of the _source, because of _source filtering.

Closes #9766
</description><key id="63957051">10235</key><summary>Fix nested stored field support.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-24T10:07:21Z</created><updated>2015-05-29T16:28:21Z</updated><resolved>2015-03-28T07:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-26T23:25:40Z" id="86752147">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 1.5.0 initialization failure (?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10234</link><project id="" key="" /><description>Java Version:

```
java version "1.7.0_75"
OpenJDK Runtime Environment (IcedTea 2.5.4) (7u75-2.5.4-1~utopic1)
OpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)
```

Logs:

```
[2015-03-24 07:16:06,813][INFO ][node                     ] [elasticsearch-node] version[1.5.0], pid[7739], build[5448160/2015-03-23T14:30:58Z]
[2015-03-24 07:16:06,814][INFO ][node                     ] [elasticsearch-node] initializing ...
[2015-03-24 07:16:07,175][INFO ][plugins                  ] [elasticsearch-node] loaded [cloud-aws], sites []
[2015-03-24 07:16:10,046][DEBUG][discovery.zen.elect      ] [elasticsearch-node] using minimum_master_nodes [-1]
[2015-03-24 07:16:10,049][DEBUG][discovery.zen.ping.multicast] [elasticsearch-node] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2015-03-24 07:16:10,054][DEBUG][discovery.zen.ping.unicast] [elasticsearch-node] using initial hosts [], with concurrent_connects [10]
[2015-03-24 07:16:10,629][DEBUG][gateway.local            ] [elasticsearch-node] using initial_shards [quorum], list_timeout [30s]
[2015-03-24 07:16:11,308][DEBUG][gateway.local.state.shards] [elasticsearch-node] took 0s to load started shards state
[2015-03-24 07:16:11,391][ERROR][bootstrap                ] Exception
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoSuchMethodError: org.elasticsearch.discovery.zen.ZenDiscovery.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/cluster/ClusterName;Lorg/elas
ticsearch/threadpool/ThreadPool;Lorg/elasticsearch/transport/TransportService;Lorg/elasticsearch/cluster/ClusterService;Lorg/elasticsearch/node/settings/NodeSettingsService;Lorg/elasticsearch/cluster/node/
DiscoveryNodeService;Lorg/elasticsearch/discovery/zen/ping/ZenPingService;Lorg/elasticsearch/discovery/zen/elect/ElectMasterService;Lorg/elasticsearch/discovery/DiscoverySettings;)V
  at org.elasticsearch.discovery.ec2.Ec2Discovery.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.discovery.ec2.Ec2Discovery
  while locating org.elasticsearch.discovery.Discovery
    for parameter 3 at org.elasticsearch.node.service.NodeService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.node.service.NodeService
Caused by: java.lang.NoSuchMethodError: org.elasticsearch.discovery.zen.ZenDiscovery.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/cluster/ClusterName;Lorg/elasticsearch/threadpool/
ThreadPool;Lorg/elasticsearch/transport/TransportService;Lorg/elasticsearch/cluster/ClusterService;Lorg/elasticsearch/node/settings/NodeSettingsService;Lorg/elasticsearch/cluster/node/DiscoveryNodeService;
Lorg/elasticsearch/discovery/zen/ping/ZenPingService;Lorg/elasticsearch/discovery/zen/elect/ElectMasterService;Lorg/elasticsearch/discovery/DiscoverySettings;)V
        at org.elasticsearch.discovery.ec2.Ec2Discovery.&lt;init&gt;(Ec2Discovery.java:51)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:203)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:213)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)

```
</description><key id="63952578">10234</key><summary>elasticsearch 1.5.0 initialization failure (?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">noamgal</reporter><labels /><created>2015-03-24T09:46:50Z</created><updated>2015-03-25T10:01:15Z</updated><resolved>2015-03-24T10:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-24T10:08:21Z" id="85432807">Thanks for reporting this. AWS plugin 2.5.0 needs to be released. See also https://github.com/elastic/elasticsearch-cloud-aws/issues/195
</comment><comment author="dadoonet" created="2015-03-25T10:01:15Z" id="85966503">Hi

We just released version 2.5.0 for elasticsearch 1.5.0.

Let us know if you have any issue with it by opening new issues in cloud-AWS repo.
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search results differ between [All Types] and Search Entire Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10233</link><project id="" key="" /><description>I have noticed that my search results differ between 

$ curl -XGET 'http://127.0.0.1:9200/myindex/_search?pretty' -d '{ "query": { "term": {  "datacentre.id": "f96a1379-75bd-40f8-8a82-59931ac3819d" }} }'
$ curl -XGET 'http://127.0.0.1:9200/myindex/rs/_search?pretty' -d '{ "query": { "term": {  "datacentre.id": "f96a1379-75bd-40f8-8a82-59931ac3819d" }} }'

The former returns no hits.  Only the latter returns the following result:

{  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "myindex",
      "_type" : "rs",
      "_id" : "90fb8dce-6b8a-4b0d-836c-d77293a3dbaa",
      "_score" : 0.30685282,
      "_source":{"label":"My test server","id":"90fb8dce-6b8a-4b0d-836c-d77293a3dbaa","type":"Linux","hostname":"My Linux Server","description":"Test","hosts":[{"id":"c3cde0b6-d488-420c-b32f-e4f1c7c91366","label":"A Sample Web Service"}],"datacentre":[{"id":"f96a1379-75bd-40f8-8a82-59931ac3819d","label":""}]}
    } ]
  }
}

I am working around the problem in elasticsearch.js by searching all my types with:
 client.search({ type: ['rs','ws','x','y'] ... });
</description><key id="63897757">10233</key><summary>Search results differ between [All Types] and Search Entire Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">PhaedrusTheGreek</reporter><labels /><created>2015-03-24T03:56:11Z</created><updated>2015-03-30T09:40:15Z</updated><resolved>2015-03-30T09:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-24T09:05:35Z" id="85410917">HI @PhaedrusTheGreek I cant' quite reproduce this. Which version are you on? Could you send a complete curl recreation that shows the problem please?
</comment><comment author="clintongormley" created="2015-03-29T12:06:33Z" id="87403746">Hi @PhaedrusTheGreek 

Do you have a type called `datacentre`? Or do you have more than one type with a `datacentre` field, possibly with different mappings?  This is almost definitely to do with ambiguous field lookups, and should be fixed with the changes in #8870
</comment><comment author="PhaedrusTheGreek" created="2015-03-29T13:14:53Z" id="87411446">Yes, that is the case.  I have more than one type with 'datacentre' field.
</comment><comment author="javanna" created="2015-03-30T09:40:15Z" id="87615633">I think we can close, we are aware of the problem and fixing it with #8870.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>using query string query to check if field exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10232</link><project id="" key="" /><description>```
"query":{"query_string" : {
    "default_field" : "body",
    "query" : "UserID:*"
    }
}
```

I thought query above will return items which includes UserID field, but I got an unexpected result that it does not return logs with UserID=-.

I can do this by exists filter, but I think query string query above should return what I expected.

I'm trying to fix this locally and want to ask your opinion about this.

mapping on this field is string:analyzed.
</description><key id="63891952">10232</key><summary>using query string query to check if field exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2015-03-24T03:11:25Z</created><updated>2015-03-24T11:26:26Z</updated><resolved>2015-03-24T11:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-24T11:26:26Z" id="85455977">Hi @sweetest,
I think this type of question could be asked in the mailing list instead, unless you found a bug or you have a feature request.

This does not seem like a bug to me, what you describe is the expected behaviour of the `query_string` query. I believe your field ended up empty after text analysis, that would be why you don't get that specific document back by doing `UserID:*`. Have a look at how fields get analyzed using the analyze api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove delete mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10231</link><project id="" key="" /><description>Deleting a type from an index is inherently dangerous because
the type can be recreated with new mappings which may conflict
with existing segments still using the old mappings. This
removes the ability to delete a type (similar to how deleting
fields within a type is not allowed, for the same reason).

closes #8877
</description><key id="63869285">10231</key><summary>Remove delete mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-03-24T00:56:59Z</created><updated>2015-06-06T15:49:15Z</updated><resolved>2015-03-24T16:47:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-24T06:15:47Z" id="85357669">Can you add an entry to the migrate_2_0.asciidoc about the fact that this api has been deleted? Other than that LGTM.
</comment><comment author="rjernst" created="2015-03-24T06:23:05Z" id="85361103">@martijnvg Sure, added a short message to the docs.
</comment><comment author="martijnvg" created="2015-03-24T06:28:53Z" id="85362007">LGTM
</comment><comment author="jpountz" created="2015-03-24T07:54:43Z" id="85385965">LGTM
</comment><comment author="javanna" created="2015-03-24T08:21:52Z" id="85392797">ping @spalger when this gets pushed you'll have to regenerate the api for the js client otherwise the references to the non existing docs page might break docs publish process. Thanks!
</comment><comment author="s1monw" created="2015-03-24T16:13:46Z" id="85585804">LGTM too
</comment><comment author="aparo" created="2015-05-11T10:05:45Z" id="100842974">How to manage if an existing mapping is wrong?
Usually people deletes it and recreate it.
He can force deleting the documents with a scan first. Optimize the index removing deleted docs and then remove the mapping.
Otherwise people must recreate/reindex indices every time they are a broken mapping!
How we manage generic optimization of deleting empty mappings in indices?
</comment><comment author="clintongormley" created="2015-05-15T13:00:48Z" id="102391778">@aparo There are a number of reasons why reindexing is a better approach:
- Even after an optimize, old fields can still exist in the fields info which can lead to corrupt data.
- Fields are going to be stored at index level (with some per-type settings, eg `copy_to`), so you'd have to be sure that fields aren't being used across multiple types.
- Doing an optimize could involve rewriting all of your segments anyway, and stopping indexing while you do it.
- We plan on supporting reindexing directly in Elasticsearch, which will make it easier to just rewrite the index then switch aliases when it is ready.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regular expression query string within nested query doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10230</link><project id="" key="" /><description>Query-string query within a nested query doesn't seem to work. 

Version tried: 1.5.0

For e.g: Query as below to find authors of all comments can not be searched with a regular expression query string. However regexp query seems to be working fine. 

```
{
   "query": {
      "nested": {
         "path": "comments",
         "query": {
             "query_string": {
                "query": "author:/.*virus/"
             }
         }
      }
   }
}
```
</description><key id="63851904">10230</key><summary>Regular expression query string within nested query doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">photonicworld</reporter><labels><label>feedback_needed</label></labels><created>2015-03-23T22:48:13Z</created><updated>2015-04-05T11:39:05Z</updated><resolved>2015-04-05T11:39:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:24:44Z" id="85974444">Hi @photonicworld could you post a complete recreation of your problem please, including documents and the response that you get back from your query?
</comment><comment author="clintongormley" created="2015-04-05T11:39:05Z" id="89756032">Hi @photonicworld 

It does work, but you need to specify the full path to the nested field, ie `comments.author:`:

```
DELETE t

PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "comments": {
          "type": "nested",
          "properties": {
            "author": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "title": "My post",
  "comments": [
    {
      "author": "foo bar"
    }
  ]
}

GET /t/_search
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "query_string": {
          "query": "comments.author:/f.*/"
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search preference based on node attributes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10229</link><project id="" key="" /><description>Say I have data nodes in two availability zones. Each node has an attribute with its zone name, and shard allocation awareness is enabled based on that zone name. Every shard has a copy in both zones.

Now I want to minimize search traffic across zones to reduce latency. I'd like to set a search preference that prefers shards in the same zone as the coordinator, since communicating with those shards will be faster.

This would probably look something like `preference=_prefer_availability_zone:_local`. In this case, specifying a zone isn't helpful, because I want the value to be the coordinator's value.

This is a more general approach to https://github.com/elastic/elasticsearch/issues/5925.
</description><key id="63845719">10229</key><summary>Search preference based on node attributes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2015-03-23T22:08:45Z</created><updated>2015-03-23T22:51:14Z</updated><resolved>2015-03-23T22:51:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-23T22:46:08Z" id="85236322">Hi @grantr, if I'm not mistaken this is exactly what shard allocation awareness already does, beyond making sure that shards are properly distributed across the different zones. Have a look at the docs [here](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#_automatic_preference_when_searching_geting).
</comment><comment author="grantr" created="2015-03-23T22:51:14Z" id="85237208">@javanna thanks! Still learning new things after 5 years :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>get tests working on java 9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10228</link><project id="" key="" /><description>Fix build for java9.
- add compiler workarounds for java 9 (compiler bug, JI-9019884 filed for this)
- remove permgen specification during tests (this results in an error)

Fixes #10145
</description><key id="63839907">10228</key><summary>get tests working on java 9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T21:45:43Z</created><updated>2015-08-07T10:15:14Z</updated><resolved>2015-03-24T05:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-23T21:47:33Z" id="85211367">LGTM I guess we need to drop the permgen space from the init scripts as well as some point since java 9 doesn't like it
</comment><comment author="rmuir" created="2015-03-23T21:53:46Z" id="85217676">When running tests on java9 i hit IAE from java.util.concurrent.ThreadPoolExecutor.setCorePoolSize().

Maybe this should be fixed as a separate issue, but I have not seen it before.

Suite: org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests
  1&gt; [2015-03-23 17:52:16,761][INFO ][test                     ] Test testShutdownDownNowDoesntBlock(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) started
  1&gt; [2015-03-23 17:52:16,764][INFO ][test                     ] Test testShutdownDownNowDoesntBlock(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) finished
  1&gt; [2015-03-23 17:52:16,765][INFO ][test                     ] Test testScalingExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) started
  1&gt; [2015-03-23 17:52:16,767][INFO ][test                     ] Test testScalingExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) finished
  1&gt; [2015-03-23 17:52:16,767][INFO ][test                     ] Test testFixedExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) started
  1&gt; [2015-03-23 17:52:16,769][INFO ][test                     ] Test testFixedExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) finished
  1&gt; [2015-03-23 17:52:16,769][INFO ][test                     ] Test testCachedExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) started
  1&gt; [2015-03-23 17:52:16,772][INFO ][test                     ] Test testCachedExecutorType(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) finished
  1&gt; [2015-03-23 17:52:16,772][INFO ][test                     ] Test testCustomThreadPool(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests) started
  1&gt; [2015-03-23 17:52:16,777][ERROR][test                     ] FAILURE  : testCustomThreadPool(org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests)
  1&gt; REPRODUCE WITH  : mvn clean test -Dtests.seed=216A1B2C2D2B1E3C -Dtests.class=org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests -Dtests.method="testCustomThreadPool" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.locale=ru_RU -Dtests.timezone=America/Rankin_Inlet -Dtests.processors=8
  1&gt; Throwable:
  1&gt; java.lang.IllegalArgumentException
  1&gt;     __randomizedtesting.SeedInfo.seed([216A1B2C2D2B1E3C:82BF82836AB18119]:0)
  1&gt;     java.util.concurrent.ThreadPoolExecutor.setCorePoolSize(ThreadPoolExecutor.java:1541)
  1&gt;     org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:345)
  1&gt;     org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:441)
</comment><comment author="rmuir" created="2015-03-23T22:06:07Z" id="85224433">In java8, javadocs for this method look like:

Throws:
    IllegalArgumentException - if corePoolSize &lt; 0

But in java9 its changed, we are hitting the second condition:

```
 * @throws IllegalArgumentException if {@code corePoolSize &lt; 0}
 *         or {@code corePoolSize} is greater than the {@linkplain
 *         #getMaximumPoolSize() maximum pool size}
```
</comment><comment author="rmuir" created="2015-03-23T23:21:52Z" id="85244581">I have no idea how to fix the code to dynamically increase threadpool sizes. 

With the new check, it seems impossible: setCorePoolSize complains if its &gt; maximumPoolSize, but setMaximumPoolSize complains if its &lt; corePoolSize. This check was added here: https://bugs.openjdk.java.net/browse/JDK-7153400

I feel like i'm losing my mind, maybe someone else can take a second look here?
</comment><comment author="rmuir" created="2015-03-23T23:30:32Z" id="85245796">I guess, depending if we are growing or shrinking the pool, we have to do the code either:

```
setMaximumPoolSize(x);
setCorePoolSize(y);
```

or

```
setCorePoolSize(y);
setMaximumPoolSize(x);
```

This seems pretty crazy to me, i can't imagine its intended. Its just like ConcurrentMergeScheduler in lucene used to be.
</comment><comment author="rmuir" created="2015-03-23T23:53:12Z" id="85250675">I fixed threadpool sizing with https://github.com/rmuir/elasticsearch/commit/88c3934af9f1b9b6c91de2092d5096500376efe9

Please have a look, I think its ready. All tests pass on java9 now:

[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 09:02 min
[INFO] Finished at: 2015-03-23T19:53:03-05:00
[INFO] Final Memory: 38M/1404M
[INFO] ------------------------------------------------------------------------
</comment><comment author="rjernst" created="2015-03-24T00:06:04Z" id="85254780">LGTM
</comment><comment author="rmuir" created="2015-03-24T00:12:08Z" id="85256038">&gt; LGTM I guess we need to drop the permgen space from the init scripts as well as some point since java 9 doesn't like it

I don't think we are setting this anymore. We were only setting it for tests. I grepped for MaxPermSize and don't see it anywhere else, except dependency-reduced-pom.xml, which is under .gitignore?!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] add "ignore" option to geo_shape orientation param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10227</link><project id="" key="" /><description>Spawned from #9511 discussion, there is an interest in providing an option for "orientation" indifference on `geo_shape` types.  That is, the "old" way of computing coordinate orientation for polygons (which is not compliant with OGC standards and will fail on ambiguous polygon).  Requested by the community, this option would be in the form of: `orientation: ignore` to provide backwards compatibility with projects that handle their own coordinate orientation logic (or already provide shapes in a manner than skirts orientation issues prior to ES 1.4.3).

Note that if a user opts to ignore orientation and an ambiguous polygon is provided, InvalidShapeExceptions will occur - this is the price the user is willing to pay.  
</description><key id="63833972">10227</key><summary>[GEO] add "ignore" option to geo_shape orientation param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label><label>feature</label></labels><created>2015-03-23T21:13:32Z</created><updated>2015-12-06T16:31:07Z</updated><resolved>2015-12-06T16:31:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-03-23T21:17:19Z" id="85201422">/cc @colings86 @clintongormley @rjernst @tschaub

Note: In addition to permitting "invalid" polygons, this option (aside from being 'opt in') goes against the "prevent users from 'hurting' themselves" philosophy.  Its low hanging fruit but I open this issue for quick feedback before bringing it in.
</comment><comment author="rjernst" created="2015-03-23T21:30:41Z" id="85206070">Is there a real need for this, or is it laziness? Are there any performance benefits to the "old" way if you don't have holes? Is it just saving the user from having to sort their points and supply them in a deterministic order when the poly is "simple"?
</comment><comment author="tschaub" created="2015-03-23T23:47:04Z" id="85248836">Thanks for your willingness to work on this @nknize.  @rjernst - you're right to assume this can mostly be explained away as laziness - see https://trac.osgeo.org/gdal/ticket/2310.  We're serializing geometries as GeoJSON without first checking or enforcing any winding order.  This was working with Elasticsearch &lt; 1.4.3.  Now it doesn't work for polygons that have interior rings and also span &gt; 180 degrees longitude.  I'll confirm this now, but it struck me initially that the heuristic about dateline crossing was only being applied when determining if an interior ring was within an exterior ring, and _not_ when performing an intersects query.  I'll do some more testing to determine where it looks like the right hand rule is being assumed.

I think what we'll do for now is update things to always create geometries following the right-hand rule.  And then we'll decide what to do with geometries we accept from clients (before sending to Elasticsearch).

So, thanks again for considering the `orientation: ignore` property.  I'll update the clients that we control and the other places we construct geometries to always use the right-hand rule.  We've added text to the proposed GeoJSON IETF draft encouraging people to do the same (see geojson/draft-geojson#43), so maybe things will get better over time.
</comment><comment author="clintongormley" created="2015-12-05T20:52:00Z" id="162244634">@nknize Is this still relevant or can it be closed?
</comment><comment author="nknize" created="2015-12-06T16:31:07Z" id="162327276">Closing this issue. As discussed in #12325 we need to require OGC/ISO compliance. There are already separate tools that fix bad (non-compliant) data and adding this option/leniency just invites trouble in the long run.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add node id to _cat/shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10226</link><project id="" key="" /><description>would be nice having node id in here. Even though having nodes with the same name and ip is not a likely scenario, it's still possible.

if this sounds reasonable, I would be more than happy to send a PR.
</description><key id="63830066">10226</key><summary>add node id to _cat/shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:CAT API</label><label>enhancement</label></labels><created>2015-03-23T20:54:12Z</created><updated>2015-03-25T11:23:58Z</updated><resolved>2015-03-25T11:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-24T09:11:23Z" id="85412856">Makes sense to me, what do you think @drewr ?
</comment><comment author="drewr" created="2015-03-24T14:53:02Z" id="85539223">Agree, we should have node name and id on all APIs.
</comment><comment author="lmenezes" created="2015-03-24T14:56:42Z" id="85540263">Will you take care of it, or should I send a PR?
</comment><comment author="drewr" created="2015-03-24T14:59:53Z" id="85541376">@lmenezes Happy to have a PR!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.5.0 won't start on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10225</link><project id="" key="" /><description>I downloaded ES 1.5 today, unzipped into a new folder and ran the batch file. This is result:

C:\Users\XXXXX\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0\bin&gt;elasticsearch.bat
Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version
        at org.elasticsearch.bootstrap.Bootstrap.buildErrorMessage(Bootstrap.java:261)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:248)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)

C:\Users\XXXXXX\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0\bin&gt;java -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)

Am I missing something?
</description><key id="63807664">10225</key><summary>ES 1.5.0 won't start on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wbsimms</reporter><labels /><created>2015-03-23T19:11:29Z</created><updated>2015-03-23T19:42:39Z</updated><resolved>2015-03-23T19:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2015-03-23T19:21:55Z" id="85156095">The 1.5 release works for me on windows. If you place an `echo` in front of https://github.com/elastic/elasticsearch/blob/master/bin/elasticsearch.bat#L44 what does `bin\elasticsearch` print? It might be that your `%JAVA_HOME%` or `%ES_HOME%` is set to something that interferes with the classpaths.
</comment><comment author="wbsimms" created="2015-03-23T19:40:02Z" id="85162696">C:\Users\Wm.Barrett\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0\bin&gt;elasticsearch.bat
"C:\Program Files\Java\jre1.8.0_31\bin\java"  -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8  -Delasticsearch -Des-foreground=yes -Des.path.home="C:\Users\Wm.Barrett\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0"  -cp ";C:\Projects\elasticsearch/lib/elasticsearch-1.5.0.jar;C:\Projects\elasticsearch/lib/_;C:\Projects\elasticsearch/lib/sigar/_;C:\Users\Wm.Barrett\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0/lib/elasticsearch-1.5.0.jar;C:\Users\Wm.Barrett\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0/lib/_;C:\Users\Wm.Barrett\Downloads\elasticsearch-1.5.0\elasticsearch-1.5.0/lib/sigar/_" "org.elasticsearch.bootstrap.Elasticsearch"

I see what's going on. A failed process had setup environment variables for ES_*.

The easy work around was to start a new shell.
A sanity check would be to remove any existing ES_\* environment vars if present. However, I would just chalk this up to a users learning curve.

Thanks!
</comment><comment author="wbsimms" created="2015-03-23T19:42:39Z" id="85164142">Closed. Not a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Excessive fixed_bit_set_memory_in_bytes causing GC thrashing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10224</link><project id="" key="" /><description>We're running ES 1.4.4 on a 4 node c3.2xlarge EC2 cluster, configured for 8GB heaps. Sometimes nodes start GC thrashing b/c it's not freeing up enough memory. For example:

```
[2015-03-23 17:25:53,342][INFO ][monitor.jvm              ] [Ocelot] [gc][old][212973][1077] duration [6.1s], collections [1]/[6.4s], total [6.1s]/[1.8h], memory [7.2gb]-&gt;[7.2gb]/[7.3gb], all_pools {[young] [38mb]-&gt;[188mb]/[0b]}{[survivor] [0b]-&gt;[0b]/[0b]}{[old] [7.1gb]-&gt;[7gb]/[7.3gb]}
[2015-03-23 17:25:59,627][INFO ][monitor.jvm              ] [Ocelot] [gc][old][212974][1078] duration [6.1s], collections [1]/[6.2s], total [6.1s]/[1.8h], memory [7.2gb]-&gt;[7.1gb]/[7.3gb], all_pools {[young] [188mb]-&gt;[70mb]/[0b]}{[survivor] [0b]-&gt;[0b]/[0b]}{[old] [7gb]-&gt;[7gb]/[7.3gb]}
```

It's all old gen. We switched from concurrent mark sweep to G1, but that didn't help. Looking at node stats while this is going on

curl http://localhost:9200/_nodes/Ocelot/stats

```
"segments": {
          "count": 6015,
          "memory_in_bytes": 909820630,  =  0.84734 GB
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory_in_bytes": 1504223232,  = 1.40092 GB
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set_memory_in_bytes": 2779968664    = 2.58905 GB
        },
```

I noticed that fixed_bit_set_memory_in_bytes is taking up quite a large chunk of our 8GB heap and that by design it never evicts
https://github.com/elastic/elasticsearch/pull/7037

We use nested documents heavily. I was wondering if there's a way to control this memory usage, if this is a bug, or if we simply need more memory/nodes to cover this memory overhead. 
</description><key id="63786012">10224</key><summary>Excessive fixed_bit_set_memory_in_bytes causing GC thrashing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">drewdahlke</reporter><labels><label>:Nested Docs</label></labels><created>2015-03-23T17:38:04Z</created><updated>2015-08-13T14:02:56Z</updated><resolved>2015-06-15T06:36:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-24T06:49:44Z" id="85364970">@drewdahlke The nested feature heavily leans on in-memory bitsets. Essentially each parent nested field needs an in-memory bitset. In the case all the nested fields are in the root level, only one bitset needs to be in memory, but once you have multiple levels of nested fields more bitsets need to be loaded in memory and this can become expensive. In your case do you have multiple levels of nested fields?

Are you using nested sorting? Unfortunately at the moment this is quite memory in-efficient. It has been fixed in 2.0: #9199

In the case that you don't use all of your nested fields you may want to disable eager loading by setting `index.load_fixed_bitset_filters_eagerly` to false in your elasticsearch.yml file.

If you have many documents marked as deleted it may make sense to prune those documents by running an optimize. (this can also be found in node stats under docs/deleted) This can reduce the space these in-memory bitsets need.

Other than that you can always increase the maximum heap space a node is allowed to use or add more nodes.
</comment><comment author="drewdahlke" created="2015-03-24T14:03:14Z" id="85507977">Hey, thanks for getting back to me. Our doc structure only nests one layer deep right now, but we are sorting on a nested date field. It's good to know there's an improvement for that in a future release. I've got include_in_parent set to false on the nested docs. Our docs get updated typically 8-10 times but are never deleted.

We're on 1.4.4 and it looks like in 1.4.1 the eager loading of bitsets was disabled by default so we should be good there. However, I have a question about that. Is it holding onto 1 bitset per field or one bitset per field per term filter value ever used while querying with _cache=true? 

Also is there a way to see a breakdown on field name to bit set overhead? It would be handy to see which fields have the most overhead. 
</comment><comment author="drewdahlke" created="2015-03-24T14:45:15Z" id="85536122">Scratch what I said about 1.4.4 defaulting to false. I must have missread https://github.com/elastic/elasticsearch/issues/8394 because setting index.load_fixed_bitset_filters_eagerly=false seems to have done the trick. fixed_bit_set_memory_in_bytes shrunk 2 orders of magnitude from that. Sweet!
</comment><comment author="martijnvg" created="2015-03-24T14:45:46Z" id="85536536">&gt; However, I have a question about that. Is it holding onto 1 bitset per field or one bitset per field per term filter value ever used while querying with _cache=true?

The bitset cache caches on a per nested field basis.

&gt; Also is there a way to see a breakdown on field name to bit set overhead?

No, at the moment that isn't possible

If a document get deleted then under the hood the Lucene document(s) get marked as deleted and are re-added. So perhaps it does make sense to prune deleted docs at some point.

In 1.4.4 only the bitset for parent level nested field are eagerly loaded, in your case that is only one bitset, so that should be fine. The nested sorting also causes the nested fields referenced in the nested sorting to be cached, so that adds up.
</comment><comment author="martijnvg" created="2015-03-24T14:53:50Z" id="85539491">One note about the disabling the eager loading is that unfortunately will not prevent jvm heap issues. If a search request gets executed that does need the bitsets, it will load it and you may run into the same issues. 
</comment><comment author="drewdahlke" created="2015-03-24T18:27:43Z" id="85632441">Makes sense. Any chance we could make this a feature request to persist those bitsets on disk and LRU cache them in memory rotated by a configurable max memory cap? As is with time series data (we have 1 index per day of data) old data, less frequently queried, has the same bitset memory cost as the hot recent data and it'd be nice to get some control over that. 
</comment><comment author="holm" created="2015-03-29T14:04:07Z" id="87418541">We have been trying to upgrade to 1.4 from 1.2, but are currently unable to because the we run out of heap, even with more than 50% additional heap being made available. Based on testing it seems this is the issue that is causing the increased memory usage. 

We use nested documents for sorting. Is there any chance the fix in 2.0 will be backported or are there any other ways to get around this issue?
</comment><comment author="martijnvg" created="2015-03-31T11:38:42Z" id="88054900">@holm sorry to hear that nested sorting is preventing you from upgrading... We decided not to backport the nested sorting fix into 1.x, because of a change in Lucene 5 (2.0 will use this Lucene version) on which the fix relies on. Basically documents are guaranteed to always score in order in Lucene 5, in Lucene 4 this isn't always to case and the nested sorting fix relies on that. 

@drewdahlke The reason that these bitsets are on the jvm heap space is because the nested query, nested sorting etc. require random access to it.
</comment><comment author="jpountz" created="2015-03-31T12:44:22Z" id="88070473">I don't think we should try to backport this change, this would be dangerous. I would rather recommend increasing the heap size or adding more nodes to the cluster until we have a better solution for this issue (ie. 2.0).

But even on 2.0, nested documents will still be a heavy feature since we will still require a full bit set for each nested field. In general, `nested` should not be considered an improved version of `object` but rather a useful feature that comes with a non-negligible cost. This needs to be taken into account when designing mappings. Also, `nested` docs don't only increase memory usage, they also make the index sparser, which in-turn hurts compression of the index on disk and increase memory usage of all data-structures that key by doc ID (such as fielddata).

@holm As a side note, I see that you have 6015 segments on a single node, which is quite large for a single node. Do you know how many shards this maps to (I would guess around 120 since it is common for shards to have about 50 segments)? Maybe you could save memory by having fewer shards, eg. by allocating fewer shards per index, merging read-only indices to a single segment per shard, or having eg. weekly indices instead of daily if you have time-based data.
</comment><comment author="drewdahlke" created="2015-03-31T14:15:49Z" id="88103623">Keeping the bitsets on heap makes total sense. It's my impression from https://github.com/elastic/elasticsearch/pull/7037 that bitsets moved from being in field cache with an LRU eviction policy to something separate &amp; non-evicting in 1.4.0. 

Breaking the bitsets out from field cache is a huge improvement b/c bitsets &amp; field data are different enough that giving them different eviction policies makes a lot of sense. However, omitting the eviction policy all-together on bitsets seems atypical for the project. I'd like to see a cap similar to indices.fielddata.cache.size but for those bitsets. For us a slow query here and there trumps OOMs or the expense of growing the cluster to compensate. 

Sans you all adding a cap we're looking at either having to fork ES 1.4.4 to add a cap or downgrading to 1.3.x to keep our ec2 costs down. 

@jpountz Merging old shards down to a single segment is a great idea; putting it on my list. The bitsets have my attention right now b/c upgrading from 1.1.x to 1.4.x with no other changes immediately started costing us more money to host b/c we have to have so much more heap available. 
</comment><comment author="cdmicacc" created="2015-03-31T20:54:57Z" id="88241482">We are in the same situation -- the memory used by the bitsets grows until the entire heap is consumed (we have 12 nodes, currently, each with a 32GB heap).  

By chance, we discovered that issuing `POST https://localhost:9200/index/_cache/clear` will clear the bitset cache (but there is no corresponding explicit parameter to clear it!).  

However, without some form of cache expiry (LRU or otherwise), we're either going to have to periodically flush all the caches across the cluster, or downgrade back to 1.3.
</comment><comment author="martijnvg" created="2015-04-07T12:33:52Z" id="90533684">One note to @jpountz's note about memory usage for nested docs: Nested documents requires a full bit set for each nested field that is a parent of another nested field. This applies for both 2.0 and 1.x (the nested sorting is just an exception here).
</comment><comment author="holm" created="2015-04-07T12:46:07Z" id="90536461">We use the nested document feature to associate a personal "score" for each user to a document. We then use the score for the active user when sorting the results. For most documents only very few users will have a score, so the length of the nested documents are quite short. 

I think having an eviction policy for the bitsets sounds like a good idea. Barring that, I would love to know if there are other ways to solve the scoring, outside a nested sorting. I guess a function score could work also, although less direct.
</comment><comment author="schlosna" created="2015-06-10T16:25:33Z" id="110821899">I am experiencing bloat of the fixed bit set caches which has led to OutOfMemoryErrors on our data nodes and ES cluster instability. We make heavy use of nested documents to provide access controls for parent documents, and our queries include filters for these nested documents.

Similar to @drewdahlke, we may have to fork ES if we cannot bound the `FixedBitSetFilterCache` so I am beginning to look into implementing size and/or time bounding for the `FixedBitSetFilterCache`, and wanted to gauge community interest in a possible PR and get feedback on what might be an acceptable implementation. I am planning on an implementation similar to `IndicesFieldDataCache`. I am also interested in thoughts around if the `FixedBitSetFilterCache` should participate in the existing circuit breakers (i.e. indices.breaker.total.limit per
https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#circuit-breaker), and possibly add `indices.breaker.filter.limit` and `indices.breaker.filter.overhead`.
</comment><comment author="jepettoruti" created="2015-06-14T19:11:22Z" id="111865048">Is there any official word of when this will be fixed? Is it for ES 2.x or would it come in some previous release?
@schlosna I don't think we have the resources to help with the development of a fork, but happy to test it.
</comment><comment author="jpountz" created="2015-06-15T06:36:31Z" id="111949974">Making this cache bounded is a bit trappy because then even very simple queries that match few documents might need to scan the entire index to load these bit sets that we use for nested queries/sorting.

Like @martijnvg said, this situation will improve a lot in 2.0 thanks to #9199. However I don't think we can do anything in 1.x.
</comment><comment author="drewdahlke" created="2015-06-15T12:31:05Z" id="112046083">@jpountz If I recall doc values were added b/c un-inverting the index into field data cache on demand was expensive. It sounds like building the bitsets on demand are in the same boat, so wouldn't it make sense to persist them to disk as well? Putting a bounded cache on something disk backed would make it less trappy and allow people to take the performance hit on indexing/merging rather than heap usage at query time. 

I could see defaulting it to unbounded (prioritizing query performance), but providing a bounded option for people who need more control over heap usage. 

@schlosna If you end up forking before an official fix comes out, I could spare some cycles for code review/testing. 
</comment><comment author="jpountz" created="2015-06-15T15:05:21Z" id="112099826">@drewdahlke Totally agreed, we shouldn't have to load all this information about the nested structure in memory. It should reside on disk and we should read from there directly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>debian repository for 1.4 is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10223</link><project id="" key="" /><description>It seems the repository is broken - it complains like the following when I do an apt-upgrade after inserting the repo like this:

```
wget -qO - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
add-apt-repository "deb http://packages.elasticsearch.org/elasticsearch/1.4/debian stable main"
```

The error looks like this

```
W: Failed to fetch http://packages.elasticsearch.org/elasticsearch/1.4/debian/dists/stable/Release  Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
```
</description><key id="63783054">10223</key><summary>debian repository for 1.4 is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gardsted</reporter><labels /><created>2015-03-23T17:27:37Z</created><updated>2015-04-05T09:44:18Z</updated><resolved>2015-04-05T09:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2015-03-23T17:33:15Z" id="85116052">Hi, using `add-apt-repository` automatically adds a deb-src line for source packages.
But since we don't have those we also don't have the source data.
If you remove that line you can run `apt-get update` without any errors.
</comment><comment author="clintongormley" created="2015-04-05T09:44:13Z" id="89745495">I've added a warning to the docs explaining this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java client instance doesn't recover from temporary ES unavailability </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10222</link><project id="" key="" /><description>Hey,

I'm using ES 1.4.2 and it seems to me that if java client cannot temporarily ping node within `client.transport.ping_timeout` then it never manages to ping it again and [this happens](https://gist.github.com/l15k4/f40a115edb67c1fd4779)... Let say that I do a query from a different client (like Sense) that returns big result set, say `search?size=200000` that takes more than `client.transport.ping_timeout` then java client never recovers and a java client request that occurred within the unavailability never gets a response back... 

So my solution to that was increasing `client.transport.ping_timeout`, but the problem with this now is that the transport client never returns response on that original request that occurred within the temporary unavailability...

This means a lot of "resiliency" coding on the java-client side ....

Btw I tried that when `sniffing` was both enabled and disabled.
</description><key id="63768625">10222</key><summary>Java client instance doesn't recover from temporary ES unavailability </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">l15k4</reporter><labels><label>discuss</label></labels><created>2015-03-23T16:35:03Z</created><updated>2015-12-05T20:51:32Z</updated><resolved>2015-12-05T20:51:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T09:20:44Z" id="89739042">Hi @l15k4 

I've labelled this for discussion but as a side note, you should never be asking for 200,000 results in one request.  Instead you should use scrolling to just keep pulling batches of results until you have the requisite number.
</comment><comment author="clintongormley" created="2015-12-05T20:51:32Z" id="162244608">So much has changed since this issue was opened that I'm going to close.  Please reopen if you're still seeing issues with recent versions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sampler aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10221</link><project id="" key="" /><description>Used to limit any nested aggregations' processing to a sample of the top-scoring documents.

Optionally, a &#8220;diversify&#8221; setting can limit the number of collected matches that share a common value such as an "author".

The original "DeferringBucketCollector" is now abstracted with the bulk of the original code in new subclass BestBucketsDeferringCollector and the new alternative policy for deferring is implemented in the BestDocsDeferringCollector subclass.

The diversifying logic is reliant on Lucene 5.1 which has changes to support this specialized form of result collection.

Closes #8108
</description><key id="63751365">10221</key><summary>Sampler aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T15:28:20Z</created><updated>2015-06-06T17:49:32Z</updated><resolved>2015-04-21T09:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-04-14T11:16:57Z" id="92774214">@colings86 rebased on latest master if you get a chance to review
</comment><comment author="colings86" created="2015-04-15T14:57:23Z" id="93433671">@markharwood Left some comments
</comment><comment author="markharwood" created="2015-04-15T17:52:13Z" id="93500145">@colings86 Thanks for the review. I added a couple of comments above on execution_hint test coverage and updated the code based on your other comments. 
</comment><comment author="markharwood" created="2015-04-16T11:29:34Z" id="93712496">@jpountz @clintongormley This PR allows users to do analytics on a sample where you can also choose to diversify results on the basis of a particular field (e.g. analyse top X tweets but no more than Y tweets from a single Twitter account on each shard).

The question is what is the least-worst thing to do on each shard given the unmapped problem ie the choice of diversifying field doesn't exist on one of the indexes/shards being queried:
1) Throw an error
2) Return no results (because we can't guarantee diversification)
3) Return top results but without applying any of the diversity constraints
</comment><comment author="markharwood" created="2015-04-20T13:11:40Z" id="94447799">Took a decision with Colin on the 2 remaining questions:
1) execution_hint follows the precedent set in terms agg - never errors e.g. when used in relation to a numeric field in which case is ignored
2) Unmapped choices of diversifying field will return no results rather than a sample of undiversified results
</comment><comment author="markharwood" created="2015-04-20T13:12:15Z" id="94447901">Poke @colings86 
</comment><comment author="colings86" created="2015-04-20T15:32:53Z" id="94484760">LGTM
</comment><comment author="markharwood" created="2015-04-21T09:41:08Z" id="94722429">Pushed to master https://github.com/elastic/elasticsearch/commit/63db34f649d1de038c30d61f3c5e17e059b19b69
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: fix racing condition in timeout handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10220</link><project id="" key="" /><description>If a request comes in at the same moment the timeout handler for it runs, we may leak a timeoutInfoHolder and erroneously log "Transport response handler not found of id" . The same issue could cause the request tracer to fire a traceUnresolvedResponse call instead of traceReceivedResponse , causing a failure of testTracerLog ( see #10187 ) .

This commit synchronizes the two execution paths and also unifies the TransportService.Adapter#remove(requestId) with TransportService.Adapter#onResponseReceived(requestId), as they are always called together to indicate a response was received.

Closes #10187
</description><key id="63674863">10220</key><summary>Transport: fix racing condition in timeout handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Network</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T09:23:27Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-23T13:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-23T12:28:56Z" id="84971571">@kimchy I push another commit. No mutex :)
</comment><comment author="kimchy" created="2015-03-23T13:23:00Z" id="84995223">small formatting comment, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better template handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10219</link><project id="" key="" /><description>Currently elasticsearch handles a faulty index template in it's templates folder, by reverting to built in default :(

Recommendation is to "push" the template to ES.. but the problem with that recommendation, is that I no longer have a documented change process.. when I control all servers in version controlled setup - and roll out config, using puppet - I have complete documentation of all changes in production.

Thats why I want to use the template files method.. also - that means we can change the templates - without giving access to elasticsearch (it may contain sensitive information).

I'd simply like to use the index templates defined in file "feature" - and have it be a bit less dangerous (ie. handle errors better) to use.

My suggestion would be to actually parse templates every hour - to check if they look ok - so I have an entire day to react to parse errors.. and the other suggestion, is to save the "last valid/used" template in memory atleast (or in yesterdays index) - so it can use "last known good version" - instead of throwing everything out, because I made a mistake when updating the template.
</description><key id="63656717">10219</key><summary>Better template handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KlavsKlavsen</reporter><labels><label>:Index Templates</label></labels><created>2015-03-23T08:01:59Z</created><updated>2015-05-30T11:02:35Z</updated><resolved>2015-05-30T11:02:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-30T11:02:34Z" id="107022023">Support for file based index templates have been removed in #11052, in favour of using the API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only cancel recovery when primary completed relocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10218</link><project id="" key="" /><description>When a primary moves to another node, we cancel ongoing recoveries and retry from the primary's new home. At the moment this happens when the primary relocation _starts_. It's a shame as we cancel recoveries that may be close to completion and will finish before the primary has been fully relocated. This commit only triggers the cancelation once the primary relocation is completed.

Next to this, it fixes a race condition between recovery cancellation and the recovery completion. At the moment we may trigger remove a recovered shard just after it was completed. Instead, we should use the recovery cancellation logic to make sure only one code path is followed.

All of the above caused the recoverWhileUnderLoadWithNodeShutdown test to fail (see http://build-us-00.elastic.co/job/es_core_15_debian/32/ ). The test creates an index and then increasingly disallows nodes for it, until only 1 node is left in the allocation filtering rules. Normally, this means we stay in green, but the premature recovery cancellation plus the race condition mentioned above caused a shard to be failed and stay unassigned and the test asserts to fail. This happens due to the following sequence:
- The shard has finished recovering and sent the master a shard started command.
- The recovery is cancelled locally, removing the index shard.
- Master starts shard (deleting it's other copy).
- Local node gets a cluster state with the shard started in it, which cause it to send a shard failed (to make the master aware).
- Shard is failed and can't be re-assigned due to the allocation filter.

The  recoverWhileUnderLoadWithNodeShutdown is also adapted a bit to fit the current behavior of allocation filtering (in the past it used to really shut down nodes). Last, all tests in that class are given better names to fit the current terminology.
</description><key id="63655728">10218</key><summary>Only cancel recovery when primary completed relocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T07:56:33Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-23T13:28:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-23T10:15:53Z" id="84931301">Left some comments - LGTM in general
</comment><comment author="bleskes" created="2015-03-23T10:54:46Z" id="84944424">@s1monw I pushed another commit addressing your feedback
</comment><comment author="s1monw" created="2015-03-23T12:14:29Z" id="84968602">left one minor comment
</comment><comment author="bleskes" created="2015-03-23T12:54:29Z" id="84983744">@s1monw pushed another commit.
</comment><comment author="s1monw" created="2015-03-23T12:57:25Z" id="84986628">LGTM
</comment><comment author="bleskes" created="2015-03-23T13:28:50Z" id="84997203">closing.. typo in commit message failed to auto close.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor parsing of queries/filters, aggs, suggester APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10217</link><project id="" key="" /><description>Copied from #9901:

Today we have a massive infrastructure to parse all our requests. We have client side builders and server side parsers but no real representation of the query, filter, aggregation etc until it's executed. What is produced from a XContent binary is a Lucene query directly which causes huge parse methods in separate classes etc. that hare hard to test and don't allow decoupled modifications or actions on the query itself between parsing and executing. 

This refactoring splits the parsing and the creation of the lucene query, this has a couple of advantages
- XContent parsing creation are in one file and can be tested more easily
- the class allows a typed in-memory representation of the query that can be modified before a lucene query is build
- the query can be normalized and serialized via Streamable to be used as a normalized cache key (not depending on the order of the keys in the XContent)
- the query can be parsed on the coordinating node to allow document prefetching etc. forwarding to the executing nodes would work via Streamable binary representation --&gt; https://github.com/elasticsearch/elasticsearch/issues/8150
- for the query cache a query tree can be "walked" to rewrite range queries into match all queries with MIN/MAX terms to get cache hits for sliding windows --&gt; https://github.com/elasticsearch/elasticsearch/issues/9526
- code wise two classes are merged into one which is nice
## Queries (Completed)
- [x] AndQueryBuilder  (https://github.com/elastic/elasticsearch/pull/11628) 
- [x] BoolQueryBuilder (https://github.com/elastic/elasticsearch/pull/11121)
- [x] BoostingQueryBuilder (https://github.com/elastic/elasticsearch/pull/11621)
- [x] CommonTermsQueryBuilder (https://github.com/elastic/elasticsearch/pull/11345)
- [x] ConstantScoreQueryBuilder (https://github.com/elastic/elasticsearch/pull/11629)
- [x] DisMaxQueryBuilder (https://github.com/elastic/elasticsearch/pull/11703)
- [x] ExistsQueryBuilder (https://github.com/elastic/elasticsearch/pull/11427)
- [x] FieldMaskingSpanQueryBuilder (https://github.com/elastic/elasticsearch/pull/11717)
- [x] FilteredQueryBuilder (https://github.com/elastic/elasticsearch/pull/11885)
- [x] FQueryFilterBuilder (https://github.com/elastic/elasticsearch/pull/11729)
- [x] FunctionScoreQueryBuilder (#13653)
- [x] FuzzyQueryBuilder (https://github.com/elastic/elasticsearch/pull/11865)
- [x] GeoBoundingBoxQueryBuilder (https://github.com/elastic/elasticsearch/pull/11969)
- [x] GeoDistanceQueryBuilder(https://github.com/elastic/elasticsearch/pull/13496)
- [x] GeoDistanceRangeQueryBuilder (https://github.com/elastic/elasticsearch/pull/13496)
- [x] GeohashCellQuery (#13393)
- [x] GeoPolygonQueryBuilder (#13426)
- [x] GeoShapeQueryBuilder (https://github.com/elastic/elasticsearch/pull/13466)
- [x] HasChildQueryBuilder (#13333)
- [x] HasParentQueryBuilder (#13408)
- [x] IdsQueryBuilder (#10670)
- [x] IndicesQueryBuilder (https://github.com/elastic/elasticsearch/pull/12031)
- [x] LimitQueryBuilder (#11551)
- [x] MatchAllQueryBuilder (#10668)
- [x] MatchQueryBuilder (#13402)
- [x] MissingQueryBuilder (https://github.com/elastic/elasticsearch/pull/12030)
- [x] MoreLikeThisQueryBuilder (https://github.com/elastic/elasticsearch/pull/13486)
- [x] MultiMatchQueryBuilder (#13405)
- [x] NestedQueryBuilder (https://github.com/elastic/elasticsearch/pull/13424)
- [x] NotQueryBuilder (https://github.com/elastic/elasticsearch/pull/11823)
- [x] OrQueryBuilder (https://github.com/elastic/elasticsearch/pull/11817)
- [x] PrefixQueryBuilder (https://github.com/elastic/elasticsearch/pull/12032)
- [x] QueryFilterBuilder (https://github.com/elastic/elasticsearch/pull/11729)
- [x] QueryStringQueryBuilder (#13284)
- [x] QueryWrappingQueryBuilder (will be removed once all queries refactored)
- [x] RangeQueryBuilder (#11108)
- [x] RegexpQueryBuilder (https://github.com/elastic/elasticsearch/pull/11896)
- [x] ScriptQueryBuilder (https://github.com/elastic/elasticsearch/pull/12115)
- [x] SimpleQueryStringBuilder (https://github.com/elastic/elasticsearch/pull/11274)
- [x] SpanContainingQueryBuilder (https://github.com/elastic/elasticsearch/pull/12057)
- [x] SpanFirstQueryBuilder (https://github.com/elastic/elasticsearch/pull/12073)
- [x] SpanMultiTermQueryBuilder (https://github.com/elastic/elasticsearch/pull/12182)
- [x] SpanNearQueryBuilder (https://github.com/elastic/elasticsearch/pull/12156)
- [x] SpanNotQueryBuilder (https://github.com/elastic/elasticsearch/pull/12365)
- [x] SpanOrQueryBuilder (https://github.com/elastic/elasticsearch/pull/12342)
- [x] SpanTermQueryBuilder (#11005)
- [x] SpanWithinQueryBuilder (#12396)
- [x] TemplateQueryBuilder (https://github.com/elastic/elasticsearch/pull/13253)
- [x] TermQueryBuilder (#10669)
- [x] TermsLookupQueryBuilder (https://github.com/elastic/elasticsearch/pull/12042)
- [x] TermsQueryBuilder (https://github.com/elastic/elasticsearch/pull/12042)
- [x] TypeQueryBuilder (https://github.com/elastic/elasticsearch/pull/12035)
- [x] WildcardQueryBuilder (https://github.com/elastic/elasticsearch/pull/12110)
- [x] WrapperQueryBuilder (https://github.com/elastic/elasticsearch/pull/12037)

Total of 54 Queries
54 done

Former filters were mostly merged or converted to queries and are included in this list.
## Aggregations (Completed - https://github.com/elastic/elasticsearch/pull/14136)
- [x] Min Aggregation (https://github.com/elastic/elasticsearch/pull/12832)
- [x] Max Aggregation (https://github.com/elastic/elasticsearch/pull/14687)
- [x] Sum Aggregation (https://github.com/elastic/elasticsearch/pull/14687)
- [x] Avg Aggregation (https://github.com/elastic/elasticsearch/pull/14687)
- [x] Stats Aggregation (https://github.com/elastic/elasticsearch/pull/14688)
- [x] Extended Stats Aggregation (https://github.com/elastic/elasticsearch/pull/14688)
- [x] Value Count Aggregation (https://github.com/elastic/elasticsearch/pull/14689)
- [x] Percentiles Aggregation (https://github.com/elastic/elasticsearch/pull/14836)
- [x] Percentile Ranks Aggregation (https://github.com/elastic/elasticsearch/pull/14836)
- [x] Cardinality Aggregation (https://github.com/elastic/elasticsearch/pull/14893)
- [x] Geo Bounds Aggregation (https://github.com/elastic/elasticsearch/pull/14934)
- [x] Top hits Aggregation (https://github.com/elastic/elasticsearch/pull/15566)
- [x] Scripted Metric Aggregation (https://github.com/elastic/elasticsearch/pull/14966)
- [x] Global Aggregation (https://github.com/elastic/elasticsearch/pull/14139)
- [x] Filter Aggregation (https://github.com/elastic/elasticsearch/pull/14974)
- [x] Filters Aggregation (https://github.com/elastic/elasticsearch/pull/15437)
- [x] Missing Aggregation (https://github.com/elastic/elasticsearch/pull/14975)
- [x] Nested Aggregation (https://github.com/elastic/elasticsearch/pull/15006)
- [x] Reverse nested Aggregation (https://github.com/elastic/elasticsearch/pull/15006)
- [x] Children Aggregation (https://github.com/elastic/elasticsearch/pull/15008)
- [x] Terms Aggregation (https://github.com/elastic/elasticsearch/pull/15386)
- [x] Significant Terms Aggregation (https://github.com/elastic/elasticsearch/pull/15386)
- [x] Sampler Aggregation (https://github.com/elastic/elasticsearch/pull/15418)
- [x] Range Aggregation (https://github.com/elastic/elasticsearch/pull/15399)
- [x] Date Range Aggregation (https://github.com/elastic/elasticsearch/pull/15399)
- [x] IPv4 Range Aggregation (https://github.com/elastic/elasticsearch/pull/15399)
- [x] Histogram Aggregation (https://github.com/elastic/elasticsearch/pull/14140)
- [x] Date Histogram Aggregation (https://github.com/elastic/elasticsearch/pull/14140)
- [x] Geo Distance Aggregation (https://github.com/elastic/elasticsearch/pull/15399)
- [x] GeoHash grid Aggregation (https://github.com/elastic/elasticsearch/pull/14138)
- [x] Geo Centroid Aggregation (https://github.com/elastic/elasticsearch/pull/15002)
- [x] Min Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Max Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Avg Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Sum Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Stats Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Extended Stats Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Percentiles Bucket Aggregation (https://github.com/elastic/elasticsearch/pull/15009)
- [x] Bucket Script Aggregation (https://github.com/elastic/elasticsearch/pull/15014)
- [x] Cumulative Sum Aggregation (https://github.com/elastic/elasticsearch/pull/15015)
- [x] Derivative Aggregation (https://github.com/elastic/elasticsearch/pull/14137)
- [x] Bucket Selector Aggregation (https://github.com/elastic/elasticsearch/pull/15147)
- [x] Moving Average Aggregation (https://github.com/elastic/elasticsearch/pull/15180)
- [x] Serial Differencing Aggregation (https://github.com/elastic/elasticsearch/pull/15058)

Total of 44 Aggregations
44 done
## Suggesters
- [x] Term suggester
- [x] Phrase Suggester
- [x] Completion Suggester
- [x] Context Suggester

Total of 4 Suggesters
4 done, 0 in open PRs
## Highlighters
- [x] plain
- [x] fvh
- [x] postings

Total of 3 Highlighters
3 done
## Others
- [x] from (https://github.com/elastic/elasticsearch/pull/13752)
- [x] size (https://github.com/elastic/elasticsearch/pull/13752)
- [x] min_score (https://github.com/elastic/elasticsearch/pull/13752)
- [x] fields (https://github.com/elastic/elasticsearch/pull/13752)
- [x] script_fields (https://github.com/elastic/elasticsearch/pull/13752)
- [x] fielddata_fields (https://github.com/elastic/elasticsearch/pull/13752)
- [x] _source (https://github.com/elastic/elasticsearch/pull/13752)
- [x] sort (https://github.com/elastic/elasticsearch/pull/17205)
- [x] timeout (https://github.com/elastic/elasticsearch/pull/13752)
- [x] explain (https://github.com/elastic/elasticsearch/pull/13752)
- [x] terminate_after (https://github.com/elastic/elasticsearch/pull/13752)
- [x] version (https://github.com/elastic/elasticsearch/pull/13752)
- [x] stats (https://github.com/elastic/elasticsearch/pull/13752)
- [x] indices_boost (https://github.com/elastic/elasticsearch/pull/13752)
- [x] track_scores (https://github.com/elastic/elasticsearch/pull/13752)
- [x] inner_hits (https://github.com/elastic/elasticsearch/pull/17291)
- [x] ~~query_binary~~: (removed, queries should only be specified via type-safe builders in the Java API, see #14308)
- [x] ~~filter_binary~~: (removed, filters should only be specified via type-safe builders in the Java API, see #14308)
- [x] ~~aggregations_binary~~: (to be removed, aggregations should only be specified via type-safe builders in the Java API, see #14308 and #14136)
- [x] rescore (https://github.com/elastic/elasticsearch/issues/15559)
- [x] post_filter (https://github.com/elastic/elasticsearch/pull/13752)
## APIs to be adapted/revised besides _search
- [x] ~~search exists api~~ (removed, see #13911)
- [x] explain api: (#14270)
- [x] validate query api: (#14384)
- [x] suggest api (#17198)
- [x] percolator (Planned to be removed, see #16349)
- [x] ~~index warmers~~ (removed, see #15614)
- [ ] alias filters

The above apis don't necessarily have to change to parse queries in our intermediate format, for instance the percolator will still need to parse to lucene query straight-away, but we should still have a look at each of those and double check if anything needs to be adjusted after all the infra changes we have made.
</description><key id="63624736">10217</key><summary>Refactor parsing of queries/filters, aggs, suggester APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Search Refactoring</label><label>breaking</label><label>enhancement</label><label>Meta</label></labels><created>2015-03-23T04:43:40Z</created><updated>2016-11-04T17:05:34Z</updated><resolved>2016-11-04T17:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2015-03-23T18:38:06Z" id="85136619">+1, In #3278 I perform a terms lookup after parsing but this happened on all shards and resulted in multiple lookup requests for a single query.  This would allow the expensive lookup to be performed once on the coordinating node which would be very beneficial!
</comment><comment author="javanna" created="2015-03-31T17:13:59Z" id="88175368">Here is our rough plan:

First step (everything still happens on the nodes that hold the relevant shards):
- split existing `QueryParser#parse()` method into 1) `fromXContent()` that parses the query but allows to have an intermediate format for it and 2) `toQuery()` that allows to create the lucene query ouf the intermediate query. Keep the exisiting `Query parse()` method around temporarily, which will call both `fromXContent` and `toQuery`. 
- Move `toQuery` to `QueryBuilder`s so that every query (intermediate format) can be transformed into the corresponding lucene query on each data node.
- make all the QueryBuilders implement `Streamable` so that the intermediate query format can be sent over the wire between the nodes (will be unused at first)

Second step (query parsing moved to the coordinating node):
- Refactor the whole `SearchRequest` so that instead of holding the source bytes array in json format, it holds different elements of a search request, all in `Streamable` format
- leverage existing request validation mechanism so that `SearchRequest#validate` validates the query too (during `TransportSearchAction#execute`): this is very convenient as it gets called on the coordinating node, no matter where the request comes from (could come as json through rest layer, or as java objects through java api, either transport client or node client).
- call `fromXContent` on the coordinating node and make use of `Streamable` methods to send queries over the wire
- delete `parse` method from all QueryBuilders, not needed anymore.

Things we are not too happy about and might need improvement, will be tackled later on:
- rename QueryBuilders by removing the Builders suffix, as they will not be really just builders anymore
- java api side of things is heavier on users, since a single class exposes a lot of internal aspects, we might be able to make methods package private
</comment><comment author="dakrone" created="2015-04-02T20:49:12Z" id="89041305">@clintongormley et al,

Because we are touching every single query in this change, it also gives us the ability to remove support for camel-casing in queries where it exists. How do we feel about removing the camel casing in these PRs as well?
</comment><comment author="clintongormley" created="2015-04-05T17:52:55Z" id="89820357">@dakrone ++ - should we be doing this by using parseField and then later using https://github.com/elastic/elasticsearch/issues/8963 to warn about the deprecations?
</comment><comment author="cbuescher" created="2015-04-17T19:06:38Z" id="94054388">We decided to reset the feature branch to the current tip of master and branch from there starting with #10580.  
</comment><comment author="javanna" created="2015-09-03T09:49:22Z" id="137394610">I've just updated the description of this issue and added all the other search sections that will need to be refactored, along with queries and aggregations.
</comment><comment author="javanna" created="2016-08-11T11:46:56Z" id="239138850">I had a quick look at what is left to do here and marked `inner_hits` and `sort` done. There are some specific open issues marked `:Search Refactoring`, so I am wondering if we should close this issue. @cbuescher @colings86 Thoughts? What were your plans around alias filters?
</comment><comment author="colings86" created="2016-08-11T13:49:05Z" id="239165976">I'm for closing this issue. The few issues which are still open and tagged as `:Search Refactoring` can stand alone as issues and the success of this issue doesn't rely directly on them (though it would be nice to fix those issues. As for the alias filters maybe we can open a separate issue for them too?
</comment><comment author="cbuescher" created="2016-08-11T13:54:02Z" id="239167310">+1 on closing and opening a separate issue for the alias filters. Are they part of the SearchSourceBuilder? Just curious what place they have in terms of parsing incoming requests. 
</comment><comment author="colings86" created="2016-08-11T13:58:29Z" id="239168669">@cbuescher I think the reason alias filter were on the list was because they are represented by a QueryBuilder object and could be stored as a serialised QueryBuilder rather than JSON. I agree though that it's not directly connected to this issue
</comment><comment author="javanna" created="2016-11-04T17:05:32Z" id="258489958">What was most important to do for alias filters was done with #20916. Each search request against filtered indices was previously sending the filter as a string all the way to the shards, where the actual filter was parsed and converted to lucene query. Now the parsing happens once on the coordinating node, only `toFilter` gets called on each shard to get the corresponding lucene query. We still store alias filters in compressed XContent format as part of the cluster state, but I don't think that is going to change anytime soon. That said we can close this issue, nothing left to be done. Yay!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percentile ranks metric bug?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10216</link><project id="" key="" /><description>I'm trying to use percentile rank metrics and it works quite odd. ElasticSearch docs say - "Percentile rank show the percentage of observed values which are below certain value." which is exactly what I need, though it gives queer results even for small set of docs (5-10 docs)

```
{
  "responses": [
    {
      "took": 2,
      "timed_out": false,
      "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
      },
      "hits": {
        "total": 7,
        "max_score": 0,
        "hits": []
      },
      "aggregations": {
        "1": {
          "values": {
            "22.0": 70.12987012987013,
            "23.0": 50
          }
        }
      }
    }
  ]
}
```

Set of numbers I use: 20,  5, 10,  25, 14,  27, 13

I don't get how it's possible that value 23 shows lower percentage than value 22. Do I misunderstand idea of percentile ranks or it's a bug?

I am using Elastic search 1.4.4
</description><key id="63624204">10216</key><summary>Percentile ranks metric bug?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drej82</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-03-23T04:39:39Z</created><updated>2016-11-06T07:52:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-23T10:11:36Z" id="84929411">@drej82 thanks for raising this issue. It does look like a bug.

I have reproduced this on the master branch on an index with only one shard using the below script: 

``` javascript
PUT percenttest
{
  "settings": {
    "number_of_shards": 1, 
    "number_of_replicas": 0
  }
}
POST percenttest/doc/1
{
  "i": 20
}
POST percenttest/doc/2
{
  "i": 5
}
POST percenttest/doc/3
{
  "i": 10
}
POST percenttest/doc/4
{
  "i": 25
}
POST percenttest/doc/5
{
  "i": 14
}
POST percenttest/doc/6
{
  "i": 27
}
POST percenttest/doc/7
{
  "i": 13
}
GET percenttest/_search?search_type=count
{
  "aggs": {
    "percent_rank": {
      "percentile_ranks": {
        "field": "i",
        "values": [
          1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
        ]
      }
    }
  }
}
```

The response from the last request is:

``` json
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 7,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "percent_rank": {
         "values": {
            "1.0": 0,
            "1.0_as_string": "0.0",
            "2.0": 0,
            "2.0_as_string": "0.0",
            "3.0": 1.4285714285714286,
            "3.0_as_string": "1.4285714285714286",
            "4.0": 4.285714285714286,
            "4.0_as_string": "4.285714285714286",
            "5.0": 7.142857142857142,
            "5.0_as_string": "7.142857142857142",
            "6.0": 10,
            "6.0_as_string": "10.0",
            "7.0": 12.85714285714286,
            "7.0_as_string": "12.85714285714286",
            "8.0": 16.071428571428573,
            "8.0_as_string": "16.071428571428573",
            "9.0": 19.642857142857142,
            "9.0_as_string": "19.642857142857142",
            "10.0": 23.214285714285715,
            "10.0_as_string": "23.214285714285715",
            "11.0": 26.785714285714285,
            "11.0_as_string": "26.785714285714285",
            "12.0": 32.142857142857146,
            "12.0_as_string": "32.142857142857146",
            "13.0": 39.285714285714285,
            "13.0_as_string": "39.285714285714285",
            "14.0": 44.89795918367347,
            "14.0_as_string": "44.89795918367347",
            "15.0": 48.97959183673469,
            "15.0_as_string": "48.97959183673469",
            "16.0": 53.06122448979592,
            "16.0_as_string": "53.06122448979592",
            "17.0": 57.14285714285714,
            "17.0_as_string": "57.14285714285714",
            "18.0": 59.74025974025974,
            "18.0_as_string": "59.74025974025974",
            "19.0": 62.33766233766234,
            "19.0_as_string": "62.33766233766234",
            "20.0": 64.93506493506493,
            "20.0_as_string": "64.93506493506493",
            "21.0": 67.53246753246754,
            "21.0_as_string": "67.53246753246754",
            "22.0": 70.12987012987013,
            "22.0_as_string": "70.12987012987013",
            "23.0": 50,
            "23.0_as_string": "50.0",
            "24.0": 57.14285714285714,
            "24.0_as_string": "57.14285714285714",
            "25.0": 64.28571428571429,
            "25.0_as_string": "64.28571428571429",
            "26.0": 71.42857142857143,
            "26.0_as_string": "71.42857142857143",
            "27.0": 78.57142857142857,
            "27.0_as_string": "78.57142857142857",
            "28.0": 100,
            "28.0_as_string": "100.0",
            "29.0": 100,
            "29.0_as_string": "100.0",
            "30.0": 100,
            "30.0_as_string": "100.0"
         }
      }
   }
}
```

@jpountz any idea what might be going on with the decrease in value from 22 to 23?
</comment><comment author="colings86" created="2015-03-23T10:17:27Z" id="84932199">The problem seems to happen at the value 22.5:

``` javascript
GET percenttest/_search?search_type=count
{
  "aggs": {
    "percent_rank": {
      "percentile_ranks": {
        "field": "i",
        "values": [
          22.4999999,22.50
        ]
      }
    }
  }
}
```

response:

``` json
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 7,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "percent_rank": {
         "values": {
            "22.4999999": 71.42857116883116,
            "22.4999999_as_string": "71.42857116883116",
            "22.5": 46.42857142857143,
            "22.5_as_string": "46.42857142857143"
         }
      }
   }
}
```
</comment><comment author="drej82" created="2015-03-28T22:07:12Z" id="87321738">I actually found a bug in t-digest, it skips the last interval while doing the linear interpolation. I will create pull request with a fix.

For tracking purposes: https://github.com/tdunning/t-digest/pull/44
</comment><comment author="jpountz" created="2015-03-29T20:43:47Z" id="87473625">Thanks @drej82 for investigating this bug, opening a bug upstream and proposing a fix!
</comment><comment author="drej82" created="2015-03-31T21:46:36Z" id="88260445">The fix was committed, please let me know when it propagates to official snapshot.
</comment><comment author="jpountz" created="2015-04-03T13:47:38Z" id="89294053">Thanks! We'll watch for new t-digest releases and integrate it once a new version is released with this fix.
</comment><comment author="jpountz" created="2015-04-06T17:28:58Z" id="90163833">A new release is out, I'll upgrade the dependency soon. http://search.maven.org/#artifactdetails%7Ccom.tdunning%7Ct-digest%7C3.1%7Cjar
</comment><comment author="jpountz" created="2015-04-07T10:52:54Z" id="90506029">I tried to upgrade (see https://github.com/jpountz/elasticsearch/tree/upgrade/t-digest) but we have test failures due to the fact that the new cdf computation returns 100 even for values that are below the max value. For instance if I replay @colings86 's example above, I get the following response:

```
      "percent_rank": {
         "values": {
            "1.0": 0,
            "2.0": 0,
            "3.0": 1.4285714285714286,
            "4.0": 4.285714285714286,
            "5.0": 7.142857142857142,
            "6.0": 10,
            "7.0": 12.85714285714286,
            "8.0": 16.071428571428573,
            "9.0": 19.642857142857142,
            "10.0": 23.214285714285715,
            "11.0": 26.785714285714285,
            "12.0": 32.142857142857146,
            "13.0": 39.285714285714285,
            "14.0": 44.89795918367347,
            "15.0": 48.97959183673469,
            "16.0": 53.06122448979592,
            "17.0": 57.14285714285714,
            "18.0": 59.74025974025974,
            "19.0": 62.33766233766234,
            "20.0": 64.93506493506493,
            "21.0": 67.53246753246754,
            "22.0": 70.12987012987013,
            "23.0": 73.46938775510205,
            "24.0": 77.55102040816327,
            "25.0": 81.63265306122449,
            "26.0": 100,
            "27.0": 100,
            "28.0": 100,
            "29.0": 100,
            "30.0": 100
         }
      }
   }
```

It returns 100 for 26 although the max value is 27. But maybe it's still better than what we have currently and we should make the test more lenient and work at the same time on improving the cdf computation?
</comment><comment author="colings86" created="2015-04-07T10:57:15Z" id="90506606">@jpountz I would agree, since t-digest is an approximate algorithm it may calculate the 100th percentile as a lower value that the actual data depending on how it buckets the data.

+1 on making the test a bit more lenient as well as improving the cdf computation
</comment><comment author="jpountz" created="2015-04-07T10:58:24Z" id="90506757">OK, I'll do that then. Thanks for the feedback.
</comment><comment author="drej82" created="2015-04-07T14:19:40Z" id="90581282">The change I've made couldn't cause such behavior. I'm wondering if Ted made some other changes. The way it works, it should return values &lt; 100 % for all neighbors of max, even for neighbors &gt; max. Instead of relaxing the test, I'd suggest we find out the root cause.

Sent from my Windows Phone

-----Original Message-----
From: "Adrien Grand" notifications@github.com
Sent: &#8206;4/&#8206;7/&#8206;2015 3:59 AM
To: "elastic/elasticsearch" elasticsearch@noreply.github.com
Cc: "Andrey" andrew.obraztsov@gmail.com
Subject: Re: [elasticsearch] Percentile ranks metric bug? (#10216)

OK, I'll do that then. Thanks for the feedback.
&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="rcrezende" created="2015-11-19T03:34:01Z" id="157940354">Any update on this?
</comment><comment author="rcrezende" created="2016-03-14T20:20:55Z" id="196505146">Any update on this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow indices containing too-old segments to be opened</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10215</link><project id="" key="" /><description>Elasticsearch 2.x (Lucene 5.x) will support all Lucene 4.x segments, but not Lucene 3.x segments. We should not allow any indices with too-old segments to be opened on cluster restart.  These indices should be left in a closed state, and the reason for not opening should be logged.

If the user tries to open the index with the index-open API, they should receive an exception explaining the reason.

We will advise users of 1.x to install the migration advisory plugin #10214 before upgrading to check the age of their segments.  The upgrade path in this case will be to install Elasticsearch 1.6 and to run the upgrade API.

If users ignore this advice and go straight to 2.x, then they will be unable to open these indices.  At this point, the upgrade path will be to move these indices to a new directory, install 1.6, and use the upgrade API.  Once upgraded, they can install 2.0 and rely on the dangling indices function.
</description><key id="63621095">10215</key><summary>Don't allow indices containing too-old segments to be opened</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T04:23:18Z</created><updated>2015-05-20T03:53:47Z</updated><resolved>2015-05-20T03:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="skearns64" created="2015-03-31T16:13:48Z" id="88149452">We'll have to be sure to handle the case where some of the shards of a given index have 3.x segments, while other shards contain only 4.x segments. This means we should try to find a way to take/keep the whole index offline, or ensure that the manual process index upgrade process works in this case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migration advisory plugin </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10214</link><project id="" key="" /><description>It would be useful to have a _site plugin that can be installed on 1.x, which would warn users migrating to 2.0 about the need to upgrade any indices, and any use they are making of deprecated features.

For instance, the tool would:
- use the segments API to look for 3.x segments - if found, it would recommend upgrading to 1.6 first, and running the upgrade API
- check mappings for any use of deprecated functionality in #8870, and warn about the actions that will be taken in 2.0
- similarly, check index settings, analyzers, index templates, dynamic mappings etc
- possibly also deprecated cluster settings? (although won't have access to the yaml config file)

This tool won't be able to check use of deprecated query DSL, as that only happens at run time.
</description><key id="63618996">10214</key><summary>Migration advisory plugin </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T04:09:10Z</created><updated>2015-05-29T08:29:36Z</updated><resolved>2015-05-29T08:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T08:29:36Z" id="106743988">Still more tests to be added, but the migration plugin is already usable: https://github.com/elastic/elasticsearch-migration
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow upgrade API to only upgrade too-old segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10213</link><project id="" key="" /><description>The upgrade API today will upgrade any segments which are not the latest Lucene version.  When upgrading to 2.0, we won't be able to read 3.x segments, but we can read 4.x segments, so it would be faster for the upgrade API to upgrade JUST the 3.x segments, and leave the 4.x segments to the merge process.
</description><key id="63618079">10213</key><summary>Allow upgrade API to only upgrade too-old segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Upgrade API</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T04:02:32Z</created><updated>2015-04-16T09:23:58Z</updated><resolved>2015-04-16T09:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-04-07T10:54:39Z" id="90506234">Upgrading only 3.x is short-sighted and will leave the same trap for userse for 2.0-&gt;3.0.

I do think only segments that are "out of date" should be upgraded. I do think its also nice to have an option to upgrade only the SUPER ANCIENT ones and maybe some tools and stuff can use that. But it should not be the default behavior.
</comment><comment author="rmuir" created="2015-04-07T11:14:40Z" id="90509176">I would go further too, if we add an 'ancient' only option (segments from previous lucene major version), and make sure logging and various stats apis etc scream bloody murder when users have ancient segments. Really make the user feel shame and humiliation for still having them around. Otherwise nothing will get done.
</comment><comment author="clintongormley" created="2015-04-07T12:38:36Z" id="90534372">&gt; I do think only segments that are "out of date" should be upgraded. I do think its also nice to have an option to upgrade only the SUPER ANCIENT ones and maybe some tools and stuff can use that. But it should not be the default behavior.

Agreed - it's just about adding the option to do the least work possible to get your cluster moving again.  This would be particularly useful when combined with the upgrade-old-segments-as-part-of-background-merge-process functionality that we've discussed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for cluster state diffs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10212</link><project id="" key="" /><description>First iteration of cluster state diffs that adds support for diffs to the most frequently changing elements - cluster state, meta data and routing table.
</description><key id="63597850">10212</key><summary>Add support for cluster state diffs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>:Core</label><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-23T01:22:59Z</created><updated>2015-06-06T19:07:22Z</updated><resolved>2015-04-27T03:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-23T12:19:19Z" id="84969591">I will review this today or tomorrow...
</comment><comment author="s1monw" created="2015-03-24T15:27:14Z" id="85554208">I went over this and I have to admit I am in the middle of a jetlag. I really like it. What I miss is extensive unittesting of the individual components. I think there should be way more test code that is not integration tests to make this fly. I think code-wise we are super close.
</comment><comment author="bleskes" created="2015-03-25T10:55:04Z" id="85981719">I went through it and I like it a lot. Left some naming comments (these are opinionated, feel free to reject). My biggest feed back is about the separation of the writing of diffs (Under Diff) and the reading of them which is part of Diffable.readDiffs  . I think this this will cause problems as it is good to keep this logic next to each other.  

My gut feeling says we should make Diffs reand and write themselves and then make Diffable have two methods, one to produce a diff and the other to applyIt . I think it will also make it easier to unit tests as the diffs do not necessary need to go through a stream.

I will do another careful review of the Zen Disco side one the changes are settled.
</comment><comment author="imotov" created="2015-03-30T17:32:20Z" id="87763230">@s1monw, @bleskes thanks for the review. I have updated the PR to address your comments and suggestions.
</comment><comment author="bleskes" created="2015-04-01T12:45:03Z" id="88466227">@imotov I like the change. Left comments. I still believe we should have the readFrom and writeTo method of diffs in the same place (on the Diff). Also I think we need to beef up the testing of diff generation , do things like checking we do the right thing when we have two cluster state with an index of the same name but a different uuid. Another example is to test we can concurrently the `PublishClusterStateRequestHandler` and it doesn't get confused.
</comment><comment author="imotov" created="2015-04-03T18:21:10Z" id="89380036">@bleskes, @s1monw I pushed another round of changes to incorporate most of @bleskes's comments and and added more diff tests. I am still looking into the Diff modifications that @bleskes has requested, but so far I wasn't able to find a solution that wouldn't add too much unnecessary complexity. 
</comment><comment author="imotov" created="2015-04-13T13:47:55Z" id="92360299">@bleskes so, what do you think about diffs refactoring that I pushed last weekend?
</comment><comment author="s1monw" created="2015-04-15T15:52:50Z" id="93457436">I left a bunch of comments, should we also add tests for all individual diffables etc? like unittests? I think we are reasonably close here btw.
</comment><comment author="imotov" created="2015-04-16T03:30:05Z" id="93628633">@s1monw, I push another commit. I was thinking about the unittest for diffables and I started to write them, but I quickly found that they are way to close to the code in the diffable itself. So, basically they tend to follow the structure and therefore most likely to repeat the errors. For example, if you forget to send a diff for a field. So, I took a different (and very randomized approach) that was implemented in ClusterStateDiffTests. It tests each diffable in  common framework. by applying random changes to the random diffables and making sure they are propagated through diffs correctly. What do you think?
</comment><comment author="s1monw" created="2015-04-16T12:23:38Z" id="93720805">@imotov I left a comment regarding BWC - I think we should not think about backporting here at all
</comment><comment author="s1monw" created="2015-04-23T20:34:49Z" id="95712216">I took another look at the two added commits and it looks good to me... I know @bleskes had some concerns about the diff logic somewhere but I don't recall what it was. @bleskes can you take anothter look at it?
</comment><comment author="s1monw" created="2015-04-24T08:19:49Z" id="95848600">I did another review with a fresh brain after good sleep and I think this LGTM except of the couple of comments I left. @imotov what do you think if we add a method to our test framework that after a test fetches the current clusterstate from all the nodes and if their IDs match we compare them and then must be identical? I think this is a pretty fair  check and it gives us a much better coverage? 
</comment><comment author="imotov" created="2015-04-24T14:41:20Z" id="95951457">@s1monw I pushed another revision. I tried adding the check at the and of the test but ran into an issue with some tests. The problem is that not all nodes arrived to the end of the test with the same state, there are typically some nodes that are lagging behind. So, I would need to bring them up to speed somehow in a reliable fashion.  Any ideas what would be the best way to do it? 
</comment><comment author="s1monw" created="2015-04-24T18:31:45Z" id="96026562">&gt; Any ideas what would be the best way to do it?

IMO we don't test that all nodes are on the same state, I'd just get all the states that are the same as the master and compare them. If some nodes are lacking that's fine we can just omit them most of the cases should give us something to compare though.
</comment><comment author="imotov" created="2015-04-24T19:04:49Z" id="96037256">@s1monw makes sense. I pushed 4eec468
</comment><comment author="s1monw" created="2015-04-24T19:18:18Z" id="96040314">@imotov can we compare the bytes instead of the length? using `Arrays.equals(master, current)`
</comment><comment author="imotov" created="2015-04-24T20:17:03Z" id="96052130">@s1monw https://github.com/elastic/elasticsearch/commit/4eec46832408d6d0b038eee07807555c394b3a66#diff-1f6d077b9f4832f69435467e77060039R1138
</comment><comment author="s1monw" created="2015-04-24T20:31:33Z" id="96055789">@imotov I think we should try to do this... I wonder how much effort it really is here. Maybe toXContent helps and we can play some tricks here and deserialize and sort or whatever... This would really be good to have though.
</comment><comment author="imotov" created="2015-04-24T20:37:29Z" id="96057650">@s1monw yes, I probably can add an additional check that will use toXContent, parse it into a map, remove local node reference and then compare maps. I will give it a short.
</comment><comment author="imotov" created="2015-04-25T04:42:39Z" id="96128230">@s1monw I have added the check that generated JSON is same for all nodes. What do you think?
</comment><comment author="s1monw" created="2015-04-25T10:00:56Z" id="96170998">left some minor comments. LGTM otherwise. Feel free to push once you fixed them! Good stuff, another big step.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Define logger options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10211</link><project id="" key="" /><description>This came up [via the community](https://groups.google.com/d/msgid/elasticsearch/0ec824b1-e98f-404a-afcf-b78390bae81e%40googlegroups.com).

Basically we need to expand our docs around what different log options are available for each aspect of Elasticsearch; indexing, cluster, discovery etc etc.
</description><key id="63586775">10211</key><summary>[DOC] Define logger options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-03-22T22:48:05Z</created><updated>2016-09-27T14:17:12Z</updated><resolved>2016-09-27T14:17:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T14:17:12Z" id="249878282">Closing this as logging now uses the stock log4j2.properties file, which is documented by the log4j2 project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10210</link><project id="" key="" /><description>Note, Jackson 2.5 is less lenient when it comes to not starting an object before starting to add fields on a fresh builder, fixed where applicable.
</description><key id="63571092">10210</key><summary>Upgrade to Jackson 2.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-22T20:21:36Z</created><updated>2015-08-25T13:25:03Z</updated><resolved>2015-03-23T08:04:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-22T22:08:13Z" id="84712941">LGTM. Jackson change log looks innocent enough.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable doc values by default, when appropriate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10209</link><project id="" key="" /><description>Doc values significantly reduced heap usage, which results in faster GCs. This change makes the default for doc values dynamic: any field that is indexed but not analyzed now has doc values. This only affects fields on indexes created with 2.0+.

Notes:
- Doc values for geo was already messed up.  With lat/lon subfields enabled, doc values settings were never passed through.  I have explicitly disabled doc values for geo points for now.
- Percolator queries with fielddata execution are broken (see comment in test).  We should just remove execution mode...

closes #8312
</description><key id="63548268">10209</key><summary>Enable doc values by default, when appropriate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-22T16:10:57Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-27T06:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-03-22T17:05:25Z" id="84655184">&gt; This also simplifies the way to set doc values by removing the fielddata format path (again, for 2.0+ indexes).

I'm concerned this change will disable docvalues for more users than it will enable it for. Please, if we are going to remove this "way of doing it" then the system must fail hard if it sees it.
</comment><comment author="rjernst" created="2015-03-23T01:07:01Z" id="84741531">I changed the `return null` in case of `fielddata.format=doc_values` to an IAE.  IndexFieldDataService already has other places it checks values and throws IAE, so this should be fine I think.
</comment><comment author="clintongormley" created="2015-03-23T03:53:21Z" id="84784397">Would be nice get this syntax change https://github.com/elastic/elasticsearch/issues/8693#issuecomment-84784051 done at the same time (but as a separate PR, so it doesn't block this one)
</comment><comment author="rjernst" created="2015-03-23T05:45:42Z" id="84818518">&gt; &gt; I'm concerned this change will disable docvalues for more users than it will enable it for. Please, if we are going to remove this "way of doing it" then the system must fail hard if it sees it.
&gt; 
&gt; I changed the return null in case of fielddata.format=doc_values to an IAE. IndexFieldDataService already has other places it checks values and throws IAE, so this should be fine I think.

Actually, this produced a lot of test failures.  There are still numerous tests using this old way of specifying doc values.  I've added back the old way, to simplify this change.  We should still simplify doc values to have only one way to enable/disable, but this can be done in another PR.
</comment><comment author="rjernst" created="2015-03-23T06:48:42Z" id="84843539">`_timestamp` was also giving problems with tests, randomly causing various test failures. The failures seemed to happen whenever _timestamp was enabled, and doc values explicitly disabled (through the randomized index template).  Any subsequent updates that did not contain _timestamp would cause a merge conflict.  I've set _timestamp back to doc values disabled by default.  We can improve this in a subsequent issue.
</comment><comment author="nirmalc" created="2015-03-23T19:59:56Z" id="85168899">Currently if the field length is more than 32766 , doc_values throws an error ( we avoid ignore_above analyzer in analyzed fields for same ) , Any helpers there planned ? 
</comment><comment author="rjernst" created="2015-03-23T20:08:27Z" id="85171536">@nirmalc This is an internal limitation of the format used for doc values. I do wonder why you have an unanalyzed value that long, but in any case, you will still be able to disable doc values explicitly, and any full text that is analyzed won't have doc values enabled (it can't).
</comment><comment author="nirmalc" created="2015-03-23T20:19:06Z" id="85175915">As you guessed it its bad data from feeds we get mostly,  i guess we'll end up adding validation in data ingestion side as doc_Values would be great addition there  in all other cases. 
</comment><comment author="rjernst" created="2015-03-23T23:34:30Z" id="85246663">@s1monw @jpountz any thoughts here?

@clintongormley In #8312 you had two extra ideas about disabling fielddata by default for `analyzed` strings and adding a token limit for `not_analyzed` strings. I think we should move these to separate issues? They are good ideas, but at least the latter I know will require a little thought in the mappers because the setting default becomes dynamic, which is similar to what made doc values by default a little complex here.
</comment><comment author="rjernst" created="2015-03-24T18:03:56Z" id="85624077">@jpountz I pushed changes to address your comments.
</comment><comment author="rjernst" created="2015-03-24T19:26:59Z" id="85658460">I pushed another change which no longer allows overriding `hasDocValues()`. This forces subclasses who want to always disable doc values to explicitly pass `false` to the super ctor. It also simplifies the API, so there is only 1 method regarding doc values that can be overridden.

It is also worthwhile to note that with this PR, `_uid` will have doc values enabled (because it is indexed and not analyzed, although maybe we should be explicit there and always pass `true` to the super ctor?).
</comment><comment author="rmuir" created="2015-03-24T23:23:40Z" id="85738291">&gt; It is also worthwhile to note that with this PR, _uid will have doc values enabled (because it is indexed and not analyzed, although maybe we should be explicit there and always pass true to the super ctor?).

docvalues of what type? Maybe we can defer this one, since there is still some stuff to sort out about it (e.g. parent-child)
</comment><comment author="rjernst" created="2015-03-25T01:26:59Z" id="85771302">&gt; &gt; It is also worthwhile to note that with this PR, _uid will have doc values enabled (because it is indexed and not analyzed, although maybe we should be explicit there and always pass true to the super ctor?).
&gt; &gt; docvalues of what type? Maybe we can defer this one, since there is still some stuff to sort out about it (e.g. parent-child)

Ok, I don't want to open up this can of worms. I'll set it to false explicitly, and we can enable it in future issue.
</comment><comment author="rjernst" created="2015-03-25T01:34:40Z" id="85774839">@rmuir @jpountz I pushed another commit addressing your comments.
</comment><comment author="jpountz" created="2015-03-25T15:21:39Z" id="86080416">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to put mapping on indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10208</link><project id="" key="" /><description>i have several big (200-300 gb) indices using one mapping.
when trying to delete one of them it took 1-2 minute/s
at that time new inserts keep getting : 
failed to put mapping on indices [[index_name]], type [type_name] 

how can i avoid those errors ?
is there a timeout parameter for the mapping ?
</description><key id="63527017">10208</key><summary>failed to put mapping on indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sdba2</reporter><labels><label>feedback_needed</label></labels><created>2015-03-22T12:22:45Z</created><updated>2015-04-26T20:04:07Z</updated><resolved>2015-04-26T20:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T18:32:27Z" id="89634632">HI @sdba2 

You deleted the index? or a mapping?  Deleting an index should be quick, but deleting a mapping could take a long time.  In fact, in master we've removed the ability to delete mappings.

If you deleted the index, then I'm surprised it took that long, but perhaps you have slow disks.  
</comment><comment author="clintongormley" created="2015-04-26T20:04:07Z" id="96429185">No more info. Closing. Feel free to reopen with more detilas
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to write to elasticsearch from hadoop hive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10207</link><project id="" key="" /><description>I've a table in hive and I want to write it's data to elastic index.

i created an index in elastic : 
curl -XPOST localhost:9200/aaa -d '{
"mappings" : {
"aaa_type" : {
"properties" : {
"f1" : {"type" : "string"},
"f2" : {"type" : "string"}
}
}
}
},

I've downloaded the elasticsearch-hadoop-2.0.2 and in hive i did :

1) add jar elasticsearch-hadoop-2.0.2.jar ;
2) create external table tbl1(f1 string, f2 string) stored by 'org.elasticsearch.hadoop.hive.EsStorageHandler' tblproperties('es.resource'='aaa/aaa_type' , 'es.index.auto.create'='true','es.nodes'='server1') ;
3) insert overwrite table tbl1 select f1,f2 from source_table;

command number 3 get the error :
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask

Then i've downloaded a new beta version elasticsearch-hadoop-2.1.0.Beta3 and instead of the above step 1 i've used : 
add jar elasticsearch-hadoop-2.1.0.BUILD-SNAPSHOT.jar;

but the error was the same!

what am i missing ?
</description><key id="63517299">10207</key><summary>failed to write to elasticsearch from hadoop hive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sdba2</reporter><labels /><created>2015-03-22T10:06:14Z</created><updated>2015-03-25T10:12:40Z</updated><resolved>2015-03-25T10:12:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:12:33Z" id="85970239">Hi @sdba2 I think this type of question would be better answered in our mailing list. We generally use github issues for actual bugs of feature requests. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>connectOnNetworkDisconnect doesn't have retry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10206</link><project id="" key="" /><description>connectOnNetworkDisconnect by default is turned off. When enabled, because it does an immediate reconnect without a wait, it most likely fail to reconnect to a node that has a transient network connectivity issues.

Proposed fix: introduce two new settings - (1) connectOnNetworkDisconnect Retry; (2) connectOnNetworkDisconnect Interval.

https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java
Source code - 

```
    if (connectOnNetworkDisconnect) {
        NodeFD fd = new NodeFD(node);
        try {
            transportService.connectToNode(node);
            nodesFD.put(node, fd);
            // we use schedule with a 0 time value to run the pinger on the pool as it will run on later
            threadPool.schedule(TimeValue.timeValueMillis(0), ThreadPool.Names.SAME, fd);
        } catch (Exception e) {
            logger.trace("[node  ] [{}] transport disconnected (with verified connect)", node);
            // clean up if needed, just to be safe..
            nodesFD.remove(node, fd);
            notifyNodeFailure(node, "transport disconnected (with verified connect)");
        }
    } else {
        logger.trace("[node  ] [{}] transport disconnected", node);
        notifyNodeFailure(node, "transport disconnected");
    }
```

thx
Daniel
</description><key id="63460070">10206</key><summary>connectOnNetworkDisconnect doesn't have retry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielli90</reporter><labels><label>feedback_needed</label></labels><created>2015-03-21T22:12:28Z</created><updated>2015-12-05T20:49:40Z</updated><resolved>2015-12-05T20:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-23T14:52:21Z" id="85034594">@danielli90 this is tricky. If the node is not connected, it will not get cluster state updates from the master. Originally we had this reconnect to deal with environments that sometimes close connections but the network weren't down. However, it turned out to be an issue in the cases where a network issue causes timeouts, in which case we just waited on the timeout to fire and the transport to disconnect and then we retried and waited for another timeout period, during which the node will not get any updates. This is why we changed it.

As a side note - if this happens, the node will ping for and rejoin the master, making sure it has the latest cluster state.

I'm wondering - why did you need this? I assume something didn't work well for you.
</comment><comment author="clintongormley" created="2015-12-05T20:49:40Z" id="162244517">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed duplicate timeout param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10205</link><project id="" key="" /><description>Minor change; I removed the duplicate timeout param in the bulk.json rest-api-spec. It causes errors when deserialising the params to a map.
</description><key id="63456644">10205</key><summary>Removed duplicate timeout param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">timschlechter</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.3.10</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-21T21:32:45Z</created><updated>2015-04-11T16:52:55Z</updated><resolved>2015-03-22T09:30:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timschlechter" created="2015-03-21T21:37:34Z" id="84455539">Oops, I submitted the PR before signing the CLA. 
I've signed it now, do I need to take any action on the PR?
</comment><comment author="javanna" created="2015-03-21T22:24:01Z" id="84462594">All good @timschlechter thanks a lot for your PR, will merge it soon.
</comment><comment author="timschlechter" created="2015-03-21T22:30:58Z" id="84462858">Ok, thanks!

I noticed the duplicate exists in several release branches, it was introduced more than ayear ago in  e613ecf9b4c825f0652e42141690abed8a92eac2

Would you like me to create a PR for each branch, or could someone with commit rights fix it directly in the other branches?
</comment><comment author="javanna" created="2015-03-22T09:31:07Z" id="84575173">Merged, thanks! I also backported it to 1.x, 1.5, 1.4 and 1.3 branches.
</comment><comment author="timschlechter" created="2015-03-22T09:33:31Z" id="84575238">Great, thank you for the quick merge and fixes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix string comparisons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10204</link><project id="" key="" /><description /><key id="63453196">10204</key><summary>Fix string comparisons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">slachiewicz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-21T20:26:34Z</created><updated>2015-05-29T15:59:23Z</updated><resolved>2015-03-23T09:55:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-22T09:52:42Z" id="84575871">Hi @slachiewicz in this specific case `==` works just fine, as long as we make sure that `TransportResponseHandler#executor` always returns exactly one of the constants listed in `ThreadPool.Names` class, which we use on the other side of the comparison. This is not really a string comparison though, in order for the result to be true the two variables need to reference the same object (the constant `ThreadPool.Names.SAME`), regardless of whether they contain the same string.

While `equals` would make a compiler warning go away, and be less error-prone if things change in the future, `==` is ok here. What do you think?
</comment><comment author="slachiewicz" created="2015-03-22T23:22:48Z" id="84732173">Yes, I agree with You, code works correct and we can only remove compiler warnings.
In NettyTransportTests we have code without any warnings  ie. Names.SAME.equals(..) call. 
I found also this same warning '==' in LocalTransport class (pull attached).
For me it's ok to close this defect if You not prefer to fix this - i found this while working with code merg for netty 4 update.
</comment><comment author="javanna" created="2015-03-23T09:55:44Z" id="84920512">Thanks @slachiewicz merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to derive xcontent from if ignore_unavailable specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10203</link><project id="" key="" /><description>This requests file:

``` json
{"index":"logstash-*"}
{"size":1,"sort":{"@timestamp":"desc"},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}}},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"30s","pre_zone":"+01:00","pre_zone_adjust_large_interval":true,"min_doc_count":0,"extended_bounds":{"min":1426964874564,"max":1426965774564}}}},"query":{"filtered":{"query":{"match_all":{}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"gte":1426964874566,"lte":1426965774566}}}],"must_not":[]}}}},"fields":["*","_source"],"script_fields":{},"fielddata_fields":["@timestamp"]}
```

And then I do:

```
curl -XPOST localhost:9200/_msearch?timeout=30000&amp;ignore_unavailable=true --data-binary @requests; echo
```

And I get:

```
[1] 1416
{"error":"ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1]","status":400}--data-binary: command not found
[1]+  Done                    curl -XGET localhost:9200/_msearch?timeout=30000
```

But if I do just

```
curl -XPOST localhost:9200/_msearch?timeout=30000 --data-binary @requests; echo
```

Then I get normal response

``` json
{"responses":[{"took":1,"timed_out":false,"_shards":{"total":0,"successful":0,"failed":0},"hits":{"total":0,"max_score":0.0,"hits":[]}}]}
```

This could be related to issue I am having here: https://github.com/elastic/kibana/issues/3404 

Any ideas?
</description><key id="63451822">10203</key><summary>Failed to derive xcontent from if ignore_unavailable specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fodoj</reporter><labels><label>non-issue</label></labels><created>2015-03-21T20:09:17Z</created><updated>2015-03-22T14:27:38Z</updated><resolved>2015-03-22T14:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-22T14:27:33Z" id="84617632">Hi @Fodoj , the shell sees the `&amp;` character in your curl call and thinks that it's the end of the command, which gets run in the background. The number `1416` that you get back is the pid assigned to the curl process. The error returned by elasticsearch is due to the missing body for that request. The other "command not found" error that you see is the rest of your command that gets executed as a second command.
You just need to wrap your url in single quotes: 

```
curl -XPOST 'localhost:9200/_msearch?timeout=30000&amp;ignore_unavailable=true' --data-binary @requests; echo
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String values are compared using '==' in MessageChannelHandler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10202</link><project id="" key="" /><description>In org.elasticsearch.transport.netty.MessageChannelHandler i found

if (handler.executor() == ThreadPool.Names.SAME) {
</description><key id="63450465">10202</key><summary>String values are compared using '==' in MessageChannelHandler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">slachiewicz</reporter><labels /><created>2015-03-21T19:53:09Z</created><updated>2015-03-22T09:53:35Z</updated><resolved>2015-03-22T09:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-22T09:53:35Z" id="84575900">Closing as duplicate of #10204 , PR that addresses this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rest-api-spec in its own repository?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10201</link><project id="" key="" /><description>I would like to add [rest-api-spec](https://github.com/elastic/elasticsearch/tree/master/rest-api-spec) as a submodule to my project. At the moment, it's located in the main elasticsearch repository, which forces me to add the whole elasticsearch-repo as a submodule.

Would it be possible to move the rest-api-spec folder to its own repository and add it as a submodule to the elasticsearch repository?
</description><key id="63449920">10201</key><summary>rest-api-spec in its own repository?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timschlechter</reporter><labels /><created>2015-03-21T19:49:13Z</created><updated>2015-03-22T11:25:42Z</updated><resolved>2015-03-22T11:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-22T10:18:28Z" id="84580167">Hi @timschlechter , the `rest-api-spec` dir used to be a separate project a while ago, and every language client would use it as a submodule exactly as you describe. We decided though to move it to the `elasticsearch` repo for a few reasons, if memory serves. First of all we wanted to make sure that the REST spec and tests stayed up-to-date with the actual elasticsearch code and the `elasticsearch` repo felt like a more natural location for it. Also, REST spec and tests evolve in the different `elasticsearch` repo branches, and we found ourselves needing to have exactly the same branches on the separate repo that we had. Last but not least, I think we weren't too happy with git submodules and some of their limitations and things are working better without for us.

I don't think we want to go back at this point, I hope that is not a blocker for you. If you do wish do checkout only that directory out of the elasticsearch repo, there should be a couple of ways to do it with git though.
</comment><comment author="timschlechter" created="2015-03-22T11:25:42Z" id="84588721">Keeping submodules in sync whith the main repo is indeed a painfull excersize, and one might ask why use submodules when main repo and submodule evolve at the same pace. So I totally understand the choices you made.

It's not a blocker for me, it just makes it a bit more convinient for other projects which want to use the spec files directly. But as said it's not a blocker for me, and I can't imagin it would be a blocker for anyone.

Thanks for the info, I'll close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verify shards index UUID when fetching started shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10200</link><project id="" key="" /><description>Today we simply fetch the shards metadata without verifying the
index UUID the shard belongs to. We recently added this UUID
to the shard state metadata. This commit adds verification
to the shard metadata fetching to prevent bringing shards
back into an index it doesn't belong to due to name collisions.
</description><key id="63357842">10200</key><summary>Verify shards index UUID when fetching started shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-03-21T04:10:34Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-23T13:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-23T09:59:14Z" id="84922690">@bleskes wanna take a look
</comment><comment author="bleskes" created="2015-03-23T11:26:28Z" id="84957016">Left one minor comment. LGTM otherwise.
</comment><comment author="s1monw" created="2015-03-23T13:03:52Z" id="84989975">pushed a new commit
</comment><comment author="bleskes" created="2015-03-23T13:04:19Z" id="84990116">LGTMx2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding -i</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10199</link><project id="" key="" /><description>without -i you never see the status:200 or status:404 messages
</description><key id="63350185">10199</key><summary>adding -i</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">coreydaley</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-21T03:08:10Z</created><updated>2015-03-21T07:57:10Z</updated><resolved>2015-03-21T07:57:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T07:57:10Z" id="84277092">Thanks @corey112358 merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding the -i option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10198</link><project id="" key="" /><description>If you leave out the -i, you never see the status:200 or status:404 that is returned
</description><key id="63349361">10198</key><summary>adding the -i option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coreydaley</reporter><labels /><created>2015-03-21T03:01:36Z</created><updated>2015-03-21T03:07:31Z</updated><resolved>2015-03-21T03:07:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Accessing other documents in top hit aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10197</link><project id="" key="" /><description>I wonder whether it is possible to access another document in top hit aggregations.
For example, I am adding a script_field to the top hits and I want to compute the value of this field based on the next document. So If I want to compute this field for the first hit, I need to access a certain field in the second hit. Is it possible to do so?
</description><key id="63335812">10197</key><summary>Accessing other documents in top hit aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ysholqamy</reporter><labels /><created>2015-03-21T00:15:45Z</created><updated>2015-04-04T18:10:01Z</updated><resolved>2015-04-04T18:10:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T18:10:01Z" id="89632279">Hi @usfhussam 

No, this wouldn't be possible.  This sounds like some post-processing phases that should be done on the client side.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle RejectedExecutionException in ClusterStateObserver on timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10196</link><project id="" key="" /><description>When nodes shutdown then any ClusterStatObserver is notified (onClose() called).
If the cluster state observer executes code which throws RejectedExecutionException then this
is not caught automatically.
This is problematic for example when one wants to send something via a TransportChannel
on close but this operation is rejected because the node is shutting down.
Instead, rejections should be caught and ignored.

I have this problem in #10172 (https://github.com/elastic/elasticsearch/pull/10172/files#diff-60f857f2de28d320835e75281ae52146R371). It makes the tests fail occasionally.
</description><key id="63333088">10196</key><summary>Handle RejectedExecutionException in ClusterStateObserver on timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-20T23:46:19Z</created><updated>2015-03-21T17:37:22Z</updated><resolved>2015-03-21T15:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-03-21T03:44:57Z" id="84238553">Don't bother reviewing  yet, tests fail.
</comment><comment author="brwe" created="2015-03-21T15:37:53Z" id="84374019">I was mixing up rejections this change does not help. Will close.
</comment><comment author="s1monw" created="2015-03-21T17:22:06Z" id="84400407">i think this change is good anyways?
</comment><comment author="brwe" created="2015-03-21T17:37:22Z" id="84407320">The NotifyTimeout seems to be only added here: https://github.com/elastic/elasticsearch/blob/1.x/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L232 and there we catch EsRejectedExecutionException that comes from the ThreadPoolExecutor anyway. I do not yet see how the change would improve anything?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring: combine similar GroupShardsIterators in RoutingTable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10195</link><project id="" key="" /><description>Three RoutingTable methods
- GroupShardsIterator activePrimaryShardsGrouped()
- GroupShardsIterator allActiveShardsGrouped()
- GroupShardsIterator allAssignedShardsGrouped()

Are almost similar internally and as far as I can see only differ in one line like `if (shardRouting.active())` where they use different criteria to include that particular shardRouting in the result. These methods could possibly be combined into one where the criterion to filter on is passed in as an argument, eg.

```
public GroupShardsIterator allAssignedShardsGrouped(String[] indices, boolean includeEmpty, boolean includeRelocationTargets, ShardRoutingCriterion criterion) {
...
            if (criterion.includes(shardRouting) {
                  ....
            }
}
```

This would avoid code duplication and make testing RoutingTable easier.
</description><key id="63308899">10195</key><summary>Refactoring: combine similar GroupShardsIterators in RoutingTable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-20T21:13:39Z</created><updated>2015-08-25T09:20:53Z</updated><resolved>2015-08-25T09:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-08-25T09:20:53Z" id="134537265">Has been addressed by by #13082, so closing here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[search] make sure search does not fail when shard is in post recovery after relo...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10194</link><project id="" key="" /><description>...cating

Just opening this to discuss. I wrote a test that reproduces #9421 reliably and also have a fix but we need to figure out if this is the right way to go.
</description><key id="63308348">10194</key><summary>[search] make sure search does not fail when shard is in post recovery after relo...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-20T21:09:46Z</created><updated>2015-09-02T08:04:25Z</updated><resolved>2015-09-02T08:04:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mfussenegger" created="2015-05-18T15:08:12Z" id="103090219">Just curious if there is an update on this? Run into this issue as well.
</comment><comment author="brwe" created="2015-09-02T08:04:12Z" id="136970640">@mfussenegger Sorry for the late reply. This will be fixed in 2.0, see https://github.com/elastic/elasticsearch/pull/13246
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to retrieve on-disk templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10193</link><project id="" key="" /><description>Templates that go into `{config}/templates` are usable, however they do not show up in the templates API (`GET 0:9200/_templates`).

It would be great if there were a way to retrieve the templates via the REST API.

Follow-up to #10160
</description><key id="63305552">10193</key><summary>Add ability to retrieve on-disk templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index Templates</label><label>adoptme</label><label>discuss</label></labels><created>2015-03-20T20:55:05Z</created><updated>2016-01-10T14:02:22Z</updated><resolved>2015-12-05T20:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T20:48:47Z" id="162244471">Disk based templates are no longer supported.  Closing
</comment><comment author="drekbour" created="2015-12-18T10:53:54Z" id="165746716">@clintongormley "Disk based templates are no longer supported" I cannot find where it mentioned this in the release notes? What is the recommended replacement?

https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-2.0.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-2.1.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html
</comment><comment author="ogirardot" created="2015-12-28T11:58:03Z" id="167552042">and if I may, why ??
It's a key feature to be able to "provision" an Elasticsearch cluster in a programatically _easy_ way.
</comment><comment author="alexjhart" created="2016-01-04T06:13:28Z" id="168591081">I agree this was a useful feature. 
</comment><comment author="TigranTsat" created="2016-01-05T15:47:31Z" id="169040314">Was a useful feature...
</comment><comment author="clintongormley" created="2016-01-10T13:50:48Z" id="170348856">@drekbour it is mentioned here: 
https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking_20_index_api_changes.html#_file_based_index_templates

It was removed here (https://github.com/elastic/elasticsearch/pull/10870) and disk based mappings was removed here (https://github.com/elastic/elasticsearch/pull/10870) 

It was removed as part of a cleanup of all disk based resources (except node-level config and, unfortunately, scripts https://github.com/elastic/elasticsearch/pull/10870#issuecomment-97424660).  Elasticsearch is API driven.  File based resources can differ between nodes, can't be retrieved or checked via the cluster etc.  Instead, templates can be setup across all nodes in the cluster by a single PUT request.
</comment><comment author="ogirardot" created="2016-01-10T14:02:22Z" id="170350795">I understand, but with provisioning tools like Ansible, that means that we need to wait for the node we're currently adding to be up to PUT the proper templates / mappings / configs ... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TokenStream expanded to 512 finite strings. Only &lt;= 256 finite strings are supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10192</link><project id="" key="" /><description>Have this exception during indexation documents for suggest completion:

```
    suggest_top_products:
        client: default
        index_name: my_index_suggest_top_products
        use_alias: true
        settings:
            number_of_shards:   6
            number_of_replicas: 0
            index:
                analysis:
                    analyzer:
                        completion:
                            type: custom
                            tokenizer: whitespace
                            filter: [short_word_delimiter, lowercase, asciifolding, apostrophe, op_synonyms, snow_english]
                            char_filter: html_strip

                    filter:
                        snow_english:
                            type: snowball
                            language: English

                        short_word_delimiter:
                            type: word_delimiter
                            generate_word_parts: true
                            generate_number_parts: true
                            catenate_words: true
                            catenate_numbers: true
                            catenate_all: true
                            split_on_case_change: true
                            preserve_original: true

                        op_synonyms:
                            type: synonym
                            ignore_case: true
                            expand: true
                            synonyms:
                              - "riflescopes, rifle scopes"
                              - "riflescope, rifle scope"

        types:
            auto_suggest:
                mappings:
                    text: {type: string, index: not_analyzed}
                    top_products:
                        type: completion
                        index_analyzer: completion
                        search_analyzer: completion
                        payloads: true
                        preserve_position_increments: false
                        preserve_separators: false
```

Document is:
{"text":"Sure-Fire Z59 Click-On Lock-Out Tailcap for C2 \/ C3 \/ D2 \/ D3 \/ G3 \/ M2 \/ M3 \/ M3T \/ M4 Flashlights","top_products":{"input":["Sure-Fire Z59 Click-On Lock-Out Tailcap for C2 C3 D2 D3 G3 M2 M3 M3T M4 Flashlights","C2 C3 D2 D3 G3 M2 M3 M3T M4 Flashlights"],"output":"Sure-Fire Z59 Click-On Lock-Out Tailcap for C2 \/ C3 \/ D2 \/ D3 \/ G3 \/ M2 \/ M3 \/ M3T \/ M4 Flashlights","weight":"144","payload":{"type":"top_product","product_id":"32378","weight":"144","primary_image":"opplanet-surefire-tailcaps-switches-z59","rating":"0.00","review_count":"0","url":"surefire-tailcaps-switches-z59"}}}
</description><key id="63296660">10192</key><summary>TokenStream expanded to 512 finite strings. Only &lt;= 256 finite strings are supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maximTarleckiy</reporter><labels /><created>2015-03-20T19:59:34Z</created><updated>2015-04-20T14:29:37Z</updated><resolved>2015-04-04T17:57:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T17:57:48Z" id="89630647">Duplicate of #9466. Closing in favour of #8909
</comment><comment author="missinglink" created="2015-04-14T12:11:18Z" id="92785773">Sorry, late to the party on this one, I think you'll find that this is not a duplicate of #9466 but instead related to how your TokenStream expands synonyms.

For example, if you have a string which has ~26 tokens and they are expanded x10 times by synonym expansion or another method you get to a point where you have over 256 finite strings and :boom:

We have also encountered this issue in https://github.com/pelias/pelias/issues/33 and you should be able to mask/fix the error by increasing your `max_token_length` setting for the relevant analyzer.
</comment><comment author="maximTarleckiy" created="2015-04-20T12:48:49Z" id="94443533">clintongormley
How it can be related to #9466? I have not context here.
Also I have this error with context, but I indexed context &lt; 256 value per document (max 111)!
But context have two keys:
context:
                                category_id:
                                    type: category
                                    default: "default"
                                department_id:
                                    type: category
                                    default: "default"
category_id has 111 values
department_id has 10 values

missinglink
If I ever remove 'op_synonyms' from token filter this error still appears.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: clearer MLT documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10191</link><project id="" key="" /><description>Backport of MLT 2.0 clearer documentation (#9351) for 1.x.
</description><key id="63288338">10191</key><summary>Docs: clearer MLT documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>docs</label><label>v1.5.1</label><label>v1.6.0</label></labels><created>2015-03-20T19:31:12Z</created><updated>2015-03-23T20:36:17Z</updated><resolved>2015-03-23T20:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T20:32:55Z" id="84137537">+1 for backporting to 1.5 we don't need to do this before the release really so no rush here since docs are build on demand
</comment><comment author="clintongormley" created="2015-03-22T08:36:41Z" id="84558346">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Add serial differencing aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10190</link><project id="" key="" /><description>This is still a WIP, just putting it up for discussion.  We may want to roll this functionality into a different Agg.
### Serial Differencing

Serial differencing (or just differencing) is a technique where values in a time series are subtracted from itself at different time lags or periods.  For example, the datapoint f(x) = f(x&lt;sub&gt;t&lt;/sub&gt;) - f(x&lt;sub&gt;t-n&lt;/sub&gt;), where `n` is the period being used.

A period of 1 is equivalent to a derivative: it is simply the change from one point to the next.  Single periods are useful for removing constant, linear trends.

Single periods are also useful for transforming data into a stationary series.  In this example, the Dow Jones is plotted over ~250 days.  The raw data is not stationary, which would make it difficult to use with some techniques.  

But once we plot the first-difference, it becomes a stationary series (we know this because the first difference is randomly distributed around zero, and doesn't seem to exhibit any pattern/behavior).  The transformation reveals that the dataset is a random-walk model, which allows us to use further analysis.

![screen shot 2015-03-19 at 10 42 04 am](https://cloud.githubusercontent.com/assets/1224228/6756802/b4ff11b4-cf02-11e4-86e5-26d36dc9114c.png)

Larger periods can be used to remove seasonal / cyclic behavior.  In this example, a population of lemmings was synthetically generated with a sine wave + constant linear trend + random noise.  The sine wave has a period of 30 days.

The first-difference removes the constant trend, leaving just a sine wave.  The 30th-difference is then applied to the first-difference to remove the cyclic behavior, leaving a stationary series which is amenable to other analysis.

![screen shot 2015-03-19 at 12 15 06 pm](https://cloud.githubusercontent.com/assets/1224228/6756873/36be2cd0-cf03-11e4-9141-cdc96a51c118.png)
### API

``` json
{
   "aggs": {
      "my_date_histo": {
         "date_histogram": {
            "field": "timestamp",
            "interval": "day"
         },
         "aggs": {
            "the_sum": {
               "sum": {
                  "field": "lemmings"
               }
            },
            "first_difference": {
               "diff": {
                  "bucketsPath": "the_sum",
                  "periods" : 1
               }
            },
            "thirtieth_difference": {
               "diff": {
                  "bucketsPath": "first_difference",
                  "periods" : 30
               }
            }
         }
      }
   }
}
```
### TODO
- Tests :)
- Javadocs, cleanup, etc
- How does this interact with Derivative?  The first difference is technically a derivative.  We could roll this behavior into Deriv, but if Deriv ever gets time normalization this will get weird.  We could also tell people to just use Diff with first-period, but I quite like having a single, simple Deriv agg
</description><key id="63268798">10190</key><summary>Aggregations: Add serial differencing aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">polyfractal</reporter><labels /><created>2015-03-20T17:22:57Z</created><updated>2016-02-25T14:03:59Z</updated><resolved>2015-05-15T21:04:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-03-24T17:19:59Z" id="85605951">Just a note: I think I want to rename `periods` parameter to `lags`.  More standardized naming, and I think a bit more descriptive.
</comment><comment author="colings86" created="2015-03-25T03:41:11Z" id="85816518">+1 to renaming `periods` to `lags`, but what is the affect of having multiple lags? or is that not the reason why it's plural? We can still only get one output per bucket right?
</comment><comment author="polyfractal" created="2015-03-25T14:19:06Z" id="86047015">Oh, good point.  It should be `lag`.  I think supporting multiple lags would be very confusing and unnecessary.
</comment><comment author="colings86" created="2015-03-25T14:23:30Z" id="86048240">Agreed, we should not try to support multiple lags in one reducer
</comment><comment author="polyfractal" created="2015-05-15T21:04:06Z" id="102525351">Closing since this is against an outdated branch (`feature/aggs_2_0`)
</comment><comment author="kingaj" created="2016-02-25T14:02:19Z" id="188795374">any java example if have then please give me a link please
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Schedule transport ping interval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10189</link><project id="" key="" /><description>Sometimes, when using transport client for example, through a load balancer, there is a need to send a scheduled ping message to keep each channel alive.

Add support for `transport.ping_schedule`, which controls the schedule (-1 for disabled) at which a ping message will be sent. For transport client case, it gets enabled automatically since almost always this is the desired behavior.

We use the same 6 bytes header format for the ping message, with ES header and -1 for data length for ping message, and simply continue to process the next messages once this is encountered.
</description><key id="63262028">10189</key><summary>Schedule transport ping interval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-20T16:45:43Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-21T17:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:39:24Z" id="84112160">I left some comments
</comment><comment author="kimchy" created="2015-03-20T20:03:17Z" id="84130554">@s1monw addressed your comments, ready for another round
</comment><comment author="s1monw" created="2015-03-20T22:52:50Z" id="84175518">left one comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove duplicated consistency level and replication type setters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10188</link><project id="" key="" /><description>`setConsistencyLevel` and `setReplicationType` are already present in the base class `ShardReplicationOperationRequestBuilder`. They are not needed in `DeleteRequestBuilder` and `IndexRequestBuilder`.
</description><key id="63260230">10188</key><summary>Remove duplicated consistency level and replication type setters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-20T16:35:24Z</created><updated>2015-05-30T10:52:03Z</updated><resolved>2015-03-21T09:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T16:43:15Z" id="84065165">this is 1.6 only I removed those all in 2.0 already
</comment><comment author="javanna" created="2015-03-20T16:48:31Z" id="84066169">@s1monw async replication has been removed from master, write consistency is still there: https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java#L259 .

That is why this PR is against 1.x, when cherry-picking to master I will take care of conflicts.
</comment><comment author="s1monw" created="2015-03-20T17:49:20Z" id="84086995">kk
</comment><comment author="s1monw" created="2015-03-20T19:40:07Z" id="84112354">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SimpleNettyTransportTests#testTracerLog stalls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10187</link><project id="" key="" /><description>`SimpleNettyTransportTests#testTracerLog` stalls on my macbook, I saw this a couple of times today while running the whole core suite from command line. The suite timeout kicks in at some point and stops it.
</description><key id="63247059">10187</key><summary>SimpleNettyTransportTests#testTracerLog stalls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label></labels><created>2015-03-20T15:17:56Z</created><updated>2015-03-23T13:40:42Z</updated><resolved>2015-03-23T13:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-20T15:18:25Z" id="84045501">Assigning this to @bleskes as he has the logs and said he knows what is going on. Thanks @bleskes !
</comment><comment author="bleskes" created="2015-03-23T09:10:46Z" id="84902512">example: http://build-us-00.elastic.co/job/es_core_15_centos/64/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recover from DNS outage on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10186</link><project id="" key="" /><description>If you start an elasticsearch node, that has trouble with DNS, it will never recover from this and continue spitting exceptions, even if the DNS problems are fixed. The reason for this is, that in `UnicastZenPing` constructor we have the following code:

```
        for (String host : hosts) {
            try {
                TransportAddress[] addresses = transportService.addressesFromString(host);
                // we only limit to 1 addresses, makes no sense to ping 100 ports
                for (int i = 0; (i &lt; addresses.length &amp;&amp; i &lt; LIMIT_PORTS_COUNT); i++) {
                    configuredTargetNodes.add(new DiscoveryNode(UNICAST_NODE_PREFIX + unicastNodeIdGenerator.incrementAndGet() + "#", addresses[i], version.minimumCompatibilityVersion()));
                }
            } catch (Exception e) {
                throw new ElasticsearchIllegalArgumentException("Failed to resolve address for [" + host + "]", e);
            }
        }
        this.configuredTargetNodes = configuredTargetNodes.toArray(new DiscoveryNode[configuredTargetNodes.size()]);
```

`transportService.addressesFromString(host)` calls `InetSocketAddress` which in turn tries to resolve the applied hostname and fails, thus marking returning `InetSocketAddress.isResolved()` as `false` - forever. This method is used by netty to check if connecting to the endpoint makes sense at all.
### How to reproduce locally

If you want to reproduce, take this config and disable network on your system (will work when network is enabled, as `localhost.spinscale.de` resolves to 127.0.0.1.

```
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["localhost.spinscale.de:9300" ]
```
### Fix proposal
1. First, remove the exception output, catch `UnresolvedAddressException` in `UnicastZenPing.sendPings()` and log a single line, telling the problem including the hostname
2. Make sure the `InetAddress` and its `isResolved()` method is not cached. Not sure what is the best approach here, either create the InetSocketAddress object before each connect try or maybe there are some configurable properties around this
</description><key id="63237067">10186</key><summary>Recover from DNS outage on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Discovery</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-20T14:18:27Z</created><updated>2017-03-17T23:21:06Z</updated><resolved>2017-03-17T23:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="turtlemonvh" created="2015-05-29T23:55:42Z" id="106963638">Ahhh.  This hit me hard the last few days.  I spent the last few hours convinced this was [a java settings issue](http://stackoverflow.com/questions/1256556/any-way-to-make-java-honor-the-dns-caching-timeout-ttl).

We're using consul to manage elasticsearch running in docker containers and we're using consul's built-in dns server.  The dns entries are only added a few seconds after the container starts up, so they aren't resolving when elasticsearch first checks on them.

A fix to this would be very helpful.
</comment><comment author="jasontedor" created="2017-03-17T23:21:01Z" id="287494537">We resolve addresses on every pinging round now, and an unresolved host will not abort pinging.</comment><comment author="jasontedor" created="2017-03-17T23:21:06Z" id="287494547">Closed by #21630</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase default rate limiting for snapshot, restore and recovery to 40 MB/sec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10185</link><project id="" key="" /><description>Lucene's RateLimiter can do too much sleeping ("over rate limiting") if you call it on small values for bytes.

This fixes recovery source and target to only call every RateLimiter.minPauseCheckBytes.

I think at default settings this is not hurting recovery speed too much, because we rate limit at 20 MB/sec by default, and use a buffer size of 512 KB, but we do still call on small values, e.g. for small files or for the last small chunk of a large file.

This is similar to #6018, but for recovery instead of store throttling during merging.

This change does mean that files &lt; minPauseCheckSize (100 KB @ 20 MB/sec) won't visit the rate limiter but I think that's OK.

However, @bleskes was worried this change might weaken tests?
</description><key id="63231250">10185</key><summary>Increase default rate limiting for snapshot, restore and recovery to 40 MB/sec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-20T13:45:00Z</created><updated>2015-05-30T10:56:48Z</updated><resolved>2015-03-20T20:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-20T15:45:01Z" id="84052685">Left some minor comments. The test issue is not a concern (anymore) because RateLimiter#getMinPauseCheckBytes is updated based on the current throttle setting (so it responds well to small quotas, like the tests use)
</comment><comment author="s1monw" created="2015-03-20T15:48:58Z" id="84053480">can we have a `RateLimiterWrapper` that does this for us? I wonder why this is not build in?
</comment><comment author="bleskes" created="2015-03-20T16:03:11Z" id="84056276">PS. This code changed slightly this morning. Might make sense to rebase before the next review cycle
</comment><comment author="mikemccand" created="2015-03-20T16:09:03Z" id="84057364">&gt; can we have a RateLimiterWrapper that does this for us? I wonder why this is not build in?

I had explored this in https://issues.apache.org/jira/browse/LUCENE-5641 but the concurrency quickly got non-trivial, and since in general this .pause method can be called with very small values (e.g. 1-5 bytes from writeVInt or something) I was concerned about adding too much cost here.

It was also (usually) easy for callers to do their own aggregation since usage is already obviously thread private in Lucene.
</comment><comment author="mikemccand" created="2015-03-20T17:50:50Z" id="84087271">I pushed another iteration, tests pass: I cutover both recovery source
and target to AtomicLongs.

There is a small concurrency bug: if more than one thread checks for
pausing at once, then too much pausing could temporarily happen, but
it would later "catch up" by doing less pausing in the future (unless
the bug struck right at the end of recovery) ... I think it's
acceptable tradeoff for the simpler code.

I also rebased to master, resolved conflicts ... @bleskes can you
double-check I did this correctly?  I had to force push!

I don't think we should add a RateLimiterWrapper to ES nor fold this
"thread safety" into Lucene's RateLimiter: we already have too many
wrapper classes, and anyway I think it's already bad that ES does
hardwired 20 MB/sec rate limiting by default ... so many users must
disable/increase this throttling because their recovery or snapshot
restores or rolling restarts take so long.  I don't want to make the
situation worse by adding API surface area lending any credibility to
rate limiting.
</comment><comment author="rmuir" created="2015-03-20T17:55:21Z" id="84088333">&gt; anyway I think it's already bad that ES does
&gt; hardwired 20 MB/sec rate limiting by default

we had this same discussion about a year ago. time to apply moore's law at least. Lets increase to 40MB.
</comment><comment author="s1monw" created="2015-03-20T18:08:16Z" id="84090842">&gt; we had this same discussion about a year ago. time to apply moore's law at least. Lets increase to 40MB.

+1
</comment><comment author="mikemccand" created="2015-03-20T18:35:16Z" id="84096183">OK I pushed another commit increase default throttling to 40 MB/sec....
</comment><comment author="s1monw" created="2015-03-20T19:14:38Z" id="84103972">LGTM
</comment><comment author="bleskes" created="2015-03-20T19:32:42Z" id="84110192">LGTM. Thx @mikemccand!

Can we change the title of the PR to say it modifies default recovery throttling to 40mb? It's the important change here. IMHO the better rate limiting can go in the body. 
</comment><comment author="mikemccand" created="2015-03-20T19:50:44Z" id="84119721">@bleskes kk will do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove benchmark api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10184</link><project id="" key="" /><description>Remove benchmark api from master, it's being worked on in https://github.com/elastic/elasticsearch/tree/feature/bench branch.
</description><key id="63218590">10184</key><summary>Remove benchmark api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Benchmark</label><label>non-issue</label><label>v2.0.0-beta1</label></labels><created>2015-03-20T12:18:45Z</created><updated>2015-03-22T09:23:40Z</updated><resolved>2015-03-21T09:53:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:40:32Z" id="84112440">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot sometimes lose some shards data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10183</link><project id="" key="" /><description>I used 1.4.2 version of elasticsearch,  each index is 10 shards,  when use snapshot to backup indices sometimes lose shards. 

Some of the information listed below&#65306;

```
[elasticsearch@lin-mongo-65-182 indices] curl -XPUT 'localhost:9200/_snapshot/log_backup/snapshot-logstash-2015.03.15_20150320-160201?wait_for_completion=true&amp;pretty' -d '{
       "indices":"logstash-2015.03.15"
 }'
{
  "snapshot" : {
    "snapshot" : "snapshot-logstash-2015.03.15_20150320-160201",
    "indices" : [ "logstash-2015.03.15" ],
    "state" : "SUCCESS",
    "start_time" : "2015-03-20T08:35:41.641Z",
    "start_time_in_millis" : 1426840541641,
    "end_time" : "2015-03-20T08:36:23.932Z",
    "end_time_in_millis" : 1426840583932,
    "duration_in_millis" : 42291,
    "failures" : [ ],
    "shards" : {
      "total" : 10,
      "failed" : 0,
      "successful" : 10
    }
  }
}
```

result is snapshot successful for 10 shards; but  when i restore this snapshot, ES report many exception:

```
[2015-03-20 16:09:12,088][WARN ][indices.cluster          ] [lin-65-181] [logstash-2015.03.15][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2015.03.15][0] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [logstash-2015.03.15][0] restore failed
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
    ... 3 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [logstash-2015.03.15][0] failed to restore snapshot [snapshot-logstash-2015.03.15_20150320-160201]
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:165)
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
    ... 4 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [logstash-2015.03.15][0] failed to read shard snapshot file
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$Context.loadSnapshot(BlobStoreIndexShardRepository.java:319)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:709)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:162)
    ... 5 more
Caused by: java.io.FileNotFoundException: /home/elasticsearch/ES_backup/log/indices/logstash-2015.03.15/0/snapshot-snapshot-logstash-2015.03.15_20150320-160201 (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146)
    at org.elasticsearch.common.blobstore.fs.FsBlobContainer.openInput(FsBlobContainer.java:79)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$Context.loadSnapshot(BlobStoreIndexShardRepository.java:316)
    ... 7 more
```

I jump to the backup directory, found miss "0" directory for index "logstash-2015.03.15".

```
[elasticsearch@lin-mongo-65-182 indices]$ pwd
/home/elasticsearch/ES_backup/log/indices
[elasticsearch@lin-mongo-65-182 indices]$ tree -L 2 ./
./
&#9500;&#9472;&#9472; logstash-2015.03.14
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 0
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 1
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 2
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 3
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 4
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 5
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 6
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 7
&#9474;&#160;&#160; &#9500;&#9472;&#9472; 8
&#9474;&#160;&#160; &#9492;&#9472;&#9472; 9
&#9492;&#9472;&#9472; logstash-2015.03.15
    &#9500;&#9472;&#9472; 1
    &#9500;&#9472;&#9472; 2
    &#9500;&#9472;&#9472; 3
    &#9500;&#9472;&#9472; 4
    &#9500;&#9472;&#9472; 5
    &#9500;&#9472;&#9472; 6
    &#9500;&#9472;&#9472; 7
    &#9500;&#9472;&#9472; 8
    &#9500;&#9472;&#9472; 9
    &#9492;&#9472;&#9472; snapshot-snapshot-logstash-2015.03.15_20150320-160201
```
</description><key id="63206976">10183</key><summary>Snapshot sometimes lose some shards data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aqlu</reporter><labels><label>non-issue</label></labels><created>2015-03-20T10:42:24Z</created><updated>2015-03-21T17:22:35Z</updated><resolved>2015-03-21T14:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-20T16:52:35Z" id="84067444">@aqlu thanks for reporting. How frequent is this error? Do you use a specific tool to do the snapshots?

I didn't reproduce this issue. Can you please try to reproduce it with TRACE log enabled for the snapshots and post the output here?

To enable TRACE logging:
- edit logging.yml
- add `snapshots: TRACE` under `logger` section
- you should see traces like this: `[TRACE][snapshots                ] [Wind Warrior] [my_backup:snapshot_1] Updating shard [[library2][0]] with status [SUCCESS]`
</comment><comment author="aqlu" created="2015-03-21T08:21:55Z" id="84278994">@tlrx Thanks for you reply. I' m so sorry, it's not a ES bug, It's some problems in my environment about NFS on one ES node, some data of shards is saved into local disk. 
Thanks for your advice again!
</comment><comment author="tlrx" created="2015-03-21T14:30:20Z" id="84358076">Ok, thanks to let us know :)

I close the issue, feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark api in master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10182</link><project id="" key="" /><description>We currently have the benchmark api in the master branch. We are going to refactor it once #6914 is in, which may take a while. I don't think it will make it into `2.0`.

This issue is just a reminder to back out the current benchmark api code before we release `2.0`, unless ready by then.
</description><key id="63192367">10182</key><summary>Benchmark api in master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>blocker</label></labels><created>2015-03-20T09:09:32Z</created><updated>2015-06-08T13:40:52Z</updated><resolved>2015-03-21T09:53:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-20T09:11:00Z" id="83960507">Can we remove the code now and just put it in a separate branch?
</comment><comment author="javanna" created="2015-03-20T09:12:50Z" id="83960705">Sure I can take care of this. WIll create a new branch from master and then remove any reference of it from master. Makes sense?
</comment><comment author="bleskes" created="2015-03-20T09:36:40Z" id="83969029">+1 

&gt; On 20 Mar 2015, at 10:13, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; Sure I can take care of this. WIll create a new branch from master and then remove any reference of it from master. Makes sense?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2015-03-20T10:39:00Z" id="83983607">I did some digging, we already have a branch for this, which is ahead of master, where the refactoring of the benchmark api was taking place till it stalled waiting for #6914 : 

https://github.com/elastic/elasticsearch/tree/feature/bench

No need to recreate a new branch then, everything should be there. Will just go ahead and remove the benchmark api from master if nobody objects.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Seeing too many merge threads (Elasticsearch 1.3.4)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10181</link><project id="" key="" /><description>There are 562 merge threads in a hprof dump of our elasticsearch process, we have no merge related configs, the default max number of merge threads seems to be 3 (http://www.elastic.co/guide/en/elasticsearch/reference/1.3/index-modules-merge.html),
then how could I control the number of concurrent merge threads?

![image](https://cloud.githubusercontent.com/assets/3242209/6748485/0f287612-cf21-11e4-93db-e50d854470c9.png)
</description><key id="63190643">10181</key><summary>Seeing too many merge threads (Elasticsearch 1.3.4)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mashudong</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2015-03-20T08:50:00Z</created><updated>2015-08-26T19:14:00Z</updated><resolved>2015-08-26T19:14:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T17:43:07Z" id="89625475">@mikemccand any ideas about this?
</comment><comment author="mikemccand" created="2015-04-04T22:04:52Z" id="89671695">Egads, that's way too many running merge threads!  @mashudong can you pull all threads (use /_nodes/hot_threads but pass threads=10000 so we get all of them not just the top 3 hot ones)?
</comment><comment author="saiprasadmishra" created="2015-04-14T23:16:07Z" id="93106603">Hi All

I am seeing a similar issue in our ES version 1.3.2 also. Did you figure out why it has so many lucene merge threads....looks like threads are just started and not getting stopped.
</comment><comment author="saiprasadmishra" created="2015-04-14T23:18:07Z" id="93107117">@mikemccand I am seeing 100s of lucene merge threads apart from 2 [merge] threads even though max_thread count was 1

curl -XPUT 'localhost:9201/index_v1/_settings' -d '
{
  "index": {
    "merge.scheduler.max_thread_count": 1
  }
}'
</comment><comment author="mikemccand" created="2015-04-15T10:40:13Z" id="93316647">@saiprasadmishra can you also post all threads?
</comment><comment author="clintongormley" created="2015-04-23T11:17:54Z" id="95544928">@saiprasadmishra Please could you post the output of this command:

```
curl -XGET "http://localhost:9200/_nodes/hot_threads?threads=10000"
```
</comment><comment author="jpountz" created="2015-08-26T19:14:00Z" id="135143588">Closing due to lack of feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark api: removed leftovers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10180</link><project id="" key="" /><description>Removes any mention of the benchmark api from 1.x, which has been backed out a while ago, but there were still some references to it here and there.
</description><key id="63185737">10180</key><summary>Benchmark api: removed leftovers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Benchmark</label><label>breaking</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label></labels><created>2015-03-20T08:16:08Z</created><updated>2015-03-20T12:34:20Z</updated><resolved>2015-03-20T09:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-20T08:45:10Z" id="83957761">LGTM. I take it that nothing needs be done on master?
</comment><comment author="javanna" created="2015-03-20T09:00:08Z" id="83959354">thanks for the review @bleskes ! this is only for 1.x, master still contains the benchmark api as a whole.
</comment><comment author="javanna" created="2015-03-20T09:00:32Z" id="83959414">I wonder if we should backport this to `1.5` and `1.4` though
</comment><comment author="bleskes" created="2015-03-20T09:03:11Z" id="83959727">+1 to back port to 1.5 and 1.4 . Code impact is minimal and it's important to remove the docs imho
</comment><comment author="javanna" created="2015-03-20T10:59:57Z" id="83986060">Marked as breaking for `elasticsearch-test` as it changes the constructor signature for `InternalTestCluster` and removes the `numBenchNodes` method from `TestCluster`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wipe shard state before switching recovered files live</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10179</link><project id="" key="" /><description>Today we leave the shard state behind even if a recovery is half finished
this causes in rare conditions shards to be recovered and promoted as
primaries that have never been fully recovered.

Closes #10053
</description><key id="63170012">10179</key><summary>Wipe shard state before switching recovered files live</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-20T05:55:47Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-20T15:43:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T05:56:45Z" id="83922095">@bleskes if you have a few moments
</comment><comment author="bleskes" created="2015-03-20T09:35:32Z" id="83968898">LGTM. Left a minor comment
</comment><comment author="dakrone" created="2015-03-26T18:19:49Z" id="86654773">@s1monw it looks like this was just merged to master, is that correct? Can you label the PR?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix #9469, fix #7408, fix #8041, fix #8285</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10178</link><project id="" key="" /><description /><key id="63161671">10178</key><summary>Fix #9469, fix #7408, fix #8041, fix #8285</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">stephenfournier</reporter><labels /><created>2015-03-20T04:36:35Z</created><updated>2015-03-26T15:00:31Z</updated><resolved>2015-03-26T15:00:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-20T19:48:16Z" id="84116737">@Myll this looks pretty good in general, but can I ask you to split it up into 3 different pull requests? One for #9469, one for #8041, and one for #8285? That makes it easier to review and merge in. It looks like #7408 has been fixed already.

Also, instead of commenting on the commit with "Fixes #nnnn", you can put it in the commit message and it will automatically be linked and the issue will be fixed when I merge it.
</comment><comment author="stephenfournier" created="2015-03-20T21:10:43Z" id="84148071">Ok, will do.
</comment><comment author="dakrone" created="2015-03-26T15:00:31Z" id="86554689">Closing this since @Myll will follow up with individual pull requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster API feature request: Node Startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10177</link><project id="" key="" /><description>We have Node Shutdown API points. http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-shutdown.html

But to instrument the Rolling Restart discussed at the bottom of the page^^ -- I need a Node Start API as well.

Please consider writing a Node Start API  -- this will make cluster management possible from the Cluster API 

Cheers,
</description><key id="63154454">10177</key><summary>Cluster API feature request: Node Startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jzinner</reporter><labels><label>discuss</label></labels><created>2015-03-20T03:56:08Z</created><updated>2015-04-04T17:35:09Z</updated><resolved>2015-04-04T17:35:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T17:35:09Z" id="89624638">How could we call node-start when the node is not running?

There used to be a restart API (briefly), but it was removed because it was problematic.  I'm not sure what the problem was, but it was big enough that the feature wasn't reliable, and it wouldn't help with upgrades anyhow.

I don't believe that this can be provided as part of Elasticsearch itself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>build docs to PDF and post</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10176</link><project id="" key="" /><description>I've heard a few requests for a PDF version of the docs.  The prototype generated content was pretty good, but was missing branding.  This issue is to track building these docs regularly and adding some amount of branding (e.g. a header or footer).
</description><key id="63133513">10176</key><summary>build docs to PDF and post</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kevinkluge</reporter><labels><label>docs</label></labels><created>2015-03-20T01:30:33Z</created><updated>2015-12-05T20:43:04Z</updated><resolved>2015-12-05T20:43:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="scharfmn" created="2015-06-23T11:56:18Z" id="114466658">From a user: please do make this happen. It would really help.
</comment><comment author="clintongormley" created="2015-06-23T19:19:29Z" id="114615262">We're working on it. 
</comment><comment author="scharfmn" created="2015-06-24T11:20:16Z" id="114834688">That's great. Thank you for the info. In the meantime, if someone could post an answer to [this question](http://stackoverflow.com/q/31003832/1599229) (I'm trying to self-generate) it would be a big help, and probably not just to me. 
</comment><comment author="clintongormley" created="2015-12-05T20:43:04Z" id="162244227">This issue was moved to elastic/docs#61
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove unused method. close() is neither needed nor called anywhere</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10175</link><project id="" key="" /><description>Just opening the pull request to make sure I am not missing anything.
</description><key id="63116463">10175</key><summary>remove unused method. close() is neither needed nor called anywhere</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-19T23:36:48Z</created><updated>2015-03-20T00:11:16Z</updated><resolved>2015-03-20T00:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-19T23:51:03Z" id="83812151">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 'fields' parameter to index aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10174</link><project id="" key="" /><description>index aliases currently allow filtering horizontically (subset of documents), the idea here is to add vertical filtering as well (subset of fields). This is akin to creating an SQL view with only some of the fields from the table.

this is an initial hackathon hack from @martijnvg and I, and could definitely use some help and extra eyes.

A few notes:
- the impl is basically FieldFilterLeafReader from lucene's test-framework[1], except with some special handling for ES internal meta fields like _source and _field_names. 
- internal meta fields like _uid/_parent/_source/ are always allowed, otherwise features may break. A notable exception is _all, which is currently always filtered out.
- these aliases are still lightweight: fielddata caches are shared with the underlying index. but in this iteration, filter cache is not used (this would be complex but nice to fix)

[1] http://svn.apache.org/viewvc/lucene/dev/branches/lucene_solr_5_0/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java?view=markup
</description><key id="63114665">10174</key><summary>Add 'fields' parameter to index aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Aliases</label><label>feature</label><label>review</label></labels><created>2015-03-19T23:27:13Z</created><updated>2016-03-08T19:18:08Z</updated><resolved>2016-03-08T19:18:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T17:31:27Z" id="89622862">@rmuir Will the `_source` field be filtered to only return the listed fields as well? If so, then this should probably be applied to document GET as well?

What happens if the user tries to update the document?  Would it drop the fields they don't have access to?  If not, then you'll also need to add the _source filter to the response from the update request (the updated `_source` can be requested as part of the update response)
</comment><comment author="rmuir" created="2015-04-04T17:39:30Z" id="89624880">&gt; @rmuir Will the _source field be filtered to only return the listed fields as well? If so, then this should probably be applied to document GET as well?

It does. for aliases, its my understanding GET just uses the reader (or at least should here). 

&gt; What happens if the user tries to update the document? Would it drop the fields they don't have access to? If not, then you'll also need to add the _source filter to the response from the update request (the updated _source can be requested as part of the update response)

We should not allow writes to field-filter aliases here for this and other reasons. Something fancier can be done in a followup if its really needed, but I don't feel it is. This is just like a read-only SQL view.

cc @martijnvg (who did all the ES side, I just did the lucene reader piece so I am less qualified).
</comment><comment author="martijnvg" created="2015-04-05T12:17:19Z" id="89757810">@clintongormley @rmuir I forgot to implement this (I just need to write a test for it), but effectively the 'realtime' get will be disabled when using the get api. By going to the Lucene index the filtered reader Robert has created will do the source filtering as if a search was executed. I think we can also support the realtime get, but in order to move forward with the PR I suggest that we do this in a follow up issue.
</comment><comment author="rmuir" created="2015-04-15T10:46:48Z" id="93318112">i will force push this to master in 72 hours unless there are any objections.
</comment><comment author="s1monw" created="2015-04-15T15:11:41Z" id="93439912">I like this a lot! I left a bunch of comments nothing serious I like how it's implemented in a single filter reader basically! good stuff!
</comment><comment author="rmuir" created="2015-04-15T15:30:36Z" id="93450046">Thanks @s1monw ! I only really understand the filterreader, i will ask @martijnvg for help on some of the ES side. I do know, from working thru test failures, that today "null" vs "empty set" is important in some of those places to have the distinction that no filtering is happening on fields. We could indicate this an alternative way though.
</comment><comment author="s1monw" created="2015-04-16T16:46:53Z" id="93782145">@rmuir ok fair enough I just wonder if we can simplify some code paths here
</comment><comment author="s1monw" created="2015-04-23T20:22:28Z" id="95709212">hey guys @martijnvg @rmuir what's happening here - the review gets stale 
</comment><comment author="martijnvg" created="2015-04-24T22:39:37Z" id="96085893">@s1monw Sorry for the late reaction here... I updated the PR and applied your comments.
</comment><comment author="s1monw" created="2015-04-25T10:06:50Z" id="96172442">LGTM thanks @martijnvg 
</comment><comment author="martijnvg" created="2015-04-27T11:09:19Z" id="96611068">After trying to make alias fields work in the get api, I realized that the `FieldSubsetReader` was wrapping at the wrong location. I basically needed to duplicate the logic that was written for the search api. On top of this the internal get api need additional option to pass down the fact that the get api was invoked from an alias with 'fields' option specified and the actual fields needed to be resolved.

In order to support 'fields' alias support in a better way I did the following:
- Instead of wrapping the index reader when the search context gets created, I moved to Engine#acquireSearcher(...)
- Introduced a FieldsViewService (maybe a better name?) where all shard level read data operations are delegated to.
- The FieldsViewService sets a FieldsViewContext that the Engine uses to decide if a Engine.Searcher needs to be wrapped and for the creation of a FieldSubsetReader instance.

Because of this change the alias field filtering is supported in:
- Get api
- Multi get api
- search api
- count api
- term vector api
- multi term vector api.

Also for the apis that have a realtime option (which allows operations to work on the translog), logic has been added to always switch the `realtime` option to `false` if it has been invoked via an alias with `fields` configured.
</comment><comment author="dakrone" created="2015-06-04T21:13:39Z" id="109053900">@martijnvg what's the latest on this? Does it need another round of review from someone?
</comment><comment author="martijnvg" created="2015-06-16T07:05:18Z" id="112315416">@dakrone Yes, I would love to have another review for this PR. There were significant changes added since the last review.
</comment><comment author="clintongormley" created="2016-03-08T19:18:08Z" id="193926353">No longer needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add before/afterIndexShardDelete callbacks to index lifecycle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10173</link><project id="" key="" /><description>This commit allows code to be executed before or after a shards contentvis deleted from disk. This is only executed if the shard owns the content ie. on a shard file system only a primary shard will execute these operations.
</description><key id="63108350">10173</key><summary>Add before/afterIndexShardDelete callbacks to index lifecycle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T22:44:00Z</created><updated>2015-06-07T11:49:04Z</updated><resolved>2015-03-19T23:18:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-19T22:51:00Z" id="83789971">LGTM, left one comment.

Also, I think this needs to go into 1.5.0?
</comment><comment author="s1monw" created="2015-03-19T22:56:47Z" id="83792863">updated... will push soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard not deleted after relocation if relocated shard is still in post recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10172</link><project id="" key="" /><description>When deleting a shard the node that deletes the shard first checks if all shard copies are
started on other nodes. A message is sent to each node and each node checks locally for
STARTED or RELOCATED.
However, it might happen that the shard is still in state POST_RECOVERY, like this:

shard is relocating from node1 to node2
1. relocated shard on node2 goes in POST_RECOVERY and node2 sends shard started to master
2. master updates routing table and sends new cluster state to node1 and node2
3. node1 processes the cluster state and asks node2 if it has the active shard
  before node2 processes the new cluster state (which would cause it to set the shard to started)
4. node2 sends back it does not have the shard started and so node1 does not delete it

This can be avoided by waiting until cluster state that sets the shard to started is actually processed.

closes #10018
</description><key id="63108015">10172</key><summary>Shard not deleted after relocation if relocated shard is still in post recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Store</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T22:41:11Z</created><updated>2015-05-29T17:41:41Z</updated><resolved>2015-03-31T14:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-19T22:57:44Z" id="83793596">cool stuff I left some comments
</comment><comment author="brwe" created="2015-03-20T00:30:05Z" id="83824102">thanks for the review! addressed all comments. want to take another look?
</comment><comment author="s1monw" created="2015-03-20T18:05:56Z" id="84090397">left some 3 comments other than that LGTM
</comment><comment author="brwe" created="2015-03-21T17:44:18Z" id="84409031">Pushed another commit. We have to catch EsRejectedExecutionException when we try to send back whether the shard is active or not. For example, InternalNode.stop will cause  ObserverClusterStateListener.onClose of the listener to be called at some point and the reject exception that this might throw is not caught anywhere it seems. Alternatively we might also consider not trying to send back a response on close?
</comment><comment author="s1monw" created="2015-03-23T09:43:29Z" id="84914653">I think we should catch there exception in the caller of the close method - this can hit the next user of this as well?
</comment><comment author="bleskes" created="2015-03-23T10:18:49Z" id="84933068">I like the approach. Left some comments.
</comment><comment author="brwe" created="2015-03-30T10:36:37Z" id="87630214">@bleskes @s1monw thanks a lot for the review! I implemented all changes except where I added a comment because I was unsure what to do. 
@s1monw about the exception handling: It seems in general unchecked exceptions are not handled when listeners are called when the cluster service closes. I can add a catch for them (see b4e88ed9a155a9cd3832403b574efbdf9db612eb) but because they are not handled anywhere I suspect there is method behind it. I'd be happy for any insight into how exceptions should be handled properly here.
</comment><comment author="s1monw" created="2015-03-30T11:28:45Z" id="87639564">@brwe I think we should detach the exception handling problem from this issue. Yet, we should still address it. IMO we really need to make sure that all listeners are notified even if one of them threw an exception. Can you open a folllowup?
</comment><comment author="s1monw" created="2015-03-30T11:29:40Z" id="87639695">other than that ^^ LGTM :)
</comment><comment author="bleskes" created="2015-03-30T21:10:31Z" id="87832368">LGTM. Left some very minor comments. no need for another review cycle.
</comment><comment author="brwe" created="2015-03-31T14:01:18Z" id="88098646">pushed to master and 1.x (17dffe222b923c17614905515773614d6963e13e)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `async` replication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10171</link><project id="" key="" /><description>Closes #10114
</description><key id="63092374">10171</key><summary>Remove `async` replication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:CRUD</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T21:17:02Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-19T21:45:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-19T21:36:19Z" id="83768749">LGTM, great stats :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Deprecate 'validate_*' options in GeoPointMapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10170</link><project id="" key="" /><description>No need to carry 3 validate options in GeoPointFieldMapping. This is just causing confusion so next version of ES will deprecate the validate_lat, validate_lon option and simply use validate.  The following version will remove validate_{lat|lon} altogether.
</description><key id="63079183">10170</key><summary>[GEO] Deprecate 'validate_*' options in GeoPointMapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>deprecation</label><label>low hanging fruit</label></labels><created>2015-03-19T20:03:00Z</created><updated>2015-08-18T11:19:09Z</updated><resolved>2015-08-17T19:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-07-16T09:56:14Z" id="121912709">Bumping the version up to 1.7.1 for the today's release.
</comment><comment author="MaineC" created="2015-07-17T08:35:32Z" id="122217402">+1 on merging validate_lat/lon and normalize_lat/lon into one version each.

As this will break backwards compatibility anyway - what do you think about the proposal from @clintongormley here: https://github.com/elastic/elasticsearch/pull/10248#issuecomment-110442561 ?
</comment><comment author="nknize" created="2015-07-17T15:36:07Z" id="122315069">re: `coerce`, I really like it from a mapping consistency stand point. Perhaps we use it to eliminate `normalize` and set the default to `false`.  Keep `validate` and default to `true` which I think maximizes use-case flexibility?  Default behavior is the expectation that ES will receive valid lat/lon positions and throw an exception when it doesn't. This forces a user to explicitly update the mapping as an indication that they really want ES to do the normalization (and/or silence the exception) instead of the application.  /cc @clintongormley
</comment><comment author="clintongormley" created="2015-07-17T16:23:50Z" id="122332399">&gt; re: coerce, I really like it from a mapping consistency stand point. Perhaps we use it to eliminate normalize and set the default to false

++

&gt; Keep validate and default to true which I think maximizes use-case flexibility? Default behavior is the expectation that ES will receive valid lat/lon positions and throw an exception when it doesn't. 

On other fields we validate by default, and turn off validation with `ignore_malformed: true` - why not use this instead of `validate`?
</comment><comment author="nknize" created="2015-07-17T16:41:58Z" id="122336423">&gt; On other fields we validate by default, and turn off validation with ignore_malformed: true - why not use this instead of validate?

++ wonderful!!  I'll make this change to the existing PR for 1.7.1 and completely remove validate/normalize_\* in master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove references to the thrift and memcached transport plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10169</link><project id="" key="" /><description>Remove references to the thrift and memcached transport plugins as they are no longer supported

Closes #10166
</description><key id="63077883">10169</key><summary>Remove references to the thrift and memcached transport plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T19:56:13Z</created><updated>2015-03-19T19:56:29Z</updated><resolved>2015-03-19T19:56:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] fix docs for geo_point "validate" option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10168</link><project id="" key="" /><description>Documentation states false as the default for "validate", "validate_lon", and "validate_lat" leading to confusion as described in issue #9539. This simple fix corrects the documentation and communicates that these fields will be deprecated and removed in upcoming versions.

closes #9539
</description><key id="63077021">10168</key><summary>[GEO] fix docs for geo_point "validate" option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>docs</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T19:51:06Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-23T20:37:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-20T22:42:30Z" id="84171459">LGTM.
</comment><comment author="nknize" created="2015-03-23T20:37:27Z" id="85184381">merged in commit 9e46cf6529da33d555e2f41667c896a25c018859
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecated the thrift and memcached transports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10167</link><project id="" key="" /><description>The thrift and memcached transport layers were experimental, but in
the end didn't buy us much. Memcached is very limited in scope,
supporting only a portion of the REST API, and thrift has much
the same throughput as HTTP.

They are deprecated as of 1.5.0, and will be removed in 2.0.

Document changes for #10166
</description><key id="63067629">10167</key><summary>Deprecated the thrift and memcached transports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>deprecation</label><label>v1.5.0</label><label>v1.6.0</label></labels><created>2015-03-19T19:09:00Z</created><updated>2015-05-25T15:07:51Z</updated><resolved>2015-03-19T19:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ruflin" created="2015-05-20T10:21:59Z" id="103836098">@clintongormley I assume Thrift will also be removed in version 2.0? I'm asking as you only marked as deprecated in the memcache file and not in the Thrift file: https://github.com/elastic/elasticsearch/blob/d63bd7228beb79a957d9b7bef059487f3b9f7d81/docs/reference/modules/thrift.asciidoc
</comment><comment author="clintongormley" created="2015-05-25T13:22:47Z" id="105230807">@ruflin thanks for spotting - i've added the missing deprecation notice https://github.com/elastic/elasticsearch/commit/dbcc08fd04388f78ffa67b5557830295d7c46f58
</comment><comment author="ruflin" created="2015-05-25T15:07:51Z" id="105249168">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate thrift and memcached transports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10166</link><project id="" key="" /><description>The thrift and memcached transport layers were experimental, but in the end didn't buy us much.  Memcached is very limited in scope, supporting only a portion of the REST API, and thrift has much the same throughput as HTTP.

Deprecate in 1.5, and remove in 2.0
</description><key id="63065350">10166</key><summary>Deprecate thrift and memcached transports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>deprecation</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T18:56:03Z</created><updated>2015-12-19T06:49:43Z</updated><resolved>2015-03-19T19:56:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2015-03-19T20:45:19Z" id="83755929">http://build-eu-00.elastic.co/view/Clients/job/es-py_core/ thirft/memcached disabled
</comment><comment author="piotrjurkiewicz" created="2015-03-20T06:50:29Z" id="83940059">I use memcached transport in production for storing log messages and I am able to achieve far more higher throughput than with HTTP REST API.

Furthermore, memcached protocol can work in connectionless mode. This plugin can be fairly simple extended to allow binding to UDP and DGRAM sockets. Memcached client libraries already support that. This would make possible to use ES in connectionless mode. It will suit perfectly for storing log messages.

Does ES team have another ideas how to provide connectionless operation?
</comment><comment author="ghost" created="2015-03-25T08:58:52Z" id="85936964">In theory TCP+thrift should provide better performance. What is the reason to drop this? Could you give us a more detailed explanation of why this is removed?
</comment><comment author="trentcioran" created="2015-03-26T17:30:00Z" id="86634023">yes, could you provide more detailed explanation? Thanks
</comment><comment author="micherr" created="2015-04-05T13:51:06Z" id="89774380">We've been investigating for a while and just decided to switch to thrift for transporting. 
Do you plan any alternative in 2.0 in case of depreciation?
</comment><comment author="xenji" created="2015-05-07T05:45:26Z" id="99728072">Same thing with us. We've decided to build a new part of the architecture using thrift as default communication technology. Although we are not doomed when ES removes thrift, it is still a desirable piece in the puzzle. The Thrift TSocket communication is also much faster than Thrift over http (about 5 to 10 times in our tests).

Have you taken the thrift socket communication option into consideration while benchmarking?
</comment><comment author="wkv" created="2015-05-19T17:05:45Z" id="103593936">The Thrift interface is 5 to 10 times faster than the HTTP interface and has been for 3 years now.  What testing has shown otherwise?
</comment><comment author="3lnc" created="2015-05-21T13:17:14Z" id="104274873">Same question about thrift. Any benchmarks about it? Any potential replacement so far?
</comment><comment author="clintongormley" created="2015-05-25T13:27:14Z" id="105231243">Hi all

We have compared HTTP to Thrift on the officially supported clients in .net, python, ruby, and perl, (with fast http backends, persistent connections, good json encoders etc) and we all found that the performance difference was negligible.  This is why we decided to stop supporting thrift.  I'd be interested to see the code that you used where you found such large performance differences.
</comment><comment author="wkv" created="2015-12-19T06:49:43Z" id="165955426">So we have tested the thrift interface to http interface and the thrift interface is 70% faster.  Comparison of negligible means that you were only testing on the same host node.  Really challenge you to prove the assertion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix validate_* merge policy for GeoPointFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10165</link><project id="" key="" /><description>Fail merge if validate_lat or validate_lon values are not equal. This will prevent inconsistencies between geo_points in a merged index, and parse exceptions for bounding_box and distance filters.

Also merged separate GeoPoint test classes into a single GeoPointFieldMapperTest to be consistent with GeoShapeFieldMapperTests.

closes #10164
</description><key id="63056232">10165</key><summary>Fix validate_* merge policy for GeoPointFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label><label>v1.5.1</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T18:17:41Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-24T13:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:45:36Z" id="84114850">@rjernst can you take a look at this please?
</comment><comment author="rjernst" created="2015-03-20T20:00:58Z" id="84129905">LGTM, just two tiny comments. Thanks for merging these test suites!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] GeoPointFieldMapper.validate_* overwritten on merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10164</link><project id="" key="" /><description>During a merge the `validate_lat` and `validate_lon` parameters for the `geo_point` field mapper are being overwritten by the incoming field mappings.  This can cause problems during search where invalid geo_points (which were formerly acceptable) are no longer acceptable causing exceptions when parsing the (now invalid) geo points.
</description><key id="63052329">10164</key><summary>[GEO] GeoPointFieldMapper.validate_* overwritten on merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label><label>v1.5.1</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T17:53:33Z</created><updated>2015-03-24T13:57:05Z</updated><resolved>2015-03-24T13:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Return `matched_queries` for named queries in percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10163</link><project id="" key="" /><description>I've got a bunch of multi-match percolator queries, which are named queries as well. It would be great to be able to get a response of matched queries including `matched_queries` when percolating. e.g.: 

```
  {
     "took": 0,
     "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
     },
     "total": 2,
     "matches": [
        {
           "_index": "rethinkdb_ex",
           "_id": "user-1",
           "matched_queries": [
              "queryA"
           ]
        },
        {
           "_index": "rethinkdb_ex",
           "_id": "user-2",
           "matched_queries": [
              "queryA",
              "queryB"
           ]
        }
     ]
  }
```

Is this something that's planned?
</description><key id="63019535">10163</key><summary>Return `matched_queries` for named queries in percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gebrits</reporter><labels><label>:Percolator</label><label>discuss</label></labels><created>2015-03-19T15:34:35Z</created><updated>2015-09-27T22:47:33Z</updated><resolved>2015-09-27T22:47:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T19:03:22Z" id="83719416">Hi @gebrits why no storing them as different queries in the percolator index? Can you elaborate on the usecase?
</comment><comment author="gebrits" created="2015-03-19T21:23:37Z" id="83764216">hi @javanna, Each multi-match query is bound to a single user for notification purposes (e.g.: sending out a mail). If I were to split out the multi-match in various queries, my notification client code would get a a bit more complicated. I.e.: having to track the results of all queries matching the same user, wrapping up the results, constructing a mail based on the aggregate results, deduping, etc. 

It just feels cleaner to have it all handled by 1 multi-match query. What do you think?
</comment><comment author="javanna" created="2015-03-20T15:15:05Z" id="84041587">I am on the fence to be honest :) I always imagined this usecase with multiple alerts/queries per user and assumed that indeed the different matching queries would need to be aggregated on the client side. It feels cleaner this way to me, but I don't have a super strong opinion. I marked this for discussion, we'll see what other folks think about it. Thanks for describing your usecase!
</comment><comment author="gebrits" created="2015-03-24T21:40:39Z" id="85705292">No probs :) To fuel the discussion, I feel the need for clientside de-duping is the 'biggest' hurdle that would be solved with this. It's just messy. (e.g.: taking care of ordening, etc.)
</comment><comment author="jpountz" created="2015-09-27T22:47:33Z" id="143599632">The trade-off between the cost of this feature (both in terms of dev cost and runtime) and the value it provides doesn't look good to me: closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove async replication from the docs and REST spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10162</link><project id="" key="" /><description>Async replication was deprecated in 1.5, and removed in 2.0.

Relates to #10114
</description><key id="63005414">10162</key><summary>Remove async replication from the docs and REST spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T14:35:13Z</created><updated>2015-03-19T14:35:28Z</updated><resolved>2015-03-19T14:35:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Deprecate async replication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10161</link><project id="" key="" /><description>Doc changes to deprecate async replication in 1.5.0 and 1.x

Relates to #10114
</description><key id="63003950">10161</key><summary>Deprecate async replication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>deprecation</label><label>v1.5.0</label><label>v1.6.0</label></labels><created>2015-03-19T14:29:52Z</created><updated>2015-03-19T14:30:46Z</updated><resolved>2015-03-19T14:30:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update templates.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10160</link><project id="" key="" /><description>I've been attempting to programatically verify that adding index templates via the `{path.conf}/templates/` directory works fine although I was never able to validate this via an API call to the `/_template/`.  It seems that these templates do not appear in that API call, which I discovered in the following mail thread:

http://elasticsearch-users.115913.n3.nabble.com/Loading-of-index-settings-template-from-file-in-config-templates-td4024923.html#d1366317284000-912

My question is why wouldn't the `/_template/*` method return these templates?  This tends to complicate things for those that want to perform automated tests to verify that they are in fact being recognized and used by Elasticsearch.
</description><key id="62976903">10160</key><summary>Update templates.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">hartfordfive</reporter><labels><label>docs</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T12:58:11Z</created><updated>2015-03-20T20:53:43Z</updated><resolved>2015-03-20T20:53:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-20T20:53:43Z" id="84142151">@hartfordfive merged this to 1.5, 1.x and master. I'll open a new issue for a way to retrieve the on-disk templates.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase of test coverage on Exception raising</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10159</link><project id="" key="" /><description /><key id="62968358">10159</key><summary>Increase of test coverage on Exception raising</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">ferdegje</reporter><labels><label>review</label><label>test</label></labels><created>2015-03-19T12:11:11Z</created><updated>2015-09-15T12:58:43Z</updated><resolved>2015-09-15T12:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:50:37Z" id="84119563">hey can you sign the CLA so I can pull this in?
</comment><comment author="s1monw" created="2015-04-21T10:16:44Z" id="94733008">ping - can you sign the CLA for this?
</comment><comment author="s1monw" created="2015-04-28T09:26:09Z" id="96987040">@ferdegje any feedback on the CLA
</comment><comment author="ferdegje" created="2015-04-28T09:31:19Z" id="96988301">@s1monw just signed it.
</comment><comment author="clintongormley" created="2015-06-04T19:27:20Z" id="109019596">@s1monw CLA signed - you want to merge this in?
</comment><comment author="nik9000" created="2015-09-15T12:23:13Z" id="140371795">Stolen from @s1monw. I will now review.
</comment><comment author="nik9000" created="2015-09-15T12:27:14Z" id="140372435">LGTM. I'll have a go at merging this into master, 2.x, and 2.0.
</comment><comment author="nik9000" created="2015-09-15T12:58:43Z" id="140380022">Sadly, master, 2.x, and 2.0 have drifted a long, long way from where they were. They are far enough where any attempt to merge this would be me writing new code. @ferdegje, if you'd like to work on this have then please send another pull request! I promise we'll review it promptly this time!

For notes: the test you edited has moved and been renamed and massively expanded. Now its called `ESExceptionTests`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining Grandparents aggregations, getting a NullPointer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10158</link><project id="" key="" /><description>I am using Elasticsearch 1.4.4.
I am defining a parent-child index with three levels (grandparents) following the instructions in the document: https://www.elastic.co/guide/en/elasticsearch/guide/current/grandparents.html
My structure is Continent -&gt; Country -&gt; Region
## MAPPING

I create an index with the following mapping:

```
curl -XPOST 'localhost:9200/geo' -d'
{
  "mappings": {
    "continent": {},
    "country": {
    "_parent": {
      "type": "continent" 
    }
  },
  "region": {
    "_parent": {
     "type": "country" 
  }
}
}
}' 
```
## INDEXING

I index three entities:

```
curl -XPOST 'localhost:9200/geo/continent/europe' -d'
{
  "name":"Europe"
}'

curl -XPOST 'localhost:9200/geo/country/italy?parent=europe' -d'
{
  "name":"Italy"
}'

curl -XPOST 'localhost:9200/geo/region/lombardy?parent=italy&amp;routing=europe' -d'
{
  "name":"Lombardia"
}'
```
## QUERY THAT WORKS

If I query and aggregate according to the document everything works fine:

```
curl -XGET 'localhost:9200/geo/continent/_search?pretty=true' -d '
{
"query": {
    "has_child": {
        "type": "country",
        "query": {
            "has_child": {
            "type": "region",
                "query": {
                    "match": {
                        "name": "Lombardia"
                    }
                }
            }
        }
    }
},
"aggs": {
    "country": {
        "terms": { 
            "field": "name"
        },
        "aggs": {
            "countries": {
                "children": {
                    "type": "country"
                },
                "aggs": {
                    "country_names" : {
                        "terms" :  {
                            "field" : "country.name"
                        }
                    }                   
                }
            }
        }
    }
}
}'
```
## QUERY THAT DOES NOT WORK

However, if I try with multi-level aggregations like in:

```
curl -XGET 'localhost:9200/geo/continent/_search?pretty=true' -d '
{
"query": {
    "has_child": {
        "type": "country",
        "query": {
            "has_child": {
            "type": "region",
                "query": {
                    "match": {
                        "name": "Lombardia"
                    }
                }
            }
        }
    }
},
"aggs": {
    "continent_names": {
        "terms": { 
            "field": "name"
        },
        "aggs": {
            "countries": {
                "children": {
                    "type": "country"
                },                  
                "aggs": {
                    "regions": {
                        "children": {
                            "type": "region"
                        }, 
                        "aggs": {
                            "region_names" : {
                                "terms" :  {
                                    "field" : "region.name"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
}'
```

I get back the following
{

  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[b5CbW5byQdSSW-rIwta0rA][geo][0]: QueryPhaseExecutionException[[geo][0]: query[filtered(child_filter[country/continent](filtered%28child_filter[region/country]%28filtered%28name:lombardia%29-&gt;cache%28_type:region%29%29%29-&gt;cache%28_type:country%29))-&gt;cache(_type:continent)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[b5CbW5byQdSSW-rIwta0rA][geo][1]: QueryPhaseExecutionException[[geo][1]: query[filtered(child_filter[country/continent](filtered%28child_filter[region/country]%28filtered%28name:lombardia%29-&gt;cache%28_type:region%29%29%29-&gt;cache%28_type:country%29))-&gt;cache(_type:continent)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[b5CbW5byQdSSW-rIwta0rA][geo][2]: QueryPhaseExecutionException[[geo][2]: query[filtered(child_filter[country/continent](filtered%28child_filter[region/country]%28filtered%28name:lombardia%29-&gt;cache%28_type:region%29%29%29-&gt;cache%28_type:country%29))-&gt;cache(_type:continent)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[b5CbW5byQdSSW-rIwta0rA][geo][3]: QueryPhaseExecutionException[[geo][3]: query[filtered(child_filter[country/continent](filtered%28child_filter[region/country]%28filtered%28name:lombardia%29-&gt;cache%28_type:region%29%29%29-&gt;cache%28_type:country%29))-&gt;cache(_type:continent)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[b5CbW5byQdSSW-rIwta0rA][geo][4]: QueryPhaseExecutionException[[geo][4]: query[filtered(child_filter[country/continent](filtered%28child_filter[region/country]%28filtered%28name:lombardia%29-&gt;cache%28_type:region%29%29%29-&gt;cache%28_type:country%29))-&gt;cache(_type:continent)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",

  "status" : 500

}

The 'grandparents' document says: "Querying and aggregating across generations works, as long as you step through each generation."

How do I get the last query and aggregation to work across all three levels of the index?

What am I missing?

Thank you,
Paolo
</description><key id="62966786">10158</key><summary>Defining Grandparents aggregations, getting a NullPointer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">paolociccarese</reporter><labels><label>bug</label></labels><created>2015-03-19T12:06:35Z</created><updated>2015-03-28T07:49:21Z</updated><resolved>2015-03-28T07:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T10:14:34Z" id="85970981">this seems like a bug, @martijnvg can you have a look please?
</comment><comment author="martijnvg" created="2015-03-25T14:03:19Z" id="86042215">@paolociccarese @javanna Sorry, I missed this! I opened PR #10263 for this bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unsafe field in BytesStreamInput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10157</link><project id="" key="" /><description>It is only used once (and defaults to `false` by default), so copying the array in this case makes more sense.
</description><key id="62966545">10157</key><summary>Remove unsafe field in BytesStreamInput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T12:05:37Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-20T20:14:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:47:49Z" id="84116334">LGTM
</comment><comment author="dadoonet" created="2015-03-30T13:00:11Z" id="87668498">Just a note here. This PR broke mapper attachment plugin versions 2.6.0-SNAPSHOT and 3.0.0-SNAPSHOT.

I'll fix it here: https://github.com/elastic/elasticsearch-mapper-attachments/issues/120
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What should we do with groovy sandboxing?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10156</link><project id="" key="" /><description>Our groovy engine supports sandboxing, which can be enabled/disabled through settings. Sandboxing was disabled by default from `1.4.3` on though since we found out that it could be worked around (see also https://www.elastic.co/blog/running-groovy-scripts-without-dynamic-scripting).

At this point I wonder if it makes sense to maintain the sandboxing that we currently have, as we know it's not effective. I would vote for just stating that groovy is not sandboxed and remove all the sandboxing shenanigans from master (2.0), unless we have a better idea for safe sandboxing... especially once we got #10116 in.

Thoughts?
</description><key id="62956755">10156</key><summary>What should we do with groovy sandboxing?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T11:07:39Z</created><updated>2015-04-28T09:40:15Z</updated><resolved>2015-04-28T09:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-20T15:04:36Z" id="84038558">hmm.. good question, I wonder though whether someone with an extremely narrow use-case for scripts could still use the sandbox by only whitelisting exactly what methods they need?

Other than that, if it's not useful I'm +1 on removing it, we can always re-add it later if needed.
</comment><comment author="nik9000" created="2015-03-20T15:43:33Z" id="84052348">+1 on removing it
</comment><comment author="clintongormley" created="2015-04-04T17:49:18Z" id="89627487">+1 on removing it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Add tests that triggers truncated files on recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10155</link><project id="" key="" /><description>Several issues where reported showing truncated files where footers
didn't match and checksums read past EOF. This test reproduces the issue
on the latest 1.4 branch but passes on all versions above.
</description><key id="62908404">10155</key><summary>[TEST] Add tests that triggers truncated files on recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-19T07:47:24Z</created><updated>2015-03-19T10:16:40Z</updated><resolved>2015-03-19T08:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-19T07:55:53Z" id="83401091">test looks good to me. It is very similar in nature to disconnectsWhileRecoveringTest . I wonder if we should just make disconnectsWhileRecoveringTest more evil (let some file chunks go through before simulating a disconnect)
</comment><comment author="s1monw" created="2015-03-19T07:56:35Z" id="83401193">&gt;  It is very similar in nature to disconnectsWhileRecoveringTest . I wonder if we should just make disconnectsWhileRecoveringTest more evil (let some file chunks go through before simulating a disconnect)

why not having two test this one tests one thing and fails every time.
</comment><comment author="bleskes" created="2015-03-19T08:01:45Z" id="83402016">&gt; why not having two test this one tests one thing and fails every time.

I think it's a good case for randomization as there are many things that can go wrong during recovery. The other test will fail just as reliably if it ends up simulating up this type of corruption. I'm wary of having too many tests doing almost similar things. We already have RelocationTests and IndexRecoveryTests which have some overlap (different issue, to be solved at a different time)
</comment><comment author="s1monw" created="2015-03-19T08:03:10Z" id="83402538">I don't think we can have enough tests - this one is very reliable in producing the problem though :) it fails every time
</comment><comment author="s1monw" created="2015-03-19T08:05:00Z" id="83403217">I also think randomization is not a wonder weapon that we should use to sometimes reproduce the issue. I think we need more dedicated tests for specific scenarios like this one...
</comment><comment author="mikemccand" created="2015-03-19T08:05:47Z" id="83403499">LGTM, this is a nice test.  I like that we can tap into MockTransportService like that...

Re too many tests: I think it's really important to have dedicated narrow non-random test (i.e., fails 100% of the time) for a given specific bug (a "rifle" test) and separately we can have randomized tests to cast a wider net (a "shotgun" test)... and really the more tests the better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cutoff_frequency in match query does not work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10154</link><project id="" key="" /><description>I use cutoff_frequency in match query to exclude stopwords. However, the cutoff_frequency does not seem to behave as mentioned in Elasticsearch documentation.

Steps to reproduce the problems: (I used Elasticsearch Sense plugin)
1) Index the following 2 documents with standard analyzer:
POST /me/test
{ "text": "the test" }
and 
POST /me/test
{ "text": "the building" }

2) Query:
GET /me/test
{
   "query": { "match": { "text": { "query" : "the car", "cutoff_frequency" : 0.001 } } }
}

I would expect the query returns 0 result ("the" is treated as a high-frequency word and should be excluded). However, both indexed documents are returned.

I am not sure if I misuse cutoff_frequency in this case...
</description><key id="62897999">10154</key><summary>cutoff_frequency in match query does not work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">truongduy134</reporter><labels /><created>2015-03-19T07:00:33Z</created><updated>2015-04-05T08:00:04Z</updated><resolved>2015-04-04T16:53:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T16:53:33Z" id="89613943">HI @truongduy134 

By default, your index has 5 primary shards, so each of your two documents is sitting on a shard by itself.

See http://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html

If you create your index as follows, then your example works as expected:

```
PUT me
{
  "settings": {
    "number_of_shards": 1
  }
}
```
</comment><comment author="truongduy134" created="2015-04-05T08:00:04Z" id="89732549">@clintongormley  : Thanks a lot for the explanation :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Describe types as "partition by" instead of tables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10153</link><project id="" key="" /><description>We often describe types as "like a database table" (eg http://www.elastic.co/guide/en/elasticsearch/reference/master/glossary.html#glossary-type), when really they are closer to "PARTITION BY" in Oracle.  We should update this description, especially in the light of #8870
</description><key id="62896778">10153</key><summary>Describe types as "partition by" instead of tables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-03-19T06:51:28Z</created><updated>2015-12-05T20:42:38Z</updated><resolved>2015-12-05T20:42:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-03-19T07:12:16Z" id="83376777">+1 and linking a related one:  https://github.com/elastic/elasticsearch/issues/10149

http://www.elastic.co/guide/en/elasticsearch/reference/master/mapping.html also has a brief note on mapping type is like a database table.  It does describe it as not a complete isolation, but perhaps we can remove the reference to database table. 

And also here: https://www.elastic.co/guide/en/elasticsearch/guide/current/_document_metadata.html#_type
</comment><comment author="jpountz" created="2015-08-26T19:04:49Z" id="135141740">+1 I think this is the right way to describe types.
</comment><comment author="nik9000" created="2015-08-26T19:39:37Z" id="135148850">&gt; +1 I think this is the right way to describe types.

Its certainly a ton better than tables. +1
</comment><comment author="kingIZZZY" created="2015-09-09T17:46:12Z" id="138988197">As a newb on day 3 with ES, I can attest that the guide's analogy from RDBMS structures (index&gt;type&gt;doc&gt;field is like database&gt;table&gt;row&gt;column) versus what i gathered after learning the _truth_ about indexes and types (after some IRC chatting) &amp;mdash; the true picture is **_completely**_ different, and using that RDBMS analogy was unhelpful in understanding the right idea of how ES works and how to plan/design indexes.
</comment><comment author="clintongormley" created="2015-12-05T20:42:38Z" id="162244202">I've rewritten the section on types.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe/Query node doesn't work in pure IPv6 environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10152</link><project id="" key="" /><description>In pure IPv6 environment, we configure query node and send query request to query node, and found it  doesn't work and requests finally send to 127.0.0.1, we run the request:
http://masternode:9200/_nodes/is_query:true/settings?pretty

would return some result like:

```
{
  "cluster_name": "ElasticsearchCluster",
  "nodes": {
    "-eddeG7aRtOowARRz-QJRA": {
      "name": "ES-QueryNode1/ElasticsearchCluster",
      "transport_address": "inet[/fd3e:842e:1c9d:10:0:0:0:1fe%0:9301]",
      "host": "Query",
      "ip": "127.0.0.1",
      "version": "1.3.2",
      "build": "dee175d",
      "attributes": {
        "client": "true",
        "is_query": "true",
        "data": "false",
        "is_index": "false"
      },
      "settings": {
        "node": {
          "client": "true",
          "is_query": "true",
          "is_index": "false",
          "name": "ES-QueryNode1/ElasticsearchCluster"
        },
        "bootstrap": {
          "mlockall": "true"
        },
        "http": {
          "enabled": "false",
          "netty": {
            "maxInitialLineLength": "65536"
          }
        },
        "tribe": {
          "name": "ElasticsearchCluster"
        },
        "name": "ES-QueryNode1/ElasticsearchCluster",
        "path": {
          "data": "\\xxxx\\data",
          "work": "\\xxxx\\elasticsearch-1.3.2",
          "home": "\\xxxx\\elasticsearch-1.3.2",
          "conf": "\\xxxx\\elasticsearch-1.3.2\\config",
          "logs": "\\xxxxelasticsearch-1.3.2/logs"
        },
        "config": "\\xxxx\\elasticsearch-1.3.2\\config\\elasticsearch.yml",
        "cluster": {
          "name": "ElasticsearchCluster"
        },
        "discovery": {
          "zen": {
            "ping": {
              "unicast": {
                "hosts": "masternode:9300"
              }
            }
          }
        }
      }
    }
  }
}
```

Please notice the line "ip" : "127.0.0.1",

If we setup such ES cluster in IPv4 environment, and run same request:

```
{
  "cluster_name": "ElasticsearchCluster",
  "nodes": {
    "3lmWVMs1T_upCWAFHOxS-w": {
      "name": "ES-QueryNode1/ElasticsearchCluster",
      "transport_address": "inet[/192.168.1.66:9302]",
      "host": "Instance2",
      "ip": "192.168.1.66",
      "version": "1.3.2",
      "build": "dee175d",
      "attributes": {
        "client": "true",
        "is_query": "true",
        "data": "false",
        "master": "false"
      },
      "settings": {
        "path": {
          "logs": "C:/elasticsearch-1.3.2 Tribe/logs",
          "home": "C:\\elasticsearch-1.3.2 Tribe"
        },
        "cluster": {
          "name": "ElasticsearchCluster"
        },
        "node": {
          "data": "false",
          "client": "true",
          "is_query": "true",
          "master": "false",
          "name": "ES-QueryNode1/ElasticsearchCluster"
        },
        "http": {
          "enabled": "false"
        },
        "discovery": {
          "zen": {
            "ping": {
              "unicast": {
                "hosts": [
                  "masternode:9300"
                ]
              }
            }
          }
        },
        "tribe": {
          "name": "ElasticsearchCluster"
        },
        "name": "ES-QueryNode1/ElasticsearchCluster"
      }
    }
  }
}
```

We can get correct IP: 
"ip" : "192.168.1.66",
</description><key id="62896349">10152</key><summary>Tribe/Query node doesn't work in pure IPv6 environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cheramilm</reporter><labels><label>feedback_needed</label></labels><created>2015-03-19T06:48:52Z</created><updated>2015-04-26T20:02:07Z</updated><resolved>2015-04-26T20:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T16:49:12Z" id="89613661">Hi @cheramilm 

It is not clear to me how you have configured your cluster, and whether it has even formed a cluster.  Could you provide more details please?
</comment><comment author="clintongormley" created="2015-04-26T20:02:06Z" id="96428802">No more info provided. Closing, Feel free to reopen with more info
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10151</link><project id="" key="" /><description /><key id="62882480">10151</key><summary>Updated URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MiteshShah</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-03-19T05:32:39Z</created><updated>2015-03-24T09:24:52Z</updated><resolved>2015-03-24T09:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T20:01:32Z" id="84130091">hey can you sign the cla for this?
</comment><comment author="s1monw" created="2015-03-24T09:24:52Z" id="85418572">will close this for now...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"minimum_should_match" does not work with "cutoff_frequency"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10150</link><project id="" key="" /><description>total documents: 20675074

term, document count, divided by total documents
![image](https://cloud.githubusercontent.com/assets/8093917/6722281/616744d2-cd94-11e4-87b0-d4a81de4e1d9.png)

Elasticsearch 1.4.1
## Query 1

``` json
{
  "from" : 1,
  "size" : 10,
  "query" : {
    "multi_match" : {
      "query" : "cat and mouse",
      "fields" : [ "tag", "tag.stemmed", "anothertag" ],
      "type" : "most_fields",
      "minimum_should_match" : "100%",
      "cutoff_frequency" : 0.0002
    }
  }
}
```

return me 3316 documents, I expected 3312 documents.
## Query 2

``` json
{
  "from" : 1,
  "size" : 10,
  "query" : {
    "multi_match" : {
      "query" : "cat and mouse",
      "fields" : [ "tag", "tag.stemmed", "anothertag" ],
      "type" : "most_fields",
      "minimum_should_match" : "100%",
      "cutoff_frequency" : 0.002
    }
  }
}
```

return me 100270 documents, I expected less than 3312 documents.
## Query 3

``` json
{
  "from" : 1,
  "size" : 10,
  "query" : {
    "multi_match" : {
      "query" : "cat and mouse",
      "fields" : [ "tag", "tag.stemmed", "anothertag" ],
      "type" : "most_fields",
      "minimum_should_match" : "100%",
      "cutoff_frequency" : 0.005
    }
  }
}
```

return me 100279 documents, I expected less than 3312 documents.
## Query 4

``` java
{

  "from" : 1,
  "size" : 10,
  "query" : {
    "multi_match" : {
      "query" : "cat and mouse",
      "fields" : [ "tag", "tag.stemmed", "anothertag" ],
      "type" : "most_fields",
      "operator": "or"
    }
  }
}
```

100279 is the total document returns by this query

**I guess somehow "cutoff_frequency" will make "minimum_should_match" not used.**
</description><key id="62834368">10150</key><summary>"minimum_should_match" does not work with "cutoff_frequency"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">szou-cp</reporter><labels><label>feedback_needed</label></labels><created>2015-03-19T00:30:30Z</created><updated>2016-04-12T16:35:03Z</updated><resolved>2015-04-26T20:05:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="micpalmia" created="2015-04-01T08:01:44Z" id="88385502">I'm also noticing a (very) weird behavior of `cutoff_frequency` with `multi_match` queries. Any updates on this?
</comment><comment author="MaineC" created="2015-04-02T08:20:16Z" id="88820552">This looks related to #10154 - @micpalmia do you only see weird behaviour in multi-match queries or also in plain match queries?
</comment><comment author="clintongormley" created="2015-04-05T15:30:51Z" id="89792903">Hi @szou-cp 

Let's reduce the complexity of your example by (1) reducing the number of fields involved (because we don't know what is happening in those other fields), (2) reducing the number of shards involved (because the cutoff frequency is per shard, not per index, see https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-is-broken.html) and (3) working with a finite set of docs with known term overlaps:

```
DELETE t

PUT t 
{
  "settings": {
    "number_of_shards": 1
  }
}

POST t/t/_bulk
{"index":{}}
{ "text": "cat and mouse"}
{"index":{}}
{ "text": "cat and dog"}
{"index":{}}
{ "text": "dog and mouse"}
{"index":{}}
{ "text": "dog and mouse"}
{"index":{}}
{ "text": "dog and mouse"}

GET t/_search
{
  "query": {
    "match": {
      "text": {
        "query": "cat and mouse",
        "minimum_should_match": "100%",
        "cutoff_frequency": 0.60
      }
    }
  }
}
```

By playing with `cutoff_frequency` you can see that its behaviour works exactly as expected (except that `mouse` seems to move from high frequency to low frequency at a cutoff of 0.6, instead of at 0.8 (which is what I'd expect)

Add `?explain'  to the search request to get more detailed info about how the query executes.

This just leaves me with the question of why the cutoff frequency doesn't seem to be exact, unless you can show something else which appears to be incorrect?
</comment><comment author="clintongormley" created="2015-04-26T20:05:27Z" id="96429359">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update documentation to warn about potential issues with conflicting field definitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10149</link><project id="" key="" /><description>http://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html#all-mapping-types

&gt; Field names with the same name across types are highly recommended to have the same type and same mapping characteristics (analysis settings for example). There is an effort to allow to explicitly "choose" which field to use by using type prefix (my_type.my_field), but it&#8217;s not complete, and there are places where it will never work (like faceting on the field). n In practice though, this restriction is almost never an issue. The field name usually ends up being a good indication to its "typeness" (e.g. "first_name" will always be a string). Note also, that this does not apply to the cross index case.

Based on comments in https://github.com/elastic/elasticsearch/issues/8870 and the known issues around not having validation today on same field name across multiple types in an index, it sounds like it is more appropriate to have a more forceful warning vs. a recommendation - until alternatives are available for #8870.
</description><key id="62830436">10149</key><summary>Update documentation to warn about potential issues with conflicting field definitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-03-18T23:59:01Z</created><updated>2015-08-26T23:05:23Z</updated><resolved>2015-08-26T23:05:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-08-26T23:05:23Z" id="135201239">With #8870 complete, the documentation has been updated to reflect that field types must match for the same field across document types. I don't think there is anything more to do here. Please reopen if you feel the documentation is still missing something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove expansion of empty index arguments in RoutingTable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10148</link><project id="" key="" /><description>RoutingTables activePrimaryShardsGrouped(), allActiveShardsGrouped() and
allAssignedShardsGrouped() methods treated empty index array input
parameters as meaning "all" indices and expanded to the routing maps
keyset. However, the expansion of index names is now already done in
MetaData#concreteIndices(). Returning an empty index name list here
when a wildcard pattern didn't match any index name could lead to
problems like #9081 because the RoutingTable still expanded this
list of names to "_all". In case of e.g. the recovery endpoint this
could lead to problems.

This fix removes the index name expansion in RoutingTable and introduces
some more checks for preventing NPEs in MetaData#concreteIndices().

Closes #9081
</description><key id="62828972">10148</key><summary>Remove expansion of empty index arguments in RoutingTable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Index APIs</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T23:44:16Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-04-02T10:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T10:10:14Z" id="83476945">I like the change! Left a bunch of comments
</comment><comment author="cbuescher" created="2015-03-19T16:32:41Z" id="83652537">I adressed your comments, will revert the changes in IndicesRequestTests and create rest tests for that later. Let me know if my other replies make sense.
</comment><comment author="cbuescher" created="2015-03-19T20:03:35Z" id="83742016">@javanna pushed a couple of changes regarding your comments. For one, I reverted the changes to IndicesRequestTests and introduced rest tests for these cases. Other changes address your comments to code style mostly. Let me know what you think.
</comment><comment author="javanna" created="2015-03-20T08:55:40Z" id="83958910">Hi @cbuescher I did another review round. it's close.

I am not sure if we should rely solely on REST tests to test the `RoutingTable` changes. REST tests are great but I would love to have java unit tests around them. The tests that you previously added to `IndicesRequestTests` were not bad, just in the wrong place. Also, I wondered if we could move them to a more isolated unit tests instead of having to send requests and read responses to find out how many shards were touched. Replacing with REST tests makes tests even less isolated, makes sense?
</comment><comment author="cbuescher" created="2015-03-20T21:57:48Z" id="84165029">@javanna I adressed two of your comments and started adding a RoutingTable unit test, but it's still a very rudimentary test. One thing I'm currently not sure about is how to set up the RoutingTable for the test so that the shard routings that are created are in a different state than "unassigned". When I figured that out I will extend the test. Any suggestion on how to do that in a simple unit test way are.
</comment><comment author="javanna" created="2015-03-23T18:06:01Z" id="85126902">Thanks for the update @cbuescher I had another look, looks good. You are on the right track with the `RoutingTable` test, looks great.
</comment><comment author="cbuescher" created="2015-03-26T18:17:20Z" id="86654210">Added changes according to latest review comments and extended RoutingTableTest so it covers moving through different ShardRoutingStates now.
</comment><comment author="javanna" created="2015-03-30T10:02:01Z" id="87619795">Had another look, left a bunch of super minor comments, this is very close.
</comment><comment author="cbuescher" created="2015-03-30T11:45:45Z" id="87643133">I left the two exception types in MetaData.concreteIndices() but extended the test to check for throwing the correcty type. Can also make both IndexMissingExceptions, but that one wants the name of the missing index and it realy is more like a call to the function with mismatch in arguments. I hope its okay to give the IndicesOptions a simple toString() to see whats going on in case of an exception.
</comment><comment author="cbuescher" created="2015-03-31T10:56:13Z" id="88041697">We agreed on pulling the changes in MetaData and MetaDataTests out into a separate PR. Will open that soon. This PR now should only remove the expansion of empty or null index name lists in RoutingTable.
</comment><comment author="cbuescher" created="2015-04-01T10:18:34Z" id="88427682">@javanna With the removal of the changes in MetaData to #10339, anything else to change here?
</comment><comment author="javanna" created="2015-04-01T16:04:47Z" id="88534506">LGTM @cbuescher thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate API: provide more verbose explanation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10147</link><project id="" key="" /><description>For Fuzzy Queries:

```
GET /imdb/movies/_validate/query?explain=true
{
  "query": {
    "fuzzy": {
      "actors": "kyle"
    }
  }
}

Response:

{
   ...
   "explanations": [
      {
         "index": "imdb",
         "valid": true,
         "explanation": "filtered(actors:eyle^0.75 actors:kale^0.75 actors:kayle^0.75 ... )-&gt;cache(_type:movies)"
      }
   ]
}
```

For More Like This:

```
GET /imdb/movies/_validate/query?explain=true
{
  "query": {
    "more_like_this": {
      "like": {
        "_id": "88247"
      }
    }
  }
}

Response:

{
   ...
   "explanations": [
      {
         "index": "imdb",
         "valid": true,
         "explanation": "filtered((((title:terminator^3.71334 plot:kyle^1.0604408 plot:cyborg^1.0863208 ... )~2)) -ConstantScore(_uid:movies#88247))-&gt;cache(_type:movies)"
      }
   ]
}
```

Relates to #1412
</description><key id="62794945">10147</key><summary>Validate API: provide more verbose explanation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Search</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T20:45:53Z</created><updated>2015-05-29T15:06:38Z</updated><resolved>2015-04-13T19:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T19:20:04Z" id="84105804">I think this LGTM - I wonder if we should make the rewrite optional and add a boolean to the request to trigger it?
</comment><comment author="alexksikes" created="2015-03-20T19:42:08Z" id="84112868">I wondered this too if I should not add a verbose flag, but the slight problem is when the query is not valid, in this case, the explanation of the error is not more verbose. So the explain flag should explain the error if the query is not valid, and otherwise should explain the query itself if valid.
</comment><comment author="s1monw" created="2015-03-20T19:42:51Z" id="84113220">just add `rewrite: true|false`?
</comment><comment author="alexksikes" created="2015-03-20T19:52:21Z" id="84121557">`rewrite: true|false` would only make sense if `explain: true` then. I agree that this is more descriptive as to what is actually happening, but wondering if having to set two flags `explain: true` and `rewrite: true` is a good pattern for the task. Perhaps @clintongormley has some ideas.
</comment><comment author="clintongormley" created="2015-04-04T17:55:55Z" id="89630011">I can imagine this output could be quite heavy, so I'd be happy having the extra `rewrite:true` flag.  The other place I'd love to see this is in the `common_terms` query and the `match` query when combined with `cutoff_frequency`.
</comment><comment author="s1monw" created="2015-04-13T10:08:47Z" id="92299366">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rename UNICODE_CHAR_CLASS to UNICODE_CHARACTER_CLASS in regex flags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10146</link><project id="" key="" /><description>The documentation states that one should use the Java Pattern API flags for ES regex. However I received an error when I used `UNICODE_CHARACTER_CLASS`. Looking at the source, `UNICODE_CHAR_CLASS` is used instead. Can this be renamed to match the Java Pattern API?

See #2895 for the source code.
</description><key id="62792660">10146</key><summary>rename UNICODE_CHAR_CLASS to UNICODE_CHARACTER_CLASS in regex flags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rpedela</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-03-18T20:34:48Z</created><updated>2015-12-11T10:53:36Z</updated><resolved>2015-12-11T10:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T16:37:49Z" id="89611638">... or at least adding a synonym
</comment><comment author="srogljan" created="2015-06-11T11:44:12Z" id="111098404">What about this? :-)
https://github.com/elastic/elasticsearch/pull/11598
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>master/1.x branch does not compile with java9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10145</link><project id="" key="" /><description>master works fine. Maybe there is a real bug here, e.g. wildcard imports, and a new class in java9 causes a conflict? This is why i hate wildcard imports :)

Otherwise it could be a compiler bug or something:

main:
     [echo] Using OpenJDK Runtime Environment 1.9.0-internal-rmuir_2015_03_18_00_27-b00 Oracle Corporation
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (invalid-patterns) @ elasticsearch ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- buildnumber-maven-plugin:1.2:create (default) @ elasticsearch ---
[INFO] Checking for local modifications: skipped.
[INFO] Updating project files from SCM: skipped.
[INFO] Executing: /bin/sh -c cd /home/rmuir/workspace/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/rmuir/workspace/elasticsearch
[INFO] Storing buildNumber: 78b5a6a7fc6caa61e1af9fb67e9a2ec2df4f831f at timestamp: 1426705014243
[INFO] Executing: /bin/sh -c cd /home/rmuir/workspace/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/rmuir/workspace/elasticsearch
[INFO] Storing buildScmBranch: UNKNOWN
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ elasticsearch ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ elasticsearch ---
[INFO] Compiling 3171 source files to /home/rmuir/workspace/elasticsearch/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/Scopes.java:[112,23] error: method scope in interface Scope cannot be applied to given types;
[ERROR]   required: Key&lt;T#1&gt;,Provider&lt;T#1&gt;
  found: Key&lt;T#2&gt;,ProviderToInternalFactoryAdapter&lt;CAP#1&gt;
  reason: inferred type does not conform to equality constraint(s)
    inferred: CAP#1
    equality constraints(s): CAP#1,T#2
  where T#1,T#2 are type-variables:
    T#1 extends Object declared in method &lt;T#1&gt;scope(Key&lt;T#1&gt;,Provider&lt;T#1&gt;)
    T#2 extends Object declared in method &lt;T#2&gt;scope(Key&lt;T#2&gt;,InjectorImpl,InternalFactory&lt;? extends T#2&gt;,Scoping)
  where CAP#1 is a fresh type-variable:
    CAP#1 extends T#2 from capture of ? extends T#2
/home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/collect/Iterators2.java:[34,63] error: incompatible types: PeekingIterator&lt;CAP#1&gt; cannot be converted to PeekingIterator&lt;T&gt;
[ERROR]   where T is a type-variable:
    T extends Object declared in method &lt;T&gt;deduplicateSorted(Iterator&lt;? extends T&gt;,Comparator&lt;? super T&gt;)
  where CAP#1 is a fresh type-variable:
    CAP#1 extends T from capture of ? extends T
/home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/util/Modules.java:[92,58] error: incompatible types: ImmutableSet&lt;CAP#1&gt; cannot be converted to Set&lt;Module&gt;
[ERROR]   where CAP#1 is a fresh type-variable:
    CAP#1 extends Module from capture of ? extends Module
/home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/util/Modules.java:[123,50] error: incompatible types: ImmutableSet&lt;CAP#1&gt; cannot be converted to ImmutableSet&lt;Module&gt;
[INFO] 4 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 26.162 s
[INFO] Finished at: 2015-03-18T14:57:07-05:00
[INFO] Final Memory: 23M/531M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project elasticsearch: Compilation failure: Compilation failure:
[ERROR] /home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/Scopes.java:[112,23] error: method scope in interface Scope cannot be applied to given types;
[ERROR] required: Key&lt;T#1&gt;,Provider&lt;T#1&gt;
[ERROR] found: Key&lt;T#2&gt;,ProviderToInternalFactoryAdapter&lt;CAP#1&gt;
[ERROR] reason: inferred type does not conform to equality constraint(s)
[ERROR] inferred: CAP#1
[ERROR] equality constraints(s): CAP#1,T#2
[ERROR] where T#1,T#2 are type-variables:
[ERROR] T#1 extends Object declared in method &lt;T#1&gt;scope(Key&lt;T#1&gt;,Provider&lt;T#1&gt;)
[ERROR] T#2 extends Object declared in method &lt;T#2&gt;scope(Key&lt;T#2&gt;,InjectorImpl,InternalFactory&lt;? extends T#2&gt;,Scoping)
[ERROR] where CAP#1 is a fresh type-variable:
[ERROR] CAP#1 extends T#2 from capture of ? extends T#2
[ERROR] /home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/collect/Iterators2.java:[34,63] error: incompatible types: PeekingIterator&lt;CAP#1&gt; cannot be converted to PeekingIterator&lt;T&gt;
[ERROR] where T is a type-variable:
[ERROR] T extends Object declared in method &lt;T&gt;deduplicateSorted(Iterator&lt;? extends T&gt;,Comparator&lt;? super T&gt;)
[ERROR] where CAP#1 is a fresh type-variable:
[ERROR] CAP#1 extends T from capture of ? extends T
[ERROR] /home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/util/Modules.java:[92,58] error: incompatible types: ImmutableSet&lt;CAP#1&gt; cannot be converted to Set&lt;Module&gt;
[ERROR] where CAP#1 is a fresh type-variable:
[ERROR] CAP#1 extends Module from capture of ? extends Module
[ERROR] /home/rmuir/workspace/elasticsearch/src/main/java/org/elasticsearch/common/inject/util/Modules.java:[123,50] error: incompatible types: ImmutableSet&lt;CAP#1&gt; cannot be converted to ImmutableSet&lt;Module&gt;
[ERROR] -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
</description><key id="62773239">10145</key><summary>master/1.x branch does not compile with java9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T18:59:20Z</created><updated>2015-05-09T10:21:41Z</updated><resolved>2015-03-24T05:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-03-19T15:53:11Z" id="83641516">We hit a similar issue in lucene when upgrading. @uschindler is filing a bug at oracle.
</comment><comment author="uschindler" created="2015-03-19T16:30:58Z" id="83652168">I submitted a bug report! Its is not yet visible, I will report back. The problem is caused by the diamond operator with -source/target 1.7. The diamond's real type is deducted not from left side of assignement as JLS spec says, but instead from the parameter passed to the constructor (e.g. new HashSet&lt;&gt;(otherSet)). In that case it takes type from otherSet which is wrong.
</comment><comment author="rmuir" created="2015-03-19T16:34:17Z" id="83652887">Thanks for digging Uwe. I tried to understand the problems, I felt like i was looking at c++ compiler errors with tons of templates :)
</comment><comment author="uschindler" created="2015-03-19T16:34:46Z" id="83653005">Link to Lucene: http://mail-archives.apache.org/mod_mbox/lucene-dev/201503.mbox/%3C066501d06256%242f949300%248ebdb900%24%40thetaphi.de%3E
</comment><comment author="uschindler" created="2015-03-24T09:34:03Z" id="85422658">Hi,
the bug report at Oracle is: https://bugs.openjdk.java.net/browse/JDK-8075793
</comment><comment author="s1monw" created="2015-03-24T09:34:53Z" id="85422797">thanks @uschindler 
</comment><comment author="uschindler" created="2015-05-09T09:07:09Z" id="100454233">Hi,
according to the OpenJDK people, this is not really a bug in the JDK, because the Java Language Spec was implemented in a wrong way up to Java 8. The current code is only valid according to JLS with Java 8+ (where the JLS was adopted to allow this behaviour that we expected) - this is why it works in Lucene trunk with Java 8. It was just a bug in the Java 5, 6, 7 compilers that did a "wrong"shortcut to get the type of the diamond.

This wrong behaviour in Javac is fixed in Java 9, so the broken code should only compile with -source 8 or -source 9 (LOL).

In fact you have to add the type into the diamond for Java 7, so basically the code on our side was wrong (I did not verify that from the JLS, this is what was written by Dan Smith).

Dan Smith wrote:

&gt; Simplified test:
&gt; 
&gt; ```
&gt;   class C&lt;T&gt; {}
&gt; 
&gt;   &lt;T&gt; C&lt;T&gt; m(C&lt;? extends T&gt; x) { return null; }
&gt; 
&gt;   void test(C&lt;? extends Number&gt; arg) {
&gt;     C&lt;Number&gt; c = m(arg);
&gt;   }
&gt; ```
&gt; 
&gt; Type checking behavior is clear, per JLS 3 and JLS 7:
&gt; - The type of 'arg' is C&lt;CAP#1&gt;, where CAP#1 extends Number (6.5.6.1)
&gt; - Inference determines that T has lower bound CAP#1 (15.12.2.7)
&gt; - The type of T is inferred as CAP#1 (15.12.2.7)
&gt; - The type of the method invocation ('m(arg)') is C&lt;CAP#1&gt; (15.12.2.6)
&gt; - C&lt;CAP#1&gt; cannot be assigned to C&lt;Number&gt; (4.10.2)
&gt; 
&gt; Traditionally, javac has performed some unspecified "simplifications" that led to T being inferred as 'Number' rather than 'CAP#1'. These unspecified behaviors break programs that should compile, but also allow programs like these to be compiled when the spec disallows them (see JDK-8039214).
&gt; 
&gt; For the class of programs described by this bug, changes made to inference in JLS 8 make these JLS/javac inconsistencies moot, because both strategies get the same answer. But under -source 7, the distinction is apparent, and fixing JDK-8039214 exposes these programs' dependency on incorrect type checking behavior.
&gt; 
&gt; Broadly, our choices:
&gt; - Not A Bug, expect users to fix their code
&gt; - Freeze the -source 7 type checking implementation, and stop fixing bugs in it (hard to separate JDK-8039214 from many other bugs...)
&gt; - Retroactively "specify" javac's legacy behavior as part of inference in 5-7, re-implement it, and hope this doesn't cause subtle breakage elsewhere
</comment><comment author="uschindler" created="2015-05-09T10:21:41Z" id="100461015">Because it is unclear, how this issue is solved on the OpenJDK side, I would recommend to stay with the current fix, because those diamonds and type guessing by the compiler was always broken (my personal opinion). There also exist differences between Eclipse's compiler and JDK's and most of those differences are around this type of stuff.

I always hated diamonds and was always more happy with being explicit. So we should be more explicit in our code, if the type inference is not easy to explain!

Thanks,
the Generics Policeman :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update object-type.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10144</link><project id="" key="" /><description>Just fixing some awkward/incorrect grammar.
</description><key id="62769257">10144</key><summary>Update object-type.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">noambenami</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-03-18T18:40:58Z</created><updated>2015-05-27T08:12:22Z</updated><resolved>2015-05-27T08:12:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T20:42:13Z" id="83163000">Thanks @noambenami could you please sign our [CLA](https://www.elastic.co/contributor-agreement) so we can merge your PR? 
</comment><comment author="javanna" created="2015-05-27T08:12:21Z" id="105812617">No CLA signed after two months, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>use bigger back compat indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10143</link><project id="" key="" /><description>use 2000..3000 docs instead of 10..100
Don't randomly optimize, we would like multiple segments (e.g. some cfs some not).
Don't try to use completion suggester on versions that dont support it.
</description><key id="62767350">10143</key><summary>use bigger back compat indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T18:31:15Z</created><updated>2015-03-18T19:11:50Z</updated><resolved>2015-03-18T18:49:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-18T18:43:20Z" id="83117207">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing duplicates from merged aggregation results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10142</link><project id="" key="" /><description>**Scenario 1**
This is a very simplified scenario but captures the problem we are facing.

We have a team lead document with a field for &#8220;Primary Product&#8221; and a field for &#8220;Secondary Product&#8221;.  We want to show the distribution of all products amongst team leads. To do so we would do a term aggregation on &#8220;Primary Product&#8221; and a term aggregation on &#8220;Secondary Product&#8221;. 

As this is the same products we want to merge the buckets together (done manually). This is simple to do with the term aggregation but has a problem. If both fields have the product, the document count will be one more than it should be once you merge the buckets. We need a way to realise if a document is already included in the count of the first aggregation, so we don&#8217;t add the same document from the second aggregation.

**Scenario 2**
This next scenario is similar to our actual use case where we make use of nested documents but it is the same problem as above.

Each team lead is a top level document nested with team members. We then want to show the distribution of teams that contain males or females, assuming a single team leader exists per team. 

Again we would run two aggregations. For team leaders this would be a term aggregation on gender. For team members this would be a nested term aggregation on gender followed by a reverse nested aggregation to get team leader counts for each gender. The two aggregation result buckets would then have to be manually combined. But it&#8217;s the same problem of working out when a team lead was already included in the first aggregation so any duplicate team leads that match in the second aggregation can be ignored.  (The aggregation we would use can be seen here https://gist.github.com/SMUnlimited/86e03e8f59ac6a09935c)

Another solution could be to include the team lead himself as a nested team member so we can just do a single nested term aggregation followed by a reverse nested aggregation to get the team member counts.  But this means you are duplicating all the team lead data which is not ideal.

:boom: What do people think would be the best way to solve this problem?

_Note we haven&#8217;t thought about other types of aggregations yet (some may not be as straightforward to merge buckets) but in theory could also be applied._
</description><key id="62741583">10142</key><summary>Removing duplicates from merged aggregation results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SMUnlimited</reporter><labels><label>feedback_needed</label></labels><created>2015-03-18T16:59:35Z</created><updated>2015-04-23T11:05:33Z</updated><resolved>2015-04-23T11:05:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SMUnlimited" created="2015-03-23T09:29:52Z" id="84911260">A crazy solution that just might work. Set no parent fields! The parent fields are themselves only available in a nested document. And all nested documents are only in a single nested path. Then when we do an aggregation we can get the merged results of the parents and children without any duplication of parent doc counts or data. 

If at some point we are only interested in the parent or children on our aggregation, we set a field on nested documents that says whether it is the parent fields or not. Then do a (term?) aggregation to split the nested documents into 2 buckets of parent nested docs and children nested docs, followed by the actual aggregation we want to do. 

The upcoming inner hits feature can be used when we are after displaying the parent fields to a user.

Only disadvantage is this doesn't solve the problem for multiple levels of nesting.
</comment><comment author="clintongormley" created="2015-04-04T18:37:59Z" id="89636174">Hi @SMUnlimited 

Any chance you could upload some actual REST examples?  It's easier reading code than descriptions :)
</comment><comment author="SMUnlimited" created="2015-04-15T07:25:52Z" id="93236503">@clintongormley There is an example in the gist:

https://gist.github.com/SMUnlimited/86e03e8f59ac6a09935c
</comment><comment author="clintongormley" created="2015-04-23T11:05:31Z" id="95542202">@SMUnlimited I think you need to solve these problems at index time.  For instance, if you want to count identical primary and secondary products as 1, then you should merge these products into a single field, eg using `copy_to`:

```
PUT myindex
{
  "mappings": {
    "lead": {
      "properties": {
        "primary_product": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "all_products"
        },
        "secondary_product": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "all_products"
        },
        "all_products": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}

PUT myindex/lead/1
{
  "name": "John",
  "primary_product": "Foo",
  "secondary_product": "Bar"
}

PUT myindex/lead/2
{
  "name": "Mary",
  "primary_product": "Foo",
  "secondary_product": "Foo"
}

GET /myindex/lead/_search
{
  "size": 0,
  "aggs": {
    "products": {
      "terms": {
        "field": "all_products"
      }
    }
  }
}
```

This returns:

```
  "products": {
     "doc_count_error_upper_bound": 0,
     "sum_other_doc_count": 0,
     "buckets": [
        {
           "key": "Foo",
           "doc_count": 2
        },
        {
           "key": "Bar",
           "doc_count": 1
        }
     ]
  }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[release script] Check for //NORELEASE in code before release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10141</link><project id="" key="" /><description>Lines in the code that should be removed before a release can be annotated with
//NORELEASE . This can be useful when debugging test failures. For example,
one might want to add additional logging that would be too verbose for production
and therfore should be removed before releasing.
</description><key id="62735798">10141</key><summary>[release script] Check for //NORELEASE in code before release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-18T16:31:53Z</created><updated>2015-03-18T16:59:56Z</updated><resolved>2015-03-18T16:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-18T16:40:44Z" id="83051387">LGTM
</comment><comment author="mikemccand" created="2015-03-18T16:46:36Z" id="83054096">The matching is case insensitive right?
</comment><comment author="s1monw" created="2015-03-18T16:47:06Z" id="83054226">`re.IGNORECASE` &lt;== @mikemccand 
</comment><comment author="mikemccand" created="2015-03-18T16:53:41Z" id="83055845">&gt; re.IGNORECASE &lt;== @mikemccand

Super :)
</comment><comment author="mikemccand" created="2015-03-18T16:54:00Z" id="83055928">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo when primary is not available to index a document (UnavailableShardsException)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10140</link><project id="" key="" /><description /><key id="62729618">10140</key><summary>Fix typo when primary is not available to index a document (UnavailableShardsException)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T16:02:59Z</created><updated>2015-06-07T10:16:36Z</updated><resolved>2015-03-18T16:40:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T16:35:22Z" id="83049332">LGTM ;)
</comment><comment author="s1monw" created="2015-03-18T16:38:28Z" id="83050470">LGTM too :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>updated module.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10139</link><project id="" key="" /><description>The filenames are updated to fit to the current elasticsearch version 1.4.4.
</description><key id="62728360">10139</key><summary>updated module.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">fahrradfahrer</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T15:56:49Z</created><updated>2015-04-07T17:34:13Z</updated><resolved>2015-04-07T16:14:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T20:37:01Z" id="83160890">Thanks @fahrradfahrer  may I ask if you already signed our [CLA](https://www.elastic.co/contributor-agreement)? If not could you please do so we can merge your PR?
</comment><comment author="clintongormley" created="2015-04-04T16:40:06Z" id="89611730">CLA signed - need similar updates in 1.5, 1.x and master
</comment><comment author="javanna" created="2015-04-07T16:14:35Z" id="90627202">merged, thanks @fahrradfahrer !

@clintongormley we don't have placeholders for the current es and lucene version do we? that would help keeping them in sync, otherwise they will get outdated again very soon.
</comment><comment author="clintongormley" created="2015-04-07T17:34:13Z" id="90660988">@javanna we have placeholders at the start of some docs, eg see https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/reference/index.asciidoc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_ttl problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10138</link><project id="" key="" /><description>Dear Elasticsearch,

We use the elasticsearch's _ttl feature which is not clear to us. The next is written in the latest documentation : 

Expired documents will be automatically deleted regularly. You can dynamically set the indices.ttl.interval to fit your needs. The default value is 60s . 
The deletion orders are processed by bulk. You can set indices.ttl.bulk_size to fit your needs. The default value is 10000.  

---

Knowing this, we modified the indices.ttl.interval to 30s and indices.ttl.bulk_size to 3. After, we sent some  (2000) logs into our index with _ttl : 1m . 
After a while, we noticed that more logs have been deleted as expected. So all logs were deleted within 30s after one minute elapsed but there only 3 logs should have been deleted.
Is this normal ? We thought that only 3 logs will be deleted. How is the deletion of logs implemented on the elasticsearch ? Could you give me more information on the deletion of logs ?
</description><key id="62711468">10138</key><summary>_ttl problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emraxxor</reporter><labels /><created>2015-03-18T14:46:04Z</created><updated>2015-03-19T12:01:18Z</updated><resolved>2015-03-19T12:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T12:01:18Z" id="83523978">Hi @onthefloorr I think this would get a better answer if you ask it on the mailing list. If it then turns out to be a bug or a feature request you can then move it here to github. Closing for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Getting exception in indexing data using node client </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10137</link><project id="" key="" /><description>Hi,

I have used transport client before for the same cluster and same index, which works fine. I want to use node client now. I tried the following:

```package es.code;

import org.apache.log4j.Logger;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.client.Client;
import org.elasticsearch.node.Node;

import static org.elasticsearch.node.NodeBuilder.*;

import java.io.IOException;

public class elasticsearch {

```
private elasticsearch() {
}

public static void main(String[] args) throws IOException {
    System.out.println("Hello, World");
    Client client = null;

    Node node = nodeBuilder().clusterName("sample_test").client(true).node();
    client = node.client();
    String json = "{" +
            "\"user\":\"prachi\"," +
            "\"postDate\":\"2013-01-30\"," +
            "\"message\":\"trying out Elasticsearch\"" +
            "}";

    IndexResponse response = client.prepareIndex("estest", "testing")
            .setSource(json)
            .execute()
            .actionGet();

    System.out.println(response.getId());    
    node.close();  
}
```

}```

I am getting the following exception:

`Exception in thread "main" org.elasticsearch.discovery.MasterNotDiscoveredException: waited for [1m]
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$4.onTimeout(TransportMasterNodeOperationAction.java:164)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:239)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:520)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)`

What is it which I am doing wrong? If I remove client(true), the program runs successfully. But I don't see data in ES index.

What is the correct way to do it?
</description><key id="62697387">10137</key><summary> Getting exception in indexing data using node client </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prachicsa</reporter><labels /><created>2015-03-18T13:44:13Z</created><updated>2015-03-18T14:17:47Z</updated><resolved>2015-03-18T14:17:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-18T14:17:47Z" id="82992821">You should ask this on the mailing list.

It is most likely caused by your node which can not join the cluster. Might be firewall issue or multicast disabled...

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete api: remove broadcast delete if routing is missing when required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10136</link><project id="" key="" /><description>This commit changes the behaviour of the delete api when processing a delete request that refers to a type that has routing set to required in the mapping, and the routing is missing in the request. Up until now the delete api sent a broadcast delete request to all of the shards that belong to the index, making sure that the document could be found although the routing value wasn't specified. This was probably not the best choice: if the routing is set to required, an error should be thrown instead.

A `RoutingMissingException` gets now thrown instead, like it happens in the same situation with every other api (index, update, get etc.). Last but not least, this change allows to get rid of a couple of `TransportAction`s, `Request`s and `Response`s and simplify the codebase.

Closes #9123
</description><key id="62693624">10136</key><summary>Delete api: remove broadcast delete if routing is missing when required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:CRUD</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T13:26:25Z</created><updated>2016-08-08T23:03:06Z</updated><resolved>2015-03-20T08:36:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-18T15:33:46Z" id="83022687">+1 LGTM nice stats!! can you update the migration guide to ensure we mention it?
</comment><comment author="javanna" created="2015-03-18T18:25:41Z" id="83110693">good point @s1monw I just updated the migration guide.
</comment><comment author="mrkamel" created="2016-07-14T14:25:35Z" id="232680915">The docs state:

&gt; Many times, the routing value is not known when deleting a document. For those cases, when specifying the _routing mapping as required, and no routing value is specified, the delete will be broadcast automatically to all shards.

This doesn't seem to be the case anymore, unfortunately :-( :-( :-(
</comment><comment author="javanna" created="2016-07-14T14:28:56Z" id="232681912">this is a leftover @mrkamel sorry about that. Care to send a PR?
</comment><comment author="mrkamel" created="2016-07-14T14:33:25Z" id="232683125">is it somehow possible to do a broadcast? i use the mysql binlog to to have ES in sync - and at the time of the delete there is no way to know the routing key :-(
</comment><comment author="clintongormley" created="2016-07-14T14:41:37Z" id="232685559">@mrkamel use delete-by-query instead 
</comment><comment author="mrkamel" created="2016-07-15T05:49:48Z" id="232865378">thx, ... unfortunately, delete-by-query doesn't seem to work for this case, as the query only sees records after index refresh, right? So, this has consistency issues for records that are deleted right after being created.
</comment><comment author="javanna" created="2016-07-15T07:05:28Z" id="232878969">Hi @mrkamel I see what you mean. May I ask how come that you know the routing key when you index documents but not when deleting? sorry if I previously missed that. I also wonder if at this point it wouldn't be better for you to use ordinary routing rather than custom routing keys.
</comment><comment author="mrkamel" created="2016-07-15T07:54:51Z" id="232887011">@javanna i) i'm using mysql with binlog_row_image=minimal. Thus, my binlog-to-es-replicator has the PK only ... no chance to get the routing key for delete operations. I'll probably switch to binlog_row_image=full, but this will have huge performance implications - i need to check ii) I can't use ordinary routing, because for a statistical/reporting app, i'm heavily using aggregations - and to get correct counts per bucket, i have to use custom routing.

Having an option to at least explicitly state, that you want the request to be broadcasted would be awsome - or - to explicitly state to which shard you want the request to be sent, such that you can broadcast "manually" would be okish.
</comment><comment author="rayward" created="2016-08-08T00:03:33Z" id="238117681">@javanna @clintongormley wondering if a bit more context can be supplied around this and if you guys will consider rethinking the necessity of this change.

I got hit hard by this change today, we don't use custom routing but we have parent/child mappings which in ES 2 automatically sets `{"_routing": {"required": true}}` on the child type. This is fine for indexing as it enforces the requirement for child docs existing on the same shard as the parent doc. It's also easy to fulfil this requirement as we have that data readily available.

Deletes however aren't as straight forward. Even the Delete API documentation mentions that routing values aren't often known at the time of a delete:

&gt; Many times, the routing value is not known when deleting a document. For those cases, when specifying the _routing mapping as required, and no routing value is specified, the delete will be broadcast automatically to all shards.

https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete.html#delete-routing

The breaking changes documentation only mentions 'custom routing'  in regards to the delete API - https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_crud_and_routing_changes.html#_delete_api_with_custom_routing. I didn't take this to mean deleting child docs would also be affected.

When using _custom routing_ typically that routing value would be static and known at all times, but with parent/child configurations, it now requires extra leg work to look up associated parent ids.

I can understand from a purely correctness point of view that the routing should be enforced but from a practical perspective it makes the APIs (including bulk) much harder to consume.
</comment><comment author="javanna" created="2016-08-08T08:55:41Z" id="238178631">Hi @rayward I amended the documentation, sorry about the leftover. I think that broadcasting the delete request to all shards was a wrong tradeoff. Trying the request on all shards is a scary thing, also made worse by the problems caused in the bulk api as described in https://github.com/elastic/elasticsearch/issues/16645#issuecomment-184251436.

Ideally one would always provide the routing value when deleting a document. It is certainly a shame if this requirement causes problems to our users. We recommend using delete by query when the routing value is not known.
</comment><comment author="mrkamel" created="2016-08-08T09:03:42Z" id="238180492">i'd like to mention that delete by query doesn't seem to be a perfect fit, because it seems much slower and i guess you need to refresh the index before the call to be sure to not miss docs.
</comment><comment author="javanna" created="2016-08-08T09:16:11Z" id="238183279">You are right @mrkamel delete by query executes a search. The perfect fit is providing the routing values when they were used at index time. Or even consider not using custom routing as a whole given that it complicates things in some cases.
</comment><comment author="rayward" created="2016-08-08T23:03:06Z" id="238404087">Thanks for amending the docs @javanna, I guess I can understand sending those cluster wide requests could be problematic, especially on indexes with many shards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias with filter is not reliable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10135</link><project id="" key="" /><description>I am using elasticsearch 1.4.4 and I want to create a filtered alias. 
The index contains 3 types A, B and C. B and C are children to A. The filter extracts from the index all the types applying some conditions.For one of the types (let set it's A) the filter contains a nested and a has_child filter.
The alias gets created and works fine until I try to update the filter. I need to update the filter because 
some entities are soft deleted, but the alias should not return them. In this case the filter changes to something like extract all documents with type A where B deleted date &gt; some date.

After I update the filter and try to query for A with a simple search it sometimes return all A (applying the filter) and sometimes it returns only some of the documents, excluding others that should be returned. This happens on consecutive queries sent seconds apart. Do you have any ideas on what happens? Would it be helpful to post the filter?

The filter works fine if not used in the alias context.
</description><key id="62660128">10135</key><summary>Alias with filter is not reliable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikiot</reporter><labels><label>:Aliases</label><label>bug</label></labels><created>2015-03-18T10:59:26Z</created><updated>2015-05-25T21:25:44Z</updated><resolved>2015-04-08T13:09:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T13:35:34Z" id="82973885">Hi @mikiot can you please post a complete curl recreation of the issue you are facing so we can have a look at what's causing it?
</comment><comment author="mikiot" created="2015-03-20T13:05:13Z" id="84007439">1. I notices that sometimes when I run a search on the alias I receive this
   "QueryPhaseExecutionException[[esbug][0]: query[ConstantScore(+cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@16c808f0) +BooleanFilter(BooleanFilter(+cache(_type:person) +QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(cache(address.number:[2 TO 2])))-&gt;random_access(_type:__address))) +CustomQueryWrappingFilter(child_filter[order/person](filtered%28ConstantScore%28BooleanFilter%28+cache%28_type:order%29 NotFilter%28cache%28_parent:person#AUw3EnRZSNvEdeURvB8t%29%29 BooleanFilter%28+cache%28_parent:person#AUw3EnRZSNvEdeURvB8t%29 +org.elasticsearch.index.query.QueryParseContext$1@3c7dd88c%29%29%29%29-&gt;cache%28_type:order%29))) BooleanFilter(+cache(_type:order) NotFilter(cache(_parent:person#AUw3EnRZSNvEdeURvB8t)) BooleanFilter(+cache(_parent:person#AUw3EnRZSNvEdeURvB8t) +org.elasticsearch.index.query.QueryParseContext$1@6a13ee5a))))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "
2. I have an index which I called it esbug with the following mapping

```
{
    "mappings":
    {
        "person" : {
            "properties": {
                "name": {"type": "string", "index": "not_analyzed"},
                "address": {
                 "type" : "nested",
                    "properties" : {
                        "street": {"type": "string", "index": "not_analyzed"},
                        "number": {"type": "integer"}
                    }
                }
            }
        },
        "order" : {
            "_routing": {"store": false},
            "_parent" : { "type" : "person" },
            "properties" : {
                "name" : { "type": "string"},
                "created" : { "type": "date"}
            }
        }
    }
} 
```

I add the following documents to it

```
POST esbug/person/AUw3EnRZSNvEdeURvB8t
{
"name": "Person One",
"address": {
        "street": "NA",
        "number": "2"
    }
}

POST esbug/person/AUw3EiZpSNvEdeURvB8s
{
"name": "Person Two",
"address": {
        "street": "NA",
        "number": "2"
    }
}

POST esbug/order?parent=AUw3EnRZSNvEdeURvB8t
{
    "name": "Order One",
    "created": "2015-03-20T08:58:50.941Z"
}


POST esbug/order?parent=AUw3EnRZSNvEdeURvB8t
{
    "name": "Order Two",
    "created": "2015-03-20T09:01:50.941Z"
}

POST esbug/order?parent=AUw3FHJUSNvEdeURvB8w
{
    "name": "Order Three",
    "created": "2015-03-20T09:01:50.941Z"
}
```

Then I create the index alias

```
POST _aliases
{
    "actions" : [
        {
            "add" : {
                 "index" : "esbug",
                 "alias" : "esbug_123",
                 "filter":{
               "bool": {
                  "should": [
                     {
                        "bool": {
                           "must": [
                              {
                                 "term": {
                                    "_type": [
                                       "person"
                                    ]
                                 }
                              },
                              {
                                 "nested": {
                                    "path": "address",
                                    "filter": {
                                       "term": {
                                          "number": 2
                                       }
                                    }
                                 }
                              },
                              {
                                    "has_child": {
                                        "type": "order",
                                        "filter": {
                                            "bool": {
                                               "must": [
                                                  {
                                                     "term": {
                                                        "_type": "order"
                                                     }
                                                  }
                                               ],
                                               "should": [
                                                  {
                                                     "not": {
                                                        "terms": {
                                                           "_parent": [
                                                              "AUw3EnRZSNvEdeURvB8t"
                                                           ]
                                                        }
                                                     }
                                                  },
                                                  {
                                                     "bool": {
                                                        "must": [
                                                           {
                                                              "term": {
                                                                 "_parent": "AUw3EnRZSNvEdeURvB8t"
                                                              }
                                                           },
                                                           {
                                                              "range": {
                                                                 "created": {
                                                                    "gt": "2015-03-20T09:00:50.941Z"
                                                                 }
                                                              }
                                                           }
                                                        ]
                                                     }
                                                  }
                                               ]
                                            }
                                        }
                                    }
                                } 
                           ]
                        }
                     },
                     {
                        "bool": {
                           "must": [
                              {
                                 "term": {
                                    "_type": "order"
                                 }
                              }
                           ],
                           "should": [
                              {
                                 "not": {
                                    "terms": {
                                       "_parent": [
                                          "AUw3EnRZSNvEdeURvB8t"
                                       ]
                                    }
                                 }
                              },
                              {
                                 "bool": {
                                    "must": [
                                       {
                                          "term": {
                                             "_parent": "AUw3EnRZSNvEdeURvB8t"
                                          }
                                       },
                                       {
                                          "range": {
                                             "created": {
                                                "gt": "2015-03-20T09:00:50.941Z"
                                             }
                                          }
                                       }
                                    ]
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            }
            }
        }
    ]
}
```

Performing a simple query like GET esbug_123/_search produces inconsistent results.
</comment><comment author="javanna" created="2015-03-24T15:36:50Z" id="85563012">Hi @mikiot I'm having troubles parsing your alias request, the filter doesn't fit in my screen :) Did you try to trim it down a bit and see which part is causing the problem that you see exactly? Did you try executing the flter by itself, without going through the alias?
</comment><comment author="mikiot" created="2015-03-24T16:07:17Z" id="85580783">What do you mean by it does not fit in the screen?
The problem is caused by the has_child filter. The filter works fine by it's on. 
I removed some parts of the filter and removed indentation so you can see the filter.

```
{"actions":[{"add":{"index":"esbug","alias":"esbug_123","filter":{"bool":{"should":[{"bool":{"must":[{"term":{"_type":["person"]}},{"has_child":{"type":"order","filter":{"bool":{"must":[{"term":{"_type":"order"}}],"should":[{"not":{"terms":{"_parent":["AUw3EnRZSNvEdeURvB8t"]}}},{"bool":{"must":[{"term":{"_parent":"AUw3EnRZSNvEdeURvB8t"}},{"range":{"created":{"gt":"2015-03-20T09:00:50.941Z"}}}]}}]}}}}]}}]}}}}]}
```
</comment><comment author="javanna" created="2015-03-25T14:49:33Z" id="86064028">sorry @mikiot about the irony, I just meant that the filter is very big and hard to read. Much better now, thanks a lot for trimming it down. I managed to reproduce the problem, it does seem like a bug.

@martijnvg can you have a look please? here is the stacktrace (ran from current 1.x), seems like `docIdsSet` is null [here](https://github.com/elastic/elasticsearch/blob/1.x/src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java#L82). I miss why though, might it be that the filter gets closed before the end of its execution?

```
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.search.child.CustomQueryWrappingFilter.getDocIdSet(CustomQueryWrappingFilter.java:82)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:59)
    at org.elasticsearch.common.lucene.search.AndFilter.getDocIdSet(AndFilter.java:54)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:46)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:157)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:131)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.bulkScorer(ConstantScoreQuery.java:141)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
```
</comment><comment author="mikiot" created="2015-03-25T15:49:17Z" id="86091375">@javanna You know you can always use a json formatter like this one https://www.jsoneditoronline.org/ for big jsons. This is what I used to remove the indentation. ;)
</comment><comment author="javanna" created="2015-03-25T15:50:25Z" id="86091999">@mikiot I know and I spent some time doing it yesterday but somehow elasticsearch would refuse that filter although the json seemed valid, now it's better, thanks again
</comment><comment author="javanna" created="2015-03-30T09:51:18Z" id="87617880">Talked with @martijnvg (parent-child wizard) and he says that has_child &amp; has_parent don't work properly as part of alias filters and percolator. There is no currently way to support them properly, thus we are leaning towards rejecting them completely when presented as part of alias filters or percolator queries.
</comment><comment author="jpountz" created="2015-03-30T13:48:51Z" id="87687422">+1 to reject p/c filters from aliases for now.

One issue with those filters is that the set of documents that match on a segment also depends on data that are stored in other segments because of the join. However, filters work per segment and it is expected that they always produce the same set of matches per segment (otherwise we could never cache). This does not work for p/c queries. We have some hacks in place to make sure when using p/c filters that they are never cached (NoCacheFilter) and that the join is performed only once (the horrible CustomQueryWrappingFilter) but it does not work with aliases which enforce caching.

We might have opportunities to make it better in 2.x with the queries/filter merge, but that would still be a lot of work and until then it is probably wiser to just reject such filters instead of adding on the existing hacks.
</comment><comment author="martijnvg" created="2015-03-30T14:05:33Z" id="87691758">What I realized was that the actually issue here with `CustomQueryWrappingFilter` is that it keeps state and that alias filters produce a Lucene filter that is shared on the index level. So many shard level requests use the same `CustomQueryWrappingFilter` instance and each request modifies the `docIdSets` field...

I think the only sane thing to do here is to prohibit the use of p/c filters in index aliases. Also when #8134 gets fixed then p/c filters can no longer be used in index aliases.
</comment><comment author="mikiot" created="2015-04-03T09:05:06Z" id="89229552">So you say it is impossible to be fixed for now? Should I find another solution for problem?
</comment><comment author="jpountz" created="2015-04-03T09:20:52Z" id="89234340">@mikiot Sadly yes. A work-around is to provide the parent/child filter on every query execution instead of relying on the alias.
</comment><comment author="martijnvg" created="2015-04-03T10:24:30Z" id="89248979">If alias filter were parsed at search time instead of alias creation time then this problem wouldn't exist. Maybe for certain alias filter this would make sense? Also this not so nice work around can be removed: #8534
</comment><comment author="kimchy" created="2015-04-04T13:37:30Z" id="89579164">@martijnvg ++ on parsing alias filters on search time each time, its cheap and safer
</comment><comment author="clintongormley" created="2015-04-08T13:09:50Z" id="90910130">Closing in favour of #10485
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10134</link><project id="" key="" /><description /><key id="62607705">10134</key><summary>Upgrade to Jackson 2.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T06:51:39Z</created><updated>2015-08-25T13:25:02Z</updated><resolved>2015-03-22T20:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-20T17:57:34Z" id="84088759">LGTM
</comment><comment author="s1monw" created="2015-03-20T20:00:31Z" id="84129723">I did get some test failures with this.. need to look more into it
</comment><comment author="kimchy" created="2015-03-20T20:27:27Z" id="84135562">I suggest we get #10026 in for latest 2.4 version with better cbor support, I can then look into the jackson upgrade for 2.5 if nobody picks it up
</comment><comment author="kimchy" created="2015-03-22T17:38:13Z" id="84663438">seems like we have places where we don't call "startObject" before adding fields, and 2.5 is not lenient around it (which is great!). Will try and chase those up.
</comment><comment author="kimchy" created="2015-03-22T20:22:53Z" id="84692872">I create a pull request fixing it where applicable, closing in favor of #10210
</comment><comment author="s1monw" created="2015-03-23T13:26:16Z" id="84996230">thx @kimchy 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore doesn't prefix restore index settings with `index.` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10133</link><project id="" key="" /><description>When creating an index we allow to omit `index.` ie. `number_of_replicas` will also be accepted. If we do this during restore the setting is silently ignored. 
</description><key id="62561766">10133</key><summary>Restore doesn't prefix restore index settings with `index.` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-18T01:50:46Z</created><updated>2015-03-30T23:05:25Z</updated><resolved>2015-03-30T23:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unable to add user using shield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10132</link><project id="" key="" /><description>I have installed license and shield, and get this stack trace:

```
[vagrant@es02 elasticsearch]$ sudo bash bin/shield/esusers adduser es_user -r admin
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/common/cli/CliTool
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.cli.CliTool
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 13 more
```

Any ideas
</description><key id="62542933">10132</key><summary>Unable to add user using shield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rishi-freshbooks</reporter><labels /><created>2015-03-17T23:35:25Z</created><updated>2015-03-18T18:32:14Z</updated><resolved>2015-03-18T17:08:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T13:33:00Z" id="82972655">Hi @rishi-freshbooks which version of elasticsearch are you using? 
</comment><comment author="rishi-freshbooks" created="2015-03-18T13:36:50Z" id="82974712">Hi @javanna, I am using 1.4.4 with Java 1.7.0_75 on CentOS 5.
</comment><comment author="javanna" created="2015-03-18T16:48:13Z" id="83054532">that sounds good @rishi-freshbooks . how did you install elasticsearch? Is the `lib` directory containing the elasticsearch jar in `$ES_HOME/lib`? Does the current user have access to it?
</comment><comment author="rishi-freshbooks" created="2015-03-18T17:00:55Z" id="83057729">@javanna  I installed elasticseach through "yum install elasticseach".
`$ES_HOME = /usr/share/elasticsearch`

`$ES_HOME/lib` does have the jar.

Current user does have read access:

```
[vagrant@es01 ~]$ ls -l /usr/share/elasticsearch/lib/
total 20228
-rw-r--r-- 1 root root 12480928 Jul  9  2014 elasticsearch-1.2.2.jar
-rw-r--r-- 1 root root   914597 Jul  9  2014 jna-4.1.0.jar
-rw-r--r-- 1 root root   794991 Jul  9  2014 jts-1.13.jar
-rw-r--r-- 1 root root   489884 Jul  9  2014 log4j-1.2.17.jar
-rw-r--r-- 1 root root  1688588 Jul  9  2014 lucene-analyzers-common-4.8.1.jar
-rw-r--r-- 1 root root   404512 Jul  9  2014 lucene-codecs-4.8.1.jar
-rw-r--r-- 1 root root  2411877 Jul  9  2014 lucene-core-4.8.1.jar
-rw-r--r-- 1 root root   107738 Jul  9  2014 lucene-grouping-4.8.1.jar
-rw-r--r-- 1 root root   138211 Jul  9  2014 lucene-highlighter-4.8.1.jar
-rw-r--r-- 1 root root    64185 Jul  9  2014 lucene-join-4.8.1.jar
-rw-r--r-- 1 root root    36027 Jul  9  2014 lucene-memory-4.8.1.jar
-rw-r--r-- 1 root root    95888 Jul  9  2014 lucene-misc-4.8.1.jar
-rw-r--r-- 1 root root   211084 Jul  9  2014 lucene-queries-4.8.1.jar
-rw-r--r-- 1 root root   390591 Jul  9  2014 lucene-queryparser-4.8.1.jar
-rw-r--r-- 1 root root    54579 Jul  9  2014 lucene-sandbox-4.8.1.jar
-rw-r--r-- 1 root root   108199 Jul  9  2014 lucene-spatial-4.8.1.jar
-rw-r--r-- 1 root root   178673 Jul  9  2014 lucene-suggest-4.8.1.jar
drwxr-xr-x 2 root root     4096 Mar 16 16:31 sigar
-rw-r--r-- 1 root root   102177 Jul  9  2014 spatial4j-0.4.1.jar
```
</comment><comment author="javanna" created="2015-03-18T17:04:13Z" id="83058456">the lib directory doesn't contain `1.4.4` but `elasticsearch-1.2.2.jar`, that is why you get that error and the creation of users fail. Also...not related but important too, do you really need to install elasticsearch as root? That doesn't seem like a great idea to me.
</comment><comment author="rishi-freshbooks" created="2015-03-18T17:08:51Z" id="83060454">My apologies! What a silly a mistake, I was toggling between environments (one has 1.4.4 and the other 1.2.2)... you are right.

I agree;  it does not need to be installed as root; should create a elasticsearch user.

Thanks!! Closing ticket.
</comment><comment author="javanna" created="2015-03-18T18:32:14Z" id="83113208">Glad to hear you found the issue, thanks for letting us know!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Always send current ES version when downloading plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10131</link><project id="" key="" /><description>to enable download servers to send the correct plugin version for the
node that is installing it this PR sends the current version as a header
to the server.
</description><key id="62542236">10131</key><summary>Always send current ES version when downloading plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-17T23:30:40Z</created><updated>2015-05-29T16:04:30Z</updated><resolved>2015-03-17T23:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-17T23:31:18Z" id="82639687">@drewr can you take a look
</comment><comment author="drewr" created="2015-03-17T23:37:03Z" id="82641136">LGTM. BTW, the reason it doesn't have an `X-` prefix is due to [RFC 6648](https://tools.ietf.org/html/rfc6648). Better explanation at http://stackoverflow.com/a/3561399/3227.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add time zone setting for relative date math for date range aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10130</link><project id="" key="" /><description>Hi, 

The following issue has been resolved for the range filter/query.
https://github.com/elastic/elasticsearch/issues/3729

Is it possible to add something similar to date range aggregation? The default date is UTC time by default, but we need to use now in PST time zone. 

``` php
{
    "aggs": {
        "range": {
            "date_range": {
                "field": "date",
                "format": "YYYY-MM-dd HH:mm:ss",
                "ranges": [
                    { "to": "now-1d/d" }, 
                    { "from": "now-1d/d" } 
                ]
            }
        }
    }
}
```
</description><key id="62460680">10130</key><summary>add time zone setting for relative date math for date range aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">csezheng</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-03-17T17:03:53Z</created><updated>2016-03-07T15:11:55Z</updated><resolved>2016-03-07T15:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wkoot" created="2015-05-21T11:48:16Z" id="104240418">Ran into the same issue, would be nice if `time_zone` is indeed picked up.
I'm currently using the following workaround in python, as per your example:

```
from datetime import datetime
timezone_offset = "+%iH" % round(float((datetime.now() - datetime.utcnow()).seconds) / 60 / 60)

query = {
    "aggs": {
        "range": {
            "date_range": {
                "field": "date",
                "format": "YYYY-MM-dd HH:mm:ss",
                "ranges": [
                    { "to": "now-1d/d" + timezone_offset }, 
                    { "from": "now-1d/d" + timezone_offset } 
                ]
            }
        }
    }
}
```
</comment><comment author="mj1856" created="2015-06-29T19:16:43Z" id="116801153">Keep in mind that offsets can change within a single time zone, due to daylight saving time, and other various anomalies.  See "Time Zone != Offset" in [the timezone tag wiki on StackOverflow](http://stackoverflow.com/tags/timezone/info).

For an aggregation to be meaningful in a specific time zone, such as US Pacific Time, the time zone itself would have to be understood by ElasticSearch.  This would probably be best expressed as an IANA/Olson time zone identifier, such as `"America/Los_Angeles"`.   This would also mean that an ElasticSearch cluster would have to have a process for receiving regular updates to the IANA tzdb, which can happen about a dozen times per year or so.

@wkoot - In your example, the `timezone_offset` field is calculated based on the _current_ difference between the local time zone and UTC.  There's no guarantee that that offset is the correct one for any of the dates covered by the to/from values of the query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>download.elasticsearch.org: ERROR: no certificate subject alternative name matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10129</link><project id="" key="" /><description>Sorry I couldn't see a repository specifically for the elasticsearch.org site. We have a travis install script that uses `wget` to get the elasticsearch `.deb` from download.elasticsearch.org:

https://github.com/mozilla/kuma/blob/3ae992e8f1552c227b26aa02954035da8014043e/scripts/travis-install#L18

But we just started to get this error today from [our Travis builds](https://travis-ci.org/mozilla/kuma/jobs/54732362).

```
+wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.3.7.deb
--2015-03-17 15:20:49--  https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.3.7.deb
Resolving download.elasticsearch.org (download.elasticsearch.org)... 2406:da00:ff00::36f3:4d9e, 2406:da00:ff00::36e1:85c3, 2406:da00:ff00::36e1:40a1, ...
Connecting to download.elasticsearch.org (download.elasticsearch.org)|2406:da00:ff00::36f3:4d9e|:443... connected.
ERROR: no certificate subject alternative name matches
    requested host name `download.elasticsearch.org'.
To connect to download.elasticsearch.org insecurely, use `--no-check-certificate'.
```
</description><key id="62459149">10129</key><summary>download.elasticsearch.org: ERROR: no certificate subject alternative name matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">groovecoder</reporter><labels /><created>2015-03-17T16:58:36Z</created><updated>2015-04-04T16:25:35Z</updated><resolved>2015-04-04T16:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rkh" created="2015-03-17T17:11:33Z" id="82485012">see https://github.com/travis-ci/travis-ci/issues/3394#issuecomment-82483124 for explanation and workaround
</comment><comment author="clintongormley" created="2015-04-04T16:25:35Z" id="89609671">Also, switching the domain to `download.elastic.co` should work
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Left replicas on former cluster node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10128</link><project id="" key="" /><description>Hi everyone,

we recently seperated a two-node-cluster into two single elasticsearch instances for testing porposes. In the cluster, every index had 5 shards and 1 replicas. Now, we set the number of replicas to zero and restarted the node. The problem is that the "replica shards" remain unassigned. How can we delete these shards? They cannot be reassigned, because there is only a single node left...

Greetings,
Marco
</description><key id="62419407">10128</key><summary>Left replicas on former cluster node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">misterunknown</reporter><labels /><created>2015-03-17T14:54:52Z</created><updated>2015-03-17T15:20:37Z</updated><resolved>2015-03-17T15:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="misterunknown" created="2015-03-17T15:20:37Z" id="82407696">Hi,

I had to set the number of replicas individually for every existing index. Now everything works fine :)

Greetings,
Marco
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent/child failing when using the same type in child and parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10127</link><project id="" key="" /><description>I am trying to model a employee hierarchy where every employee can be a manager of a team of employees and, at the same time, she can be part of a team managed by other employee:

curl -XPOST 'http://localhost:9200/manager_employee' -d '{
   "mappings": {
       "employee": {
           "_parent": {
               "type": "employee"
           },
           "properties": {
               "name": {
                   "type": "string"
               }
           }
       }
   }
}'

The problem is when modeling a employee which is not managed by anyone (let's think about the CEO). I tried both alternatives bellow: 

curl -s -XPOST localhost:9200/_bulk --data-binary '
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee1" , _parent:"employee1"} }
{ "name" : "chris"}
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee2" , _parent:"employee1"} }
{ "name" : "joseph" }
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee3" , _parent:"employee1"} }
{ "name" : "john" }'

i.e., the CEO is managed by herself and 

curl -s -XPOST localhost:9200/_bulk --data-binary '
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee1" , _parent:"dummyID1"} }
{ "name" : "chris"}
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee2" , _parent:"employee1", _routing:"dummyID1"} }
{ "name" : "joseph" }
{ "index" : { "_index" : "manager_employee", "_type" : "employee", "_id" : "employee3" , _parent:"employee1", _routing:"dummyID1"} }
{ "name" : "john" }'

the CEO is managed by some dummy manager

In both cases: 

curl -XGET 'localhost:9200/manager_employee/employee/_search' -d '{
"filter" : {
   "has_child" : {
       "child_type" : "employee",
       "query" : {
           "term" : {
               "employee.name" : "john"
           }
       }
   }}}'

works returning chris, but the other side of the relations fails: 

curl -XGET 'localhost:9200/manager_employee/employee/_search' -d '{
"filter" : {
   "has_parent" : {
       "parent_type" : "employee",
       "query" : {
           "term" : {
               "employee.name" : "chris"
           }
       }
   }}}'

returns an empty hit set. 

Is there any other way (without defining different) types of implementing this? Any workaround? Is it a bug that the relation is correctly mapped from children to parent but not from parent to children when using the same type in both sides of the relation? 

Thank you in advance, 
</description><key id="62368662">10127</key><summary>Parent/child failing when using the same type in child and parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">parapar</reporter><labels /><created>2015-03-17T11:21:15Z</created><updated>2015-04-04T16:18:22Z</updated><resolved>2015-04-04T16:18:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T16:18:22Z" id="89608606">Hi @parapar 

This appears to be a duplicate of #7357.  Btw, while parent-child seems to be a tempting way to model relationships like a relational DB, it is usually the wrong approach and ends up complicating things considerably.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to install plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10126</link><project id="" key="" /><description>I want to install some river plugins on my elasticsearch instance but because I installed elasticsearch via deb, I cannot locate the bin directory. Please how do I go about this?
</description><key id="62358566">10126</key><summary>Unable to install plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OElesin</reporter><labels /><created>2015-03-17T10:44:02Z</created><updated>2015-03-17T11:01:47Z</updated><resolved>2015-03-17T11:01:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JLVeiga" created="2015-03-17T10:49:27Z" id="82267993">If you&#8217;ve installed the .deb package, then the plugin exectuable will be available at /usr/share/elasticsearch/bin/plugin
</comment><comment author="OElesin" created="2015-03-17T11:01:44Z" id="82278153">Found it.Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sum Aggregation script does not work in version 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10125</link><project id="" key="" /><description>We have a query where we do a lot of aggregation. The query looks like this:
{
  "aggs": {
    "mileage": {
      "extended_stats": {
        "field": "mileage"
      }
    },
    "retailPrice": {
      "stats": {
        "field": "retailPrice"
      }
    },
    "sum": {
      "sum": {
        "script": "doc['mileage'].value \* doc['retailPrice'].value"
      }
    }
  },
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "and": {
          "filters": [
            {
              "bool": {
                "must": [
                  {
                    "exists": {
                      "field": "mileage"
                    }
                  },
                  {
                    "exists": {
                      "field": "retailPrice"
                    }
                  }
                ]
              }
            },
            {},
            {
              "term": {
                "isNew": false
              }
            },
            {
              "term": {
                "isInternal": false
              }
            },
            {
              "term": {
                "isTest": false
              }
            },
            {
              "exists": {
                "field": "dealerId"
              }
            },
            {
              "term": {
                "isSold": false
              }
            },
            {
              "missing": {
                "field": "deletedDate"
              }
            }
          ]
        }
      }
    }
  }
}
It use the script tag in the aggregation it does not work. It worked in an earlier version, we used 1.0.2 (very old i know)
I get this error message:
{
"error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[xBDvuVNyQGiRf2RrkNUMYw][dealerhub2][0]: SearchParseException[[dealerhub2][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"aggs":{"mileage":{"extended_stats":{"field":"mileage"}},"retailPrice":{"stats":{"field":"retailPrice"}},"sum":{"sum":{"script":"doc['mileage'].value \* doc['retailPrice'].value"}}},"query":{"filtered":{"query":{"match_all":{}},"filter":{"and":{"filters":[{"bool":{"must":[{"exists":{"field":"mileage"}},{"exists":{"field":"retailPrice"}}]}},{},{"term":{"isNew":false}},{"term":{"isInternal":false}},{"term":{"isTest":false}},{"exists":{"field":"dealerId"}},{"term":{"isSold":false}},{"missing":{"field":"deletedDate"}}]}}}}}]]]; nested: ScriptException[dynamic scripting for [groovy] disabled]; }]",
"status": 400
}

I have looked in the documentation, and from that it looks like it should still be a valid query:
http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-sum-aggregation.html
</description><key id="62352100">10125</key><summary>Sum Aggregation script does not work in version 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thrane</reporter><labels /><created>2015-03-17T10:07:32Z</created><updated>2015-03-17T14:15:05Z</updated><resolved>2015-03-17T13:11:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thrane" created="2015-03-17T14:15:05Z" id="82374793">This is not an issue after all. It was due to the fact that we didnt set script.groovy.sandbox.enabled: false in our elasticsearch.yml. After setting this it works as usual
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Override jackson's TextBuffer to avoid NPE in SmileParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10124</link><project id="" key="" /><description>If you send a smile request with an empty value in the term query in ES 1.4.4, NPE may occur.
There is a bug in jackson https://github.com/FasterXML/jackson-core/issues/182.
But they haven't released it yet. 
That's why I proposed you to use a fixed version of this file until they release it.

Anyway you'd better know about this problem.
</description><key id="62346661">10124</key><summary>Override jackson's TextBuffer to avoid NPE in SmileParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">v1ctor</reporter><labels><label>discuss</label></labels><created>2015-03-17T09:40:00Z</created><updated>2015-04-06T04:00:03Z</updated><resolved>2015-04-06T04:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="masaruh" created="2015-03-17T22:15:58Z" id="82620963">FYI, there's an issue for this, #8629.
</comment><comment author="s1monw" created="2015-03-18T06:46:40Z" id="82780782">I am kind of hesitating to do this. Not sure if it's worth adding a fork of this file or if we should just wait for a 2.6.0 release? @kimchy opinions?
</comment><comment author="masaruh" created="2015-03-31T06:47:05Z" id="87966121">@s1monw Not mean to hijack this PR but I created a PR(#10333) to fix the issue in a different way in case overriding TextBuffer isn't feasible.
</comment><comment author="kimchy" created="2015-04-02T14:02:50Z" id="88915384">agreed its tricky, and I think this should be solved in Jackson and for now we can do #10333 
</comment><comment author="masaruh" created="2015-04-06T04:00:03Z" id="89911894">Closing in favor of #10333.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Balance Shard Recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10123</link><project id="" key="" /><description>When recovering it appears that the choice of which shards to recover concurrently does not lead to a deterministic and sensible order.  For example, in a cluster where the indices have 3 replicas (for a total of 4 copies of data) and 4 shards, within a single index all of shard 1 can be recovered before shard 2.  Additionally, across indices within the same cluster, it would be preferable to ensure that multiple copies of data exists as soon as possible for all shards in all indices, before loading extra replicas.
</description><key id="62322944">10123</key><summary>Balance Shard Recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kadaan</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-03-17T07:57:21Z</created><updated>2015-12-05T20:36:34Z</updated><resolved>2015-12-05T20:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kadaan" created="2015-03-17T07:58:25Z" id="82191311">Related to: elastic/elasticsearch#10121
</comment><comment author="kadaan" created="2015-05-29T13:57:05Z" id="106811530">Additionally, it would be useful for Elasticsearch to try harder to determine which shards can be recovered with zero copying and perform those first. This would allow more shards to be recovered earlier in the recovery. 
</comment><comment author="kadaan" created="2015-06-02T20:11:32Z" id="108082736">The node_concurrent_recoveries  setting also can cause slow recoveries by forcing shard to start recovering on a non-optimal node because there are not enough recovery slots on the optimal node.  ES should determine that it is better to wait to start recovery on certain shards rather than force data movement.
</comment><comment author="kadaan" created="2015-06-03T17:33:53Z" id="108538447">Related to: elastic/elasticsearch#2908
</comment><comment author="clintongormley" created="2015-12-05T20:36:34Z" id="162243934">We now recover sync flushed shards first, and we support recovery order. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dead Master Resulted In Red Cluster Health</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10122</link><project id="" key="" /><description>The master node on our cluster died, which resulted in a cluster with red health.  All shards were unassigned and had to be recovered.  The failure of the master node should not result in a dead cluster.
</description><key id="62321545">10122</key><summary>Dead Master Resulted In Red Cluster Health</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kadaan</reporter><labels><label>feedback_needed</label></labels><created>2015-03-17T07:47:23Z</created><updated>2015-12-05T20:35:10Z</updated><resolved>2015-12-05T20:35:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-20T11:16:30Z" id="83988096">Indeed it shouldn&#8217;t. Can you give some more information? Typically a new master should be elected and take over the role of the old master&#8230;

&gt; On 17 Mar 2015, at 08:47, kadaan notifications@github.com wrote:
&gt; 
&gt; The master node on our cluster died, which resulted in a cluster with red health. All shards were unassigned and had to be recovered. The failure of the master node should not result in a dead cluster.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="kadaan" created="2015-03-21T03:46:22Z" id="84239034">Basically I had a node which would not join the cluster.  It was not able to communicate with the current master and the master could not communicate with the node. After numerous attempts to repair it I decided to shutdown the master to see if moving the master to another node would help. I shut it down via kopf (which uses the shutdown api I believe). Right after this kopf went dead and then the cluster was red with all shards unassigned. i will get the logs from all modes to see what happened.
</comment><comment author="clintongormley" created="2015-04-26T20:03:17Z" id="96428934">@kadaan any more info?  Also, did you only have one master eligible node?  What is your cluster config?
</comment><comment author="kadaan" created="2015-04-27T05:27:33Z" id="96512026">Unfortunately I don't have any more info.  All nodes are master eligible, so there should have been 11 others able to step in.  I have not seen this since, so I would be fine to close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery Priority</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10121</link><project id="" key="" /><description>It would be really useful for time based indices to be able to indicate recovery priority so that we can indicate that the latest index should be recovered first.  This will allow us to resume indexing more quickly.
</description><key id="62321000">10121</key><summary>Recovery Priority</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kadaan</reporter><labels /><created>2015-03-17T07:44:33Z</created><updated>2015-03-17T08:05:29Z</updated><resolved>2015-03-17T08:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kadaan" created="2015-03-17T07:58:01Z" id="82191270">Related to: elastic/elasticsearch#10123
</comment><comment author="kadaan" created="2015-03-17T08:05:29Z" id="82193745">Duplicate of: elastic/elasticsearch#10069
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove recommendation for rolling upgrades.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10120</link><project id="" key="" /><description>Add read-only/flush option for restart to improve recovery time.
Closes #10118 
Also removed old 0.90.x instructions.  Those may be better served in their own document covering upgrading from 0.90 to 1.x.
</description><key id="62248947">10120</key><summary>Remove recommendation for rolling upgrades.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">seang-es</reporter><labels /><created>2015-03-17T01:06:17Z</created><updated>2016-03-08T19:17:44Z</updated><resolved>2016-03-08T19:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T19:17:44Z" id="193926002">No longer needed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add command-line configuration check command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10119</link><project id="" key="" /><description>I'd like to run a command that confirms:
  The contents of my `elasticsearch.yml` does not result in an exception being thrown at startup due to a configuration item value being invalid.

An example of this pattern can be found in the bind nameserver:
`named-checkconf /etc/named.conf`

This would allow configuration automation tools to validate files after modification but before restarting the service and finding out it doesn't start.
</description><key id="62245840">10119</key><summary>add command-line configuration check command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tamsky</reporter><labels><label>:Settings</label><label>enhancement</label><label>stalled</label></labels><created>2015-03-17T00:44:25Z</created><updated>2015-12-05T20:34:48Z</updated><resolved>2015-12-05T20:34:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T16:02:47Z" id="89604854">Interesting idea.  First step though is actually detecting bad settings in Elasticsearch itself.  Very often we'll just ignore anything we don't understand.  Stalled by #6732
</comment><comment author="clintongormley" created="2015-12-05T20:34:48Z" id="162243804">Closing in favour of #6732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change upgrade docs to avoid rolling upgrades</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10118</link><project id="" key="" /><description>Rolling upgrades are causing problems with data corruption in some cases.  Updating documentation to recommend full cluster stops during upgrades.

Also adding recommendation to halt writes/flush data before restarting to reduce recovery time.
</description><key id="62245722">10118</key><summary>Change upgrade docs to avoid rolling upgrades</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/seang-es/following{/other_user}', u'events_url': u'https://api.github.com/users/seang-es/events{/privacy}', u'organizations_url': u'https://api.github.com/users/seang-es/orgs', u'url': u'https://api.github.com/users/seang-es', u'gists_url': u'https://api.github.com/users/seang-es/gists{/gist_id}', u'html_url': u'https://github.com/seang-es', u'subscriptions_url': u'https://api.github.com/users/seang-es/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/6352850?v=4', u'repos_url': u'https://api.github.com/users/seang-es/repos', u'received_events_url': u'https://api.github.com/users/seang-es/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/seang-es/starred{/owner}{/repo}', u'site_admin': False, u'login': u'seang-es', u'type': u'User', u'id': 6352850, u'followers_url': u'https://api.github.com/users/seang-es/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>discuss</label></labels><created>2015-03-17T00:42:50Z</created><updated>2015-12-05T20:33:08Z</updated><resolved>2015-12-05T20:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kadaan" created="2015-03-17T08:03:13Z" id="82192989">While I understand your reasoning for removing the recommendation, this will require an outage to upgrade ES, which for many use cases will not be acceptable.  Being able to upgrade a live system and in a backwards compatible way (and being able to roll back the changes if needed) should be a core principle for all distributed systems.  Without that ability, customers will be much more reticent to upgrade.
</comment><comment author="allenmchan" created="2015-03-18T00:02:49Z" id="82647751">If elasticsearch needs a full down to be upgraded, that means that the logstash ecosystem would screech to a halt as well. I sure hope this recommendation is just temporary and ES devs are working hard to make upgrades possible without full cluster down.
</comment><comment author="nik9000" created="2015-03-18T14:44:25Z" id="83002900">&gt; If elasticsearch needs a full down to be upgraded, that means that the logstash ecosystem would screech to a halt as well. I sure hope this recommendation is just temporary and ES devs are working hard to make upgrades possible without full cluster down.

+1

This is pretty nightmarish for those of us with site search powered by Elasticsearch.  We do plan to build a second cluster in a backup data center that we could fail over to.  But that is many months away.

This is also pretty different than the story that I heard at Elasticon regarding sealed indexes making rolling restarts faster and simpler.  We were planning on doing them _more_ often with support from that one day.

Is the trouble writes during the restart?  Merges?

Not supporting rolling restart would have caused us not to select Elasticsearch.
</comment><comment author="s1monw" created="2015-03-18T15:02:32Z" id="83011549">I think this PR needs some clarification. IMO the relevant versions that are risky to upgrade are 1.3.x and 1.4.x where a lot of checksum problems were uncovered. We had false positives on old lucene 3.x segments as well as truncation issues due to the way recoveries worked. In the upcoming release (planned for next week) of Elasticsearch `1.5.0` all known issues related to this are resolved and it's safe to do a rolling upgrade. I think this PR should specify the risky version rather than making a general statement.

I also would love the users to not get paranoid about this, we have not even had a review cycle on this PR. We operate very open here and obviously don't want to hold this information back from anybody but there is no reason for get worried that this won't get resolved. I can assure you we spend almost all our time these days on infrastructure and stability that's also a reason why `1.5.0` doesn't have that many user-facing features. There will likely be a `1.6.0` release with https://github.com/elastic/elasticsearch/issues/10032 to improve this situation dramatically. So stay tuned we are working on all ends.

&gt; Being able to upgrade a live system and in a backwards compatible way (and being able to roll back the changes if needed) should be a core principle for all distributed systems. 

you can downgrade unless you have written to the new cluster. I personally don't know a lot of systems that are forwards compatible to be honest :)
</comment><comment author="clintongormley" created="2015-12-05T20:33:08Z" id="162243729">No longer relevant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to only return simple exception messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10117</link><project id="" key="" /><description>Adds a setting to disable detailed error messages and full exception stack traces
in http responses. When set to false, the error_trace request parameter will be
ignored and only the message of the first ElasticsearchException will be output;
no nested exception messages will be output.
</description><key id="62242265">10117</key><summary>Add option to only return simple exception messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-17T00:11:10Z</created><updated>2015-06-07T16:59:28Z</updated><resolved>2015-03-18T22:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-03-17T00:16:17Z" id="82002113">@s1monw can you review this?
</comment><comment author="jaymode" created="2015-03-18T11:41:55Z" id="82923499">@rjernst pushed a commit addressing the feedback
</comment><comment author="jaymode" created="2015-03-18T18:43:47Z" id="83117322">@rjernst pushed some commits based on the additional feedback. I also changed the setting name; I think the original name was confusing and didn't accurately represent all of the behavior being changed.
</comment><comment author="rjernst" created="2015-03-18T20:37:39Z" id="83161269">Looks good, just a couple more comments, no need for further review.
</comment><comment author="jaymode" created="2015-03-18T22:49:20Z" id="83216573">Committed in 105bdd486a0d5d374004234692f5b66450e787be and backported. Thanks @rjernst 
</comment><comment author="dadoonet" created="2015-03-28T17:37:43Z" id="87272048">@jaymode Note that this commit breaks transport plugins.
I'll fix that. What should be the default value for `RestChannel#detailedErrorsEnabled` in transport plugins? 
</comment><comment author="rjernst" created="2015-03-28T18:04:02Z" id="87274656">I think the default should always be `true`. This should only be disabled in rare circumstances (e.g. security concerns).
</comment><comment author="jaymode" created="2015-03-28T18:21:45Z" id="87277925">@dadoonet thanks for fixing it. @rjernst is right, the default should always be `true`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for fine-grained settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10116</link><project id="" key="" /><description>Note that this PR is against `1.x`.

Allow to enable/disable scripting based on their source (where they get loaded from), the  operation that executes them and their language.

The settings cover the following combinations:
- mode: `on`, `off`, `sandbox`
- source: `indexed`, `dynamic`, `file`
- engine: `groovy`, `expression`, `mustache`, etc
- operation: `update`, `search`, `aggs`, `mapping`

The following settings are supported for every engine:

```
script.engine.groovy.indexed.update:    sandbox/on/off
script.engine.groovy.indexed.search:    sandbox/on/off
script.engine.groovy.indexed.aggs:      sandbox/on/off
script.engine.groovy.indexed.mapping:   sandbox/on/off
script.engine.groovy.dynamic.update:    sandbox/on/off
script.engine.groovy.dynamic.search:    sandbox/on/off
script.engine.groovy.dynamic.aggs:      sandbox/on/off
script.engine.groovy.dynamic.mapping:   sandbox/on/off
script.engine.groovy.file.update:       sandbox/on/off
script.engine.groovy.file.search:       sandbox/on/off
script.engine.groovy.file.aggs:         sandbox/on/off
script.engine.groovy.file.mapping:      sandbox/on/off
```

For ease of use, the following more generic settings are supported too:

```
script.indexed: sandbox/on/off
script.dynamic: sandbox/on/off
script.file:    sandbox/on/off
```

and

```
script.update:  sandbox/on/off
script.search:  sandbox/on/off
script.aggs:    sandbox/on/off
script.mapping: sandbox/on/off
```

These will be used to calculate the more specific settings, using the stricter setting of each combination. Operation based settings have precedence over conflicting source based ones.

Note that the `mustache` engine is affected by generic settings applied to any language, while native scripts aren't as they are static by definition.

Also, the previous `script.disable_dynamic` setting can now be deprecated.

Closes #6418 
Closes #10274 
</description><key id="62241219">10116</key><summary>Add support for fine-grained settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>feature</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-17T00:01:49Z</created><updated>2015-05-30T10:49:33Z</updated><resolved>2015-03-26T18:57:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-17T00:23:06Z" id="82003817">@s1monw can you have a look please? @clintongormley can you please review the updated docs and the new settings api-wise?

@dadoonet have time to double check that this breaks only potentially rivers, similar to #9992 ?
</comment><comment author="s1monw" created="2015-03-18T06:29:26Z" id="82777316">Overall this looks very very good @javanna good job! I left a bunch of comments...
</comment><comment author="javanna" created="2015-03-18T18:18:38Z" id="83108251">I pushed a few more commits to address review comments, and replied to all of them. Sorry if the comments got messed up, I rebased by mistake, not intentionally...
</comment><comment author="clintongormley" created="2015-03-19T18:19:12Z" id="83699447">Docs look great @javanna 
</comment><comment author="javanna" created="2015-03-19T19:59:03Z" id="83739637">@s1monw can you have another look please?
</comment><comment author="s1monw" created="2015-03-23T09:54:41Z" id="84920047">left some more comments
</comment><comment author="javanna" created="2015-03-23T13:46:25Z" id="85001491">I pushed a few more commits that address the comments I got. The only item that is still under discussion is about the `inline` vs `dynamic` terminology above.
</comment><comment author="s1monw" created="2015-03-23T13:50:13Z" id="85003837">LGTM
</comment><comment author="javanna" created="2015-03-24T11:25:00Z" id="85455495">I updated again, to use `inline` everywhere instead of `dynamic`, @s1monw you may want to have a last look. I also adjusted the docs a bit (added coming tags and a note about mustache scripts and search templates). I think this is ready, pretty much waiting for @clintongormley 's blessing :)
</comment><comment author="clintongormley" created="2015-03-25T14:37:13Z" id="86054355">LGTM
</comment><comment author="javanna" created="2015-03-26T19:01:50Z" id="86671775">marking as breaking as it breaks plugins that make use of `ScriptService`, for instance rivers that might allow to modify documents via scripts before indexing them. It doesn't break lang plugins though. ing @dadoonet :)
</comment><comment author="javanna" created="2015-04-08T09:58:22Z" id="90867459">Removing the breaking label, this is not breaking anymore after #10419. This change itself used to be breaking, but the breakage was never released, thus 1.6 won't break anything around `ScriptService`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change cluster name not indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10115</link><project id="" key="" /><description>Hi, I have a problem with changing elasticsearch cluster name.
If I change the cluster name (tag cluster.name) in elasticsearch.yml file config, elasticsearch doesn't create any index, when receive the input by logtash.
If I reset the elasticsearch cluster name with the default value of standard configuration (elasticsearch), elasticsearch server restart to create indexes.

Are there any steps or commands to execute to have a new cluster (not default name!), with an own name and that create correctly the indexes?

Tnxs a lot, 
</description><key id="62233651">10115</key><summary>Change cluster name not indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DanyHubGit</reporter><labels /><created>2015-03-16T23:16:44Z</created><updated>2015-03-16T23:20:19Z</updated><resolved>2015-03-16T23:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-16T23:20:19Z" id="81986341">You should ask questions on the mailing list instead of here.

That said, it's IMO a misusage of logstash. You need also to change the cluster name in LS output (depending on the elasticsearch output type you are using).

Closing as it's not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate replication:async</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10114</link><project id="" key="" /><description>Async replication clashes with index sealing (#10032).  Another downside of async replication is that clients cannot be sure that changes have reached the replica. The benefits of async replication can be achieved simply by having more clients (threads or forks). 

Let's deprecate this in 1.5
</description><key id="62228415">10114</key><summary>Deprecate replication:async</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>enhancement</label><label>v1.5.0</label></labels><created>2015-03-16T22:48:58Z</created><updated>2015-10-08T13:57:43Z</updated><resolved>2015-03-19T21:45:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-16T22:49:23Z" id="81974090">+1 to deprecate in 1.5, remove in 2.0
</comment><comment author="rjernst" created="2015-03-16T23:38:07Z" id="81992749">+1
</comment><comment author="s1monw" created="2015-03-17T00:00:49Z" id="81999463">+1
</comment><comment author="s1monw" created="2015-03-18T05:42:41Z" id="82751955">@clintongormley can you deprecate it in the docs please for 1.5
</comment><comment author="clintongormley" created="2015-03-19T14:37:04Z" id="83611327">Deprecated in the docs for 1.5.0, and removed from the docs and REST spec in 2.0.

Still needs to be removed in the code.
</comment><comment author="dadoonet" created="2015-03-31T08:08:36Z" id="87989649">Just a note here. It sounds like we deprecated this in docs but we did not add `@Deprecated` annotation in elasticsearch code itself. Should we add it so Java developers would be aware of it when compiling their code?
</comment><comment author="javanna" created="2015-03-31T08:11:44Z" id="87990333">Yes I think we should @dadoonet , can you open an issue or even take care of this with a PR against 1.x?
</comment><comment author="fsaravia" created="2015-10-07T22:00:24Z" id="146345354">Hey everyone, does [this document](https://www.elastic.co/guide/en/elasticsearch/guide/current/distrib-write.html) needs to be updated to state that `async` replication has been deprecated? The document has a `replication` section that talks about `sync` and `async` options.
</comment><comment author="clintongormley" created="2015-10-08T12:30:37Z" id="146523357">@fsaravia the Guide targets ES 1.4.  Yes, it does need a big update for 2.0... i know, i know
</comment><comment author="fsaravia" created="2015-10-08T13:57:43Z" id="146553674">@clintongormley oh, sorry about that. I didn't realize it targets ES 1.4 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping transform doesn't support file and indexed scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10113</link><project id="" key="" /><description>Looks like the mapping transform feature supports dynamic scripts only. When submitting the following request:

```
curl -XPUT localhost:9200/test -d '{
  "mappings" : {
    "example" : {
        "transform" : {
            "script_id" : "1",
            "lang": "groovy"
        }
    }
  }
}
'
```

the mapping stored in the cluster state becomes the following, hence it loses the information about where the script should be loaded from:

```
{
  "test": {
    "mappings": {
      "example": {
        "transform": {
          "script": "1",
          "lang": "groovy"
        },
        "properties": {
          "test": {
            "type": "string"
          }
        }
      }
    }
  }
}
```

Might relate to #9995.
</description><key id="62223631">10113</key><summary>Mapping transform doesn't support file and indexed scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>adoptme</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T22:18:45Z</created><updated>2015-06-02T17:53:31Z</updated><resolved>2015-06-02T17:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T17:46:48Z" id="106884698">@colings86 i thought that the change in https://github.com/elastic/elasticsearch/pull/11164 would fix this, but apparently not?  
</comment><comment author="colings86" created="2015-05-29T17:48:45Z" id="106885455">@clintongormley it should do. Have you tried it?
</comment><comment author="clintongormley" created="2015-05-29T17:52:43Z" id="106886590">yes, eg i save a file called `test.groovy`, then

```
PUT t
{
  "mappings": {
    "t": {
      "transform": {
        "script": {
          "file": "test"
        }
      }
    }
  }
}
```

then i get:

```
{
   "error": {
      "root_cause": [
         {
            "type": "script_parse_exception",
            "reason": "Value must be of type String: [script]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "mapping [t]",
      "caused_by": {
         "type": "script_parse_exception",
         "reason": "Value must be of type String: [script]"
      }
   },
   "status": 400
}
```
</comment><comment author="colings86" created="2015-05-29T17:54:21Z" id="106887690">Hmm, ok I'll take a look
</comment><comment author="colings86" created="2015-06-01T11:44:04Z" id="107410811">@clintongormley currently the `transform` object is treated as the script (this is actually (almost) backwards compatible as previously the `transform` object could have `lang`, `script` and `params` fields) , which is why you are getting the error. The following request does work:

``` javascript
PUT t
{
  "mappings": {
    "t": {
      "transform": {
          "file": "test"
      }
    }
  }
}
```

I can change it so that we require the `script` object under the `transform` object but since we only have scripted transforms it seemed cleaner this way. WDYT?
</comment><comment author="clintongormley" created="2015-06-02T17:53:30Z" id="108030282">@colings86 doh - i should have realised.  thanks for testing. 

Closed by https://github.com/elastic/elasticsearch/pull/11164
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `ignore_malformed` behaviour for ip fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10112</link><project id="" key="" /><description>Previously to this fix we were not catching the ElasticsearchIllegalArgumentException which is thrown by the IpFieldMapper when a malformed IP address is in a document which is being indexed.
</description><key id="62204397">10112</key><summary>Fix `ignore_malformed` behaviour for ip fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T20:45:19Z</created><updated>2015-06-07T18:34:40Z</updated><resolved>2015-03-16T20:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-16T20:46:11Z" id="81927855">LGTM I marked it as `1.4.5` as well thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: improve logging of external node version and build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10111</link><project id="" key="" /><description>The BWC tests also run against a snapshot build of previous release branches. Upon a failure it's important to know what commit exactly was used.
</description><key id="62182883">10111</key><summary>Tests: improve logging of external node version and build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T19:16:48Z</created><updated>2015-03-19T10:16:40Z</updated><resolved>2015-03-16T19:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-16T19:31:41Z" id="81887988">LGTM +1 to the info logging
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add filename to corruption message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10110</link><project id="" key="" /><description>Today we rely on the IndexOutput#toString method to print the actual
resource name we are verifying. This has a but in the 4.10.x series
that leaves us with the default toString. This commit adds the filename
to each corruption message for easier debugging.

Relates to #10062
</description><key id="62168576">10110</key><summary>Add filename to corruption message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Logging</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label></labels><created>2015-03-16T18:21:46Z</created><updated>2015-05-29T16:30:25Z</updated><resolved>2015-03-16T18:34:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-16T18:22:11Z" id="81853014">@rmuir can you take a look
</comment><comment author="rmuir" created="2015-03-16T18:28:05Z" id="81855415">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cat health not respecting 'ts' parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10109</link><project id="" key="" /><description>The _cat/health endpoint [is documented](http://www.elastic.co/guide/en/elasticsearch/reference/current/cat-health.html) that the timestamp can be disabled in the output. This is not working for me in 1.4.4:

```
$ curl localhost:9200
{
  "status" : 200,
  "name" : "lb-1",
  "cluster_name" : "es-example-service",
  "version" : {
    "number" : "1.4.4",
    "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
    "build_timestamp" : "2015-02-19T13:05:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
} 

$ curl 'localhost:9200/_cat/health?v'
epoch      timestamp cluster            status node.total node.data shards pri relo init unassign 
1426529705 18:15:05  es-example-service green           7         3      0   0    0    0        0 

$ curl 'localhost:9200/_cat/health?v&amp;ts=0'
epoch      timestamp cluster            status node.total node.data shards pri relo init unassign 
1426529713 18:15:13  es-example-service green           7         3      0   0    0    0        0
```
</description><key id="62167736">10109</key><summary>cat health not respecting 'ts' parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">seallison</reporter><labels><label>:CAT API</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-03-16T18:17:08Z</created><updated>2016-04-13T09:56:12Z</updated><resolved>2016-04-13T09:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-16T19:06:24Z" id="81876345">I agree that the documentation is incorrect here as we don't support this `ts=0` option.

For now, you can run:

``` sh
curl -XGET "http://localhost:9200/_cat/health?h=epoch,cluster,status,node.total,node.data,shards,pri,relo,init,unassign,pending_tasks"
```

@drewr Are we supposed to support this `ts=0` option?
</comment><comment author="drewr" created="2015-03-18T20:25:35Z" id="83156684">This was originally supported, but got lost somewhere.

The feature really needs to be a `RestTable` enhancement that can add timestamp to any API (and possibly leave it off by default).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return useful error message on potential HTTP connect to Transport port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10108</link><project id="" key="" /><description>In case a HTTP client connects to the transport protocol and issues a
HTTP method followed by a space, we can just try to be smart and return
a string back to the client to point the user to the fact that the wrong
port has been used.

Closes #2139
</description><key id="62155866">10108</key><summary>Return useful error message on potential HTTP connect to Transport port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T17:30:20Z</created><updated>2015-06-07T16:30:48Z</updated><resolved>2015-03-17T16:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-16T17:52:05Z" id="81840731">Left one comment, This looks pretty good to me, now I'm just curious how different clients will handle a non-HTTP response :)
</comment><comment author="spinscale" created="2015-03-16T18:48:39Z" id="81865033">I tested with curl and chrome and those displayed correctly.. (display as in returned the string as response)
</comment><comment author="spinscale" created="2015-03-16T18:52:31Z" id="81868112">updated to support PATCH
</comment><comment author="dakrone" created="2015-03-16T21:02:29Z" id="81937160">Left comments about adding documentation, LGTM
</comment><comment author="dakrone" created="2015-03-17T06:20:47Z" id="82136681">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run script on write to multi-index alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10107</link><project id="" key="" /><description>Indexing into multi-index aliases would be super useful. What if it were possible to define a script on an alias that runs on incoming documents and emits an index name? Sort of like an alias filter.

For example:

```
curl -XPOST 'http://localhost:9200/_aliases' -d '{
    "actions" : [
        {
            "add" : {
                 "index" : "logs-20150316",
                 "alias" : "logs-daily",
                 "placement": {
                   "lang": "groovy",
                   "script": "
                     date = new Date().parse(doc['timestamp'])
                     'logs-' + date.format('YYYYmmdd')"
                 }
            }
        }
    ]
}'
```

In real life, the script would probably be defined in a file instead of dynamic. If the script doesn't return a valid index name, the usual "multiple indices matched" error could be thrown.

/cc @drewr @TwP
</description><key id="62153308">10107</key><summary>Run script on write to multi-index alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2015-03-16T17:22:08Z</created><updated>2015-04-06T21:37:58Z</updated><resolved>2015-04-06T21:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-16T17:40:49Z" id="81832855">@grantr this is going to be tough because the `doc['timestamp']` would be loaded from field data or doc_values, however, the document doesn't yet exist in the index, so there's nowhere to load field data from?
</comment><comment author="dakrone" created="2015-03-16T17:44:26Z" id="81835243">In order to support this, I think we would need to parse the document itself and provide a way of accessing it (perhaps just using `_source` would work, I don't know) in a script.
</comment><comment author="grantr" created="2015-03-16T17:44:38Z" id="81835377">@dakrone The index request has the document in hand, so it doesn't have to come from the index. I figured it could be passed to the script somehow.
</comment><comment author="dakrone" created="2015-03-16T17:53:27Z" id="81841366">@grantr yeah, it would be the `_source` equivalent (pre-analysis) instead of `doc` equivalent (post-analysis)
</comment><comment author="grantr" created="2015-03-16T18:00:24Z" id="81844942">@dakrone that's fine. the `doc['timestamp']` example was just pseudocode. I wouldn't expect the document to have already gone through the indexing pipeline. I expect 99% of the use cases for this would be looking at dates and numbers, and the other 1% at non-analyzed strings.
</comment><comment author="javanna" created="2015-03-24T09:58:06Z" id="85428511">I marked this for discussion. I am -1 on this specific proposal, I think it would slow indexing down and, while scripting is very appealing and powerful, I am not sure I would want to see it supported here, feels scary.

Why would you want to write into a different index depending on document fields though? When it comes to logging, there usually is a single index (the most recent one?) that you would write into out of the indices that the alias points to. We could add a new property to the multi-index alias that holds the index name for write operations, which needs to be a single index and part of the indices that the alias points to for reads. If not specified, behaviour remains the same as now (no write will happen). Thoughts?
</comment><comment author="grantr" created="2015-03-24T17:32:54Z" id="85610507">Thanks for your thoughts @javanna!

&gt; Why would you want to write into a different index depending on document fields though?

Specifying a single write index would be a good start, but doesn't fit our use case. For one of our time-series indices used for tracking exceptions, it isn't enough to always write to the most recent index. Documents might be queued or in flight during the rollover, causing some documents to be indexed in the wrong index.

We also move documents that are marked as interesting (i.e. viewed by a human) out of rotating indices and into a permanent index so that links to them never expire. The index must be deterministic so we can find the interesting documents to be moved later.

Another use case is splitting a large mutable index into multiple smaller ones for better maintainability. Documents would be bucketed by id or some other field.

&gt; I think it would slow indexing down

Scripting might slow indexing slightly, but the user who needs this functionality pays that cost whether it's in the app or in Elasticsearch, so the end-to-end latency is the same.

&gt; I am not sure I would want to see it supported here, feels scary.

What about scripting support here feels scary? It doesn't seem any scarier than running scripts on searches, where a misbehaving or poorly written script could take a node down. Scripting in any context is a power feature that needs to be used with care.
</comment><comment author="clintongormley" created="2015-04-05T11:12:02Z" id="89754690">It feels to me like the right place for this is logstash, which already does things like using the timestamp to determine which index the document should be sent to.  Logstash is an ETL pipeline, but Elasticsearch isn't.  I'd much rather not add this kind of complexity to ES.
</comment><comment author="grantr" created="2015-04-06T21:37:58Z" id="90252485">&gt; Logstash is an ETL pipeline, but Elasticsearch isn't.

The recent deprecation of rivers makes this statement more acceptable. Elasticsearch definitely was an ETL pipeline until then.

I still don't understand the resistance to this feature, but it seems pretty definite, so I'll close this. We'll implement it as a proxy most likely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Warn about the caveats with aliasing aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10106</link><project id="" key="" /><description>Aliases are just tags on indices and not constructs in their own right. At the moment you can create an alias using aliases by doing the following:

```
curl -XPOST 'localhost:9200/_aliases' -d '{
    "actions" : [
        { "add" : { "index" : "2015-01-07", "alias" : "today" } }
        { "add" : { "index" : "2015-01-06", "alias" : "yesterday" } }
        { "add" : { "index" : "2015-01-05", "alias" : "3DaysAgo" } }
        { "add" : { "index" : "2015-01-04", "alias" : "4DaysAgo" } }
        { "add" : { "index" : "2015-01-03", "alias" : "5DaysAgo" } }
        { "add" : { "index" : "2015-01-02", "alias" : "6DaysAgo" } }
        { "add" : { "index" : "2015-01-01", "alias" : "7DaysAgo" } }
    ]
}'

curl -XPOST 'localhost:9200/_aliases' -d '{
    "actions" : [
        { "add" : { "index" : "today", "alias" : "thisWeek" } }
        { "add" : { "index" : "yesterday", "alias" : "thisWeek" } }
        { "add" : { "index" : "3DaysAgo", "alias" : "thisWeek" } }
        { "add" : { "index" : "4DaysAgo", "alias" : "thisWeek" } }
        { "add" : { "index" : "5DaysAgo", "alias" : "thisWeek" } }
        { "add" : { "index" : "6DaysAgo", "alias" : "thisWeek" } }
        { "add" : { "index" : "7DaysAgo", "alias" : "thisWeek" } }
    ]
}'
```

This makes it look like `thisWeek` points to `today` which points to `2015-01-07`. What actually happens is that `2015-01-07` gets two aliases: `today` and `thisWeek`. This works absolutely fine but anyone using this method needs to be aware that if they update the `today` alias to point to `08-01-2015` it will not update the `thisWeek` alias and they will need to update that alias as well.

Is this something we should add to the documentation?
</description><key id="62138426">10106</key><summary>[DOCS] Warn about the caveats with aliasing aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aliases</label><label>docs</label><label>enhancement</label></labels><created>2015-03-16T16:41:44Z</created><updated>2017-06-06T09:04:27Z</updated><resolved>2017-06-06T09:04:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-17T05:22:42Z" id="82114797">yeah tricky bits @colings86 , least we can do is documenting this. I wonder if we should just prevent users from creating aliases against aliases till we properly support it as this seems like a trap.
</comment><comment author="javanna" created="2017-06-06T09:04:27Z" id="306426404">With #23977, in master we expand index expressions against indices only when managing aliases. I find that this is even better than updating our docs on the caveats of aliasing aliases, as that is not no longer allowed. Closing.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog: stats fail to serialize size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10105</link><project id="" key="" /><description /><key id="62127604">10105</key><summary>Translog: stats fail to serialize size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T16:05:59Z</created><updated>2015-06-08T00:42:43Z</updated><resolved>2015-03-16T17:57:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-16T17:16:58Z" id="81816978">left some comment
</comment><comment author="dakrone" created="2015-03-16T17:17:49Z" id="81817739">@bleskes do you think it'd be worth adding a backwards compatibility test for translog stats in general?
</comment><comment author="bleskes" created="2015-03-16T17:30:25Z" id="81826064">@dakrone @s1monw thx. pushed another commit.

@dakrone we have some BWC test where we check that the stats are serialized and deserialized, so for example, NodesStatsBasicBackwardsCompatTests . good enough for now?
</comment><comment author="s1monw" created="2015-03-16T17:31:13Z" id="81826958">LGTM
</comment><comment author="dakrone" created="2015-03-16T17:31:38Z" id="81827359">@bleskes works for me, LGTM too!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return an HTTP error code when a suggest request failed instead of 200</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10104</link><project id="" key="" /><description>The `_suggest` API always returns 200 regardless of the call being successful or not.
This PR sets the http status in a way similar to the SearchResponse https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/search/SearchResponse.java#L71
</description><key id="62046745">10104</key><summary>Return an HTTP error code when a suggest request failed instead of 200</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T11:09:03Z</created><updated>2015-06-08T00:41:57Z</updated><resolved>2015-03-19T17:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-18T06:57:31Z" id="82784600">I like this change, can we instead of duplicating the functionality add a static method to `RestStatus` that looks like this `public static RestStatus status(int successfulShards, int totalShards, ShardOperationFailedException... failures)` and use it in both places?
</comment><comment author="s1monw" created="2015-03-18T06:58:14Z" id="82784967">@areek I assigned this to you, can you take care of it?
</comment><comment author="obourgain" created="2015-03-18T07:25:44Z" id="82790538">With a quick search, it seems that there are some other place that have a similar loop over failures and could also use a static method.
</comment><comment author="s1monw" created="2015-03-18T15:27:26Z" id="83021180">&gt; With a quick search, it seems that there are some other place that have a similar loop over failures and could also use a static method.

cool, if you could update your PR that would be awesome!
</comment><comment author="obourgain" created="2015-03-18T21:03:09Z" id="83175752">I updated it with the `status()` method on `RestStatus` as suggested.

`CountResponse`, `RestSuggestAction`, `SearchResponse` and `SnapshotInfo` will use this method.

For the record, there are also loops over failures in `SearchPhaseExecutionException` and `DeleteByQueryResponse` but they are quite different so I didn't try to modify those.
</comment><comment author="areek" created="2015-03-19T16:44:55Z" id="83656615">@obourgain Thanks for the the PR! This looks good to me. I will merge this in early next week
</comment><comment author="s1monw" created="2015-03-19T16:52:32Z" id="83660132">@areek it's pretty low risk and is a bug, should we pull it for 1.5?
</comment><comment author="areek" created="2015-03-19T16:58:54Z" id="83662666">@s1monw makes sense, will merge it in
</comment><comment author="areek" created="2015-03-19T17:41:07Z" id="83690584">Merged to 1.5 (https://github.com/elastic/elasticsearch/commit/47d0ae635afa07244523b2bc1a4c05235178d5aa),
1.x (https://github.com/elastic/elasticsearch/commit/35fcdf5d0b8d3f29acfe39f6a1f9eb8e8491f5f3) and master (https://github.com/elastic/elasticsearch/commit/deade2eb714f5bfabb3022f5297f29b02be0bd9a)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DIsk based allocator should take into account the amount of disk space which can be reused</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10103</link><project id="" key="" /><description>Assume we have a node where the disk usage is above the low watermark. If we restart this node then no shards will be allocated on it since the disk usage is above the low watermark.

It would be better if the disk based allocator could realize that it can throw away or at least reuse the space which is currently used for outdated shard copies on the node.
</description><key id="62019546">10103</key><summary>DIsk based allocator should take into account the amount of disk space which can be reused</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maf23</reporter><labels><label>:Allocation</label><label>stalled</label></labels><created>2015-03-16T09:17:33Z</created><updated>2015-12-09T08:27:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T20:07:31Z" id="89845934">@dakrone any thoughts about this?
</comment><comment author="dakrone" created="2015-05-08T16:15:56Z" id="100284233">@clintongormley hmm.. to implement this we would have to send a request for each shard to all nodes to see what files and checksums they have in common. This would be quite heavy, especially for an `AllocationDecider` (because it has to run in tight loops).
</comment><comment author="clintongormley" created="2015-05-08T17:31:20Z" id="100305643">@dakrone thanks for commenting, that sounds... heavy.  Perhaps once sequence IDs (https://github.com/elastic/elasticsearch/issues/10708) are in, it'll be more feasible.

Marking as stalled.
</comment><comment author="clintongormley" created="2015-12-05T20:29:46Z" id="162243580">@dakrone @bleskes I think this has been resolved already, no?
</comment><comment author="dakrone" created="2015-12-09T00:02:17Z" id="163061756">@clintongormley no, it hasn't be resolved still, I'm not sure if the seqids work is far enough along to work on this, maybe @bleskes knows?
</comment><comment author="bleskes" created="2015-12-09T08:27:06Z" id="163148874">I don't think we can expect this in the first iteration. The problem is very similar to the current file based copy - we don't know how much will be reused (or if seq no base recovery will succeed) until we start the work. The master needs to know this in advance somehow (as Lee already said).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove reflection call to waitForMerges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10102</link><project id="" key="" /><description>Since the index writer is now final we can remove the readlock around
the forceMerge calls and use the official API to wait for merges.
</description><key id="61958867">10102</key><summary>Remove reflection call to waitForMerges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-16T03:38:13Z</created><updated>2015-06-09T11:51:41Z</updated><resolved>2015-03-16T05:59:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-16T03:52:31Z" id="81390043">Left some comments but they're pretty minor, other than that it looks good to me!
</comment><comment author="s1monw" created="2015-03-16T05:31:57Z" id="81428274">@dakrone can you take another look?
</comment><comment author="mikemccand" created="2015-03-16T05:39:47Z" id="81431219">Left a couple comments...
</comment><comment author="mikemccand" created="2015-03-16T05:53:23Z" id="81433476">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>service stops after a couple of hours</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10101</link><project id="" key="" /><description>Hello,
I am running elasticsearch on my linux vps server. While starting the service, it is ok but I am getting the following stuffs : 

root@vps15042 [~]# sudo service elasticsearch start 
error: permission denied on key 'vm.max_map_count' 
Starting elasticsearch: [ OK ] 
root@vps15042 [~]# log4j:WARN No appenders could be found for logger (common) 
log4j:WARN Please initialize the log4j system properly. 
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for morefo. 

I do have my elasticsearch.yml and logging.yml under etc/elasticsearch/

And after a couple of hours, it stops by itself, whether it has been kept idle or the service has been used via the webinterface. I have tried to modify the ES_HEAP_SIZE = 2g on etc/sysconfig, but it is still not working. Note that the VPS has 4gb of RAM.

Can anyone please help me please? I am stuck with this issue for more than a week and my whole website is based on elasticsearch which runs fine on my windows for development.
</description><key id="61950164">10101</key><summary>service stops after a couple of hours</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Yashin-s</reporter><labels><label>:Packaging</label></labels><created>2015-03-16T02:52:44Z</created><updated>2015-06-17T11:27:23Z</updated><resolved>2015-03-20T08:55:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-16T03:13:02Z" id="81370344">@Yashin-s thanks for reporting.

I think there are few problems here. The first one is that the logging system hasn't been initialized so we can't really know why the elasticsearch node has been stopped.

The second one is that you start elasticsearch with the root user while executing a `sudo` command. If possible, I recommand to remove completely elasticsearch and reinstall it, then starting it with the `elasticsearch` user. Also, you can check that the `elasticsearch` user has the correct privileges on  the log directory (see /var/log/elasticsearch)
</comment><comment author="Yashin-s" created="2015-03-16T12:59:28Z" id="81644632">I've uninstalled elasticsearch completly. I previously installed it through yum. I've now followed the instructions on http://tecadmin.net/install-elasticsearch-on-linux/ to install the 1.4.4 package. But I am still getting the same error on start. Below is the output. Also, how do I log as elasticsearch user ? I am accessing the VPS via putty.

root@vps15042 [/usr/share/elasticsearch/elasticsearch-1.4.4]# log4j:WARN No appenders could be found for logger (common).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[2015-03-16 07:14:05,484][INFO ][node                     ] [Ape] version[1.4.4], pid[7785], build[c88f77f/2015-02-19T13:05:36Z]
[2015-03-16 07:14:05,485][INFO ][node                     ] [Ape] initializing ...
[2015-03-16 07:14:05,490][INFO ][plugins                  ] [Ape] loaded [], sites [head]
[2015-03-16 07:14:07,900][INFO ][node                     ] [Ape] initialized
[2015-03-16 07:14:07,900][INFO ][node                     ] [Ape] starting ...
[2015-03-16 07:14:08,030][INFO ][transport                ] [Ape] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/23.235.207.40:9300]}
[2015-03-16 07:14:08,048][INFO ][discovery                ] [Ape] elasticsearch/IeHVdHdZSVqvR2U84cvK-g
[2015-03-16 07:14:08,056][WARN ][discovery.zen.ping.multicast] [Ape] failed to send multicast ping request: IOException[Operation not permitted]
[2015-03-16 07:14:09,557][WARN ][discovery.zen.ping.multicast] [Ape] failed to send multicast ping request: IOException[Operation not permitted]
[2015-03-16 07:14:11,060][WARN ][discovery.zen.ping.multicast] [Ape] failed to send multicast ping request: IOException[Operation not permitted]
[2015-03-16 07:14:11,818][INFO ][cluster.service          ] [Ape] new_master [Ape][IeHVdHdZSVqvR2U84cvK-g][vps15042.inmotionhosting.com][inet[/23.235.207.40:9300]], reason: zen-disco-join 

(elected_as_master)
[2015-03-16 07:14:11,845][INFO ][gateway                  ] [Ape] recovered [0] indices into cluster_state
[2015-03-16 07:14:11,845][INFO ][http                     ] [Ape] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/23.235.207.40:9200]}
[2015-03-16 07:14:11,846][INFO ][node                     ] [Ape] started
</comment><comment author="tlrx" created="2015-03-16T17:13:21Z" id="81815009">Here is how I usually proceed:
- uninstall everything is needed
- check if java is correctly installed
- install elasticsearch with YUM
- check if the `elasticsearch` user has been created
- check if /var/log/elasticsearch, /var/lib/elasticsearch, /etc/elasticsearch directories are created
- check if elasticsearch user has read/write privileges on these directories
- start elasticsearch as a service

I also recommend you to follow the documentation, these pages http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html#yum and http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html should help you.
</comment><comment author="Yashin-s" created="2015-03-18T13:43:11Z" id="82977350">Thanks for the instructions, I've already uninstall and re-install the plugin several times but I keep getting the same issue. I think that it has something to do with virtual memory setting. My vm.max_map_count is only 65,530 compared to the 262,144 that es is trying to assign. It may be that it is resulting in OOM exceptions. I cannot change vm.max_map_count either to test it, since I do not have the permission on the VPS server. What can I do to detect if there are indeed out of memory exceptions and what can I do to solve the issue?
</comment><comment author="tlrx" created="2015-03-20T08:55:19Z" id="83958868">Hi @Yashin-s,

I close this issue because it seems to be more a configuration problem than a bug. Could you please post your question on the mailing list https://groups.google.com/group/elasticsearch/? It will be easier and there are more people there to help you.
</comment><comment author="Ziink" created="2015-04-26T01:02:03Z" id="96299760">It seems like the problem is due to the kernel version of OpenVZ. At least that is the case with me.

Yashin-s I'm guessing your VPS is on OpenVZ. Run the command 
uname -a

On the VPS where elasticsearch keeps crashing at random times, I get an output like

&gt; Linux nyserver1 2.6.32-042stab104.1 #1 SMP Thu Jan 29 12:58:41 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux

On another VPS, where elasticsearch is working fine, the output I get is

&gt; Linux zine 2.6.32-042stab103.6 #1 SMP Wed Jan 21 13:07:39 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux
</comment><comment author="clintongormley" created="2015-04-26T17:55:57Z" id="96413866">Possibly related to https://github.com/elastic/elasticsearch/issues/9582 ?
</comment><comment author="fundup" created="2015-06-17T11:27:23Z" id="112760586">I had same problem. I did my first install using root. Then I remove and install once again using a user and sudo apt-get (https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) . It seems to work now. Hope it will help
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch java.lang.Error when getting a mac address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10100</link><project id="" key="" /><description>Apparently Azure WebJobs throw a java.lang.Error when enumerating network interfaces.
Catch this error and just use the generated one in this case.

See #10099
</description><key id="61945048">10100</key><summary>Catch java.lang.Error when getting a mac address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">GaelTadh</reporter><labels /><created>2015-03-16T02:15:03Z</created><updated>2016-03-08T19:17:22Z</updated><resolved>2016-03-08T19:17:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T02:56:53Z" id="81364227">I'm concerned with catching a java.lang.Error. Quoting the [javadocs](http://docs.oracle.com/javase/7/docs/api/java/lang/Error.html): "An Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch."
</comment><comment author="GaelTadh" created="2015-03-16T03:03:26Z" id="81366037">Yeah I know it's not a great error to catch but it does look like that is what the WebJob container is causing to be raised from `java.net.NetworkInterface.getAll`

```
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ] Exception in thread "main" java.lang.Error: IP Helper Library GetIfTable function failed
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ] at java.net.NetworkInterface.getAll(Native Method)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ] at java.net.NetworkInterface.getNetworkInterfaces(NetworkInterface.java:334)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ] at org.elasticsearch.common.MacAddressProvider.getMacAddress(MacAddressProvider.java:35)
```
</comment><comment author="jpountz" created="2015-03-16T03:14:01Z" id="81371073">We should probably report this issue upstream? In the meantime, maybe a reasonable workaround would be to provide a way to configure the mac address in the elasticsearch.yml file in order to avoid calling NetworkInterface.getNetworkInterfaces?
</comment><comment author="s1monw" created="2015-03-18T06:59:35Z" id="82785668">+1 to what @jpountz said!
</comment><comment author="s1monw" created="2015-03-20T20:03:35Z" id="84130626">@GaelTadh can you update the PR to use a dummy value?
</comment><comment author="clintongormley" created="2016-03-08T19:17:21Z" id="193925655">Closed by #15266
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TimeBasedUUIDGenerator relies on getMacAddress which doesn't work in Azure WebJob</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10099</link><project id="" key="" /><description>Trying to penny pinch in Azure by running Elastic Search as a WebJob.

Azure Websites are fairly locked down and don't allow you to enumerate network addresses (at least that's my theory).  I assumed this wouldn't be needed if I bind to a specific IP but it appears the TimeBasedUUIDGenerator also uses the machines Mac Address.

[03/15/2015 23:56:11 &gt; 6e43bd: ERR ] Exception in thread "main" java.lang.Error: IP Helper Library GetIfTable function failed
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at java.net.NetworkInterface.getAll(Native Method)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at java.net.NetworkInterface.getNetworkInterfaces(NetworkInterface.java:334)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.common.MacAddressProvider.getMacAddress(MacAddressProvider.java:35)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.common.MacAddressProvider.getSecureMungedAddress(MacAddressProvider.java:67)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.common.TimeBasedUUIDGenerator.&lt;clinit&gt;(TimeBasedUUIDGenerator.java:41)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.common.Strings.&lt;clinit&gt;(Strings.java:48)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.common.settings.ImmutableSettings$Builder.replacePropertyPlaceholders(ImmutableSettings.java:1063)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:53)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:106)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:177)
[03/15/2015 23:56:11 &gt; 6e43bd: ERR ]    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)

Replication Instructions
- Download elastic search zip file
- Add a batch file to the root that contains the following
  set ES_MIN_MEM=32m
  set ES_MAX_MEM=500m
  bin\elasticsearch.bat 
- Create a website in Azure, go to the web jobs tab and upload the zip file
- Open up the log for the webjob to view its output.
</description><key id="61931212">10099</key><summary>TimeBasedUUIDGenerator relies on getMacAddress which doesn't work in Azure WebJob</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bzbetty</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2015-03-16T01:03:06Z</created><updated>2015-12-06T13:19:53Z</updated><resolved>2015-12-06T13:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T20:28:42Z" id="162243518">@mikemccand anything we can do here? I'm guessing the MAC address is fairly important when generating UUIDs :)
</comment><comment author="mikemccand" created="2015-12-06T10:53:58Z" id="162302999">Actually this is pretty easy to fix @clintongormley: we already catch and `logger.warn` `SocketException` if there are problems getting the mac address, I can widen that to any `Throwable`.  I'll open a PR.

Still it's likely there are other problems with running ES in a WebJob environment...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type level fielddata settings should be checked more aggressively</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10098</link><project id="" key="" /><description>Parsing of the fielddata doesn't happen in the type parser, but instead in the AbstractFieldMapper constructor.  We should move this up to the type parser where other settings are validated.
</description><key id="61898767">10098</key><summary>Type level fielddata settings should be checked more aggressively</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-15T20:58:44Z</created><updated>2016-08-24T15:44:08Z</updated><resolved>2016-08-24T15:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T17:28:49Z" id="81824360">+1
</comment><comment author="jpountz" created="2016-08-24T15:44:07Z" id="242112496">Fixed in master, we do not have freestyle settings anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add throttle stats to index stats and recovery stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10097</link><project id="" key="" /><description>This commit adds throttling statistics to the index stats API and to the recovery state.
</description><key id="61844741">10097</key><summary>Add throttle stats to index stats and recovery stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-15T15:37:05Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-20T09:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-17T22:36:14Z" id="82628317">left minor comments
</comment><comment author="bleskes" created="2015-03-18T16:49:56Z" id="83054931">@s1monw I address the comments, improved the test and solved a couple of issue. Can you take another look? 
</comment><comment author="bleskes" created="2015-03-19T20:32:55Z" id="83753452">@s1monw ping?
</comment><comment author="s1monw" created="2015-03-19T20:43:43Z" id="83755527">left a minor comment otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move IndicesStatsRequest to expose CommonStatsFlags directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10096</link><project id="" key="" /><description>Currently IndicesStatsRequest has a method per possible index metric. Internally theses are stored using a `CommonStatsFlags` object. This means that every time we add a stat we need to remember to add a method and remember to map it in `RestIndicesStatsAction` rather we should just expose the underlying flag, just like `NodesStatsRequest.indices()` allowing for some automation in the mapping.
</description><key id="61834748">10096</key><summary>Move IndicesStatsRequest to expose CommonStatsFlags directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Java API</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-03-15T15:06:42Z</created><updated>2017-03-23T03:18:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] Fix geo_shape orientation parameter persistence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10095</link><project id="" key="" /><description>Fixing geo_shape field mapper to persist the orientation parameter. Adding parsing and integration tests to ensure persistence across cluster restarts.
</description><key id="61718951">10095</key><summary>[GEO] Fix geo_shape orientation parameter persistence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels /><created>2015-03-15T00:25:26Z</created><updated>2015-03-18T23:56:57Z</updated><resolved>2015-03-18T23:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-15T05:18:17Z" id="80863579">LGTM, just one comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Switch Cluster State Part to common serialization mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10094</link><project id="" key="" /><description>Opening PR for preliminary review. This version still has a few serializations broken.
</description><key id="61704547">10094</key><summary>WIP: Switch Cluster State Part to common serialization mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2015-03-14T22:07:15Z</created><updated>2015-03-23T02:58:35Z</updated><resolved>2015-03-23T02:58:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-03-23T02:58:32Z" id="84765995">Superseded by #10212
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move GatewayShardsState logic into IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10093</link><project id="" key="" /><description>The index shard should take care of shard state persistence since it has
all the information and the gateway concept has been removed in master.
</description><key id="61695957">10093</key><summary>Move GatewayShardsState logic into IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-14T21:11:58Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-19T22:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-16T00:49:00Z" id="81314899">@bleskes do you mind taking a look
@kimchy you maybe too?
</comment><comment author="bleskes" created="2015-03-19T10:36:57Z" id="83497537">I like this change. Makes it simpler and doesn't require a global map. Left a couple of comments
</comment><comment author="s1monw" created="2015-03-19T18:29:21Z" id="83703977">pushed a new commit
</comment><comment author="bleskes" created="2015-03-19T19:21:27Z" id="83726818">replied to the replies ... 
</comment><comment author="s1monw" created="2015-03-19T20:41:59Z" id="83755111">pushed more feedback @bleskes thx
</comment><comment author="bleskes" created="2015-03-19T20:47:10Z" id="83756422">LGTM. left one minor typo comment. @s1monw thx.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Always fail engine on corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10092</link><project id="" key="" /><description>I think we should remove (2.0) and deprecate (1.5.0) the index.fail_on_corruption and always fail the engine if corruption is detected?  Doesn't seem like this should be something you can turn off.
</description><key id="61683173">10092</key><summary>Always fail engine on corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Engine</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-14T19:28:42Z</created><updated>2015-06-08T13:41:41Z</updated><resolved>2015-03-14T21:11:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-14T20:04:55Z" id="80696622">LGTM
</comment><comment author="s1monw" created="2015-03-14T20:33:29Z" id="80708420">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No easy way to access score when using a ValuesSourceMetricsAggregationBuilder-based aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10091</link><project id="" key="" /><description>I attempted to implement the aggregation documented at the bottom of [this article](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-top-hits-aggregation.html). Since dynamic scripting is disabled by default, I was not able to find an easy way to implement it.

The solutions I've discovered so far are:
- Enable dynamic scripting via `script.disable_dynamic: false`
- Upload a stored script and use with `script_file`

All of which can be difficult if the developer using elasticsearch does not have access to administrate the service, or if it is a public facing service.
</description><key id="61669991">10091</key><summary>No easy way to access score when using a ValuesSourceMetricsAggregationBuilder-based aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">udoprog</reporter><labels><label>:Expressions</label><label>:Scripting</label></labels><created>2015-03-14T18:11:04Z</created><updated>2016-03-08T09:17:21Z</updated><resolved>2015-05-15T16:11:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="theQuazz" created="2015-03-14T18:13:55Z" id="80652596">:+1:

I have tried using a Lucene Expressions to solve this problem, but it look like the language doesn't support this use case yet
</comment><comment author="dakrone" created="2015-03-14T18:16:07Z" id="80653716">@rjernst how hard would it be to add super simple support for `_score` to expressions?
</comment><comment author="rjernst" created="2015-03-27T05:17:30Z" id="86828240">I investigated this for a while.  Expressions already support scores; the problems are with aggs.

The first problem is scorers are not passed through the many layers of objects consistently.  In the case of the `max` agg example there, I think there are at least 2 levels missing, first in the anonymous `LeafBucketCollectorBase`, and then in the anonymous `NumericDoubleValues` created in `MultiValueMode.MAX.select()`.

The bigger problem though is possibly more of a design problem that might prevent this from working. Within `getLeafCollector` of the `MaxAggregator`, `doubleValues()` is called before creating the `LeafBucketCollectorBase`, which binds the values object to a specific leaf reader (ie segment). The collector there is what implements `ScorerAware`.  However, expressions, in order to be efficient, map the bindings to concrete function values eagerly (when the binding to a leaf reader occurs). Here it looks like a lazy binding, which probably works fine for `groovy` since its variables are a hash based lookup, but not for expressions.  This eager binding is what allows expressions to be really fast.

It is possible though I misunderstood the aggs classes. @jpountz do you have any thoughts here?
</comment><comment author="egpbos" created="2015-05-11T05:38:43Z" id="100771414">+1
</comment><comment author="clintongormley" created="2015-05-15T15:44:00Z" id="102440835">@jpountz you may have missed this?
</comment><comment author="jpountz" created="2015-05-15T16:11:58Z" id="102450351">@clintongormley Yes totally!

I just tried on master with the following sense recreation and this now works:

```
DELETE test

PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "cat": {
          "type": "string",
          "index": "not_analyzed"
        },
        "text": {
          "type": "string"
        }
      }
    }
  }
}

PUT test/test/1
{
  "cat": "a",
  "text": "foo bar"
}

PUT test/test/2
{
  "cat": "a",
  "text": "bar"
}

PUT test/test/3
{
  "cat": "b",
  "text": "bar quux"
}

PUT test/test/4
{
  "cat": "c",
  "text": "bar"
}

GET test/_search
{
  "query": {
    "match": {
      "text": "bar"
    }
  }, 
  "aggs": {
    "by_cat": {
      "terms": {
        "field": "cat",
        "size": 10,
        "order": {
          "top_score": "desc"
        }
      },
      "aggs": {
        "top_hits": {
          "top_hits": {}
        },
        "top_score": {
          "max": {
            "script": "_score",
            "lang": "expression"
          }
        }
      }
    }
  }
}
```

It apparently got fixed through https://github.com/elastic/elasticsearch/pull/10389/files#diff-867bb5fa33f1c72de5e99bc120550a7cR92. When setScorer is called on the LeafSearchScript object we re-create the function values in order to take the new scorer into account.
</comment><comment author="avdv" created="2016-03-08T07:25:40Z" id="193642619">Is this still supported?

My aggregations look almost identical:

```
{
  "aggregations": {
    "parent": {
        "terms": {
            "field": "container",
            "size": 10,
            "order": {
                "max_score": "desc"
            }
        },
        "aggregations": {
            "max_score": {
                "max": {
                    "lang": "expression",
                    "script": "_score"
                }
            }
        }
    }
  }
}
```

but it fails with:

```
Caused by: java.lang.IllegalStateException: Expressions referencing the score can only be used for sorting
```

I'm using Elasticsearch 1.7.5.
</comment><comment author="jpountz" created="2016-03-08T09:06:28Z" id="193676966">@avdv This is fixed in elasticsearch 2.0+.
</comment><comment author="avdv" created="2016-03-08T09:17:21Z" id="193683378">@jpountz Thanks, it's time to upgrade I guess.. :wink: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How optimize elasticsearch query slow?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10090</link><project id="" key="" /><description>In my use scene(5.6TB data, 13 Node),  I will use many "query_string" to find result, I hope quickly return query result, query filters as follow,

```
{
    "query": {
        "bool": {
            "must": [
                {
                    "query_string": {
                        "fields": [
                            "text"
                        ],
                        "query": "\"&#24494;&#36719;\" OR \"&#33529;&#26524;\" OR \"&#35895;&#27468;\" OR \"&#31934;&#30805; &#24191;&#21578;\" OR \"&#24494;&#36719;2\" OR \"google\" OR \"windows 10\" OR \"&#31435;&#30333;\" OR \"BOE\" OR \"&#23453;&#39532;\" OR \"&#22868;&#39536;\" OR \"boe\""
                    }
                },
                {
                    "bool": {
                        "must": [
                            {
                                "bool": {
                                    "should": [
                                        {
                                            "bool": {
                                                "must": [
                                                    {
                                                        "term": {
                                                            "platform": 0
                                                        }
                                                    },
                                                    {
                                                        "query_string": {
                                                            "fields": [
                                                                "text"
                                                            ],
                                                            "query": "\"&#23453;&#39532;\""
                                                        }
                                                    }
                                                ]
                                            }
                                        }
                                    ]
                                }
                            },
                            {
                                "query_string": {
                                    "fields": [
                                        "text"
                                    ],
                                    "query": "\"&#32422;&#20250;\" OR \"&#20241;&#38386;\" OR \"&#25955;&#27493;\" OR \"&#30005;&#24433;\""
                                }
                            },
                            {
                                "range": {
                                    "cdate": {
                                        "gte": "2015-02-07",
                                        "lte": "2015-03-13"
                                    }
                                }
                            }
                        ],
                        "must_not": [
                            {
                                "query_string": {
                                    "fields": [
                                        "text"
                                    ],
                                    "query": "test"
                                }
                            }
                        ]
                    }
                },
                {
                    "bool": {
                        "should": [
                            {
                                "query_string": {
                                    "fields": [
                                        "text"
                                    ],
                                    "query": "\"&#32422;&#20250;\" OR \"&#20241;&#38386;\" OR \"&#25955;&#27493;\" OR \"&#30005;&#24433;\" OR \"&#22622;&#36710;\" OR \"&#25171;&#36710;\""
                                }
                            }
                        ]
                    }
                }
            ]
        }
    },
    "aggs": {},
    "highlight": {
        "fields": {
            "text": {}
        }
    }
}
```
</description><key id="61509355">10090</key><summary>How optimize elasticsearch query slow?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-03-14T08:47:38Z</created><updated>2015-03-14T10:34:29Z</updated><resolved>2015-03-14T10:34:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-14T10:34:29Z" id="80291669">Please ask questions on the mailing list. We can  definitely help you there.

Prefer using filters each time you can. Basically each time you don't require to compute a score (filter results by a given Term or a Range of values...)
It will be really much faster.
Use a filtered query and split the "full text" part of your search and the other part.

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding site plugin(kopf)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10089</link><project id="" key="" /><description>https://github.com/lmenezes/elasticsearch-kopf
</description><key id="61418817">10089</key><summary>adding site plugin(kopf)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jeesim2</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-03-14T05:00:58Z</created><updated>2015-05-27T08:29:18Z</updated><resolved>2015-05-27T08:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-18T20:35:01Z" id="83159952">Thanks @jeesim2 may I ask if you already signed our [CLA](https://www.elastic.co/contributor-agreement)? If not could you please do so we can merge your PR? 
</comment><comment author="javanna" created="2015-05-27T08:29:16Z" id="105822459">CLA not signed after 2 months, closing and treating as a bug report. Will push the fix shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `index.fail_on_merge_failure`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10088</link><project id="" key="" /><description>#10084 deprecates index.fail_on_merge_failure in 1.x and this PR removes it from master.
</description><key id="61201961">10088</key><summary>Remove `index.fail_on_merge_failure`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>deprecation</label><label>v2.0.0-beta1</label></labels><created>2015-03-13T19:39:33Z</created><updated>2015-06-06T16:09:58Z</updated><resolved>2015-03-14T13:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-13T19:48:49Z" id="79302807">I agree we should fail all the time on merges @kimchy any historic reasons to not do this?
</comment><comment author="kimchy" created="2015-03-13T19:50:29Z" id="79304576">++, agreed, when we added it, we wanted to have the ability to disable it in case there were problems, but I think its pretty much safe now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix PageCacheRecycler's max page size computation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10087</link><project id="" key="" /><description>PageCacheRecycler was mistakenly using the maximum number of items per page
instead of the byte size of a page. This could result in higher memory usage
than configured.

Close #10077
</description><key id="61177309">10087</key><summary>Fix PageCacheRecycler's max page size computation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-13T18:35:51Z</created><updated>2015-06-07T18:18:05Z</updated><resolved>2015-03-16T16:14:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-13T19:59:07Z" id="79315576">LGTM
</comment><comment author="s1monw" created="2015-03-15T21:27:22Z" id="81247851">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query string query: Remove `lowercase_expanded_terms` and `locale` options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10086</link><project id="" key="" /><description>The analysis chain should be used instead of relying on this, as it is
confusing when dealing with different per-field analysers.

The `locale` option was only used for `lowercase_expanded_terms`, which,
once removed, is no longer needed, so it was removed as well.

Fixes #9978
Relates to #9973
</description><key id="61177298">10086</key><summary>Query string query: Remove `lowercase_expanded_terms` and `locale` options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>breaking</label></labels><created>2015-03-13T18:35:49Z</created><updated>2015-07-20T18:45:11Z</updated><resolved>2015-03-13T19:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-13T19:13:14Z" id="79267646">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>setup elasticsearch on a server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10085</link><project id="" key="" /><description>Am new in elastic search, i am building a search engine for my website using elasticsearch and php.
i want to update my project in my hosting server everything works perfectly in my localhost (my PC) but when i update the project in my hosted server this is what i get 

&lt;code&gt;
Fatal error: Uncaught exception 'Elasticsearch\Common\Exceptions\Curl\CouldNotConnectToHost' with message 'couldn't connect to host' in /home/uhuru372/public_html/unilodb/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/AbstractConnection.php:320 Stack trace: #0 /home/uhuru372/public_html/unilodb/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/GuzzleConnection.php(313): Elasticsearch\Connections\AbstractConnection-&gt;throwCurlException(7, 'couldn't connec...') #1 /home/uhuru372/public_html/unilodb/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/GuzzleConnection.php(200): Elasticsearch\Connections\GuzzleConnection-&gt;processCurlError(Object(Guzzle\Http\Exception\CurlException)) #2 /home/uhuru372/public_html/unilodb/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/GuzzleConnection.php(104): Elasticsearch\Connections\GuzzleConnection-&gt;sendRequest(Object(Guzzle\Http\Message\EntityEnclosingRequest), '{"query":{"bool...') #3 /home/uhuru372/public_html/unilodb/vendor/ in /home/uhuru372/public_html/unilodb/vendor/elasticsearch/elasticsearch/src/Elasticsearch/Connections/AbstractConnection.php on line 320
&lt;/code&gt;

Help or suggestion will be much appreciated
</description><key id="61177135">10085</key><summary>setup elasticsearch on a server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mtuchi</reporter><labels /><created>2015-03-13T18:35:25Z</created><updated>2015-03-13T19:29:03Z</updated><resolved>2015-03-13T19:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-13T19:29:03Z" id="79282802">Hi @mtuchi 

You should better ask this on the mailing list. We can definitely help you there.
Provide there more details about what you are exactly doing. I don't see why you are using CURL from PHP but may be I am misunderstanding your trace here.

Closing as it's not an issue but more a usage question.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: deprecate index.fail_on_merge_failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10084</link><project id="" key="" /><description>When a merge exception happens I think we should always fail the engine, so I'd like to deprecate this setting in 1.5.0 and remove in 2.0.
</description><key id="61159175">10084</key><summary>Core: deprecate index.fail_on_merge_failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>deprecation</label><label>v1.5.0</label></labels><created>2015-03-13T17:52:24Z</created><updated>2015-03-19T11:02:19Z</updated><resolved>2015-03-14T19:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-13T19:20:33Z" id="79274477">I think we should also deprecate/remove fail_on_corruption?  I.e. if corruption is detected we should always fail...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't rethrow already handled merge exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10083</link><project id="" key="" /><description>ES already has a custom merge exception handler that 1) logs the exception and 2) fails the engine ... and then it calls super, which throws an unhandled exception, which I think is redundant.  The JVM catches that and logs it again (to stderr), and this causes test failures like http://build-us-00.elastic.co/job/es_core_1x_centos/3701

I think we should remove the super call?
</description><key id="61154717">10083</key><summary>Don't rethrow already handled merge exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-13T17:41:12Z</created><updated>2015-06-06T19:12:04Z</updated><resolved>2015-03-13T17:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-13T17:42:24Z" id="79176148">I also changed logger.warn to logger.error...
</comment><comment author="dakrone" created="2015-03-13T17:45:27Z" id="79179159">LGTM but @s1monw should probably look also
</comment><comment author="s1monw" created="2015-03-13T17:47:34Z" id="79181403">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: add deprecation messages for delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10082</link><project id="" key="" /><description>Patch is based on 1.x.

This commit just adds deprecation messages to delete-by-query docs ... but I'm not sure I got the deprecated markup correct.
</description><key id="61142052">10082</key><summary>Core: add deprecation messages for delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Search</label><label>deprecation</label><label>v1.5.0</label></labels><created>2015-03-13T17:11:53Z</created><updated>2015-03-24T09:04:56Z</updated><resolved>2015-03-13T18:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-13T17:12:34Z" id="79146267">LGTM
</comment><comment author="uschindler" created="2015-03-24T08:48:06Z" id="85406195">This commit does not deprecate the corresponding methods prepareDeleteByQuery in the Client class, nor does it deprecate the RequestBuilders. So my Java code did not bring a warning while compiling, which is bad.
</comment><comment author="mikemccand" created="2015-03-24T09:04:56Z" id="85410666">&gt; This commit does not deprecate the corresponding methods prepareDeleteByQuery in the Client class, 

Woops, thanks @uschindler!  I'll make a new PR deprecating the client APIs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switching from Facets to Aggregations problems in Elastic Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10081</link><project id="" key="" /><description>We were using facets in elastic search. But since they are deprecated now, We are trying to use aggregations now. The code snippet for both the approaches are 

https://gist.github.com/parasb/751baf0fca2a7682a8d1

Refering the code snippet for facets,  we pass 2 IMEIs in should array of facet 0, it returns the result corresponding to those IMEIs in facet 0 &amp; we pass 1 IMEI in should array of facet 1, it returns the result corresponding to that IMEI in facet 1. So, we get the aggregated results of those multiple fquery search in the corresponding facet.

But while using aggregation, we pass all the IMEIs in the should array and get the response as per IMEI. I need to aggregate multiple IMEIs in multiple buckets. for eg: 2 IMEI in one bucket and another 1 IMEI in different bucket (similar to 2 IMEIs in 0 facet &amp; 1 IMEI in first facet).

Does Elastic Search aggregation provide a way to do this? 
</description><key id="61130740">10081</key><summary>Switching from Facets to Aggregations problems in Elastic Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">parasb</reporter><labels /><created>2015-03-13T16:44:57Z</created><updated>2015-03-13T17:21:31Z</updated><resolved>2015-03-13T17:21:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-13T17:21:31Z" id="79154919">Sorry but I'm not sure to understand the issue here. That said, maybe the answer to your question is in our facets to aggregations migration guide? http://www.elastic.co/guide/en/elasticsearch/reference/current/search-facets-migrating-to-aggs.html#_facet_filters

Closing the issue as it sounds to me like a question rather than a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices statistics reports -1 memory usage for percolator of index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10080</link><project id="" key="" /><description>I am trying to gauge how much memory is used by the percolator on an index. I noticed that the statistics report for each index a 'percolate' value. I am using the `elasticsearch-ruby` client to query the Elasticsearch cluster.

```
[18] pry(main)&gt; client.indices.stats['indices']['posts_20150311084808']['total']['percolate']
=&gt; {"total"=&gt;15,
 "time_in_millis"=&gt;282,
 "current"=&gt;0,
 "memory_size_in_bytes"=&gt;-1,
 "memory_size"=&gt;"-1b",
 "queries"=&gt;15178}
[19] pry(main)&gt; client.indices.stats['indices']['posts_20150311084808']['primaries']['percolate']
=&gt; {"total"=&gt;15,
 "time_in_millis"=&gt;282,
 "current"=&gt;0,
 "memory_size_in_bytes"=&gt;-1,
 "memory_size"=&gt;"-1b",
 "queries"=&gt;15178}
[20] pry(main)&gt; 
```

As you can see I get back `-1`  as the size. I have ~15000 queries so I would expect this to be non-zero. Is this just not implemented, or does it have a special meaning?

Here is what aptitude has to say about the Elasticsearch version I am running 

```
 aptitude show elasticsearch
Package: elasticsearch                   
New: yes
State: installed
Automatically installed: no
Version: 1.4.2
Priority: optional
Section: web
Maintainer: Elasticsearch Team &lt;info@elasticsearch.com&gt;
Architecture: all
Uncompressed Size: 30.2 M
Depends: libc6, adduser
Description: Open Source, Distributed, RESTful Search Engine
 Elasticsearch is a distributed RESTful search engine built for the cloud. 

 Features include: 

 + Distributed and Highly Available Search Engine. 
 * Each index is fully sharded with a configurable number of shards. 
 * Each shard can have one or more replicas. 
 * Read / Search operations performed on either one of the replica shard. 
 + Multi Tenant with Multi Types. 
 * Support for more than one index. 
 * Support for more than one type per index. 
 * Index level configuration (number of shards, index storage, ...). 
 + Various set of APIs 
 * HTTP RESTful API 
 * Native Java API. 
 * All APIs perform automatic node operation rerouting. 
 + Document oriented 
 * No need for upfront schema definition. 
 * Schema can be defined per type for customization of the indexing process. 
 + Reliable, Asynchronous Write Behind for long term persistency. + (Near) Real Time Search. + Built on top of Lucene 
 * Each shard is a fully functional Lucene index 
 * All the power of Lucene easily exposed through simple configuration/plugins. 
 + Per operation consistency 
 * Single document level operations are atomic, consistent, isolated and durable. 
 + Open Source under the Apache License, version 2 ("ALv2").
Homepage: http://www.elasticsearch.org/

```
</description><key id="61117729">10080</key><summary>Indices statistics reports -1 memory usage for percolator of index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hydrogen18</reporter><labels><label>:Percolator</label></labels><created>2015-03-13T16:11:14Z</created><updated>2015-04-26T14:28:28Z</updated><resolved>2015-04-26T14:28:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-05T19:55:40Z" id="89841850">@martijnvg any ideas here?
</comment><comment author="nz" created="2015-04-21T02:51:59Z" id="94621787">I've been seeing something similar on a cluster recently. Odd because it's been experiencing out of memory errors and we suspect percolation is contributing to that. Running ES 1.4.4.

``` json
"total" : {
  "percolate" : {
    "total" : 28019,
    "time_in_millis" : 26028,
    "current" : 0,
    "memory_size_in_bytes" : -1,
    "memory_size" : "-1b",
    "queries" : 1416
  }
}
```
</comment><comment author="sibagatov" created="2015-04-23T07:50:35Z" id="95479753">See src\main\java\org\elasticsearch\index\percolator\stats\ShardPercolateService.java stats() method.
Also below RamEstimator is commented out with comment "Enable when a more efficient manner is found for estimating the size of a Lucene query."
</comment><comment author="hydrogen18" created="2015-04-23T12:07:20Z" id="95561428">So, it seems that this statistic is not useful in the 1.4.x versions?
</comment><comment author="clintongormley" created="2015-04-26T14:28:26Z" id="96392154">Yes it is currently disabled. See https://github.com/elastic/elasticsearch/issues/5339
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel reports incorrect document count and store size.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10079</link><project id="" key="" /><description>See the query result which says about 45M docs and 94GB  as size, Marvel graph have these values nearly half of actual values.
health status index       pri rep docs.count docs.deleted store.size pri.store.size
green  open   test_insert   5   1   44999486            0    188.2gb         94.1gb

![doccount](https://cloud.githubusercontent.com/assets/8180763/6639123/2c67705c-c963-11e4-86ac-333760152919.png)
</description><key id="61064075">10079</key><summary>Marvel reports incorrect document count and store size.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels /><created>2015-03-13T13:30:28Z</created><updated>2015-03-13T17:32:14Z</updated><resolved>2015-03-13T17:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-13T14:02:53Z" id="78984691">Thx for reporting. Just to make sure we put some things aside, I want to double check you have a single node cluster (the image is from a node stat chart while the cat call looks like it comes from an index stats). If not, the marvel chart just shows the shards assigned to that node, not the entire index. 

Also - can you check what the `_nodes/stats` api reports when calling ES directly? look for `indices.docs.count` &amp; `indices.store.size_in_bytes` in the output
</comment><comment author="ajaybhatnagar" created="2015-03-13T14:26:14Z" id="78994815">Thanks for prompt review. I just wiped the whole cluster and will be running the same test again provide requested details. For info,  the cluster had 5 data nodes, one index , 5 shards and one replica. Each data node had one primary and one replica shard.

If I understood correctly, the info is only for the node (i.e. documents stored on the node). Rough estimates (assuming balanced distribution), should be about (45Pri+45Rep)=90M/5 ~=18M documents on the node and size ~= 188/5=38Gb 

May be the titles on the could be improved to make it more specific.
</comment><comment author="bleskes" created="2015-03-13T14:30:51Z" id="78997278">&gt; If I understood correctly, the info is only for the node (i.e. documents stored on the node). Rough estimates (assuming balanced distribution), should be about (45Pri+45Rep)=90M/5 ~=18M documents on the node and size ~= 188/5=38Gb

Yeah. I think this means the marvel stats are OK?

&gt; May be the titles on the could be improved to make it more specific.

The dashboard is called node stats and you get to it by clicking on a node. The chart is under "Indices Store" which needs to be clicked open. What would you suggest renaming the chart to in order to make it clearer, given this context?
</comment><comment author="ajaybhatnagar" created="2015-03-13T15:01:59Z" id="79019800">Suggested titles for the two charts 
Documents on &lt;Host&gt; and 
Doc. Size on &lt;Host&gt; 
where &lt;Host&gt; is the actual hostname in cluster.
</comment><comment author="bleskes" created="2015-03-13T17:32:14Z" id="79165372">yeah. These are a bit tricky as they don't scale when people view data for multiple nodes. I hear you though and I'll think about possible improvements here. I'm closing this for now, as the stats seems correct. Please feel free to reopen if I missed something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch.bat fails to start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10078</link><project id="" key="" /><description>D:\elasticsearch-1.4.4\elasticsearch-1.4.4\bin&gt;elasticsearch.bat
The system cannot find the path specified.

No error message. Not really what the batch file does not start.
</description><key id="61003817">10078</key><summary>elasticsearch.bat fails to start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">agiletestware</reporter><labels><label>:Packaging</label></labels><created>2015-03-13T07:27:01Z</created><updated>2015-03-15T23:00:46Z</updated><resolved>2015-03-15T01:02:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-13T14:23:52Z" id="78993674">I would bet that JAVA_HOME is not set or "java" command can not be found.
Could you check your Java settings?
</comment><comment author="agiletestware" created="2015-03-15T01:02:46Z" id="80786177">yup, you were right. Maybe a better error message in the future
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug in PageCacheRecycler?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10077</link><project id="" key="" /><description>I hit a problem where PageCacheRecycler seems to be too big. I have an impression this is bigger than the limit. Default page.limit.heap is 10% and default weights for byte/long/int/float/double are 1/1/1/1/1. This implies that for 30G heap NONE of pages should be bigger than 600MB if I'm not wrong.

Looking into PageCacheRecycler.maxCount method calculates higher values for bigger data types. If for float maxCount is 1000 for double it is 2000. I think this is not correct because (later in PageCacheRecycler class) I can see this:

```
return new byte[BigArrays.BYTE_PAGE_SIZE];
return new int[BigArrays.INT_PAGE_SIZE];
return new long[BigArrays.LONG_PAGE_SIZE];
return new float[BigArrays.FLOAT_PAGE_SIZE];
return new double[BigArrays.DOUBLE_PAGE_SIZE];
return new Object[BigArrays.OBJECT_PAGE_SIZE];
```

All of those arrays seems to have by default the same size 16kB.

My intuition says that all calls like:

```
bytePage = build(type, maxCount(limit, BigArrays.BYTE_PAGE_SIZE, bytesWeight, totalWeight), searchThreadPoolSize, availableProcessors, new AbstractRecyclerC&lt;byte[]&gt;() {
doublePage = build(type, maxCount(limit, BigArrays.DOUBLE_PAGE_SIZE, doubleWeight, totalWeight), searchThreadPoolSize, availableProcessors, new AbstractRecyclerC&lt;double[]&gt;() {
```

should be

```
bytePage = build(type, maxCount(limit, BigArrays.PAGE_SIZE_IN_BYTES, bytesWeight, totalWeight), searchThreadPoolSize, availableProcessors, new AbstractRecyclerC&lt;byte[]&gt;() {
doublePage = build(type, maxCount(limit, BigArrays.PAGE_SIZE_IN_BYTES, doubleWeight, totalWeight), searchThreadPoolSize, availableProcessors, new AbstractRecyclerC&lt;double[]&gt;() {
```

I'll simulate maxSize of double[] with current calls of maxCount, 30G heap and 10% limit for recycled pages size.

maxCount(3'000'000'000, 2048, 1, 4.1) -&gt; 1 / 4.1 \* 3'000'000'000 / 2048 = 357278
Each page size is 16kB so ... 16kB \* 357278 implies **5.7GB** (only for doubles). Isn't it too much? Can anyone take a look and verify?

Moreover i see another - minor bug

```
final double totalWeight = bytesWeight + intsWeight + longsWeight + doublesWeight + objectsWeight;
```

It seems that floatWeight is missing
</description><key id="60903221">10077</key><summary>Bug in PageCacheRecycler?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2015-03-12T21:10:08Z</created><updated>2015-03-16T16:14:12Z</updated><resolved>2015-03-16T03:29:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-12T22:51:16Z" id="78672590">You are totally right! Would you like to open a pull request? I'm just asking in case you would like to do it since you already did the investigation but if you have no time for it, I'll happily do it.
</comment><comment author="prog8" created="2015-03-13T13:48:02Z" id="78977650">I'd be glad if you can prepare the change. Thanks!
</comment><comment author="jpountz" created="2015-03-13T17:11:27Z" id="79145048">No problem, I will!
</comment><comment author="jpountz" created="2015-03-16T16:14:12Z" id="81768150">It's been fixed and the fix will be in 1.5 and future versions. Thanks for reporting this bug and investigating!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make assert less strict to ensure local node is not null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10076</link><project id="" key="" /><description>we can trip this assert if we create a shard while we are shutting down the node 

see http://build-us-00.elasticsearch.org/job/es_core_master_window-2012/1081/consoleFull
</description><key id="60879542">10076</key><summary>Make assert less strict to ensure local node is not null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-12T19:12:24Z</created><updated>2015-06-07T10:17:03Z</updated><resolved>2015-03-12T19:17:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-12T19:13:00Z" id="78575147">@bleskes can you take a quick look?
</comment><comment author="bleskes" created="2015-03-12T19:13:20Z" id="78575271">LGTM
</comment><comment author="s1monw" created="2015-03-12T19:13:33Z" id="78575341">w00t
</comment><comment author="bleskes" created="2015-03-12T19:13:50Z" id="78575448">async reviews rock
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10075</link><project id="" key="" /><description>I propose this change to avoid error bellow.

{
  "error" : "ElasticsearchIllegalArgumentException[failed to execute script]; nested: GroovyScriptEx
ecutionException[NullPointerException[Cannot execute null+null]]; ",
  "status" : 400
}
</description><key id="60868294">10075</key><summary>Fix typo in reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hasen</reporter><labels /><created>2015-03-12T18:14:34Z</created><updated>2015-03-30T08:38:39Z</updated><resolved>2015-03-18T20:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hasen" created="2015-03-13T01:57:03Z" id="78744639">I already signed the CLA, but I don't know how to update the status.
</comment><comment author="javanna" created="2015-03-18T20:29:51Z" id="83158748">Hi @nesah I just tested the scripts in the docs and they seem ok as is. You get that error when executing the update script if you haven't executed the previous script before, which updates the existing document and adds the `age` field to it. Once the field is in part of the document the existing script can be executed against it using the `ctx._source.age` notation to refer to it.
</comment><comment author="hasen" created="2015-03-30T08:38:39Z" id="87592076">@jacobevans , I understand. Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need additional line in config file, to work tutorial's example.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10074</link><project id="" key="" /><description>I'm afraid to say, I wrote bellow 'additional line' in ' elasticsearch/config/elasticsearch.yml
', to work tutorial's example about 'Updating Documents' by 'script'.
So, it is needed to add an explanation in tutorial.

// additional line
script.groovy.sandbox.enabled: true

// error
{
  "error" : "ElasticsearchIllegalArgumentException[failed to execute script]; nested: ScriptExceptio
n[dynamic scripting for [groovy] disabled]; ",
  "status" : 400
}
</description><key id="60867379">10074</key><summary>Need additional line in config file, to work tutorial's example.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">hasen</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-12T18:10:10Z</created><updated>2015-03-30T08:36:39Z</updated><resolved>2015-03-19T14:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T14:33:23Z" id="83610409">Thanks @nesah I just added a note mentioning that dynamic scripting needs to be enabled in order for the script based update to work.
</comment><comment author="hasen" created="2015-03-30T08:36:39Z" id="87591815">@javanna, thank you very much : )
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restarting node 1.4.4 cluster resulted in red state, with dangling indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10073</link><project id="" key="" /><description>Hi,

How to recover from the following?

```
dangling index, exists on local file system, but not in cluster metadata,
auto import to cluster state [YES]
```

This morning (~6 hours ago) we restarted one node (not master). After restart the following was in the logging. Cluster is now in red state, because primary and replica shards are unassigned.

```
[2015-03-12 11:49:44,001][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-adm-log4json-2015.01.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,002][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-shd-apache-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,004][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-adm-apache-2015.01.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,004][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-pro-log4j-2015.01.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,005][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-squid-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,005][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-apache-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,006][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-shd-log4json-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,007][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-shd-squid-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,007][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-log4json-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,008][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-log4j-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,008][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-endeca-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,009][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-pro-endeca-2015.01.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,009][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] [logstash-xpr-oracle-2015.02.26] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state [YES]
[2015-03-12 11:49:44,085][INFO ][http                     ] [adm-logsearch-db-001.esprodata01] bound_address {inet[/10.98.252.21:9200]}, publish_address {inet[/10.98.252.21:9200]}
[2015-03-12 11:49:44,085][INFO ][node                     ] [adm-logsearch-db-001.esprodata01] started
```

Please advice.
</description><key id="60845018">10073</key><summary>Restarting node 1.4.4 cluster resulted in red state, with dangling indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rtoma</reporter><labels /><created>2015-03-12T16:15:02Z</created><updated>2015-03-15T21:31:07Z</updated><resolved>2015-03-12T18:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-12T18:55:08Z" id="78567608">please use the mailing list for questions like this. this is only for dev issues/bug. 

Regarding your question, if you don't use those indices why don't you just delete them?
</comment><comment author="rtoma" created="2015-03-13T08:30:52Z" id="78866446">Hi Simon,

I guess I used to much 'I need support' trigger words?

My issue was: holy crap, ES is dropping indices after a restart of a single node. This sounds like a bug right?

Anyways I've done some more digging and think I have found something interesting for you. It seems that the dangling indices got deleted by curator. Some 7 hours later one node got restarted and the deleted indices reappeared, but without data, so cluster got red. Thats a bug to me.

Lets take one of those dangling indices as example: logstash-xpr-apache-2015.02.26

Here is what happens:
- curator deletes it at Mar 12th 04:05
- 1 node is restarted and upon startup at Mar 12th 11:49 it detected a dangling index
- the master auto-imports it at Mar 12th 11:49
- cluster got red, because the dangling indices did not have a primary or replica available
- curator deletes it (again) at Mar 13th 04:07
- cluster goes green

So on 1st DELETE did work only half: data got removed, but index stayed in local metadata on 001.

Raw logs:

```
# on master
[2015-03-12 04:05:39,636][INFO ][cluster.metadata         ] [shd-logsearch-db-007.esprodata07] \
  [logstash-xpr-apache-2015.02.26] deleting index

# on node 001
[2015-03-12 11:49:44,005][INFO ][gateway.local.state.meta ] [adm-logsearch-db-001.esprodata01] \
  [logstash-xpr-apache-2015.02.26] dangling index, exists on local file system, but not in cluster 
  metadata, auto import to cluster state [YES]

# on master
[2015-03-12 11:49:44,103][INFO ][gateway.local.state.meta ] [shd-logsearch-db-007.esprodata07] \
  auto importing dangled indices  ... [logstash-xpr-apache-2015.02.26/OPEN]... ] from \
  [[adm-logsearch-db-001.esprodata01][568FE5YIQDqOLBA_JJbOZw][adm-logsearch-db-\
  001.bolcom.net][inet[/10.98.252.21:9300]]{datacenter=ams5, zone=adm-logsearch-db-001, \
  master=true}]
```
</comment><comment author="rtoma" created="2015-03-13T13:02:49Z" id="78961394">FYI, restarted the node 001 again and now no more zombie indices. 
</comment><comment author="s1monw" created="2015-03-13T17:37:43Z" id="79171274">@rtoma sorry for moving so quickly. If you stop a node, delete indices and start that node after that again you will get those indices back. That is just a safety feature to not wipe your data without asking you to do it. I am afraid that this is a feature rather than a bug. It's just safer to bring them back and let the user decide. you can opt out of this feature but in 2.0 we make it non-opt-out and always import. Hope this makes sense?
</comment><comment author="rtoma" created="2015-03-14T12:15:14Z" id="80397963">@s1monw not a problem. I seem to be unable to relay the issue. 
Its not:
- stop
- delete
- start
- issue

But:
- delete
- stop
- start
- issue

So this is not the dangling issue, but more that an index delete did delete its data, but not its metadata on disk.
</comment><comment author="s1monw" created="2015-03-15T21:31:07Z" id="81250576">yeah we fixed several things related to this in 1.5 that makes sure we delete the metadata even if the index data can not be deleted. We should likely go further and write tombstones instead of deleting to make sure even if we can't delete we mark indices as deleted
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date format using seconds since epoch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10072</link><project id="" key="" /><description>We currently use seconds since epoch to store dates in Elasticsearch. However, this is interpreted as milliseconds since epoch and stores them internally as such. 

Example: 10/Feb/2015:17:54:07 in seconds (1423590847) becomes 01/17/1970 11:26:30

Unfortunately this issue means that we have to use date histogram facets instead of aggregations in order to use the "factor" field to convert it accordingly (discussed in #6490). 

Possible solutions would be: mention that Unix time in seconds isn't a supported date format in the documentation or allow the Unix seconds format to be specified in the mapping.
</description><key id="60836080">10072</key><summary>Date format using seconds since epoch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cameronevans</reporter><labels><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-03-12T15:22:49Z</created><updated>2015-04-09T10:28:33Z</updated><resolved>2015-04-09T10:28:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T04:40:17Z" id="81409084">I agree an option on date fields to interpret numbers as seconds since epoch instead of milliseconds would make sense.
</comment><comment author="jpountz" created="2015-03-16T04:42:26Z" id="81409765">Dates would still be stored as milliseconds since Epoch internally, which should be fine since Lucene doc values have gcd compression exactly for this use-case: it would figure out that all values share a common divisor and use it for compression.
</comment><comment author="saffroy" created="2015-04-06T22:25:15Z" id="90263976">Thanks @jpountz for working on this!
One question regarding your commit c7115f8: with 'numeric_resolution' set to 'seconds', will it parse a double (or string) value of 42.001 into 42001 ms or 42000 ms?
</comment><comment author="jpountz" created="2015-04-07T12:04:08Z" id="90527671">I just checked: it would be parsed as 42000ms.
</comment><comment author="jpountz" created="2015-04-07T12:06:01Z" id="90528529">Unfortunately, the abstraction we are using right now (`java.util.concurrent.TimeUnit`) cannot deal with fractions so making it parse as 42001 would require some work. I suggest that we merge the `numeric_resolution` change first and then open a new issue if better handling of double values is desirable?
</comment><comment author="saffroy" created="2015-04-07T13:53:19Z" id="90560888">I support your suggestion of opening a new issue on top of the above change, though for me it's mostly a convenience issue, ie. not really a big deal.

In my case, the annoyance was that I thought it would work without an extra step (seconds since epoch seemed like a common format to me, much more than milliseconds since epoch), until I found that I had to explicitly tell Logstash to convert from seconds+fraction of a second since epoch (luckily Logstash makes it easy, once you know that you need it).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug when fvh highlighting a phrase query on an ngrammed field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10071</link><project id="" key="" /><description>Not really sure how to accurately describe this bug. I've narrowed it down to a combination of the following elements:
- Fast Vector Highligter
- Phrase query with slop
- Performing a search where at least one of the words is a prefix of another word in the query (in the case below: `in` and `internet`.

Steps to reproduce:

``` bash
#1. Create index with mapping
curl -XPUT "http://localhost:9200/bugtest" -d'
{
  "settings": {
    "analysis": {
      "analyzer": {
        "suggest_analyzer": {
          "type": "custom",
          "tokenizer": "my_edgeNgram",
          "filter": [
            "lowercase"
          ]
        }
      },
      "tokenizer": {
        "my_edgeNgram": {
          "type": "edgeNGram",
          "min_gram": 1,
          "max_gram": 20,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  },
  "mappings": {
    "case": {
      "properties": {
        "text": {
          "type": "string",
          "term_vector": "with_positions_offsets",
          "index_analyzer": "suggest_analyzer",
          "search_analyzer": "simple"
        }
      }
    }
  }
}'

#2. Put a document in it that will match the query
curl -XPUT "http://localhost:9200/bugtest/case/1" -d'
{
  "text": "Something is happening in the internet.."
}'

#3. Perform a highlighted phrase query where the first token is a prefix of the second
curl -XPOST "http://localhost:9200/bugtest/_search" -d'
{
  "query": {
    "match": {
      "text": {
        "query": "in internet",
        "type": "phrase",
        "slop": 50
      }
    }
  },
  "fields": [],
  "highlight": {
    "fields": {
      "text": {}
    }
  }
}'
```

This results in the following error on es 1.4.4:

``` json
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 4,
      "failed": 1,
      "failures": [
         {
            "index": "bugtest",
            "shard": 2,
            "status": 500,
            "reason": "FetchPhaseExecutionException[[bugtest][2]: query[text:\"in internet\"~50],from[0],size[10]: Fetch Failed [Failed to highlight field [text]]]; nested: StringIndexOutOfBoundsException[String index out of range: -2]; "
         }
      ]
   },
   "hits": {
      "total": 1,
      "max_score": 0.039147545,
      "hits": []
   }
}
```

As said, this only happens when a search is performed where the first word is a prefix of the second. When I change the query just one letter to `is internet` everything works as expected.
</description><key id="60791078">10071</key><summary>Bug when fvh highlighting a phrase query on an ngrammed field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsander</reporter><labels><label>:Highlighting</label><label>adoptme</label><label>bug</label></labels><created>2015-03-12T09:26:12Z</created><updated>2016-11-24T19:21:35Z</updated><resolved>2016-11-24T19:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-12T19:00:59Z" id="78570907">hmm can you do me a favor and try simple analyzer for both search and index? It seems like the suggest analyzer is messing with positions here...
</comment><comment author="bsander" created="2015-03-12T21:19:11Z" id="78622529">Yes, I tried that too while chasing down this thing. If all analysis on the field is changed to `simple` the problem doesn't occur.
</comment><comment author="nik9000" created="2015-03-13T01:26:00Z" id="78734634">My Experimental highlighter doesn't seem to mind that use case.  I imagine its something with the FVH.  I'll do a bit of debugging. 
</comment><comment author="nik9000" created="2015-03-13T01:39:26Z" id="78738713">Looks like the FVH just messes up the fragments somehow: `subInfos=(ininternet((30,32)(30,38)))/1.0(0,100)`
</comment><comment author="nik9000" created="2015-03-13T01:54:05Z" id="78743669">It puts the "in" part of the match on top of the "internet" part of the match....

It does that because `in` is a prefix of `internet`.....

The experimental highlighter doesn't mind these because it contains a filter that removes overlaps like this.

I think the right thing to do here is to stop FieldTermStack from spitting out overlaps.  Or to teach BaseFragListBuilder to merge the overlaps.  Probably the second one is better because its broader - it'd handle other overlaps in case they come up.
</comment><comment author="bsander" created="2015-05-12T07:31:22Z" id="101166616">Hi @nik9000, is this still something that's being worked on?
</comment><comment author="nik9000" created="2015-05-12T13:53:15Z" id="101290553">@bsander I'm not, no. I diagnosed it but never worked on it. Right now issues like this are a weekend project for me and with three kids I don't typically get to it. Its also a bit of a conflict of interest for me because the experimental highlighter is my baby. And the more time I work on it the less I want to go back and fix stuff in the other 3 highlighters....
</comment><comment author="clintongormley" created="2016-11-24T19:21:35Z" id="262834636">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Bug] 'ignore_malformed' does not always work, and occasionally throws an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10070</link><project id="" key="" /><description>The idea behind `ignore_malformed` setting, is to ensure all records being added get added to the index, even if some of the JSON attribute use an invalid data type (compared to the index's expected mapping). This is particularly useful with dynamic JSON document, where it's most important that all objects are "added" (rather than rejected via exception).

However, there are some cases where malformed JSON object are being reject (exception) instead of getting accepted. Repro steps:
-  Make sure the `ignore_malformed` setting is enabled
- Add a first record

```
~$ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/123' -d '{ "value" : 12345 }'
{"_index":"tweets","_type":"tweet","_id":"123","_version":1,"created":true}~ $
```

The index now expected a `long` value for the "value" field.
- Add a malformed record

```
~ $ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/124' -d '{ "value" : "Hello" }'
{"_index":"tweets","_type":"tweet","_id":"124","_version":1,"created":true}
```

The malformed record is accepted, as expected.
- Add another malformed record

```
~ $ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/125' -d '{ "value" : 123123123123123123123 }'
{"error":"MapperParsingException[failed to parse [value]]; nested: JsonParseException[Numeric value (123123123123123123123) out of range of long (-9223372036854775808 - 9223372036854775807)\n at [Source: [B@52259927; line: 1, column: 34]]; ","status":400}
```

Expected: This malformed record should still be added, with the `123123123123123123123` value coerced into a `string`.
</description><key id="60782604">10070</key><summary>[Bug] 'ignore_malformed' does not always work, and occasionally throws an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Chetane</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-12T07:49:12Z</created><updated>2015-12-05T20:25:23Z</updated><resolved>2015-12-05T20:25:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T15:28:11Z" id="89596146">Hi @Chetane 

I agree that ignore malformed just ignore your last document instead of throwing an exception. (It shouldn't try to coerce to a string, which makes no sense for a numeric field, but it should just ignore the illegal value).
</comment><comment author="clintongormley" created="2015-12-05T20:25:22Z" id="162243359">Closing in favour of #11513
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prioritizing shard recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10069</link><project id="" key="" /><description>Feature request:

When multiple shard replicas need recovery (eg, when you restart the whole cluster), it can be beneficial to get some indices green before others.
In the Logstash case, getting the most recent indices by date.
In other cases, there may be application demands on which index you really need back first.

Being able to set a specific priority on indices, or have it automatically determined for date-based indices, would be quite helpful.
</description><key id="60780278">10069</key><summary>Prioritizing shard recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels /><created>2015-03-12T07:17:22Z</created><updated>2015-06-23T17:30:02Z</updated><resolved>2015-06-23T17:30:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-12T18:53:03Z" id="78566340">once we fixed https://github.com/elastic/elasticsearch/issues/10032 this will be much easier since we can recover unsealed shards first or even last since all the sealed once are instant and you problem will be going away entirely.
</comment><comment author="kadaan" created="2015-03-17T08:09:48Z" id="82194690">#10032 may not address the full request as certain indices may be more important for a business then others.  Ideally, ES could take this priority into consideration when determining which shards to recover given limited recovery resources.
</comment><comment author="clintongormley" created="2015-06-23T17:30:00Z" id="114580729">Closing in favour of #11787
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Red indexes with nullpointer exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10068</link><project id="" key="" /><description>We are having problem while serving query. We were running ES 1.2.1 on Cent OS installed as RPM package and suddenly started getting errors while querying. The log showed this:

```
[2015-03-09 18:35:25,102][INFO ][cluster.metadata         ] [Morpheus] [thedealspoint] update_mapping [publicDeals] (dynamic)
[2015-03-09 18:35:25,131][WARN ][index.engine.internal    ] [Morpheus] [thedealspoint][0] failed engine [refresh failed]
[2015-03-09 18:35:25,162][WARN ][cluster.action.shard     ] [Morpheus] [thedealspoint][0] sending failed shard for [thedealspoint][0], node[Sa7pUWi2SNmVSL22U4eITw], [P], s[STARTED], indexUUID [v-u_MHhfQMyTbZ6-_XqH0w], reason [engine failure, message [refresh failed][NullPointerException[null]]]
[2015-03-09 18:35:25,163][WARN ][cluster.action.shard     ] [Morpheus] [thedealspoint][0] received shard failed for [thedealspoint][0], node[Sa7pUWi2SNmVSL22U4eITw], [P], s[STARTED], indexUUID [v-u_MHhfQMyTbZ6-_XqH0w], reason [engine failure, message [refresh failed][NullPointerException[null]]]
[2015-03-09 18:35:26,842][DEBUG][action.search.type       ] [Morpheus] All shards failed for phase: [query_fetch]
[2015-03-09 18:35:34,371][DEBUG][action.search.type       ] [Morpheus] All shards failed for phase: [query_fetch]
[2015-03-09 18:35:34,395][DEBUG][action.search.type       ] [Morpheus] All shards failed for phase: [query_fetch]
[20
```

Our setup was default one node installation with memory increased and ES was running without any maintenance for almost a year now. We felt marvel was taking too much space and hence cleared up the marvel indexes. We also deleted our own index and did a fresh indexing. But there was no luck. Things work for few queries and then fails with the same message above. Ultimately unzipped latest ES version (elasticsearch-1.4.4) on a new directory. Started with default settings and did again fresh indexing (Our content is in MySQL and ES is used as search engine). Things work for few queries and then the shard gets unassigned after a while. The error message is shown below:

[vantage@vc-prod elasticsearch-1.4.4]$ bin/elasticsearch

```
[2015-03-11 18:46:17,904][INFO ][node                     ] [Doctor Strange] version[1.4.4], pid[13798], build[c88f77f/2015-02-19T13:05:36Z]
[2015-03-11 18:46:17,905][INFO ][node                     ] [Doctor Strange] initializing ...
[2015-03-11 18:46:17,911][INFO ][plugins                  ] [Doctor Strange] loaded [], sites []
[2015-03-11 18:46:21,450][INFO ][node                     ] [Doctor Strange] initialized
[2015-03-11 18:46:21,450][INFO ][node                     ] [Doctor Strange] starting ...
[2015-03-11 18:46:21,626][INFO ][transport                ] [Doctor Strange] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/128.199.143.152:9300]}
[2015-03-11 18:46:21,700][INFO ][discovery                ] [Doctor Strange] elasticsearch/2bQbRn93SOeDdmDLHijwBg
[2015-03-11 18:46:25,482][INFO ][cluster.service          ] [Doctor Strange] new_master [Doctor Strange][2bQbRn93SOeDdmDLHijwBg][vc-prod][inet[/128.199.143.152:9300]], reason: zen-disco-join (elected_as_master)
[2015-03-11 18:46:25,519][INFO ][http                     ] [Doctor Strange] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/128.199.143.152:9200]}
[2015-03-11 18:46:25,520][INFO ][node                     ] [Doctor Strange] started
[2015-03-11 18:46:26,459][INFO ][gateway                  ] [Doctor Strange] recovered [2] indices into cluster_state
[2015-03-11 18:46:26,639][DEBUG][action.search.type       ] [Doctor Strange] All shards failed for phase: [query_fetch]
[2015-03-11 18:46:26,639][DEBUG][action.search.type       ] [Doctor Strange] All shards failed for phase: [query_fetch]
[2015-03-11 18:46:26,862][WARN ][indices.cluster          ] [Doctor Strange] [thedealspoint][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [thedealspoint][0] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.engine.FlushFailedEngineException: [thedealspoint][0] Flush failed
    at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:926)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryFinalization(InternalIndexShard.java:749)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:291)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    ... 3 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$CompletionLookupProvider.parsePayload(Completion090PostingsFormat.java:337)
    at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$CompletionPostingsConsumer.addPosition(AnalyzingCompletionLookupProvider.java:189)
    at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$GroupedPostingsConsumer.addPosition(Completion090PostingsFormat.java:188)
    at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:488)
    at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:80)
    at org.apache.lucene.index.DefaultIndexingChain.flush(DefaultIndexingChain.java:114)
    at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:439)
    at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:513)
    at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:624)
    at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2949)
    at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3104)
    at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3071)
    at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:916)
    ... 6 more
[2015-03-11 18:46:26,890][WARN ][cluster.action.shard     ] [Doctor Strange] [thedealspoint][0] sending failed shard for [thedealspoint][0], node[2bQbRn93SOeDdmDLHijwBg], [P], s[INITIALIZING], indexUUID [vjORGaDpSsKPcGddR2ELdg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[thedealspoint][0] failed recovery]; nested: FlushFailedEngineException[[thedealspoint][0] Flush failed]; nested: NullPointerException; ]]
[2015-03-11 18:46:26,890][WARN ][cluster.action.shard     ] [Doctor Strange] [thedealspoint][0] received shard failed for [thedealspoint][0], node[2bQbRn93SOeDdmDLHijwBg], [P], s[INITIALIZING], indexUUID [vjORGaDpSsKPcGddR2ELdg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[thedealspoint][0] failed recovery]; nested: FlushFailedEngineException[[thedealspoint][0] Flush failed]; nested: NullPointerException; ]]
[2015-03-11 18:46:28,818][DEBUG][action.search.type       ] [Doctor Strange] All shards failed for phase: [query_fetch]
[2015-03-11 18:46:30,173][DEBUG][action.search.type       ] [Doctor Strange] All shards failed for phase: [query_fetch]
[2015-03-11 18:46:33,339][DEBUG][action.search.type       ] [Doctor Strange] All shards failed for phase: [query_fetch]
```

---

Ultimately we created a completely new CentOS instance, installed ES and did a fresh installation and indexing. Things are working so far but we would like to get to the bottom of the null pointer issue and also move to the old installation if possible. 

Thanks, 
Anjan
</description><key id="60768583">10068</key><summary>Red indexes with nullpointer exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anjanpathak</reporter><labels><label>feedback_needed</label></labels><created>2015-03-12T04:10:02Z</created><updated>2015-10-30T21:03:24Z</updated><resolved>2015-10-30T21:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T15:25:02Z" id="89594683">Hi @anjanpathak 

I see that you're using the completion suggester.  I wonder if you have two fields with the same name in two different types in the same index, one mapped as a completion field and one not?  In your new installation, are you still using the completion suggester?  It may also be a bug that has already been fixed.
</comment><comment author="clintongormley" created="2015-04-26T19:59:01Z" id="96428652">Hi @anjanpathak 

Any more info here?
</comment><comment author="dakrone" created="2015-10-30T21:03:24Z" id="152650590">Closing since we have not heard back in over 6 months.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: remove delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10067</link><project id="" key="" /><description>This method is exceptionally trappy.  I think we should remove it and add it back only once we can do it safely.

It secretly does a refresh with each request, which means it makes changes visible if maybe you didn't want to (https://github.com/elastic/elasticsearch/issues/3593).

It also means if you call this while concurrently index you can easily blow up the number of segments in the shard (merging can't keep up), leading to all sorts of problems e.g. OOME (#https://github.com/elastic/elasticsearch/issues/6025), super slow indexing, etc.

Finally, it can cause inconsistent docs indexed in primary vs. replica since the query is re-executed on the replica possibly matching different documents.

Until just recently we failed to throttle delete-by-query when merges were falling behind (https://github.com/elastic/elasticsearch/pull/9986)

I think we should deprecate (remove in 2.0) for now, and only once we have task management should we add it back without all these traps (https://github.com/elastic/elasticsearch/issues/7052).
</description><key id="60750215">10067</key><summary>Core: remove delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>breaking</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-11T23:58:54Z</created><updated>2015-05-28T17:00:15Z</updated><resolved>2015-05-28T17:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="simonbrandhof" created="2015-03-12T18:56:30Z" id="78568455">If delete-by-query is removed, then how would you truncate an index ? Deleting and creating again the index requires to know metadata, which is not always possible.
</comment><comment author="mikemccand" created="2015-03-12T20:54:11Z" id="78612598">&gt; If delete-by-query is removed, then how would you truncate an index

Deleting the entire index is much faster than delete-by-query of all docs in the index.  But if that's not an option I suggest you run scan+scroll of your query to get all hits and then do bulk-request deletes of the returned ids.
</comment><comment author="s1monw" created="2015-03-12T22:16:22Z" id="78652326">&gt; If delete-by-query is removed, then how would you truncate an index ? Deleting and creating again the index requires to know metadata, which is not always possible.

In this case I recommend to just read the metadata from the index and put it in your create request, then delete the index and you are done with it.
</comment><comment author="s1monw" created="2015-03-12T22:17:28Z" id="78653173">@mikemccand I am not sure if we should do this in 1.5 but deprecating it I think is a no brainer?
</comment><comment author="mikemccand" created="2015-03-12T22:50:57Z" id="78672474">&gt; @mikemccand I am not sure if we should do this in 1.5 but deprecating it I think is a no brainer?

Yeah my plan is deprecate in 1.5 and remove in 2.0.
</comment><comment author="mikemccand" created="2015-03-23T08:42:30Z" id="84887389">I started to remove this in 2.0 but got stuck because we use delete-by-query internally to allow deleting an entire type from an index ... once we remove this in 2.0 (#8877) then we can do this issue.
</comment><comment author="mikemccand" created="2015-03-24T19:39:10Z" id="85663622">Since #8877 is done I went and removed all delete-by-query logic but ... then I realized: what if a user has a DBQ in the translog on a shard and upgrades to 2.0?  Must we support this back-compat case?  Can we "require" that users flush (clears the translog) before upgrading?

If not ... I need to put back the DBQ logic for translog and Engine.
</comment><comment author="s1monw" created="2015-03-24T20:25:31Z" id="85678347">&gt; If not ... I need to put back the DBQ logic for translog and Engine.

I am afraid we have to.
</comment><comment author="rjernst" created="2015-03-24T20:54:02Z" id="85687385">But we can still remove the public API right? This can just be an internal backcompat implementation detail?
</comment><comment author="mikemccand" created="2015-03-24T20:58:24Z" id="85688292">&gt; But we can still remove the public API right? This can just be an internal backcompat implementation detail?

Yeah ... I also need to fix the static back compat indices to leave some DBQs in the translog and confirm on upgrade the docs are in fact deleted ...
</comment><comment author="mikemccand" created="2015-03-25T16:28:34Z" id="86105228">&gt; &gt; If not ... I need to put back the DBQ logic for translog and Engine.
&gt; 
&gt; I am afraid we have to.

Maybe we can simply detect when this ("DBQ in translog on upgrade") happens and refuse to start the shard, saying that you must go back to the prior version and flush the shard?
</comment><comment author="rjernst" created="2015-03-25T21:54:14Z" id="86230080">That seems reasonable? This is an extreme edge case. If they are upgrading from 1.5+, the flush is done automatically on shutdown.  And we already recommend running flush before any shutdown in general. So the only way for them to hit this is to have done a DBQ since the last time a flush happened (max 30 mins by default?), and then to have shutdown without doing a flush.
</comment><comment author="mikemccand" created="2015-03-26T09:13:57Z" id="86417327">&gt; That seems reasonable? This is an extreme edge case. If they are upgrading from 1.5+, the flush is done automatically on shutdown.

Alas, the flush is only done in 1.5+ if a recovery was not also in process when the node was shut down, because a recovery blocks flushes (separately, we need to fix translog so it's ref counted instead: the two operations really should be independent).

Anyway, I think for this we should just keep the "replay DBQ on upgrade" ... the DBQ code that we need to keep around to do this is very small: just Engine, TransLog, IndexShard.
</comment><comment author="kimchy" created="2015-03-26T15:44:28Z" id="86581542">++ on fixing current behavior of delete by query. I think we all agree on the fix, which is to do a scan search and bulk deletes, instead of using delete-by-query, since thats safer and more consistent with our replication model. It might be a long running task, but relatively lightweight, so I don't think we need to wait for the task management infra if we plan to remove delete-by-query in 2.0, and then add it back when we have it, but just go ahead and replace the current implementation with scan/delete and not deprecate it.
</comment><comment author="bleskes" created="2015-03-26T15:50:58Z" id="86586024">+1 on giving the user an alternative, albeit a slowish instead of removing the current implementation first and then later adding a managed one.
</comment><comment author="mikemccand" created="2015-03-26T18:40:54Z" id="86663053">I agree it would be great to "delete old way and add new way" in one go...

&gt; so I don't think we need to wait for the task management infra if we plan to remove delete-by-query in 2.0

That would be great, if it really is OK to just "be slow" sometimes (when the query deleted many docs).  I guess there is precedent here: the optimize API (with wait_for_merge=true) can clearly take a very long time...

Somehow, whichever node receives the DBQ request, must open a client connection (to itself?), run the scan search, scroll through the hits and make bulk delete requests.  I think #10251 is an example of how to do this ...

I'll add a comment on #7052 that we don't need to wait for task management API, and block this issue on it.
</comment><comment author="mikemccand" created="2015-03-26T18:42:35Z" id="86663389">This issue is blocked on #7052.
</comment><comment author="s1monw" created="2015-03-26T20:11:35Z" id="86696473">I don't think this need to be blocked on #7052 We can remove this now and add the other one as a followup. Stuff like this should not be blocked syntactic sugar
</comment><comment author="mikemccand" created="2015-03-26T22:43:14Z" id="86743497">&gt; I don't think this need to be blocked on #7052 

OK I opened #10288
</comment><comment author="s1monw" created="2015-05-28T16:01:36Z" id="106444511">@mikemccand can we close this?
</comment><comment author="mikemccand" created="2015-05-28T17:00:14Z" id="106489593">Yes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OOM causes index corruption on replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10066</link><project id="" key="" /><description>v 1.4.3

Our cluster started doing very heavy garbage collection, probably due to an expensive query.
In the process of this, all nodes in the cluster OOM'd.
After restarting the entire cluster, we found that a number of replicas were giving errors when trying to bring shards up. This is the log from the master during recovery:

```
[2015-03-11 22:19:39,311][WARN ][gateway.local            ] [logdbmaster04] [logstash-2015.03.01][1]: failed to list shard stores on node [3-sHwrfRQ4O-GEiloWFyRA]
org.elasticsearch.action.FailedNodeException: Failed node [3-sHwrfRQ4O-GEiloWFyRA]
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:185)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:175)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.transport.RemoteTransportException: [logdb26-2][inet[/10.x.x.x:9301]][internal:cluster/nodes/indices/shard/store[n]]
Caused by: org.elasticsearch.ElasticsearchException: Failed to list store metadata for shard [[logstash-2015.03.01][1]]
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:143)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:62)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.lucene.index.CorruptIndexException: codec footer mismatch: actual footer=-1593622416 vs expected footer=-1071082520 (resource: SimpleFSIndexInput(path="/data/elasticsearch/elasticsearch/nodes/1/indices/logstash-2015.03.01/1/index/_1fq7_es090_0.doc"))
        at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
        at org.apache.lucene.codecs.CodecUtil.retrieveChecksum(CodecUtil.java:228)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:717)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:613)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:596)
        at org.elasticsearch.index.store.Store.readMetadataSnapshot(Store.java:317)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:189)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:141)
        ... 7 more
```

When the node OOM'd, this is the stack trace we saw:

```
[2015-03-11 21:53:19,001][WARN ][transport.netty          ] [logdb26-2] exception caught on transport layer [[id: 0xef397851, /10.x.x.x:51786 =&gt; /10.x.x.x:9300]], closing connection
java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.common.compress.BufferRecycler.allocDecodeBuffer(BufferRecycler.java:137)
        at org.elasticsearch.common.compress.lzf.LZFCompressedStreamInput.&lt;init&gt;(LZFCompressedStreamInput.java:46)
        at org.elasticsearch.common.compress.lzf.LZFCompressor.streamInput(LZFCompressor.java:121)
        at org.elasticsearch.common.io.stream.CachedStreamInput.cachedHandlesCompressed(CachedStreamInput.java:69)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:104)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
[2015-03-11 21:57:38,951][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
```

My understanding is that when an OOM happens, Elasticsearch shouldn't be writing out data to a live file.
I think there was a resiliency improvement in 1.4.0 which only switched in files to the live index once they were fully written? This knowledge was gleaned from https://github.com/elastic/elasticsearch/issues/8707.
The interesting thing here is that we optimise our indices nightly, and this happened on 241 out of ~3250 shards. 

The cluster is currently recovering all replica shards, which is good (recovery works!) but also not great (the replica shards should not have been corrupt).
We also optimize 
</description><key id="60741500">10066</key><summary>OOM causes index corruption on replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-03-11T22:33:53Z</created><updated>2015-12-05T20:22:50Z</updated><resolved>2015-12-05T20:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T15:15:09Z" id="89593605">@bleskes any thoughts on this?
</comment><comment author="bleskes" created="2015-04-07T20:25:28Z" id="90720942">It's hard to say, but with 1.4.0, if a recovery was cancelled half way (due to OOM or what ever) and one was unlucky, you could end up with half baked files files and thus corrupted shard.  This was fixed in 1.5.0.

I'm not sure what we can do now to support / disprove this theory. If there is no other information I will go with a broken recovery and close it.
</comment><comment author="nguafogu78" created="2015-05-20T09:03:24Z" id="103816863">Hi,

A similar error happened to me for a 1.3.4 ES version. It seems too there is a broken recovery for one shard (among 5 for the same index). In the #8707 they suggest it could be resolved by renaming the shard directory. The 'unassigned' replica would become primary so data could be accessible again and a new replica created. Can someone confirm that.

bleskes : do you suggest the #8707 error would not happen in a 1.5.0 version ?
</comment><comment author="bleskes" created="2015-05-20T11:41:47Z" id="103854103">@nguafogu78 1.5 fixes that type of corruption when it happens due to a recovery/relocation which is abruptly cut. Renaming the dir will help _if_ you have a good other copy. 
</comment><comment author="clintongormley" created="2015-12-05T20:22:50Z" id="162243239">This should be fixed now. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Interoperability of /_query and /_search with GET/POST/DELETE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10065</link><project id="" key="" /><description>Hi there,

I've got an issue where /_query POST creates something, yet you need to use /_query to DELETE from a Query DSL search.

It would be great to have it so /_query is interoperable with /_search, unless I'm missing something in the documentation it would make sense for this.

I'm proposing that /_search becomes an alias to /_query (eventually removed) and then you can do the folllowing:

GET: To perform a search relating to terms in URI
POST: To perform a search using the Query DSL provided in the RAW data
DELETE: To perform removal of documents matching the Query DSL provided in the RAW data.
</description><key id="60735296">10065</key><summary>Interoperability of /_query and /_search with GET/POST/DELETE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewbroadley</reporter><labels /><created>2015-03-11T21:46:05Z</created><updated>2015-04-04T15:13:21Z</updated><resolved>2015-04-04T15:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T15:13:21Z" id="89593098">Hi @drewbroadley 

GET/POST _search both allow searches with or without body. Either of these can contain queries, post-filters, aggs, suggestions etc.  DELETE _query is delete-by-query, and all it accepts is a query, nothing else.

We're trying to reduce "synonyms" in the API, so I'm not keen to add more options - i think that would just add to the confusion.

Most people will be using one of the official clients, which I think are a bit clearer, eg:

```
$es-&gt;search( body =&gt; { query =&gt; {....}})
$es-&gt;delete_by_query( body =&gt; { query =&gt; {...}})
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting Issues generating compound _id from fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10064</link><project id="" key="" /><description>Hi,

I am trying to generating compound _id from _source fields.

curl -XPUT http://localhost:9200/_template/my_tplt -d '{
"template" : "my*",
"order":    1,
"settings" : {
"index.cache.field.type" : "soft",
"index.refresh_interval" : "5s",
"index.store.compress.stored" : true,
"mapping.allow_type_wrapper": true
},
"mappings" : {
"_default_" : {
"_all" : {"enabled" : false},
"_timestamp" : {"enabled" : true,"path" : "my_time","format" : "YYYY-MM-dd HH:mm:ss"},
"_ttl" : { "enabled" : true, "default" : "30d"},
"transform" : {
"script_file" : "gen_id"
},
"properties" : {
"my_time": {
"type": "date",
"format": "YYYY-MM-dd HH:mm:ss"
},
"my_roll" : {"type": "string", "index" : "not_analyzed" },
"my_name" : {"type": "string", "index" : "not_analyzed" },
"my_address":{"type":"string","index" : "not_analyzed" },
"my_message" : {"type": "string", "index" : "not_analyzed"}
}
}
}
}
'

File gen_id.groovy in \elastic_home\config\scripts\ 
contains the following line
ctx._source['_id']=ctx._source['my_roll']+ctx._source['my_name'];

input file contains 
{"index":{"_index":"my_index","_type":"test"}}
{"my_time":"2015-03-10 17:04:45","my_name":"nithya","my_roll":"A0001","my_address":"somewhere here","my_message":"This is me"}

But I get the following 
errorMapperParsingException[Provided id [AUwKuP4ndtjQr1ZnsG_U] does not match the content one [A0001nithya] 

I have tried with 
File gen_id.groovy updated with this line
ctx._source['my_message']=ctx._source['my_roll']+ctx._source['my_name'];

But even my_message does not insert with new value my_message value. 

What am I missing here?
</description><key id="60734083">10064</key><summary>Scripting Issues generating compound _id from fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-03-11T21:37:15Z</created><updated>2015-03-19T15:24:16Z</updated><resolved>2015-03-19T15:24:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T15:24:15Z" id="83629345">I think this question would be better answered in the mailing list. That said, you cannot change the `_id` of a document through a script. I suppose that AUwKuP4ndtjQr1ZnsG_U is the auto generated id for your document, since you didn't specify one in the url, and A0001nithya the result of the script. 
Not sure about the last bit on the my_message field, maybe you can share some more details on the mailing list around it, that sounds like it should work.

In general, and especially if you want to modify your ids, I would pre-process your documents before sending them to elasticsearch to be honest, so that they have the right id from the beginning.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide stats on memory used in queues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10063</link><project id="" key="" /><description>Would be nice for node stats to also report the amount of memory used by items in a thread pool queue (esp. for the bulk thread pool with increased queue size).
</description><key id="60731544">10063</key><summary>Provide stats on memory used in queues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2015-03-11T21:19:57Z</created><updated>2015-04-04T15:07:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Data Corruption reported from ES but not Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10062</link><project id="" key="" /><description>After a rolling upgrade from 1.3.9 to 1.4.4 I'm left with ~270 shards in 206 (of 1082) indices that ES is reporting are corrupted for primaries and all replicas and seems unable to recover them, but the lucene CheckIndex tool can find no problem with the underlying data.

Here's a sample log message from ES:

[2015-03-11 13:40:45,047][WARN ][cluster.action.shard     ] [Murmur II] [logstash-2014.01.07][2] sending failed shard for [logstash-2014.01.07][2], node[vnlCCnbRQBiJFj-16Xvuyg], [P], s[INITIALIZING], indexUUID [eKh7osfjRfmqmE5Mbl2-Tw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.01.07][2] failed to fetch index version after copying it over]; nested: CorruptIndexException[[logstash-2014.01.07][2] Preexisting corrupted index [corrupted_JLDbR2yHQ9SVrwgY_LeOJQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=1dclij9 actual=1wjox1n resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1e8de440)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1dclij9 actual=1wjox1n resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1e8de440)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:393)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [Devastator][inet[/10.226.73.178:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=5546sn actual=15nanx1 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@75934f20)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:393)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
## ]; ]]

But Lucene finds no issue with the index:

$ /usr/lib/jvm/java-7-openjdk-amd64/bin/java -cp /usr/share/elasticsearch/lib/elasticsearch-0.90.13.jar:/usr/share/elasticsearch/lib/lucene-core-4.10.3.jar:/usr/share/elasticsearch/lib/\* -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /local/mnt/elasticsearch/bait/nodes/0/indices/logstash-2014.01.07/2/index

Opening index @ /local/mnt/elasticsearch/bait/nodes/0/indices/logstash-2014.01.07/2/index

Segments file=segments_4ir numSegments=6 versions=[4.6.0 .. 4.9.0] format= userData={translog_id=1389052807442}
  1 of 6: name=_tyc docCount=63647
    version=4.6.0
    codec=Lucene46
    compound=false
    numFiles=12
    size (MB)=19.953
    diagnostics = {timestamp=1389121243139, os=Linux, os.version=3.2.0-40-generic, mergeFactor=10, source=merge, lucene.version=4.6.0 1543363 - simon - 2013-11-19 11:05:50, os.arch=amd64, mergeMaxNumSegments=-1, java.version=1.6.0_26, java.vendor=Sun Microsystems Inc.}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [113 fields]
    test: field norms.........OK [2 fields]
    test: terms, freq, prox...OK [392120 terms; 4498364 terms/docs pairs; 340461 tokens]
    test: stored fields.......OK [127294 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  2 of 6: name=_11s4 docCount=17508
    version=4.7.0
    codec=Lucene46
    compound=false
    numFiles=14
    size (MB)=5.576
    diagnostics = {timestamp=1407350741636, os=Linux, os.version=3.2.0-40-generic, mergeFactor=10, source=merge, lucene.version=4.7.0 1570806 - simon - 2014-02-22 08:25:23, os.arch=amd64, mergeMaxNumSegments=-1, java.version=1.6.0_26, java.vendor=Sun Microsystems Inc.}
    has deletions [delGen=1]
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK [1 deleted docs]
    test: fields..............OK [119 fields]
    test: field norms.........OK [2 fields]
    test: terms, freq, prox...OK [118686 terms; 1248860 terms/docs pairs; 105214 tokens]
    test (ignoring deletes): terms, freq, prox...OK [118834 terms; 1249008 terms/docs pairs; 105214 tokens]
    test: stored fields.......OK [35014 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  3 of 6: name=_121s docCount=11
    version=4.7.0
    codec=Lucene46
    compound=true
    numFiles=3
    size (MB)=0.008
    diagnostics = {timestamp=1407971871508, os=Linux, os.version=3.2.0-40-generic, source=flush, lucene.version=4.7.0 1570806 - simon - 2014-02-22 08:25:23, os.arch=amd64, java.version=1.6.0_26, java.vendor=Sun Microsystems Inc.}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [17 fields]
    test: field norms.........OK [0 fields]
    test: terms, freq, prox...OK [243 terms; 660 terms/docs pairs; 0 tokens]
    test: stored fields.......OK [22 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  4 of 6: name=_16a8 docCount=30
    version=4.7.0
    codec=Lucene46
    compound=true
    numFiles=3
    size (MB)=0.012
    diagnostics = {timestamp=1422479687275, os=Linux, os.version=3.2.0-40-generic, source=flush, lucene.version=4.7.2 1586229 - rmuir - 2014-04-10 09:00:35, os.arch=amd64, java.version=1.7.0_15, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [16 fields]
    test: field norms.........OK [0 fields]
    test: terms, freq, prox...OK [365 terms; 1777 terms/docs pairs; 0 tokens]
    test: stored fields.......OK [60 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  5 of 6: name=_16a9 docCount=1
    version=4.7.0
    codec=Lucene46
    compound=true
    numFiles=3
    size (MB)=0.004
    diagnostics = {timestamp=1422479743815, os=Linux, os.version=3.2.0-40-generic, source=flush, lucene.version=4.7.2 1586229 - rmuir - 2014-04-10 09:00:35, os.arch=amd64, java.version=1.7.0_15, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [17 fields]
    test: field norms.........OK [0 fields]
    test: terms, freq, prox...OK [79 terms; 79 terms/docs pairs; 0 tokens]
    test: stored fields.......OK [2 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

  6 of 6: name=_16z7 docCount=14
    version=4.9.0
    codec=Lucene49
    compound=true
    numFiles=3
    size (MB)=0.012
    diagnostics = {timestamp=1426010857824, os=Linux, os.version=3.2.0-40-generic, source=flush, lucene.version=4.9.1 1625909 - mike - 2014-09-18 04:03:13, os.arch=amd64, java.version=1.7.0_15, java.vendor=Oracle Corporation}
    no deletions
    test: open reader.........OK
    test: check integrity.....OK
    test: check live docs.....OK
    test: fields..............OK [21 fields]
    test: field norms.........OK [1 fields]
    test: terms, freq, prox...OK [281 terms; 553 terms/docs pairs; 14 tokens]
    test: stored fields.......OK [28 total field count; avg 2 fields per doc]
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    test: docvalues...........OK [1 docvalues fields; 0 BINARY; 1 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]

No problems were detected with this index.
</description><key id="60719102">10062</key><summary>Data Corruption reported from ES but not Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">brunson</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-03-11T19:54:06Z</created><updated>2015-03-17T06:17:33Z</updated><resolved>2015-03-17T06:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-03-16T17:59:37Z" id="81844607">Is the data sensitive? The example shard you have here seems pretty small. If you can share the data files I can dig deeper into it.
</comment><comment author="s1monw" created="2015-03-16T18:46:23Z" id="81862663">@brunson if you are up for it can you maybe share the content of the `checksums-*` file in your index directory? or maybe upload them somewhere?
</comment><comment author="brunson" created="2015-03-16T19:54:54Z" id="81903097">Let me check on the contents of the index, I don't believe it's sensitive.

I've got 3 (of 6) nodes with a checksum file for that shard, two are the same size and one slightly smaller, they all have different shas.  Would seeing all 3 be helpful?
</comment><comment author="s1monw" created="2015-03-16T19:56:55Z" id="81904068">yeah please gimme all of them
</comment><comment author="s1monw" created="2015-03-16T19:57:31Z" id="81904348">and if possible can we get the entire shard as a zipfile?
</comment><comment author="brunson" created="2015-03-16T20:17:22Z" id="81914750">Definitely.  Can I give you a download link privately?
</comment><comment author="s1monw" created="2015-03-16T20:20:46Z" id="81916592">sure `simon` at `elastic` dot `co`
</comment><comment author="s1monw" created="2015-03-17T06:17:33Z" id="82134038">@brunson thanks so much for sending me all these information. I tried your shards on 1.5.0-snapshot and it recovered just fine. this is very likely to be caused by https://github.com/elastic/elasticsearch/issues/8587 so you will have a fix for this in the upcoming 1.5.0 release which should be very soon given that I branched today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for doc values for analyzed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10061</link><project id="" key="" /><description>We would like to use doc values for analyzed fields.

The doc mentions that it is not possible:
http://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html

&gt; Doc values can be enabled for numeric, date, Boolean, binary, and geo-point fields, and for not_analyzed string fields. They do not currently work with analyzed string fields.

Will that eventually be supported?
</description><key id="60711073">10061</key><summary>Add support for doc values for analyzed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Asimov4</reporter><labels><label>discuss</label></labels><created>2015-03-11T18:56:39Z</created><updated>2015-08-05T17:34:21Z</updated><resolved>2015-08-05T17:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T15:03:45Z" id="89591868">Hi @Asimov4 

We have been discussing it internally.  The biggest question is: what is the right datastructure? The answer depends on what people want to use the analyzed values for.

Could you talk about your use case(s)?
</comment><comment author="mattweber" created="2015-04-04T16:23:32Z" id="89609174">+1, case-insensitive sorting of string fields where you use the keyword tokenizer and lowercase token filter.
</comment><comment author="rmuir" created="2015-04-04T16:26:44Z" id="89609954">I tend to think @mattweber 's use case is an important one. But we should think of a way to solve it but where its not trappy if someone wants this. They should be able to have docvalues, but they should also not have norms and other things: it should still act 'not analyzed'. 

There is a big difference to me between supporting that, and putting the entire tokenized contents of moby dick into a docvalues field.
</comment><comment author="Asimov4" created="2015-04-04T17:34:20Z" id="89624360">The main use case I have in mind is performing top terms aggregations or significant terms aggregations to do keyword analysis on the text fields of the documents stored in our index. We use the [default language analyzers](http://www.elastic.co/guide/en/elasticsearch/reference/1.5/analysis-lang-analyzer.html) for these text fields.
</comment><comment author="mattweber" created="2015-04-04T18:30:29Z" id="89634026">Another use-case is ICU collation.  Really any kind of minimal analysis that produces a couple tokens at most. 
</comment><comment author="rmuir" created="2015-04-04T18:55:36Z" id="89639859">ICU collation IMO is different. its just a DV sort key: doesn't need tokens or an analyzer anymore, and thats how exposed in lucene since 4.2
</comment><comment author="reardencode" created="2015-04-05T17:30:00Z" id="89815767">:+1: for at least allowing doc_values on fields whose whose analyzer is guaranteed to only generate a single value.
</comment><comment author="rayward" created="2015-06-22T23:40:48Z" id="114303178">:+1: 

Have the exact same situation as @mattweber, I've got several sub-fields using an analyzer like that for sorting:

```
"lowercase_single_token" : {
  "filter": [
    "lowercase",
    "asciifolding"
  ],
  "tokenizer" : "keyword",
  "type": "custom"
}
```
</comment><comment author="passing" created="2015-08-05T14:33:10Z" id="128017316">hi @clintongormley 

in the context of log messages, we have a few cases where we use analyzers that generate always a single token:

_synonyms_
we have an analyzer that uses a filter to group severity values that mean the same thing, like "warn" and "warning"

```
                "filter" : {
                    "synonym_severity" : {
                        "type" : "synonym",
                        "expand" : "false",
                        "synonyms" : [
                            "crit, critical, alert, emerg, emergency, panic =&gt; fatal",
                            "err, severe =&gt; error",
                            "warning =&gt; warn",
                            "trace =&gt; debug"
                        ]
                    }
                }
```

_replacing/masking_
we have an analyzer that uses a char_filter to remove numbers from error messages, so we can aggregate on similar messages ("could not read attribute x for object 123" and "could not read attribute x for object 4567" are both transformed to "could not read attribute x for object #")

```
                "char_filter" : {
                    "clean_number" : {
                        "type" : "pattern_replace",
                        "pattern" : "[0-9,.]*[0-9]",
                        "replacement" : "#"
                    }
                }
```

_trim_
we have an analyzer that uses a tokenizer which returns only the first line of an error message. so we can aggregate on that..

```
                "tokenizer" : {
                    "string_head_tokenizer" : {
                        "type" : "pattern",
                        "flags" : "DOTALL",
                        "pattern" : "([^\\n]{1,512}).*",
                        "group" : "1"
                    }
                }
```

it would be very helpful to be able to use doc-values for these cases
</comment><comment author="clintongormley" created="2015-08-05T17:34:21Z" id="128085344">Hi @passing 

Thanks for the detailed comment.  We're intending to deprecate the `string` type and replace it with `text` and `keyword` types, where the keyword type will allow analysis (as long as the tokenizer is `keyword`)  and will support doc values.  See https://github.com/elastic/elasticsearch/issues/12394

For your trim example, you could change to using the pattern replace token filter and it would still work with the `keyword` field type. https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern_replace-tokenfilter.html

I'm going to close this issue in favour of #12394
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Page cache recycler using &gt;10% of heap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10060</link><project id="" key="" /><description>From talking to @jpountz , the page cache recycler is not supposed to use &gt; 10% of heap.  We are seeing it use more than 10% in one of the heap dumps received (ES 1.4.1).  Filing this ticket to track. Thx @jpountz !

![image](https://cloud.githubusercontent.com/assets/7216393/6603370/e918cc18-c7de-11e4-813a-e4565d822ab4.png)

![image](https://cloud.githubusercontent.com/assets/7216393/6603356/ddf4e88a-c7de-11e4-8ebc-5709096e23f9.png)
</description><key id="60703901">10060</key><summary>Page cache recycler using &gt;10% of heap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>bug</label></labels><created>2015-03-11T18:11:30Z</created><updated>2015-03-12T22:51:46Z</updated><resolved>2015-03-12T22:51:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-11T18:14:25Z" id="78333360">For the record, this is on a 30.1GB heap.
</comment><comment author="jpountz" created="2015-03-12T22:43:27Z" id="78668680">This looks related to #10077
</comment><comment author="jpountz" created="2015-03-12T22:51:46Z" id="78672779">I just verified #10077 . This is very likely the cause of this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sub aggregation of metric aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10059</link><project id="" key="" /><description>Is  it  possible to support sub  aggregation of  metric aggregation ?
For  example ,term agg&#8594;mean  agg &#8594;max agg
</description><key id="60675943">10059</key><summary>sub aggregation of metric aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zijian1981</reporter><labels /><created>2015-03-11T15:25:34Z</created><updated>2015-03-25T03:33:20Z</updated><resolved>2015-03-25T03:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-17T21:05:13Z" id="82600511">What would you expect the output of this to be? if you want to calculate the mean and max of a field for each terms bucket you can do the following:

```
{
  "aggs": {
    "terms_agg": {
      "terms": {
        "field": "category",
        "size": 10
      },
      "aggs": {
        "mean_agg": {
          "avg": {
            "field": "value"
          }
        },
        "max_agg": {
          "max": {
            "field": "value"
          }
        }
      }
    }
  }
}
```

Does this fit what you wanted to do?
</comment><comment author="zijian1981" created="2015-03-18T23:40:33Z" id="83225604">No,i want to calculate the max value from all the mean value
</comment><comment author="markwalkom" created="2015-03-18T23:42:21Z" id="83225816">You cannot do this at this stage. Once you get an aggregation result you can only sub-aggregate that, not do a different aggregation across those results.
</comment><comment author="colings86" created="2015-03-19T16:13:32Z" id="83646504">@zijian1981 there is an issue open for this kind of thing (https://github.com/elastic/elasticsearch/issues/10000) which is being considered as part of https://github.com/elastic/elasticsearch/issues/9876
</comment><comment author="colings86" created="2015-03-25T03:33:18Z" id="85813743">Closing in favour of #10000 and #9876
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filtered query w/ has_child query and filter applied to child documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10058</link><project id="" key="" /><description>Enable a filtered query to apply filter to both parent and child documents but return parent documents for has_child query.

e.g. 
curl -X PUT "http://localhost:9200/foobar" -d "{\"mappings\":{\"Foo\":{},\"Bar\":{\"_parent\":{\"type\":\"Foo\"}}}}"

curl -X PUT "http://localhost:9200/foobar/Foo/1" -d "{\"foo\":\"abc\",\"access\":\"yes\"}"

curl -X PUT "http://localhost:9200/foobar/Bar/2?parent=1" -d "{\"bar\":\"xyz\",\"access\":\"no\"}"

The following filtered query would not return a result, since the filter would be applied to both the parent and child documents; however, as implemented now, the parent document would be returned, since the filter is not applied to the child document.

{
   "query": {
      "filtered": {
         "query": {
            "has_child": {
               "type": "Bar",
               "query": {
                  "term": {
                     "bar": "xyz"
                  }
               }
            }
         },
         "filter": {
            "term": {
               "access": "yes"
            }
         }
      }
   }
}

Perhaps an additional filtered_query strategy could be specified which would apply the filter first to both parent and child documents.
</description><key id="60670782">10058</key><summary>filtered query w/ has_child query and filter applied to child documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asanderson</reporter><labels><label>:Parent/Child</label><label>discuss</label></labels><created>2015-03-11T14:54:31Z</created><updated>2017-03-21T15:06:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T14:55:34Z" id="89590655">Hi @asanderson 

Trying to extend the filtered query to say "also apply to this has-child subclause" doesn't make sense to me.  Things would suddenly get very complicated with action-at-a-distance.

If you want the condition to apply to the sub-clause, then you should include it in the sub-clause.  It'll still be able to use the cached filter.
</comment><comment author="asanderson" created="2015-04-04T15:58:55Z" id="89604699">Again, it makes sense in the very real scenario where you have a common access control field for which you want to guarantee a filter is applied to all parent and child records.  The filters are already huge, so having to wrap the has_child subclause into a filtered query and copying the huge filter from the outer filtered query is very ugly.
</comment><comment author="asanderson" created="2015-04-04T16:01:47Z" id="89604815">So, good luck w/ implementing a common access control field strategy w/ Shield that applies to both parent and child documents, if you leave this closed.
</comment><comment author="clintongormley" created="2015-04-05T19:23:15Z" id="89835865">You raise a good point...  How to completely exclude documents from all queries that match some filter.  Reopening for discussion
</comment><comment author="clintongormley" created="2015-04-05T19:23:38Z" id="89836034">@uboness this may be of interest to you
</comment><comment author="asanderson" created="2015-04-06T20:30:54Z" id="90230779">Currently, it's a pain to have to rewrite an end-user's query to wrap all their queries and filters w/ a common access control filter.  There should be an easier way to do this.  From an API user perspective, the ideal solution would be to specify top-level pre_filter similarly to how easier it is to specify a post_filter. If agreed, then feel free to close this again and open up a new feature to support this pre_filter concept.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Data separation and access control - collect logs of multiple tenants</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10057</link><project id="" key="" /><description>Please don't point me at the new product 'shield' as is not open-source :(

If I will use ELK stack to collect logs from different tenants(example: all websites/web application hosted by me)
How can I provide my 'clients' the possibility to check and query their logs without seeing the others?
How can I assure that is not possible to a user to see the logs not related to his applications?
Should I have to find a way to have this data-separation on elasticsearch, kibana or both?

On a MySQL Server I would just use different DB and users but here I am a bit new about that project.

Is the only way do deploy an ELK stack for each 'client'? 

Thank you
</description><key id="60636817">10057</key><summary>Data separation and access control - collect logs of multiple tenants</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tyyko</reporter><labels /><created>2015-03-11T10:19:33Z</created><updated>2015-04-04T14:44:46Z</updated><resolved>2015-04-04T14:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T14:44:46Z" id="89590151">Hi @tyyko 

You should probably ask for suggestions about approaches to this on the mailing list instead.  Like with MySQL, you could use a separate index per client, then in your web front end, you could limit each user to just their index or indices.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Bug] Unexpected behavior when indexing boolean values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10056</link><project id="" key="" /><description>There is inconsistent behavior when saving boolean json values as boolean, or as text. The occurring parse error is cryptic and confusing. 

Repro steps (assumming the `tweets` index doesn't exist):
1. Insert a first record:

```
~ $ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/1' -d '{ "key" : 123123123 }'
{"_index":"tweets","_type":"tweet","_id":"1","_version":2,"created":false}
```
1. Insert a boolean as a string

```
~ $ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/2' -d '{ "key" : "true" }'
{"error":"MapperParsingException[failed to parse [key]]; nested: NumberFormatException[For input string: \"true\"]; ","status":400}
```

The error is expected, as auto-mapping has decided that `key` is a `number`, not a `boolean`
1. Insert a boolean as boolean

```
~ $ curl -XPUT 'http://127.0.0.1:9200/tweets/tweet/3' -d '{ "key" : true }'
{"error":"MapperParsingException[failed to parse [key]]; nested: JsonParseException[Current token (VALUE_TRUE) not numeric, can not use numeric value accessors\n at [Source: [B@464291b8; line: 1, column: 15]]; ","status":400}
```

The expected result was supposed to be same as in 2). However, there is a cryptic message instead (which I personally don't understand). Is this a bug, or expected behavior for reasons still unknown to me? (disclaimer: I am new to ElasticSearch)
</description><key id="60628784">10056</key><summary>[Bug] Unexpected behavior when indexing boolean values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">Chetane</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-03-11T09:07:41Z</created><updated>2015-07-09T19:20:57Z</updated><resolved>2015-07-09T19:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-07-09T19:20:56Z" id="120115820">I think the excepiton is just fine since we are throwing them on a different level. We expect numers or strings that we can parse. If you specify a boolean it fails on the json parser rather than when we try to interpret the number it's fails way earlier.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regarding TTL Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10055</link><project id="" key="" /><description>I am getting an Error as below while configuring ELK.Caused by: org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [logstash-\* : {_ttl={enabled=true, default=500}}] [_default_ : {dynamic_templates=[{string_fields={mapping={index=analyzed, omit_norms=true, type=string, fields={raw={index=not_analyzed, ignore_above=256, type=string}}}, match_mapping_type=string, match=*}}], properties={geoip={dynamic=true, path=full, properties={location={type=geo_point}}, type=object}, @version={index=not_analyzed, type=string}}, _all={enabled=true}}]
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)

My default configuration is :

{
"logstash-*" : {
    "_ttl" : { "enabled" : true, "default" : "500" }
}
}

I also tried the below configuration but it fails:

{
"template" : "logstash-*",
    "mappings" : {
        "_default_" : {
            "_ttl" : { "enabled" : true, "default" : "5s"  }
        }
}
}

Can anyone please suggest what could be the issue?

Thanks
</description><key id="60622049">10055</key><summary>Regarding TTL Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Sameerkpatro</reporter><labels><label>feedback_needed</label></labels><created>2015-03-11T07:50:04Z</created><updated>2015-04-26T19:57:25Z</updated><resolved>2015-04-26T19:57:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T14:40:09Z" id="89589979">Hi @Sameerkpatro 

How are you configuring this?  The below works fine for me:

```
PUT _template/test
{
  "template": "logstash-*",
  "mappings": {
    "_default_": {
      "_ttl": {
        "enabled": true,
        "default": "5s"
      }
    }
  }
}

PUT /logstash-one/test/1
{}

GET logstash-one/_mapping
```

Btw, using TTL for logging is likely a very bad approach, as older documents need to be physically deleted.  Much better to just drop old indices.
</comment><comment author="clintongormley" created="2015-04-26T19:57:24Z" id="96428597">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleted index comes back</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10054</link><project id="" key="" /><description>Reproduction:
1. Create two nodes cluster, node1 and node2 (both are master eligible)
2. Create an index
3. Stop node2
4. Delete the index
5. Stop node1
6. Start node1 and node2
7. Deleted index comes back
</description><key id="60598471">10054</key><summary>Deleted index comes back</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels /><created>2015-03-11T01:52:43Z</created><updated>2016-02-23T02:22:20Z</updated><resolved>2015-03-20T01:29:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-03-11T03:03:46Z" id="78191807">This is 1.4.4 according to @masaruh 
</comment><comment author="masaruh" created="2015-03-11T03:26:51Z" id="78196769">Forgot to mention it's not dangling index.

It can be reproduced by:
1. Create three nodes cluster, node1 and node2 are master eligible and node3 is data only node.
2. Create index
3. Stop node2
4. Delete the index
5. Stop node1 and node3
6. Start node1, node2 and node3
7. Deleted index comes back

It looks old cluster state on node2 is imported on restart during `LocalGateway.performStateRecovery`.
It looks indices found in any `nodeState` are getting recovered. I wonder if it should recover only indices in `electedGlobalState`.
</comment><comment author="brwe" created="2015-03-18T22:50:58Z" id="83216980">This is actually expected behavior. When a master eligible node joins a cluster then all information is has about indices from it's last cluster state are imported as well. This is a safety measure to make sure that a master with an empty cluster state (for example because the data folder of a master eligible node was deleted and it becomes master after full cluster restart) does not cause all indices to be deleted. We are even changing that now to work in a similar way for data only nodes, see https://github.com/elastic/elasticsearch/issues/8823
</comment><comment author="masaruh" created="2015-03-20T01:29:11Z" id="83841471">Thanks @brwe. Makes sense.
</comment><comment author="bleskes" created="2015-03-20T09:47:45Z" id="83970321">A common failure here is people forgetting to configure min_master_nodes or recover_after, do a full cluster restart (or what ever) while adding new nodes to the cluster. If one of the new nodes gets elected as master before the older nodes join, it will not wait for them and iniitialize an empty cluster state (thinking it&#8217;s alone in a fresh cluster). We need this protection to make sure that we don&#8217;t delete all data once the old nodes join.

&gt; On 18 Mar 2015, at 23:51, Britta Weber notifications@github.com wrote:
&gt; 
&gt; This is actually expected behavior. When a master eligible node joins a cluster then all information is has about indices from it's last cluster state are imported as well. This is a safety measure to make sure that a master with an empty cluster state (for example because the data folder of a master eligible node was deleted and it becomes master after full cluster restart) does not cause all indices to be deleted. We are even changing that now to work in a similar way for data only nodes, see #8823
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery should wipe the shard state file before starting recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10053</link><project id="" key="" /><description>When we start recovery of a shard we should wipe the state file of the copy if it's present otherwise gateway allocating can get confused interpreting a shard that is not fully recovered ie. due to a recovery failure as a valid copy since we only write the state when the shard is started.
</description><key id="60588435">10053</key><summary>Recovery should wipe the shard state file before starting recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2015-03-10T23:46:15Z</created><updated>2015-03-20T15:43:02Z</updated><resolved>2015-03-20T15:43:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-10T23:47:47Z" id="78173487">@brwe can you take care of this?
</comment><comment author="bleskes" created="2015-03-10T23:52:33Z" id="78174017">I wonder if the correct time to wipe any _state file is before the temp file rename. Until then, the recovery doesn&#8217;t mess with any non-temp files. If the recover is cancelled, we leave the target shard intact.

&gt; On 10 Mar 2015, at 16:48, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; @brwe can you take care of this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2015-03-15T21:26:43Z" id="81247565">@bleskes agreed.. we should remove it before we rename the first file.
</comment><comment author="brwe" created="2015-03-17T20:58:57Z" id="82597946">Just for reference, here is the relevant test failure: http://build-us-00.elasticsearch.org/job/es_core_1x_small/1800/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch crashed with same error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10052</link><project id="" key="" /><description>Hey , 

First of all this is the error : 

[2015-03-10 14:59:24,165][WARN ][indices.breaker          ] [log1-01] [FIELDDATA] New used memory 64378163272 [59.9gb] from field [@timestamp] would be larger than configured breaker: 64356483072 [59.9gb], breaking
[2015-03-10 14:59:24,166][DEBUG][action.search.type       ] [log1-01] [logstash-2015.03.09][0], node[Tm4cqA_iQHOrY-rMhPR7Dg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7533f62d]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2015.03.09][0]: query[ConstantScore(_:_)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:193)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:171)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.ElasticsearchException: org.elasticsearch.common.breaker.CircuitBreakingException: [FIELDDATA] Data too large, data for [@timestamp] would be larger than limit of [64356483072/59.9gb]
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
    at org.elasticsearch.search.facet.datehistogram.ValueDateHistogramFacetExecutor$Collector.setNextReader(ValueDateHistogramFacetExecutor.java:91)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:67)
    at org.apache.lucene.search.MultiCollector.setNextReader(MultiCollector.java:113)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 8 more
Caused by: org.elasticsearch.common.util.concurrent.UncheckedExecutionException: org.elasticsearch.common.breaker.CircuitBreakingException: [FIELDDATA] Data too large, data for [@timestamp] would be larger than limit of [64356483072/59.9gb]
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:174)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:74)
    ... 15 more
Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [FIELDDATA] Data too large, data for [@timestamp] would be larger than limit of [64356483072/59.9gb]
    at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.circuitBreak(ChildMemoryCircuitBreaker.java:97)
    at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:148)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.flush(RamAccountingTermsEnum.java:71)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:85)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:472)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:109)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:49)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:187)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:174)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 19 more

Server details : 

Memory : 125GB ( Used - 54 GB , Free - 71 GB ) 
CPU :  16 

Can you please advice to me what is the issue ? what should i do ?

Thanks , 
Amit Daniel
</description><key id="60518520">10052</key><summary>Elasticsearch crashed with same error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amitdaniel86</reporter><labels /><created>2015-03-10T15:33:10Z</created><updated>2015-09-08T12:02:20Z</updated><resolved>2015-07-22T09:44:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-10T17:17:04Z" id="78101729">@amitdaniel86 this exception is not crashing Elasticsearch, this exception means that Elasticsearch prevented you from loading too much data into memory and causing your server to go out of memory, you can see from this line:

Caused by: org.elasticsearch.ElasticsearchException: org.elasticsearch.common.breaker.CircuitBreakingException: [FIELDDATA] Data too large, data for [@timestamp] would be larger than limit of [64356483072/59.9gb]

This means that the `@timestamp` field is trying to load more than 60gb of memory. You will need to either limit the fielddata size (see: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#index-modules-fielddata) or you will need to switch to doc_values for the timestamp field: https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html#doc-values
</comment><comment author="amitdaniel86" created="2015-03-11T09:13:55Z" id="78228042">Can you help me to understand why the @timestamp field trying to load more than 60GB of data ? 
Maybe i have another issue to solve.

Thanks for your help !!! @dakrone 
</comment><comment author="dakrone" created="2015-03-11T16:33:46Z" id="78300579">@amitdaniel86 field data is loaded for a field when that field is used for facets, aggregations, or when sorting on the field, so it could be any (or all) of those three.
</comment><comment author="clintongormley" created="2015-07-22T09:44:06Z" id="123644524">Looks like there is nothing to do here. Closing
</comment><comment author="bolee" created="2015-07-31T08:29:13Z" id="126607430">How to switch doc_values,for every filed?
</comment><comment author="mksk3tch" created="2015-09-08T10:47:19Z" id="138514036">Switching to doc_values for every field is probably a bad idea. Looking at the documentation linked above I can see that doc_values will remove some load from the JVM heap however will increase disk I/O. I think the wise choice here is to find a good balance between field data and doc value.
</comment><comment author="clintongormley" created="2015-09-08T11:22:43Z" id="138522340">@majormashup sure doc values will take more I/O, but that is much better than stressing the heap.  doc values on all fields (except analyzed string fields) is the default in 2.0. This is to provide the best experience out of the box.

Of course, if you have fields that you never plan on using for sorting, aggs, or in scripts, then you can disable doc values for them and save I/O and disk space.
</comment><comment author="mksk3tch" created="2015-09-08T12:02:19Z" id="138532644">@clintongormley - thanks very much. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score on ChildrenQuery execute diffrent in master version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10051</link><project id="" key="" /><description>Hi, I am testing new master version of elasticsearch and have one question.

I am using some statistics (store in usagestatistics object) which is child object of lets say document object. I implemented sort by most user used document by function score on child query. In a version 1.4.x it is ok, on master version i got error on some shard where i dont have childs (error below). It is any option to ignore this.

error is

```
{
  "index": "develop_compositedata_0",
  "shard": 3,
  "status": 500,
  "reason": "QueryPhaseExecutionException[[develop_compositedata_0][3]: query[+companyId:[88 TO 88]^0.0 +deleted:F^0.0 +((typeId:[2 TO 2] typeId:[4 TO 4]^0.08 typeId:[32 TO 32]^0.02 typeId:[1 TO 1]^0.3 typeId:[2048 TO 2048]^0.0 typeId:[65536 TO 65536]^0.0)~1) +((authorizationTokens:noauth^0.0 authorizationTokens:noauthsubject^0.0 authorizationTokens:111^0.0 authorizationTokens:89^0.0 authorizationTokens:187^0.0 authorizationTokens:214^0.0 authorizationTokens:217^0.0 authorizationTokens:10244^0.0 authorizationTokens:10257^0.0 authorizationTokens:161^0.0)~1) +ConstantScore(+stringFields.dynstring_30303030303030302d303030302d303030302d303030302d3030303030303030303030302f47656e6572616c2f4e616d65.value:mi*)^0.01 +((main:[0 TO 0]^0.0 main:[-1 TO -1]^0.0)~1) +blocked:F^0.0 ChildrenQuery[min(0) max(2147483647)of usagestatistics/compositedata](filtered(function score (+userId:[111 TO 111]^0.0,function=script[doc['usagestatistics.searchBoost'].value], params [null]))-&gt;cache(_type:usagestatistics))],from[0],size[7],sort[&lt;score&gt;,&lt;custom:\"sortName\": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@2c1574e4&gt;!]: Query Failed [Failed to execute main query]]; nested: RuntimeException[org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [usagestatistics.searchBoost] in mapping with types []]]; nested: GroovyScriptExecutionException[ElasticsearchIllegalArgumentException[No field found for [usagestatistics.searchBoost] in mapping with types []]]; "
}
```

and query:

```
{
  "from": 0,
  "size": 7,
  "sort": [
    {
      "_score": {
        "missing": "_last",
        "order": "desc"
      }
    },
    {
      "sortName": {
        "missing": "_last",
        "order": "desc"
      }
    }
  ],
  "fields": [
    "id",
    "compositeId",
    "name",
    "typeId",
    "external",
    "blocked",
    "deleted",
    "serviceId",
    "processInstanceId",
    "serviceInstanceId",
    "ownerServiceId",
    "ownerProcessInstanceId",
    "main",
    "modified"
  ],
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "companyId": {
              "value": 88,
              "boost": 0
            }
          }
        },
        {
          "term": {
            "deleted": {
              "value": false,
              "boost": 0
            }
          }
        },
        {
          "bool": {
            "should": [
              {
                "term": {
                  "typeId": {
                    "value": 2,
                    "boost": 1
                  }
                }
              },
              {
                "term": {
                  "typeId": {
                    "value": 4,
                    "boost": 0.07999999821186066
                  }
                }
              },
              {
                "term": {
                  "typeId": {
                    "value": 32,
                    "boost": 0.019999999552965164
                  }
                }
              },
              {
                "term": {
                  "typeId": {
                    "value": 1,
                    "boost": 0.30000001192092896
                  }
                }
              },
              {
                "term": {
                  "typeId": {
                    "value": 2048,
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "typeId": {
                    "value": 65536,
                    "boost": 0
                  }
                }
              }
            ],
            "minimum_should_match": "1"
          }
        },
        {
          "bool": {
            "should": [
              {
                "term": {
                  "authorizationTokens": {
                    "value": "noauth",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "noauthsubject",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "111",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "89",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "187",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "214",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "217",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "10244",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "10257",
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "authorizationTokens": {
                    "value": "161",
                    "boost": 0
                  }
                }
              }
            ],
            "minimum_should_match": "1"
          }
        },
        {
          "constant_score": {
            "query": {
              "bool": {
                "must": [
                  {
                    "query_string": {
                      "query": "mi*",
                      "fields": [
                        "stringFields.dynstring_30303030303030302d303030302d303030302d303030302d3030303030303030303030302f47656e6572616c2f4e616d65.value"
                      ],
                      "default_operator": "and",
                      "analyzer": "standard-4office",
                      "allow_leading_wildcard": false,
                      "analyze_wildcard": true,
                      "minimum_should_match": "100%",
                      "use_dis_max": false
                    }
                  }
                ]
              }
            },
            "boost": 0.009999999776482582
          }
        },
        {
          "bool": {
            "should": [
              {
                "term": {
                  "main": {
                    "value": 0,
                    "boost": 0
                  }
                }
              },
              {
                "term": {
                  "main": {
                    "value": -1,
                    "boost": 0
                  }
                }
              }
            ],
            "minimum_should_match": "1"
          }
        },
        {
          "term": {
            "blocked": {
              "value": false,
              "boost": 0
            }
          }
        }
      ],
      "should": [
        {
          "has_child": {
            "type": "usagestatistics",
            "score_type": "sum",
            "query": {
              "function_score": {
                "query": {
                  "bool": {
                    "must": [
                      {
                        "term": {
                          "userId": {
                            "value": 111,
                            "boost": 0
                          }
                        }
                      }
                    ]
                  }
                },
                "score_mode": "sum",
                "boost_mode": "max",
                "script_score": {
                  "script": "doc['usagestatistics.searchBoost'].value"
                }
              }
            }
          }
        }
      ],
      "minimum_should_match": "0"
    }
  }
}
```
</description><key id="60506309">10051</key><summary>Function score on ChildrenQuery execute diffrent in master version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukapor</reporter><labels /><created>2015-03-10T14:20:14Z</created><updated>2015-04-04T14:11:03Z</updated><resolved>2015-04-04T14:11:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T14:11:03Z" id="89584631">Hi @lukapor 

I think you're running into this change: https://github.com/elastic/elasticsearch/commit/6079d88d433387933d55d6500e6aaa04d6a6b1ce

You can no longer prepend the field name with the type name, as it results in ambiguous field lookups.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch fails when kibana makes request over https</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10050</link><project id="" key="" /><description>Apache2 has SSL module on to make connections to localhost over https

Kibana config:

```
elasticsearch: "https://localhost:9200",
```

ElasticSearch config:

```
http.cors.enabled: true
http.cors.allow-origin: /https?:\/\/localhost/

```

When we access kibana over https, we get all types of exceptions:

```
Caught exception while handling client http traffic, closing connection [id: 0x18a54cf5, /10.127.153.212:55525 =&gt; /173.39.242.2:9200]
java.lang.IllegalArgumentException: empty text
```

```
Caught exception while handling client http traffic, closing connection [id: 0xa5767d95, /10.127.153.212:55489 :&gt; /173.39.242.2:9200]
java.lang.IllegalArgumentException: invalid version format: &#65469;&#65413;F&#65461;&#65430;;&#65427;&#65459;&#65515;&#65472;
```

Any help much appreciated. 
</description><key id="60481460">10050</key><summary>ElasticSearch fails when kibana makes request over https</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">91pavan</reporter><labels /><created>2015-03-10T10:52:36Z</created><updated>2016-04-20T17:28:26Z</updated><resolved>2015-03-10T16:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="91pavan" created="2015-03-10T16:13:00Z" id="78087610">Sorry guys for opening the issue, managed to fix it by replacing the localhost part with actual IP of the machine!

Closing this one!
</comment><comment author="sureshrv83" created="2016-04-20T17:28:26Z" id="212525181">I have added below to elastic search config , still get the same exceptions.
http.cors.enabled: true
http.cors.allow-origin: /https?:\/\/localhost/

However http://localhost:9200 works but https://localhost:9200 doesn't work!

Can you please help ? Is there config that I'm missing in elasticsearch ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature][function_score] Execute a function on scores in score_mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10049</link><project id="" key="" /><description>Currently `score_mode` only supports a basic set of operations on scores provided by the functions executed in function score query: `multiply`, `sum`, `avg`, `first`, `max`, `min`.

It will be very useful to have the ability to execute a more involved function on them instead, e.g.: `_score1 + _score3 - _score2*_score5`, where scores can be numbered or names after the functions order / names.
</description><key id="60480498">10049</key><summary>[Feature][function_score] Execute a function on scores in score_mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Search</label><label>discuss</label><label>feature</label></labels><created>2015-03-10T10:43:48Z</created><updated>2016-11-26T19:26:35Z</updated><resolved>2016-11-26T19:20:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T14:18:55Z" id="249878794">Related to https://github.com/elastic/elasticsearch/pull/19710
</comment><comment author="clintongormley" created="2016-11-26T19:20:00Z" id="263080770">Closing in favour of #19710</comment><comment author="synhershko" created="2016-11-26T19:26:35Z" id="263081087">Thanks @clintongormley @dakrone - as long as that solution allows for sibling scores to be passed as parameters to that script, that would work.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature] Reciprocal decay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10048</link><project id="" key="" /><description>Support for a reciprocal decay function will be highly useful for one use-case I'm helping with:

(1/x)^c where c is a constant and x is the parameter to the decay function (datetime field etc).

/cc @brwe 
</description><key id="60479601">10048</key><summary>[Feature] Reciprocal decay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>discuss</label><label>feature</label></labels><created>2015-03-10T10:35:50Z</created><updated>2016-09-27T14:20:10Z</updated><resolved>2016-09-27T14:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-03-26T14:44:58Z" id="86545368">What would the score be as c approaches 0? We added exp, gauss and linear because like this we can at least make sure the score is between 0 and 1 always. What is your usecase?
I cannot promise there will be any resources soon to implement this but it should be really easy to implement it as a plugin like so: https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/search/functionscore/FunctionScorePluginTests.java
</comment><comment author="ofir-reich" created="2015-03-27T11:37:13Z" id="86907596">You mean where x approaches 0. It would be best to add a constant b, so that it's (x+b)^c, or (abs(x)+b)^c to handle negative cases.
But to be more exact, what we need is a minus logarithmic decay:
Score = -log( abs(x)+b ),
where x is the difference between the query term and the document.
Our use case is for time decay. We want the time difference between documents to affect the result in this way, so that more recent documents are given a higher score. The reason we can't use any of the other decay functions is that we use a probabilistic interpretation for the IDF and we want it to be consistent all through our search to get the exact results we want. We've found that the probability that a document is relevant to our query decays as a power law: probability ~ 1/(delta_t+b) . (more precisely, the odds ratio, but never mind).
Since IDF is (close to) the log of a probability of a random document to match the query, we take the exponent of the IDF (the resulting score). We want to add to the probability the power law decay, but since scores are additive and we take an exponent of the result in the end, we want a log( abs(delta_t)+b) function for the score so that when we take the exponent to obtain the probability it would become 1/(delta_t+b).
I hope this was clear. I'm open to other clarifications if needed.
Thank you!

p.s. notice that in the probabilistic interpretation, a decay function of (-x) for the score would correspond, when taking the exponent, to an exponential decay in probability, which also might be useful for many use-cases. It doesn't make much sense to decay the score itself as en exponent.
</comment><comment author="brwe" created="2015-04-01T14:36:20Z" id="88507939">I think I understand the usecase but I am not convinced that this is a very common one so I am leaning against adding this scoring function to core. 
What keeps you from implementing this in a script (potentially native) or as a plugin to decay functions?
</comment><comment author="synhershko" created="2015-04-01T14:39:43Z" id="88508824">Maintenance, mostly. We could do that, but the internal APIs are not stable and aren't going to be for a while. I've been maintaining various plugins for over 2 years now.

We think this could be very useful for many others as well, hence this proposals. We do hope it will make it in, and will gladly provide a complete PR for it.
</comment><comment author="clintongormley" created="2015-12-05T20:14:06Z" id="162242807">The maths was above me, but it looks like this could be implemented easily with a (very fast) `expression` script, no? Plus the `weight` parameter allows for tuning between functions.

Any reason to keep this open?
</comment><comment author="nik9000" created="2015-12-07T19:24:05Z" id="162631624">&gt; Any reason to keep this open?

I think expression scripts are likely to be good for this. The API there is pretty stable.

If we think the shape of the decay is useful it might be worth adding. If I'm reading it right they are talking about something like `1 + log(b) - log( a * abs(now - doc['time']) +b )`

This creates something that starts at 0 and decays quickly at first but more and more slowly as time goes on. And, if my very very very rusty calculus doesn't fail me, it will eventually become negative so we'd want to clamp it at 0.
</comment><comment author="dakrone" created="2016-09-27T14:20:10Z" id="249879176">This is possible with the new Painless scripting language, so I am closing this since it's been almost a year.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature] Query template modularity (or, re-using template queries in template queries)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10047</link><project id="" key="" /><description>It would be super useful to have the ability to reuse query templates from other query templates, e.g.

```
POST /_search/template/title_search
{
    "template": {
        "query": {
            "match": {
                "title": "{{query_string}}"
            }
        }
    }
}

POST /_search/template/article_search
{
    "template": {
        "query": {
            "bool":{
             "should":[
                "match": {
                   "content": "{{query_string}}"
                },
                "template_id":"title_search"
          ]
         }
        }
    }
}
```

It will then be the user's responsibility to make sure one change to a template doesn't break other templates, and that required params are all satisfied across all used templates.

In particular I see this being asked for over and over again by various clients, as their queries are growing bigger and bigger beyond what they could maintain easily.
</description><key id="60478666">10047</key><summary>[Feature] Query template modularity (or, re-using template queries in template queries)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Search Templates</label><label>discuss</label></labels><created>2015-03-10T10:28:15Z</created><updated>2017-03-30T14:07:26Z</updated><resolved>2015-12-05T20:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T20:07:23Z" id="162242460">This works already:

```
PUT t/t/1
{
  "title": "foo"
}

PUT t/t/2
{
  "title": "bar"
}

POST /_search/template/title_search
{
  "template": {
    "query": {
      "match": {
        "title": "{{foo}}"
      }
    }
  }
}

POST /_search/template/article_search
{
  "template": {
    "query": {
      "template": {
        "id": "title_search",
        "params": {
          "foo": "{{bar}}"
        }
      }
    }
  }
}


GET _search
{
  "query": {
    "template": {
      "id": "article_search",
      "params": {
        "bar": "foo"
      }
    }
  }
}
```
</comment><comment author="mbystryantsev" created="2017-03-29T15:19:05Z" id="290123763">&gt;This works already

@clintongormley, in elasticsearch 5.0+ `Template Query` marked as deprecated. How to reuse templates now?</comment><comment author="clintongormley" created="2017-03-30T14:07:26Z" id="290421507">@mbystryantsev we won't be supporting template embedding going forward</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Engine: update index buffer size during recovery and allow configuring version map size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10046</link><project id="" key="" /><description>To support real time gets, the engine keeps an in-memory map of recently index docs and their location in the translog. This is needed until documents become visible in Lucene. With 1.3.0, we have improved this map and made tightly integrated with refresh cycles in Lucene in order to keep the memory signature to a bare minimum. On top of that, if the version map grows above a 25% of the index buffer size, we proactively refresh in order to be able to trim the version map back to 0 (see #6363) . In the same release, we have fixed an issue where an update to the indexing buffer could result in an unwanted exception during recovery (#6667) . We solved this by waiting with updating the size of the index buffer until the shard was fully recovered. Sadly this two together can have a negative impact on the speed of translog recovery.

During the second phase of recovery we replay all operations that happened on the shard during the first phase of copying files. In parallel we start indexing new documents into the new created shard. At some point (phase 3 in the recovery), the translog replay starts to send operation which have already been indexed into the shard. The version map is crucial in being able to quickly detect this and skip the replayed operations, without hitting lucene. Sadly #6667 (only updating the index memory buffer once shard is started) means that a shard is using the default 64MB for it's index buffer, and thus only 16MB (25%) for the version map. This much less then the default index buffer size 10% of machine memory (shared between shards).

Since we don't flush anymore when updating the memory buffer, we can remove #6667 and update recovering shards as well. Also, we make the version map max size configurable, with the same default of 25% of the current index buffer.
</description><key id="60449937">10046</key><summary>Engine: update index buffer size during recovery and allow configuring version map size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-10T04:35:07Z</created><updated>2015-03-19T10:16:39Z</updated><resolved>2015-03-15T22:15:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-10T12:35:28Z" id="78044541">This is a nice catch @bleskes!

Do we increase the indexing buffer of the target during phase 2 or 3 of recovery?  If not, I'm missing how these issues might impact recovery...
</comment><comment author="bleskes" created="2015-03-13T14:31:36Z" id="78997622">@mikemccand thx. I pushed another commit.
</comment><comment author="mikemccand" created="2015-03-13T15:28:11Z" id="79046335">LGTM
</comment><comment author="bleskes" created="2015-03-13T17:24:44Z" id="79158103">@s1monw pushed another commit
</comment><comment author="s1monw" created="2015-03-13T17:33:58Z" id="79167342">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10045</link><project id="" key="" /><description>FWIW I've signed the CLA but not sure how to update the status.
</description><key id="60438097">10045</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">petrbela</reporter><labels><label>docs</label><label>v1.3.10</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-10T01:37:09Z</created><updated>2015-03-12T22:30:13Z</updated><resolved>2015-03-12T22:30:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-12T22:30:13Z" id="78659756">Thanks @petrbela merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add alias filter to slowlog entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10044</link><project id="" key="" /><description>When using filtered aliases filtered by type, it will be helpful for slowlog to report the types queried.   Currently, it returns `types[]`.

```
POST /_aliases
{
  "actions": [
    {
      "add": {
        "index": "index_name",
        "alias": "slowlogalias",
        "filter": {
          "term": {
            "_type": "doc_type"
          }
        }
      }
    }
  ]
}
```
</description><key id="60429893">10044</key><summary>Add alias filter to slowlog entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-09T23:54:36Z</created><updated>2015-04-05T13:31:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T13:32:48Z" id="89578375">Hi @ppf2 

I don't think we should single out the doc types, but I agree that we should include the full filter that is being applied thanks to the alias.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support min_children &amp; max_children for nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10043</link><project id="" key="" /><description>I am opening this as a separate issue since the previous issue was closed with support for parent-child docs (https://github.com/elasticsearch/elasticsearch/issues/6019#issuecomment-77785163). 

We would love to have support for min_children &amp; max_children or similar also for nested filters/docs. http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-has-child-filter.html#_min_max_children_2

Thanks a keep up the great work.
</description><key id="60369836">10043</key><summary>Support min_children &amp; max_children for nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsuchal</reporter><labels><label>:Nested Docs</label><label>adoptme</label><label>feature</label><label>stalled</label></labels><created>2015-03-09T16:25:56Z</created><updated>2017-05-10T12:54:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-09T18:04:52Z" id="77908524">+1 we should add this! I think we should also open an issue for this in Lucene, because the `nested` query uses the `ToParentBlockJoinQuery` Lucene query to do the actual work.
</comment><comment author="martijnvg" created="2015-03-09T18:49:50Z" id="77917487">I opened: https://issues.apache.org/jira/browse/LUCENE-6354 to get this in Lucene
</comment><comment author="gmenegatti" created="2015-03-24T21:25:50Z" id="85699054">+1
</comment><comment author="kmcs" created="2015-04-16T12:07:54Z" id="93718293">+1
</comment><comment author="lonre" created="2016-09-19T08:26:58Z" id="247936067">+1
</comment><comment author="guilherme-santos" created="2017-01-10T15:35:13Z" id="271607299">+1</comment><comment author="asuiu" created="2017-04-11T20:41:48Z" id="293393837">+1</comment><comment author="conan" created="2017-05-10T12:54:40Z" id="300472899">+1</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery: add total operations to the `_recovery` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10042</link><project id="" key="" /><description>This commit adds the current total number of translog operations to the recovery reporting API.  We also expose the recovered / total percentage:

```
"translog": {
     "recovered": 536,
     "total": 986,
     "percent": "54.3%",
     "total_time": "2ms",
     "total_time_in_millis": 2
},
```

Closes #9368
</description><key id="60346947">10042</key><summary>Recovery: add total operations to the `_recovery` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-09T14:01:58Z</created><updated>2015-03-19T11:05:12Z</updated><resolved>2015-03-17T14:43:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-03-13T15:43:19Z" id="79059594">LGTM
</comment><comment author="kimchy" created="2015-03-13T16:57:30Z" id="79131378">@bleskes something minor that I saw, I think the percent should be a number, so it can be plotted for example, so: `percent: 54.3`, it would be nice not to call it percent, but something more descriptive
</comment><comment author="bleskes" created="2015-03-13T17:11:30Z" id="79145096">@kimchy yeah, I followed the existing pattern but it always bugged me. I'll change it. Double checking we're OK with breaking bwc here? I want this changed to go in 1.5. /cc @s1monw 
</comment><comment author="bleskes" created="2015-03-16T18:06:32Z" id="81847538">@kimchy I discussed the percentage formatting with @s1monw . Plan is to get this is in as is and change the percentage formatting in another PR for 2.0 only.
</comment><comment author="bleskes" created="2015-03-16T18:15:25Z" id="81850361">@s1monw pushed another round
</comment><comment author="s1monw" created="2015-03-16T18:22:39Z" id="81853234">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog: make sure stats's op count and size are in sync</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10041</link><project id="" key="" /><description>During translog flush there is a very small window where translog stats reports can be inconsistent.

Example:

```
"translog" : {
   "operations" : 37415948,
   "size_in_bytes" : 0
},
```
</description><key id="60342135">10041</key><summary>Translog: make sure stats's op count and size are in sync</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-09T13:24:03Z</created><updated>2015-03-19T11:05:31Z</updated><resolved>2015-03-13T17:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-10T18:00:46Z" id="78111116">LGTM
</comment><comment author="s1monw" created="2015-03-10T23:16:32Z" id="78169331">left one comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update monitoring.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10040</link><project id="" key="" /><description>Hope you don't mind to help a little bit to tell people about the product...
</description><key id="60339396">10040</key><summary>Update monitoring.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">popovae</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-09T13:00:07Z</created><updated>2015-03-21T09:18:31Z</updated><resolved>2015-03-21T09:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T08:40:09Z" id="84282885">Hi @popovae could you please sign our [CLA](https://www.elastic.co/contributor-agreement) ?
</comment><comment author="popovae" created="2015-03-21T08:51:02Z" id="84283153">Hi @javanna 
CLA is signed)
</comment><comment author="javanna" created="2015-03-21T09:12:51Z" id="84284153">Thanks @popovae merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update client.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10039</link><project id="" key="" /><description>change a typo.
</description><key id="60324751">10039</key><summary>Update client.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gingerhot</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-03-09T10:36:47Z</created><updated>2015-04-04T13:21:44Z</updated><resolved>2015-04-04T13:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T08:40:48Z" id="84282901">Hi @gingerhot thanks, could you please sign our [CLA](https://www.elastic.co/contributor-agreement) so we can merge this in?
</comment><comment author="clintongormley" created="2015-04-04T13:21:33Z" id="89576812">thanks @gingerhot - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to create Filtered Alias on empty index with template mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10038</link><project id="" key="" /><description>Hi,

ES 1.4.x gives the error `[repro] Strict field resolution and no field mapping can be found for the field with name [testfield]` when I try to create an alias on an empty Index that has template mappings.
This works fine in ES 1.3.x.

I see in the guide (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html#filtered) that in 1.4.0.beta1 there was added a clause stating that the field has to exist in the mapping, but it does not mention anything about mapping in templates - so I'm not sure if this is a documentation bug or and ES bug.

Repro case:

```
curl -XPUT 'http://localhost:9200/_template/repro' -d '{
  "template": "repro",
  "settings": {
    "index.number_of_shards": 1,
    "index.number_of_replicas": 0
  },
  "mappings": {
    "_default_": {
      "properties": {
        "testfield": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}'
```

```
curl -XPOST 'http://localhost:9200/repro'
```

```
curl -XPOST  'http://localhost:9200/_aliases' -d '{
   "actions": [
      {
         "add": {
            "index": "repro",
            "alias": "testalias",
            "filter": {
               "term": {
                  "testfield": "filtervalue"
               }
            }
         }
      }
   ]
}'
```
</description><key id="60310803">10038</key><summary>Failure to create Filtered Alias on empty index with template mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andersosthus</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2015-03-09T08:15:02Z</created><updated>2015-12-05T19:46:36Z</updated><resolved>2015-12-05T19:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shivangshah" created="2015-03-11T16:17:15Z" id="78296983">+1. I have the exact same problem. I have more details here: 
http://stackoverflow.com/questions/28977610/elastic-search-filtered-aliases-with-unmapped-fields
</comment><comment author="javanna" created="2015-03-17T06:24:10Z" id="82140727">Templates per se are fine, this happens because the `_default_` type doesn't hold actual mappings, but mappings that will get applied whenever a new type is created. A new type is created either when you issue a put mapping or when you index the first document under a new type.

There are a couple of workarounds for this: index a dummy document that contains the field that you want to filter on before you create the alias, or replace the `_default_` mapping in the index template with an explicit type (or add the explicit mapping that contains the field definition to the create index call).

This is unfortunate, but I tend not to see it as a bug. Thoughts?
</comment><comment author="andersosthus" created="2015-03-17T07:04:52Z" id="82164571">Hi,

Yeah, I did that workaround when I encountered this issue, and it's no problem, at least not for us. But the documentation should then maybe clarify this explicitly, especially since this is a change in behaviour from 1.3.x, since I think a lot of people, including myself, thought that the `_default` mapping in templates worked like regular mappings :)
</comment><comment author="shivangshah" created="2015-03-17T14:20:44Z" id="82379100">I would like to add here that even if _default is not considered an actual mapping and is applied when a document is indexed/a new mapping is explicitly put, it would still make sense for a filtered alias to also be able to look up the same mapping from _default, especially when in most cases you create a filtered alias when you already know your data (just like you put mappings in _default because you already know your data and thus some default mapping). From my perspective, they both kind of go hand in hand. For both filtered alias &amp; _default, you know your mappings (and filter in case of filtered alias) and that's why you want to define them BEFORE indexing any data. The workaround is tested and is working, but it'd be good to have an actual fix and/or change in documentation.
</comment><comment author="javanna" created="2015-03-17T16:06:09Z" id="82436261">@shivangshah it's subtle, but the `_default_` mapping is just a fallback mechanism to avoid repetitions throughout different types. When the concrete mapping gets created (e.g. when creating the index), you could have a field with same name but a different date type, which takes precedence over the default one. That affects how the filter gets parsed so that it properly works depending on the data type. That is why we have to wait till the concrete mapping is defined and we cannot parse the filter depending on the default mapping, so we make sure that the filter is properly parsed. The workaround here is to use the concrete type(s) rather than the `_default_` one. Makes sense?
</comment><comment author="shivangshah" created="2015-03-17T16:22:28Z" id="82444121">@javanna Actually yes. I see your point now. A pre-defined filtered alias would be a problem if the concrete mapping of a specific type is different than the default. 
So now here's another question. Assume that people who know their data and want "default pre-defined filtered aliases" based off of the default mappings, is that a feasible feature request? Would it make sense to have something like this. The usecase should be quite common for people using one common index for multiple tenants and want predefined filtered aliases based on default mapping of a specific field (in most cases, the "tenantId" field or some other "id" field that uniquely identifies tenant specific data). 
</comment><comment author="javanna" created="2015-03-17T17:27:58Z" id="82489404">I am marking this issue for discussion. I do see your point @shivangshah , I am just not sure how we can get there. The feature you are requesting turns around the `_default_` mapping as we would need to  lock down the mappings for those fields and make sure that the parsed filter is always consistent with the actual mapping for the field. That would mean that instead of a fallback mechanism we would need to be able to force certain mappings for all of the types.
</comment><comment author="andersosthus" created="2015-03-17T17:31:07Z" id="82490251">@javanna Btw, what changed in 1.4 that changed this, since it worked in 1.3 ?
</comment><comment author="javanna" created="2015-03-17T17:34:04Z" id="82490892">Good question @andersosthus ! Nothing changed around the default mappings. We made alias filters parsing stricter, have a look at #6664 . The reason for it is that you really want to make sure that the the alias filter gets properly parsed based on the mapped data type.
</comment><comment author="andersosthus" created="2015-03-17T17:36:59Z" id="82491923">Ok, thanks @javanna. That explains why this "worked" in 1.3 :)
</comment><comment author="shivangshah" created="2015-03-17T23:09:23Z" id="82634985">@javanna : How about make it more of a configurable flag just like many other configurations handled by the clients ?
</comment><comment author="javanna" created="2015-03-18T13:42:53Z" id="82977240">I don't think this can be just a flag since it's a completely different feature compared to the current behaviour...
</comment><comment author="clintongormley" created="2015-04-04T15:40:44Z" id="89600849">&gt; I am marking this issue for discussion. I do see your point @shivangshah , I am just not sure how we can get there. The feature you are requesting turns around the _default_ mapping as we would need to lock down the mappings for those fields and make sure that the parsed filter is always consistent with the actual mapping for the field. That would mean that instead of a fallback mechanism we would need to be able to force certain mappings for all of the types.

Given the work happening in #8871 to have fields of the same name in different types always have the same mapping, perhaps this is something we can support after all?
</comment><comment author="javanna" created="2015-04-07T16:19:52Z" id="90629519">yea, good point @clintongormley that makes sense. Or if we go for parsing filters at search time instead, we could turn off strict query parsing I believe: https://github.com/elastic/elasticsearch/issues/10135#issuecomment-89579164 .
</comment><comment author="javanna" created="2015-04-08T13:16:06Z" id="90911618">we now have an issue for parsing filters at search time, see #10485.
</comment><comment author="clintongormley" created="2015-12-05T19:46:36Z" id="162240613">Alias filters are now parsed at search time, so this works again
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable queryNorm function in search client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10037</link><project id="" key="" /><description>diasble coord as lucene funtion
            public float coord(int overlap, int maxOverlap) {
                return 1;
            }
es can set as BoolQueryBuilder bqb = QueryBuilders.boolQuery();
               bqb.disableCoord(true);
can ES has diasble  queryNorm as lucene function
            public float queryNorm(float sumOfSquaredWeights) {
                return 1;
            }
in search client with java?
</description><key id="60308903">10037</key><summary>Disable queryNorm function in search client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gucasbrg</reporter><labels /><created>2015-03-09T07:48:17Z</created><updated>2015-04-04T13:15:30Z</updated><resolved>2015-04-04T13:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T13:15:30Z" id="89575375">Hi @gucasbrg 

No, we have no control over query norm.  It is the same for every result anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es count With kibana4 hits quantity is different&#65311;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10036</link><project id="" key="" /><description>HI guys
curl localhost:9200/nginx-2015.0301-08*/_count

{"count":3463956,"_shards":{"total":5,"successful":5,"failed":0}}

![image](https://cloud.githubusercontent.com/assets/3368758/6550621/9d5c017e-c666-11e4-86ff-2450110a8c99.png)

ES return count is 3463956,but kibana4 is 2,947,387 hits,why?
</description><key id="60303319">10036</key><summary>es count With kibana4 hits quantity is different&#65311;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wujiandong</reporter><labels /><created>2015-03-09T06:17:08Z</created><updated>2015-04-04T13:14:19Z</updated><resolved>2015-04-04T13:14:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T13:14:19Z" id="89575176">Hi @wujiandong 

I'd ask this on the Kibana list instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES startup failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10035</link><project id="" key="" /><description>Hello all,

We are running ES 1.4.2 on SLES 11 SP2 with Graylog2, today we found ES log show the error information as below:

[2015-03-09 07:16:14,655][WARN ][cluster.routing.allocation.decider] [Hulk] high disk watermark [10%] exceeded on [bXcrFjuTRjayaGF4ZK7hpg][Hulk] free: 742mb[6.9%], shards will be relocated away from this node
[2015-03-09 07:16:14,655][INFO ][cluster.routing.allocation.decider] [Hulk] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-03-09 07:16:44,652][WARN ][cluster.routing.allocation.decider] [Hulk] high disk watermark [10%] exceeded on [bXcrFjuTRjayaGF4ZK7hpg][Hulk] free: 742mb[6.9%], shards will be relocated away from this node

then we extended the filesystem then restart the ES but failed, the log as below:

[2015-03-09 13:45:37,411][INFO ][node                     ] [Black Widow] version[1.4.2], pid[32291], build[927caff/2014-12-16T14:11:12Z]
[2015-03-09 13:45:37,412][INFO ][node                     ] [Black Widow] initializing ...
[2015-03-09 13:45:37,416][INFO ][plugins                  ] [Black Widow] loaded [], sites []
[2015-03-09 13:45:40,213][INFO ][node                     ] [Black Widow] initialized
[2015-03-09 13:45:40,213][INFO ][node                     ] [Black Widow] starting ...
[2015-03-09 13:45:40,332][INFO ][transport                ] [Black Widow] bound_address {inet[/153.95.192.128:9300]}, publish_address {inet[/153.95.192.128:9300]}
[2015-03-09 13:45:40,343][INFO ][discovery                ] [Black Widow] graylog2/8O5XeTbST0unN3cIRilDag
[2015-03-09 13:45:43,361][INFO ][cluster.service          ] [Black Widow] new_master [Black Widow][8O5XeTbST0unN3cIRilDag][fitl05v128][inet[/153.95.192.128:9300]], reason: zen-disco-join (elected_as_master)
[2015-03-09 13:45:43,405][INFO ][http                     ] [Black Widow] bound_address {inet[/153.95.192.128:9200]}, publish_address {inet[/153.95.192.128:9200]}
[2015-03-09 13:45:43,405][INFO ][node                     ] [Black Widow] started
[2015-03-09 13:45:44,196][INFO ][gateway                  ] [Black Widow] recovered [21] indices into cluster_state

the ES cluster status and health status

{
  "status" : 200,
  "name" : "Black Widow",
  "cluster_name" : "graylog2",
  "version" : {
    "number" : "1.4.2",
    "build_hash" : "927caff6f05403e936c20bf4529f144f0c89fd8c",
    "build_timestamp" : "2014-12-16T14:11:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"
  },
  "tagline" : "You Know, for Search"
}

{
  "cluster_name" : "graylog2",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 80,
  "active_shards" : 80,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 4
}

we only adapted the following parameter for ES:

discovery.zen.ping.multicast.enabled: false
cluster.name: graylog2
path.conf: /etc/elasticsearch
path.data: /usr/share/elasticsearch
path.work: /tmp/elasticsearch
path.logs: /usr/share/elasticsearch
network.host: 153.95.192.128
transport.tcp.port: 9300
http.port: 9200

please kindly help to solve it, thanks.

B.R
Lance
</description><key id="60303271">10035</key><summary>ES startup failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaman478</reporter><labels /><created>2015-03-09T06:16:07Z</created><updated>2015-04-04T13:20:21Z</updated><resolved>2015-04-04T13:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zerocoolys" created="2015-03-09T07:05:11Z" id="77808652">hi, there are 4 unassigned shards, what is your index configuration? how many shards ,how many repl?
what is your index status of these 4 shards?
</comment><comment author="shaman478" created="2015-03-09T07:13:58Z" id="77809243">@zerocoolys 

thanks for your reply, the index configuration as below, we are keeping  the default parameter so there are don't taken effect

#index.number_of_shards: 5
#index.number_of_shards: 1
#index.number_of_replicas: 0

and could you please kindly tell us how to check the index status? thanks a lot

B.R
Lance
</comment><comment author="shaman478" created="2015-03-09T07:56:10Z" id="77812569">Hello we found one index on "red" status, is ok or not?

http://153.95.192.128:9200/_cat/indices?v

health status index         pri rep docs.count docs.deleted store.size pri.store.size 
green  open   graylog2_2128   4   0       1015            0    366.2kb        366.2kb 
green  open   graylog2_2140   4   0       1009            0    351.3kb        351.3kb 
green  open   graylog2_2123   4   0       1007            0    357.5kb        357.5kb 
green  open   graylog2_2125   4   0       1017            0    335.2kb        335.2kb 
green  open   graylog2_2124   4   0       1075            0    327.2kb        327.2kb 
green  open   graylog2_2129   4   0       1003            0    349.4kb        349.4kb 
green  open   graylog2_2141   4   0   20000109            0      3.4gb          3.4gb 
green  open   graylog2_2131   4   0       1011            0      350kb          350kb 
green  open   graylog2_2137   4   0       1013            0    353.9kb        353.9kb 
green  open   graylog2_2126   4   0       1006            0      372kb          372kb 
red    open   graylog2_2142   4   0  
green  open   graylog2_2122   4   0       1042            0    353.8kb        353.8kb 
green  open   graylog2_2133   4   0       1033            0    346.4kb        346.4kb 
green  open   graylog2_2134   4   0       1017            0    318.5kb        318.5kb 
green  open   graylog2_2139   4   0       1014            0      327kb          327kb 
green  open   graylog2_2138   4   0       1033            0    304.1kb        304.1kb 
green  open   graylog2_2132   4   0       1001            0    351.4kb        351.4kb 
green  open   graylog2_2127   4   0       1058            0    349.5kb        349.5kb 
green  open   graylog2_2130   4   0       1011            0    360.7kb        360.7kb 
green  open   graylog2_2136   4   0       1001            0    358.6kb        358.6kb 
green  open   graylog2_2135   4   0       1007            0    356.8kb        356.8kb
</comment><comment author="zerocoolys" created="2015-03-09T08:35:40Z" id="77816312">"red" is not ok , your "red" index's primary shards are not recovery ok.
have you check the es logs? any output?
</comment><comment author="shaman478" created="2015-03-09T08:40:24Z" id="77816811">@zerocoolys 

we only checked the startup log which as below, no further information, does have other log can be check? thanks a lot.

[2015-03-09 15:34:24,866][INFO ][node                     ] [Flubber] version[1.4.2], pid[1428], build[927caff/2014-12-16T14:11:12Z]
[2015-03-09 15:34:24,866][INFO ][node                     ] [Flubber] initializing ...
[2015-03-09 15:34:24,873][INFO ][plugins                  ] [Flubber] loaded [], sites []
[2015-03-09 15:34:27,932][INFO ][node                     ] [Flubber] initialized
[2015-03-09 15:34:27,932][INFO ][node                     ] [Flubber] starting ...
[2015-03-09 15:34:28,104][INFO ][transport                ] [Flubber] bound_address {inet[/153.95.192.128:9300]}, publish_address {inet[/153.95.192.128:9300]}
[2015-03-09 15:34:28,114][INFO ][discovery                ] [Flubber] graylog2/WqAduEF5StO_DtmqoAuuHQ
[2015-03-09 15:34:31,139][INFO ][cluster.service          ] [Flubber] new_master [Flubber][WqAduEF5StO_DtmqoAuuHQ][fitl05v128][inet[/153.95.192.128:9300]], reason: zen-disco-join (elected_as_master)
[2015-03-09 15:34:31,222][INFO ][http                     ] [Flubber] bound_address {inet[/153.95.192.128:9200]}, publish_address {inet[/153.95.192.128:9200]}
[2015-03-09 15:34:31,223][INFO ][node                     ] [Flubber] started
[2015-03-09 15:34:32,366][INFO ][gateway                  ] [Flubber] recovered [21] indices into cluster_state

B.R
Lance
</comment><comment author="zerocoolys" created="2015-03-09T08:46:00Z" id="77817431">@shaman478 

please show me the graylog2_2142 index status, 3ks.

# 

BTW,

![image](https://cloud.githubusercontent.com/assets/2757581/6551990/9703bb18-c67b-11e4-8611-69cfd05bf857.png)

default shards is 5, but why 4? does pri means "primary"? 
</comment><comment author="shaman478" created="2015-03-09T08:54:27Z" id="77818389">@zerocoolys 

Index graylog2_2142 status as below:
{"cluster_name":"graylog2","status":"red","timed_out":false,"number_of_nodes":1,"number_of_data_nodes":1,"active_primary_shards":0,"active_shards":0,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":4}

as you know we don't used the default shards setting on ES,  Graylog2 configuration parameter defined the "default shards" is 4
</comment><comment author="zerocoolys" created="2015-03-09T09:45:47Z" id="77824598">@shaman478 

change the log level to debug and check the graylog2_2142 index log.

try to close this index and reopen it. Have you check the index files , could be broken?
</comment><comment author="shaman478" created="2015-03-09T09:57:42Z" id="77826183">@zerocoolys 

I don't know the index is works fine or not, many thanks.
root@fitl05v128:/usr/share/elasticsearch/bin &gt; curl -XPOST 'http://153.95.192.128:9200/graylog2_2142/_open'
{"acknowledged":true}

root@fitl05v128:/usr/share/elasticsearch/bin &gt; curl -XPOST 'http://153.95.192.128:9200/graylog2_2142/_close'
{"acknowledged":true}

root@fitl05v128:/usr/share/elasticsearch/graylog2/nodes/0/indices/graylog2_2142/_state &gt; strings state-4
graylog2_2142
version
stateCopen
settings
index.number_of_replicas@0
index.analysis.analyzer.analyzer_keyword.tokenizerFkeyword
index.number_of_shards@4
index.analysis.analyzer.analyzer_keyword.filterHlowercase
index.version.createdF1040299
index.creation_dateL1425495965346
index.uuidUi1NtOkV2ScWTdpBbbMpuNQ
mappings
{"message":{"dynamic_templates":
[{"store_generic@&amp;
mapping@
index":"not_analyzed"}, "
tch
*"}}],"_ttl@.
enabl !
:true &amp;
_sourc`{
compr
properti@
{"full_
typ 6
andar
_keywo
ti l
tamp@
d!D
for!
-MM-dd HH:mm:ss.SSS!$
aliases
</comment><comment author="shaman478" created="2015-03-09T10:10:18Z" id="77827751">@zerocoolys 

We just checked the index status after we open &amp; close this problem index, looks like it's works!

but graylog can't connect to ES as well..

ERROR: Could not successfully connect to Elasticsearch, if you use multicast check that it is working in your network and that Elasticsearch is running properly and is reachable. Also check that the cluster.name setting is correct.

health status index         pri rep docs.count docs.deleted store.size pri.store.size 
green  open   graylog2_2128   4   0       1015            0    366.2kb        366.2kb 
green  open   graylog2_2140   4   0       1009            0    351.3kb        351.3kb 
green  open   graylog2_2123   4   0       1007            0    357.5kb        357.5kb 
green  open   graylog2_2125   4   0       1017            0    335.2kb        335.2kb 
green  open   graylog2_2124   4   0       1075            0    327.2kb        327.2kb 
green  open   graylog2_2129   4   0       1003            0    349.4kb        349.4kb 
green  open   graylog2_2141   4   0   20000109            0      3.4gb          3.4gb 
green  open   graylog2_2131   4   0       1011            0      350kb          350kb 
green  open   graylog2_2137   4   0       1013            0    353.9kb        353.9kb 
green  open   graylog2_2126   4   0       1006            0      372kb          372kb 
       close  graylog2_2142  
green  open   graylog2_2122   4   0       1042            0    353.8kb        353.8kb 
green  open   graylog2_2133   4   0       1033            0    346.4kb        346.4kb 
green  open   graylog2_2134   4   0       1017            0    318.5kb        318.5kb 
green  open   graylog2_2139   4   0       1014            0      327kb          327kb 
green  open   graylog2_2138   4   0       1033            0    304.1kb        304.1kb 
green  open   graylog2_2132   4   0       1001            0    351.4kb        351.4kb 
green  open   graylog2_2127   4   0       1058            0    349.5kb        349.5kb 
green  open   graylog2_2130   4   0       1011            0    360.7kb        360.7kb 
green  open   graylog2_2136   4   0       1001            0    358.6kb        358.6kb 
green  open   graylog2_2135   4   0       1007            0    356.8kb        356.8kb

{"cluster_name":"graylog2","status":"green","timed_out":false,"number_of_nodes":1,"number_of_data_nodes":1,"active_primary_shards":0,"active_shards":0,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":0}
</comment><comment author="zerocoolys" created="2015-03-09T10:25:29Z" id="77829604">@shaman478 

for cluster status, if you close the problem index, it will ok  :)

for graylog2, I am not familiar with it. Does it use Zen multicast discovery es nodes ?

But if you can access ELK through http, so I think es could be OK for connection , might check the network.
</comment><comment author="clintongormley" created="2015-04-04T13:20:21Z" id="89576563">The right place for these discussions is the mailing list, rather than here.  @zerocoolys thanks for helping.  I'm going to close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>keyword statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10034</link><project id="" key="" /><description>Please how can I collect top 10 keywords,for example
request 
1. apple
2. apple
3. footbal
4. computer

the frist keyword is apple 
</description><key id="60295582">10034</key><summary>keyword statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">songwie</reporter><labels /><created>2015-03-09T03:58:33Z</created><updated>2015-04-04T13:13:06Z</updated><resolved>2015-04-04T13:13:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T13:13:06Z" id="89575117">Hi @songwie 

Please ask questions like these on the mailing list.  The GitHub issues list is for bug reports and feature requests.  That said, you are looking for the terms aggregation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>File scripts cache key to include language and prevent conflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10033</link><project id="" key="" /><description>The file scripts cache key should include the language of the script to prevent conflicts between scripts with same name but different extension (hence lang). Note that script engines can register multiple acronyms that may be used as lang at execution time (e.g. javascript and js are synonyms). We then need to make sure that the same script gets loaded no matter which of the acronyms is used at execution time. This problem didn't exist before this change as the lang was simply ignored, while now we take it into account.

This change has also some positive effect on inline scripts caching. Up until now, the same script referred to with different acronyms would be compiled and cached multiple times, once per acronym. After this change every script gets compiled and cached only once, as we chose internally the acronym used as part of the cache key, no matter which one the user provides.

Also made sure that the same engine is closed only once.
</description><key id="60281370">10033</key><summary>File scripts cache key to include language and prevent conflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-08T22:58:25Z</created><updated>2015-06-08T00:24:30Z</updated><resolved>2015-03-13T06:24:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-10T14:46:50Z" id="78068409">hey @s1monw @dakrone can either of you have a look at this please ;)
</comment><comment author="javanna" created="2015-03-11T00:00:31Z" id="78174982">Pushed a new commit, can you have a look @s1monw please?
</comment><comment author="s1monw" created="2015-03-12T20:00:29Z" id="78593075">left one comment other than that LGTM
</comment><comment author="javanna" created="2015-03-12T22:24:07Z" id="78656570">@s1monw I made `ScriptEngineService` extend `Closeable` and moved to `IOUtils#close`. This doesn't break existing plugins though as as the signature of the `close` method doesn't change.
</comment><comment author="s1monw" created="2015-03-13T00:20:34Z" id="78710930">left a tiny comment LGTM otherwise 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to seal an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10032</link><project id="" key="" /><description>In a lot of use cases indices are used for indexing only for a limited amount of time. Ie. in the daily index use case indices are created with a highish number of shards to scale out indexing and then after a couple of days these indices are idle in terms of writing. Yet, we still keep all the resources open since we are accepting writes at any time. This not necessary in a lot of cases and would allow for a large amount of optimizations:
- we can close IndexWriter and all it's resources.
- primary promotions are very simple since no changes are coming in
- indices might be able to be merged ie. if you have 7 indices for per week we can allow people to 
  merge them into a weekly index to reduce the number of indices.
- query caches can rely on the fact that the index doesn't change and we can pull out min/max values for numeric fields with more confidence that it's worth the effort.
- maybe most important, recoveries are instant. We would issue a special commit when we seal that ensures that all changes have been written containing some kind of identifier that we can use in recovery which will prevent the entire copying of files in situations of full cluster restarts etc. It will reduce the restart times dramatically with very low complexity for all the time-based indices cases which usually suffer from slow cluster restarts due to the large number of indices and shards 
</description><key id="60273146">10032</key><summary>Allow to seal an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label></labels><created>2015-03-08T19:58:21Z</created><updated>2015-05-26T14:04:51Z</updated><resolved>2015-05-26T13:59:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-08T20:00:45Z" id="77770815">++, I think the improved recovery part idea is a killer. I wonder if we can re-use one way or another the read only flag we have today on an index.
</comment><comment author="bleskes" created="2015-03-08T20:11:55Z" id="77771390">nice
</comment><comment author="s1monw" created="2015-03-08T20:25:29Z" id="77772107">@kimchy I think we can. To me it's a 2 staged process, first we switch the index read only using the flag we have and once we are there and the cluster state has been published we seal the index which causes the actual optimizations to happen. so I think we can just reuse it?
</comment><comment author="kimchy" created="2015-03-08T20:46:51Z" id="77773162">@s1monw ++, exactly what I was thinking about, and we have the flag recorded in the cluster state, so might need to record it somewhere else as well to make sure nothing went in, ++.
</comment><comment author="s1monw" created="2015-03-09T16:12:13Z" id="77884867">adding this is also going to make full-cluster restart as well as rolling restarts likely instant. Even for the non-timeseries data / logging case for full restarts we can seal all the indices, shutdown, restart &amp; unseal. This also works for rolling restarts if folks can afford having read only indices which I think is reasonable im most cases since the restart will be pretty fast. very promising! 
</comment><comment author="runningman84" created="2015-03-16T10:33:06Z" id="81568609">+1
</comment><comment author="nik9000" created="2015-03-16T16:57:15Z" id="81804835">&gt; This also works for rolling restarts if folks can afford having read only indices which I think is reasonable im most cases since the restart will be pretty fast. very promising!

This is great!  Our use case wouldn't allow us to seal an index outside of a rolling restart window or some other temporary maintenance action but we can absolutely get away with sealing them all for an hour or so.

So this is a great solution for us! 
</comment><comment author="synhershko" created="2015-03-16T17:18:23Z" id="81818315">+1, especially if sealing is reversible (which https://github.com/elastic/elasticsearch/issues/10032#issuecomment-77884867 implies)
</comment><comment author="rtoma" created="2015-03-19T08:54:03Z" id="83426832">+100 for 'instant recoveries' 
</comment><comment author="bleskes" created="2015-03-19T08:55:41Z" id="83427180">@synhershko yeah. the plan is to allow to unseal as well, making it a viable upgrade/full restart strategy (if you can afford stopping indexing).
</comment><comment author="s1monw" created="2015-03-23T19:51:45Z" id="85167019">&gt; This is great! Our use case wouldn't allow us to seal an index outside of a rolling restart window or some other temporary maintenance action but we can absolutely get away with sealing them all for an hour or so.

yes it's absolutely possible to unseal and the operation should be very fast. ie. makeing a cluster state update essentially.
</comment><comment author="s1monw" created="2015-03-27T12:43:24Z" id="86929585">We had some internal discussions how to implement this and I wanted to make sure they are recorded here on the issue. Sealing an index happens basically on two levels, the index and the shard level. 

# Index Level sealing

On the index level we use a `ClusterBlock` to prevent any write operations on the index via `index.blocks.read_only`. This is basically a cluster state update that sets the write block on the index that will be sealed together with a `seal_id` the seal id is a token that is generated to identify a seal operation on a shard level and also at a later point in time to utilize during recovery. So essentially there are three values on a ClusterState for each sealed index:
- a `index.blocks.read_only` to prevent writes 
- a seal state `index.seal.state` which can either be `sealing` or `sealed`
- a `index.seal.id` in `IndexMetaData.settings`  

This also requires the entire cluster to be on a version that supports and understands index sealing otherwise this feature will not be available. (we have the ability to check this via `DiscoveryNodes#smallestNonClientNodeVersion()`). The seal id should not be updatable outside of the seal API while the `index.blocks.read_only` is. Yet we need to prevent that this can be changed while the index is sealed.

The seal process is essentially a cluster state update (setting the block and the id) that waits for all shards to respond. This is very similar to how deleting an index works today. We issue the cluster state update that subsequently gets propagated to all the nodes in the cluster. Inside `IndicesClusterStateService` we listen to the relevant changes and update the `IndexService` where applicable. 

Once the seal operation is issued we set `index.seal.state : sealing`. The master now registers a listener on a dedicated endpoint waiting for all relevant shards to reply with successful or failed seal operations. Once all shards replied the master issues the `index.seal.state : sealed` and responds to the user unless we already ran into a timeout. (note this is a non-blocking operation on the master just like all other actions). In the case of a timeout or if the cluster is stuck in `sealing` mode only an `unseal` operation can recover from that state. Unsealing is a pretty straight forward it basically removes the seal state from the index settings and publishes the clusterstate. 

This is also very similar to the delete logic which is currently implemented in `IndicesClusterStateService#applyDeletedShards` and `NodeIndexDeletedAction`

# Shard Level sealing

Once the `IndexService` knows about the sealing it essentially needs to wait until all in-flight operations are finished on the shards primary as well on all the replicas. 

## Shard level sealing

For this we are currently planning to use ref-counting similar to what we do on `Store.java`. The ref-counting implemented in `AbstractRefCoutned` works in a way that prevents the caller from incrementing the reference count once it reached `0`. We can utilize this and increment the counter once in the `IndexShard` itself  and decrement it again once the shard is either closed or sealed. Once we decremented the shards reference we only need to wait until
we reach `0` on the counter in order to process the sealing. 

The good news is that due to the cluster block (read only settings) no new indexing operations can be issues such that we will reach 0 eventually. Certainly this requires reference counting (`incRef` / `decRef`) in the relevant API users which might be likely reducible to `TransportShardReplicationOperationAction`.
Once we reached `0` on the ref-count we can executed the following actions:
- issue a seal-commit on the engine that writes the shards seal ID to the commit metadata just like we do with the transaction log ID with every commit. This action is complex and should wait for all recoveries to finish (or even cancel?), flush the transaction log etc. to ensure all changes are committed to the lucene index.
- switch the engine from `InternalEngine` to an engine that is `read-only` to reduce resource utilization. We might be able to reuse, modify or abstract `ShadowEngine` for this purpose.
- send a seal command to the replicas of the shard which basically repeats the three steps above.
- send a seal acknowledgement to the master to eventually return to the user

At that point the index is sealed and no write operation can be submitted to the index anymore. 

# Unseal operation

The unseal operation pretty much reverses the sealing. We process a cluster state update that marks the index as `unsealed` by removing the seal states and processes the update. Once the cluster state update was successfully published we remove the index block and the index is good to go.

On a shard level we basically `re-increment` the reference count to `1` (allowing incoming requests to increment the count as well) and before doing that switching to `InternalEngine` to allow writes. The first write to the engine basically invalidates the seal such that we can't use it anymore for other operations like recovery.

# Fast Recovery

Today recovery is very resource heavy and often super slow since we don't know if two shards are identical on a document level ie. did all operations reach the replica or not. We can tell on a lucene segment level but the segments are different on all replicas unless we copied the over which takes a huge amount of time. With index sealing we basically mark the replicas as `identical` on a operation / document level since we prevent all writes on the shards. That allows us to side-step the entire shard copying and startup replicas immediately.

Luckily implementing fast recovery on top of the sealing is very straight forward. Basically what we need is an extension of the `RecoverySourceHandler` or `Engine.RecoveryHandler` that can sidestep the entire recovery process if the seal-ID matches on both the replica and the primary and if the recovery source has no operations in it's translog. If this applies we can simply skip the entire heavy part of the recovery procedure and startup the replica immediately. We only need to be carful to not remove the seal ID from the indices by issuing commits at the end of the recovery.

For safety reasons, if any operations exist in the transaction log we can't utilize the seal ID for fast recovery. Any operation in the translog indicates an illegal state in the context of the seal ID or in other words it _breaks the seal_. For instance if an old replica is started on a node that was sealed before but the primary is already accepting writes again we can in theory only recover from the transaction log but for the initial iteration we should skip this optimization. In the future we might be even able to extend this process to issue seal commits on a per shard level while accepting writes. 

# Optimizing / Force Merge on a Sealed index

For the time based indices usecase it's important to run `force-merge` / `optimize` on these indices even if they are sealed. As an later extension we can allow users  to run these operations even on a read-only engine. This is a feature that can be implemented at a later stage.

# Proposed work items
- [ ] add ref-counting to `IndexShard` and use it in index modifying operations on ie. on `TransportShardReplicationOperationAction` This should be nicely testable and can be implemented entirely stand-alone
- [ ] implement a `Engine#seal(String sealId)` that `flushes` the translog, writes the seal id to the lucene index. This operation should fail if there is a recovery in progress and should maybe even close the engine forcefully. The later integration of this action should be straight forward since we can open a read-only engine next to the `InternalEngine`, swap the `read-write` engine out, seal it and refresh the `read-only` engine.
- [ ] implement a seal action on `IndexShard` that utilize the `Engine#seal` as stated above.
- [ ] implement a `TransportShardReplicationOperationAction` like class that runs seal commands on all replicas of a shard
- [ ] implement sealing on the master 
- [ ] implement un-sealing on the master
- [ ] use seal ID in recovery. In the first iteration we only use it if there is no operation in the translog. Future iterations might allow to recover from translog as well if the seal ID is the same in the commit point on primary and replica.

I hope I covered all the moving parts at least on a high level. if there are any questions feel free to ask. Once we basically agree I will move this to the issue itself.
</comment><comment author="andrassy" created="2015-03-29T05:47:01Z" id="87360679">+1
</comment><comment author="bleskes" created="2015-04-16T10:47:46Z" id="93706101">Discussing this we came up with a new and simpler plan, which works independently of the cluster update. This gist of it is to have a best effort operation to sync the commit points both primaries and replicas. This "synced flush" is guaranteed to succeed if there are no concurrent indexing operations but will fail gracefully if there are. The result is a marker (sync id) on lucene commit points which allows us to shortcut the phase1 of recoveries which will give us the desired speed up. Since this is a best effort approach we can trigger it when ever a shard becomes inactive or in regular, longish intervals (say 30m) or any other time (TBD).

Solution sketch (this is a shard operation):
1. Pre Synced Commit phase:
   1. reaches out to all assigned shard copies and flush them.
   2. returns the lucene commit ID resulting of  this flush.
2. Validate there are no inflight operations (if there are, we abort). This can be done using the request ref count described above.
3. Synced Commit phase:
   1. Generate a sync id (a GUID).
   2. On primary:
           1. If there are pending writes or the lucene commit id is not identical to the one retrieved by the pre sync phase, abort.
           2. Create a new commit point with the sync id (while blocking writes to make sure nothing slips in)
   3. On replicas, we repeat the steps done on the primary. Note that if a replica doesn't participate in the sync it's OK.

TODOs:
- [x] Introduce ref counting of TransportShardReplicationOperation #10610
- [x] Implement preSyncCommit action based on TransportShardSingleOperationAction #10732 and afdab84f2df7c3bec2db81b25432fe76f25f3a98
- [x] Implement shard level IndexShard.syncIfNoPendingChanges(syncId, expectedCommitId) to be used by step 3 above. ebeb324fb9e625c9d18ff51c8526acedda3ec230
- [x] implement Synced Commit action base on TransportShardReplicationOperation to do step 3. The request of this action will carry the relevant shard lucene commit ids. #10732 and afdab84f2df7c3bec2db81b25432fe76f25f3a98
- [x] extend the shard stats API to return the current Commit Data from lucene. This will allow inspecting the current sync id on all shards. #10687
- [x] change recovery code to shortcut phase1 if the source and target sync id are the same. #10775
- [x] implement check for pending writes based on index shard ref counter (3.2)
- [x] hook synced flush into IndexingMemoryController
- [x] api for manually triggering a sync flush

[x] -&gt; in feature branch https://github.com/elastic/elasticsearch/tree/feature/synced_flush
</comment><comment author="s1monw" created="2015-04-16T10:52:40Z" id="93706787">thx for updating @bleskes 
</comment><comment author="s1monw" created="2015-05-22T18:11:12Z" id="104733456">@brwe can we close this one?
</comment><comment author="bleskes" created="2015-05-26T14:04:43Z" id="105535784">a short note that this has been in implemented as synced flush - see #11179 &amp; #11336  for more info
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be lenient when converting local to utc time in time zone roundings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10031</link><project id="" key="" /><description>This solves a problem in the time zone rounding classes where time dates that
fall into a DST gap will cause joda time library to throw an exception.
Changing the conversion methods 'strict' option to false prevents this.

Closes #10025
</description><key id="60271728">10031</key><summary>Be lenient when converting local to utc time in time zone roundings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-08T19:24:57Z</created><updated>2015-06-07T17:45:46Z</updated><resolved>2015-03-16T03:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T02:25:48Z" id="81353009">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix handling of IndicesOptions in update settings REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10030</link><project id="" key="" /><description>The Update Settings API tries to merge the query params with the settings sent as body and excluding some "well known params"  such as pretty, timeouts ...
Those well known params does not include the params used by IndicesOptions, so ES merges those params with the settings but the resulting settings are invalid.
e.g. 

```
curl -XPUT 'localhost:9200/twitter/_settings?expand_wildcards&amp;pretty' -d '
{
    "index" : {
        "number_of_replicas" : 2
    }
}'
```

returns 

```
{
  "error" : "ElasticsearchIllegalArgumentException[Can't update non dynamic settings[[index.expand_wildcards]] for open indices[[twitter]]]",
  "status" : 400
}
```

but works correctly without params relate to IndicesOptions.

I did not look at other APIs but some may suffer from the same issue.
</description><key id="60271669">10030</key><summary>Fix handling of IndicesOptions in update settings REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>:Settings</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-08T19:23:15Z</created><updated>2015-06-08T00:31:43Z</updated><resolved>2015-03-21T10:33:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T10:34:22Z" id="84303034">Thanks a lot @obourgain good catch and good fix. I just merged it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some docs n typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10029</link><project id="" key="" /><description>Hello, 
Here are fixes for small errors I found while reading the docs and source.
Should I squash this as the changes are unrelated to each other ?
</description><key id="60270399">10029</key><summary>Fix some docs n typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">obourgain</reporter><labels><label>docs</label><label>v1.3.10</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-08T18:51:34Z</created><updated>2015-03-10T18:18:06Z</updated><resolved>2015-03-10T18:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-10T18:18:06Z" id="78114639">Thanks @obourgain merged to master, 1.x, 1.4 and 1.3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: improve assertion at the end of shard recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10028</link><project id="" key="" /><description>we want to make sure the recovery finished all the way to post recovery. Current check, validating the shard is either in POST_RECOVERY or STARTED is not good because the shard could be also closed if things go fast enough (like in our tests). The assertion is changed to check the shard is not left in CREATED or RECOVERING.
</description><key id="60267363">10028</key><summary>Gateway: improve assertion at the end of shard recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-08T17:38:37Z</created><updated>2015-03-19T11:05:48Z</updated><resolved>2015-03-08T17:57:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-08T17:42:09Z" id="77763621">LGTM
</comment><comment author="kimchy" created="2015-03-08T17:46:47Z" id="77763899">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add min_doc_count support to range aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10027</link><project id="" key="" /><description>Currently, range aggregations doesn't have `min_doc_count` support AFAIK. As a temporary solution client-side filtering could be used but being able to filter out empty buckets from range aggregations (and from other aggregations if possible) would be neater.
</description><key id="60212461">10027</key><summary>Add min_doc_count support to range aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">halilim</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-03-07T16:10:47Z</created><updated>2015-12-15T10:59:43Z</updated><resolved>2015-12-15T10:59:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T04:48:31Z" id="81410852">This is something we started discussing on https://github.com/elastic/elasticsearch/issues/8110. The issue has been closed since it probably had too much scope, but essentially we are thinking/working on adding the ability to do more stuff at reduce time (when all shard responses get merged) and filtering of the reduced buckets could certainly be done this way. Hopefully this would also allow to have a single way to filter buckets instead of having to implement it for every aggregation.
</comment><comment author="clintongormley" created="2015-12-05T19:44:16Z" id="162240509">@colings86 any thoughts about this?
</comment><comment author="colings86" created="2015-12-07T08:42:20Z" id="162449165">With the addition of the bucket selector pipeline aggregation in 2.0 you can now filter buckets of any aggregation type at reduce time for minimum doc count or any other criteria
</comment><comment author="clintongormley" created="2015-12-15T10:59:43Z" id="164728165">thanks @colings86 - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better detection of CBOR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10026</link><project id="" key="" /><description>CBOR has a special header that is optional, if exists, allows for exact detection. Also, since we know which formats we support in ES, we can support the object major type case.
closes #7640
</description><key id="60175408">10026</key><summary>Better detection of CBOR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T23:41:50Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-20T21:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-18T07:09:15Z" id="82787317">left some comments - this is also related to #10134
</comment><comment author="kimchy" created="2015-03-20T20:35:54Z" id="84138101">@s1monw applied comments
</comment><comment author="s1monw" created="2015-03-20T20:43:09Z" id="84139701">LGTM
</comment><comment author="kimchy" created="2015-03-20T21:07:31Z" id="84146686">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Illegal instant due to time zone offset transition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10025</link><project id="" key="" /><description>When issuing a date_histogram with `America/Sao_Paulo` as timezone, the query fails with the following stack trace:

Stack Trace:

```
org.elasticsearch.common.joda.time.IllegalInstantException: Illegal instant due to time zone offset transition (daylight savings time 'gap'): 2014-10-19T00:00:00.000 (America/Sao_Paulo)
        at org.elasticsearch.common.joda.time.DateTimeZone.convertLocalToUTC(DateTimeZone.java:1025)
        at org.elasticsearch.common.rounding.TimeZoneRounding$TimeTimeZoneRoundingFloor.nextRoundingValue(TimeZoneRounding.java:175)
        at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:323)
        at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)
        at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:407)
        at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.moveToSecondPhase(TransportSearchCountAction.java:77)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:397)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:198)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:174)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:171)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:568)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

Please note that this is a very critical bug, we're already running on master due to another ElasticSearch bug(#9491).
</description><key id="60168240">10025</key><summary>Illegal instant due to time zone offset transition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">greenboxal</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T22:29:11Z</created><updated>2015-04-22T20:08:51Z</updated><resolved>2015-03-16T02:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-06T23:44:41Z" id="77656996">@cbuescher can you take a look?

@greenboxal You should absolutely never be running on master (or 1.x).  These are based on snapshots of Lucene, and those snapshots use as yet unreleased versions of Lucene's index format.  There are zero backcompat guarantees when you do this. 

Only use officially released versions of Elasticsearch (which are based on officially released version of Lucene).
</comment><comment author="greenboxal" created="2015-03-06T23:57:28Z" id="77658282">I can't use a stable release as #9491 fix didn't land in any released version(at least when we started using master).
Without that fix when the DST went off all date histograms(interval=1d) broke showing one day with 24 hours and another day on the same date with 1 hour of duration.
</comment><comment author="cbuescher" created="2015-03-08T19:05:02Z" id="77767973">We started to use Joda times utility method for local to UTC time conversions in the Rounding classes. It seems that doing this with the "strict" option set to true makes Joda time to throw exceptions when using this conversion on local time stamps that would fall in the DST gaps when DST is switched on. I could reproduce this on 1.x, but it also seems to affect the 1.4 and master branch where we now use DateTimeTone#convertLocalToUTC().
I think it is save to switch the "strict" option when using this method to "false" to prevent it from throwing the exception. I will open a PR with tests and the changes in the Rounding classes.
</comment><comment author="deusofnull" created="2015-04-22T18:12:30Z" id="95288504">@cbuescher Do you think that the commit referenced above would handle this exception as well?

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [day_timeinterval]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:416)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:709)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:500)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:542)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:491)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:444)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:150)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [2015-03-08 02:15:00 EST], tried both date format [yyyy-MM-dd HH:mm:ss z], and timestamp number with locale []
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:621)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:549)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:235)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:406)
        ... 12 more
Caused by: org.elasticsearch.common.joda.time.IllegalInstantException: Cannot parse "2015-03-08 02:15:00 EST": Illegal instant due to time zone offset transition (America/New_York)
        at org.elasticsearch.common.joda.time.format.DateTimeParserBucket.computeMillis(DateTimeParserBucket.java:390)
        at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:749)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:615)

```

I know its pretty similar, but I want to make sure it would be solved by upgrading ES before I go about doing that.
</comment><comment author="cbuescher" created="2015-04-22T20:03:41Z" id="95320647">@deusofnull Thanks, from a first glance I would not say that this is related. IllegaInstantException is used in various places in joda time. This issue concerns a case where it is raised when converting time zones in DateTimeZone that fall into the DST gap. Your issue seems to be related to parsing a date that falls into the DST gap, thats why they look similar, but the rest of the stack trace is different. It would be great if you could open a separate issue and report what you are doing and which version of ES you are running there.
</comment><comment author="deusofnull" created="2015-04-22T20:06:24Z" id="95321632">@cbuescher  Ill bring up a new issue then!  Thanks!  

What is strange is that I was parsing 90 days back of timestamps and I got this error for all of them.  All 15k of the timestamps.  If I'm correct, DST started about a month ago... So why did those dates still elicit the error?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Add moving average aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10024</link><project id="" key="" /><description>Adds an aggregation to calculate the moving average of sibling metrics in histogram-style data (histogram, date_histogram). Moving averages are useful when time series data is locally stationary and has a mean that changes slowly over time.

Seasonal data may need a different analysis, as well as data that is bimodal, "bursty" or contains frequent extreme values (which are not necessarily outliers).
#### Request

``` json
GET /test/_search?search_type=count
{
   "aggs": {
      "my_date_histo": {
         "date_histogram": {
            "field": "@timestamp",
            "interval": "day"
         },
         "aggs": {
            "the_sum": {
               "sum": {
                  "field": "price"
               }
            },
            "the_movavg": {
               "movavg": {
                  "bucketsPath": "the_sum",
                  "window": 3,
                  "weighting" : "single_exp",
                  "gap_policy" : "ignore",
                  "settings" : {
                    "alpha" : 0.5
                  }
               }
            }
         }
      }
   }
}
```
##### `bucketsPath` (Required)

Path to buckets/metric values to calculate moving average over
##### `window` (Optional - default 5)

The user specifies the window size they wish to calculate a moving average for. E.g. a user may want a 30-day sliding window over a histogram of 90 days total.

Currently, if there is not enough data to "fill" the window, the moving average will be calculated with whatever is available. For example, if a user selects 30-day window, days 1-29 will calculate the moving average with between 1-29 days of data.

We could investigate adding more "edge policies", which determine how to handle gaps at the edge of the moving average
##### `weighting` (Optional - default `simple`)

Currently, the agg supports four types of weighting:
- simple: A simple (arithmetic) average. Default.
- linear: A linearly weighted average, such that data becomes linearly less important as it gets "older" in the window
- single_exp: Single exponentially weighted average (aka EWMA or Brown's Simple Exp Smoothing), such that data becomes exponentially less important as it get's "older".
- double_exp: Double exponentially weighted average (aka Holt-Winters). Uses two exponential terms: first smooth data exponentially like single_exp, but then apply second corrective smoothing to account for a trend.
##### `gap_policy` (Optional - default `ignore`)

Determines the policy for handling gaps in the data.  Default is to ignore gaps.
##### `settings` (Optional)

Extra settings which apply to individual weighting types.  
- `alpha` can be set for `single_exp`. Defaults to 0.5
- `alpha` and `beta` can be set for `double_exp`.  Defaults to 0.5 and 0.5 respectively.
#### Response

``` json
{
   "took": 3,
   "timed_out": false,
   "aggregations": {
      "my_date_histo": {
         "buckets": [
            {
               "key_as_string": "2014-12-01T00:00:00.000Z",
               "key": 1417392000000,
               "doc_count": 1,
               "the_sum": {
                  "value": 1,
                  "value_as_string": "1.0"
               },
               "the_movavg": {
                  "value": 1
               }
            },
            {
               "key_as_string": "2014-12-02T00:00:00.000Z",
               "key": 1417478400000,
               "doc_count": 1,
               "the_sum": {
                  "value": 2,
                  "value_as_string": "2.0"
               },
               "the_movavg": {
                  "value": 1.5
               }
            },
            {
               "key_as_string": "2014-12-04T00:00:00.000Z",
               "key": 1417651200000,
               "doc_count": 1,
               "the_sum": {
                  "value": 4,
                  "value_as_string": "4.0"
               },
               "the_movavg": {
                  "value": 2.3333333333333335
               }
            },
            {
               "key_as_string": "2014-12-05T00:00:00.000Z",
               "key": 1417737600000,
               "doc_count": 1,
               "the_sum": {
                  "value": 5,
                  "value_as_string": "5.0"
               },
               "the_movavg": {
                  "value": 3.6666666666666665
               }
            },
            {
               "key_as_string": "2014-12-08T00:00:00.000Z",
               "key": 1417996800000,
               "doc_count": 1,
               "the_sum": {
                  "value": 8,
                  "value_as_string": "8.0"
               },
               "the_movavg": {
                  "value": 5.666666666666667
               }
            },
            {
               "key_as_string": "2014-12-09T00:00:00.000Z",
               "key": 1418083200000,
               "doc_count": 1,
               "the_sum": {
                  "value": 9,
                  "value_as_string": "9.0"
               },
               "the_movavg": {
                  "value": 7.333333333333333
               }
            }
         ]
      }
   }
}
```

Closes #10002
</description><key id="60167636">10024</key><summary>Aggregations: Add moving average aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-03-06T22:23:37Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-04-08T14:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-17T20:11:01Z" id="82576991">@polyfractal looks good so far. Left some comments, mostly around making the Weighting use a common interface with each weighting as a sub-class which might help make it easier to plug in new weighting types
</comment><comment author="polyfractal" created="2015-03-18T15:10:19Z" id="83016777">Awesome, thanks @colings86, all seemed reasonable but had question about the Weighting builder stuff.  I'll fixup the other stuff right now.
</comment><comment author="polyfractal" created="2015-03-20T16:58:04Z" id="84069463">@colings86 @clintongormley Ok, so I refactored everything as a compromise between generic API and specific internal structure.  I think it looks good now.

API looks the same as before (settings hash, optional):

``` json
"movavg": {
  "bucketsPath": "the_sum",
  "weighting" : "double_exp",
  "window": 5,
  "settings": {
    "alpha": 0.8   
  }
}
```

Internally, it uses a similar architecture as the SignificantHeuristics, e.g. a parsers are stored in a central map, the relevant parser is retrieved after we are done parsing the request, a MovAvgModel object is constructed and given to the aggregation.  All models are responsible for construction, custom settings, serialization, etc.

The only difference is that we extract a Map&lt;&gt; for settings (if it exists), instead of using a more specialized XContent parser.  I think this is a fair tradeoff.

`bucketsPath` needs fixing, but that's actually a problem with Reducer.Parser.  I'll fix that separately.
</comment><comment author="colings86" created="2015-03-23T06:51:24Z" id="84843746">@polyfractal I like the new changes but I wonder if we should have a MovAvgModelBuilder where you can set the settings for a particular model and then we call settings() to get the settings object to add to the `movavg` reducer? the builder should also get a type() method so we can use the builder to populate the `weighting` field too.
</comment><comment author="polyfractal" created="2015-03-23T18:53:13Z" id="85142910">@colings86 Newbie java dev question:  what does a builder buy us?  It doesn't seem necessary from a REST point of view (still need to parse a Map), and in java-land users can just make a `new SingleExpMovAvgModel(alpha)`, etc?  Is it just because we have the builder convention elsewhere?

Just seems like even more boilerplate for such a minor thing :)
</comment><comment author="colings86" created="2015-03-24T09:57:17Z" id="85428402">@polyfractal It allows someone using the Java API to have concrete methods describing the options available for each model rather than having to know what keys are valid for the settings map. At the moment the MovAvgBuilder takes the weight and settings separately and as far as I can see there is no way to pass a MovAvgModel into it? When https://github.com/elastic/elasticsearch/issues/10217 is complete we should have a MovAvgModel objects which has builders and parser methods as well being POJOs which we can pass around in the Java API. To avoid creating the MovAvgModelBuilder classes you could pre-empt this by adding the builder methods to the MovAvgModel classes now.
</comment><comment author="polyfractal" created="2015-03-24T13:02:57Z" id="85486003">@colings86 Ah, that makes sense...thanks for the explanation.  I'll fix it up on my flight home, will ping you when done :)
</comment><comment author="polyfractal" created="2015-03-26T21:45:20Z" id="86730055">- Renamed to `smooth`.  I think this is better representative of what the agg does (it "smooths" your data).  It also allows us to add other models in the future, like ARMA, which aren't moving averages
- Merged upstream changes
- Added builders.  I'm not sure if the way I did it is kosher...will have to chat when you get back.
</comment><comment author="rjernst" created="2015-03-26T23:02:49Z" id="86748946">&gt; Renamed to smooth. I think this is better representative of what the agg does (it "smooths" your data). It also allows us to add other models in the future, like ARMA, which aren't moving averages

Won't the different models have different parameters potentially? The name `smooth` seems quite arbitrary to me, while `moving_avg` just "makes sense" and is easily found with a google search. I'm not saying we shouldn't use `smooth`, I just want to make sure we aren't choosing a name too generic, just so we can _potentially_ add other models in the future, but perhaps at the cost of confusion early on.
</comment><comment author="polyfractal" created="2015-03-27T01:52:11Z" id="86787515">&gt; Won't the different models have different parameters potentially? 

@rjernst Yeah, different models may will have different parameters.  The current setup is that you specify which model you want to use, then pass in a generic `settings` hash which is specific for that model.  I know I would like to (near-to-medium term) add ARMA, ARIMA, Wavelet and Savitzky-Golay filters for smoothing.  There are dozens others we could add too.

&gt; The name smooth seems quite arbitrary to me, while moving_avg just "makes sense" and is easily found with a google search. I'm not saying we shouldn't use smooth, I just want to make sure we aren't choosing a name too generic, just so we can potentially add other models in the future, but perhaps at the cost of confusion early on.

This is my current thinking...please feel free to poke holes in it. :)

I'm thinking that we should to group functionality by "usage" whenever possible, since we shouldn't expect the average user to know how all the specific parts work.  Obviously some will be experts, but I think most will be new to time series analysis.

If we clump functionality by usage, a user can sit down and say "I would like to smooth my data", pull out the smoothing agg, and start fiddling with different models and parameters.  Similarly, they should be able to pull out an `outlier` agg and try different models/params to find anomalies, or a `predict` agg and start forecasting.

Importantly, an "outlier" agg and "prediction" agg will also support some of the same models as smoothing.  There is a lot of overlap between the three...but not full overlap, which is really the problem.

The alternative is that users will need to navigate a large set of aggs that inconsistently support functionality.  E.g. this table illustrates the problem:

|  | Smooth | Predict | Outlier |
| --- | --- | --- | --- |
| movavg | X | sorta | sorta |
| ARMA / ARIMA | X | X | X |
| Wavelet | X |  | X |
| SG filter | X |  | X |
| High / low / bandpass filter | X |  |  |
| Regression |  | X | X |
| Thresholding |  |  | X |
| SVM, ANN, etc |  | X | X |

The other concern is that if each agg supports several "modes", the output from that agg will depend on which options you have toggled.  That might be irritating if toggling a new param changes the output (e.g. predict will start adding new buckets, outlier will add a new "was_outlier" field or something)

Basically, the goal was to prevent an explosion of small aggs that each do one highly technical thing...leaving a lot of users in the dust because they've never done time series analysis before.

There is also a bunch of extra stuff that can't be grouped easily, like autocorrelation, changepoint, statistical testing, etc.

_(__Oh goodness, that turned into an essay.  Sorry_ _:( )_
</comment><comment author="rjernst" created="2015-03-27T02:45:12Z" id="86793797">@polyfractal That totally makes sense. +1 for `smooth` :)
</comment><comment author="polyfractal" created="2015-03-27T16:04:36Z" id="86990172">@rjernst Urgh, second guessing myself now...thoughts?

I agree there is concern about making things toooo generic.  There is also something nice about having an agg represent a single-purpose, with one well defined set of params.  Maybe it's a pain to have one agg with like 10 different modes?

Maybe we could do a hybrid approach?  I was thinking about it last night and realized that most functionality has an innate purpose, but can be used for prediction or outlier based on that functionality.  E.g. a moving average smooths the data, but can find outliers by comparing a value against the smoothed average.

So we could do:
- Aggs are individual for their innate purpose: `movavg`, `wavelet`, `regression`, etc
- Aggs are bundled for `predict`, `outlier` and `changepoint`, since those tend to use the features of the individual components plus some extra logic

I think I'm still leaning towards the prior option, but wanted to write this down.  Not sure, just braindumping now :)
</comment><comment author="polyfractal" created="2015-03-30T15:22:02Z" id="87721265">So...after a lot of discussion we decided to revert the naming change and go back to `moving_avg`.  We did this for a few reasons:
- Simpler is better to start, since we aren't quite sure how users will be using the new functionality.  Single-function aggs are simpler, and people are already accustomed to looking for a "moving average"...whereas a "smoothing" agg might confuse them
- Since each model has very different settings, using a single generic settings hash is not ideal (even though it's validated).
- We can re-expose these individual aggs through a "sugar" agg later if we see people needing some guidance when getting started.  Similar to how the `match` query is essentially a smart wrapper for `term`, `phrase`, `phrase_prefix`, etc.
- At least for prediction, the code complexity is not large on a per-agg basis.  Outlier will likely need a bundled agg to start, since it will be considerably more complex.
- Individual aggs also let us throw a lot of functionality at the wall, and see what sticks with users.  We can prune or reorganize into sugar as required later

/cc @rjernst 
</comment><comment author="colings86" created="2015-04-07T10:13:14Z" id="90495985">@polyfractal left a comment but I think it's pretty close now
</comment><comment author="polyfractal" created="2015-04-08T14:46:31Z" id="90937723">Closed via a squash-merge in a824184bf2fddecb0ed4daa2c2deacbb66d33c30, because I'm terrible at Git and messed up the merge process :(
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mark shadow replicas with 's' in _cat/shards output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10023</link><project id="" key="" /><description>Fixes #9772
</description><key id="60141299">10023</key><summary>Mark shadow replicas with 's' in _cat/shards output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T18:43:39Z</created><updated>2015-03-19T11:06:02Z</updated><resolved>2015-03-06T20:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-06T19:19:57Z" id="77618645">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lack of warning for parsing incorrect parameters in elasticsearch.yml </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10022</link><project id="" key="" /><description>There is a lack of warning for incorrectly parsed configurations. In this case I set invalid values for the following configurations in elasticsearch.yml : 

```
# incorrect values because booleans have to be lowercase
node.master: True
node.data: True
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["localhost"]
```

But elastic search starts and I am able to see the following cluster nodes:

```
curl localhost:9200/_nodes/http?pretty=true
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
      ...
      "attributes" : {
        "master" : "True",
        "data" : "True"
      },
     ...
    },
```

The only indication when I get that there is a problem is when I try to run other commands to access the data in the cluster. This is the error that I get back when I try to add a mapping to my_index:

```
curl -X PUT localhost:9200/my_index -d @mapping.json'
{"error":"MasterNotDiscoveredException[waited for [30s]]","status":503}
```

In this case, it would be useful to fail server start if there is an incorrect parameter or issue some sort of warning or have it logged. I ran this on elasticsearch 1.3.4.
</description><key id="60120530">10022</key><summary>Lack of warning for parsing incorrect parameters in elasticsearch.yml </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mtbottle</reporter><labels /><created>2015-03-06T16:02:14Z</created><updated>2015-04-04T12:38:53Z</updated><resolved>2015-04-04T12:38:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T12:38:53Z" id="89563276">Agreed - we're planning a big settings cleanup.  Closing as a duplicate of #6732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ban Throwable.toString() and Throwable.getMessage()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10021</link><project id="" key="" /><description>This way, we never have "partial exceptions" with incomplete information anywhere.

I havent yet fixed any of the 97 violations in question.

 Errors in the branch look like this:
[ERROR] Forbidden method invocation: java.lang.Throwable#getMessage() [Use ExceptionsHelper.stackTrace(Throwable) instead]
[ERROR]   in org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository (BlobStoreIndexShardRepository.java:152)
</description><key id="60119445">10021</key><summary>ban Throwable.toString() and Throwable.getMessage()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2015-03-06T15:53:59Z</created><updated>2016-09-14T17:43:01Z</updated><resolved>2016-09-12T21:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-06T17:33:19Z" id="77599934">+1
</comment><comment author="nik9000" created="2015-03-06T17:36:32Z" id="77600511">+1
I hate these.

On Fri, Mar 6, 2015 at 12:33 PM, Luca Cavanna notifications@github.com
wrote:

&gt; +1
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/10021#issuecomment-77599934
&gt; .
</comment><comment author="uschindler" created="2015-03-06T19:17:24Z" id="77618203">Very nice :)

+1
</comment><comment author="rjernst" created="2015-03-06T19:28:41Z" id="77620119">+1

FWIW, the main place I see exceptions being simplified is in `SearchShardFailure`, in the constructor it calls `ExceptionsHelper.detailedMessage`.  This thing loses the stack trace.
</comment><comment author="s1monw" created="2015-03-18T07:10:13Z" id="82787387">lets fix them +1 rob, we can also open a public branch for this...
</comment><comment author="s1monw" created="2015-03-23T13:25:22Z" id="84995934">@rmuir do you wanna open a public branch and we fix it there?
</comment><comment author="MaineC" created="2015-09-03T10:03:00Z" id="137397120">I spent some time the past few days looking at where we actually use .getMessage() in the code:
- There's some occurrences due to the fact that exceptions we created do not support a Exception(Throwable cause) constructor but always request a message. Can easily be fixed by either adding the simpler constructor or using a more meaningful message.
- There's simple things like creating log messages. I suppose these can simply be switched to .stackTrace(...)
- There's instances where from a coarse scan it looks like instances of serialising failures, would have to dig deeper to see what exactly is going on there.
- Then there's a handful of trickier instances like e.g. parsing the message to figure out what really went wrong in a certain call.

Caveat: I was starting to make changes in a local bug fix branch a bit too quickly resulting in a) a pretty large diff with changes related to all of the above mixed together, b) breaking two unit tests as a result. Trying to concentrate on the first bullet point for a cleaner approach first.
</comment><comment author="dakrone" created="2016-09-12T21:17:58Z" id="246496847">Closing this for now, someone can base work off of it if desired in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix thread leak in Hunspell service tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10020</link><project id="" key="" /><description>An unchecked exception might be thrown when instantiating the HunspellService, leading to thread leaks in tests.

Closes #9849
</description><key id="60117625">10020</key><summary>Fix thread leak in Hunspell service tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>test</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T15:40:59Z</created><updated>2015-03-27T10:05:28Z</updated><resolved>2015-03-27T09:12:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-06T15:42:28Z" id="77579555">@rjernst Simple fix but I think it's enough. Can you have a look please? :)
</comment><comment author="rjernst" created="2015-03-06T19:16:48Z" id="77618097">I don't think we should be hiding failures in loading the dictionaries.  Exceptions should propagate.  Is the problem Guice can't deal with unchecked exceptions properly? I see the injected constructor throws IOException; if we wrap in that will guice do the right thing?
</comment><comment author="tlrx" created="2015-03-20T11:05:50Z" id="83986784">The exception is not hidden but logged as an error. Guice is not really good at handling exception during provisioning (see Google Guice wiki page [here](https://github.com/google/guice/wiki/ThrowingProviders) ), that's why I think it's not a good idea to propagate the exception in this case... Wrapping the unchecked exception into an IOException won't change anything because Guice will re-wrap it into an unchecked ProvisionException later :)
</comment><comment author="rjernst" created="2015-03-27T08:39:43Z" id="86869716">LGTM. 2 minor spelling issues in the comments
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy upsert nested script no longer working on 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10019</link><project id="" key="" /><description>Hello,

I am using the bulk processor with the Update API of the ElasticSeach Java package. I'm running an upsert script which inserts records within a nested field of a document.

"if (ctx._source.records.contains(data)){ctx.op='none'; }else {ctx._source.records+=data;}"

I am running the following script on a 1.4.2 cluster and it works perfectly using the 1.4.4 client Java API.

However since I have upgraded on the new 1.4.4 ElasticSearch Cluster Server, the script is no longer working, documents are not deleted and reindexed. I think that might be related to the groovy modifications that were made in 1.4.3 server.

Thank you
</description><key id="60116783">10019</key><summary>Groovy upsert nested script no longer working on 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phil2396</reporter><labels /><created>2015-03-06T15:34:47Z</created><updated>2016-11-04T16:57:57Z</updated><resolved>2015-03-06T17:09:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-06T15:47:37Z" id="77580412">@phil2396 do you have the error or exception that you are getting back from Elasticsearch when you use the update API?

Dynamic scripting for Groovy was disabled in 1.4.3, which may be what you are seeing.
</comment><comment author="phil2396" created="2015-03-06T15:51:06Z" id="77581019">@dakrone I don't see any errors from elasticsearch using the Update API. I will follow the procedure here http://www.elasticsearch.org/blog/running-groovy-scripts-without-dynamic-scripting/ and see if that fixes the issue.  I update later today. 

Thank you
</comment><comment author="phil2396" created="2015-03-06T16:46:48Z" id="77591499">Using the Java API 1.4, I cannot specify file parameter to use the pre-compiled script that is in 
/config/scripts/ on disk.

=&#160;&gt; UpdateRequest.java

I am now trying to bypass the default by enabling dynamic scripting

script.groovy.sandbox.enabled: true 

@dakrone 
</comment><comment author="dakrone" created="2015-03-06T17:02:18Z" id="77594437">@phil2396 you should be able to use `UpdateRequestBuilder` for this:

``` java
client().prepareUpdate("index", "type", "id").setScript("scriptname", ScriptService.ScriptType.FILE).get();
```
</comment><comment author="phil2396" created="2015-03-06T17:09:09Z" id="77595708">Thank you 

@dakrone 
</comment><comment author="femaffioletti" created="2016-11-03T18:34:54Z" id="258234513">Hi Phil, I'm facing the same problem. How did you resolve this? Just with the 'enabling dynamic scripting'?

Thanks for the help!!
</comment><comment author="phil2396" created="2016-11-03T18:43:46Z" id="258237138">Hello @femaffioletti 

yes i solved it using dynamic scripting :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] IndicesStoreIntegrationTests.indexCleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10018</link><project id="" key="" /><description>http://build-us-00.elasticsearch.org/job/es_core_1x_regression/1415/

Related to https://github.com/elasticsearch/elasticsearch/pull/9985
</description><key id="60112589">10018</key><summary>[CI] IndicesStoreIntegrationTests.indexCleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>jenkins</label></labels><created>2015-03-06T15:02:51Z</created><updated>2015-04-04T12:36:04Z</updated><resolved>2015-03-31T13:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-06T15:03:47Z" id="77572603">I looked at it, not obvious..., seems like the shard got moved, but ended up not being deleted. Seems unrelated so far to my change.
</comment><comment author="brwe" created="2015-03-18T16:55:20Z" id="83056341">Here is what happens:
When we delete shards we first check if all expected copies of this shard are actually up and running somewhere else (IndicesStore.deleteShardIfExistElseWhere()). This check checks for STARTED and RELOCATED. In the test we relocate a shard from one node (node_1) to another (node_3) but at the moment node_1 checks if enough shards available the shard is still in POST_RECOVERY on node_3 and therefore the check fails.

This is easily reproducible by adding an artificial delay in IndexShardState.changeState.

Chatted with @s1monw about it and will make a PR to schedule a retry of the check in case the check runs into shards which are in POST_RECOVERY.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Race condition in lifecycles of DiscoveryService and Discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10017</link><project id="" key="" /><description>A fresh master does (rarely) not send the first cluster state due to a race condition in lifecycles of `DiscoveryService` and its member `Discovery`. In `DiscoveryService.doStart()` the `Discovery` is started but the lifecycle for `DiscoveryService` is started only after that. This is why when the first cluster state reaches `DiscoveryService.publish` the lifecycle might or might not have started.

I am not yet sure what the implications are. However, when I add an artificial delay in `DiscoveryService.doStart()` like this:

```
@Override
    protected void doStart() throws ElasticsearchException {
        initialStateListener = new InitialStateListener();
        discovery.addListener(initialStateListener);
        discovery.start();
        logger.info(discovery.nodeDescription());
        try {
            //NOCOMMIT
            sleep(1000);
        } catch (InterruptedException e) {

        }
    }
```

The `NoMasterNodeTests` fail occasionally. It seems like sometimes the cluster is not blocked properly for writes. In addition, this makes all test in pr #9952 fail as described [here](https://github.com/elasticsearch/elasticsearch/pull/9952#issuecomment-77535805).
I think this needs to be investigated.
</description><key id="60103927">10017</key><summary>Race condition in lifecycles of DiscoveryService and Discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Discovery</label><label>adoptme</label><label>bug</label><label>discuss</label></labels><created>2015-03-06T13:51:34Z</created><updated>2016-10-25T16:10:56Z</updated><resolved>2016-10-25T16:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T19:43:30Z" id="162240459">@brwe Is this still an issue?
</comment><comment author="dakrone" created="2016-09-27T14:20:47Z" id="249879362">@brwe is this still an issue?
</comment><comment author="brwe" created="2016-10-25T16:10:56Z" id="256082072">No, the code above does not even exist anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dangling indices settings, always import it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10016</link><project id="" key="" /><description>Remove the settings around dangling indices, such as no import and timeout for deletion, we always want to import dangling indices for safety, and we should not allow to change the behavior. This also cleans up the code quite a bit.
</description><key id="60103794">10016</key><summary>Remove dangling indices settings, always import it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Settings</label><label>deprecation</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T13:50:16Z</created><updated>2015-06-06T16:10:22Z</updated><resolved>2015-03-08T20:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-06T19:29:19Z" id="77620219">@dakrone addressed your comments
</comment><comment author="dakrone" created="2015-03-06T20:55:21Z" id="77633957">Left one comment about a missing argument, other than that LGTM
</comment><comment author="s1monw" created="2015-03-06T22:11:32Z" id="77645717">i like the simplifications here. Yet, I think we need to work on how these huge methods are structured and tested. In other classes I already started to add simple package private methods that are individually tested in unittests reducing the size of the loops dramatically. I think we have to break things up into small chunks when we do these kind of refactorings and add real unittests. It's pretty simple to do IMO such that the main look really only processes the end result and loops over the dangeling indices. Can you please simplify, test and document that would be awesome.
</comment><comment author="kimchy" created="2015-03-07T01:57:53Z" id="77667105">@s1monw I was thinking about refactoring it, was thinking of doing it as a next step to keep the change smaller. I updated the pull request with refactoring out the handling of dangling indices into its own class, and it has specific tests for it. Needed to refactor out also the persistence logic of meta state as well.
</comment><comment author="s1monw" created="2015-03-08T17:58:31Z" id="77764500">left some comments
</comment><comment author="kimchy" created="2015-03-08T19:51:23Z" id="77770396">@s1monw applied comments, and did another round of simplifications
</comment><comment author="s1monw" created="2015-03-08T20:03:48Z" id="77770976">left one comment - LGTM
</comment><comment author="s1monw" created="2015-03-08T20:04:07Z" id="77770991">thanks for doing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>network.bind_host affects network.publish_host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10015</link><project id="" key="" /><description>Hi,

I am experiencing problems trying to seperate the bind and the publish host address. Obviously the configuration option "network.publish_host" hast no effect, but elasticsearch binds also the publish port (9300) on the interface configured either in "network.host" or "network.bind_host". I am using elasticsearch 1.4.4.

Can anyone reproduce this bug?

Cheers,
Marco
</description><key id="60101686">10015</key><summary>network.bind_host affects network.publish_host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">misterunknown</reporter><labels><label>:Settings</label><label>feedback_needed</label></labels><created>2015-03-06T13:30:01Z</created><updated>2015-04-12T14:23:03Z</updated><resolved>2015-04-12T14:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-04T12:33:00Z" id="89560351">Hi @misterunknown 

Can you provide the actual settings that you're using, explain what you expect their effect to be and what appears to be different?

There is no point in having a publish host/port that is not also bound.
</comment><comment author="misterunknown" created="2015-04-09T07:02:42Z" id="91131721">Hi,

in our testing environment we try to limit access to elasticsearch via an Apache proxy. Therefore elasticsearch only listens on localhost (127.0.0.1:9200). Now we had the intention to build a cluster by adding another host. For cluster communication, elasticsearch should bind to another IP (e.g. 192.168.20.13). Our configuration looks like this:

```
network.bind_host: 127.0.0.1
network.publish_host: 192.168.20.13
```

The expected behavoior: Elasticsearch binds 127.0.0.1:9200 and 192.168.20.13:9300 (this one for cluster communication).
The actual behavior: Elasticsearch binds 127.0.0.1:9200 and 127.0.0.1:9300.

Cheers,
Marco
</comment><comment author="clintongormley" created="2015-04-12T14:23:03Z" id="92070245">Hi @misterunknown 

You appear to be confusing publish/bind with transport/http.  Transport is for intra-node communication, which defaults to using port 9300. HTTP is for the REST interface, which defaults to 9200.

However, the bind address is the host/port that Elasticsearch listens to, and the publish address is the host/port the Elasticsearch publicises to other nodes, etc.

So I think what you're actually after is:

```
transport.host: 192.168.20.13
http.host: 127.0.0.1
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use provided cluster state for indices service validations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10014</link><project id="" key="" /><description>Since the method can be called in an #execute event of the cluster service, we need the ability to use the cluster state that will be provided in the ClusterChangedEvent, have the ClusterState be provided as a parameter
</description><key id="60094615">10014</key><summary>Use provided cluster state for indices service validations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T12:13:27Z</created><updated>2015-03-19T11:06:21Z</updated><resolved>2015-03-06T12:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-06T12:19:54Z" id="77550845">good change, LGTM
</comment><comment author="kimchy" created="2015-03-06T12:42:36Z" id="77553160">git merge fail, closing manually
</comment><comment author="s1monw" created="2015-03-06T13:05:06Z" id="77555528">was there a bug due to that? I wonder if there was one what test failed? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Java 8 _ variable warning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10013</link><project id="" key="" /><description>The _ variable causes a warning when compiling with Java 8, noting that it might be removed in a future version
</description><key id="60091955">10013</key><summary>Fix Java 8 _ variable warning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-03-06T11:42:58Z</created><updated>2015-06-08T13:41:53Z</updated><resolved>2015-03-06T12:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-06T12:05:48Z" id="77549285">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings = 3 Shards =&gt; Result = 5 shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10012</link><project id="" key="" /><description>Hi there,

I don't know if it is really an issue or if it is a misconfiguration on my side but here is my problem.
I configured an index settings that way :

GET iislog/_settings
{
   "iislog": {
      "settings": {
         "index": {
            "creation_date": "1425458515905",
            "number_of_shards": "3",
            "number_of_replicas": "1",
            "version": {
               "created": "1040299"
            },
            "uuid": "FJrBfCRkSR-bh3HUlmC4Ww"
         }
      }
   }
}

But, I just realized that, in fact, there is 5 shards :

GET _cat/shards
iislogs    4 p INITIALIZING                  127.0.0.1 S3DEV-BI-ES09 
iislogs    4 r UNASSIGNED  
iislogs    0 p STARTED      15864679   5.3gb 127.0.0.1 S3DEV-BI-ES08 
iislogs    0 r UNASSIGNED  
iislogs    3 p STARTED      15999341   9.2gb 127.0.0.1 S3DEV-BI-ES09 
iislogs    3 r UNASSIGNED  
iislogs    1 p STARTED      15819303   5.2gb 127.0.0.1 S3DEV-BI-ES08 
iislogs    1 r UNASSIGNED  
iislogs    2 p STARTED      15986204   5.5gb 127.0.0.1 S3DEV-BI-ES10 
iislogs    2 r UNASSIGNED 

Notice that I recently deleted this index and recreated it. It was initialy having 5 shards but I always get and unassigned shard (the first one). So I decided to reconfigure it with only 3 shards.

I will keep this index today if you need more inputs but I will need to recreate it before the week-end begin. 

Thank you for your help
Alek
</description><key id="60079599">10012</key><summary>Settings = 3 Shards =&gt; Result = 5 shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ZeAleks</reporter><labels /><created>2015-03-06T09:41:32Z</created><updated>2015-03-06T10:28:53Z</updated><resolved>2015-03-06T09:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-06T09:51:51Z" id="77533231">hey Alek, you pasted `GET iislog/_settings` but cat shows an index called `iislogs` &lt;== note the plural `s`? does it matter?
</comment><comment author="ZeAleks" created="2015-03-06T09:55:55Z" id="77533756">Hi Simon,

I really feel stupid now :(

Thank you !
</comment><comment author="s1monw" created="2015-03-06T10:28:53Z" id="77537919">no worries, thanks for reporting back
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Java API does not match the REST layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10011</link><project id="" key="" /><description>The REST layer has the `script`, `script_id` and `script_file` options that can be used to configure scripts that are respectively inline, indexed or in a file in `config/scripts`. However, the Java API only accepts a `String script` parameter, which makes the reference documentation confusing for users of the Java API.

For examples, see eg. ScriptSortBuilder for sorting or StatsBuilder for aggregations.
</description><key id="60076393">10011</key><summary>Scripting: Java API does not match the REST layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>adoptme</label><label>bug</label></labels><created>2015-03-06T09:07:01Z</created><updated>2015-06-15T08:27:02Z</updated><resolved>2015-06-15T08:26:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-06T09:11:22Z" id="77527982">+1 we should streamline this to any builder/api that can execute scripts and make the java api more similar to the REST layer...
</comment><comment author="colings86" created="2015-03-06T09:17:47Z" id="77528800">+1 From 1.4.5 onwards we will have a ScriptParameterParser class to ensure consistent parsing of script parameters, so it seem logical to also have a ScriptParameterBuilder to ensure consistent building of script parameters
</comment><comment author="uboness" created="2015-03-06T10:14:04Z" id="77536036">Why not just have a `Script` construct that is both `ToXContent` and exposes a static `parse` method... Then we can use it everywhere and clean up the code (we recently added it in alerts.... Would love to have it in core)
</comment><comment author="bradvido" created="2015-03-06T14:15:35Z" id="77564539">Being able to specify the script language and script parameters is important too. I can't figure out any way to do it with the 1.4.4 Java API and the StatsBuilder class.
</comment><comment author="colings86" created="2015-06-15T08:26:33Z" id="111975732">This was fixed in https://github.com/elastic/elasticsearch/pull/11164. All script APIs in 2.0 have the same request syntax and can specify type, language and parameters in both the Java API and the REST layer
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10010</link><project id="" key="" /><description>Fixed a typo.
</description><key id="60069506">10010</key><summary>Typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wittyameta</reporter><labels><label>docs</label></labels><created>2015-03-06T07:39:19Z</created><updated>2015-03-09T18:02:44Z</updated><resolved>2015-03-09T18:02:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:57:36Z" id="77786078">thanks @wittyameta - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index disappearing issue on 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10009</link><project id="" key="" /><description>Hi there, I'm using elasticsearch v1.4.4 (oracle jvm 1.7.0_75) but strange issues occurred.

I have a index which consists of one parent doc type and 25 different types of child doc type. 
After create index 400 billion documents, garbage collect did't work and elasticsearch was terminated abnormally.

![image](https://cloud.githubusercontent.com/assets/9878011/6521399/b2a72de0-c41a-11e4-9400-6905882c666e.png)

After restart a elasticsearch, all 400 billion document are missing.

I was checked a hprof dump file and finded out a retain memory of org.apache.lucene.store.RAMFile was 29G.

![image](https://cloud.githubusercontent.com/assets/9878011/6521402/bad87910-c41a-11e4-9583-bd6c4658e1e7.png)

is there any connection between parent-child document and retain memory of org.apache.lucene.store.RAMFile?

And I wonder why documents were disappeared.

Let me know if you need more information.
</description><key id="60067406">10009</key><summary>index disappearing issue on 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kwongb98</reporter><labels /><created>2015-03-06T07:07:42Z</created><updated>2015-03-09T01:21:21Z</updated><resolved>2015-03-09T01:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-06T10:33:11Z" id="77538513">can I see some of your configurations ie. what kind of store do you use? Are you using `"index.store.type": "memory"` by any chance?
</comment><comment author="kwongb98" created="2015-03-09T01:08:08Z" id="77786624">Oops,  I set "index.store.type": "memory" as you expect...
thank you, for your advice
</comment><comment author="s1monw" created="2015-03-09T01:21:21Z" id="77787369">thanks for clarifying.. we removed it in 1.5 actually to prevent these problems
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to track status of optimize api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10008</link><project id="" key="" /><description>Optimize is an intensive process and can run for many hours.  It will be nice to provide a way to track its status (esp. when we switch to a sync operation in the future: https://github.com/elasticsearch/elasticsearch/pull/9640).   

This is likely dependent on task management:
https://github.com/elasticsearch/elasticsearch/issues/6914
</description><key id="60035078">10008</key><summary>Ability to track status of optimize api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label><label>stalled</label></labels><created>2015-03-05T23:35:50Z</created><updated>2015-12-05T19:42:27Z</updated><resolved>2015-12-05T19:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kjelle" created="2015-08-19T18:30:18Z" id="132733896">+1

E.g. im optimizing a 24shard index from 1127 segments down to max_num_segments=1... it would be nice to see that it is actually moving forward, estimate how long it will take etc.
</comment><comment author="clintongormley" created="2015-12-05T19:42:27Z" id="162240415">Closing in favour of https://github.com/elastic/elasticsearch/issues/15117
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>script_file does not work with _update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10007</link><project id="" key="" /><description>Running with 1.4.4, when trying to update an existing document, one cannot use the undocumented `script_file` because it complains of a validation error (expecting "script" or "doc").

```
POST /test/type/1/_update
{
  "script_file" : "AddTag",
  "lang": "groovy",
  "params": {
    "tag": "tag5"
  }
}
```

Fails due to:

```
{
   "error": "ActionRequestValidationException[Validation Failed: 1: script or doc is missing;]",
   "status": 400
}
```

As a workaround, you can supply the script file directly using the `script` parameter (even with dynamic scripting disabled for Groovy because it recognizes that it is a file).

```
POST /test/type/1/_update
{
  "script" : "AddTag",
  "lang": "groovy",
  "params": {
    "tag": "tag5"
  }
}
```
</description><key id="60034944">10007</key><summary>script_file does not work with _update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:CRUD</label><label>:Scripting</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-03-05T23:34:40Z</created><updated>2015-04-11T16:50:51Z</updated><resolved>2015-03-20T17:48:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-06T09:20:25Z" id="77529169">@pickypg This should be fixed from 1.4.5 onwards with the addition of the ScriptParameterParser. would you be able to run the same test on the HEAD of the 1.4 branch an confirm?
</comment><comment author="lmenezes" created="2015-03-20T12:21:02Z" id="84002288">@colings86 I ran into the same issue today, and with latest from 1.4 branch it works correctly.
For previous version of 1.4, sending the script_file parameter on the URL also works. Problem was just that while reading the body, the scirpt_file parameter was missing.
</comment><comment author="javanna" created="2015-03-20T17:48:38Z" id="84086851">I think we can close this, I added version labels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtering term with "_" doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10006</link><project id="" key="" /><description>I'm trying to make a query like this:

{
  "query" :{
    "filtered" :{
      "query" :{
          "multi_match" : {
            "query" : "Vonder Carregador de Baterias CBV 0900 - 110V",
            "fields" : [ "nome^10", "descricao" ],
            "operator" : "or"
          }
       },
       "filter" : {
         "bool" : {
           "must" : [
             {"term" : { "ativo" : 1 }},
             {"term" : {"id_instancia": "Master_cleaner"}},
           ]
         }
       }
     }
   }
}
But when I filter some term with "_" it doesn't work.
</description><key id="60027430">10006</key><summary>Filtering term with "_" doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">limabetalabs</reporter><labels /><created>2015-03-05T22:33:13Z</created><updated>2015-03-06T10:30:16Z</updated><resolved>2015-03-06T10:30:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-06T10:30:16Z" id="77538110">please use the mailing list for questions like this. The issue tracker is only for bugs and features. 

thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date type has not enough precision for the logging use case.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10005</link><project id="" key="" /><description>At present, the 'date' type is millisecond precision. For many log use cases, higher precision time is valuable - microsecond, nanosecond, etc.

The biggest impact of this is during sorting of search results. If you sort chronologically, newest-first, by a date field, documents with the same date will probably be sorted incorrectly (because they match). This is often reported by users seeing events "out of order" when they have the same timestamp. Specific example being sorting by date and seeing events in newest-first order, unless there is a tie, in which case oldest-first (or first-written?) appears. This causes a bit of confusion for the ELK use case.

Related: https://github.com/logstash-plugins/logstash-filter-date/pull/8

I don't have any firm proposals, but I have two different implementation ideas:
- Proposal 1, use a separate field: Store our own custom-precision time in a separate field as a _long_. This allows us to do correct sorting (because we have higher precision), but it makes any date-related functionality in Elasticsearch not usable (searching `now-1h` or doing date_histogram, etc)
- Proposal 2, date type has tunable precision: Have the `date` type have configurable precision, with the default (backwards compatible) precision being milliseconds. This would let us choose, for example, nanosecond precision for the logging use case, and year precision for an archaeological use case (billions of years ago, or something). Benefit here is date histogram and other date-related features could still work. Further, having the precision configurable would allow us to keep the underlying data structure a 64bit long and users could choose their most appropriate precision.
</description><key id="60026809">10005</key><summary>Date type has not enough precision for the logging use case.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels><label>:Dates</label><label>high hanging fruit</label><label>stalled</label></labels><created>2015-03-05T22:28:11Z</created><updated>2017-03-31T19:17:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2015-03-05T22:54:59Z" id="77470437">I know Joda's got a precision limit (the Instant class is millisecond precision) and a year limit ("year must be in the range [-292275054,292278993]"). I'm open to helping explore solutions in this area.
</comment><comment author="synhershko" created="2015-03-06T00:00:00Z" id="77479544">What about consequences to field_date size? even with docvalues in place, cardinality will be ridiculously high. Even for those scenarios which need this, this could be an overkill, no?
</comment><comment author="nikonyrh" created="2015-03-16T10:11:18Z" id="81557655">Couldn't you just store the decimal part of the second in a secondary field (as a float or long) and sort by these two fields when needed? You could still aggregate based on the standard date field but not at a microsecond resolution.
</comment><comment author="markwalkom" created="2015-04-16T04:16:37Z" id="93635444">I've been speaking to a few networking firms lately and it's dawned on me that microsecond level is going to be critical for IDS/network analytics.
</comment><comment author="markwalkom" created="2015-06-24T05:10:13Z" id="114729084">Another request from the community - https://discuss.elastic.co/t/increase-the-resolution-of-elastic-timestamps-to-nanoseconds/24227
</comment><comment author="anoinoz" created="2015-06-24T05:14:44Z" id="114729503">that last request is mine I believe. I would add that to monitor networking (and other) activities in our field, nanosecond support is paramount. 
</comment><comment author="abrisse" created="2015-09-04T16:25:17Z" id="137782107">:+1: for this feature
</comment><comment author="jack-pappas" created="2015-09-13T17:43:28Z" id="139899024">What about switching from Joda Time to [date4j](http://www.date4j.net/)? It supports higher-precision timestamps compared to Joda and supposedly the performance is better as well.
</comment><comment author="dadoonet" created="2015-09-13T17:54:08Z" id="139899487">Before looking on the technical side, is BSD License compatible with Apache2 license?
</comment><comment author="dadoonet" created="2015-09-13T18:04:38Z" id="139900286">So BSD is compatible with Apache2.
</comment><comment author="clintongormley" created="2015-09-19T11:59:14Z" id="141657179">I'd like to hear @jpountz's thoughts on this comment https://github.com/elastic/elasticsearch/issues/10005#issuecomment-77479544 about high cardinality with regards to index size and performance.

I could imagine adding a `precision` parameter to date fields which defaults to `ms`, but also accepts `s`, `us`, `ns`.

We would need to move away from Joda, but I wouldn't be in favour of replacing Joda with a different dependency. Instead, we have this issue discussing replacing Joda with Java.time https://github.com/elastic/elasticsearch/issues/12829
</comment><comment author="jpountz" created="2015-09-24T13:24:20Z" id="142926039">@clintongormley It's hard to predict because it depends so much on the data so I ran an experiment for an application that ingests 1M messages at a 2000 messages per second per shard rate.

| Precision | Terms dict (kB) | Doc values (kB) |
| --- | --- | --- |
| milliseconds | 3348 | 2448 |
| microseconds | 10424 | 3912 |

Millisecond precision is much more space-efficient, in particular because with 2k docs per second, several messages are in the same millisecond, but even if we go with 1M messages at a rate of 200 messages per second so that sharing the same millisecond is much more unlikely, there are still significant differences between millisecond and microsecond precision.

| Precision | Terms dict (kB) | Doc values (kB) |
| --- | --- | --- |
| milliseconds | 7604 | 2936 |
| microseconds | 10680 | 4888 |

That said, these numbers are for a single field, the overall difference would be much lower if you include `_source` storage, indexes and doc values for other fields, etc.

Regarding performance, it should be pretty similar.
</comment><comment author="clintongormley" created="2015-09-25T11:33:57Z" id="143191963">From what users have told me, by far the most important reason for storing microseconds is for the sorting of results.  it make no sense to aggregate on buckets smaller than a millisecond.

This can be achieved very efficiently with the two-field approach: one for the date (in milliseconds) and one for the microseconds.  The microseconds field would not need to be indexed (unless you really need to run a range query with finer precision than one millisecond), so all that would be required is doc_values.  Microseconds can have a maximum of 1,000 values, so doc_values for this field would require just 12 bits per document. 

For the above example, that would be only an extra 11kB.

A logstash filter could make adding the separate microsecond field easy.
</comment><comment author="clintongormley" created="2015-09-25T11:37:15Z" id="143192382">Meh - there aren't 1000 bits in a byte.  /me hangs his head in shame.

It would require 1,500kB
</comment><comment author="pfennema" created="2015-09-30T15:27:14Z" id="144446125">If we want to use the ELK framework proper analyzing network latency we really need nanosecond resolution. Are there any firm plans/roadmap to change the timestamps?
</comment><comment author="portante" created="2015-10-01T04:43:17Z" id="144618651">Let's say I index the following JSON document with nanosecond precision timestamps:

```
{ "@timestamp": "2015-09-30T12:30:42.123456789-07:00", "message": "time is running out" }
```

So the internal date representation will be, `2015-09-30T19:30:42.123` UTC, right?

But if I issue a query matching that document, and ask for either the `_source` document or the `@timestamp` field explicitly, won't I get back the original string?  If so, then in cases where the original time string lexicographically sorts the same as the converted time value, would that be sufficient for a client to further sort to get what they need?

Or is there a requirement that internal date manipulations in ES need such nanosecond precision?  I am imagining that if one has records with nanosecond precision, only being able to query for a date range with millisecond precision could potentially result in more document matches than wanted.  Is that the major concern?
</comment><comment author="pfennema" created="2015-10-02T13:59:28Z" id="145029479">I think the latter, internal date manipulations need probably nanosecond precision. Reason is that when  monitoring latency on 10Gb networks we get pcap records (or packets directly from the switch via UDP) which include multiple fields with nanosecond timestamps in the record. We like to find out the difference between the different timestamps in order to optimize our network/software and find correlations. In order to do this we like to zoom in on every single record and not aggregate records.
</comment><comment author="abierbaum" created="2015-10-25T22:13:09Z" id="150979596">:+1: for solving this.  It is causing major issues for us now in our logging infrastructure.
</comment><comment author="clintongormley" created="2015-10-26T10:42:12Z" id="151096664">@pfennema @abierbaum What problems are you having that can't be solved with the two-field solution?
</comment><comment author="pfennema" created="2015-10-26T11:46:17Z" id="151109180">What we like to have is that we have a timescale in the display (Kibana) where we can zoom in on the individual measurements which have a timestamp with nanosecond resolution. A record in our case has multiple fields (NICTimestamp, TransactionTimestamp, etc) which we like to correlate with each other on an individual basis hence not aggregated. We need to see where spikes occur to optimize our environment. If we can have on the x-axis the time in micro/nanosecond resolution we should be able to zoom in on individual measurements.
</comment><comment author="abierbaum" created="2015-10-26T12:27:55Z" id="151117222">@clintongormley Our use case is using ELK to analyze logs from the backend processes in our application.  The place we noticed it was postgresql logs.  With the current ELK code base, even though the logs coming from the database server have the commands in order, once they end up elastic search and are visualized in kibana the order of items happening on the same millisecond are lost.  We can add a secondary sequence number field, but that doesn't work well in Kibana queries (since you can't sort on multiple fields) and causes quite a bit of confusion on the team because they just expect the data in Kibana to be sorted in the same order as it came in from postgresql and logstash.
</comment><comment author="gigi81" created="2015-11-20T09:20:34Z" id="158333795">We have the same problem as @abierbaum described. When events happen on the same millisecond the order of the messages is lost.
Any workaround or suggestion on how to fix this would be really appreciated.
</comment><comment author="dtr2" created="2016-01-17T16:33:03Z" id="172347328">You don't need to increase the timestamp accuracy: instead, the time sorting should be based on both timestamp and ID: message IDs are monotonically increasing, and specifically, they are monotonically increasing for a set of messages with the same timestamp...
</comment><comment author="jcollie" created="2016-01-17T16:46:53Z" id="172348562">@dtr That may be true for IDs that are automatically assigned, but only if the messages are indexed in the correct order in the first place, and only if the application isn't supplying it's own IDs. There's definitely no way that I could depend on that behavior. Also is the monotonically increasing IDs guaranteed, or is it an implementation artifact, especially when considering clusters of more than one node? 
</comment><comment author="dtr2" created="2016-01-18T07:47:45Z" id="172453609">I believe the original intent was to see a bulk of log messages originated from the same source. If a cluster is involved, then probably the timestamp is the only clustering item (unless, of course, there is a special "context" or "session" field)
For that purpose, we can rely on the id (assuming, of course, its monotonically increasing at least per source)
</comment><comment author="jcollie" created="2016-01-18T21:02:01Z" id="172653000">@dtr2 that's a lot of "ifs" to be relying on ElasticSearch's autogenerated IDs, nearly all of which are violated in my cluster:

1) Some messages supply their own ID if the source has a unique ID already associated with it (systemd journal messages in my case).
2) All of my data runs through a RabbitMQ server (sometimes passing through multiple queues depending on the amount of processing that needs to be done) with multiple consumers per queue so there's no way that I can expect documents to be indexed in any specific order, much less by the same ElasticSearch node.

In any case, ElasticSearch does not guarantee the behavior of autogenerated IDs.  The docs only guarantee that the IDs are unique:

https://www.elastic.co/guide/en/elasticsearch/guide/current/index-doc.html

So I can hope that you see that trying to impose an order based upon the ID cannot be relied upon in the general case.  Yes, there may be certain specific instances where that would work, but you'd be relying on undocumented implementation details.
</comment><comment author="dtr2" created="2016-01-18T22:21:07Z" id="172669491">@jcollie , In that case, trying to find a "context" is impossible - unless your data source provides it. The idea was to find a context and filter "related" lines together.
</comment><comment author="bobrik" created="2016-01-18T22:22:18Z" id="172669677">@jcollie "IDs" (in fact they are counters per log source) have to be generated before ingestion, outside of elasticsearch.

Instead of sorting on time you sort on time, counter.
</comment><comment author="jcollie" created="2016-01-19T00:23:13Z" id="172691049">I don't get it - what is the resistance to extending timestamps to nanosecond accuracy?  I realize that would take a lot of work and would likely be a "3.x" feature, but anything else is just a workaround until nanosecond timestamps are available.

Having a counter per log source only really helps correlating messages from the same source, but is really not very useful in correlating messages across sources/systems.

As an example, let's say that I implement this counter in each of my log sources (for example as a logstash plugin).  Then let's say that I have one source that generates 1000 messages per millisecond and another source that generates 100000 messages per millisecond.  There's no way that I could reliably tell what order those messages should be in relative to each source.  That may be an extreme example but I think that it illustrates the point.
</comment><comment author="bobrik" created="2016-01-19T09:31:17Z" id="172790794">&gt; Having a counter per log source only really helps correlating messages from the same source, but is really not very useful in correlating messages across sources/systems.

@jcollie can you tell me how you keep clocks on your machines in perfect sync so nanosecond accuracy starts making sense? Even Google struggles to do so:
- http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/spanner-osdi2012.pdf

&gt; Then let's say that I have one source that generates 1000 messages per millisecond and another source that generates 100000 messages per millisecond. There's no way that I could reliably tell what order those messages should be in relative to each source.

You are right here. There is no way. You can easily see jitter of a few ms between ntp sync between machines in the same rack:

```
19 Jan 09:27:40 ntpdate[11731]: adjust time server 10.36.14.18 offset -0.002199 sec
19 Jan 09:27:50 ntpdate[11828]: adjust time server 10.36.14.18 offset 0.004238 sec
```

On the other hand, you can reliably say in which order messages were processed by a single source:
- Single thread of your program.
- Single queue in logstash.
- Some other strictly ordered sequence (ex: kafka partition).

I'd be happy to be proven wrong, though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weighted centroid for geohash_grid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10004</link><project id="" key="" /><description>Currently when using the geohash_grid we get back a a grid. This is useful, but when drawing a map the best assumption we can make is that the data all falls square in the middle of the, well, square. For example, When mapping a geohash grid over California, a large portion of the population of the Bay Area tends to appear lost at sea when using lower precision geohash_grids.

It would be great to get a high precision "center" for lower precision geohash_grid aggs for situations in which many of the document fall to one edge or another of the bucket.
</description><key id="60020960">10004</key><summary>Weighted centroid for geohash_grid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">rashidkpc</reporter><labels><label>:Geo</label><label>feature</label></labels><created>2015-03-05T21:43:27Z</created><updated>2015-09-11T15:00:51Z</updated><resolved>2015-09-11T15:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-08-12T21:36:31Z" id="130455884">@rashidkpc I expect this feature to be in for 2.0beta2. It currently depends on feature branch: https://github.com/elastic/elasticsearch/tree/enhancement/LuceneGeoPointField-Integration which is blocked by elastic/infra#315.

In the meantime here is an example of the output:

``` javascript
  "aggregations" : {
    "geohashgrid" : {
      "buckets" : [ {
        "key" : "g",
        "doc_count" : 8,
        "centroid" : "[72.37982577458024, -18.86329485476017]"
      }, {
        "key" : "v",
        "doc_count" : 7,
        "centroid" : "[64.23251533415169, 68.72181602753699]"
      }, {
        "key" : "5",
        "doc_count" : 6,
        "centroid" : "[-63.65684769116342, -24.55287636257708]"
      }, {
        "key" : "3",
        "doc_count" : 6,
        "centroid" : "[-19.161130998283625, -103.60427623987198]"
      }, {
        "key" : "j",
        "doc_count" : 5,
        "centroid" : "[-68.90129841165617, 71.04619401041418]"
      } ]
    }
  }
```

The centroid is a GeoJSON compliant point - so lon, lat ordering.

You can try this out if you want, just clone my working branch at: https://github.com/nknize/elasticsearch/tree/feature/10004
</comment><comment author="eskibars" created="2015-09-08T19:28:58Z" id="138676905">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodeNotConnectedException after upgrading to ES 1.4.1 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10003</link><project id="" key="" /><description>[Link to Google Group Conversation](https://groups.google.com/forum/#!starred/elasticsearch/sy8K-48bwbU)

Hi,

We recently upgraded one of our ES Clusters from ES Version 1.1.0 to 1.4.1.

We have dedicated master-data-search deployment in AWS. Cluster settings are same for all the clusters.

Strangely, only in one cluster (6 nodes); we are seeing that nodes are constantly failing to connect to Master node and rejoining back.
It happens all the time, even during idle period (when there are no read or writes).

We keep on seeing following exception in the logs **org.elasticsearch.transport.NodeNotConnectedException**

Because of this, Cluster has slowed down considerably. 

We use kopf plugin for monitoring and it keeps popping up message - "Loading cluster information is talking too long"

There is not much data on individual nodes; almost 80% disk is free. CPU and Heap are doing fine. 

Only difference between this cluster and other clusters, is the number of indices and shards. Other clusters have shards in hundreds and indices in double digit. 

But this cluster has around 5000 shards and close to 250 indices.

Has there been any change in 1.4.1 which can cause reconnection issues between nodes , if number of shards or indices are high ?

Any help will be appreciated !

PS. After rolling back the cluster to 1.3.2 version, things are back to normal.

Thanks,
</description><key id="60018297">10003</key><summary>NodeNotConnectedException after upgrading to ES 1.4.1 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sagarl</reporter><labels><label>feedback_needed</label></labels><created>2015-03-05T21:23:30Z</created><updated>2015-04-26T19:57:08Z</updated><resolved>2015-04-26T19:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:51:53Z" id="77785773">Hi @sagarl 

Does the problem disappear if you stop monitoring the cluster? ie no marvel, no kopf, no other tool which is requesting nodes info or stats? I'm wondering if it is related to this: https://github.com/elasticsearch/elasticsearch/pull/9683
</comment><comment author="sagarl" created="2015-03-09T20:51:44Z" id="77939657">Thanks @clintongormley ... I'll try doing that and will update the thread. 
</comment><comment author="clintongormley" created="2015-04-04T13:25:15Z" id="89577391">Hi @sagarl 

Any news on this?
</comment><comment author="clintongormley" created="2015-04-26T19:57:07Z" id="96428582">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation to calculate the moving average on a histogram aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10002</link><project id="" key="" /><description>This aggregation will calculate the moving average of sibling metrics in histogram-style data (`histogram`, `date_histogram`).  Moving averages are useful when time series data is locally stationary and has a mean that changes slowly over time. 

Seasonal data may need a different analysis, as well as data that is bimodal, "bursty" or contains frequent extreme values (which are not necessarily outliers).

The `movavg` aggregation supports several configurable options:
#### Window Size

The user specifies the `window` size they wish to calculate a moving average for.  E.g. a user may want a 30-day sliding window over a histogram of 90 days total.  

Currently, if there is not enough data to "fill" the window, the moving average will be calculated with whatever is available.  For example, if a user selects 30-day window, days 1-29 will calculate the moving average with between 1-29 days of data.

We could investigate adding more "edge policies", which determine how to handle gaps at the edge of the moving average
#### Weighting Type

Currently, the agg supports four types of weighting:
- `simple`:  A simple (arithmetic) average.  Default.
- `linear`: A linearly weighted average, such that data becomes linearly less important as it gets "older" in the window
- `single_exp`: Single exponentially weighted average (aka EWMA or Brown's Simple Exp Smoothing), such that data becomes exponentially less important as it get's "older". 
- `double_exp`: Double exponentially weighted average (aka Holt-Winters).  Uses two exponential terms:  first smooth data exponentially like `single_exp`, but then apply second corrective smoothing to account for a trend.
#### Todo: Expose alpha and beta

Alpha and beta are parameters which control the behavior of `single_exp` and `double_exp`.
- Alpha: controls how far the single exponential smoothing term lags behind the "turning points" in the mean by 1/alpha periods.  Alpha = 1 means the smoothing term has no memory (period of 1), and emulates a random walk.  Alpha = 0 means the smoothing term has infinite memory and reports the mean of the data
- Beta: Only used in `double_exp`.  Analogous to alpha, but applied to the trend smoothing rather than the data smoothing.
#### Todo: Investigate metric-weighting

It's sometimes useful to weight a time period not by it's distance from the current time, but rather by some metric that happened in that time interval.  E.g. weight by the volume of transactions that happened on that day.

It should be possible to weight based on metrics within the bucket, although it could get complicated if the value is missing.
#### Sample Request

This will calculate a moving average (sliding window of three days) over the sum of prices in each day:

``` json
GET /test/_search?search_type=count
{
   "aggs": {
      "my_date_histo": {
         "date_histogram": {
            "field": "@timestamp",
            "interval": "day"
         },
         "aggs": {
            "the_sum": {
               "sum": {
                  "field": "price"
               }
            },
            "the_movavg": {
               "movavg": {
                  "bucketsPath": "the_sum",
                  "window": 3
               }
            }
         }
      }
   }
}
```
#### Sample Response

``` json
{
   "took": 3,
   "timed_out": false,
   "aggregations": {
      "my_date_histo": {
         "buckets": [
            {
               "key_as_string": "2014-12-01T00:00:00.000Z",
               "key": 1417392000000,
               "doc_count": 1,
               "the_sum": {
                  "value": 1,
                  "value_as_string": "1.0"
               },
               "the_movavg": {
                  "value": 1
               }
            },
            {
               "key_as_string": "2014-12-02T00:00:00.000Z",
               "key": 1417478400000,
               "doc_count": 1,
               "the_sum": {
                  "value": 2,
                  "value_as_string": "2.0"
               },
               "the_movavg": {
                  "value": 1.5
               }
            },
            {
               "key_as_string": "2014-12-04T00:00:00.000Z",
               "key": 1417651200000,
               "doc_count": 1,
               "the_sum": {
                  "value": 4,
                  "value_as_string": "4.0"
               },
               "the_movavg": {
                  "value": 2.3333333333333335
               }
            },
            {
               "key_as_string": "2014-12-05T00:00:00.000Z",
               "key": 1417737600000,
               "doc_count": 1,
               "the_sum": {
                  "value": 5,
                  "value_as_string": "5.0"
               },
               "the_movavg": {
                  "value": 3.6666666666666665
               }
            },
            {
               "key_as_string": "2014-12-08T00:00:00.000Z",
               "key": 1417996800000,
               "doc_count": 1,
               "the_sum": {
                  "value": 8,
                  "value_as_string": "8.0"
               },
               "the_movavg": {
                  "value": 5.666666666666667
               }
            },
            {
               "key_as_string": "2014-12-09T00:00:00.000Z",
               "key": 1418083200000,
               "doc_count": 1,
               "the_sum": {
                  "value": 9,
                  "value_as_string": "9.0"
               },
               "the_movavg": {
                  "value": 7.333333333333333
               }
            }
         ]
      }
   }
}
```
</description><key id="60015331">10002</key><summary>Aggregation to calculate the moving average on a histogram aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T21:02:17Z</created><updated>2017-06-08T17:41:19Z</updated><resolved>2015-04-08T14:50:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-04-08T14:50:56Z" id="90938798">Added in #10024 
</comment><comment author="elnur" created="2017-05-21T16:14:55Z" id="302946433">Anything like that for moving maximum?</comment><comment author="polyfractal" created="2017-06-08T17:41:19Z" id="307175666">@elnur Just opened a PR for this functionality, see #25137 </comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimum_should_match not working as expected in case of fields containing synonyms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10001</link><project id="" key="" /><description>Here is my settings and mappings

Settings

```
PUT test3
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "filter": [
            "lowercase",
            "english_stemmer",
            "my_synonyms"
          ],
          "tokenizer": "standard"
        }
      },
      "filter": {
        "my_synonyms": {
          "type": "synonym",
          "synonyms" : [
                "image, image skincare"
            ]
        },
        "english_stemmer": {
          "type": "stemmer",
          "language": "light_english"
        }
      }
    }
  }
}
```

Mappings

```
PUT test3/type/_mapping
{
  "type": {
    "properties": {
      "name": {
        "type": "string",
        "analyzer": "my_analyzer"
      }
    }
  }
}
```

Data insertion

```
POST /test3/data/_bulk
{"index":{"_id":4}}
{"name":["image skincare"]}
```

Here goes the search query. Irrespective of whatever i give in the second word, query is giving results, though that word is not there in the data itself and when there is 100% match.
Same results even when I change the operator to "and"  

My elastic search version is 1.3.4

```
GET test3/_search?size=100
{
  "query": {
    "multi_match": {
      "query": "image SOMETHINGHERE",
      "fields": [
        "name"
      ],
      "minimum_should_match":"100%",
      "tie_breaker": 0.2
    }
  }
}
```
</description><key id="59999197">10001</key><summary>minimum_should_match not working as expected in case of fields containing synonyms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mallikarjuna91</reporter><labels /><created>2015-03-05T19:04:36Z</created><updated>2015-03-09T00:46:58Z</updated><resolved>2015-03-09T00:46:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:46:58Z" id="77785510">Hi @mallikarjuna91 

The reason for this is revealed by doing a validate-query request, which returns:

```
"+(name:image name:image) +(name:somethinghere name:skincare)"
```

The term `image` is being expanded by your synonyms filter to `[ [image,image], [skincare] ]`, and "skincare" is in the same position as whatever other query term is specified, so both positions always match.

You have to be very careful with multi-word synonyms.  I suggest having a read of this chapter: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/synonyms.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation to calculate the bucket which has the maximum value in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10000</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find the bucket with the maximum value of a specified sub-metric (or the doc_count). The output of the aggregation will be the maximum value and an array of keys for the buckets with that value.
</description><key id="59965494">10000</key><summary>Aggregation to calculate the bucket which has the maximum value in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T15:16:21Z</created><updated>2015-04-29T15:35:53Z</updated><resolved>2015-04-29T15:35:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregation to calculate the bucket which has the minimum value in a given aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9999</link><project id="" key="" /><description>This aggregation will take the output of any multi bucket sibling aggregation and find the bucket with the minimum value of a specified sub-metric (or the doc_count). The output of the aggregation will be the minimum value and an array of keys for the buckets with that value.
</description><key id="59965486">9999</key><summary>Aggregation to calculate the bucket which has the minimum value in a given aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T15:16:19Z</created><updated>2016-05-26T06:26:26Z</updated><resolved>2015-04-30T12:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shuangshui" created="2016-05-26T06:26:26Z" id="221788127">where is the code example? sorry I'm newbie to here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring of the plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9998</link><project id="" key="" /><description>This commit is a cleanup of the current implementation of the PluginManager with the addition of some new features. The main changes concern the move to the CLI infrastructure and the split from one big PluginManager class to multiple &amp; more specialized classes and the addition of a very basic dependency management feature between plugins.

Here a list of all notable changes:
- Move PluginManager to the CLI infrastructure (similar to #7339)
- Split PluginManager in more specialized classes easier to test
- Add PluginRepository interface to manage plugins into a given repository with two implementations (Local &amp; URL based)
- URLPluginRepository supports multiple URL patterns (ex: http://download.elasticsearch.org/[organisation]/[name]/[name]-[version].zip)
- Add PluginDescriptor to describe a plugin
- Support meta.yml file to describe dependencies between plugins
- Move the old HttpDownloadHelper from common package to plugin package since it's only used there
- Rename HttpDownloadHelper to DefaultPluginDownloader, remove unused code, clean few things
- Add "ES-Version" and "User-Agent" HTTP headers to the plugin downloader
- Add PluginResolver to resolve plugins and detects simple conflicts
- One-pass Zip extraction to install a plugin (no more extraction followed by multiple directory moves/copies)
- Removes zip files in src/test/resources
- Tests use zipped plugins created in a in-memory FS
- Update doc and help files (copied from #7339, thanks @dadoonet)
- Make list command output machine-friendly when no plugin is installed (closes #8563)
- Add the permission javax.net.ssl.SSLPermission "setHostnameVerifier" to tests policy file
</description><key id="59962362">9998</key><summary>Refactoring of the plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2015-03-05T14:55:08Z</created><updated>2015-08-07T11:34:04Z</updated><resolved>2015-08-07T09:35:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-03-05T14:55:41Z" id="77377393">To test the dependency feature, I added sample plugins here:
https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/elasticsearch-plugin-a-1.0.0.zip
https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/elasticsearch-plugin-a-1.1.0.zip
https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/elasticsearch-plugin-b-1.0.0.zip
https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/elasticsearch-plugin-b-1.0.1.zip
https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/elasticsearch-plugin-c-1.0.0.zip

Where:
- plugin A has no dependency
- plugin B V1.0.0 depends on A V1.0.0
- plugin B V1.0.1 depends on A V1.1.0
- plugin C V1.0.0 depends on B V1.0.1

Then execute:
`bin/plugin install elasticsearch/plugin-c/1.0.0 --url "https://s3.amazonaws.com/users.elasticsearch.org/tlrx/public/pr-plugins/[organisation]-[name]-[version].zip"`
</comment><comment author="tlrx" created="2015-03-05T14:57:01Z" id="77377669">@dadoonet I assign it to you for review since this is a domain you know well :) I'll be glad to have your comment and suggestion here!
</comment><comment author="s1monw" created="2015-03-05T20:38:03Z" id="77446664">just an idea, can the plugin manager maybe even be it's own project somehow? Would move some code our of core that is fairly self-contained?
</comment><comment author="tlrx" created="2015-03-06T09:16:18Z" id="77528612">@s1monw yes and I think few of us would like to see the PluginManager moving in its own project.

It should be fairly simple if the PluginManager is repackaged as a dependency in elasticsearch and if the scripts bin/plugin are kept in the elasticsearch project.

Removing completely the PluginManager code with scripts from the elasticsearch project is a bit more difficult and will impact the building process of elasticsearch, but it's feasable too.
</comment><comment author="dadoonet" created="2015-03-18T03:58:29Z" id="82729581">@tlrx Could you check what is the impact of #10131 on your PR and may be rebase?
</comment><comment author="drewr" created="2015-03-18T04:53:30Z" id="82738880">Please merge master instead of rebasing.  Also, #10131 would be overwritten by @tlrx change, which is fine.
</comment><comment author="dadoonet" created="2015-04-13T14:52:25Z" id="92387948">@tlrx That's a HUGE change! :) 
I like a lot of changes in this PR. I left some comments. Most of them are more about readability.

Could you look to them and commit your changes? I think we will need someone else to review it as well!
</comment><comment author="tlrx" created="2015-08-07T09:35:52Z" id="128656339">``` java
throw new TooLargePullRequestException()
```

Closed in favor of multiple, reviewable and smaller pull requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeouts to refresh, flush and optimize apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9997</link><project id="" key="" /><description>At the moment when the refresh, flush and optimize apis are executed and there is a cluster block (for example no master block) then these apis fail immediately. It would be nicer if those apis would wait for a defined timeout period and see if the cluster block is resolved within that period. This should be similar in how deal with cluster blocks in the index, update and delete apis (which have a timeout parameter).
</description><key id="59961659">9997</key><summary>Add timeouts to refresh, flush and optimize apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-05T14:50:19Z</created><updated>2016-11-26T19:17:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T19:41:12Z" id="162240355">These requests do now wait instead of failing immediately, but the timeout is not configurable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Added Factory for all MultiBucketAggregations to implement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9996</link><project id="" key="" /><description>This allows things like reducers to add aggregations to buckets without needing to know how to construct the aggregation or bucket itself.
</description><key id="59957797">9996</key><summary>Aggregations: Added Factory for all MultiBucketAggregations to implement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2015-03-05T14:22:19Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-24T15:29:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-05T14:23:00Z" id="77371264">This issue intentionally doesn't have a version label as it will be merged into the feature/aggs_2_0 branch
</comment><comment author="jpountz" created="2015-03-05T15:10:05Z" id="77380240">It confuses me a bit to have another factory which is different from the one which creates aggregators. Maybe these methods should be directly defined on InternalAggregation like reduce?
</comment><comment author="colings86" created="2015-03-05T16:17:35Z" id="77394125">The problem there is that the Histogram and Range aggregation already had a Factory which is actually passed into the Aggregator and Aggregation classes to describe how they should behave (numbers/dates/IPs) so to me it feels cleaner to follow that precedence? I did think about the fact that having two things called Factory for each agg type might be confusing, is this something that could be solved with a different name?
</comment><comment author="jpountz" created="2015-03-05T16:47:32Z" id="77400549">It's not only the name, but also the fact that we already have:
- Aggregator.Parser
- Aggregator.Factory
- Aggregator
- Aggregation
- AggregationStreams.Stream
- BucketStreams.Stream

I would rather like to add some factory methods to InternalAggregation than having a new abstraction (like we have a reduce() method instead of getReducer()). Maybe this way we might not need the 'prototype' argument which would implicitely be 'this' and could also simplify generics?
</comment><comment author="colings86" created="2015-03-17T18:18:25Z" id="82508397">@jpountz I have pushed a commit which moves the factory methods into the Aggregation classes
</comment><comment author="jpountz" created="2015-03-19T03:38:04Z" id="83298845">Just left one comment about the API, otherwise it looks good
</comment><comment author="colings86" created="2015-03-23T07:43:54Z" id="84861936">@jpountz you were right about not needing the prototype argument, I have added a new commit which removes it.
</comment><comment author="jpountz" created="2015-03-24T13:55:46Z" id="85504678">@colings86 Thanks. This makes me wonder if we should move the createBucket method to buckets instead of aggregations and remove the prototype parameter too? Feel free to ignore this comment if it makes the API harder to consume.

Otherwise it looks good to me. I guess we just need some javadocs on these new methods to explain there semantics?
</comment><comment author="colings86" created="2015-03-24T15:04:07Z" id="85542894">@jpountz I added the JavaDocs. I tried to move createBucket as you suggested but it made the generics too complex and the API would be awful IMHO. I think it works a lot better to keep the method on the aggregation for now.

Do you want to take another look or can I go ahead and merge this?
</comment><comment author="jpountz" created="2015-03-24T15:12:38Z" id="85545293">Go ahead!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change QueryTemplateParser to use script naming convention</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9995</link><project id="" key="" /><description>At the moment the QueryTemplateParser uses `query`, `file` and `id` for inline, file and indexed scripts respectively. Since the naming convention for scripts is now `script`, `script_file` and `script_id` (enforced by using the ScriptParameterParser) we should change this API to follow the same convention. We may want to keep `query` as an option for inline scripts as it makes sense in this context but we should support `script` too. `file` and `id` should be replaced with `script_file` and `script_id`.

Original comment in: https://github.com/elasticsearch/elasticsearch/pull/9992/files#r25855691
</description><key id="59938105">9995</key><summary>Change QueryTemplateParser to use script naming convention</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>breaking</label><label>discuss</label><label>enhancement</label></labels><created>2015-03-05T11:18:31Z</created><updated>2015-05-29T17:40:38Z</updated><resolved>2015-05-29T17:40:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-05T18:15:57Z" id="77417754">&gt;  file and id should be replaced with script_file and script_id.

Or `query_file` and `query_id`?  Not sure it is obvious to users that "templates" are a form of scripting.

Also, we can leave `file` and `id` and just mark them as deprecated, no?
</comment><comment author="colings86" created="2015-03-06T08:56:39Z" id="77526287">@clintongormley yep, good point. It should be noted that ScriptParameterParser can still be used for this since it accepts arbitrary prefixes as well as the default `script` prefix. As for deprecation, yes we can but as will the script options we could also deprecate in 1.5 and remove in 2.0?
</comment><comment author="javanna" created="2015-03-20T17:45:28Z" id="84086236">I think we should come up with a `Script` construct that holds all the relevant parsing code, which needs to be streamlined whenever we support scripts in the codebase. `Script` would then hold 'lang', 'type', the script itself etc. and could be passed as a single argument to `ScriptService` etc. instead of the different arguments that we currently have.
</comment><comment author="uboness" created="2015-03-21T00:49:28Z" id="84209632">+1 on the `Script` construct... it should also implement `ToXContent`. 
</comment><comment author="clintongormley" created="2015-05-29T17:40:32Z" id="106882592">Closed in favour of #11164
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix potential NPE in new tracer log if request timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9994</link><project id="" key="" /><description>this was introduced in #9286 and wasn't released yet.
</description><key id="59937274">9994</key><summary>Fix potential NPE in new tracer log if request timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>bug</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T11:10:14Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-03-19T20:32:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-16T18:46:24Z" id="81862667">@spinscale can you perhaps give this a look?
</comment><comment author="s1monw" created="2015-03-18T17:04:27Z" id="83058509">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] update JAVA API with aggregation changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9993</link><project id="" key="" /><description>The Histogram and Range APIs for the aggregations changed so that there was a common interface between he types of Range/Histogram. This PR reflects that change in the Java API docs

Contributes to #9976 
</description><key id="59935856">9993</key><summary>[DOCS] update JAVA API with aggregation changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T10:56:38Z</created><updated>2015-04-20T14:28:34Z</updated><resolved>2015-03-05T11:11:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-05T11:05:33Z" id="77345744">LGTM. Left some comments.
Wondering if you should also link to issue #9976 in your commit message?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup ScriptService &amp; friends in preparation for #6418</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9992</link><project id="" key="" /><description>While working on #6418, I found myself cleaning some things up around scripting code, which I figured would be better to isolate on a different PR, to make the fine grained settings change easier to review.
- Added NAME constants for each script language, avoiding to repeat the same strings all over the place.
- Simplified `compile` method signatures by removing a couple of variants. Note that all of these signatures are going to change again with #6418 as in order to compile/execute a script the caller will need to specify which operation is attempting to execute the script, info that will be provided as an additional mandatory argument.
- Removed double call to ScriptService#verifyDynamicScripting for every indexed or dynamic script.
- Decreased ScriptService inner classes visibility to private (CacheKey, IndexedScript, ApplySettings)
- Moved ScriptService inner classes to the bottom of the class, I think it makes it more readable.
- Resolved some compiler warnings
</description><key id="59934606">9992</key><summary>Cleanup ScriptService &amp; friends in preparation for #6418</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-05T10:44:45Z</created><updated>2015-06-06T17:26:57Z</updated><resolved>2015-03-07T09:30:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-05T11:01:46Z" id="77345245">@javanna Just a reminder here for when the PR is merged to also fix :
- https://github.com/elasticsearch/elasticsearch-lang-javascript
- https://github.com/elasticsearch/elasticsearch-lang-python
- https://github.com/elasticsearch/elasticsearch-lang-mvel

in their `es-1.x` and `master` branches.

I believe there will be changes to port there.

I think also that some river plugins might not compile anymore (test part) as if IIRC we are doing some script tests there (couchdb, rabbitmq).
</comment><comment author="javanna" created="2015-03-05T11:04:12Z" id="77345545">HI @dadoonet not sure what changes you mean that we should port there. In my mind these changes shouldn't break anything in scripting plugins, they shouldn't access the `ScriptService` directly. As for rivers, that might be the case, but then those `compile` signatures would be broken anyway for #6418 , which should be ok.
</comment><comment author="dadoonet" created="2015-03-05T12:08:23Z" id="77353338">@javanna Ignore me. I got confused by Github diff and thought you removed inner classes we are using in plugins. It's not the case. Sorry for the false alarm.
</comment><comment author="javanna" created="2015-03-05T14:50:54Z" id="77376462">I think you read my mind @dadoonet I wanted to do what you feared badly :) but I went for not breaking bw comp. I will probably do that on master only later on.
</comment><comment author="jpountz" created="2015-03-06T09:19:41Z" id="77529063">LGTM
</comment><comment author="dadoonet" created="2015-03-16T00:01:04Z" id="81305854">@javanna It effectively breaks: See https://github.com/elastic/elasticsearch-river-couchdb/blob/master/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java#L145-145

The method signature changed. I need to fix that I guess. :)

We just need to change 

``` java
Maps.newHashMap()
```

to 

``` java
Maps.&lt;String, Object&gt;newHashMap()
```
</comment><comment author="javanna" created="2015-03-16T23:59:36Z" id="81999303">Marked this as breaking. It breaks plugins that make use of scripting by depending on `ScriptService` (e.g. some rivers that allow to transform documents before indexing them). It doesn't break plugins that implement a scripting engine though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI Failure IndicesRequestTests#testFlush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9991</link><project id="" key="" /><description>http://build-us-00.elasticsearch.org/job/es_core_14_medium/1262/

```
Throwable:
  1&gt; java.lang.AssertionError: 
  1&gt; Expected: an empty iterable
  1&gt;      got: &lt;[indices:data/read/search[free_context]=[org.elasticsearch.search.action.SearchServiceTransportAction$SearchFreeContextRequest@4cd7dc77]]&gt;
  1&gt; 
  1&gt;     __randomizedtesting.SeedInfo.seed([DF61E66DA6AA45A5:6BECAC623F37391]:0)
  1&gt;     [...org.junit.*]
  1&gt;     org.elasticsearch.action.IndicesRequestTests.assertAllRequestsHaveBeenConsumed(IndicesRequestTests.java:888)
  1&gt;     org.elasticsearch.action.IndicesRequestTests.cleanUp(IndicesRequestTests.java:162)
```
</description><key id="59929098">9991</key><summary>[CI Failure IndicesRequestTests#testFlush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label></labels><created>2015-03-05T09:57:42Z</created><updated>2015-09-30T10:38:48Z</updated><resolved>2015-09-30T10:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-30T10:38:48Z" id="144350679">I can't repro this anymore, I think we can close for now and reopen if it fails again, given that it didn't happen for a long while it probably got fixed meanwhile.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ubuntu doesn't recommend init.d, they want upstart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9990</link><project id="" key="" /><description>...and yes, upstart will be removed in some future version, but for now, it'd be best if elasticsearch would play nice with the various other services folks run on Ubuntu.
</description><key id="59897633">9990</key><summary>Ubuntu doesn't recommend init.d, they want upstart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">SamuelMarks</reporter><labels><label>:Packaging</label></labels><created>2015-03-05T02:55:26Z</created><updated>2015-06-26T09:14:09Z</updated><resolved>2015-06-26T09:14:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SamuelMarks" created="2015-03-05T02:56:24Z" id="77297344">FYI: I have signed the agreement, and am only contributing code that I have written, except for something I found in a gist (which I referenced).
</comment><comment author="clintongormley" created="2015-03-09T00:33:29Z" id="77784814">Hi @SamuelMarks 

You're saying that Ubuntu recommends upstart, even though they're intending to remove it?  I don't follow...  Could you explain?
</comment><comment author="SamuelMarks" created="2015-03-09T00:44:51Z" id="77785402">Upstart is used across a variety of distributions: http://en.wikipedia.org/wiki/Upstart#Adoption

From Ubuntu 15.04 systemd will be used instead of Upstart. See [this bug](https://bugs.launchpad.net/ubuntu/+source/ubuntu-meta/+bug/1427654) for progress.

Given that the latest [LTS](https://wiki.ubuntu.com/LTS) was early last year, it makes sense to maintain an upstart conf for server components such as elasticsearch.
</comment><comment author="clintongormley" created="2015-04-04T13:10:33Z" id="89574994">@tlrx could you take a look at this please?
</comment><comment author="tlrx" created="2015-06-17T07:38:57Z" id="112694809">Since the latest version of Ubuntu (15.04) has moved to SystemD (and it works) I think we should stick to SystemD only.
</comment><comment author="clintongormley" created="2015-06-18T16:30:45Z" id="113210288">@tlrx can users of Ubuntu 14.04 LTS use systemd already?
</comment><comment author="tlrx" created="2015-06-26T09:14:06Z" id="115595399">Ubuntu 14.04 uses `init.d` scripts and works with the current scripts.

Since Ubuntu 15.04 has moved on SystemD I think we can close this pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ubuntu doesn't recommend init.d, they want upstart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9989</link><project id="" key="" /><description>...and yes, upstart will be removed in some future version, but for now, it'd be best if elasticsearch would play nice with the various other services folks run on Ubuntu.
</description><key id="59897168">9989</key><summary>Ubuntu doesn't recommend init.d, they want upstart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SamuelMarks</reporter><labels /><created>2015-03-05T02:48:26Z</created><updated>2015-03-05T02:55:15Z</updated><resolved>2015-03-05T02:55:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SamuelMarks" created="2015-03-05T02:55:11Z" id="77297230">Signed agreement
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.2.4 is crashing suddenly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9988</link><project id="" key="" /><description>Full dump: https://gist.github.com/rvller/beca1245584e8158a50f

Stack: [0x00007f2e3cbb1000,0x00007f2e3cbf2000],  sp=0x00007f2e3cbf06b0,  free space=253k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 2442 C2 sun.nio.ch.SelectorImpl.select(J)I (34 bytes) @ 0x00007f2e5d6602e8 [0x00007f2e5d6600e0+0x208]
J 2561% C2 org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run()V (409 bytes) @ 0x00007f2e5d315d68 [0x00007f2e5d315b40+0x228]
j  org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run()V+1
j  org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run()V+1
j  org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run()V+55
j  org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run()V+14
j  java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x6000f5]  JavaCalls::call_helper(JavaValue_, methodHandle_, JavaCallArguments_, Thread_)+0x365
V  [libjvm.so+0x5feb58]  JavaCalls::call(JavaValue_, methodHandle, JavaCallArguments_, Thread_)+0x28
V  [libjvm.so+0x5fee27]  JavaCalls::call_virtual(JavaValue_, KlassHandle, Symbol_, Symbol_, JavaCallArguments_, Thread_)+0x197
V  [libjvm.so+0x5fef47]  JavaCalls::call_virtual(JavaValue_, Handle, KlassHandle, Symbol_, Symbol_, Thread_)+0x47
V  [libjvm.so+0x67bf65]  thread_entry(JavaThread_, Thread_)+0xe5
V  [libjvm.so+0x95a08f]  JavaThread::thread_main_inner()+0xdf
V  [libjvm.so+0x95a195]  JavaThread::run()+0xf5
V  [libjvm.so+0x820c28]  java_start(Thread*)+0x108
</description><key id="59877361">9988</key><summary>ES 1.2.4 is crashing suddenly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rvller</reporter><labels><label>feedback_needed</label></labels><created>2015-03-04T23:04:58Z</created><updated>2015-10-30T21:03:09Z</updated><resolved>2015-10-30T21:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cooniur" created="2015-03-05T21:20:22Z" id="77454094">Is it because of SELinux? (enabled in CentOS7 by default)
</comment><comment author="silentmoose" created="2015-03-06T04:34:20Z" id="77505327">Have we tested on different operating systems? It seems a logging class that uses the system properties util would be handy to make debugging much easier.
</comment><comment author="clintongormley" created="2015-03-09T00:30:56Z" id="77784697">@rvller What changed on your system?
</comment><comment author="rvller" created="2015-03-09T08:39:58Z" id="77816769">@clintongormley really nothing was changed before this happened. Same indecies on the same ES. Currently it's up and running. 
Can it be related to the SELinux as @cooniur mentioned? 
</comment><comment author="rvller" created="2015-03-15T20:59:43Z" id="81236970">I 've updated ES to 1.4.3. So it crashed again...

See full log: https://gist.github.com/rvller/3cea11802f861f9e9233

 A fatal error has been detected by the Java Runtime Environment:
  SIGILL (0x4) at pc=0x00007f2e991ce1f0, pid=30884, tid=139837175879424

 JRE version: Java(TM) SE Runtime Environment (7.0_72-b14) (build 1.7.0_72-b14)
 Java VM: Java HotSpot(TM) 64-Bit Server VM (24.72-b04 mixed mode linux-amd64 compressed ops)
 Problematic frame:
 V  [libjvm.so+0x9251f0]  SystemDictionary::resolve_or_null(Symbol_, Thread_)+0x20

 Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c nlimited" before starting Java again

 If you would like to submit a bug report, please visit:
   http://bugreport.sun.com/bugreport/crash.jsp

---------------  T H R E A D  ---------------

Current thread (0x00007f2e94934000):  JavaThread "elasticsearch[Man-Spider][scheduler][T#1]" daemon [_thread_in_vm, id=30907, stack(0x00007f2e612dd000,0x00007f2e6131e000)]

siginfo:si_signo=SIGILL: si_errno=0, si_code=2 (ILL_ILLOPN), si_addr=0x00007f2e991ce1f0

Registers:
RAX=0x0000000000000000, RBX=0x00007f2e94934000, RCX=0x0000000000000000, RDX=0x0000000000000000
RSP=0x00007f2e6131c428, RBP=0x00007f2e6131c460, RSI=0x0000000000000000, RDI=0x00007f2e948db280
R8 =0x00007f2e94934000, R9 =0x0000000000000000, R10=0x00007f2e909c0d6c, R11=0x00007f2e998a1420
R12=0x0000000000000000, R13=0x00007f2e949341e8, R14=0x0000000000000000, R15=0x0000000000000000
RIP=0x00007f2e991ce1f0, EFLAGS=0x0000000000010246, CSGSFS=0x0000000000000033, ERR=0x0000000000000000
  TRAPNO=0x0000000000000006

Top of Stack: (sp=0x00007f2e6131c428)
0x00007f2e6131c428:   00007f2e98f2066a 000000062514fd28
0x00007f2e6131c438:   00007f2e94934000 00007f2e6131c520
0x00007f2e6131c448:   00007f2e94934000 0000000000000000
0x00007f2e6131c458:   00007f2e948db280 00007f2e6131c4f0
0x00007f2e6131c468:   00007f2e98f3102d 00007f2e94934000
0x00007f2e6131c478:   00007f2e3000a2e0 0000000000000000
0x00007f2e6131c488:   00000000000003d8 00007f2e949341e8
0x00007f2e6131c498:   0000000000000000 00007f2e94934000
0x00007f2e6131c4a8:   0000000000000001 00007f2e94934000
0x00007f2e6131c4b8:   0000000000000025 00007f2e6131c4f0
0x00007f2e6131c4c8:   00007f2e6131c520 00007f2e949341e8
0x00007f2e6131c4d8:   00007f2e6131c680 00007f2e6131c520
0x00007f2e6131c4e8:   0000000000000025 00007f2e6131c5d0
0x00007f2e6131c4f8:   00007f2e93de2690 00007f2e6131c520
0x00007f2e6131c508:   0000000000000000 0000000000000000
0x00007f2e6131c518:   0000000004dd5000 6e616c2f6176616a
0x00007f2e6131c528:   6567616e616d2f67 6675422f746e656d
0x00007f2e6131c538:   4d6c6f6f50726566 0000006e61654258
0x00007f2e6131c548:   00007f2e9905de77 00007f2e0000000e
0x00007f2e6131c558:   00007f2efb17ab98 00007f2e30009b40
0x00007f2e6131c568:   00007f2e9492d730 00007f2e30009b30
0x00007f2e6131c578:   00007f2e94934000 0000000000000000
0x00007f2e6131c588:   0000000000000005 00007f2e94934000
0x00007f2e6131c598:   000000000000009a 00007f2e6131c5f0
0x00007f2e6131c5a8:   00000007fae0e9a8 0000000000000000
0x00007f2e6131c5b8:   00000007fae0e9a0 00007f2e6131c680
0x00007f2e6131c5c8:   00007f2e94934000 00007f2e6131c648
0x00007f2e6131c5d8:   00007f2e909c0d98 00007f2e6131c680
0x00007f2e6131c5e8:   00007f2e94934000 000000000000009a
0x00007f2e6131c5f8:   00007f2e622a3cc8 00007f2e6131c630
0x00007f2e6131c608:   00007f2e6131c608 0000000000000000
0x00007f2e6131c618:   00007f2e6131c680 00000007faeb40c8 

Instructions: (pc=0x00007f2e991ce1f0)
0x00007f2e991ce1d0:   55 48 89 f1 31 d2 31 f6 48 89 e5 c9 e9 0f ef ff
0x00007f2e991ce1e0:   ff 90 66 66 66 66 66 2e 0f 1f 84 00 00 00 00 00
0x00007f2e991ce1f0:   55 48 89 e5 48 89 5d d8 4c 89 6d e8 4c 89 c3 4c
0x00007f2e991ce200:   89 75 f0 4c 89 7d f8 49 89 d6 4c 89 65 e0 48 83 

Register to memory mapping:

RAX=0x0000000000000000 is an unknown value
RBX=0x00007f2e94934000 is a thread
RCX=0x0000000000000000 is an unknown value
RDX=0x0000000000000000 is an unknown value
RSP=0x00007f2e6131c428 is pointing into the stack for thread: 0x00007f2e94934000
RBP=0x00007f2e6131c460 is pointing into the stack for thread: 0x00007f2e94934000
RSI=0x0000000000000000 is an unknown value
RDI=0x00007f2e948db280 is an unknown value
R8 =0x00007f2e94934000 is a thread
R9 =0x0000000000000000 is an unknown value
R10=0x00007f2e909c0d6c is at code_begin+620 in an Interpreter codelet
method entry point (kind = native)  [0x00007f2e909c0b00, 0x00007f2e909c1360]  2144 bytes
R11=0x00007f2e998a1420: &lt;offset 0x181420&gt; in /lib64/libc.so.6 at 0x00007f2e99720000
R12=0x0000000000000000 is an unknown value
R13=0x00007f2e949341e8 is an unknown value
R14=0x0000000000000000 is an unknown value
R15=0x0000000000000000 is an unknown value

Stack: [0x00007f2e612dd000,0x00007f2e6131e000],  sp=0x00007f2e6131c428,  free space=253k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x9251f0]  SystemDictionary::resolve_or_null(Symbol_, Thread_)+0x20
V  [libjvm.so+0x68802d]  JVM_FindClassFromCaller+0x26d
C  [libjava.so+0xd690]  Java_java_lang_Class_forName0+0x130
j  java.lang.Class.forName0(Ljava/lang/String;ZLjava/lang/ClassLoader;Ljava/lang/Class;)Ljava/lang/Class;+0
J 6115 C2 sun.reflect.GeneratedMethodAccessor22.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (49 bytes) @ 0x00007f2e917e42ec [0x00007f2e917e3ee0+0x40c]
J 4597 C2 org.elasticsearch.monitor.jvm.JvmStats.jvmStats()Lorg/elasticsearch/monitor/jvm/JvmStats; (1055 bytes) @ 0x00007f2e9178cb24 [0x00007f2e9178be20+0xd04]
J 5314 C2 org.elasticsearch.monitor.jvm.JvmMonitorService$JvmMonitor.monitorLongGc()V (1098 bytes) @ 0x00007f2e91945ec0 [0x00007f2e91945e40+0x80]
j  org.elasticsearch.monitor.jvm.JvmMonitorService$JvmMonitor.run()V+1
J 5941 C2 org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run()V (43 bytes) @ 0x00007f2e9143836c [0x00007f2e91438320+0x4c]
J 5945 C2 java.util.concurrent.FutureTask.runAndReset()Z (128 bytes) @ 0x00007f2e90fdf2c4 [0x00007f2e90fdf200+0xc4]
J 4617 C2 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V (59 bytes) @ 0x00007f2e9175b5ac [0x00007f2e9175b400+0x1ac]
J 3482% C2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00007f2e91406710 [0x00007f2e91406500+0x210]
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x6000f5]  JavaCalls::call_helper(JavaValue_, methodHandle_, JavaCallArguments_, Thread_)+0x365
V  [libjvm.so+0x5feb58]  JavaCalls::call(JavaValue_, methodHandle, JavaCallArguments_, Thread_)+0x28
V  [libjvm.so+0x5fee27]  JavaCalls::call_virtual(JavaValue_, KlassHandle, Symbol_, Symbol_, JavaCallArguments_, Thread_)+0x197
V  [libjvm.so+0x5fef47]  JavaCalls::call_virtual(JavaValue_, Handle, KlassHandle, Symbol_, Symbol_, Thread_)+0x47
V  [libjvm.so+0x67bf65]  thread_entry(JavaThread_, Thread_)+0xe5
V  [libjvm.so+0x95a08f]  JavaThread::thread_main_inner()+0xdf
V  [libjvm.so+0x95a195]  JavaThread::run()+0xf5
V  [libjvm.so+0x820c28]  java_start(Thread*)+0x108
</comment><comment author="clintongormley" created="2015-04-26T19:53:02Z" id="96428416">What OS are you running on?  Could it be OpenVZ? (see https://github.com/elastic/elasticsearch/issues/9582)

Try with SELinux disabled, try upgrading Java, try removing lib/sigar...
</comment><comment author="dakrone" created="2015-10-30T21:03:09Z" id="152650538">Closing since we have not heard back in over 6 months.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Global Settings/Mapping for an Index - not_analyzed and whitespace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9987</link><project id="" key="" /><description>Hello, 
I want to globally set the following for all current string fields and future(new) string fields on an Index while creating that Index.   Thanks. Mka

"index": "not_analyzed", 
"analyzer":"whitespace"
</description><key id="59874738">9987</key><summary>Global Settings/Mapping for an Index - not_analyzed and whitespace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MKA-MKA</reporter><labels /><created>2015-03-04T22:42:00Z</created><updated>2015-03-05T07:45:39Z</updated><resolved>2015-03-05T07:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-03-05T07:45:39Z" id="77320450">Hi,

You should better ask on the mailing for question. We could help you in a better way than here because this space is reserved for issues and feature requests.
Mailing list is here. http://www.elasticsearch.org/help

That said, I believe you should look at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#indices-templates
And
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates

Closing as it's not an issue. Feel free to reopen if you think it is.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also throttle delete by query when merges fall behind</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9986</link><project id="" key="" /><description>Delete by query is extremely costly: it forces a refresh every time, which can create many segments if there is also concurrent indexing.  This changes throttles delete by query so only 1 thread can run at a time if merges are falling behind.
</description><key id="59853138">9986</key><summary>Also throttle delete by query when merges fall behind</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-04T20:02:17Z</created><updated>2015-06-07T17:59:27Z</updated><resolved>2015-03-04T21:43:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-03-04T20:17:04Z" id="77237577">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete index folder if all shards were allocated away from a data only node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9985</link><project id="" key="" /><description>If a folder for an index was created that folder is never deleted from that node unless the index is deleted.
Data only nodes therefore can have empty folders for indices that they do not even have shards for.
This commit makes sure empty folders are cleaned up after all shards have moved away from a data only 
node. The behavior is unchanged for master eligible nodes. 

I changed the `testShardActiveElseWhere` test to sometimes use data only nodes, not sure if this is useful. Also I added the check for `clusterState.metaData().hasIndex(indexName)` in  `IndicesService.deleteIndexStore` again. Did not come up with a better way to solve this. 
</description><key id="59847846">9985</key><summary>Delete index folder if all shards were allocated away from a data only node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-03-04T19:24:31Z</created><updated>2015-03-06T12:44:10Z</updated><resolved>2015-03-05T14:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-05T13:41:42Z" id="77364954">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to restore specific shard(s) from snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9984</link><project id="" key="" /><description>Shard corruption is not common but it can happen and we have seen situations where both primary and replica shards got corrupted.  When this occurs, it will be nice to be able to restore specific shard(s) or a single shard via snapshot restore without having to restore the entire index which can take a long time esp. for indices that are TBs in size (eg. 20+ TB)
</description><key id="59835725">9984</key><summary>Ability to restore specific shard(s) from snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-04T17:57:43Z</created><updated>2016-02-29T20:36:47Z</updated><resolved>2016-02-29T20:36:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmenegatti" created="2015-03-11T22:47:55Z" id="78389601">+1
</comment><comment author="ChrisRimondi" created="2015-05-07T21:11:53Z" id="100020676">+1
</comment><comment author="iostat" created="2015-12-23T20:22:01Z" id="166984032">(see referencing issue below)
</comment><comment author="clintongormley" created="2016-02-29T20:36:47Z" id="190376346">Closing in favour of #15653
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting Elasticsearch to work on Windows 2008 64bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9983</link><project id="" key="" /><description>I am following the below link and when I try to open http://127.0.0.1:9200/ i get a pop-up asking for a program to open 127_0_0_1.json. Please help!!

Thank you
</description><key id="59832945">9983</key><summary>Getting Elasticsearch to work on Windows 2008 64bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apilgrimda</reporter><labels /><created>2015-03-04T17:37:47Z</created><updated>2015-03-09T00:28:10Z</updated><resolved>2015-03-09T00:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:28:10Z" id="77784544">Hi @apilgrimda 

Please ask questions like these on the mailing list. The github issues list is for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequest.templateParam fix: Chery-picked from 1.x branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9982</link><project id="" key="" /><description /><key id="59814808">9982</key><summary>SearchRequest.templateParam fix: Chery-picked from 1.x branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikedias</reporter><labels><label>won't fix</label></labels><created>2015-03-04T15:39:01Z</created><updated>2015-03-20T21:42:06Z</updated><resolved>2015-03-20T20:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T20:42:19Z" id="84139557">we decided to not backport this on https://github.com/elastic/elasticsearch/pull/8255 so I am closing this PR as well. Thanks for opening it anyways.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete operation should ignore finalizing shards on nodes that no longer exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9981</link><project id="" key="" /><description>Related to #9924
</description><key id="59806000">9981</key><summary>Delete operation should ignore finalizing shards on nodes that no longer exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-04T14:38:27Z</created><updated>2015-06-08T00:36:15Z</updated><resolved>2015-03-12T12:30:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-12T18:50:05Z" id="78565268">thanks for merging I reviewed this yesterday with igor 1 on 1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GATEWAY] copy translog file if rename fails after retries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9980</link><project id="" key="" /><description>Today we ignore a translog file if the rename operation fails. This can be problematic
on windows if another process hodls on to the translog file. If we can't rename it we should
at least try to copy it since otherwise its content will just be lost.
This is a workaround for an already fixed issue in 2.0 since all translog files are write
once in 2.0 and renaming / copying is not needed anymore.
</description><key id="59803498">9980</key><summary>[GATEWAY] copy translog file if rename fails after retries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2015-03-04T14:20:12Z</created><updated>2015-03-19T10:53:29Z</updated><resolved>2015-03-04T14:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-04T14:23:09Z" id="77165688">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sum Aggregation On Nested Doc Not Handling Negative Numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9979</link><project id="" key="" /><description>I have a an index with 30M documents, spread across 4 nodes, 30 shards, with 2 replicas each. Each document has a nested mapping type "a", representing a transaction log entry. Each transaction log entry has a positive or negative dollar value and a timestamp. When I attempt to bucket the nested objects using the date range and sum aggregations, the sum aggregation appears to break on transactions with both positive and negative numbers.

For example, consider the following query. I include a reverse nested agg to show that the nested objects are different than the output of the sum agg.

``` JSON
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "match_all": {}
            },
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": "2011-10-01"
                                }
                            }
                        },
                        {
                            "term": {
                                "bar.name": "ASpecialName"
                            }
                        }
                    ]
                }
            }
        }
    },
    "aggs": {
        "BAR": {
            "terms": {
                "field": "bar.name",
                "size": 2,
                "order": {
                    "NESTED&gt;TOTAL": "asc"
                }
            },
            "aggs": {
                "NESTED": {
                    "nested": {
                        "path": "a"
                    },
                    "aggs": {
                        "TOTAL": {
                            "sum": {
                                "field": "a.obligatedamount"
                            }
                        },
                        "DATES": {
                            "date_range": {
                                "field": "a.signeddate",
                                "keyed": true,
                                "ranges": [
                                    {
                                        "key": "FY2012",
                                        "from": "2011-10-01",
                                        "to": "2012-09-30"
                                    },
                                    {
                                        "key": "FY2013",
                                        "from": "2012-10-01",
                                        "to": "2013-09-30"
                                    },
                                    {
                                        "key": "FY2014",
                                        "from": "2013-10-01",
                                        "to": "2014-09-30"
                                    },
                                    {
                                        "key": "FY2015",
                                        "from": "2014-10-01",
                                        "to": "2015-09-30"
                                    }
                                ]
                            },
                            "aggs": {
                                "DATEBUCKET_SUBTOTAL": {
                                    "sum": {
                                        "field": "a.obligatedamount"
                                    }
                                },
                                "HITS_REVERSE": {
                                    "reverse_nested": {},
                                    "aggs": {
                                        "HITS": {
                                            "top_hits": {
                                                "_source": {
                                                    "include": [
                                                        "a.obligatedamount",
                                                        "a.signeddate"
                                                    ]
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

The following result is returned. Note the odd number in the DATEBUCKET_SUBTOTAL in FY2012 for "ASpecialPlace". This appears to only be an issue when the transaction log contains both negative and positive numbers. (Response slightly truncated...)

``` JSON
{
    "aggregations": {
        "BAR": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "ASpecialPlace",
                    "doc_count": 2,
                    "NESTED": {
                        "doc_count": 3,
                        "DATES": {
                            "buckets": {
                                "FY2012": {
                                    "from_as_string": "2011-10-01T00:00:00.000Z",
                                    "to_as_string": "2012-09-30T00:00:00.000Z",
                                    "doc_count": 1,
                                    "HITS_REVERSE": {
                                        "doc_count": 1,
                                        "HITS": {
                                            "hits": {
                                                "total": 1,
                                                "max_score": 1,
                                                "hits": [
                                                    {
                                                        "_index": "myIndex",
                                                        "_type": "myType",
                                                        "_id": "AG9J61P110025",
                                                        "_score": 1,
                                                        "_source": {
                                                            "a": [
                                                                {
                                                                    "signeddate": "2011-02-01T00:00:00+0000",
                                                                    "obligatedamount": 4000
                                                                },
                                                                {
                                                                    "signeddate": "2012-07-11T00:00:00+0000",
                                                                    "obligatedamount": -1694
                                                                }
                                                            ]
                                                        }
                                                    }
                                                ]
                                            }
                                        }
                                    },
                                    "DATEBUCKET_SUBTOTAL": {
                                        "value": -8.365e-321
                                    }
                                },
                                "FY2013": {
                                    "from_as_string": "2012-10-01T00:00:00.000Z",
                                    "to": 1380499200000,
                                    "doc_count": 0,
                                    "HITS_REVERSE": {
                                        "doc_count": 0,
                                        "HITS": {
                                            "hits": {
                                                "total": 0,
                                                "max_score": null,
                                                "hits": []
                                            }
                                        }
                                    },
                                    "DATEBUCKET_SUBTOTAL": {
                                        "value": 0
                                    }
                                },
                                "FY2014": {
                                    "from_as_string": "2013-10-01T00:00:00.000Z",
                                    "to_as_string": "2014-09-30T00:00:00.000Z",
                                    "doc_count": 1,
                                    "HITS_REVERSE": {
                                        "doc_count": 1,
                                        "HITS": {
                                            "hits": {
                                                "total": 1,
                                                "max_score": 1,
                                                "hits": [
                                                    {
                                                        "_index": "myIndex",
                                                        "_type": "myType",
                                                        "_id": "AG04GGP140011",
                                                        "_score": 1,
                                                        "_source": {
                                                            "a": [
                                                                {
                                                                    "signeddate": "2013-12-19T00:00:00+0000",
                                                                    "obligatedamount": 3449
                                                                }
                                                            ]
                                                        }
                                                    }
                                                ]
                                            }
                                        }
                                    },
                                    "DATEBUCKET_SUBTOTAL": {
                                        "value": 3449
                                    }
                                },
                                "FY2015": {
                                    "from_as_string": "2014-10-01T00:00:00.000Z",
                                    "to_as_string": "2015-09-30T00:00:00.000Z",
                                    "doc_count": 0,
                                    "HITS_REVERSE": {
                                        "doc_count": 0,
                                        "HITS": {
                                            "hits": {
                                                "total": 0,
                                                "max_score": null,
                                                "hits": []
                                            }
                                        }
                                    },
                                    "DATEBUCKET_SUBTOTAL": {
                                        "value": 0
                                    }
                                }
                            }
                        },
                        "TOTAL": {
                            "value": 3449
                        }
                    }
                }
            ]
        }
    }
}
```

If I set a filter at the nested level to remove any nested "a" objects with a negative value, everything works fine. Note, this behavior is also apparent if using a stats agg instead of just a sum.  
</description><key id="59802892">9979</key><summary>Sum Aggregation On Nested Doc Not Handling Negative Numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">natenash203</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label></labels><created>2015-03-04T14:15:42Z</created><updated>2015-03-16T21:14:18Z</updated><resolved>2015-03-16T16:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:23:27Z" id="77784338">@martijnvg @jpountz any idea where things might be going wrong here?
</comment><comment author="jpountz" created="2015-03-16T04:13:18Z" id="81399361">The exponent of the sum is suspicious (-321), it's typically what you would get when interpreting bits of a small long as a double. For instance `Double.longBitsToDouble(2000)` returns 9.88E-321. So I'm wondering if this could be this mapping issue we have seen a couple of times where shards would mistakenly interpret long as doubles because of inconsistent mappings across shards.
</comment><comment author="jpountz" created="2015-03-16T04:37:00Z" id="81407735">I quickly tried to reproduce the issue with no success. Here is the script I used:

```
curl -XDELETE 'localhost:9200/test'

curl -XPUT 'localhost:9200/test' -d '
{
  "mappings": {
    "test": {
      "properties": {
        "a": {
          "type": "nested",
          "properties": {
            "d": {
              "type": "date"
            },
            "v": {
              "type": "long"
            }
          }
        }
      }
    }
  }
}'

curl -XPUT 'localhost:9200/test/test/1' -d '
{
  "a": [
    {
      "d": 10000000,
      "v": 4000
    },
    {
      "d": 20000000,
      "v": -1694
    }
  ]
}'

curl -XPOST 'localhost:9200/test/_refresh'

curl -XGET 'localhost:9200/test/_search' -d '
{
  "aggs": {
    "NESTED": {
      "nested": {
        "path": "a"
      },
      "aggs": {
        "TOTAL": {
          "sum": {
            "field": "a.v"
          }
        },
        "HISTO": {
          "date_histogram": {
            "field": "a.d",
            "interval": "day"
          },
          "aggs": {
            "TOTAL2": {
              "sum": {
                "field": "a.v"
              }
            }
          }
        }
      }
    }
  }
}'
```
</comment><comment author="jpountz" created="2015-03-16T04:43:28Z" id="81410075">@natenash203 Could you let us know which elasticsearch version you are running and whether you rely on dynamic mappings?
</comment><comment author="natenash203" created="2015-03-16T14:25:44Z" id="81698938">@jpountz Currently running 1.4.4. However, I am pretty sure I was running 1.4.0 when I initially created the index, set the mappings, and indexed the data. 

W/re to mapping, I use a mix of explicit and dynamic mappings. The field in question however, is mapped dynamically.  Currently, the index is reporting it's mapped as a double.

For reference, the range of possible values for the "v" field go from roughly -11,000,000,000.00 to 11,000,000,000.00 and as they are dollars, will include decimal places to the hundreds (i.e 12.34).

Do you think I should explicitly map the "v" field to a float? Perhaps double, but isn't that not good for currency values?   
</comment><comment author="jpountz" created="2015-03-16T16:26:35Z" id="81776196">OK, then I probably know what happened:
- two documents have been indexed on two different shards at about the same time
- one shard mapped the field as a long and the other one as a double
- the double mapping was the first one propagated to the master node so it "won"
- later the shard mapped with a long has been relocated to somewhere else where the field was mapped as a double and so its indexed fields are now interpreted as a double although they were indexed as a long

This is unfortunately a know issue, see #8688 for more background.

&gt; Do you think I should explicitly map the "v" field to a float? Perhaps double, but isn't that not good for currency values?

Explicit mapping is certainly a good idea as it avoids running into #8688. Regarding the type of the field, a good option is usually to store the number of cents as a long in order to avoid floating-point rounding issues. 64 bits would be more than enough to store the range of values you are interested in.

Closing as a duplicate of #8688
</comment><comment author="natenash203" created="2015-03-16T21:14:18Z" id="81944227">After looking at #8688, that looks right to me. When I initially indexed, my "v" values were sometimes floats and sometimes doubles. I index at about 1000 docs/sec so with 30 shards, it's entirely possible, one shard dynamically mapped a float and another mapped a double.

I will reindex with an explicit mapping and go from there. Thanks much for the help.    
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove lowercase_expanded_terms option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9978</link><project id="" key="" /><description>for the record, i think we should remove this lowercasing option completely in parsers, disable it, and let the analysis chain take care. For multitermqueries, its a little tricky, which subset of the filters should be used? For example lowercasing is reasonable, stemming is not. 

But lucene already annotates each analyzer component deemed to be "reasonable" for wildcards with a marker interface (MultiTermAwareComponent). Things like lowercasefilter have it and things like stemmers dont have it.  This is enough to build a "chain", automatically from the query analyzer, that acts reasonably for multitermqueries. 

I know we don't use the lucene factories (es has its own), but we have a table that maps between them, i know because its in a test I wrote. So the information is there :)

All queryparsers have hooks (e.g. factory methods for prefix/wildcard/range) that make it possible to use this, for example solr does it by default, as soon as it did this, people stopped complaining about confusing behavior: both for the unanalyzed, and the analyzed case. it just works.

Sorry for the long explanation.

Compare:
- http://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilterFactory.java?view=markup
- http://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilterFactory.java?view=markup
</description><key id="59799133">9978</key><summary>Remove lowercase_expanded_terms option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-03-04T13:45:15Z</created><updated>2016-11-21T13:19:06Z</updated><resolved>2016-11-02T13:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-03-04T13:45:48Z" id="77159618">background: #9973 (i copied my comment here)
</comment><comment author="dakrone" created="2015-03-31T16:03:54Z" id="88143961">@rmuir can you explain a little bit more how the `MultiTermAwareComponent` should be used? When I look for usages of it in the Lucene 4.x branch, the only users are tests and Solr.

Looking at the `QueryBuilder`, it looks like it uses the analyzer to get the tokenStream for the text to get tokens, but that this doesn't seem to interface with choosing whether or not to do things like stemming? Do you think this is something that we should hold off on doing until we figure out how to exclude things like stemming from the analysis chain when analyzing expanded terms, or should we let the user deal with it by specifying a separate query-time analyzer?

(I originally merged a change for this that did it in the generic way but then reverted it when Ryan reminded my about the MultiTermAwareComponent part of this)
</comment><comment author="rmuir" created="2015-09-01T11:47:45Z" id="136685521">&gt; @rmuir can you explain a little bit more how the MultiTermAwareComponent should be used?

You create a separate chain (analyzer) for multitermqueries by iterating thru the query analysis chain. implicitly the tokenizer should be keywordtokenizer.

imagine query analysis chain is: lowercase + snowball. you look at lowercase (http://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilterFactory.java?view=markup) and with instanceof you see it has that interface, so you call getMultiTermComponent() and add that to the chain. On the other hand snowball does not implement it, so nothing gets added.
</comment><comment author="javanna" created="2015-09-01T11:49:45Z" id="136685800">As a side note, if we remove the `lowercase_expanded_terms` option we should be able to remove the `locale` parameter from simple_query_string and query_string as well, that would help with solving #13229 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search sort on number filed gives error </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9977</link><project id="" key="" /><description>Facing problem when sorting an integer parameter Elastic search 1.3.7, status variable is causing problem, I have status field with different mapping in another collection. Does this effect?
Mapping 
{
"outboxprov1": {
"mappings": {
"deleted_user": {
"properties": {
"status": {
"type": "integer"
}
}
}
}
}
}
Query 
POST /outboxprov1/deleted_user/_search
{
"sort": [
{
"status": {
"order": "desc"
}
}
], 
"query": {
"match_all": {
}
}
}
Error

{
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[0kPf5NYmRlC15ov89wjQ2g][outboxprov1][0]: QueryPhaseExecutionException[[outboxprov1][0]: query[ConstantScore(cache(_type:user))],from[0],size[10],sort[!]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }{[0kPf5NYmRlC15ov89wjQ2g][outboxprov1][1]: QueryPhaseExecutionException[[outboxprov1][1]: query[ConstantScore(cache(_type:user))],from[0],size[10],sort[!]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }{[0kPf5NYmRlC15ov89wjQ2g][outboxprov1][2]: QueryPhaseExecutionException[[outboxprov1][2]: query[ConstantScore(cache(_type:user))],from[0],size[10],sort[!]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }{[0kPf5NYmRlC15ov89wjQ2g][outboxprov1][3]: QueryPhaseExecutionException[[outboxprov1][3]: query[ConstantScore(cache(_type:user))],from[0],size[10],sort[!]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }{[0kPf5NYmRlC15ov89wjQ2g][outboxprov1][4]: QueryPhaseExecutionException[[outboxprov1][4]: query[ConstantScore(cache(_type:user))],from[0],size[10],sort[!]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }]",
"status": 500
}
</description><key id="59792905">9977</key><summary>Elastic search sort on number filed gives error </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dibeesh</reporter><labels /><created>2015-03-04T12:46:37Z</created><updated>2015-03-09T00:17:31Z</updated><resolved>2015-03-09T00:17:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:17:31Z" id="77784052">@dibeesh please see https://github.com/elasticsearch/elasticsearch/issues/9191#issuecomment-77784022
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Are there outdated APIs  (e.g., DateHistogram) in Java API docs? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9976</link><project id="" key="" /><description>I am checking the APIs in Java API docs, and it seems that I found outdated APIs. I hope some contributors may confirm whether the followings are outdated or not: 
1. QueryBuilders.queryString
   https://github.com/elasticsearch/elasticsearch/blob/master/docs/java-api/search.asciidoc. The page includes the following code snippet:
   MultiSearch API &#8230;

``` java
SearchRequestBuilder srb1 = node.client()
     .prepareSearch().setQuery(QueryBuilders.queryString("elasticsearch")).setSize(1);
```

==&gt; I found that the QueryBuilders class does not have the queryString method.
1. DateHistogram
   https://github.com/elasticsearch/elasticsearch/blob/master/docs/java-api/aggs.asciidoc. The page includes the following code snippet:
   Structuring aggregations &#8230;

``` java
SearchResponse sr = node.client().prepareSearch()     
   .addAggregation(               
      AggregationBuilders.terms("by_country").field("country")         
      .subAggregation(AggregationBuilders.dateHistogram("by_year")             
      .field("dateOfBirth")             
      .interval((DateHistogram.Interval.YEAR)             
 .subAggregation(AggregationBuilders.avg("avg_children").field("children"))         )     
)     
.execute().actionGet();
```

==&gt; I found that the DateHistogram class was deleted in Jan. 27th, this year. For the class, the following pages also should be checked: https://github.com/elasticsearch/elasticsearch/blob/master/docs/java-api/aggregations/bucket/datehistogram-aggregation.asciidoc
1. DateRange
   https://github.com/elasticsearch/elasticsearch/blob/master/docs/java-api/aggregations/bucket/daterange-aggregation.asciidoc.
   Use aggregation response
   Import Aggregation definition classes:

``` java
import org.elasticsearch.search.aggregations.bucket.range.date.DateRange;
// sr is here your SearchResponse object 
DateRange agg = sr.getAggregations().get("agg");  
// For each entry 
for (DateRange.Bucket entry : agg.getBuckets()) {     
   String key = entry.getKey();                    
   // Date range as key     
   DateTime fromAsDate = entry.getFromAsDate();    
   // Date bucket from as a Date     
   DateTime toAsDate = entry.getToAsDate();        
   // Date bucket to as a Date     
   long docCount = entry.getDocCount();            
   // Doc count      
   logger.info("key [{}], from [{}], to [{}], doc_count [{}]", key, fromAsDate, toAsDate, docCount); }
```

==&gt; I found that the DateRange class was deleted in Jan. 27th, this year, and does not exist anymore. 
1. IPv4Range
   https://github.com/elasticsearch/elasticsearch/blob/master/docs/java-api/aggregations/bucket/iprange-aggregation.asciidoc
   Use aggregation response
   Import Aggregation definition classes:

``` java
import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4Range;
// sr is here your SearchResponse object 
IPv4Range agg = sr.getAggregations().get("agg");  
// For each entry 
for (IPv4Range.Bucket entry : agg.getBuckets()) {
     String key = entry.getKey();                    
     // Ip range as key     
     String fromAsString = entry.getFromAsString();  
     // Ip bucket from as a String     
     String toAsString = entry.getToAsString();      
     // Ip bucket to as a String     
     long docCount = entry.getDocCount();            
     // Doc count      
     logger.info("key [{}], from [{}], to [{}], doc_count [{}]", key, fromAsString, toAsString, docCount); }
```

==&gt; I found that the IPv4Range class was also deleted in Jan. 27th, this year. 

Thanks in advance,
Seonah
</description><key id="59787861">9976</key><summary>Docs: Are there outdated APIs  (e.g., DateHistogram) in Java API docs? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">saleese</reporter><labels><label>docs</label></labels><created>2015-03-04T11:53:56Z</created><updated>2015-06-16T07:45:52Z</updated><resolved>2015-06-16T07:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-03-04T12:23:07Z" id="77148629">Thanks for reporting this. I'll check that. We might have missed some part of the doc while doing #8667

Note that depending on elasticsearch version you are using, API might change.

Current production version is 1.4 and Java Guide is here: http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/index.html

But if you compare to master source code, you need to look at documentation in http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/master/index.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>All index info lost when the original master machines are not participating in master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9975</link><project id="" key="" /><description>It appears to be as resiliency problem, and i haven't seen it mentioned any place obvious so hopeful it could get fixed.

We had a problem recently about master nodes failing, so I enabled my data nodes to serve as master too - dualnode. However, even though the cluster name is the same, i don't see any of my previous indexes anymore! After quite a while I realize probably because the global cluster status is saved on master node's disk, so the new dualnode doesn't know. I switched back to the old masters when they come back up, then indexes appeared again, but many of them are in red state....

My question would be, since the index and segments are still on the data node, why can't the dualnode detect the indexes that's already in the cluster?

This is really a nasty trap...
</description><key id="59754101">9975</key><summary>All index info lost when the original master machines are not participating in master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkliu</reporter><labels /><created>2015-03-04T06:02:06Z</created><updated>2015-03-09T00:07:27Z</updated><resolved>2015-03-09T00:07:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:07:27Z" id="77783614">Hi @mkliu 

Yes agreed. We are working on a change which will write the cluster state to data-only nodes too, to avoid this problem.  See https://github.com/elasticsearch/elasticsearch/pull/9952
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Ping_timeout inconsistent documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9974</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html

Is the default value 3s or 30s?

"ping_timeout - How long to wait for a ping response, defaults to 30s."

"The discovery.zen.ping_timeout (which defaults to 3s) allows for the tweaking of election time"
</description><key id="59728754">9974</key><summary>Zen Ping_timeout inconsistent documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dharshanr</reporter><labels /><created>2015-03-04T00:05:53Z</created><updated>2015-03-09T00:02:18Z</updated><resolved>2015-03-09T00:02:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-09T00:02:17Z" id="77783413">These refer to two different settings:

```
discovery.zen.ping_timeout  (default 3s)
discovery.zen.fd.ping_timeout (default 30s)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard search on not_analyzed field behaves inconsistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9973</link><project id="" key="" /><description>I am seeing inconsistent behavior with wildcard searches that makes no sense.  I've created a Play to reproduce the issue I'm seeing here - https://www.found.no/play/gist/e452c1d68d6465540d85

For two simple documents:
## name: "7000"

name: "T100"

With a simple not_analyzed mapping:

type:
    properties: 
        name: 
            type: string
            index: not_analyzed

The query for "name:7_" matches a single document (as it should), but a query for "name:T_" does not match a document.  I'm seeing this bug in ES versions 1.3.2 and 1.4.4.

Trying various searches and documents, it appears that wildcarding starting with a numeric-looking string works, but starting with an alpha character (e.g. "T") fails to get any hits.
</description><key id="59715028">9973</key><summary>Wildcard search on not_analyzed field behaves inconsistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdutton</reporter><labels /><created>2015-03-03T22:15:22Z</created><updated>2015-03-04T08:16:44Z</updated><resolved>2015-03-04T02:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdutton" created="2015-03-03T22:17:03Z" id="77048663">Sorry for the terrible formatting.  The YAML snippets got butchered by markdown.
</comment><comment author="dakrone" created="2015-03-03T22:35:17Z" id="77052248">Here's a reproduction (just copying here in case the link doesn't work someday)

# Create an index&lt;a id="sec-1" name="sec-1"&gt;&lt;/a&gt;

```
DELETE /9973
{}

POST /9973
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "doc": {
      "properties": {
        "name": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}
```

# Index docs&lt;a id="sec-2" name="sec-2"&gt;&lt;/a&gt;

```
POST /9973/doc/1
{"name": "7000"}

POST /9973/doc/2?refresh
{"name": "T100"}
```

# Query&lt;a id="sec-3" name="sec-3"&gt;&lt;/a&gt;

This query matches correctly:

```
POST /9973/_search?pretty
{
  "query": {
    "query_string": {
      "query": "name:7*"
    }
  }
}
```

Results:

```
{
  "took" : 65,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "9973",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name": "7000"}
    } ]
  }
}
```

This one does not:

```
POST /9973/_search?pretty
{
  "query": {
    "query_string": {
      "query": "name:T*"
    }
  }
}
```

Results:

```
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```
</comment><comment author="dakrone" created="2015-03-03T23:15:34Z" id="77060818">Interestingly, using an actual wildcard query here does the right thing (as well as using a `simple_query_string` query), so the issue is only with the `query_string` query:

``` json
POST /9973/_search?pretty
{
  "query": {
    "wildcard": {
      "name": "T*"
    }
  }
}
```
</comment><comment author="dakrone" created="2015-03-03T23:41:18Z" id="77064261">@jdutton okay, @s1monw figured out what was going on here - since the `name` field is not analyzed, the token is "T100", however, the `query_string` query has the `lowercase_expanded_terms` option, which defaults to `true`, which causes it to search for "`t*`" instead of "`T*`".

This works as intended for me:

``` json
POST /9973/_search?pretty
{
  "query": {
    "query_string": {
      "query": "name:T*",
      "lowercase_expanded_terms": false
    }
  }
}
```
</comment><comment author="jdutton" created="2015-03-04T02:20:49Z" id="77085249">OK, wow thank you all for the help and fast response.  In my case I was really filtering, and this was a surprising and undesirable behavior.  But then again, in other cases (e.g. search bar) lowercasing would be the desired behavior.

I have to do some soul searching now on how to change my application :-)  Thanks again for the help!
</comment><comment author="rmuir" created="2015-03-04T02:37:18Z" id="77086729">for the record, i think we should remove this lowercasing option completely in parsers, disable it, and let the analysis chain take care. For multitermqueries, its a little tricky, which subset of the filters should be used? For example lowercasing is reasonable, stemming is not. 

But lucene already annotates each analyzer component deemed to be "reasonable" for wildcards with a marker interface (MultiTermAwareComponent). Things like lowercasefilter have it and things like stemmers dont have it.  This is enough to build a "chain", automatically from the query analyzer, that acts reasonably for multitermqueries. 

I know we don't use the lucene factories (es has its own), but we have a table that maps between them, i know because its in a test I wrote. So the information is there :)

All queryparsers have hooks (e.g. factory methods for prefix/wildcard/range) that make it possible to use this, for example solr does it by default, as soon as it did this, people stopped complaining about confusing behavior: both for the unanalyzed, and the analyzed case. it just works.

Sorry for the long explanation.

Compare:
- http://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilterFactory.java?view=markup
- http://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilterFactory.java?view=markup
</comment><comment author="s1monw" created="2015-03-04T08:16:44Z" id="77114387">@rmuir +1 to remove the option can you open an issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing request failure during master re-elect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9972</link><project id="" key="" /><description>Using v1.4.1 on 2 clusters, one 8-node(1 primary, 3 replica) and a 2-node cluster (1 primary, 1 replica). When we shutdown the master, it takes 3-3.5 seconds to elect new master. During this time we see indexing failure on the still online node with response - 

&gt; {"error":"ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/2/no master];]","status":503}

For e.g timeline for 2 node cluster - two machines es1 and es2(old master which was shut down)

```
[2015-02-22 01:15:02,448][DEBUG][discovery.zen.fd         ] [es1.node2] [master] stopping fault detection against master [[es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true}], reason [master left (reason = shut_down)]
[2015-02-22 01:15:02,448][DEBUG][cluster.service          ] [es1.node2] cluster state updated, version [2649], source [zen-disco-master_failed ([es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true})]
[2015-02-22 01:15:02,448][INFO ][cluster.service          ] [es1.node2] removed {[es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true},}, reason: zen-disco-master_failed ([es2.node2][vy1Q05dARpe96RTM_c0kFw][bgolnybcitap01][inet[/10.126.44.92:9311]]{datacenter=ny, master=true})
[2015-02-22 01:15:02,448][DEBUG][cluster.service          ] [es1.node2] set local cluster state to version 2649
[2015-02-22 01:15:02,450][DEBUG][transport.netty          ] [es1.node2] disconnecting from [[es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true}] due to explicit disconnect call
[2015-02-22 01:15:02,453][DEBUG][cluster.service          ] [es1.node2] processing [zen-disco-master_failed ([es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true})]: done applying updated cluster_state (version: 2649)
[2015-02-22 01:15:02,469][WARN ][action.index             ] [es1.node2] Failed to perform indices:data/write/index on remote replica [es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true}[spend_20150215][0]org.elasticsearch.transport.NodeDisconnectedException: [es2.node2][inet[/10.126.44.92:9311]][indices:data/write/index[r]] disconnected
[2015-02-22 01:15:02,469][WARN ][action.index             ] [es1.node2] Failed to perform indices:data/write/index on remote replica [es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true}[spend_20150215][4]org.elasticsearch.transport.NodeDisconnectedException: [es2.node2][inet[/10.126.44.92:9311]][indices:data/write/index[r]] disconnected
[2015-02-22 01:15:02,469][WARN ][action.index             ] [es1.node2] Failed to perform indices:data/write/index on remote replica [es2.node2][vy1Q05dARpe96RTM_c0kFw][es2][inet[/10.126.44.92:9311]]{datacenter=ny, master=true}[spend_20150215][1]org.elasticsearch.transport.NodeDisconnectedException: [es2.node2][inet[/10.126.44.92:9311]][indices:data/write/index[r]] disconnected
[2015-02-22 01:15:02,473][WARN ][cluster.action.shard     ] [es1.node2] can't send shard failed for [spend_20150215][4], node[vy1Q05dARpe96RTM_c0kFw], [R], s[STARTED]. no master known.
[2015-02-22 01:15:02,473][WARN ][cluster.action.shard     ] [es1.node2] can't send shard failed for [spend_20150215][0], node[vy1Q05dARpe96RTM_c0kFw], [R], s[STARTED]. no master known.
[2015-02-22 01:15:02,473][WARN ][cluster.action.shard     ] [es1.node2] can't send shard failed for [spend_20150215][1], node[vy1Q05dARpe96RTM_c0kFw], [R], s[STARTED]. no master known.

&gt;&gt; Indexing request comes at 22FEB2015_01:15:02.592 and fails immediately 

[2015-02-22 01:15:05,454][DEBUG][discovery.zen            ] [es1.node2] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2015-02-22 01:15:05,454][DEBUG][cluster.service          ] [es1.node2] processing [zen-disco-join (elected_as_master)]: execute
[2015-02-22 01:15:05,472][DEBUG][gateway.local            ] [es1.node2] [spend_phrases_20141120][0]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-02-22 01:15:05,472][DEBUG][gateway.local            ] [es1.node2] [spend_phrases_20141120][2]: not allocating, number_of_allocated_shards_found [0], required_number [1]
[2015-02-22 01:15:05,482][DEBUG][cluster.service          ] [es1.node2] cluster state updated, version [2650], source [zen-disco-join (elected_as_master)]
[2015-02-22 01:15:05,482][INFO ][cluster.service          ] [es1.node2] new_master [es1.node2][XOS-rG5sTyqAHPLUiNAS-w][es1][inet[/10.126.158.82:9311]]{datacenter=nj, master=true}, reason: zen-disco-join (elected_as_master)
```

We are using default settings for timeout etc, the error was not a timeout but an immediate failure. 
</description><key id="59705365">9972</key><summary>Indexing request failure during master re-elect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ajhalani</reporter><labels><label>discuss</label></labels><created>2015-03-03T21:06:00Z</created><updated>2015-12-05T19:29:40Z</updated><resolved>2015-12-05T19:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-08T23:59:56Z" id="77783312">Hi @ajhalani 

Please could you provide your config file, your cluster settings, and the code that you are using to index documents.

thanks
</comment><comment author="ajhalani" created="2015-03-09T21:16:11Z" id="77944409">Sure, put it at [https://gist.github.com/ajhalani/4b439e9dfa2a9bfd1b5e] . 
</comment><comment author="bleskes" created="2015-03-09T21:34:26Z" id="77948002">@ajhalani thx. Can you also post the c++ code for the request?

PS.  I don't see minimum_master_nodes set anywhere do you set it somewhere?
</comment><comment author="ajhalani" created="2015-03-09T22:56:43Z" id="77962533">We are not using that option minimum_master_nodes  currently.. We saw this issue with both 2-node and 8-node cluster if that helps debug if this option missing is causing issue. 

It's difficult to share internal C++ existing code,  will have to check with folks around if it's okay.. Could we debug without the code, as I mentioned it's doing a http _bulk index request.. 
</comment><comment author="ajhalani" created="2015-05-14T14:00:39Z" id="102045672">We still see the issue happening with v1.5.2. It seems to me that master goes down without re-electing a new one, and takes 3-4 seconds to find new one. 

It would help If the **graceful** shutdown of master waited for a new master was elected before completing shutdown.
</comment><comment author="apidruchny" created="2015-05-14T14:24:14Z" id="102050585">As a workaround, is it possible to force Elasticsearch to elect another node as master before we start shutdown? E.g., if we change the setting node.master to false on the current master node, will this cause the master election process? On the other hand, if we set node.master to false and the current master node stops being a master, we probably will get into the same state of no master situation for 3-4 seconds.
</comment><comment author="ajhalani" created="2015-06-12T15:32:41Z" id="111529287">As a temporary workaround, we are now checking for 503 error, and retry again in 3-4 seconds. 
</comment><comment author="bleskes" created="2015-06-16T15:55:57Z" id="112478494">Master (re) election takes indeed 3 seconds. However, any incoming requests that come in during this period will be kept in flight until a new master is elected. This has a maximum timeout of one minute, after which you will see and error as you describe. Do you change the timeout parameter on the request?
</comment><comment author="clintongormley" created="2015-12-05T19:29:40Z" id="162239408">No feedback since June.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option for nGram tokenizer to always tokenize whole word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9971</link><project id="" key="" /><description>It would be great if the nGram tokenizer came with an option to force creation of a whole-word token for cases when the word length falls outside the min-gram/max-gram range. Currently, we have to resort to a dual-mapping approach to make sure these tokens get created, which limits how we can construct per-field queries.
</description><key id="59701883">9971</key><summary>Add option for nGram tokenizer to always tokenize whole word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BHSPitMonkey</reporter><labels><label>:Analysis</label><label>discuss</label><label>enhancement</label></labels><created>2015-03-03T20:38:54Z</created><updated>2015-04-04T14:38:30Z</updated><resolved>2015-04-04T14:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-08T22:55:33Z" id="77780376">Hi @BHSPitMonkey 

I think you mean the edge-ngram tokenizer, rather than the ngram tokenizer.  When using `token_chars' to break up the string into words, you can just make the max_ngram as big as you need, which will serve your purpose.
</comment><comment author="BHSPitMonkey" created="2015-03-11T00:37:44Z" id="78178947">1. Actually, no; I meant the ngram tokenizer.
2. Raising the `max_gram` (and/or lowering the `min_gram`) setting is outside the question. The problem is that, once you've decided on a min/max which suit your needs (a decision based on the specific partial-matching needs and taking storage complexity into account), you still want words outside of this window to match in their entirety.

For example: Let's say we've decided to decided to restrict partial text searching to substrings from 3 to 10 characters in length. (Going below 3 would add a considerable amount of additional tokens, and we've decided it's not very useful below that threshold.)
- You want to search for a title with the (whole) word `Me`, but you can't; `Me` won't produce any token at all, despite being a full word (which the standard tokenizer would pick up).
- You want to search for `incandescent`, but you can't; However, you can search for `candescent`, or any other substring of length [3,10]. But what good does that do if the user provided the whole word in their query?

So, in order to accommodate these two seemingly reasonable scenarios, we currently have to index our text into two separate (identical) fields, using a standard tokenizer on the other one. This is a hack that comes with added storage/bandwidth costs, makes filtering on these fields harder, and adds complexity to queries (for instance, now you can't easily query by field anymore without transforming the query and introducing an `OR` subquery). An option like the one I'm describing would be really helpful to have.
</comment><comment author="clintongormley" created="2015-04-04T14:23:23Z" id="89585732">Hi @BHSPitMonkey 

OK - I understand the requirement to include whole terms which are shorter than the `min_gram` (which are currently dropped from the token stream) but the same doesn't apply to words that are longer, because they will have ngrams applied anyway, for instance:

```
DELETE t

PUT t
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_ngrams": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 4,
          "token_chars": [
            "letter"
          ]
        }
      },
      "analyzer": {
        "my_ngrams": {
          "tokenizer": "my_ngrams"
        }
      }
    }
  }
}

GET /t/_analyze?text=my tests&amp;analyzer=my_ngrams
```

The above returns:

```
{
   "tokens": [
      {
         "token": "tes",
         "start_offset": 3,
         "end_offset": 6,
         "type": "word",
         "position": 1
      },
      {
         "token": "test",
         "start_offset": 3,
         "end_offset": 7,
         "type": "word",
         "position": 2
      },
      {
         "token": "est",
         "start_offset": 4,
         "end_offset": 7,
         "type": "word",
         "position": 3
      },
      {
         "token": "ests",
         "start_offset": 4,
         "end_offset": 8,
         "type": "word",
         "position": 4
      },
      {
         "token": "sts",
         "start_offset": 5,
         "end_offset": 8,
         "type": "word",
         "position": 5
      }
   ]
}
```

So yes, the word `my` has just been dropped, but `tests` still produces all of the tokens you need.

I agree that we should probably add an option to include the whole token if it is shorter than `min_gram`    
</comment><comment author="clintongormley" created="2015-04-04T14:28:05Z" id="89586644">Oh I think I've just got your point about including the whole token if it is above `max_gram`.   

Hmm, the typical approach to this would be just to index the field twice: once with standard analyzer and once with ngrams, then to use a `multi_match` query in `most_fields` mode to query both fields at once.

@rmuir what's your take on this?
</comment><comment author="rmuir" created="2015-04-04T14:37:15Z" id="89589856">I don't understand the aversion to multi fields approach here. with n=3/4 you already have two analysis fields stuck inside of one IMO (one at n=3, one at n=4). If you have another field with standardanalyzer, you have a lot of flexibility, e.g. you can query both at the same time, or just the standardanalyzer field for a more 'exact' query (e.g., user puts term in quotes).

Of course, this thing seems to be complicated by a few issues. First of all, IMO n-grams shouldnt really pre-tokenize much at all. This doesn't make a lot of sense to me since n-grams is a different tokenization technique, just let it be different than tokenizing on words (so don't do it in n-grams!). 

If we try to be too fancy and handle everything just within n-grams, i'm afraid the code will become super-complicated and stagnate and become unmaintainable. For a real example of this, look at CJKBigramFilter.java in lucene (it has some of the logic proposed here, in its miniature world of n=1/n=2). I wrote it from scratch and really tried to make it simple and easy. Its hairy as hell :(
</comment><comment author="clintongormley" created="2015-04-04T14:38:30Z" id="89589909">@rmuir many thanks.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Roadmap for 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9970</link><project id="" key="" /><description>- [x] #8312 Use doc values by default
- [x] #8870 Field/type mapping refactor
- [x] #9876 Perform computation on aggregations
- [x] #6295 Update cluster state using diffs
- #10217 ~~Refactor query and filter, agg, suggester parsing/serialization~~ (bumped to 3.0)
- [x] #8963 Warn when using deprecated syntax
- #3226 ~~Upgrade to Netty 4~~ 
- [x] Lucene 5.1
- [x] Lucene 5.2
- [x] #12768 Tests and packaging changes 
- [x] #11271 Shards should rebalance across multiple path.data on one node 
- [x] #12783 Remove spawn modules

Index migration:
- [x] #9498 Move to one datapath per shard
- [x] #10213 Allow upgrade API to only upgrade too-old segments
- [x] #10214 Migration advisory _site plugin
- [x] #10215 Don't allow indices containing too-old segments to be opened
</description><key id="59694292">9970</key><summary>Roadmap for 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>Meta</label><label>v2.0.0</label></labels><created>2015-03-03T19:43:36Z</created><updated>2015-10-07T16:20:40Z</updated><resolved>2015-10-07T16:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joealex" created="2015-03-18T20:01:13Z" id="83147810">#6295 will be pretty important to have for very large clusters and have the flexibility to keep adding Nodes

#8312 is very much awaited along with #9498 which seems like got moved to 2.0 ?
</comment><comment author="leonardehrenfried" created="2015-07-03T12:52:03Z" id="118339511">I know it's a little impatient to ask, but is there any indication when an ES2 beta will be released?
</comment><comment author="dmitry" created="2015-07-03T13:33:12Z" id="118352431">Looks like there are still a lot of unclosed issues: https://github.com/elastic/elasticsearch/labels/v2.0.0
</comment><comment author="runarmyklebust" created="2015-08-13T10:55:05Z" id="130615171">Does this mean that the suggested filter/query-merge are postponed to v3? Would be nice to know, since it will impact our development plans.
</comment><comment author="clintongormley" created="2015-08-13T10:59:08Z" id="130616476">@runarmyklebust no - that's already merged.  the query refactoring is about the java api and where requests get parsed: coordinating node or shard level.  unrelated to the query/filter merging
</comment><comment author="runarmyklebust" created="2015-08-13T11:54:39Z" id="130638790">Ok, thanks a lot, got me a bit worried there. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated the resiliency status page for v1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9969</link><project id="" key="" /><description /><key id="59683420">9969</key><summary>Updated the resiliency status page for v1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2015-03-03T18:24:59Z</created><updated>2015-03-03T18:52:00Z</updated><resolved>2015-03-03T18:50:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T18:25:16Z" id="77003627">@bleskes @kimchy Please could you review
</comment><comment author="kimchy" created="2015-03-03T18:47:25Z" id="77008051">LGTM, I am missing more work done in 1.5 (or I just can't find it), like using the `.si` file as the unique id to check for segments on recovery, and not recovering from nodes with known bad versions like ones that have the compression bug
</comment><comment author="clintongormley" created="2015-03-03T18:51:24Z" id="77008817">OK merging - I'll address the 1.5 changes in a different PR, don't want to delay this one any more
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add random setup for DerivativeTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9968</link><project id="" key="" /><description>Changed the exiting tests to have a  more randomized setup, keeping interval constant and instead insert random number of docs per bucket in test setup. Also added common helper methods to check bucket contents, removed the test for multi-valued fields (no benefit in testing this for the derivative) and do the first and second derivative testing in one method.
</description><key id="59677843">9968</key><summary>Tests: Add random setup for DerivativeTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2015-03-03T17:45:06Z</created><updated>2015-03-17T20:57:08Z</updated><resolved>2015-03-17T20:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-03-05T17:18:14Z" id="77406793">@colings86 Added the small changes in the helper. I'd still like to randomize the gap-tests a little more, maybe tomorrow, but could also push the current state if you're okay with it.
</comment><comment author="colings86" created="2015-03-05T17:21:16Z" id="77407351">@cbuescher I want to have a closer look at some point but since its on the aggs branch anyway, feel free to push now and I'll look when I get time next week
</comment><comment author="cbuescher" created="2015-03-05T17:24:49Z" id="77408083">Okay, no prob, that way I can maybe also add the random gap thing tomorrow. If you don't mind if there a a few more changes coming in.
</comment><comment author="cbuescher" created="2015-03-06T10:56:24Z" id="77541599">Added randomized test setup for derivative where approx. half of the input buckets are empty. Btw. this only works so far when the input agg is build with minDocCount(0), I assume thats mandatory so far so I build the test around that assumption.
</comment><comment author="colings86" created="2015-03-06T11:07:47Z" id="77543001">@cbuescher yeah that assumption is fine. The min_doc_count = 0 isn't yet mandatory (in that it won't error if its not set) but it will be changed before we merge to throw a validation error if the derivative it used with a histogram which doesn't have min_doc_count = 0
</comment><comment author="cbuescher" created="2015-03-17T20:57:08Z" id="82597411">Pushed this to the aggs-2.0 feature branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disconnect between coordinating node and shards can cause duplicate updates or wrong status code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9967</link><project id="" key="" /><description>A document update can be sent to any node in the cluster (coordinating node) and this node will forward it to the ode that has the shard (the executing node). If the update fails, then under certain conditions the coordinating node tries to send the the update again (for example https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java#L447). However, the executing node might already have applied the update and will then just apply it again. This is problematic if the update was for example increasing a counter. The same effect might cause the wrong status code to be returned for versioned indexing requests. A real word scenario where this can happen is when nodes are restarted that have shards without replicas and updates are send to the restarted node. 
</description><key id="59673962">9967</key><summary>Disconnect between coordinating node and shards can cause duplicate updates or wrong status code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Core</label><label>bug</label><label>resiliency</label></labels><created>2015-03-03T17:17:12Z</created><updated>2017-07-11T04:36:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="erik777" created="2017-07-11T04:36:21Z" id="314320580">This can be an issue for an incremental counter. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add another test for reading a truncated translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9966</link><project id="" key="" /><description>Follow-up to #9797
</description><key id="59656132">9966</key><summary>Add another test for reading a truncated translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label></labels><created>2015-03-03T15:20:24Z</created><updated>2015-05-05T20:56:54Z</updated><resolved>2015-05-05T20:56:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-05-05T20:56:53Z" id="99219617">Closing this as it is not needed with Boaz's changes in #10624
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] query string syntax : small typo in range example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9965</link><project id="" key="" /><description>Hi!

Reading the docs, I think I found a small typo on page http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_ranges_2:

&gt; Curly and square brackets can be combined:
&gt; - Numbers from 1 up to but not including 5
&gt;   
&gt;   `count:[1..5}`

Shouldn't it be `count:[1 TO 5}`?

(but I would love if ".." was supported!)
</description><key id="59653851">9965</key><summary>[doc] query string syntax : small typo in range example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daniel-menard</reporter><labels /><created>2015-03-03T15:05:12Z</created><updated>2015-03-03T19:04:16Z</updated><resolved>2015-03-03T19:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T19:04:16Z" id="77011350">thanks @daniel-menard 

you're correct, i've fixed the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>reloading cluster state using PUT /_cluster/settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9964</link><project id="" key="" /><description>I was experimenting with ways to make elasticsearch reload its cluster state to e.g. detect and fix dangling indices that I created by copying or moving indices. Elasticsearch detects and fixes those on start up.

Then I figured it probably does this as well when you update the cluster settings and indeed it does, which is nice. This provides me a way to copy/move/rename indices without a cluster restart. So, for example this does the trick. Elasticsearch will detect any dangling indices and fix them:

```
PUT /_cluster/settings
{
    "transient" : {
        "discovery.zen.minimum_master_nodes" : 1
    }
}
```

However, it would be nicer if I could just do a PUT /_cluster/settings without a body since I don't actually want to change the settings and just reload the cluster state.

This currently fails with a somewhat obscure error (you may want to fix that)

```
{
   "error": "ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1]",
   "status": 400
}
```

fixing that with a 

```
{}
```

doesn't work either since it requires some kind of settings in there.

```
{
   "error": "ActionRequestValidationException[Validation Failed: 1: no settings to update;]",
   "status": 400
}
```

A nice enhancement would be for PUT /_cluster/settings without a body to simply reload the cluster state. Unless I missed something, there's currently no other way to trigger this behavior via the API than as I described above. A PUT to /_cluster/state, which might be a better alternative, does not seem to work for example.
</description><key id="59649512">9964</key><summary>reloading cluster state using PUT /_cluster/settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels><label>:Cluster</label><label>enhancement</label></labels><created>2015-03-03T14:34:27Z</created><updated>2016-09-27T15:34:30Z</updated><resolved>2016-09-27T14:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T14:13:45Z" id="249877280">&gt; Unless I missed something, there's currently no other way to trigger this behavior via the API than as I described above

This is already available by sending an empty body to `/_cluster/reroute`, which increments the cluster state and causes a new state to be sent to all nodes (with no changes).

Going to close this since there currently exists a way to do this.
</comment><comment author="jillesvangurp" created="2016-09-27T14:23:14Z" id="249880069">What does it mean to "increment" the cluster state?
</comment><comment author="dakrone" created="2016-09-27T15:09:11Z" id="249893733">&gt; What does it mean to "increment" the cluster state?

It means the master node increments the number of the cluster state, which prompts it being sent to all other nodes in the cluster.
</comment><comment author="jillesvangurp" created="2016-09-27T15:34:30Z" id="249902386">Right, none of this is really obvious from the documentation page for this
API. Somebody may want to follow up with this to improve the documentation.
Anyway, thanks for clarifying.

On Tue, Sep 27, 2016 at 5:20 PM Lee Hinman notifications@github.com wrote:

&gt; What does it mean to "increment" the cluster state?
&gt; 
&gt; It means the master node increments the number of the cluster state, which
&gt; prompts it being sent to all other nodes in the cluster.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/9964#issuecomment-249893733,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AAx_86aSBqDK0dDQ6eaFVmJWwZ3OiU2gks5quTMsgaJpZM4DovBY
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node receiving a cluster state with a wrong master node should reject and throw an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9963</link><project id="" key="" /><description>Previously it was ignored and the publish cluster state timeout would kick in. In that case a stale master node would just wait for the inevitable and waste valuable time.

This issue was discovered by the DiscoveryWithServiceDisruptionsTests#testStaleMasterNotHijackingMajority test.

Note this issue doesn't occur in any released version. (just on 1.x and master branches)
</description><key id="59635235">9963</key><summary>Node receiving a cluster state with a wrong master node should reject and throw an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-03T12:31:48Z</created><updated>2015-06-07T18:07:10Z</updated><resolved>2015-03-06T07:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-03T13:41:40Z" id="76946616">Left some small comments
</comment><comment author="martijnvg" created="2015-03-03T14:23:44Z" id="76953907">@bleskes I've updated the PR.
</comment><comment author="bleskes" created="2015-03-05T09:30:32Z" id="77332597">LGTM. Thx @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Api: every request, builder and response to implement toString() consistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9962</link><project id="" key="" /><description>Follw-up of discussion we had in #9944. After #9201, we should look into having consistent `toString()` implementations for all of our requests (I think builders will be gone by then) and responses.

Topics to discuss: when it comes to search requests we currently print out only part of the requests in json format. I'd propose to move away from json format in general, which is trappy as it makes users think that the json can be sent back to elasticsearch as is (although not complete). I would just make sure that we print out every single bit that the request holds (e.g. at the moment search request builder ignores index, type, routing etc.). 

In general, `toString` should only be a way to nicely debug requests and their content and quickly log them, it doesn't need to be json and it needs to contain all the relevant info for every class that implements it.
</description><key id="59632372">9962</key><summary>Java Api: every request, builder and response to implement toString() consistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2015-03-03T12:03:04Z</created><updated>2017-05-05T16:11:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-09-08T10:43:25Z" id="245560356">Given what we plan on doing for the java api (deprecate soon, use only internally in the future) I am wondering if it makes sense to invest time on this issue. I would vote for closing.
</comment><comment author="javanna" created="2017-05-05T16:11:01Z" id="299507426">This is still valid as the high level REST client is reusing requests and responses from the Java api.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog stored at the wrong place</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9961</link><project id="" key="" /><description>When using option --default.path.data at startup and configuring path.data with a different path in /etc/elasticsearch/elasticsearch.yml, indices are correctly stored in the path defined in elasticsearch.yml, but translogs are still stored in the path defined by --default.path.data.

Removing --default.path.data solves the problem, but when using packages for ES, it involves modifying /etc/init.d/elasticsearch, which is a packaged file.

This situation is a problem, because in my case the default data path (/var/lib/elasticsearch) is a small partition while data paths i configured in elasticsearch.yml are a group of huge SSD. I think translogs should be stored in the correct data.path in order to efficiently use available storage.
</description><key id="59628743">9961</key><summary>Translog stored at the wrong place</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">setaou</reporter><labels><label>:Settings</label><label>:Translog</label><label>adoptme</label><label>bug</label></labels><created>2015-03-03T11:25:53Z</created><updated>2015-12-05T19:26:03Z</updated><resolved>2015-12-05T19:26:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-03T23:48:31Z" id="77065152">@setaou I haven't been able to reproduce this yet, but I am trying to. What version of ES are you using and what operating system are you using? It sounds like you are using either the RPM or DEB package as well?
</comment><comment author="setaou" created="2015-03-04T08:53:48Z" id="77118537">@dakrone Thanks for working on this bug. I am using ES 1.4.4 on Ubuntu 14.04 using the official ES deb package. 
Here is the config file with which I can reproduce the problem : https://gist.github.com/setaou/7a65a242251f897aab35 .

After testing on a vanilla install with this config file and a very simple index (the example at https://github.com/elasticsearch/elasticsearch#indexing), I discovered that the problem seems to affect more than just translogs. Here is the listing of files after the indexing: https://gist.github.com/setaou/7ca4f2b8abdcae55b8de . I suppose this did not appear on my previous setup because /data/ partitions had a lot more free space than /var/lib, so segments were only stored on /data/.
</comment><comment author="clintongormley" created="2015-12-05T19:26:03Z" id="162239245">I'm unable to replicate this.  Lots has changed in this area recently, so I think this is fixed.  Please reopen if it is still a problem
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: upgrade to Lucene 4.10.4 bugfix release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9960</link><project id="" key="" /><description>Upgrade to the new Lucene 4.10.4 bugfix release.
</description><key id="59618580">9960</key><summary>Core: upgrade to Lucene 4.10.4 bugfix release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2015-03-03T09:54:51Z</created><updated>2015-08-25T13:25:02Z</updated><resolved>2015-03-03T15:14:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-03T09:55:57Z" id="76916821">LGTM
</comment><comment author="mikemccand" created="2015-03-03T09:59:38Z" id="76917335">Thanks @jpountz ... I'm hitting some test failures in 1.x, I'm gonna dig a bit before pushing.  Could just be pre-existing issues:

```
Tests with failures:
  - org.elasticsearch.search.basic.SearchWhileCreatingIndexTests.testTwoReplicas
  - org.elasticsearch.bwcompat.RestoreBackwardsCompatTests.restoreOldSnapshots
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single endpoint for getting all configuration settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9959</link><project id="" key="" /><description>I'd really like to see a single endpoint (e.g. `localhost:9200/_configuration`) for getting all of the configuration settings (usually defined in `elasticsearch.yml`) that a running Elasticsearch process uses.

This would be really helpful validating configurations in more complex environments.
</description><key id="59607226">9959</key><summary>Single endpoint for getting all configuration settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Pyppe</reporter><labels /><created>2015-03-03T07:55:48Z</created><updated>2015-03-03T09:19:44Z</updated><resolved>2015-03-03T09:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T09:19:44Z" id="76911667">Closing in favour of #6732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multiple 'children' aggregations interfere with each other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9958</link><project id="" key="" /><description>If you have two sibling aggregations that make use of the 'children' aggregation, then the second aggregation seems to count each document twice.

Here is a cut-down example

I have a mapping that has parent documents of type 'datastreams' with child documents of type 'readings'

I ran this, which has two identical sibling aggregations:

```
curl -XGET 'https://blahblahblah/index/datastreams/_search?search_type=count&amp;pretty' -d '{
"aggs" : {
     "one" : {
        "children": {
          "type": "readings"
        },
       "aggs" :{
         "grand-total" : { "sum" : { "field" : "readings.dv" } }
       }
     },
     "two" : {
        "children": {
          "type": "readings"
         },
        "aggs" :{
          "grand-total" : { "sum" : { "field" : "readings.dv" } }
        }
     }  
 }  
}' 
```

and got the response

```
{
   "took" : 23620,
   "timed_out" : false,
   "_shards" : {
      "total" : 5,
      "successful" : 5,
      "failed" : 0
   },
  "hits" : {
     "total" : 1389,
     "max_score" : 0.0,
     "hits" : [ ]
  },
 "aggregations" : {
    "two" : {
       "doc_count" : 729353222,
       "grand-total" : {
          "value" : 2.389726905175203E10
       }
    },
   "one" : {
      "doc_count" : 364676611,
      "grand-total" : {
          "value" : 1.1948634525852589E10
      }
   }
 }
}
```

Although aggregation 'one' and 'two' are identical Elasticsearch seems to have double counted the documents for aggregation 'two'
</description><key id="59598689">9958</key><summary>multiple 'children' aggregations interfere with each other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">perryn</reporter><labels><label>:Aggregations</label><label>bug</label><label>feedback_needed</label></labels><created>2015-03-03T05:53:57Z</created><updated>2015-04-23T18:23:37Z</updated><resolved>2015-04-13T10:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-03T08:57:03Z" id="76908528">Hi @perryn on what version of ES are experiencing this interference issue? Also are you able to reproduce on a smaller scale?
</comment><comment author="perryn" created="2015-03-03T23:23:22Z" id="77061878">Hi @martijnvg 

The above was seen on 1.4.4 but I was also able to reproduce the issue on a smaller scale on 1.4.2.

I ran the same query as above and got the following result

```
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 600,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "one" : {
      "doc_count" : 12,
      "grand-total" : {
        "value" : 259.251
      }
    },
    "two" : {
      "doc_count" : 24,
      "grand-total" : {
        "value" : 518.502
      }
    }
  }
}
```

cheers
Perryn
</comment><comment author="martijnvg" created="2015-03-25T14:02:26Z" id="86041691">@perryn I think that the issue you're reporting is fixed by #10263. Would be great if you can verify this!
</comment><comment author="AlexKovalevich" created="2015-03-29T23:57:38Z" id="87499155">Can reproduce in 1.5. That's actually how I came here, by trying to find a solution.

If I specify children aggregation first and then would do different sub aggregations under it - they seem to work as expected. So the example below returns two identical histograms as requested for this example.
{
    "aggregations": {
        "rating_histogram_total_1": {
            "children": {
                "type": "myIndex"
            },
            "aggregations": {
                "rating_histogram_total_1": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                },
                "rating_histogram_total_2": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                }
            }
        }
    }
}

The following request, however, doesn't work. If I put children aggregation under different filters and then try to use the same histogram, the second histogram returns values n^2, the third one - n^3 and so on. I didn't debug what would happen if filters above "children would have different criteria s".

Example: query:

{
    "aggregations": {
        "rating_histogram_total_1": {
            "children": {
                "type": "myIndex"
            },
            "aggregations": {
                "rating_histogram_total_1": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                }
            }
        },
        "rating_histogram_total_2": {
            "children": {
                "type": "myIndex"
            },
            "aggregations": {
                "rating_histogram_total_2": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                }
            }
        },
        "rating_histogram_total_3": {
            "children": {
                "type": "myIndex"
            },
            "aggregations": {
                "rating_histogram_total_3": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                }
            }
        },
        "rating_histogram_total_4": {
            "children": {
                "type": "myIndex"
            },
            "aggregations": {
                "rating_histogram_total_4": {
                    "histogram": {
                        "field": "myIndex.reviewsRatingAverage",
                        "interval": 1,
                        "min_doc_count": 0,
                        "extended_bounds": {
                            "min": 0,
                            "max": 5
                        }
                    }
                }
            }
        }
    }
}

And the results are:

{
    "aggregations": {
        "rating_histogram_total_4": {
            "doc_count": 32,
            "rating_histogram_total_4": {
                "buckets": [
                    {
                        "key": 0,
                        "doc_count": 24
                    },
                    {
                        "key": 1,
                        "doc_count": 0
                    },
                    {
                        "key": 2,
                        "doc_count": 0
                    },
                    {
                        "key": 3,
                        "doc_count": 8
                    },
                    {
                        "key": 4,
                        "doc_count": 0
                    },
                    {
                        "key": 5,
                        "doc_count": 0
                    }
                ]
            }
        },
        "rating_histogram_total_1": {
            "doc_count": 4,
            "rating_histogram_total_1": {
                "buckets": [
                    {
                        "key": 0,
                        "doc_count": 3
                    },
                    {
                        "key": 1,
                        "doc_count": 0
                    },
                    {
                        "key": 2,
                        "doc_count": 0
                    },
                    {
                        "key": 3,
                        "doc_count": 1
                    },
                    {
                        "key": 4,
                        "doc_count": 0
                    },
                    {
                        "key": 5,
                        "doc_count": 0
                    }
                ]
            }
        },
        "rating_histogram_total_3": {
            "doc_count": 16,
            "rating_histogram_total_3": {
                "buckets": [
                    {
                        "key": 0,
                        "doc_count": 12
                    },
                    {
                        "key": 1,
                        "doc_count": 0
                    },
                    {
                        "key": 2,
                        "doc_count": 0
                    },
                    {
                        "key": 3,
                        "doc_count": 4
                    },
                    {
                        "key": 4,
                        "doc_count": 0
                    },
                    {
                        "key": 5,
                        "doc_count": 0
                    }
                ]
            }
        },
        "rating_histogram_total_2": {
            "doc_count": 8,
            "rating_histogram_total_2": {
                "buckets": [
                    {
                        "key": 0,
                        "doc_count": 6
                    },
                    {
                        "key": 1,
                        "doc_count": 0
                    },
                    {
                        "key": 2,
                        "doc_count": 0
                    },
                    {
                        "key": 3,
                        "doc_count": 2
                    },
                    {
                        "key": 4,
                        "doc_count": 0
                    },
                    {
                        "key": 5,
                        "doc_count": 0
                    }
                ]
            }
        }
    }
}

I hope it will help.
Thank you.
</comment><comment author="martijnvg" created="2015-04-08T14:12:12Z" id="90928895">@AlexKovalevich Thanks for sharing. It think this was fixed via #10263 (since it looks similar to the issue you describe). This will be included in 1.5.1. I can't be sure if this really fixed your issue, because there is no reproduction. If you like you can build from the 1.5 branch and see if the issue still occurs. This would help a lot.
</comment><comment author="martijnvg" created="2015-04-09T18:59:46Z" id="91328078">@AlexKovalevich Now that 1.5.1 has been released can you check if the issue still occurs in your environment?
</comment><comment author="fozzylyon" created="2015-04-10T21:34:26Z" id="91698245">@martijnvg I was running into the same issue and I can verify that 1.5.1 fixed it for me.
</comment><comment author="clintongormley" created="2015-04-13T10:38:28Z" id="92303752">awesome. @fozzylyon thanks for letting us know. closing
</comment><comment author="AlexKovalevich" created="2015-04-23T18:23:37Z" id="95677108">Fixed for me too in 1.5.1 !
(Sorry for delay, couldn't test earlier)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: ability to limit which fields are included for each "hit"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9957</link><project id="" key="" /><description>I am running a `_search` (with a sort) and hoping to just get the _id's back. Unfortunately I get a ton of extra, mostly redundant, and mostly useless fields that I can't seem to disable.

``` javascript
      {
        "_index": "itemview_v2_2015_01_20_15_13_38",
        "_type": "itemview",
        "_id": "list.4ecc2e933434af4976729cc9:54f406ece4b0e900509d3cd9",
        "_score": null,
        "sort": [
          1425352613566,
          "asdf"
        ]
      }
```

It would be great if I could return JUST an array of `{"_id":"..."}` or even better, an array of the id values themselves. 
</description><key id="59595709">9957</key><summary>Feature Request: ability to limit which fields are included for each "hit"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottbessler</reporter><labels /><created>2015-03-03T05:05:15Z</created><updated>2015-03-03T09:18:58Z</updated><resolved>2015-03-03T09:18:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T09:18:58Z" id="76911577">Closing in favour of #7401
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hitting AlreadyExpiredException from updating document that has valid _ttl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9956</link><project id="" key="" /><description>Hi,

I hit the AlreadyExpiredException from updating document with valid _ttl, i.e. _ttl greater than 0.
Here is my index mapping (simplified), both **_timestamp** and **_ttl** are enabled, and _timestamp is provided with _customized path_:

```
{
  "activity" : {
    "mappings" : {
      "log" : {
        "_timestamp" : {"enabled" : true, "path" : "log_time"},
        "_ttl" : {"enabled" : true, "default" : 300000},
        "properties" : {
          "log_count" : {"type" : "integer"},
          "log_time" : {"type" : "date"}
        }
      }
    }
  }
}
```

I did some experiments as [documented in this article](https://medium.com/@coxchen/document-obsolescence-in-elasticsearch-c5973dd9e68d) and found that, if I have mappings like above (both **_timestamp** and **_ttl** are enabled, and _timestamp is provided with _customized path_), I'll hit the AlreadyExpiredException from updating document with valid _ttl. I have tried both 1.3.1 and 1.4.2, same behavior.

Is this a known issue? Or is there any design intent behind this?

Thank you for your help!
</description><key id="59589814">9956</key><summary>Hitting AlreadyExpiredException from updating document that has valid _ttl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">coxchen</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-03-03T03:26:26Z</created><updated>2016-05-12T12:56:15Z</updated><resolved>2016-05-12T12:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T09:17:46Z" id="76911445">Hi @coxchen 

The `_ttl` is added to the `_timestamp`, so you need to update both otherwise it uses the old `_timestamp` as a base.
</comment><comment author="coxchen" created="2015-03-03T14:52:15Z" id="76959292">Hi @clintongormley 

Thanks for your reply.

Sorry for the confusion. I didn't update the _timestamp field (or log_time in my example), instead I did partial update to the **log_count** field with:

```
curl -XPOST 'localhost:9200/activity/log/1/_update' -d '{ "script" : "ctx._source.log_count+=1" }'
```

I just want to update fields **other than** the _timestamp filed and _ttl field, so I don't see why I hit the AlreadyExpiredException.
</comment><comment author="clintongormley" created="2015-03-03T19:01:33Z" id="77010842">@coxchen It means that the document has actually expired, but not yet been deleted. Expired docs are only removed every 60 seconds.
</comment><comment author="coxchen" created="2015-03-04T02:59:02Z" id="77088566">Hi @clintongormley 

Nope, it's not that case. The document I'd like to update still has valid _ttl (&gt;&gt; 0).

I first found this issue in a production system I'm working on. In this case, I had a document with _ttl originally set to **7d**, and I got the AlreadyExpiredException from updating that document on the **4th day** after it been indexed in ES. At that time, the document **still has its _ttl about 3 days left**.

I did some experiments, which you can read the details at https://medium.com/@coxchen/document-obsolescence-in-elasticsearch-c5973dd9e68d if you have time.
</comment><comment author="coxchen" created="2015-03-04T03:34:51Z" id="77091436">@clintongormley 

Below is the result of my experiments, showing how _ttl of document changes over time.

![ttl_change_chart](https://cloud.githubusercontent.com/assets/503112/6477649/c19516c0-c260-11e4-9195-e42f1dd9564f.png)

I have four documents indexed in ES
- /activity/log1/1 is _green_ curve
- /activity/log1/2 is _yellow_ curve
- /activity/log2/3 is _blue_ curve
- /activity/log2/4 is _orange_ curve

with the following index mapping

```
{
  "activity" : {
    "mappings" : {
      "log1" : {
        "_timestamp" : {"enabled" : true, "path" : "log_time"},
        "_ttl" : {"enabled" : true, "default" : 300000},
        "properties" : {
          "log_count" : {"type" : "integer"},
          "log_time" : {"type" : "date"}
        }
      },
      "log2" : {
        "_ttl" : {"enabled" : true, "default" : 300000},
        "properties" : {
          "log_count" : {"type" : "integer"},
          "log_time" : {"type" : "date"}
        }
      }
    }
  }
}
```

I also have script to update the **log_count** field to document **2** and **4** periodically.

```
curl -XPOST 'localhost:9200/activity/log1/2/_update' -d '{ "script" : "ctx._source.log_count+=1" }'

curl -XPOST 'localhost:9200/activity/log2/4/_update' -d '{ "script" : "ctx._source.log_count+=1" }'
```

Supposedly, the curves of the 4 document should overlap. But you can see the **yellow** curve (document 2) is outstanding. It doesn't look right to me. Can you help to explain?
</comment><comment author="coxchen" created="2015-04-12T10:34:40Z" id="92031163">Hi @clintongormley 

I trace the source code and found that, in **TTLFieldMapper.java**, the _timestamp in document source will be used to check expiration when updating a document. So the workaround for my issue is to **always provide the original _ttl value** when updating the document, even though I don't have the intention to update _ttl.

You can check my article for details: https://medium.com/@coxchen/saving-document-half-life-in-es-89be764f21ca
</comment><comment author="clintongormley" created="2015-04-13T11:38:18Z" id="92322181">Hi @coxchen 

Thanks for digging!  I've just tried my own (slightly different) test and see the `_ttl` increasing by leaps and bounds:

```
DELETE activity

PUT activity
{
  "mappings": {
    "log": {
      "_timestamp": {
        "enabled": true,
        "path": "log_time",
        "store": true
      },
      "_ttl": {
        "enabled": true,
        "default": 300000
      },
      "properties": {
        "log_count": {
          "type": "integer"
        },
        "log_time": {
          "type": "date"
        }
      }
    }
  }
}
```

Index a document using tomorrow's date:

```
PUT activity/log/1
{
  "log_time": "2015-04-14"
}
```

Repeat these two steps to see the `_ttl` just keep on growing

```
GET _search?fields=_ttl

POST activity/log/1/_update 
{
  "doc": { "foo": "bar" }
}
```
</comment><comment author="coxchen" created="2015-04-13T14:16:39Z" id="92370964">Hi @clintongormley 

Interesting, the log_time is some time in the future, making the _ttl growing with UPDATE.
</comment><comment author="darylrobbins" created="2015-05-07T16:30:47Z" id="99930322">I have hit this same issue when upgrading from 1.4.2 to 1.5.2. I'm using a mvel script to update attributes in the document (neither the _timestamp or _ttl). The issue appears to happen for some documents but not others.
</comment><comment author="clintongormley" created="2016-05-12T12:56:15Z" id="218748569">Closing in favour of #18280
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documented default value for node_initial_primaries_recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9955</link><project id="" key="" /><description>Added default value to `cluster.routing.allocation.node_initial_primaries_recoveries`
</description><key id="59568280">9955</key><summary>Documented default value for node_initial_primaries_recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T23:18:57Z</created><updated>2015-03-03T09:15:53Z</updated><resolved>2015-03-03T09:15:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-02T23:50:24Z" id="76854604">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move GapPolicy and resolveBucketValues() to static helper methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9954</link><project id="" key="" /><description>Will allow reducers to share the same helper functionality without repeating code.  

Since these seemed more like helpers than inherent traits of a Reducer (e.g. we could have non-time based reducers, or reducers that don't care about contiguous buckets), I put them into their own static class.

I imagine we'll need a `NonContiguousBucketHelpers` someday that offers similar `resolveBucketValue()`, but without the gap policy stuff?
</description><key id="59562760">9954</key><summary>Move GapPolicy and resolveBucketValues() to static helper methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels /><created>2015-03-02T22:32:26Z</created><updated>2015-03-05T15:10:30Z</updated><resolved>2015-03-05T15:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-03-02T22:44:49Z" id="76843583">@polyfractal can you add some javadocs to the methods and class?
</comment><comment author="polyfractal" created="2015-03-02T23:13:16Z" id="76848206">Oops, sure.  Didn't think about it since the aggs 2.0 stuff is in such high flux.  Will add some in a minute (and the new class needs apache headers too)
</comment><comment author="colings86" created="2015-03-03T10:23:13Z" id="76920689">@polyfractal left one super minor comment and I agree with @dakrone that we should have JavaDocs (sorry that was my fault for not putting them there to begin with) but otherwise looks great
</comment><comment author="colings86" created="2015-03-05T14:47:33Z" id="77375835">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI Failure] DiscoveryWithServiceDisruptionsTests.isolatedUnicastNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9953</link><project id="" key="" /><description>http://build-us-00.elasticsearch.org/job/es_g1gc_1x_metal/5192/consoleText

```
REPRODUCE WITH  : mvn clean test -Dtests.seed=28E4213C1D78E0DF -Dtests.class=org.elasticsearch.discovery.DiscoveryWithServiceDisruptionsTests -Dtests.slow=true -Dtests.method="isolatedUnicastNodes" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops" -Dtests.locale=fr_BE -Dtests.timezone=America/Indiana/Vevay -Dtests.processors=8
  1&gt; Throwable:
  1&gt; java.lang.AssertionError: failed to reach a stable cluster of [3] nodes. Tried via [node_t2]. last cluster state:
  1&gt; version: 6
  1&gt; meta data version: 1
  1&gt; nodes: 
  1&gt;    [node_t2][JP8Kxpm2SEeQz3f-LcewfA][metal0.elasticsearch.org][local[node_2]]{mode=local, enable_custom_paths=true}, local, master
  1&gt;    [node_t1][fDYi4S48T7aMYYQgxXJhxg][metal0.elasticsearch.org][local[node_1]]{mode=local, enable_custom_paths=true}
  1&gt; routing_table (version 1):
  1&gt; routing_nodes:
  1&gt; -----node_id[JP8Kxpm2SEeQz3f-LcewfA][V]
  1&gt; -----node_id[fDYi4S48T7aMYYQgxXJhxg][V]
  1&gt; ---- unassigned
  1&gt; 
  1&gt;     [...org.junit.*]
  1&gt;     org.elasticsearch.discovery.DiscoveryWithServiceDisruptionsTests.ensureStableCluster(DiscoveryWithServiceDisruptionsTests.java:856)
  1&gt;     org.elasticsearch.discovery.DiscoveryWithServiceDisruptionsTests.ensureStableCluster(DiscoveryWithServiceDisruptionsTests.java:832)
  1&gt;     org.elasticsearch.discovery.DiscoveryWithServiceDisruptionsTests.isolatedUnicastNodes(DiscoveryWithServiceDisruptionsTests.java:676)
  1&gt;     [...sun.*, org.junit.*, java.lang.reflect.*, com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  1&gt;     org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     java.lang.Thread.run(Thread.java:745)
```
</description><key id="59538141">9953</key><summary>[CI Failure] DiscoveryWithServiceDisruptionsTests.isolatedUnicastNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label></labels><created>2015-03-02T19:54:17Z</created><updated>2015-10-23T07:25:38Z</updated><resolved>2015-10-23T07:25:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T07:25:38Z" id="150496450">this hasn't failed in a long time.... closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write state also on data nodes if not master eligible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9952</link><project id="" key="" /><description>When a node was a data node only then the index state was not written.
In case this node connected to a master that did not have the index
in the cluster state, for example because a master was restarted and
the data folder was lost, then the indices were not imported as dangling
but instead deleted.
This commit makes sure that index state for data nodes is also written
if they have at least one shard of this index allocated.

I am a little lost with this. I found that the index can still be deleted
from a data node if the state was written but the node gets a new cluster state from a 
master that does not have it, for example because it was restarted without data folder. Happens
if the data node does not get the initial cluster state from the new but a later one and state
persistence is not disabled. 
I avoid this now by this: https://github.com/elasticsearch/elasticsearch/pull/9952/files#diff-f0f71bedb3d7e6f1cec54e8dddf5c3d3R109
but am worried about side effects this might have.  Any feedback appreciated.

closes #8823
</description><key id="59520914">9952</key><summary>Write state also on data nodes if not master eligible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T18:18:55Z</created><updated>2015-06-08T13:45:18Z</updated><resolved>2015-05-05T10:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-04T11:02:38Z" id="77138702">I left some comments...
</comment><comment author="brwe" created="2015-03-04T19:26:10Z" id="77227470">Made a pr for the deletion of index folders here: #9985 Should be easy to remove all the additional deletion code from this pr.
</comment><comment author="brwe" created="2015-03-05T18:09:12Z" id="77416506">#9985  is merged, I rebased on latest master and changed the code accordingly. I wanted to remove the change in ClusterStateEvent also because I was unable to reproduce the failures I'd seen before without it. But now I found that without the change the tests only pass on my Linux machine but fail every 10 iterations or so on my mac so something is still fishy. I'll try to come up with a detailed failure analysis tomorrow.
</comment><comment author="brwe" created="2015-03-06T10:12:20Z" id="77535805">I think I know what is going on now: The fresh master with the empty cluster state does (rarely) not send the first cluster state due to a race condition in lifecycles of DiscoveryService and its member Discovery. In DiscoveryService.doStart() the Discovery is started but the lifecyle for DiscoveryService is started only after that. This is why when the first cluster state reaches DiscoveryService.publish the lifecycle might or might not have started.

I added a commit d69f2cf1ac778cc0cfea37b351ae6ff5f1039262 where I removed the ClusterStateEvent change and added an artificial delay to the DiscoveryService.doStart() so that the tests fail reliably just so you can check if you want.

I would suggest we remove the ClusterStateEvent workaround and open another issue for this because this behavior is not a result of this pull request.
</comment><comment author="s1monw" created="2015-03-06T12:55:03Z" id="77554456">I agree with your idea of opening a new issue for the ClusterSTateEvent problem
</comment><comment author="brwe" created="2015-03-10T23:48:22Z" id="78173549">Chatted with @s1monw and now rewrote it so that the selection of what to write is not done in GatewayMetaState anymore. I tried to do it similar to #10016. It is still a little raw but but would be great if you could let me know if this is the right direction. 
</comment><comment author="s1monw" created="2015-03-12T20:13:03Z" id="78595891">I left a bunch of comments 
</comment><comment author="brwe" created="2015-03-17T20:54:07Z" id="82596323">Addressed all comments. I am unsure about two things, left comments above about it: We did check on disk if we have the state already before writing in case the in memory state is null. However, this should be a rare event so I am unsure if we need this optimization. In addition, I made it so that whenever a shard is initializing on a node we write the meta state of this index and perform no check if we wrote before already. Should I check instead?
</comment><comment author="s1monw" created="2015-03-18T06:03:06Z" id="82764165">I like this  a lot - left some minor comments
</comment><comment author="brwe" created="2015-03-18T22:25:15Z" id="83207715">Addressed all comments except for the version check  thing above. We cannot use index version to ensure that the state is written because the version is not necessarily updated for the index metadata if shards are relocated. But shards should initialize too often on a node so it might be ok if we write the same state each time we see an initializing shard?
</comment><comment author="brwe" created="2015-03-19T18:14:54Z" id="83698411">Chatted with @s1monw who reminded me that shards can be in initializing state for a while and while they are we would always write, so that is not a good idea. Will now instead check in the event if the last state had shards allocated and if not then write if the new one has.  
</comment><comment author="brwe" created="2015-03-19T19:25:58Z" id="83728218">pushed a new commit to address this. we now check if a shard was already present in last cluster state and if so not write.
</comment><comment author="s1monw" created="2015-03-19T23:25:43Z" id="83801666">it LGTM I think @bleskes should take one more look 
</comment><comment author="bleskes" created="2015-03-30T08:36:13Z" id="87591729">Looks good in general. Left some comments that I think will simplify things.
</comment><comment author="brwe" created="2015-04-01T10:29:45Z" id="88431712">Implemented all suggestions. 
@bleskes thanks for the tip with the indices list caching, it simplifies think indeed! Please take another look.
</comment><comment author="bleskes" created="2015-04-28T20:07:24Z" id="97192104">Left some very minor comments. Feel free to push without another review. LGTM!
</comment><comment author="brwe" created="2015-04-29T15:24:04Z" id="97469117">need to investigate https://github.com/elastic/elasticsearch/issues/10017 before we can push
</comment><comment author="brwe" created="2015-04-30T08:34:58Z" id="97707128">The reason why the tests failed on CI is the same I described in the beginning https://github.com/elastic/elasticsearch/pull/9952#issue-59520914 : a data node receives a new cluster state from a master that does not have the index in its state but the data node missed the state with a no master block before and so state persistence was not disabled. the fact that an index is not in the cluster state is then interpreted as delete command. This can happen here for the reasons described in #10017 but there might be other reasons as well. I now think we should not delete indices at all if the cluster state that would cause a deletion comes from a new master.
I added a new commit for this but need someone to confirm that this is actually the right solution.
</comment><comment author="s1monw" created="2015-05-04T12:37:02Z" id="98695355">&gt; I added a new commit for this but need someone to confirm that this is actually the right solution.

+1 to the solution
</comment><comment author="brwe" created="2015-05-04T15:36:25Z" id="98755512">Chatted with @kimchy and we decided to push as is and add a //norelease comment and open an issue because the short term fix for the  problem (https://github.com/elastic/elasticsearch/pull/9952#issuecomment-97707128) is not very elegant.
Added another commit to address the latest comments.
</comment><comment author="s1monw" created="2015-05-05T09:40:06Z" id="99011680">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Very high disk read io when bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9951</link><project id="" key="" /><description>We have about 95 nodes in our cluster,and index have 89 shards.
We send bulk requests to these nodes random,total about 1 million per seconds.

Every thing is good except some nodes(about 5~10) have very high disk read io:
Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sdh               0.00     0.50 19739.00   18.00 573624.00  3128.00    29.19    75.48    3.82   0.05 100.00
sdg               0.00     0.00 10850.00    0.00 298556.00     0.00    27.52     4.26    0.39   0.06  65.30

but other nodes are normal,they have very little read io:
Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
sdh               0.00     0.50    0.00   41.00     0.00  8964.00   218.63     0.22    5.45   0.23   0.95
sdg               0.00     0.00    1.50    0.00    12.00     0.00     8.00     0.00    0.00   0.00   0.00

I think this is very abnormal,there is not other process except  ES ,and the disk is not bad.
I look the index codes ,it seems that only the "loadVersion" function need to read the disk,but how can it use so many read io? Please help ,thanks very much.
</description><key id="59507461">9951</key><summary>Very high disk read io when bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sylvae</reporter><labels><label>feedback_needed</label></labels><created>2015-03-02T16:46:46Z</created><updated>2016-10-27T04:06:26Z</updated><resolved>2015-03-10T16:02:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-03T12:56:57Z" id="76940303">what version of ES are you using?  And do the hot nodes have more primary shards than others?  Also, how many shards per node in total?
</comment><comment author="sylvae" created="2015-03-03T13:51:16Z" id="76948087">I am using ES 1.4.2. I have set  "index.routing.allocation.total_shards_per_node": "3", so the
number of the primary shards is almost the same. We create one index per day, and import today's data to it. Every index has 89 shards and 1 replic. Also, no search at the time.
</comment><comment author="sylvae" created="2015-03-10T16:02:18Z" id="78085294">I have found the reason&#65292;it was not the ES's bug.It was because of too much memory fragmentation and memory recall.
</comment><comment author="milodky" created="2016-01-12T23:12:07Z" id="171093192">Hey @sylvae , I have the exact same issue, how you solved this?
</comment><comment author="stephanustedy" created="2016-10-27T04:06:26Z" id="256541689">how to solve this issue ? have you guys solve it ? 
@milodky @sylvae 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logs being sent to a closed index cause logstash to lock up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9950</link><project id="" key="" /><description>When sending logs (via logstash) if the log is sent to a closed index, there is no good error message reported and it causes logstash to lock up? Is there a solution for this?
</description><key id="59504487">9950</key><summary>Logs being sent to a closed index cause logstash to lock up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">efontana</reporter><labels><label>feedback_needed</label></labels><created>2015-03-02T16:26:35Z</created><updated>2015-03-03T09:03:24Z</updated><resolved>2015-03-03T09:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-02T17:36:17Z" id="76758719">Hi @efontana 

Trying to index into a closed index returns a 403 status code and:

```
{ "error" : "IndexClosedException[[my_index] closed]", "status": 403 }
```

...which seems pretty self-explanatory.  Perhaps logstash isn't checking for this error but should be?

@jordansissel ^^
</comment><comment author="jordansissel" created="2015-03-02T17:39:09Z" id="76759321">This belongs on logstash, not elasticsearch. @efontana can you move this to logstash-plugins/logstash-output-elasticsearch?
</comment><comment author="clintongormley" created="2015-03-03T09:03:24Z" id="76909369">thanks @jordansissel - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: updated resilience page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9949</link><project id="" key="" /><description>this docs change should have been included in #7994
</description><key id="59503075">9949</key><summary>Docs: updated resilience page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2015-03-02T16:17:22Z</created><updated>2015-05-18T23:26:25Z</updated><resolved>2015-03-03T14:25:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-03T13:27:18Z" id="76944553">LGTM
</comment><comment author="bleskes" created="2015-03-03T13:33:33Z" id="76945474">LGTM. Left one small comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Empty document allows multiple usage of same "_id"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9948</link><project id="" key="" /><description>My test scenario is the following:
I am using parent &lt;=&gt; child relations in order to connect my documents. Usually I am using empty documents where I set the "_id" field as parent. All children are connected to this document.

What I figured out is: If a document is empty I can save the same document with the same "_id" multiple times. Also this document shows up multiple times in the index. If I add a field to this document I get a "document already exists" exception (this is what I would expect) when I want to insert the document multiple times.

Is this a bug?
</description><key id="59494493">9948</key><summary>Empty document allows multiple usage of same "_id"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">christtonk</reporter><labels><label>feedback_needed</label></labels><created>2015-03-02T15:18:02Z</created><updated>2015-03-03T09:21:06Z</updated><resolved>2015-03-03T09:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-02T15:29:58Z" id="76731985">@christtonk Can you provide a recreation demonstrating what you are doing please?  If you're using different parent IDs for each child document, then this is expected.
</comment><comment author="christtonk" created="2015-03-02T16:02:30Z" id="76738811">@clintongormley I am using the Java API ... I will extract the code that looks suspicious to me ... just need some time.
</comment><comment author="clintongormley" created="2015-03-02T16:14:47Z" id="76741340">If you could provide the demonstration as curl statements, it would be easier.
</comment><comment author="christtonk" created="2015-03-02T16:23:51Z" id="76743279">@clintongormley Somehow I cannot reproduce it with curl statements ... but with my Java Code I can
</comment><comment author="christtonk" created="2015-03-03T08:16:05Z" id="76903962">OK, figured out what is happening and I think the mistake is on my side: I am using routing extensively. What I am able to do is: inserting the same document into the same index with the same "_id" but with different routing information twice without getting a document already exists exception. What I understood so far is that this is the correct behavior.
</comment><comment author="clintongormley" created="2015-03-03T09:21:06Z" id="76911854">Yes, this is correct.  The default `_routing` value is the `_id`, so if you change that (by specifying a parent or routing value) then it is up to you to ensure that you don't reuse the same `_id` on a different shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TESTS] Make sure test end with ..Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9947</link><project id="" key="" /><description>This commit adds a simple testcase that ensures all our tests end with the right naming.

Closes #9945
</description><key id="59490230">9947</key><summary>[TESTS] Make sure test end with ..Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T14:47:38Z</created><updated>2015-03-19T15:32:48Z</updated><resolved>2015-03-02T16:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-02T14:48:11Z" id="76723849">@javanna can you take a look?
</comment><comment author="javanna" created="2015-03-02T15:02:27Z" id="76726571">Left a few super minor comments, LGTM though. I was hoping there would be some maven plugin to achieve the same, but a specific test is fine with me.
</comment><comment author="s1monw" created="2015-03-02T15:02:56Z" id="76726653">&gt; . I was hoping there would be some maven plugin to achieve the same, but a specific test is fine with me.

progress over perfection ... I think we can still replace if something better is around
</comment><comment author="javanna" created="2015-03-02T15:05:07Z" id="76727040">I think this is great, actually better than any plugin at this point :)
</comment><comment author="s1monw" created="2015-03-02T16:16:49Z" id="76741752">@javanna I added some more functionality and found some more problems
</comment><comment author="javanna" created="2015-03-02T17:40:13Z" id="76759531">looks great @s1monw thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Work around URI encode limitations in RestClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9946</link><project id="" key="" /><description>We've been relying on URI for url encoding, but it turns out it has some problems. For instance '+' stays as is while it should be encoded to `%2B`. If we go and manually encode query params we have to be careful though not to run into double encoding ('+'=&gt;'%2B'=&gt;'%252B'). The applied solution relies on URI encoding for the url path, but manual url encoding for the query parameters. We prevent URI from double encoding query params by using its single argument constructor that leaves everything as is.

We can also revert back the expression script REST test that revealed this to its original content (which contains an addition).

Closes #9769
</description><key id="59484226">9946</key><summary>[TEST] Work around URI encode limitations in RestClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T14:00:11Z</created><updated>2015-03-19T10:54:00Z</updated><resolved>2015-03-03T09:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-02T14:01:17Z" id="76715840">@spinscale can you have a look please?
</comment><comment author="spinscale" created="2015-03-03T08:58:25Z" id="76908688">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TESTS] Make sure test end with ..Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9945</link><project id="" key="" /><description>We currently filter test classes by suffix `Test` or `Tests` (https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L495), but there is no automation in place to guarantee that test classes actually end with this suffix. As a result, some tests did not run on CI without anyone noticing (see for example https://github.com/elasticsearch/elasticsearch/commit/600cb886da0b8a4adae9a3a0eb688697c10c1491). Would be great if we had some automation that checks that classes that derive from Test classes like `ElasticsearchIntegrationTest` have the right suffix.
</description><key id="59482045">9945</key><summary>[TESTS] Make sure test end with ..Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T13:40:36Z</created><updated>2015-03-02T16:35:10Z</updated><resolved>2015-03-02T16:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-02T13:48:01Z" id="76713832">+1 I think we should rather take `ElasticsearchTestCase` as the base class though.
</comment><comment author="martijnvg" created="2015-03-02T13:57:38Z" id="76715281">Or if a test class has '@Test' then the class name must end with *Test
</comment><comment author="javanna" created="2015-03-02T14:02:50Z" id="76716123">@martijnvg I'm afraid we don't use the `@Test` annotation consistently though... we better look also for method names that start with `test` if we take that approach.
</comment><comment author="martijnvg" created="2015-03-02T14:13:43Z" id="76717848">Yes, we take this approach we should also look for method names starting with `test`. I think we either should the test method name consistently or the test annotation, but that is a different discussion.
</comment><comment author="s1monw" created="2015-03-02T16:35:10Z" id="76745836">thanks so much @brwe for finding this... `das gl&#252;ck des t&#252;chtigen...` 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>toString for SearchRequestBuilder and CountRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9944</link><project id="" key="" /><description>Fixed SearchRequestBuilder#toString to not wipe the request source when called.

Improved SearchRequestBuilder#toString to support the different ways a query can be set to it. Also printed out a merged version of source and extraSrouce in case there in case any extraSource is set, to reflect what will happen when executing the request builder.

Implemented toString in CountRequestBuilder

Closes #5576
Closes #5555
</description><key id="59470640">9944</key><summary>toString for SearchRequestBuilder and CountRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.5.3</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T11:42:38Z</created><updated>2015-06-11T07:46:41Z</updated><resolved>2015-04-28T09:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-03-02T19:32:32Z" id="76792613">I think the `toString` here looses the meaning of the request? I wonder if actually someone won't wish to see something like `source={}, extra_source={}` then its clearer what happens in the context of this builder? It will also simplify the toString implementation quite a bit I would hope?
</comment><comment author="javanna" created="2015-03-03T08:47:47Z" id="76907393">I see what you mean @kimchy . I think that there are a couple of different problems with this `toString` implementation.

First of all it prints out only part of the request (index, type, routing etc. are ignored) which in my mind relates to the request body (when the request comes in through REST layer). This hasn't changed in this PR, but it did get more convoluted since I am now attempting to print out what is actually going to be executed (hence merging extra_source and source, cause that is what happen at execution time anyway).

Also, I wonder if users expect the output to be a valid json query that can be sent back to elasticsearch or not. If we add the `extraSource` to the current json, we break this "contract". In general I think a `toString` should just print out the content of the request, it doesn't need to be a json either and the fact that it is a json is misleading. I would consider, on the long term, moving away from json and just print out everything that the request contains, not only what we can represent as json.

That said, what I am proposing is a bigger (breaking?) change that deserves its own issue and discussion, and should be applied to every request consistently (after #9201). I am leaning towards fixing the actual bug only in this PR, the fact that the `toString` modifies the content of the request. Shall I take take out the merging then? I think that printing out the `extraSource` in the current json might not make users happy, we should just move away from json if we do that. Makes sense?
</comment><comment author="kimchy" created="2015-03-03T11:20:43Z" id="76928095">@javanna yea, I think fixing the actual bug here is the best way forward. Personally, I am leaning towards not adding the behavior of being able to take a request `toString` and using it as the body, since its more than that..., sometimes the request also have parameters that go over the URI when its a RESTful request, ... . I think we should keep toString into being a nice way to debug requests if you log them, with all the relevant information to be able to do so.

If we want to take such a project of having the request be represented as rest requests, then we need to have something like `RestRequest toRestRequest()`, but I personally, at least now, view it as a low priority task because of the combination of its scope of change and the added value.
</comment><comment author="javanna" created="2015-03-03T12:07:52Z" id="76933894">I just opened #9962 to keep track of the bigger change we've been discussing. As for fixing this bug, what should we do? 
- Simplify the PR by ignoring the extra_source part of the request, which we currently merge to the source (keeping the output as a valid json that one can send to elasticsearch). It was previously ignored as well...
- Simplify the PR by printing out the extra_source separately, which we currently merge to the source, as part of the same json object
- Leave the PR as is, print out the merged source and extra_source, so that the output is a json that represent the query that will get executed when the builder is run
- your idea goes here
</comment><comment author="javanna" created="2015-03-03T12:09:50Z" id="76934112">if I understand correctly @kimchy , you vote for 

&gt; Simplify the PR by printing out the extra_source separately, which we currently merge to the source, as part of the same json object

Is that right?
</comment><comment author="kimchy" created="2015-03-03T12:11:35Z" id="76934341">I think we need to fix the bug first of all..., we can improve toString while we are at it, but maybe in a different pull request?
</comment><comment author="javanna" created="2015-03-07T14:43:17Z" id="77691872">Agreed @kimchy I updated the PR to only fix the bug. Removed the extra_source merge. Note that extra_source is ignored, as before the fix. But the right source gets now printed out depending on what's set. Let me know what you think
</comment><comment author="kimchy" created="2015-03-08T17:51:52Z" id="77764174">@javanna left a minor comment
</comment><comment author="javanna" created="2015-03-13T00:30:25Z" id="78715504">Updated to properly handle binary formats as well and added tests for it, can you have another look please @kimchy ?
</comment><comment author="javanna" created="2015-03-19T11:46:06Z" id="83521136">@kimchy @s1monw what do you think? would like to get this in ;)
</comment><comment author="cooniur" created="2015-03-23T19:42:21Z" id="85163979">Hey @javanna , how is the bug fixing going? I saw 1.5.0 has been released. Does it include the fix of this toString() bug?

I noticed you guys talking about the use of toString(). IMHO, I feel the proper use case of toString() would be only for logging and debugging. In this case, developers probably wouldn't care too much about whether toString() returns a proper JSON string or something else, as long as it returns correct and enough information about the QueryBuilder object.

The most important thing is to make sure that calling toString() will never change the state of QueryBuilder, which is probably something you guys could start with to fix the bug, and then collect feedback, and keep improving it.

Thanks!
</comment><comment author="javanna" created="2015-03-23T19:53:12Z" id="85167416">Hi @cooniur the PR is still open which means that it didn't make it into `1.5.0`. It will most likely make it into the next release, I am just waiting for a final review here, have a look at the code and tell me what you think if you feel like it. It addresses exactly the issue of changing the content of the request. The side discussions we had were more generic around `toString` and what it should do (I agree with you btw) which I opened another issue for (#9962).
</comment><comment author="javanna" created="2015-03-25T12:14:22Z" id="86001492">ping @kimchy can you have a look please?
</comment><comment author="javanna" created="2015-04-07T16:59:54Z" id="90646297">ping @s1monw @kimchy can either of you have a look please? this bug has been around for too long, would love to get it fixed.
</comment><comment author="javanna" created="2015-04-16T16:07:17Z" id="93773502">@jpountz maybe you can have a look?
</comment><comment author="jpountz" created="2015-04-23T19:58:48Z" id="95701398">LGTM
</comment><comment author="javanna" created="2015-04-28T09:23:44Z" id="96986541">Merged to master, 1.x and 1.5.
</comment><comment author="cooniur" created="2015-06-11T07:46:41Z" id="111032099">Hey @javanna , thanks for fixing this issue in 1.6! Being waiting for it for a long time but eventually get it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Synchronize RecoveryState.timer methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9943</link><project id="" key="" /><description>Avoids a minor concurrent race condition causing time() to potentially return a negative value

Surfaced by our CI:

http://build-us-00.elasticsearch.org/job/es_g1gc_1x_metal/5035/
</description><key id="59455558">9943</key><summary>Synchronize RecoveryState.timer methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-02T09:15:43Z</created><updated>2015-06-08T00:22:02Z</updated><resolved>2015-03-02T10:04:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-02T09:28:13Z" id="76680945">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Engine: back port #9648  - Fix deadlock problems when API flush and finish recovery happens concurrently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9942</link><project id="" key="" /><description>Issue #9648 fixes a potential deadlock between two concurrent flushes - one at the end of recovery and one through the API or background flush.  This back ports the logic to 1.4 . It is slightly more contrived as we still use the write lock in the flush code. 

If we feel we have some concerns about this approach we can also move the recovery flush to happen on a generic thread.
</description><key id="59455043">9942</key><summary>Engine: back port #9648  - Fix deadlock problems when API flush and finish recovery happens concurrently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Engine</label><label>bug</label><label>v1.4.5</label></labels><created>2015-03-02T09:10:32Z</created><updated>2015-04-11T16:53:01Z</updated><resolved>2015-03-02T11:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-02T09:56:27Z" id="76684641">left one comment - LGTM though
</comment><comment author="bleskes" created="2015-03-02T10:40:37Z" id="76690390">@s1monw I added a comment can you sanity check it? these can be tricky to explain right
</comment><comment author="s1monw" created="2015-03-02T10:41:09Z" id="76690450">LGTM
</comment><comment author="bleskes" created="2015-03-02T11:00:09Z" id="76692943">pushed into 1.4 . Thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ubuntu update breaks ES ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9941</link><project id="" key="" /><description>Friday, at ~ 9 a.m. UTC, an update popped in and everyone in my office did it. Here is the apt log for this update :

&gt; Start-Date: 2015-02-27  10:12:28
&gt; Upgrade: multiarch-support:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), cups-core-drivers:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), cups-server-common:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), nscd:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), libcups2-dev:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libcups2:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), cups-daemon:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libcupsmime1:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), cups-client:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libc-dev-bin:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), libcupsimage2:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libc-bin:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), libc6:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), libc6:i386 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), cups:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), cups-bsd:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libcupscgi1:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libcupsppdc1:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libc6-i386:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), cups-common:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), cups-ppdc:amd64 (1.7.5-3ubuntu3, 1.7.5-3ubuntu3.1), libc6-dbg:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3), libc6-dev:amd64 (2.19-10ubuntu2.2, 2.19-10ubuntu2.3)
&gt; End-Date: 2015-02-27  10:13:46

 I do not use ES personally, but 2 of my colleagues do, and have been complaining about memory problems since the update : during the index population, the index would revert to a previous state. I can't see how cups could cause this bug, whereas libc6 seems to be about memory management. Did anyone else experience this problem after this upgrade ?

@clebeaupin , you experienced this bug, could you please add more details to this issue ?
</description><key id="59454421">9941</key><summary>Ubuntu update breaks ES ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">greg0ire</reporter><labels><label>feedback_needed</label></labels><created>2015-03-02T09:03:49Z</created><updated>2015-04-26T19:43:20Z</updated><resolved>2015-04-26T19:43:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-02T10:11:07Z" id="76686534">@greg0ire @clebeaupin you upgraded all of these packages (including libc) while still running Elasticsearch?  Did you restart Elasticsearch and continue to see these problems?
</comment><comment author="clebeaupin" created="2015-03-02T10:17:07Z" id="76687308">Yes ElasticSearch was running during the upgrade.
After the upgrade, I restarted my computer and I still got the same issue.
To continue to work with ElasticSearch, I had to install it on a separate virtual machine that was not upgraded with the last version of libc6. I have not test it again on my local computer.  
</comment><comment author="clintongormley" created="2015-03-02T11:50:16Z" id="76699156">@clebeaupin could you provide more info about what you saw going wrong?  at the moment it's in the "it doesn't work" category...
</comment><comment author="clintongormley" created="2015-03-02T11:50:37Z" id="76699190">a recreation would be great...
</comment><comment author="sfaribault" created="2015-03-02T14:15:03Z" id="76718050">Hi, I also experienced this issue.
I managed to work around it by commenting out this line in `elasticsearch.yml`:
`discovery.zen.ping.multicast.enabled: false`

I found this error in my logs :

```
failed to connect to master [[Bart Hamilton][7livEvjOQ8uCTGOeMuAH2A][ubuntu-parallels][inet[/192.168.0.13:9300]]], retrying...
   org.elasticsearch.transport.ConnectTransportException: [Bart Hamilton][inet[/192.168.0.13:9300]] connect_timeout[30s]
           at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:807)
          at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:741)
          at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:714)
          at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:150)
          at org.elasticsearch.discovery.zen.ZenDiscovery.joinElectedMaster(ZenDiscovery.java:441)
          at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:393)
          at org.elasticsearch.discovery.zen.ZenDiscovery.access$6000(ZenDiscovery.java:80)
          at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1320)
          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
          at java.lang.Thread.run(Thread.java:745)   
```

By googling it, I was redirected toward [this page](http://elasticsearch-users.115913.n3.nabble.com/exception-at-startup-failed-to-send-ping-request-over-multicast-td2097917.html) which hinted toward the solution.

Hope this helps
</comment><comment author="clintongormley" created="2015-04-26T19:43:20Z" id="96425906">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to set index.query.bool.max_clause_count ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9940</link><project id="" key="" /><description>Hello All,

I'm added following line in `/etc/elasticsearch/elasticsearch.yml`

```
index.query.bool.max_clause_count: 10000
```

After restart ES i was not find the above value:

```
"test-index": {
      "settings": {
         "index": {
            "number_of_replicas": "0",
            "number_of_shards": "1",
            "uuid": "oVEXByHTQWi8gzltRkqYLA",
            "version": {
               "created": "1020499"
            }
         }
      }
   }
}
```

Any help on this
</description><key id="59446371">9940</key><summary>How to set index.query.bool.max_clause_count ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MiteshShah</reporter><labels /><created>2015-03-02T07:11:34Z</created><updated>2016-04-07T07:53:15Z</updated><resolved>2015-03-02T09:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-02T09:58:45Z" id="76684923">The setting in the config file won't show up in the index.  It is a node level setting.
</comment><comment author="dylanninin" created="2016-04-07T07:53:15Z" id="206746054">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards in the Preference API can now be 'bound' to a certain index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9939</link><project id="" key="" /><description>The PR extends the syntax for `_shards` parameter for the preference API to specify the index on which it is scoped to. This is needed when dealing with aliases which can point to multiple indices and thus when asking for a shard on a certain node, one does not know what indices are being used (depends on the runtime configuration) which in turn, can lead to duplicates (3 indices with replicas on two nodes with the following setup: node1:A, B, node2: B, C - when asking for the shard on node 1 and 2, one gets back A,B,B,C and there's now to avoid this currently through the preference API).

The PR introduced the syntax `_shards=index[#shardNumber]`. So assuming one uses an alias that points to index `test` and `extra` one can now say `/test,extra/_search?preference=...`:
- `_shards=test[0]` - to return _only_ shard 0 of index `test` and ignore anything else (`extra`)
- `_shards=test[0],test[1]` - to return _only_ shards 0 and 1 for index `test`and nothing from `extra`
- `_shards=te*t[0]` - to return shard 0 for any index under the current preference that matches the pattern (in this case `test`)
- `_shards=test[*]` - to return all the shards for index `test` but nothing for `extra`
- `_shards=test[0],extra[1]` - to return shard 0 for `test` and 1 for `extra`

Note the previous syntax still works, e.g.:
- `_shards=0` - to get shard 0 from all backing indices (`test` and `extra`)
- `_shards=0,1` - to get shard 0 and 1 from all backing indices (`test` and `extra`)

relates #9483 
</description><key id="59421987">9939</key><summary>Shards in the Preference API can now be 'bound' to a certain index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>won't fix</label></labels><created>2015-03-01T22:50:44Z</created><updated>2015-05-29T12:06:21Z</updated><resolved>2015-05-29T12:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-02T08:37:05Z" id="76674898">there are a couple of things that I think we need to rethink here:
- what if an index has `[` in it's name?
- do we need to push complex parsing logic into Elasticsearch just to fit stuff into URL parameters, IMO we shouldn't.
- Can we expose a safe API to wherever its needed ie. `{ shards : { idx : [1, 3], idx1 : [1] }`  and move the parsing logic for the `_shards` somewhere else for BWC (maybe in the REST endpoint)?
</comment><comment author="s1monw" created="2015-03-20T20:43:57Z" id="84139821">@costin this didn't go anywhere right, can we close it?
</comment><comment author="s1monw" created="2015-05-29T12:06:20Z" id="106783891">this is too complicated for this API we discussed this internally and closing it for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/nodes: Thread null handling through stats and info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9938</link><project id="" key="" /><description>Was performing some testing the other day with various kinds of failure scenarios and still ran into `/_cat/nodes` giving me NPE (reminiscent of #6297).  I was able to find at least three or four different objects that turned up `null`, including `JvmStats`.  To play it safe I'd rather just catch nulls at as many points as we can (until we can use `Optional` at least).  The times when this happens are the most critical -- something is terribly wrong with the cluster -- and an erstwhile useful tool becomes useless.
</description><key id="59421818">9938</key><summary>_cat/nodes: Thread null handling through stats and info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-03-01T22:46:27Z</created><updated>2015-03-19T10:16:39Z</updated><resolved>2015-03-17T23:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-03-02T22:40:27Z" id="76842800">I like the thought of simplifying, but the nulls are actually valuable information.  The better enhancement would be to have `headers` not only be a cosmetic change but keep from generating unneeded stats.
</comment><comment author="s1monw" created="2015-03-17T16:46:37Z" id="82469873">I think at some point we should automate this a bit more than it is today and maybe use some reflection to get the info given the names. That said this LGTM as a bugfix while I agree with igor on the simpliciation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed duplicate text.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9937</link><project id="" key="" /><description /><key id="59421781">9937</key><summary>Removed duplicate text.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreasKl</reporter><labels /><created>2015-03-01T22:45:44Z</created><updated>2015-03-02T10:13:20Z</updated><resolved>2015-03-02T10:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AndreasKl" created="2015-03-02T09:21:05Z" id="76679986">Signed CA.
</comment><comment author="clintongormley" created="2015-03-02T10:13:20Z" id="76686815">Hi @AndreasKl 

Actually, the text is not duplicated.  There is `dfs_query_THEN_fetch` and `dfs_query_AND_fetch`.  But i've updated the docs to be more explicit about the fact that you shouldn't use the `AND` version

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed duplicate text.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9936</link><project id="" key="" /><description>Removed duplicate text from documentation.
</description><key id="59421357">9936</key><summary>Removed duplicate text.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreasKl</reporter><labels /><created>2015-03-01T22:35:36Z</created><updated>2015-03-01T22:41:20Z</updated><resolved>2015-03-01T22:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AndreasKl" created="2015-03-01T22:39:50Z" id="76635761">Signed CLA.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard queries and regex don't run on indices (_id)s</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9935</link><project id="" key="" /><description>Hi,

I have been trying to run wildcard queries against .kibana indices on ES 1.4.4. I am querying on _id. While prefix query runs, wildcard queries don't. 

Example 

&gt; curl localhost:9200/,kibana/_search?pretty -d '{"query":{"prefix":{"_id":"TC"}}}'

Gives results, but 

&gt; curl localhost:9200/,kibana/_search?pretty -d '{"query":{"wildcard":{"_id":"TC*"}}}'

Gives me an empty result. 
</description><key id="59409897">9935</key><summary>Wildcard queries and regex don't run on indices (_id)s</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-03-01T18:01:58Z</created><updated>2015-12-05T18:54:32Z</updated><resolved>2015-12-05T18:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T18:54:32Z" id="162234590">Duplicate of #15070
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Min or max aggregation of date returns floating point number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9934</link><project id="" key="" /><description>A min or max aggregation of a date field returns a floating point number, epoch milliseconds, despite it being internally stored as a long. Should it really return an long? In some rare cases, will floating point not return an entirely accurate answer?
</description><key id="59397372">9934</key><summary>Min or max aggregation of date returns floating point number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Aggregations</label></labels><created>2015-03-01T14:16:48Z</created><updated>2015-05-27T09:57:14Z</updated><resolved>2015-05-27T09:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="haight6716" created="2015-05-26T19:04:11Z" id="105636147">I'm seeing something similar, but worse.  Dates being returned in scientific notation with the wrong value.  I get stuff like: 

```
"aggregations": {
    "time-test": {
        "avg": 7.078292805144e-312,
        "avg_as_string": "1970-01-01T00:00:00.000Z",
        "count": 1847,
        "max": 7.078307915815e-312,
        "max_as_string": "1970-01-01T00:00:00.000Z",
        "min": 7.07828016824e-312,
        "min_as_string": "1970-01-01T00:00:00.000Z",
        "sum": 1.307360681110475e-308,
        "sum_as_string": "1970-01-01T00:00:00.000Z"
    }
}
```

The field in question contains dates from the modern day (2015), not 1970.

```
"version" : {
  "number" : "1.5.0",
  "build_hash" : "544816042d40151d3ce4ba4f95399d7860dc2e92",
  "build_timestamp" : "2015-03-23T14:30:58Z",
  "build_snapshot" : false,
  "lucene_version" : "4.10.4"
}
```

Soon after, the problem disappeared without having changed any code.  It just seems to happen based on some sensitivity to the data in the index.  I can't reproduce the problem now.  I didn't delete any data from the index, just kept adding more.  And it magically fixed itself.  Hmm/grr.
</comment><comment author="clintongormley" created="2015-05-27T09:57:13Z" id="105848280">Hi @jimmyjones2 - I'm going to close this as a duplicate of #9545
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store: renaming temp files should log errors of delete in trace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9933</link><project id="" key="" /><description>The method follows a delete then rename pattern. It currently logs errors in the delete under DEBUG and barfs if the rename fails. It also correctly ignores FileNotFoundException in the first delete round. This protection fails however since lucene uses File.delete() (which is changed in 5.0) so we get a generic IOException in this case. This can result in worrisome  log messages like:

```
[2015-02-28 16:11:42,720][DEBUG][index.store              ] [node_s2] [test][0] failed to delete file [segments_1]
java.io.IOException: Cannot delete .../nodes/2/indices/test/0/index/segments_1
    at org.apache.lucene.store.FSDirectory.deleteFile(FSDirectory.java:272)
```

I also gave the method a better name.
</description><key id="59389465">9933</key><summary>Store: renaming temp files should log errors of delete in trace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label></labels><created>2015-03-01T09:23:28Z</created><updated>2015-03-19T10:55:00Z</updated><resolved>2015-03-02T11:59:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-02T10:36:13Z" id="76689807">LGTM in general, I wonder if we should fail if the delete fails since it means something holds on to this so it will be problematic? I also wonder why this is 1.5 only?
</comment><comment author="bleskes" created="2015-03-02T10:55:17Z" id="76692278">@s1monw  this is needed because in 1.5 we don't differentiate between proper errors and a file not found. See above: 

&gt; This protection fails however since lucene uses File.delete() (which is changed in 5.0) so we get a generic IOException in this case

2.0 uses lucene 5.0 where this was changes to use Files.delete which does throw the right exception. I will pull the method rename into master.
</comment><comment author="s1monw" created="2015-03-02T11:47:04Z" id="76698795">ok thx
</comment><comment author="bleskes" created="2015-03-02T11:59:18Z" id="76700246">pushed to 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added note on max # of documents per shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9932</link><project id="" key="" /><description>Closes #9911
</description><key id="59376842">9932</key><summary>Added note on max # of documents per shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-03-01T04:03:31Z</created><updated>2015-03-01T20:08:57Z</updated><resolved>2015-03-01T20:08:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-03-01T20:08:57Z" id="76627596">thanks @ppf2 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java REST runner interprets integers as doubles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9931</link><project id="" key="" /><description>In https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/script/30_expressions.yaml#L28 the JSON response is:

```
...
          {
             "fields" : {
                "my_field" : [
                   23
                ]
             },
...
```

But the number `23` is interpreted by the Java REST runner as `23.0`, which means that the assertion that it should be `23` fails.
</description><key id="59355290">9931</key><summary>Java REST runner interprets integers as doubles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>test</label></labels><created>2015-02-28T19:50:09Z</created><updated>2015-03-02T09:27:26Z</updated><resolved>2015-03-02T09:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T19:51:33Z" id="76542643">@javanna could you take a look at what can be done here please?  If it is not easily fixable, then we could change it to check for &gt;= 23 and &lt;= 23, but that feels suboptimal...
</comment><comment author="javanna" created="2015-03-01T10:37:06Z" id="76591346">I had a look and I see something different. The json response that I print out as a string before any parsing is the following:

```
{took=340, timed_out=false, _shards={total=2, successful=2, failed=0}, hits={total=1, max_score=1.0, hits=[{_index=test123, _type=test, _id=1, _score=1.0, fields={my_field=[23.0]}}]}}
```

As far as I can see the expression lang is responsible for returning the integer field as a float. The java REST runner just uses the same parser that elasticsearch uses internally, which tries and detect the different numeric types and does see the difference between floats and integers.

I am afraid we have two potentials problems then:
- numbers are always float when using the expression language, maybe not a problem after all, we just need to be aware of this?
- the perl runner is somehow interpreting `23.0` as `23` and makes it an integer?

Long story short I don't see any problem with the java REST tests runner here... as far as I can see the original test that was checking for `23.0` was correct, cause that's what the expression script returns. Can you double check please @clintongormley ?
</comment><comment author="clintongormley" created="2015-03-01T20:25:38Z" id="76628474">@javanna you are right - this is a Perl issue.  It silently converts `23.0` in the JSON into an integer (which is why it logged the integer in the trace logs).  Apologies for the noise - I'll probably have to work around this with a range check.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SQL DBA excited, almost a convert to NOSQL and it wont start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9930</link><project id="" key="" /><description>Newbie trying to get ElasticSearch 1.4.2. or 1.4.4 simple quick start in Windows accomplished.  Dipping big toe into Open Source NOSQL waters and ... "The system cannot find the path specified".   

Been trying to debug but without minimum requirements document I don't know what is wrong.  I have played with paths (\ vs /).   Java_Home is set java -version works (1.8.0_31 Java SE Runtime).
On Windows 7.   Will try on heftier box but big disappoint this afternoon. 

So open source community, please help.  Following is result of adding echo statements to .bat and .in.bat

C:\Program Files\elasticsearch-1.4.4&gt;bin\elasticsearch.bat
SCRIPT_DIR  C:\Program Files\elasticsearch-1.4.4\bin\
ES_HOME     C:\Program Files\elasticsearch-1.4.4
ES_HOME     C:\Program Files\elasticsearch-1.4.4
ES_CLASSPATH   nothing
ES_CLASSPATH  ;C:\Program Files\elasticsearch-1.4.4\lib\elasticsearch-1.4.4.jar;C:\Program Fil
es\elasticsearch-1.4.4\lib*;C:\Program Files\elasticsearch-1.4.4\lib\sigar*
ES_PARAMS -Delasticsearch -Des-foreground=yes -Des.path.home="C:\Program Files\elasticsearch-1.4.4"

ABOUT_TO_START at bottom of .bat
JAVA_HOME C:\Program Files\Java\jre1.8.0_31\bin
JAVA_OPTS  -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8

ES_PARAMS -Delasticsearch -Des-foreground=yes -Des.path.home="C:\Program Files\elasticsearch-1.4.4"
ES_CLASSPATH ;C:\Program Files\elasticsearch-1.4.4\lib\elasticsearch-1.4.4.jar;C:\Program Files\elasticsearch-1.4.4\lib*;C:\Program Files\elasticsearch-1.4.4\lib\sigar*

The system cannot find the path specified.   

C:\Program Files\elasticsearch-1.4.4&gt;java -version
java version "1.8.0_31"
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) Client VM (build 25.31-b07, mixed mode)
</description><key id="59354991">9930</key><summary>SQL DBA excited, almost a convert to NOSQL and it wont start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JRRan64</reporter><labels /><created>2015-02-28T19:41:43Z</created><updated>2015-06-10T11:40:48Z</updated><resolved>2015-06-10T11:40:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bgiromini" created="2015-06-10T10:56:14Z" id="110699510">Did you download the program from the github repo or did you get it from the official site? I only ask because I ran into a similar issue when trying to download from the github releases. It seems there are files missing that are only available in the official package. 

https://www.elastic.co/downloads/elasticsearch
</comment><comment author="markwalkom" created="2015-06-10T10:57:57Z" id="110699868">Please also join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="JRRan64" created="2015-06-10T11:37:22Z" id="110711977">It was a java home and path issue.  Working great now.
On Jun 10, 2015 6:57 AM, "Brandon G" notifications@github.com wrote:

&gt; Did you download the program from the github repo or did you get it from
&gt; the official site? I only ask because I ran into a similar issue when
&gt; trying to download from the github releases. It seems there are files
&gt; missing that are only available in the official package.
&gt; 
&gt; https://www.elastic.co/downloads/elasticsearch
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/9930#issuecomment-110699510
&gt; .
</comment><comment author="clintongormley" created="2015-06-10T11:40:47Z" id="110712888">Awesome, thanks for letting us know
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>lucene410 not in class path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9929</link><project id="" key="" /><description>Running elasticsearch 1.3.4 (required), java 1.7.0_75 (under jenv)

error

``` sh
Caused by: java.lang.IllegalArgumentException: A SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene410' does not exist. You need to add the corresponding JAR file supporting this SPI to your classpath.The current classpath supports the following names: [SimpleText, Appending, Lucene40, Lucene3x, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49]
    at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
    at org.apache.lucene.codecs.Codec.forName(Codec.java:95)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:359)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:912)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:758)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)
```

I believe this has something to do with the java setup on my machine, but nonetheless, I'm unable to start the cluster. Is this a bug or do I have some cleanup to do on my machine?
</description><key id="59344649">9929</key><summary>lucene410 not in class path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">podopie</reporter><labels /><created>2015-02-28T15:05:55Z</created><updated>2015-03-03T15:51:50Z</updated><resolved>2015-03-03T15:51:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-28T15:37:37Z" id="76530752">Elasticsearch 1.3.4 uses         

```
&lt;lucene.version&gt;4.9.1&lt;/lucene.version&gt;
```

You are probably mixing things
</comment><comment author="podopie" created="2015-03-03T15:47:17Z" id="76970648"> All I've done is use brew to install the specific elasticsearch version; I imagine when uninstalling a previous version of elasticsearch (latest), the brew uninstall likely did not clean up completely?
</comment><comment author="podopie" created="2015-03-03T15:51:50Z" id="76971624">personal note, will close:
brew uninstall doesn't delete data/logs/plugins folders, so I deleted those specifically to clean up the install, and everything seems to now be working. Appreciate the callout, @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix bulk item response types for update and delete in case of index errors.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9928</link><project id="" key="" /><description>This fixes issue #9821.

Writing a test for this turned out to be too tricky for me at this stage of familiarity with the code base. I get some test failures running `mvn test -Dtests.filter="not @slow"`, but these seem to be the same I get on master and not clearly related to the bulk API.
</description><key id="59332513">9928</key><summary>Fix bulk item response types for update and delete in case of index errors.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">robx</reporter><labels><label>:Bulk</label><label>bug</label></labels><created>2015-02-28T08:26:00Z</created><updated>2015-06-25T09:44:14Z</updated><resolved>2015-06-25T09:44:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T15:13:05Z" id="76529781">@robx thanks for the PR - we'll have a look and get back to you.
</comment><comment author="cbuescher" created="2015-06-24T12:29:55Z" id="114850101">Hi @robx, thanks for submitting this PR and sorry it took so long getting back to you on this. The changes in your commit make sense, however while trying to find out how we can include this in tests I tried the steps you mentioned in #9821 to reproduce this with a build containing your changes and I still get the "index" key in the response. Can you verify that this fix works for you on the command line the way you described in the original issue.
Also I'd like to ask if you can rebase the PR on master? We recently switched to using maven multi-module project there, so all classes moved to a sub-directory called `core`. I will look into how we can add tests before we get this in.
</comment><comment author="robx" created="2015-06-24T18:37:56Z" id="114975386">No, sorry. I'm not that sure it ever worked, the problem doesn't bother me anymore, and I don't have an ES dev setup laying around anymore. Feel free to close.
</comment><comment author="cbuescher" created="2015-06-25T09:43:49Z" id="115189086">Thanks, will close this then, but leave the issue open since it is not fixed yet. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw an exception if field mapping is invalid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9927</link><project id="" key="" /><description>Currently we silently ignore invalid field mappings, eg:

```
PUT test1
{
  "mappings": {
    "searchText": {
      "properties": {
        "foo": {
          "type": "string",
          "anlayzer": "snowball",
          "foo": "bar"
        }
      }
    }
  }
}
```

We should throw an exception instead.

Related to #8870
</description><key id="59311515">9927</key><summary>Throw an exception if field mapping is invalid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2015-02-27T23:51:00Z</created><updated>2015-06-07T10:52:58Z</updated><resolved>2015-03-02T11:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-03-02T11:08:37Z" id="76694133">@clintongormley this should have been solved in https://github.com/elasticsearch/elasticsearch/pull/7534 (pushed only to master). If we have missed a case this issue should be re-labelled as a bug
</comment><comment author="clintongormley" created="2015-03-02T11:51:58Z" id="76699329">@colings86 it is fixed in master - thanks, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUG] SearchRequestBuilder.toString() makes JSON query string set by .setSource() method not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9926</link><project id="" key="" /><description>Related to: https://github.com/elasticsearch/elasticsearch/issues/9910

If you use JSON string as the query and set it by calling `setSource()` method, before executing the query if you call `requestBuilder.toString()` and then execute the query, you will get incorrect response, because `toString()` modified the internal state of `SearchRequestBuilder` object.

Try this code:

```
@GrabResolver(name='maven', root='http://repo1.maven.org/maven2/')
@Grab(group='org.elasticsearch', module='elasticsearch', version='1.4.4')
import org.elasticsearch.common.settings.Settings
import org.elasticsearch.common.settings.ImmutableSettings
import org.elasticsearch.common.transport.InetSocketTransportAddress
import org.elasticsearch.client.Client
import org.elasticsearch.client.transport.TransportClient
import org.elasticsearch.action.index.IndexRequestBuilder
import org.elasticsearch.action.index.IndexResponse
import org.elasticsearch.action.search.SearchRequestBuilder
import org.elasticsearch.action.search.SearchResponse

Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "elasticsearch").build();
Client client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

[1,2,3,4].each() {
    String body = """{"name":"Hello world ${it}"}"""
    IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(client, "twitter")
            .setType("data")
            .setId(it.toString())
            .setSource(body)
            .setRefresh(true)
    IndexResponse indexResponse = indexRequestBuilder.execute().actionGet()
}

String jsonQuery = """{"query":{"filtered":{"filter":{"term":{"_id":"1"}}}}}"""
SearchRequestBuilder requestBuilder = client.prepareSearch("twitter")
            .setTypes("data")
            .setSource(jsonQuery)

// This is the bug
println requestBuilder.toString()

SearchResponse response = requestBuilder.execute().actionGet()
println response.toString()
```

The expected response is one document with `id=1`, however, the actual response is all documents inside the index.

But if you remove the bug line, the response will be correct.
</description><key id="59296857">9926</key><summary>[BUG] SearchRequestBuilder.toString() makes JSON query string set by .setSource() method not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cooniur</reporter><labels /><created>2015-02-27T21:29:00Z</created><updated>2015-02-27T21:53:04Z</updated><resolved>2015-02-27T21:47:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-02-27T21:36:15Z" id="76476247">I think this is a duplicate of #5576. Can you confirm @cooniur please?
</comment><comment author="cooniur" created="2015-02-27T21:44:52Z" id="76477603">@javanna 
Yes, it is. By the way, I can't believe that this bug has been out there for almost one year. This bug is so hard to debug and caused a lot of confusion in our project.

Since normally people use `toString()` for debug purpose, the bug could be fixed by the following code at least:

```
@Override
public String toString() {
    if (sourceBuilder != null) {
        return sourceBuilder.toString();
    }

    if (request.source() != null) {
        return Arrays.toString(request.source().toBytes());
    }

    return "{ }";
}
```
</comment><comment author="javanna" created="2015-02-27T21:47:02Z" id="76477919">I agree with you @cooniur . We've been working on it, there is a PR opened for it (#7334) but for some reason it is not merged yet. We just have to get it in, I am sorry it took so long.

Closing this issue as a duplicate, thanks for reporting though!
</comment><comment author="cooniur" created="2015-02-27T21:48:32Z" id="76478124">@javanna 

Thanks! Just to point out, it would be a so dangerous bug that if it had happened in a production environment (it actually breaks the functionality of the code).

By the way, `setExtraSource()` is a workaround for this bug. But hopefully the PR can be merged soon.

Regards
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't recover from buggy version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9925</link><project id="" key="" /><description>This commit forces a full recovery if the source node is &lt; 1.4.0 and
prevents any recoveries from pre 1.3.2 nodes to
work around #7210

Closes #9922

note: this is just a start, I need to fix some BWC test first before this can be pulled in but I wanted to get the discussion going
</description><key id="59287497">9925</key><summary>Don't recover from buggy version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>release highlight</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T20:10:38Z</created><updated>2015-06-08T00:22:09Z</updated><resolved>2015-03-02T15:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-02-27T20:12:23Z" id="76463539">+1 this looks great.
</comment><comment author="rjernst" created="2015-02-27T21:19:04Z" id="76473539">+1
</comment><comment author="mikemccand" created="2015-02-27T21:36:36Z" id="76476302">+1, just left a couple minor comments.
</comment><comment author="s1monw" created="2015-03-02T10:20:45Z" id="76687791">@mikemccand @rmuir @rjernst pushed a new commit with unittests...
</comment><comment author="bleskes" created="2015-03-02T11:21:38Z" id="76695762">+1 on the change. I think the 1.3.2 part is better implemented as an allocation decider to prevent the master repeatedly trying to allocate it and failing. I checked and it's fairly easy to integrate this into `NodeVersionAllocationDecider` by adding `RecoverySettings` to the constructor

```
private Decision isVersionCompatible(final RoutingNodes routingNodes, final String sourceNodeId, final RoutingNode target, RoutingAllocation allocation) {
        final RoutingNode source = routingNodes.node(sourceNodeId);
        if (recoverySettings.compress() &amp;&amp; source.node().getVersion().before(Version.V_1_3_2)) {
            return allocation.decision(Decision.NO, NAME, "source node version [%s] has a known compression bug preventing allocation",
                    source.node().version());
        } else if (target.node().version().onOrAfter(source.node().version())) {
            /* we can allocate if we can recover from a node that is younger or on the same version
             * if the primary is already running on a newer version that won't work due to possible
             * differences in the lucene index format etc.*/
            return allocation.decision(Decision.YES, NAME, "target node version [%s] is same or newer than source node version [%s]",
                    target.node().version(), source.node().version());
        }  else {
            return allocation.decision(Decision.NO, NAME, "target node version [%s] is older than source node version [%s]",
                    target.node().version(), source.node().version());
        }
    }
```
</comment><comment author="s1monw" created="2015-03-02T14:50:18Z" id="76724236">@bleskes here is a new commit
</comment><comment author="bleskes" created="2015-03-02T14:56:08Z" id="76725354">LGTM. Thx @s1monw 
</comment><comment author="clintongormley" created="2015-03-03T10:04:22Z" id="76918020">Closes #9922
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: snapshot during rolling restart of a 2 node cluster might get stuck</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9924</link><project id="" key="" /><description>The issue was originally reported in https://github.com/elasticsearch/elasticsearch/issues/7980#issuecomment-76151889 If a current master node that contains all primary shards is restarted in the middle of snapshot operation, it might leave the snapshot hanging in `ABORTED` state. 
</description><key id="59283629">9924</key><summary>Snapshot/Restore: snapshot during rolling restart of a 2 node cluster might get stuck</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-02-27T19:39:26Z</created><updated>2015-08-20T22:20:09Z</updated><resolved>2015-08-20T22:20:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cxxr" created="2015-03-10T23:28:59Z" id="78171260">:+1: ran into this a few times.
</comment><comment author="srgclr" created="2015-06-08T08:41:08Z" id="109908823">Ran into a similar issue and tried the snapshot cleanup utility. It didn't work as all shards were ignored:

&gt; Ignoring shard [[dev1_10_event.2015-03-15][4]] with state [ABORTED] on node [kyU3N9lpTIuTbdeUGp5ThQ] - node exists : [true]

What's the reason for ignoring shards when the node exists?
</comment><comment author="imotov" created="2015-06-10T13:41:26Z" id="110757861">@srgclr if a node exists and a shard is in ABORTED state it can mean one of the two things - we hit #11314 or the shard is stuck in the I/O operation and we need to wait until the I/O operation is over or we need to restart the node. It's impossible for the cleanup utility to determine which state we are in. Because of this, it takes a safer route - assume that we are stuck in I/O operation and skip such shards.
</comment><comment author="imotov" created="2015-08-20T22:20:08Z" id="133197071">This should be solved by #11450. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use direct mapping call in Kernel32Library</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9923</link><project id="" key="" /><description>This commit modifies the Kernel32Library to use direct mapping instead of a proxy class when doing native calls on Windows platforms. It also adds the "createSecurityManager" permission to the tests.policy file, and adds unit tests that should have failed when the Java security manager is enabled.

Closes #9802
</description><key id="59247921">9923</key><summary>Use direct mapping call in Kernel32Library</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T15:09:12Z</created><updated>2015-06-08T13:50:53Z</updated><resolved>2015-03-02T09:09:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-02-27T15:10:54Z" id="76408926">@rmuir @s1monw Can you have a look please?
</comment><comment author="rmuir" created="2015-02-27T15:17:43Z" id="76410062">This looks much simpler to me!
</comment><comment author="rmuir" created="2015-02-27T15:21:52Z" id="76410783">@tlrx i added a comment, but this looks good to me.
</comment><comment author="rmuir" created="2015-02-28T05:20:10Z" id="76511490">+1, looks great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disallow recovery from ancient versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9922</link><project id="" key="" /><description>Upgrading from version &lt; 1.4 causes too much corruption. Users should not use rolling upgrade, we should just not allow this to happen.
</description><key id="59241945">9922</key><summary>disallow recovery from ancient versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label><label>bug</label><label>PITA</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T14:23:16Z</created><updated>2015-03-17T22:38:10Z</updated><resolved>2015-03-17T22:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-17T22:38:10Z" id="82629270">forgot to close this... this was fixed by #9925 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Restart API and remove Node#stop()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9921</link><project id="" key="" /><description>The restart API seems pretty esoteric and is entirely untested.
This commit removes it without a replacement.

Closes #9841
</description><key id="59232592">9921</key><summary>Remove Restart API and remove Node#stop()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T12:54:14Z</created><updated>2015-06-06T16:10:39Z</updated><resolved>2015-02-27T13:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-27T13:01:11Z" id="76390915">btw. this is entirely undocumented and disabled by default. I don't think we need to maintain it at all. It's also entirely untested...
</comment><comment author="clintongormley" created="2015-02-27T13:01:41Z" id="76390972">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations and fielddata caching behavior after filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9920</link><project id="" key="" /><description>I'm working with terms aggregations. I've 100M+ documents in one index with 100+ types. I'm trying to run the aggs query on one single type with only hundreds of docs.
Everytime I run this query:
1- It takes a lot of time 3-4 minutes 
2- It blocks the cache, preventing other queries to run on the node till the cache is manually cleared.

I tried using all types of filters, filtered, filter agg, querystring, match query, terms aggs with tiny size and shard_size, even the post filter. But still the field data cache ignores whats mentioned in any of these and caches that field for all documents in that index.

According to ES documentation, this functionality was intentionally made as a sort of search prediction for later queries.
["The logic is: if you need access to documents X, Y, and Z for this query, you will probably need access to other documents in the next query. It is cheaper to load all values once, and to keep them in memory, than to have to scan the inverted index on every request."](http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html)

I found a lot of alternatives but still I am not sure if any of them will solve my problem:
1- Creating an index for every type (Still the filtering won't work but at least it will only cache 1 type not all other types)
2- Using doc values instead caching in memory (Much slower, Cache will never get blocked, doesn't support working with text field yet, so it's useless in my case)
3- Expanding the memory, sharding, creating other nodes and giving a lot of space for the cache ( It will still be slow as I run more than 10 different aggs query, every query has a different field than the previous one. So the cache will always miss and every time I run a query it will take a lot of time to fill the cache that might get dropped on the next query to fill it with a new field)

Is there any workaround to disable that feature or at least an older version of ES that doesn't support  it? Any help on this issue would be really appreciated.
</description><key id="59232232">9920</key><summary>Aggregations and fielddata caching behavior after filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amreha2002</reporter><labels /><created>2015-02-27T12:50:22Z</created><updated>2015-02-28T03:56:28Z</updated><resolved>2015-02-28T03:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:56:28Z" id="76508931">Hi @amreha2002 

This is the way that fielddata has worked from the beginning. Nothing has changed.  If you want to run a terms agg on an analyzed string field, then you need to load into memory the values of that field for all the documents in your index.  No exceptions.

&gt; 2- Using doc values instead caching in memory (Much slower, Cache will never get blocked, doesn't support working with text field yet, so it's useless in my case)

Note: doc values DO work with string fields, as long as they are `not_analyzed`.  Do you really need to run all of these aggs on analyzed string fields?  Also note: they are **slightly** slower than fielddata, not much slower.  You should really consider using doc values.

Another option for analyzed text fields is to use fielddata filtering, to only load the most interesting terms into memory (eg not very common terms and not very uncommon terms)

But if you say you only want to run this on a few hundred docs, then putting them into their own index makes that easy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove component settings from AbstractComponent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9919</link><project id="" key="" /><description>Today we have two ways of getting a setting, either with the full settings key or with only
the last part of the key where the prefix is implicit depending on the package the class is in via
component settings. this is trappy as well as confusing for users and can break easily if a class is moved
to a new package since the prefix then implicitly changes.
This commit removes the component settings from the codebase.
</description><key id="59227675">9919</key><summary>Remove component settings from AbstractComponent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T12:02:23Z</created><updated>2015-06-06T16:10:52Z</updated><resolved>2015-02-27T12:58:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-27T12:11:00Z" id="76385558">Note that this will have of course consequences in plugins.

Now that we use full path for setting names, could we define static final String for those?
So we can reuse them in tests and plugins?
</comment><comment author="s1monw" created="2015-02-27T12:23:45Z" id="76386907">@dadoonet there are no consequences in plugins you always had to specify the full key in the settings.
</comment><comment author="mikemccand" created="2015-02-27T12:29:00Z" id="76387426">+1 to have just one way to get settings.
</comment><comment author="martijnvg" created="2015-02-27T12:43:57Z" id="76388994">+1 the bwc issues that can happen from moving a class makes component settings trappy.
</comment><comment author="dadoonet" created="2015-02-27T12:48:21Z" id="76389483">Forgot to say that I'm +1 as well.
About consequences I meant that we need only to fix the code and use fullpath there as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch can't get data from mongodb using in docker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9918</link><project id="" key="" /><description>I had install mongodb river and head plugin in the docker.
When i execute the following code, it can't synchronize data from mongodb.
But it can work well not use docker.
Anybody get trouble ?

&lt;code&gt;
curl -XPUT 'http://127.0.0.1:9200/_river/mongodb_source/_meta' -d '{ 
    "type": "mongodb", 
    "mongodb": {
      "db": "andes", 
      "collection": "source",
      "host": "192.168.90.46",
      "port": "27017"  
    }, 
    "index": {
      "name": "mongoindex_source", 
      "type": "source" 
    }
  }'
&lt;/code&gt;
</description><key id="59219984">9918</key><summary>elasticsearch can't get data from mongodb using in docker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hordemark</reporter><labels /><created>2015-02-27T10:45:21Z</created><updated>2015-02-28T03:50:12Z</updated><resolved>2015-02-28T03:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:50:12Z" id="76508730">Hi @hordemark 

Please ask questions like these on the mailing list, or on the site which supports the mongo river.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Documentation] Threading Model for SearchRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9917</link><project id="" key="" /><description>As per Documentation in the site [http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/search.html#_operation_threading_3], We shall set the Threading Model for SearchRequest. 

But the latest SearchRequest.java (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/search/SearchRequest.java) does not have any method to set this value.

However, This seems to be available in the older version of the SearchRequest.java (https://github.com/medcl/elasticsearch/blob/185f5a9e1811893f19ac4679dfe7900ef84705f9/modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchRequest.java)

Is this Option removed recently ? Is there any update needed in the documentation ?
</description><key id="59208676">9917</key><summary>[Documentation] Threading Model for SearchRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sathishk</reporter><labels /><created>2015-02-27T09:10:51Z</created><updated>2015-02-28T03:49:31Z</updated><resolved>2015-02-28T03:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:49:31Z" id="76508716">Hi @sathishk 

yes it was removed in 1.2.0.  I've remove it from the documentation. thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add back tests.cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9916</link><project id="" key="" /><description>This setting is used by the release script to run rest tests against
the version being released.  It used to work only for tests using
the global cluster.  Now it supercedes both SUITE and TEST scope
test clusters.
</description><key id="59199057">9916</key><summary>Tests: Add back tests.cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T07:10:24Z</created><updated>2015-03-24T03:09:16Z</updated><resolved>2015-02-27T19:08:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-02-27T08:10:32Z" id="76353612">Thanks a lot for looking into this @rjernst ! 

Just one comment: I wonder if it makes sense to have an external cluster with scope TEST. given that the system property value will be the same and we don't recreate anything per test... it looks like it is going to be the same as SUITE, no? Maybe we should only do this for SUITE scope? maybe even barf if we try to run something with scope TEST against the external cluster? Thoughts?
</comment><comment author="rjernst" created="2015-02-27T16:11:45Z" id="76419936">@javanna I pushed a new commit that addresses your comments.  The log message was unintentional.
</comment><comment author="javanna" created="2015-02-27T16:24:01Z" id="76422216">LGTM thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene r1662607</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9915</link><project id="" key="" /><description>this is not ready: there are test failures!

one category looks like this, even though we havent integrated mockfilesystems, it looks like some tests are using lucenetestcase so they get them anyway. when you see failures with filenames like 'extraNNN' then you know its the new ExtrasFS adding extra files to the directory. if we really need, SuppressFileSystems annotation can be used for these tests, but it would be good to look further:
example:
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=StoreTest -Dtests.method=testMixedChecksums -Dtests.seed=3E98A98307A8A08F -Dtests.locale=en_GB -Dtests.timezone=America/Kralendijk -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.02s J1 | StoreTest.testMixedChecksums &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: extra1 is not in the map: 4 vs. 5

another category of fails is wrong scores for parent/child. The api changes for this upgrade were trivial so I think something might be wrong here. @jpountz any ideas?
example:
  2&gt; NOTE: reproduce with: ant test  -Dtestcase=ChildrenQueryTests -Dtests.method=testRandom -Dtests.seed=3E98A98307A8A08F -Dtests.locale=es_DO -Dtests.timezone=EET -Dtests.asserts=true -Dtests.file.encoding=UTF-8
FAILURE 0.02s J0 | ChildrenQueryTests.testRandom &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: actual.getMaxScore() != expected.getMaxScore()
&gt; Expected: &lt;1.0F&gt;
&gt;      got: &lt;5.155753F&gt;

finally percolator looks like it has issues, stuff like this:
FAILURE 0.49s J2 | PercolatorTests.testPercolateSortingWithNoSize &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: 
&gt; Expected: &lt;2.0F&gt;
&gt;      got: &lt;0.0F&gt;
</description><key id="59177527">9915</key><summary>Upgrade to lucene r1662607</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T01:54:06Z</created><updated>2015-08-25T13:25:02Z</updated><resolved>2015-02-27T17:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-27T08:32:28Z" id="76355761">&gt; another category of fails is wrong scores for parent/child. The api changes for this upgrade were trivial so I think something might be wrong here. @jpountz any ideas?

This was an existing bug uncovered by https://issues.apache.org/jira/browse/LUCENE-6262. ParentScoreCollector did not tell the searcher that it needs scores.
</comment><comment author="jpountz" created="2015-02-27T09:12:57Z" id="76360227">The percolator failures were also due to needsScores. Things should be good now.
</comment><comment author="jpountz" created="2015-02-27T10:55:43Z" id="76376224">LGTM
</comment><comment author="rmuir" created="2015-02-27T12:42:18Z" id="76388806">thank you adrien! I can look into the mock filesystem issues.
</comment><comment author="rmuir" created="2015-02-27T16:33:20Z" id="76424069">I ran tests again after my commit, now there is only one failure:

  1&gt; REPRODUCE WITH  : mvn clean test -Dtests.seed=28DC6D0EB0CE35C0 -Dtests.class=org.elasticsearch.search.innerhits.InnerHitsTests -Dtests.method="testParentChildMultipleLayers" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.locale=mt -Dtests.timezone=Africa/Libreville -Dtests.processors=8
  1&gt; Throwable:
  1&gt; java.lang.AssertionError: Unexpected ShardFailures: [shard [[5mJs1MmtSkaRjQwrUhJY5g][articles][1]], reason [RemoteTransportException[[node_s4][local[577]][indices:data/read/search[phase/dfs]]]; nested: DfsPhaseExecutionException[[articles][1]: query[child_filter[comment/article](filtered%28child_filter[remark/comment]%28filtered%28message:bad%29-&gt;_type:remark%29%29-&gt;_type:comment)],from[0],size[10]: Dfs Failed [Exception du
ring dfs phase]]; nested: NullPointerException; ]]
  1&gt; Expected: &lt;0&gt;
  1&gt;      but: was &lt;1&gt;
</comment><comment author="rmuir" created="2015-02-27T17:11:13Z" id="76431044">Now tests are passing.
</comment><comment author="mikemccand" created="2015-02-27T17:27:09Z" id="76434694">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_ttl` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9914</link><project id="" key="" /><description>While the parser allowed changing field type settings, these would never
have been serialized.  So this change simply removes parsing using
parseField. Backcompat will still work if a user uploads old settings
(they just would never have worked anyways, so we continue ignoring
them with 1.x, and 2.x will now error).

see #8143
</description><key id="59169426">9914</key><summary>Lock down `_ttl` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T00:34:55Z</created><updated>2015-06-06T15:49:31Z</updated><resolved>2015-02-27T19:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-27T10:55:17Z" id="76376187">Wow, this kind of problems is a source of terrible bugs since things would work fine on a single node but fail badly as soon as you have several of them since serialization looses information... Thanks for fixing!

LGTM
</comment><comment author="s1monw" created="2015-02-27T11:12:07Z" id="76378581">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_size` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9913</link><project id="" key="" /><description>This also changes the stored setting for _size to true (for
indexes created in 2.x).

see #8143
</description><key id="59166562">9913</key><summary>Lock down `_size` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-27T00:05:41Z</created><updated>2015-06-06T15:49:40Z</updated><resolved>2015-02-27T19:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-27T10:53:43Z" id="76376012">LGTM
</comment><comment author="s1monw" created="2015-02-27T11:12:17Z" id="76378638">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_field_names` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9912</link><project id="" key="" /><description>Now that we have an explicit `enabled` flag, we can lock down
the field type so it is not mungeable.

see #8143
</description><key id="59159626">9912</key><summary>Lock down `_field_names` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T23:04:27Z</created><updated>2015-06-08T09:03:54Z</updated><resolved>2015-02-26T23:17:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T23:08:50Z" id="76295376">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Max # of documents per shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9911</link><project id="" key="" /><description>Would be nice to document this:

Lucene has a limit on the max # of documents per Lucene index (which is Integer.MAX_VALUE-1 : https://issues.apache.org/jira/browse/LUCENE-5843).  Which means that a shard in Elasticsearch also cannot exceed the same limit when it comes to the max # of documents per shard.    Certainly, we don't advise having a gigantic shard of this size to begin with (can affect recovery speeds and performance, etc..), but it will be nice to at least document the limitation.
</description><key id="59158299">9911</key><summary>Max # of documents per shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-02-26T22:53:27Z</created><updated>2015-03-01T20:09:18Z</updated><resolved>2015-03-01T20:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:42:38Z" id="76508502">@ppf2 how about a PR?
</comment><comment author="ppf2" created="2015-03-01T04:11:11Z" id="76572723">Ok done @clintongormley :) https://github.com/elasticsearch/elasticsearch/pull/9932
</comment><comment author="clintongormley" created="2015-03-01T20:09:18Z" id="76627611">Closed by #9932
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to use JSON query string with SearchRequestBuilder (Elasticsearch 1.4.2 &amp; 1.4.4)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9910</link><project id="" key="" /><description>Previously, I use the following code to query Elasticsearch using JSON string (code is in Groovy):

```
    Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "elasticsearch").build();
    Client client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress("localhost", "9300"));

    String jsonQuery = """{"query":{"filtered":{"filter":{"term":{"_id":"100"}}}}}"""
    SearchRequestBuilder request = client.prepareSearch("twitter")
            .setTypes("data")
            .setSource(jsonQuery)

    SearchResponse response = request.execute().actionGet()
```

As you can read from the above, I want to filter the documents to find one with `_id=100`. Now it returns all documents, totally ignoring my query.

If I use `QueryBuilders`, I get ClassNoFoundException, which is described here: https://github.com/elasticsearch/elasticsearch/issues/9891

If I use `setExtraSource()`, it works. But `setSource()` was working before (from November 2014 till yesterday). And according to the documentation, it is the method that can be used to set JSON query string.

Here is my dependencies:

```
&lt;properties&gt;
    &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;plugin.groovy-eclipse-compiler&gt;2.9.0-01&lt;/plugin.groovy-eclipse-compiler&gt;
    &lt;plugin.groovy-eclipse-batch&gt;2.3.4-01&lt;/plugin.groovy-eclipse-batch&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;version&gt;1.4.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
        &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;
        &lt;version&gt;2.4.1&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
        &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
        &lt;version&gt;1.7.10&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
        &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
        &lt;version&gt;1.1.2&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.12&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.hamcrest&lt;/groupId&gt;
        &lt;artifactId&gt;hamcrest-all&lt;/artifactId&gt;
        &lt;version&gt;1.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
```

It can be reproduced on my colleague's environment, too. We both use Oracle JDK 8 (1.8.0_31) on Mac. IDEs are Eclipse (his) and IntelliJ (mine).

Tested on Elasticsearch 1.4.2 &amp; 1.4.4, both failed.
</description><key id="59153240">9910</key><summary>Unable to use JSON query string with SearchRequestBuilder (Elasticsearch 1.4.2 &amp; 1.4.4)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cooniur</reporter><labels /><created>2015-02-26T22:15:39Z</created><updated>2015-02-27T21:45:41Z</updated><resolved>2015-02-27T21:45:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cooniur" created="2015-02-26T22:25:26Z" id="76288063">Hi, @dadoonet 
This is another issue happened yesterday at the same time as the other one.
</comment><comment author="jprante" created="2015-02-27T09:31:41Z" id="76362553">I can not reproduce. This script works for me:

```
@GrabResolver(name='maven', root='http://repo1.maven.org/maven2/')
@Grab(group='org.elasticsearch', module='elasticsearch', version='1.4.4')
import org.elasticsearch.common.settings.Settings
import org.elasticsearch.common.settings.ImmutableSettings
import org.elasticsearch.common.transport.InetSocketTransportAddress
import org.elasticsearch.client.Client
import org.elasticsearch.client.transport.TransportClient
import org.elasticsearch.action.index.IndexRequestBuilder
import org.elasticsearch.action.index.IndexResponse
import org.elasticsearch.action.search.SearchRequestBuilder
import org.elasticsearch.action.search.SearchResponse

Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "elasticsearch").build();
Client client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

String body = """{"name":"Hello world"}"""
IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(client, "twitter")
            .setType("data")
            .setId("1")
            .setSource(body)
            .setRefresh(true)
IndexResponse indexResponse = indexRequestBuilder.execute().actionGet()

String jsonQuery = """{"query":{"filtered":{"filter":{"term":{"_id":"1"}}}}}"""
SearchRequestBuilder requestBuilder = client.prepareSearch("twitter")
            .setTypes("data")
            .setSource(jsonQuery)

SearchResponse response = requestBuilder.execute().actionGet() 

println response.toString()
```

JVM / Groovy version

```
[joerg@localhost ~]$ groovy --version
Groovy Version: 2.4.1 JVM: 1.8.0_31 Vendor: Oracle Corporation OS: Linux
```
</comment><comment author="cooniur" created="2015-02-27T09:54:57Z" id="76366462">@jprante 
Thank you for the reproduce try!

Actually, could you index a few another documents with different ids (say `id=2, 3, 4, ...`), and do the filtered query (`_id=1`) again to see whether the response is the only one document with `id=1` or all the other documents?

In my case, I got all the documents back instead of the one that I filtered.
</comment><comment author="jprante" created="2015-02-27T20:36:55Z" id="76467214">@coonuir it makes no difference, works as expected here.

```
@GrabResolver(name='maven', root='http://repo1.maven.org/maven2/')
@Grab(group='org.elasticsearch', module='elasticsearch', version='1.4.4')
import org.elasticsearch.common.settings.Settings
import org.elasticsearch.common.settings.ImmutableSettings
import org.elasticsearch.common.transport.InetSocketTransportAddress
import org.elasticsearch.client.Client
import org.elasticsearch.client.transport.TransportClient
import org.elasticsearch.action.index.IndexRequestBuilder
import org.elasticsearch.action.index.IndexResponse
import org.elasticsearch.action.search.SearchRequestBuilder
import org.elasticsearch.action.search.SearchResponse

Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "elasticsearch").build();
Client client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

[1,2,3,4].each() {
    String body = """{"name":"Hello world ${it}"}"""
    IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(client, "twitter")
            .setType("data")
            .setId(it.toString())
            .setSource(body)
            .setRefresh(true)
    IndexResponse indexResponse = indexRequestBuilder.execute().actionGet()
}

String jsonQuery = """{"query":{"filtered":{"filter":{"term":{"_id":"1"}}}}}"""
SearchRequestBuilder requestBuilder = client.prepareSearch("twitter")
            .setTypes("data")
            .setSource(jsonQuery)

SearchResponse response = requestBuilder.execute().actionGet()

println response.toString()
```
</comment><comment author="cooniur" created="2015-02-27T21:00:19Z" id="76470779">@jprante 
I found one tiny difference between your code and mine:

if you add `println requestBuilder` before you execute it, the response would be incorrect.

Try the following and let me know whether you can get the document you filtered (i.e. `id=1`):

```
SearchRequestBuilder requestBuilder = client.prepareSearch("twitter")
          .setTypes("data")
          .setSource(jsonQuery)

// == Add this line ====
println requestBuilder.toString()

SearchResponse response = requestBuilder.execute().actionGet()
println response.toString()
```

# 

More details:
The `toString()` method in `SearchRequestBuilder` is like this:

```
@Override
public String toString() {
    return internalBuilder().toString();
}
```

`internalBuilder()` is:

```
/**
 * Returns the internal search source builder used to construct the request.
 */
public SearchSourceBuilder internalBuilder() {
    return sourceBuilder();
}
```

`sourceBuilder()` is:

```
private SearchSourceBuilder sourceBuilder() {
    if (sourceBuilder == null) {
        sourceBuilder = new SearchSourceBuilder();
    }
    return sourceBuilder;
}
```

So, the most likely reason is that the JSON string set by `setSource()` gets overridden by the `sourceBuilder`, because calling `setSource()` doesn't initialize the internal `sourceBuilder` object.

I think this is a bug, which is hard to debug and should be fixed. Imagine you use `logger.debug("Request is {}", request)` and it actually erases your query! One would never expect that calling `toString()` method would get the JSON query string erased.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Fix NPE in ElasticsearchIntegrationTest if no indexer is provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9909</link><project id="" key="" /><description>Closes #9907
</description><key id="59136051">9909</key><summary>[TEST] Fix NPE in ElasticsearchIntegrationTest if no indexer is provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T20:14:24Z</created><updated>2015-03-19T15:33:59Z</updated><resolved>2015-02-26T20:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-26T20:16:01Z" id="76261132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Zen Discovery references deprecated ping_timeout setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9908</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election

&gt; As part of the ping process a master of the cluster is either elected or joined to. This is done automatically. The `discovery.zen.ping_timeout` (which defaults to `3s`)

This is _still_ true, but the setting should be `discovery.zen.ping.timeout`.
</description><key id="59134785">9908</key><summary>[Docs] Zen Discovery references deprecated ping_timeout setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-02-26T20:05:53Z</created><updated>2015-09-23T17:19:43Z</updated><resolved>2015-09-23T17:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vedil" created="2015-06-08T16:00:45Z" id="110053183">i see in code "ZenDiscovery.java" that both properties are used and "discovery.zen.ping.timeout" over-rides " discovery.zen.ping_timeout"
below is the code snippet:
TimeValue pingTimeout = this.settings.getAsTime("discovery.zen.initial_ping_timeout", timeValueSeconds(3));
        pingTimeout = this.settings.getAsTime("discovery.zen.ping_timeout", pingTimeout);
        pingTimeout = settings.getAsTime("discovery.zen.ping_timeout", pingTimeout);
        this.pingTimeout = settings.getAsTime(SETTING_PING_TIMEOUT, pingTimeout);
</comment><comment author="pickypg" created="2015-06-10T15:37:19Z" id="110800666">Hi @vedil, I definitely agree that that's in the code, but the `SETTING_PING_TIMEOUT` is the last one, which means that it gets the final say-so.
</comment><comment author="jasontedor" created="2015-09-23T17:19:42Z" id="142669911">Closed by d8b29f7beb37993c77bfb3298a77513651ef81d4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE on ElasticsearchIntegrationTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9907</link><project id="" key="" /><description>Seems that there is an NPE in the `waitForDocs` method of ElasticsearchIntegrationTest. I discovered when trying to write a test case using this class which calls the method: `waitForDocs(final long numDocs)` which propagates down a null value to this portion of the code.

Problem seems to surface on this line:
https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java#L999

As noted in the documentation indexer is Nullable, but if null here, we see a null pointer. I think in the null case we could default to 0 and the code would work as documented, but wanted to get some feedback before making the change.
</description><key id="59134624">9907</key><summary>NPE on ElasticsearchIntegrationTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">cfstout</reporter><labels><label>bug</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T20:04:56Z</created><updated>2015-02-26T20:20:33Z</updated><resolved>2015-02-26T20:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-26T20:20:33Z" id="76261926">thanks for reporting it!! ;) I pushed a fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scanning and scrolling exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9906</link><project id="" key="" /><description>For certain scenarios, scanning/scrolling queries can trigger low-level exceptions probably in cases the scrolling id expired.

```
org.elasticsearch.ElasticsearchParseException: failed to parse / load source
    at org.elasticsearch.search.lookup.SourceLookup.loadSourceIfNeeded(SourceLookup.java:81)
    at org.elasticsearch.search.lookup.SourceLookup.filter(SourceLookup.java:145)
    at org.elasticsearch.search.fetch.source.FetchSourceSubPhase.hitExecute(FetchSourceSubPhase.java:77)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
    at org.elasticsearch.search.SearchService.executeScan(SearchService.java:247)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: docID must be &gt;= 0 and &lt; maxDoc=15095 (got docID=-1)
    at org.apache.lucene.index.SegmentReader.checkBounds(SegmentReader.java:376)
    at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:334)
    at org.elasticsearch.search.lookup.SourceLookup.loadSourceIfNeeded(SourceLookup.java:70)
    ... 10 more
```

Another one:

```
org.elasticsearch.transport.RemoteTransportException: [test][inet[/10.10.10.10:9300]][indices:data/read/search[phase/scan/scroll]]
Caused by: java.lang.NullPointerException
    at org.elasticsearch.search.fetch.source.FetchSourceSubPhase.hitExecute(FetchSourceSubPhase.java:79)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
    at org.elasticsearch.search.SearchService.executeScan(SearchService.java:247)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:939)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:930)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Another one:

```
[2015-02-24 17:27:18,213][DEBUG][action.search.type ] [Search 12] [959835] Failed to execute query phase 
org.elasticsearch.transport.RemoteTransportException: [Search 03][inet[/10.10.10.10:9300]][indices:data/read/search[phase/scan/scroll]] 
Caused by: java.lang.NullPointerException 
at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:259) 
at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:104) 
at org.elasticsearch.search.fetch.QueryFetchSearchResult.writeTo(QueryFetchSearchResult.java:95) 
at org.elasticsearch.search.fetch.ScrollQueryFetchSearchResult.writeTo(ScrollQueryFetchSearchResult.java:68) 
at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:91) 
at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:68) 
at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:940) 
at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:930) 
at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at java.lang.Thread.run(Thread.java:745)
```
</description><key id="59117111">9906</key><summary>Scanning and scrolling exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>feedback_needed</label></labels><created>2015-02-26T18:28:00Z</created><updated>2016-05-21T05:48:09Z</updated><resolved>2015-09-27T22:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-02-26T20:19:07Z" id="76261668">An expired scroll id should trigger a different error. Is this issue easily reproducible? and on what version did you encounter these errors?
</comment><comment author="astefan" created="2015-03-16T18:40:18Z" id="81860287">Not easily reproduceable, heavy searching with scrolling scenario is needed for this. ES 1.4.1.
</comment><comment author="clintongormley" created="2015-04-04T15:54:06Z" id="89604318">@astefan Can you show us the actual code that you're using here?  If you're able to provide a working recreation, even better.  
</comment><comment author="clintongormley" created="2015-04-26T20:00:51Z" id="96428739">Hi @astefan 

What version of Elasticsearch are you using? Are you using any parent/child or nested queries?  How about a stack trace from the logs where you get the NPE?

We don't have enough information to investigate any further at the moment.
</comment><comment author="ishare" created="2015-05-19T04:30:01Z" id="103332436">got the same err in 1.5.1, is there any idea? I tried delete all data and index, reindex all docs, it occurs again..

[2015-05-19 14:18:55,581][DEBUG][action.search.type       ] [Bart Hamilton] [26] Failed to execute fetch phase
java.lang.IndexOutOfBoundsException: docID must be &gt;= 0 and &lt; maxDoc=1 (got docID=2147483647)
    at org.apache.lucene.index.SegmentReader.checkBounds(SegmentReader.java:376)
    at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:334)
    at org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:407)
    at org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:217)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:182)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:501)
    at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:452)
    at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:449)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</comment><comment author="nik9000" created="2015-05-19T11:25:45Z" id="103449549">Without at least the query you are scrolling this is going to be hard to
debug. The mapping and some example documents would help too.
On May 19, 2015 12:30 AM, "navins" notifications@github.com wrote:

&gt; got the same err in 1.5.1, is there any idea?
&gt; 
&gt; java.lang.IndexOutOfBoundsException: docID must be &gt;= 0 and &lt; maxDoc=1
&gt; (got docID=2147483647)
&gt; at
&gt; org.apache.lucene.index.SegmentReader.checkBounds(SegmentReader.java:376)
&gt; at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:334)
&gt; at
&gt; org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:407)
&gt; at
&gt; org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:217)
&gt; at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:182)
&gt; at
&gt; org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:501)
&gt; at
&gt; org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:452)
&gt; at
&gt; org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:449)
&gt; at
&gt; org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt; at java.lang.Thread.run(Thread.java:745)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/9906#issuecomment-103332436
&gt; .
</comment><comment author="ishare" created="2015-05-21T01:48:45Z" id="104091883">Solved, I found there is exception in native script which cause this problem.
</comment><comment author="haochun" created="2016-05-21T03:36:12Z" id="220756229">@ishare what exception did you found?i occurred the same exception.but,i am use function_score query,not scan.my query dsl is:

`{
  "size": 10,
  "query": {
    "function_score": {
      "functions": [
        {
          "script_score": {
            "lang": "groovy",
            "script": "_score * log(doc['cited'].value)"
          }
        }
      ],
      "query": {
        "filtered": {
          "query": {
            "query_string": {
              "analyzer": "query_ansj",
              "query": "java",
              "fields": [
                "title^10.0",
                "keywords^6.0",
                "summary^1.0",
                "authors^2.0"
              ]
            }
          }
        }
      }
    }
  },
  "from": 90,
  "sort": []
}`

when i search from 0-90,there is no exception,but,when i search from 90-100,the exception occurred
</comment><comment author="haochun" created="2016-05-21T03:38:06Z" id="220756304">the response
`
{
"took": 8,
"timed_out": false,
"_shards": {
"total": 12,
"successful": 11,
"failed": 1,
"failures": [
{
"index": "meta",
"shard": 2,
"status": 500,
"reason": "IndexOutOfBoundsException[docID must be &gt;= 0 and &lt; maxDoc=25371497 (got docID=2147483647)]"
}
]
},
"hits": {
"total": 208,
"max_score": 8.523983,
"hits": [ ]
}
}
`
</comment><comment author="haochun" created="2016-05-21T03:48:30Z" id="220756622">sorry,i found the cause.it it my fault.the doc['cited'] maybe zero.
</comment><comment author="ishare" created="2016-05-21T05:48:09Z" id="220760463">@haochun this error always due to an exception, and the error msg is not quite obviously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search right after bulk update of existing document return wrong (older) timestamp  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9905</link><project id="" key="" /><description>Hi 

I"m using version 1.4.2. 
The scenario is as follows:
1. In the index there are set of documents in a given structure.
2. search the index using searchRequestBuilder for the latest timestamp of that set.
3. performing index update using IndexRequestBuilder with setRefresh=true on the same set with identical documents.
4. immediately after the indexing I search again for the latest timestamp

The excepted result is to get newer timestamp on the second search, but occasionally it return same timestamp. 
Putting a sleep of few second between update and search solve the issue. 
Is there other solution for this issue beside sleep ?
Shouldn't the setRefresh=true flag guarantees that the update will be reflected in immediate query after the update ?

Thanks,
Uri
</description><key id="59115237">9905</key><summary>Search right after bulk update of existing document return wrong (older) timestamp  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uhaham</reporter><labels><label>feedback_needed</label></labels><created>2015-02-26T18:19:48Z</created><updated>2015-04-26T19:36:29Z</updated><resolved>2015-04-26T19:36:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:34:08Z" id="76508225">Hi @uhaham 

Could you provide your test code please?  I'd imagine that the setRefresh flag should cause the shard to refresh, but I don't know the java API.
</comment><comment author="clintongormley" created="2015-04-26T19:36:29Z" id="96423649">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Incorrect bounding box for Multi-geometry </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9904</link><project id="" key="" /><description>Caught numerous times by our RandomGeoCollection test, the ShapeCollection.BoundingBox calculation in Spatial4j is wrong... this is a known issue that has not yet been corrected in Spatial4j https://github.com/spatial4j/spatial4j/issues/77 

For now Elasticsearch ShapeCollection corrects this, but the fix is not used internally by Spatial4j JtsGeometry.  Therefore any Multi-geometry (e.g., multipoint, multilinestring, multipolygon) which creates a spatial4j ShapeCollection under the hood, suffers from this bug.

There is a near term solution that will be applied until we can fix the fundamental problem within Spatial4j.
</description><key id="59111114">9904</key><summary>[GEO] Incorrect bounding box for Multi-geometry </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2015-02-26T17:53:28Z</created><updated>2015-10-28T02:35:44Z</updated><resolved>2015-10-28T02:35:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expand wildcards in snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9903</link><project id="" key="" /><description>This PR makes handling of expand_wildcards parameters consistent. The first commit f5d8ca8 will go to both 1.x and master and the second commit 7abe1ed will go into master only. Closes #6097.
</description><key id="59105141">9903</key><summary>Expand wildcards in snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T17:12:02Z</created><updated>2015-05-29T18:20:29Z</updated><resolved>2015-04-23T01:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-25T11:30:21Z" id="85990626">looks good, left a few comments. I think we need to drop the indices options parsing code from multi_search too and reuse this new common method there too.
</comment><comment author="javanna" created="2015-03-25T11:31:53Z" id="85991152">One more thing, I would split this PR in two, the actual change and the breaking part that removes the old parameters, otherwise this whole change ends up in the 1.x release notes as a breaking one, which it isn't.
</comment><comment author="imotov" created="2015-03-31T23:39:29Z" id="88285456">@javanna I pushed the changes based on your comments. I will open a separate PR for removal of old parameters from 2.0 once this PR is merged.
</comment><comment author="javanna" created="2015-04-02T10:27:41Z" id="88860566">Looks good @imotov , I think we can remove the breaking label? 

It might be good to add some more tests where missing around the apis that this PR touches using the different indices options, just to make sure that we still parse them correctly? I have the feeling that we have no extensive tests around that. What do you think?
</comment><comment author="imotov" created="2015-04-22T22:12:39Z" id="95353845">@javanna I pushed additional tests as we discussed. Could you take a look?
</comment><comment author="javanna" created="2015-04-22T22:44:57Z" id="95358621">thanks for adding those tests @imotov LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify RecoveryState management to IndexShard and clean up semantics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9902</link><project id="" key="" /><description>We keep track of the current stage of recovery using an instance of RecoveryState which is stored on the relevant IndexShard. At the moment changes to this object are made in many places of the code, which are charged of doing it in the right order, keeping track of timers and many more. Also the changes to shard state are decoupled from the recovery stages which caused #9503.

This PR refactors this and brings all of the changes into IndexShard. It also makes all recovery follow the exact same stages and shortcut some. This is in order to keep things simple and always the same (those shortcuts didn't add anything, we ended doing it all anyway). 

Also, all timer management is now folded into RecoveryState and unit tests are added.

This closes #9503 by moving the shard to post recovery only once the recovery is done (before they were decoupled), meaning that master promotion of the target shard to started can not cancel the recovery.

PS. This is a very tricky test to just simulate the issue in #9503 reliably. I plan to remove the test before committing as the scenario was already discovered by the CI.
</description><key id="59101521">9902</key><summary>Unify RecoveryState management to IndexShard and clean up semantics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T16:50:54Z</created><updated>2015-06-07T17:04:05Z</updated><resolved>2015-02-27T20:17:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-27T09:58:25Z" id="76367140">I left some comments. nice cleanup in general and I think we are close
</comment><comment author="bleskes" created="2015-02-27T14:16:37Z" id="76400462">@s1monw thx. I pushed an update.
</comment><comment author="s1monw" created="2015-02-27T16:26:18Z" id="76422664">cool thx LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[PROTOTYPE] Refactor how we parse queries, filter and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9901</link><project id="" key="" /><description>Today we have a massive infrastructure to parse all our requests. We have client side builders and server side parsers but no real representation of the query, filter, aggregation etc until it's executed. What is produced from a XContent binary is a Lucene query directly which causes huge parse methods in separate classes etc. that hare hard to test and don't allow decoupled modifications or actions on the query itself between parsing and executing. 

This PR is a small prototype how things could look in the future that would allow for more flexibility and cleaner code IMO.

This refactoring splits the parsing and the creation of the lucene query, this has a couple of advantages
- XContent parsing creation are in one file and can be tested more easily
- the class allows a typed in-memory representation of the query that can be modified before a lucene query is build
- the query can be normalized and serialized via Streamable to be used as a normalized cache key (not depending on the order of the keys in the XContent)
- the query can be parsed on the coordinating node to allow document prefetching etc. forwarding to the executing nodes would work via Streamable binary representation --&gt; https://github.com/elasticsearch/elasticsearch/issues/8150
- for the query cache a query tree can be "walked" to rewrite range queries into match all queries with MIN/MAX terms to get cache hits for sliding windows --&gt; https://github.com/elasticsearch/elasticsearch/issues/9526
- code wise two classes are merged into one which is nice
- filter and query can maybe share once class and we add a `toFilter(QueryParserContenxt ctx)` method that returns a filter and by default return a `new QueryWrapperFilter(toQuery(context));`
</description><key id="59081520">9901</key><summary>[PROTOTYPE] Refactor how we parse queries, filter and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2015-02-26T14:39:32Z</created><updated>2015-03-31T16:41:40Z</updated><resolved>2015-03-31T16:41:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T14:46:47Z" id="76189850">I really like having all the logic for a given query in a single place! I suspect you will find some inconsistencies around parameters that are supported in parsers but not in builders while doing this refactoring!

&gt; filter and query can maybe share once class and we add a toFilter(QueryParserContenxt ctx) method that returns a filter and by default return a new QueryWrapperFilter(toQuery(context));

Do not spend too much time on filters. They are currently being removed from Lucene, so let's focus on getting queries right?
</comment><comment author="dakrone" created="2015-02-26T16:17:05Z" id="76207375">I like collapsing the two into a single class, though I'm a little worried about what we are exposing for doing the parsing (left a comment about that), but overall much cleaner!
</comment><comment author="s1monw" created="2015-02-27T10:18:36Z" id="76370393">@dakrone from your comment I can tell that the description of this issue is not clear enough what this is going to enable in the future lemme try to clarify:

Today a request is parsed on all the nodes causing lots of trouble. Yet in the future I think it makes sense to decouple that and once a request comes into the cooridinating node or even once it comes into the system alltogether ie via REST we parse the XContent and have the `intermediate representation` which is what `fromXContent()` does. Then if that stage was succcessful we send it further to the nodes executing the request as a binary representation via streamable. (coordinating node calls `#writeTo()`) On the target nodes we then use `#readFrom()` to gain the intermediate represenation back and call `toQuery` in order to get the query. 

Today we don't do this so I just tried to model the current arch with the refactoring prototype hence the method:

```
    @Override
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+        TermQuery query = new TermQuery();
+        query.fromXContent(parseContext);
+        return query.toQuery(parseContext);
+    }
```

makes sense now?
</comment><comment author="rjernst" created="2015-02-27T18:48:03Z" id="76449134">Ok I think I understand, makes sense to me.

+1
</comment><comment author="s1monw" created="2015-03-02T08:54:26Z" id="76676863">I think a common source of confusion is that currently those methods are not on the interface all queries need to implement. In the future they will be so they can't be static.
</comment><comment author="s1monw" created="2015-03-03T16:10:21Z" id="76975554">I think we have some agreement that this refactoring can be beneficial. I'd like use to start working on it very soon maybe we can create a branch for it soon. @cbuescher do you think we can start this soon?
</comment><comment author="cbuescher" created="2015-03-03T17:31:23Z" id="76992845">@s1monw sure, will have to look at how long it takes me to do the same thing to another query on my own tomorrow. Would be great if the whole refactoring is structurally the same for all queries, since there are ~ 90 of them alone in .../index/query.
</comment><comment author="cbuescher" created="2015-03-20T22:10:21Z" id="84167064">I talked with @s1monw and we came up with this first rough sketch of how to do procede with the refactoring of the queries in the `org.elasticsearch.index.query` package. I'll start in small incremental steps, not including the filters at the moment.

This is the rough plan of how to go step by step here:
- move all the *Parser code to the corresponding *Builder, make all Builders implement QueryParser
- split existing parse() method according to this prototype into `Query toQuery()`, `fromXContent()` and still keep the exisiting `Query parse()` method
- write tests using each querys doXContent -&gt; fromXContent methods
- make queries implement Streamable, write serialization and tests

I started by creating the feature branch https://github.com/elastic/elasticsearch/tree/feature/query-parse-refactoring and already stated to merged some Builder/Parser pairs there. 
</comment><comment author="javanna" created="2015-03-31T16:41:40Z" id="88165315">I think we can close this PR, we are now working on the https://github.com/elastic/elasticsearch/tree/feature/query-parse-refactoring branch and opening PRs against it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Attachment search not working in Elasticsearch 1.4.3 if search string contains more than three characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9900</link><project id="" key="" /><description>The attachment search does not work if the search string contains more than three characters in ElasticSearch 1.4.3 whereas it works fine for the same settings in ElasticSearch 1.4.2. Can you tell me if there are any particular settings to get this working?
</description><key id="59077251">9900</key><summary>Attachment search not working in Elasticsearch 1.4.3 if search string contains more than three characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rhdip</reporter><labels><label>feedback_needed</label></labels><created>2015-02-26T14:08:21Z</created><updated>2015-04-09T10:08:43Z</updated><resolved>2015-04-09T10:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-26T14:12:36Z" id="76184067">Could you give details on What you exactly did?
And versions?
</comment><comment author="rhdip" created="2015-03-24T14:23:03Z" id="85518271">Sorry for the delayed response. I am trying to search for a given string in the content of the file. Say I have a file abc.docx and it contains the word "shark" in it. If I search for "sha", it returns the file abc.docx. However, if I search for "shar", it does not return that file i.e it seems to process only 3 character search strings while doing content search. I checked this in the latest version(1.5.0) as well and it din't work either. This seems to be working in Elasticsearch 1.4.2. Please let me know if you need further inputs on this.
</comment><comment author="clintongormley" created="2015-04-05T10:27:07Z" id="89749876">Hi @rhdip 

A full recreation (including the index settings, mappings, how you index the attachment, and the doc itself) would be helpful.
</comment><comment author="rhdip" created="2015-04-09T10:08:43Z" id="91181709">Yes I am using the mapper-attachments plugin. The file I am trying to search can be any file word, excel etc. 

I am providing the C# code. The UpdateIndex() method indexes the file name and its contents and the Search() method searches within the indexes.

The mapper class:

```
public class AttachmentES
{
    /// &lt;summary&gt;
    /// Index for Attachments table
    /// &lt;/summary&gt;    
   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public ICollection&lt;string&gt; title { get; internal set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public string name { get; internal set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public ICollection&lt;string&gt; text { get; set; }

   [ElasticProperty(Index = FieldIndexOption.NotAnalyzed)]
   public string content { get; internal set; }

   public string Id { get; internal set; }

   [ElasticProperty(Type = Nest.FieldType.Attachment, Store = false, TermVector = Nest.TermVectorOption.WithPositionsOffsets)]
   public ESAttachment File { get; set; }
}
```

The UpdateIndex method:

```
public void UpdateIndex(int attachmentID)
{
    //get the attachment details from database
    var atta = from a in dbES.GetAttachmentES(attachmentID).ToList()
               select new AttachmentES
               {
                   name = a.FileName,                      
                   Id = a.AttachmentID,                     
               };

    AttachmentES objAttachmentES = new AttachmentES();
    objAttachmentES = atta.FirstOrDefault();

    client.CreateIndex("attachment", c =&gt; c.AddMapping&lt;AttachmentES&gt;(m =&gt; m.MapFromAttributes()));

    setting.SetDefaultIndex("attachment");
    foreach (AttachmentES itm in atta)
    {

        int id;
        id = Convert.ToInt32(itm.Id.Split('_')[0]);
        var rootPath = (CommonClass.AttachmentRootPath());


        if (string.IsNullOrEmpty(itm.name) == false)
        {
            string filePath = rootPath + "\\" + id;// +new FileInfo(itm.name).Extension;


            var attachment = new ESAttachment();

            string fileExt = Path.GetExtension(itm.name);

            if (File.Exists(filePath))
                attachment._content = Convert.ToBase64String(File.ReadAllBytes(filePath));

            else if (File.Exists(filePath + fileExt))
                attachment._content = Convert.ToBase64String(File.ReadAllBytes(filePath + new FileInfo(itm.name).Extension));

            switch (new FileInfo(itm.name).Extension)
            {
                case ".txt":
                    attachment._content_type = "text/plain";
                    break;
                case ".doc":
                    attachment._content_type = "application/msword";
                    break;
                case ".docx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.wordprocessingml.document";
                    break;
                case ".xls":
                    attachment._content_type = "application/vnd.ms-excel";
                    break;
                case ".xlsx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet";
                    break;
                case ".ppt":
                    attachment._content_type = "application/vnd.ms-powerpoint";
                    break;
                case ".pptx":
                    attachment._content_type = "application/vnd.openxmlformats-officedocument.presentationml.presentation";
                    break;
                case ".pdf":
                    attachment._content_type = "application/pdf";
                    break;

                //Images
                case ".png":
                    attachment._content_type = "image/png";
                    break;
                case ".jpeg":
                case ".jpg":
                    attachment._content_type = "image/jpeg";
                    break;
                case ".bmp":
                    attachment._content_type = "image/bmp";
                    break;
                case ".gif":
                    attachment._content_type = "image/gif";
                    break;

                //Videos
                case ".wmv":
                    attachment._content_type = "video/x-ms-wmv";
                    break;
                case ".avi":
                    attachment._content_type = "video/x-msvideo";
                    break;
                case ".flv":
                    attachment._content_type = "video/x-flv";
                    break;
                case ".mp4":
                    attachment._content_type = "video/mp4";
                    break;


                //zip
                case ".zip":
                    attachment._content_type = "application/zip";
                    break;

                //xml
                case ".xml":
                    attachment._content_type = "application/xml";
                    break;
            }

            attachment._name = itm.name;
            itm.File = attachment;
            client.Index&lt;AttachmentES&gt;(itm);
        }
    }
}
```

The Search method:

```
public List&lt;ESSearchResult&gt; Search(string searchString, int userId = 0)
    {
    List&lt;ESSearchResult&gt; lstIDs = new List&lt;ESSearchResult&gt;();
    ESSearchResult objItem = new ESSearchResult();

    string ElasticSearchPath = ConfigurationManager.AppSettings["ElasticSearchURL"].ToString();
    Uri o = new Uri(ElasticSearchPath);
    setting = new ConnectionSettings(o);
    client = new ElasticClient(setting);

    setting.SetDefaultIndex("attachment");

    string q1 = "*" + searchString + "*";

    var result12 = client.Search&lt;AttachmentES&gt;(body1 =&gt;
                 body1.Index("attachment").Query(query12 =&gt;
 query12.QueryString(qs1 =&gt; qs1.Query(q1))
 ).Fields("id"));

    List&lt;string&gt; lst = (from p in result12.FieldSelections
                        select p.FieldValues(item =&gt; item.Id).ElementAt(0).ToString()).ToList();

    List&lt;String&gt; lstAttachments = new List&lt;String&gt;();
    foreach (var i in lst)
    {                        
        lstAttachments.Add(i);            
    }

    objItem = new ESSearchResult();
    objItem.ObjectEntity = Enums.ESEntity.AttachmentES;
    objItem.ObjectRowIDs = lstAttachments.Distinct().ToList&lt;string&gt;();


    lstIDs.Add(objItem);

    return lstIDs;
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ENGINE] Upgrade 3.x segments on engine startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9899</link><project id="" key="" /><description>Upgrading 3.x segments have been proven to be error prone especially when it get's to
concurrency. Bugs like LUCENE-6287 cause index corrupting when segmetns are upgraded
concurrently to a merge. To play safe this commit upgrades pending segments without
merging etc. before the engine gets started up.
</description><key id="59059697">9899</key><summary>[ENGINE] Upgrade 3.x segments on engine startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>bug</label><label>resiliency</label><label>v1.4.4</label><label>v1.5.0</label></labels><created>2015-02-26T11:37:52Z</created><updated>2015-03-19T16:34:52Z</updated><resolved>2015-02-26T15:22:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-26T11:38:49Z" id="76164055">@mikemccand can you take a look
</comment><comment author="s1monw" created="2015-02-26T15:07:28Z" id="76193655">@mikemccand @kimchy pushed a new commit
</comment><comment author="mikemccand" created="2015-02-26T15:12:21Z" id="76194481">Left 2 minor comments else LGTM.
</comment><comment author="s1monw" created="2015-02-26T15:19:31Z" id="76195760">cool thanks, I beefed up the test a bit and applied the feedback
</comment><comment author="mikemccand" created="2015-02-26T15:20:47Z" id="76195993">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close all resources if doStart fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9898</link><project id="" key="" /><description>if we for instance can't bind a port we fail to stop the client bootstrap etc. To prevent this we can just call doStop() it has all the relevant checks in place.
</description><key id="59049611">9898</key><summary>Close all resources if doStart fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T10:12:43Z</created><updated>2015-05-29T17:06:19Z</updated><resolved>2015-03-17T22:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-26T10:15:38Z" id="76153321">we had a thread leaks reported on CI due to that http://build-us-00.elasticsearch.org/job/es_g1gc_master_metal/3929/
</comment><comment author="s1monw" created="2015-02-26T10:15:46Z" id="76153338">@spinscale  can you take a look
</comment><comment author="spinscale" created="2015-03-02T09:35:37Z" id="76681921">LGTM, thinking if we should generalize this a bit more and have all `AbstractLifeCycleComponents` execute something like this to clean up
</comment><comment author="s1monw" created="2015-03-02T09:57:21Z" id="76684752">@spinscale I agreee but in general I think we should remove `#stop` completely, what do you think?
</comment><comment author="spinscale" created="2015-03-02T10:06:19Z" id="76685913">@s1monw agreed, we still have `close()` due to `Releasable` then
</comment><comment author="s1monw" created="2015-03-02T10:07:32Z" id="76686074">I'd love to also remove `#start` if possible :dancers: 
</comment><comment author="spinscale" created="2015-03-02T11:02:30Z" id="76693351">hm. Wondering, how should that work with keeping in mind that constructors in guice apps should not throw exceptions and stuff (all done via listeners?)? I'd be fine with one method for stopping/starting each
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `_boost` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9897</link><project id="" key="" /><description>This has been deprecated since 1.0.0.RC1. It is finally removed here.

closes #8875
</description><key id="59043916">9897</key><summary>Remove `_boost` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T09:29:31Z</created><updated>2015-06-06T15:49:52Z</updated><resolved>2015-02-26T23:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T14:50:12Z" id="76190431">It seems to me that with this change we would fail to parse mappings that have a `_boost`, should we only do it on 2.x indices and ignore `_boost` on older indices?
</comment><comment author="rjernst" created="2015-02-26T20:43:01Z" id="76267346">@jpountz This actually already works.  Any extra mapping elements found on indexes created before 2.0 are already ignored.  Strict mapping checks (ie no leftover fields) are only enforced &gt;= 2.0.  I added this to the static bwc tests to verify.
</comment><comment author="jpountz" created="2015-02-26T22:25:39Z" id="76288093">Perfect then! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mapping : allow user to change mappings on indices with existing data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9896</link><project id="" key="" /><description>All of a sudden I came up with this idea that it would be nice to make mappings changeable even after I have documents indexed.

Reason why, somehow I want to change the mapping but the only way to do that currently is to remove index and reindex them, which is quite burdensome.

Well, I just want to ask your opinion first about this.
</description><key id="59042277">9896</key><summary>mapping : allow user to change mappings on indices with existing data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2015-02-26T09:17:19Z</created><updated>2015-02-28T03:26:00Z</updated><resolved>2015-02-28T03:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:26:00Z" id="76507893">sorry @sweetest, this ain't happening :)  You can add mappings, but you can't change existing mappings without rewriting all of the existing data.

However, we are planning a reindex API which will make that process easier.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_routing` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9895</link><project id="" key="" /><description>`required` is now the only changeable settings (on indexes created after 1.x).

see #8143
</description><key id="59041972">9895</key><summary>Lock down `_routing` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T09:14:48Z</created><updated>2015-06-08T09:03:26Z</updated><resolved>2015-02-26T21:11:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T14:52:09Z" id="76190818">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 1.4.4 not working with elasticsearch-cloud-aws 2.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9894</link><project id="" key="" /><description>After a simple installation of Elasticsearch on Ubuntu using the repository and then doing:

``` bash
/usr/share/elasticsearch/bin/plugin --install elasticsearch/elasticsearch-cloud-aws/2.4.1
```

which finishes successfully. After that, starting Elasticsearch gives the error that a mandatory plugin (`elasticsearch-cloud-aws`) is missing.
</description><key id="59036268">9894</key><summary>Elasticsearch 1.4.4 not working with elasticsearch-cloud-aws 2.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nikolay</reporter><labels /><created>2015-02-26T08:23:08Z</created><updated>2015-02-26T18:00:05Z</updated><resolved>2015-02-26T17:57:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-26T08:25:17Z" id="76139324">Sounds like you defined mandatory plugins in your `elasticsearch.yml` file.
Remove this or if you need to check for mandatory plugins, actual name for AWS is `cloud-aws`.
</comment><comment author="nikolay" created="2015-02-26T17:57:56Z" id="76229418">@dadoonet Thanks, it works now! But `elasticsearch-cloud-aws` used to work on `1.4.2`.
</comment><comment author="dadoonet" created="2015-02-26T18:00:05Z" id="76229862">@nikolay really? I can't recall we change anything on that. Any chance you could test it with `1.4.2`, exact same configuration as before?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `enabled` flag for `_field_names` to replace disabling through `index:no`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9893</link><project id="" key="" /><description>Almost all of our meta fields that allow enabling/disabling have an `enabled`
setting.  However, _field_names is enabled by default, and disabling
requires setting `index=no`.  This change adds a flag similar to that
with other meta fields.
</description><key id="59034549">9893</key><summary>Add `enabled` flag for `_field_names` to replace disabling through `index:no`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-26T08:03:42Z</created><updated>2015-06-07T10:53:19Z</updated><resolved>2015-02-26T21:02:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T08:57:25Z" id="76142986">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Are the repositories down?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9892</link><project id="" key="" /><description>Since at least yesterday, the repositories return 404s, which make my Ubuntu builds use the standard Ubuntu packages instead.
</description><key id="59031742">9892</key><summary>Are the repositories down?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nikolay</reporter><labels /><created>2015-02-26T07:32:19Z</created><updated>2015-02-26T14:55:05Z</updated><resolved>2015-02-26T07:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-02-26T07:42:14Z" id="76134981">Seems ok to me;

``` bash
# apt-get install elasticsearch
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.
Need to get 26.7 MB of archives.
After this operation, 30.6 MB of additional disk space will be used.
0% [Working]
Get:1 http://packages.elasticsearch.org/elasticsearch/1.4/debian/ stable/main elasticsearch all 1.4.4 [26.7 MB]
Fetched 26.7 MB in 25s (1,035 kB/s)
Selecting previously unselected package elasticsearch.
(Reading database ... 103016 files and directories currently installed.)
Preparing to unpack .../elasticsearch_1.4.4_all.deb ...
Unpacking elasticsearch (1.4.4) ...
Processing triggers for ureadahead (0.100.0-16) ...
Setting up elasticsearch (1.4.4) ...
```

If you are getting errors, it'd help if you pasted them.
</comment><comment author="nikolay" created="2015-02-26T07:51:30Z" id="76135852">Hmmm... seems to be working now.
</comment><comment author="dadoonet" created="2015-02-26T08:00:07Z" id="76136638">Wondering if we have some hiccups here. @drewr WDYT?
</comment><comment author="drewr" created="2015-02-26T14:55:04Z" id="76191351">Didn't get alerted to any outages (we ping from multiple datacenters around the world). Don't see any service degradations either, just the usual lull during off-peak US hours.

![screen shot 2015-02-26 at 8 50 32 am](https://cloud.githubusercontent.com/assets/6202/6394079/a8cd0fba-bd94-11e4-9aff-1e30e755419c.png)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClassNotFoundException: com.vividsolutions.jts.geom.Coordinate </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9891</link><project id="" key="" /><description>I'm on the latest version (1.4.4) of Elasticsearch. And Elasticsearch artifact is in my Maven dependencies, too.

But when I try to build query using `QueryBuilders`, I get this `Caused by: java.lang.ClassNotFoundException: com.vividsolutions.jts.geom.Coordinate`.

I'm using Elasticsearch in Groovy code. Here is my code using `QueryBuilder`:

```
        def query = QueryBuilders
                .indicesQuery(QueryBuilders.filteredQuery(QueryBuilders.matchAllQuery(), FilterBuilders.termFilter('_id', someId)), indices)
                .noMatchQuery("none")
```

So what am I missing here? 

Thanks!
</description><key id="58994347">9891</key><summary>ClassNotFoundException: com.vividsolutions.jts.geom.Coordinate </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">cooniur</reporter><labels><label>feedback_needed</label></labels><created>2015-02-26T00:22:50Z</created><updated>2016-02-24T07:47:21Z</updated><resolved>2015-06-18T20:12:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-26T05:26:45Z" id="76124562">Read this: http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/query-dsl-queries.html#geo-shape

You are missing dependency here.
</comment><comment author="cooniur" created="2015-02-26T05:42:36Z" id="76125600">Hi @dadoonet ,

Thanks for the solution! It works if I add those dependencies.

However, the problem is that I don't use any geo related queries in my code. What I use in my code are just queries like aggregations, filters. And actually in the last 4 months I never had this problem, and it suddenly appears today.
</comment><comment author="dadoonet" created="2015-02-26T07:42:02Z" id="76134956">Interesting. Reopening for more checks.
</comment><comment author="dadoonet" created="2015-02-26T08:51:47Z" id="76142311">That's super strange. I did not manage to reproduce it.

I have a simple `pom.xml`:

``` xml
&lt;properties&gt;
    &lt;elasticsearch.version&gt;1.4.4&lt;/elasticsearch.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
        &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;
        &lt;version&gt;2.3.2&lt;/version&gt;
        &lt;classifier&gt;indy&lt;/classifier&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;log4j&lt;/groupId&gt;
        &lt;artifactId&gt;log4j&lt;/artifactId&gt;
        &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
```

And a sample code:

``` java
    @Test
    public void testDateRange() {
        AggregationBuilder aggregation =
                AggregationBuilders
                        .dateRange("agg")
                        .field("dateOfBirth")
                        .format("yyyy")
                        .addUnboundedTo("1950")             // from -infinity to 1950 (excluded)
                        .addRange("1950", "1960")           // from 1950 to 1960 (excluded)
                        .addUnboundedFrom("1960");          // from 1960 to +infinity

        SearchResponse sr = client.prepareSearch()
                .addAggregation(aggregation)
                .get();
        logger.info("response [{}]", sr.toString());

        DateRange agg = sr.getAggregations().get("agg");

        // For each entry
        for (DateRange.Bucket entry : agg.getBuckets()) {
            String key = entry.getKey();                    // Date range as key
            DateTime fromAsDate = entry.getFromAsDate();    // Date bucket from as a Date
            DateTime toAsDate = entry.getToAsDate();        // Date bucket to as a Date
            long docCount = entry.getDocCount();            // Doc count

            logger.info("key [{}], from [{}], to [{}], doc_count [{}]", key, fromAsDate, toAsDate, docCount);
        }
    }
```

Everything looks correct.

May it's related to Groovy?
</comment><comment author="cooniur" created="2015-02-26T19:52:50Z" id="76255620">Hi @dadoonet, thanks for the effort reproducing the issue!

I don't know whether it is related to Groovy and I doubt it's because of groovy because for the past 4 or 5 month this issue never happens.

I'm trying to purge my local Maven repository, purge all remote artifacts in our Artifactory server and rebuild the project to see.
</comment><comment author="dadoonet" created="2015-02-26T20:08:22Z" id="76259538">It might be depend on your data. If you have a sample mapping and some data I could try to reproduce this in Java.

Let me know how it goes.
</comment><comment author="cooniur" created="2015-02-26T21:13:37Z" id="76274664">@dadoonet 
I just setup a clean environment to test Elasticsearch API. Here is what I did.

Dependencies:

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
    &lt;project xmlns="http://maven.apache.org/POM/4.0.0"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

```
    &lt;groupId&gt;com.tliu.demo&lt;/groupId&gt;
    &lt;artifactId&gt;esclient&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;

        &lt;plugin.groovy-eclipse-compiler&gt;2.9.0-01&lt;/plugin.groovy-eclipse-compiler&gt;
        &lt;plugin.groovy-eclipse-batch&gt;2.3.4-01&lt;/plugin.groovy-eclipse-batch&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.4.4&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
            &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;
            &lt;version&gt;2.4.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;1.7.10&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
            &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
            &lt;version&gt;1.1.2&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.12&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.hamcrest&lt;/groupId&gt;
            &lt;artifactId&gt;hamcrest-all&lt;/artifactId&gt;
            &lt;version&gt;1.3&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;finalName&gt;${project.name}&lt;/finalName&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;${java.version}&lt;/source&gt;
                    &lt;target&gt;${java.version}&lt;/target&gt;
                    &lt;compilerId&gt;groovy-eclipse-compiler&lt;/compilerId&gt;
                &lt;/configuration&gt;
                &lt;dependencies&gt;
                    &lt;!-- Groovy compiler support --&gt;
                    &lt;dependency&gt;
                        &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
                        &lt;artifactId&gt;groovy-eclipse-compiler&lt;/artifactId&gt;
                        &lt;version&gt;${plugin.groovy-eclipse-compiler}&lt;/version&gt;
                    &lt;/dependency&gt;
                    &lt;dependency&gt;
                        &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
                        &lt;artifactId&gt;groovy-eclipse-batch&lt;/artifactId&gt;
                        &lt;version&gt;${plugin.groovy-eclipse-batch}&lt;/version&gt;
                    &lt;/dependency&gt;
                &lt;/dependencies&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
```

`EsClientFactory.java`: the same class I used in my project.

```
package com.tliu.demo.esclient;

import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;

public class EsClientFactory
{
    private final Client client;

    public EsClientFactory(String clusterName, String hostname, int port)
    {
        Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clusterName).build();
        client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress(hostname, port));
    }

    public Client getClient()
    {
        return client;
    }

    public void shutdown()
    {
        client.close();
    }
}
```

`EsClientFactoryTest.java`:

package com.tliu.demo.esclient

```
import org.elasticsearch.action.search.SearchRequestBuilder
import org.elasticsearch.action.search.SearchResponse
import org.elasticsearch.client.Client
import org.elasticsearch.index.query.FilterBuilders
import org.elasticsearch.index.query.QueryBuilders
import org.junit.Before
import org.junit.Test
import org.slf4j.Logger
import org.slf4j.LoggerFactory

public class EsClientFactoryTest
{
    private final Logger logger = LoggerFactory.getLogger(this.class)


    EsClientFactory esClientFactory

    @Before
    public void before()
    {
        esClientFactory = new EsClientFactory("elasticsearch", "localhost", 9335)
    }

    @Test
    public void test()
    {
        Client client = esClientFactory.getClient();
        SearchRequestBuilder request = client.prepareSearch("patient_centric_1")
                .setTypes("patient_data")
                .setQuery(QueryBuilders.filteredQuery(QueryBuilders.matchAllQuery(), FilterBuilders.termFilter("_id", "100")))

        logger.debug("Request: {}", request)

        SearchResponse response = request.execute().actionGet()

        logger.debug("Response: {}", response)
    }
}
```

And it throws the exception at `.setQuery(QueryBuilders.filteredQuery(QueryBuilders.matchAllQuery(), FilterBuilders.termFilter("_id", "100")))`

I checked the `QueryBuilders` source code, it uses `org.elasticsearch.common.geo.builders.ShapeBuilder`, where the `com.vividsolutions.jts.geom.*` was imported. But as you see in my test code, I didn't use the `geoShapeQuery()` method.  

Try to use `QueryBuilders` in your test code.
</comment><comment author="cooniur" created="2015-02-26T21:18:35Z" id="76275592">@dadoonet 
I just pushed my code to my Github: https://github.com/cooniur/esclient-demo
</comment><comment author="javanna" created="2015-02-27T21:40:45Z" id="76476997">maybe @nknize can have a look too? Related to #9462 ?
</comment><comment author="dadoonet" created="2015-06-03T15:18:24Z" id="108479949">Hey guys.

I was looking at the related mentioned issues and it looks like every issue/PR have been closed now.
Should we consider this issue as closed as well? 

@cooniur Were you able to get your stuff working?
</comment><comment author="cooniur" created="2015-06-11T07:43:32Z" id="111031393">Hey @dadoonet , I'd like to test if it's still broken after we upgrade to 1.6. Will let you know the result.

Thanks!
</comment><comment author="cooniur" created="2015-06-17T23:01:03Z" id="112975757">Hey @dadoonet I did a test and found this issue is still there for me. Both the Java client and the server is on the latest version of ElasticSearch (1.6.0).

Here is the exception I got:

```
Caused by: java.lang.NoClassDefFoundError: com/vividsolutions/jts/geom/Coordinate
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
    at java.lang.Class.getDeclaredMethods(Class.java:1975)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass$1.run(CachedSAMClass.java:101)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass$1.run(CachedSAMClass.java:99)
    at java.security.AccessController.doPrivileged(Native Method)
    ...
    ... 19 common frames omitted
Caused by: java.lang.ClassNotFoundException: com.vividsolutions.jts.geom.Coordinate
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 101 common frames omitted
```
</comment><comment author="dadoonet" created="2015-06-18T04:56:07Z" id="113037963">I think you forgot to add optional dependencies ?

What is your dependency tree ?
</comment><comment author="cooniur" created="2015-06-18T05:02:34Z" id="113038608">@dadoonet As I mentioned before (quoted below), if I add the geo dependency `com.vividsolutions:jts`, the problem will solve. However, the thing is I don't use any geo related queries in the code.

&gt; on February 25
&gt; Hi @dadoonet ,
&gt; 
&gt; Thanks for the solution! It works if I add those dependencies.
&gt; 
&gt; However, the problem is that I don't use any geo related queries in my code. What I use in my code are just queries like aggregations, filters. And actually in the last 4 months I never had this problem, and it suddenly appears today.
</comment><comment author="dadoonet" created="2015-06-18T05:08:45Z" id="113040014">Do you have the full stack trace?
I'm pretty sure I got the same error when writing tests for lang plugins.

May be we should mark this dep as not optional anymore ?
</comment><comment author="dadoonet" created="2015-06-18T07:23:43Z" id="113061178">I can confirm I just saw this issue again in azure tests. The full stack trace is the one you already provided. Nothing more than that. :( Looking at this.
</comment><comment author="dadoonet" created="2015-06-18T07:30:57Z" id="113062260">@cooniur Any chance you are running your tests from IntelliJ and not from the command line? 

I mean I can reproduce the issue when running tests from IntelliJ but not with `mvn test`.

Could you check please?
</comment><comment author="dadoonet" created="2015-06-18T07:32:04Z" id="113062403">BTW, the full stack trace is:

```
Exception in thread "main" java.lang.NoClassDefFoundError: com/vividsolutions/jts/geom/Geometry
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2570)
    at java.lang.Class.getMethod0(Class.java:2813)
    at java.lang.Class.getMethod(Class.java:1663)
    at org.junit.internal.builders.SuiteMethodBuilder.hasSuiteMethod(SuiteMethodBuilder.java:18)
    at com.intellij.junit4.JUnit46ClassesRequestBuilder.collectWrappedRunners(JUnit46ClassesRequestBuilder.java:79)
    at com.intellij.junit4.JUnit46ClassesRequestBuilder.getClassesRequest(JUnit46ClassesRequestBuilder.java:51)
    at com.intellij.junit4.JUnit4TestRunnerUtil.buildRequest(JUnit4TestRunnerUtil.java:91)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:39)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: java.lang.ClassNotFoundException: com.vividsolutions.jts.geom.Geometry
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 16 more
```

There is a specific IntelliJ Runner which might cause such an issue... ???
</comment><comment author="dadoonet" created="2015-06-18T07:52:42Z" id="113066641">For information, I just opened an issue here: https://youtrack.jetbrains.com/issue/IDEA-141605
That said @cooniur, you might not be hitting the same...
</comment><comment author="dadoonet" created="2015-06-18T08:22:23Z" id="113073293">Interesting. When I check `Search for tests: in single module` I don't get this issue anymore.
Which makes sense to me. Might be a nice workaround.
</comment><comment author="cooniur" created="2015-06-18T17:43:46Z" id="113232539">@dadoonet I tested running my project from terminal by executing `java -jar myproject.jar` (note this is not mvn test), but still got the same exception. If I run the project in IntelliJ, I get the same exceptions too.

The project is a Spring Boot project. Spring Boot version is 1.2.4.RELEASE. Groovy 2.4.3 is used via dependency `org.codehaus.groovy:groovy-all`. My environment is Mac Yosemite 10.10.3.

A full stack trace is attached below.

```
org.apache.camel.CamelExecutionException: Exception occurred during execution on the exchange: Exchange[org.apache.camel.component.file.GenericFileMessage@52ad6815]
    at org.apache.camel.util.ObjectHelper.wrapCamelExecutionException(ObjectHelper.java:1635)
    at org.apache.camel.impl.DefaultExchange.setException(DefaultExchange.java:308)
    at org.apache.camel.component.bean.MethodInfo$1.proceed(MethodInfo.java:254)
    at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:171)
    at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:77)
    at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:448)
    at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191)
    at org.apache.camel.processor.Pipeline.process(Pipeline.java:118)
    at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)
    at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191)
    at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:435)
    at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:211)
    at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:175)
    at org.apache.camel.impl.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:174)
    at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:101)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: com/vividsolutions/jts/geom/Coordinate
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
    at java.lang.Class.getDeclaredMethods(Class.java:1975)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass$1.run(CachedSAMClass.java:101)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass$1.run(CachedSAMClass.java:99)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass.getDeclaredMethods(CachedSAMClass.java:99)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass.getAbstractMethods(CachedSAMClass.java:117)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass.getSAMMethod(CachedSAMClass.java:183)
    at org.codehaus.groovy.reflection.ClassInfo.isSAM(ClassInfo.java:356)
    at org.codehaus.groovy.reflection.ClassInfo.createCachedClass(ClassInfo.java:346)
    at org.codehaus.groovy.reflection.ClassInfo.access$700(ClassInfo.java:38)
    at org.codehaus.groovy.reflection.ClassInfo$LazyCachedClassRef.initValue(ClassInfo.java:494)
    at org.codehaus.groovy.reflection.ClassInfo$LazyCachedClassRef.initValue(ClassInfo.java:485)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at org.codehaus.groovy.reflection.ClassInfo.getCachedClass(ClassInfo.java:108)
    at org.codehaus.groovy.reflection.ReflectionCache.getCachedClass(ReflectionCache.java:107)
    at org.codehaus.groovy.reflection.ParameterTypes.getParametersTypes0(ParameterTypes.java:78)
    at org.codehaus.groovy.reflection.ParameterTypes.getParameterTypes(ParameterTypes.java:64)
    at org.codehaus.groovy.reflection.CachedMethod.compareToCachedMethod(CachedMethod.java:155)
    at org.codehaus.groovy.reflection.CachedMethod.compareTo(CachedMethod.java:137)
    at java.util.ComparableTimSort.binarySort(ComparableTimSort.java:258)
    at java.util.ComparableTimSort.sort(ComparableTimSort.java:203)
    at java.util.Arrays.sort(Arrays.java:1246)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:119)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:81)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at org.codehaus.groovy.reflection.CachedClass.getMethods(CachedClass.java:257)
    at groovy.lang.MetaClassImpl.populateMethods(MetaClassImpl.java:362)
    at groovy.lang.MetaClassImpl.fillMethodIndex(MetaClassImpl.java:341)
    at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:3261)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:251)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:282)
    at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:255)
    at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:872)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallStaticSite(CallSiteArray.java:72)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallSite(CallSiteArray.java:159)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:110)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:114)
    (omitted 3)
    at sun.reflect.GeneratedMethodAccessor386.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:382)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:179)
    (omitted 1)
    at sun.reflect.GeneratedMethodAccessor385.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:292)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
    at groovy.lang.Closure.call(Closure.java:423)
    at org.codehaus.groovy.runtime.ConvertedClosure.invokeCustom(ConvertedClosure.java:51)
    at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:103)
    at com.sun.proxy.$Proxy171.accept(Unknown Source)
    at java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1540)
    at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
    at sun.reflect.GeneratedMethodAccessor309.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrap.invoke(PojoMetaMethodSite.java:210)
    at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:122)
    (omitted 3)
    at sun.reflect.GeneratedMethodAccessor383.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.apache.camel.component.bean.MethodInfo.invoke(MethodInfo.java:408)
    at org.apache.camel.component.bean.MethodInfo$1.doProceed(MethodInfo.java:279)
    at org.apache.camel.component.bean.MethodInfo$1.proceed(MethodInfo.java:252)
    ... 19 common frames omitted
Caused by: java.lang.ClassNotFoundException: com.vividsolutions.jts.geom.Coordinate
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at org.springframework.boot.loader.LaunchedURLClassLoader.doLoadClass(LaunchedURLClassLoader.java:170)
    at org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:136)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 102 common frames omitted
```
</comment><comment author="dadoonet" created="2015-06-18T18:20:24Z" id="113246787">It sounds like you are using Camel/Groovy and this is causing an issue because of reflection.
I'm not sure we need to fix something in Elasticsearch code base.

The work around looks ok to me so add this lib and mustache to your project and it should be fine.
</comment><comment author="cooniur" created="2015-06-18T18:36:11Z" id="113252592">@dadoonet Yeah, it looks like Groovy is the culprit. I checked our project's commit log. This issue happened after Groovy was upgraded from 2.3.x to 2.4.0.

But I'll do more test with another pure Java project to see if the problem is also there. I'll put the report here after I'm done.

Thanks!
</comment><comment author="dadoonet" created="2015-06-18T20:12:50Z" id="113276432">So I'm closing this for now. Feel free to reopen if you think something could be fixed on our end but I doubt it.
</comment><comment author="cooniur" created="2015-10-08T22:52:19Z" id="146710161">Hey @dadoonet 
I think now I can say for sure that this is related to Groovy. It looks like that Groovy will try to fetch the non-existent class by reflection, causing the `NoClassFoundException` eventually.

How can we solve this problem?

Thanks!
</comment><comment author="dadoonet" created="2015-10-09T01:23:01Z" id="146731560">@pickypg any idea?
</comment><comment author="luigibertolucci" created="2015-10-21T14:50:41Z" id="149920476">Hey @dadoonet , @cooniur .
I just ran into this same issue. Is there any workaround/solution?

Cheers!
</comment><comment author="cooniur" created="2015-10-22T04:47:41Z" id="150105236">@luigibertolucci 
Hey, the solution is add this dependency to your project (assuming you were using Maven):

```
&lt;dependency&gt;
    &lt;groupId&gt;com.vividsolutions&lt;/groupId&gt;
    &lt;artifactId&gt;jts&lt;/artifactId&gt;
    &lt;version&gt;1.13&lt;/version&gt;
&lt;/dependency&gt;
```
</comment><comment author="pickypg" created="2015-10-22T17:17:38Z" id="150295184">@dadoonet Sorry, I was on vacation two weeks ago. :)

I believe that this is just Groovy being naive about its MetaClass generation. My only real recommendation is to try to use the invokedynamic version of Groovy, which will be faster, and hopefully bypass the naive generation.

Perhaps even better, maybe you can switch over to using the equivalent Elasticsearch Groovy (http://github.com/elastic/elasticsearch-groovy) client instead of depending on Elasticsearch directly, which adds Groovy-friendly variants to the client methods like:

``` groovy
SearchResponse response = client.search {
  indices "patient_centric_1"
  types "patient_data"
  source {
    query {
      filtered {
        filter {
          term {
            _id = "100"
          }
        }
      }
    }
  }
}
```
</comment><comment author="janbols" created="2016-02-24T07:47:21Z" id="188129173">I don't have the issue anymore when upgrdagin groovy version from 2.4.4 to 2.4.6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow highlighting to work on script fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9890</link><project id="" key="" /><description>I have a use case where it would be helpful to use the `highlight_query` to highlight script fields to minimize returning/highlighting an otherwise massive string (or even just highlighting general, but this might be easier):

```
{
  "query": {
    "match_all": {}
  },
  "script_fields": {
    "my_script_field": {
      "script": "doc['my.enormous.string.field'].value.substring(x, y)"
    }
  },
  "highlight": {
    "fields": {
      "other": {
        "highlight_query": {
          "match": {
            "my_script_field": {
              "query": "whatever",
              "analyzer": "standard"
            }
          }
        }
      }
    }
  }
}
```

Naturally that is specifying the _search_ analyzer, but it could be treated as _both_ the index and search analyzer for highlighting (if it even needs to do that much).
</description><key id="58994061">9890</key><summary>Allow highlighting to work on script fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Highlighting</label><label>discuss</label></labels><created>2015-02-26T00:21:26Z</created><updated>2015-03-25T04:21:28Z</updated><resolved>2015-03-25T04:21:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:22:34Z" id="76507752">Hi @pickypg 

What is the use case exactly? Not returning an enormous string doesn't sound valid, because you should get back fragments of the specified size.  
</comment><comment author="pickypg" created="2015-03-02T17:16:06Z" id="76754526">@clintongormley The intended use case is effectively to page an enormous string field, while still supporting highlighting.
</comment><comment author="nik9000" created="2015-03-02T17:49:36Z" id="76761435">&gt; @clintongormley The intended use case is effectively to page an enormous string field, while still supporting highlighting.

Not to steal any thunder from elasticsearch itself but I'm happy to work with you at https://github.com/wikimedia/search-highlighter - I can't really tell what you are looking for but you might want to have a look at the [Segmenter](https://github.com/wikimedia/search-highlighter/blob/master/experimental-highlighter-core/src/main/java/org/wikimedia/search/highlighter/experimental/Segmenter.java) interface.  Its designed to allow relatively fine grained control over what segments _might_ be acceptable.

Either here or in the highlighter plugin you'd need some kind of curl recreation of what sort of paging you are going for.
</comment><comment author="clintongormley" created="2015-03-02T17:56:21Z" id="76762857">@pickypg how many highlighted snippets (or fragments) are you intending to retrieve from each doc?  If you want (eg) the best 5 snippets, then you'd be better of by indexing the string with `index_options` set to `offsets`, and using the postings highlighter (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#postings-highlighter)

That way Elasticsearch already knows the positions of each word in the original string, and it can return them without have to reanalyze any text on the fly.

Alternatively, if you're really wanting multiple fragments from each "page" (or whatever), then perhaps you'd be better indexing the document as multiple documents.
</comment><comment author="pickypg" created="2015-03-25T04:21:28Z" id="85827769">I have created the issue over at the wikimedia/search-highlighter#12 per @nik9000's suggestion and we can close this issue here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Ordering buckets based on terms should sort numerically if the underlying field is numeric.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9889</link><project id="" key="" /><description>For terms aggregations it would be great if ES either automatically or via an option applied a numeric, rather than alphabetic sort when ordering on terms where the underlying field is numeric.  
</description><key id="58993341">9889</key><summary>Aggregations: Ordering buckets based on terms should sort numerically if the underlying field is numeric.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yeroc</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-02-26T00:16:38Z</created><updated>2015-12-05T18:53:50Z</updated><resolved>2015-12-05T18:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sigpwned" created="2015-09-12T15:14:51Z" id="139783402">+1
</comment><comment author="clintongormley" created="2015-12-05T18:53:50Z" id="162234558">This appears to work, at least in 2.1.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop passing default cluster name to cluster state read operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9888</link><project id="" key="" /><description>...ions

The default cluster name was needed for backwards compatibility with pre v1.1 nodes. It's no longer needed in 2.0.
</description><key id="58973437">9888</key><summary>Stop passing default cluster name to cluster state read operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T21:32:53Z</created><updated>2015-06-06T19:12:36Z</updated><resolved>2015-02-26T00:56:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-25T21:38:07Z" id="76065856">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> fixing typo in expDecayFunction and adding offset to all dacay function...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9887</link><project id="" key="" /><description /><key id="58961722">9887</key><summary> fixing typo in expDecayFunction and adding offset to all dacay function...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">matiastealdi</reporter><labels><label>:Query DSL</label><label>docs</label></labels><created>2015-02-25T20:10:12Z</created><updated>2015-03-05T11:31:28Z</updated><resolved>2015-03-05T11:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="matiastealdi" created="2015-02-25T20:23:06Z" id="76048042">@brwe Can you check it, I have erase the extra minus in the expDecay function and I have added the offset to all of them. I get the offset part of the equation from ES code. I have tested the equations with gnuplot and I have build the documentation with elasticsearch/doc guide.
I have signed the contributor-agreement.

```
#Plot settings
set term jpeg 
#Uncomment if it is not using plot.sh
set output "img.jpeg"

set grid ytics lt 0 lw 1 lc rgb "#bbbbbb"
set grid xtics lt 0 lw 1 lc rgb "#bbbbbb"

set yrange [0:1]
set xrange [0:1] 

#Auxiliar Functions
max(x,y) = (x &gt; y) ? x : y
mod(x) = (x&lt;0) ? -x : x

#function parameters
decay = 0.5
scale = 0.2
offset = 0.1

#Auxilar Functions
ESValue(x, origin) = max(0., mod(x - origin) - offset)
Lambda = log(decay) / scale
s = scale / (1.0 - decay)
sigma = -scale*scale / (2*log(decay))

gaussDecay(x, origin) = exp(- ESValue(x, origin) * ESValue(x, origin) / (2*sigma))
linearDecay(x, origin) = max(0., (s - ESValue(x, origin)) / s) 
expDecay(x, origin) = exp(Lambda*ESValue(x,origin)) 

#plot gaussDecay(x, .5)
plot expDecay(x, .5)
#plot linearDecay(x, .5)
```
</comment><comment author="brwe" created="2015-03-05T11:30:47Z" id="77348860">@matiastealdi thanks a lot for the fix! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: check node ports availability when using unicast discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9886</link><project id="" key="" /><description>Some tests failures are seen when a node attempts to use a port that is already bound
by some other process on the test machine. This commit adds a bind to test port availability
and iterates over the port range until an available port is found. This reduces the likelihood
of a test node failing to start up due to the port already being bound.
</description><key id="58947685">9886</key><summary>Tests: check node ports availability when using unicast discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T18:27:08Z</created><updated>2015-03-16T20:28:50Z</updated><resolved>2015-02-27T17:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-02-25T19:27:44Z" id="76036339">I like it. Left some comments. Thx for picking it up.
</comment><comment author="jaymode" created="2015-02-26T13:48:55Z" id="76180326">@bleskes thanks for the feedback. I implemented the changes to have a port range of 1000 and to cycle through the range.
</comment><comment author="bleskes" created="2015-02-27T14:24:03Z" id="76401509">thx @jaymode - I wonder if we should remember the last port we used and start looking from there? now we start looking from baseport every time so it's likely the first ports we try are still not freed up?
</comment><comment author="jaymode" created="2015-02-27T14:31:32Z" id="76402700">@bleskes we start from `nextPort` which is a static variable. First time the class is initialized, it is set to the base port. While looking for ports, we increment `nextPort` for every try. On the next invocation this class will continue iterating up the port range starting from the port after the last one that was used. Do you see something else that I'm missing?
</comment><comment author="bleskes" created="2015-02-27T14:46:15Z" id="76405069">@jaymode I see I missed it.  I left some minor comments.
</comment><comment author="jaymode" created="2015-02-27T15:09:21Z" id="76408674">@bleskes updated with the feedback
</comment><comment author="bleskes" created="2015-02-27T15:19:26Z" id="76410354">LGTM thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DateMath: Use time zone when rounding. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9885</link><project id="" key="" /><description>Currently rounding in DateMathParser is always done in UTC, even 
when another time zone is specified. This is fixed by passing the time zone 
down to the rounding logic when it is specified.

Closes  #9814
</description><key id="58938820">9885</key><summary>DateMath: Use time zone when rounding. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Dates</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T17:28:11Z</created><updated>2015-03-19T10:16:59Z</updated><resolved>2015-02-25T18:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-02-25T17:50:47Z" id="76016531">LGTM
</comment><comment author="cbuescher" created="2015-02-25T23:43:50Z" id="76086607">On 1.x with 8391b51.

@rjernst I ran into a bit of a mess trying to backport the tests in DateMathParserTests for this PR to the 1.4 branch. The diff between the two branches is quiet big for that test class, so adding just the new lines for this fix is difficult. I tried backporting the the whole test class form 1.x to 1.4, but in that case I had to change  a lot of the actual tests to the current behaviour of DateMath on 1.4. 
Any suggestions how to procede here? Keep the orginal DateMathParserTests from 1.4 and only try to add the new tests there or try to update the whole thing according to the current state of tests in 1.x?
</comment><comment author="rjernst" created="2015-02-25T23:47:42Z" id="76087109">If the backport is too complicated, we should not do it.  I don't think this is critical enough that it must be in a bugfix release.
</comment><comment author="cbuescher" created="2015-02-26T09:58:57Z" id="76150995">@rjernst The backport is not difficult, would be nice to have on 1.4. Just the test class changed a lot from 1.4 to 1.x, so the tests will have to be adapted and be slightly different from this PR. Would you mind having another look if I push to my repo before merging?
</comment><comment author="rjernst" created="2015-02-27T07:13:56Z" id="76349172">@cbuescher Sure, let me know where to look.
</comment><comment author="cbuescher" created="2015-02-27T08:51:32Z" id="76357777">I pushed the changes agains 1.4 branch to https://github.com/cbuescher/elasticsearch/commit/997b8ce3c488fc3ed23789a0874c7d7cbf32f8c7. Basically it's the same fix as in 1.x, I only backportet the rounding tests and had to adapt them to the "old" rounding logic on 1.4. (I think you changed the edge inclusions on the 1.x branch)
</comment><comment author="rjernst" created="2015-02-27T16:25:08Z" id="76422450">@cbuescher +1 to the backport
</comment><comment author="cbuescher" created="2015-03-02T10:07:13Z" id="76686035">Great, pushed to 1.4 branch with dff19cb
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene Expressions can't subtract two date values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9884</link><project id="" key="" /><description>We have many docs in Elasticsearch with two date fields. When trying to have a scripted field to compute the date differences we get `0`s. The date fields are in the format of `dd/mm/yyyy` with proper mapping. I can confirm date field values are represented as a number (as expected).

Only subtraction between two fields didn't work; adding two date fields or adding or subtracting a constant worked perfectly fine.

From Kibana:

```
{
  "size": 0,
  "query": {
    "query_string": {
      "analyze_wildcard": true,
      "query": "*"
    }
  },
  "aggs": {
    "2": {
      "terms": {
        "script": "doc['dispatchDate'].value-doc['registrationDate'].value",
        "lang": "expression",
        "size": 5,
        "order": {
          "_count": "desc"
        },
        "valueType": "float"
      }
    }
  }
}
```

```
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 1001,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "2": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": 0,
          "doc_count": 1001
        }
      ]
    }
  }
}
```

FWIW it may be a Kibana bug marking this as `type:float`
</description><key id="58938592">9884</key><summary>Lucene Expressions can't subtract two date values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>feedback_needed</label></labels><created>2015-02-25T17:26:54Z</created><updated>2015-03-14T21:06:22Z</updated><resolved>2015-03-14T21:06:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:12:36Z" id="76507388">Hi @synhershko 

This worked for me on 1.4.3:

```
DELETE t

PUT /t/t/1
{
  "one": "2014-11-01",
  "two": "2014-12-01"
}

GET /t/_search
{
  "size": 0,
  "aggs": {
    "2": {
      "terms": {
        "script": "doc['two'].value-doc['one'].value",
        "lang": "expression",
        "size": 5,
        "order": {
          "_count": "desc"
        },
        "valueType": "float"
      }
    }
  }
}
```

returns:

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "2": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": 2592000000,
               "doc_count": 1
            }
         ]
      }
   }
}
```

Perhaps your dates aren't mapped as dates?
</comment><comment author="synhershko" created="2015-02-28T16:32:52Z" id="76533169">@clintongormley they were mapped as dates, we were able to do other types of math. This was using 1.4.4 IIRC, will look again later
</comment><comment author="synhershko" created="2015-03-14T21:06:22Z" id="80720016">Closing this for now, will reopen if I can come up with a minimal repro
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>investigate alternatives for jsr166e</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9883</link><project id="" key="" /><description>I don't like that we have this fork of code, uses unsafe, etc.

instead maybe these cache statistics could be done in such a way that it uses LongAdder if you are on java 8 vm but just uses AtomicLong on java 7. 
</description><key id="58928314">9883</key><summary>investigate alternatives for jsr166e</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-02-25T16:31:58Z</created><updated>2015-12-05T18:03:44Z</updated><resolved>2015-12-05T18:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T18:03:44Z" id="162231451">Fixed in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>investigate strange reflection hack found by percolation tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9882</link><project id="" key="" /><description>Maybe i unfairly blamed percolator, but at least its tests found this (in tests.policy)

// needed by percolation??? (what is going on here)
- permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
</description><key id="58928007">9882</key><summary>investigate strange reflection hack found by percolation tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2015-02-25T16:30:15Z</created><updated>2015-02-25T20:32:58Z</updated><resolved>2015-02-25T20:32:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-02-25T19:15:26Z" id="76034000">I investigated, its something coming out of the groovy engine. I'll fix the comment in the test policy,

&gt; Caused by: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.reflect")
&gt;    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
&gt;    at java.security.AccessController.checkPermission(AccessController.java:884)
&gt;    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
&gt;    at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1564)
&gt;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:311)
&gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
&gt;    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:676)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting on wildcard fields should only include appropriate fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9881</link><project id="" key="" /><description>If you highlight on `*` fields today, it includes all fields, regardless of whether they are appropriate for highlighting or not.

Instead, we should automatically include only fields that can be highlighted by the specified highlighter, ie ignore `not_analyzed` fields, numbers, dates, etc.  If the `fvh` is specified, then only highlight on fields with positions and offsets, and similar for the postings highlighter.

Fixes https://github.com/elasticsearch/elasticsearch/issues/5836
</description><key id="58917600">9881</key><summary>Highlighting on wildcard fields should only include appropriate fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Highlighting</label><label>v1.6.0</label></labels><created>2015-02-25T15:25:22Z</created><updated>2015-05-27T15:11:54Z</updated><resolved>2015-05-27T15:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-25T15:26:44Z" id="75979812">I think that `*` should be expanded to any case where there is a wildcard used, like here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java#L82
</comment><comment author="ppf2" created="2015-02-26T05:35:10Z" id="76125139">+1 

This is ultimately causing Kibana 4's document table component to fail loading when there are large text fields present (even when it is not necessary to highlight the large text field).   Here is an example query generated by Kibana 4's document table visualization:

https://gist.github.com/ppf2/f4e27207c70b99c67248
</comment><comment author="ppf2" created="2015-03-11T17:32:38Z" id="78313382">There is an increasing # of Kibana 4 users running into this (https://github.com/elastic/kibana/issues/2782)
</comment><comment author="bobrik" created="2015-03-23T11:33:02Z" id="84958602">I really hope that could be fixed soon, kibana is unusable for us because of this issue.
</comment><comment author="nik9000" created="2015-05-18T14:37:06Z" id="103081379">&gt; https://gist.github.com/ppf2/f4e27207c70b99c67248

Without a stack trace its hard to be sure this is highlighting but I suspect you tried it without and it works.

Maybe we can add an excludes expression so Kibana can do:

``` json
"*": {
  "excludes": "comments/commont.raw|_*"
}
```

or whatever it needs to do.That'd be pretty simple.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to get _ID within aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9880</link><project id="" key="" /><description>Hello,
I have the following query

```
{
"query":
    {"bool":
        {"must":
            [   
                    { "term": {"ol.EXTN_CREATE_LOC": 8119}},
                    { "term": {"ol.DOCTYPE": "0001"}},
            ]
        }
    },
    "aggs" : {
        "keys" : 
            {
                "terms" : { "field" : "ol.ORDER_LINE_KEY", "size" : 200000},
                "aggs" : 
                {
                        "max" : { "max" : { "field" : "ol.STATUS_CODE" }},
                        "ID" : { "value_count" : { "script" : "doc['_ID'].value" } }

                }
            }
        }
}
```

First it filters and gets back all the results based on my query, and then the aggregation is supposed to group all documents that have similar ORDER_LINE_KEYS and then within those ORDER_LINE_KEYS they all have another field on that document that has a code (STATUS_CODE) I want to return the MAX code which it is doing, but I also need the document ID to be returned to further index from my application.

i looked around the documentation for this and am not seeing anything that can do this, is that true?
</description><key id="58912828">9880</key><summary>Unable to get _ID within aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ap0091</reporter><labels /><created>2015-02-25T14:55:05Z</created><updated>2015-02-28T03:01:57Z</updated><resolved>2015-02-28T03:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T03:01:57Z" id="76507014">Hi @ap0091 

The `_id` (not `_ID`) isn't indexed or written as doc values, so you can't access it.  You can however use the `_uid` field (which consists of the type name plus the _id)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No validation of custom analyzer when updating index setting on a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9879</link><project id="" key="" /><description>Steps to reproduce:

``` javascript
DELETE test_idx

# create an index
PUT test_idx
{
    "settings" : {
        "number_of_shards" : 3,
        "number_of_replicas" : 2
    }
}

# Close the index to we can change non-dynamic settings
POST test_idx/_close

# Add custom analyzer that points to a non-existant tokenizer
PUT test_idx/_settings
{
        "analysis": {
            "analyzer": {
                "my_custom_analyzer_1": {
                    "tokenizer": "my_custom_tokenizer"
                }
            }
        }
}

# Open the index (no error)
POST test_idx/_open

# Get the index settings (analyzer has been added)
GET test_idx/_settings

# Try to use the analyzer in a mapping (validation error here)
PUT test_idx/_mapping/type
{
  "doc": {
    "properties": {
      "field": {
        "type": "string",
        "analyzer": "my_custom_analyzer_1"
      }
    }
  }
}
```

We should validate the analyzer when the settings are updated so we don't have invalid analyzers defined in the index settings. Interestingly we actually do validate the analyzer if you try to add it when creating the index.

``` javascript
PUT test_idx
{
    "settings" : {
        "number_of_shards" : 3,
        "number_of_replicas" : 2,
        "analysis": {
            "analyzer": {
                "my_custom_analyzer_1": {
                    "tokenizer": "my_custom_tokenizer"
                }
            }
        }
    }
}
```

Errors with: 

``` javascript
{
   "error": "IndexCreationException[[test_idx] failed to create index]; nested: IllegalArgumentException[Custom Analyzer [my_custom_analyzer_1] failed to find tokenizer under name [my_custom_tokenizer]]; ",
   "status": 400
}
```

This issue was raised in a mailing list post: https://groups.google.com/forum/#!topic/elasticsearch/roLINFQWfFc
</description><key id="58908994">9879</key><summary>No validation of custom analyzer when updating index setting on a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label></labels><created>2015-02-25T14:28:20Z</created><updated>2017-03-15T09:24:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sriram2520" created="2017-03-15T09:24:57Z" id="286686004">Encountered an exact issue today on version 5.2.2. Created a custom analyzer, but the tokenizer name was spelled incorrectly. Now I can't open the index. And seems like I can't delete the custom analyzer also.

Keep getting this error, when I try to update the settings.

Custom Analyzer [custom_minhash_analyzer] failed to find tokenizer under name [min_hash]</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException on scroll where initial query with geohash_grid aggrigations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9878</link><project id="" key="" /><description>start elasticsearch in docker

```
sudo docker run -p 9200:9200 -p 9300:9300 -v `pwd`/esdata:/data dockerfile/elasticsearch /elasticsearch/bin/elasticsearch
```

check elasticsearch version in output

script

```
#!/bin/bash
echo 'version'
curl -XGET 'http://localhost:9200' | python -mjson.tool

echo;
curl -XDELETE 'http://localhost:9200/places'
sleep 1;
curl -XPOST 'http://localhost:9200/places' -d'{
    "mappings": {
        "place": {
            "_id": {
                "path": "id"
            },
            "properties": {
                "id": {"type": "long"},
                "name": {"type": "string"},
                "alias": {"type": "string"},
                "location": {
                    "type": "geo_point",
                    "geohash": true,
                    "geohash_prefix": true,
                    "geohash_precision": 1
                },
                "popup": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "alias": {"type": "string"}
                    }
                }
            }
        }
    }
}
'
curl -XPOST 'http://localhost:9200/places/_refresh'

echo;
echo 'bulk create documents'
curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '{"index": {"_type": "place", "_id": 1455, "_index": "places"}}
{"popup": {"alias": "place0", "name": "place0"}, "alias": "place0", "location": {"lat": 41.101, "lon": 41.1}, "id": 1455, "name": "place0"}
{"index": {"_type": "place", "_id": 1456, "_index": "places"}}
{"popup": {"alias": "place1", "name": "place1"}, "alias": "place1", "location": {"lat": 41.101, "lon": 41.1}, "id": 1456, "name": "place1"}
{"index": {"_type": "place", "_id": 1457, "_index": "places"}}
{"popup": {"alias": "place2", "name": "place2"}, "alias": "place2", "location": {"lat": 41.101, "lon": 41.1}, "id": 1457, "name": "place2"}
{"index": {"_type": "place", "_id": 1458, "_index": "places"}}
{"popup": {"alias": "place3", "name": "place3"}, "alias": "place3", "location": {"lat": 41.102, "lon": 41.1}, "id": 1458, "name": "place3"}
{"index": {"_type": "place", "_id": 1459, "_index": "places"}}
{"popup": {"alias": "place4", "name": "place4"}, "alias": "place4", "location": {"lat": 41.102, "lon": 41.1}, "id": 1459, "name": "place4"}
{"index": {"_type": "place", "_id": 1460, "_index": "places"}}
{"popup": {"alias": "place5", "name": "place5"}, "alias": "place5", "location": {"lat": 41.103, "lon": 41.1}, "id": 1460, "name": "place5"}
{"index": {"_type": "place", "_id": 1461, "_index": "places"}}
{"popup": {"alias": "place6", "name": "place6"}, "alias": "place6", "location": {"lat": 41.103, "lon": 41.1}, "id": 1461, "name": "place6"}
{"index": {"_type": "place", "_id": 1462, "_index": "places"}}
{"popup": {"alias": "place7", "name": "place7"}, "alias": "place7", "location": {"lat": 41.105, "lon": 41.1}, "id": 1462, "name": "place7"}
{"index": {"_type": "place", "_id": 1463, "_index": "places"}}
{"popup": {"alias": "place8", "name": "place8"}, "alias": "place8", "location": {"lat": 41.106, "lon": 41.1}, "id": 1463, "name": "place8"}
{"index": {"_type": "place", "_id": 1464, "_index": "places"}}
{"popup": {"alias": "place9", "name": "place9"}, "alias": "place9", "location": {"lat": 41.107, "lon": 41.1}, "id": 1464, "name": "place9"}
'

RES=$(curl -XGET 'http://localhost:9200/places/place/_search?scroll=10m' -d '{
  "aggs": {
    "clusters": {
      "aggs": {
        "popup": {
          "top_hits": {
            "_source": {
              "include": [
                "location",
                "popup"
              ]
            },
            "size": 1,
            "sort": [
              {
                "_id": {
                  "order": "desc"
                }
              }
            ]
          }
        }
      },
      "geohash_grid": {
        "field": "location",
        "precision": 8
      }
    }
  },
  "filter": {
      "geo_bounding_box": {
          "location": {
            "bottom_left": {
              "lat": 41.1,
              "lon": 41.09
            },
            "top_right": {
              "lat": 41.11,
              "lon": 41.108
            }
          }
        }
   },
  "size": 2
}')

echo;
echo 'result'
echo $RES | python -mjson.tool

echo;
echo 'scroll_id'
SCROLL_ID=$(echo $RES | python -c"import sys, json; sys.stdout.write(json.loads(sys.stdin.read())['_scroll_id'])")
echo $SCROLL_ID

echo;
echo 'scroll next'
curl -XGET "http://localhost:9200/_search/scroll?scroll=10m&amp;size=10" -d"$SCROLL_ID" | python -mjson.tool
```

output

```
version
{
    "cluster_name": "elasticsearch",
    "name": "Fever Pitch",
    "status": 200,
    "tagline": "You Know, for Search",
    "version": {
        "build_hash": "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
        "build_snapshot": false,
        "build_timestamp": "2015-02-19T13:05:36Z",
        "lucene_version": "4.10.3",
        "number": "1.4.4"
    }
}

{"acknowledged":true}{"acknowledged":true}{"_shards":{"total":10,"successful":4,"failed":0}}
bulk create documents
{"took":397,"errors":false,"items":[{"index":{"_index":"places","_type":"place","_id":"1455","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1456","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1457","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1458","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1459","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1460","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1461","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1462","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1463","_version":1,"status":201}},{"index":{"_index":"places","_type":"place","_id":"1464","_version":1,"status":201}}]}
result
{
    "_scroll_id": "cXVlcnlUaGVuRmV0Y2g7NTsxNTY6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNjA6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTg6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTk6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTc6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzswOw==",
    "_shards": {
        "failed": 0,
        "successful": 5,
        "total": 5
    },
    "aggregations": {
        "clusters": {
            "buckets": [
                {
                    "doc_count": 3,
                    "key": "szm1z60n",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1457",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.101,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place2",
                                            "name": "place2"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 3
                        }
                    }
                },
                {
                    "doc_count": 2,
                    "key": "szm1z684",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1460",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.103,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place5",
                                            "name": "place5"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 2
                        }
                    }
                },
                {
                    "doc_count": 2,
                    "key": "szm1z62h",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1458",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.102,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place3",
                                            "name": "place3"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 2
                        }
                    }
                },
                {
                    "doc_count": 1,
                    "key": "szm1z721",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1464",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.107,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place9",
                                            "name": "place9"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 1
                        }
                    }
                },
                {
                    "doc_count": 1,
                    "key": "szm1z705",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1463",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.106,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place8",
                                            "name": "place8"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 1
                        }
                    }
                },
                {
                    "doc_count": 1,
                    "key": "szm1z6bj",
                    "popup": {
                        "hits": {
                            "hits": [
                                {
                                    "_id": "1462",
                                    "_index": "places",
                                    "_score": null,
                                    "_source": {
                                        "location": {
                                            "lat": 41.105,
                                            "lon": 41.1
                                        },
                                        "popup": {
                                            "alias": "place7",
                                            "name": "place7"
                                        }
                                    },
                                    "_type": "place",
                                    "sort": [
                                        null
                                    ]
                                }
                            ],
                            "max_score": null,
                            "total": 1
                        }
                    }
                }
            ]
        }
    },
    "hits": {
        "hits": [],
        "max_score": null,
        "total": 0
    },
    "timed_out": false,
    "took": 6
}

scroll_id
cXVlcnlUaGVuRmV0Y2g7NTsxNTY6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNjA6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTg6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTk6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzsxNTc6SkFXblB6OTdSZFNtZGpuQ01yMUpJZzswOw==

scroll next
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[JAWnPz97RdSmdjnCMr1JIg][places][0]: QueryPhaseExecutionException[[places][0]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[JAWnPz97RdSmdjnCMr1JIg][places][1]: QueryPhaseExecutionException[[places][1]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[JAWnPz97RdSmdjnCMr1JIg][places][2]: QueryPhaseExecutionException[[places][2]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[JAWnPz97RdSmdjnCMr1JIg][places][3]: QueryPhaseExecutionException[[places][3]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[JAWnPz97RdSmdjnCMr1JIg][places][4]: QueryPhaseExecutionException[[places][4]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",
    "status": 500
}
```

Sorry for my poor English
</description><key id="58898163">9878</key><summary>NullPointerException on scroll where initial query with geohash_grid aggrigations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">estin</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-02-25T13:04:13Z</created><updated>2015-08-16T11:04:03Z</updated><resolved>2015-08-16T11:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:54:07Z" id="76506698">Hi @colings86 - please could you take a look at this one too.
</comment><comment author="colings86" created="2015-03-02T15:54:44Z" id="76737078">Exception logged on server is listed below. This is strange as it seems that the top hits aggregator is being run on the 'scroll next' call when it should only be run on the initial scan.

```
org.elasticsearch.search.query.QueryPhaseExecutionException: [places][0]: query[ConstantScore(cache(_type:place))],from[2],size[2]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:314)
    at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:299)
    at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:1)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.TopFieldCollector$OutOfOrderOneComparatorNonScoringCollector.collect(TopFieldCollector.java:134)
    at org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator.collect(TopHitsAggregator.java:118)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucketNoCounts(BucketsAggregator.java:74)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:63)
    at org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridAggregator.collect(GeoHashGridAggregator.java:83)
    at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.searchAfter(IndexSearcher.java:243)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:136)
    ... 7 more
```
</comment><comment author="colings86" created="2015-03-02T15:55:57Z" id="76737378">The issue can be reproduced on the 1.4 branch but not on the master branch. I haven't checked any other versions.

@estin could you confirm which version of Elasticsearch you are running?
</comment><comment author="estin" created="2015-03-02T15:59:40Z" id="76738176">1.4.4

docker image - https://github.com/dockerfile/elasticsearch

curl -XGET 'http://localhost:9200

```
version
{
    "cluster_name": "elasticsearch",
    "name": "Fever Pitch",
    "status": 200,
    "tagline": "You Know, for Search",
    "version": {
        "build_hash": "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
        "build_snapshot": false,
        "build_timestamp": "2015-02-19T13:05:36Z",
        "lucene_version": "4.10.3",
        "number": "1.4.4"
    }
}
```
</comment><comment author="estin" created="2015-03-27T07:20:34Z" id="86852896">1.5.0 have this bug - NullPointerException, but error message changed compared to 1.4.4

```
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[AJziGR-zRralfguZOlUAfw][places][0]: QueryPhaseExecutionException[[places][0]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:\"id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@714de5ce&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[AJziGR-zRralfguZOlUAfw][places][1]: QueryPhaseExecutionException[[places][1]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:\"id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@7bac1fe0&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[AJziGR-zRralfguZOlUAfw][places][2]: QueryPhaseExecutionException[[places][2]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:\"id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@27e2ed3&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[AJziGR-zRralfguZOlUAfw][places][3]: QueryPhaseExecutionException[[places][3]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:\"id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@e3d27fc&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[AJziGR-zRralfguZOlUAfw][places][4]: QueryPhaseExecutionException[[places][4]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:\"id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@63735262&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",
    "status": 500
}
```

```
[2015-03-27 07:12:24,639][DEBUG][action.search.type       ] [Kang the Conqueror] [327] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [places][2]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.11, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:"id": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@27e2ed3&gt;!]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:328)
        at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:299)
        at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:296)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.lucene.search.TopFieldCollector$OutOfOrderOneComparatorNonScoringCollector.collect(TopFieldCollector.java:134)
        at org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator.collect(TopHitsAggregator.java:116)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucketNoCounts(BucketsAggregator.java:74)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:63)
        at org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridAggregator.collect(GeoHashGridAggregator.java:83)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
        at org.apache.lucene.search.IndexSearcher.searchAfter(IndexSearcher.java:424)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
        ... 7 more
```
</comment><comment author="estin" created="2015-07-02T19:56:34Z" id="118149525">on 1.6.0

```
[2015-07-02 19:54:45,895][DEBUG][action.search.type       ] [Crimson Dynamo V] [28140] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [places][3]: query[filtered(ConstantScore(+GeoBoundingBoxFilter(location, [41.1
1, 41.09], [41.1, 41.108])))-&gt;cache(_type:place)],from[2],size[2],sort[&lt;score&gt;,&lt;custom:"id": org.elasticsearch.index.fielddata.fieldcomparat
or.LongValuesComparatorSource@392631ef&gt;!]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:331)
        at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:299)
        at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:296)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
```
</comment><comment author="clintongormley" created="2015-08-16T11:03:56Z" id="131524956">Fixed in master. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add pending tasks count to cluster health</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9877</link><project id="" key="" /><description>The number of current pending tasks is useful to detect and overloaded master. This commit adds it to the cluster health API. The complete list can be retrieved from the dedicated pending tasks API.

It also adds rest tests for the cluster health variants.
</description><key id="58894104">9877</key><summary>Add pending tasks count to cluster health</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T12:31:43Z</created><updated>2015-06-07T17:34:37Z</updated><resolved>2015-02-25T13:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-25T13:44:16Z" id="75961244">left some minor comments - LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to perform computations on aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9876</link><project id="" key="" /><description>There are many instances where it is useful to perform computations on the output of aggregations to calculate new aggregations. This meta issue aims to summarize the functionality we would like to add to the aggregations framework to allow different types of computation to be performed during the reduce phase of aggregations.

This set of new aggregations are the highest priority, given their utility in a wide range of scenarios:
- [x] https://github.com/elasticsearch/elasticsearch/issues/9293 Aggregation to calculate the derivative on a histogram aggregation
- [x] https://github.com/elastic/elasticsearch/pull/10898 Derivative Aggregation x-axis units normalisation
- [x] https://github.com/elasticsearch/elasticsearch/issues/10002 Aggregation to calculate multiple types of moving averages on a histogram aggregation 
- [x] https://github.com/elasticsearch/elasticsearch/issues/10000 Aggregation to calculate the bucket which has the maximum value in a given aggregation
- [x] https://github.com/elasticsearch/elasticsearch/issues/9999 Aggregation to calculate the bucket which has the minimum value in a given aggregation

At the moment, the remainder of the list is largely explorative, to see which ideas/functionality makes sense and have community interest.  Feel free to suggest your own ideas/aggregations/algos!
- [ ] Aggregation that uses scripts to perform arbitrary computations on aggregations
- [x] https://github.com/elastic/elasticsearch/pull/11196 Aggregation to compute differences on a single series (e.g. first difference = Y&lt;sub&gt;t&lt;/sub&gt; - Y&lt;sub&gt;t-1&lt;/sub&gt;)
- [ ] https://github.com/elastic/elasticsearch/pull/10377 Aggs for autocorrelation, acf graphs, correlograms
- [x] https://github.com/elastic/elasticsearch/issues/11006 Aggregation to calculate the (mean) average value of the buckets in a given aggregation
- [x] https://github.com/elastic/elasticsearch/issues/11007 Aggregation to calculate the sum of the values of the buckets in a given aggregation
- [x] https://github.com/elastic/elasticsearch/pull/13128 Aggregation to calculate `stats` and `extended_stats` values of the buckets in a given aggregation
- ~~https://github.com/elastic/elasticsearch/issues/11008 Aggregation to calculate the number of buckets in a given aggregation~~
- ~~https://github.com/elastic/elasticsearch/issues/11009 Aggregation to calculate the cardinality of a metric in a given aggregation~~
- [x] https://github.com/elastic/elasticsearch/issues/11029 Aggregation to allow users to perform simple arithmetic operations on histogram aggregations
- [x] https://github.com/elastic/elasticsearch/pull/11825 Agg to calculate cumulative sum of a metric
- [x] https://github.com/elastic/elasticsearch/pull/11941 Agg to filter buckets based on a script
- [x] https://github.com/elastic/elasticsearch/pull/13186 Agg to calculate percentiles
- [ ] Agg detect changes in mean (cumulative-sum control chart, Kolmogorov-Smirnov)
- [ ] Agg detect periodicity, seasonality
- [x] https://github.com/elastic/elasticsearch/pull/11196 Agg to subtract known seasonality (serial differencing)
- [ ] Agg for regression
- [ ] Agg for Savitzky-Golay Filters
- [ ] Aggs for high-pass, low-pass, band-pass filters
- [ ] Agg for generic FFT and inverse FFT
- [ ] https://github.com/elastic/elasticsearch/issues/14928 Agg for selecting the `nth` bucket, and/or selecting a range + truncating
- [ ] Agg for building a sliding_histogram
</description><key id="58892321">9876</key><summary>Add ability to perform computations on aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>Meta</label></labels><created>2015-02-25T12:15:11Z</created><updated>2016-11-29T08:06:54Z</updated><resolved>2016-11-29T08:06:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jhenley45" created="2015-02-26T16:40:39Z" id="76212711">+1
</comment><comment author="javadevmtl" created="2015-02-27T19:01:08Z" id="76451447">+1
</comment><comment author="TheDeveloper" created="2015-02-28T23:26:10Z" id="76556740">+1

I want to have a histogram aggregation that can use the `doc_count` result field from a parent Terms aggregation as its field.
</comment><comment author="tcucchietti" created="2015-03-05T19:34:31Z" id="77434907">Big +1 on this!
</comment><comment author="rtremaine" created="2015-03-05T20:08:11Z" id="77441269">+1
</comment><comment author="aschokking" created="2015-03-18T21:51:08Z" id="83197524">:+1: 
</comment><comment author="aschokking" created="2015-03-18T21:55:47Z" id="83198569">Is it possible to do these awkwardly now using scripted aggregations? Is that something Kibana4 can take advantage of if they are there?
</comment><comment author="polyfractal" created="2015-03-18T22:13:04Z" id="83204703">@aschokking Nope, there's no way to hack this right now...if you want this functionality, you currently have to build it client-side yourself.

This new functionality essentially adds one or more extra `reduce` phases to the aggregation framework.  For example, currently you can get the average price per day for the last 30 days (`date_histogram` bucket with a `avg` metric).  But you can't get the sum of those averages, since the summation is operating on the agg results and not doc values.  You would have to do that client-side right now by summing up the buckets yourself.

From a high level, it looks like this:

1) `map` executes on each doc to collect value
2) `combine` all the prices together by averaging all the collected values.  This happens on each shard.
3) Send all the shard results to the coordinating node, `reduce` the shard values together by merging averages

The new functionality introduces a fourth step:

4) Execute another `reduce` phase, this time iterating over the aggregation buckets and summing the averages.

We are keeping close communication with the Kibana team, since they want to use a lot of this functionality.  And none of this will "break" existing aggregations; in fact, all the new aggs look just like the old aggs.  So Kibana will be able to implement them as they arrive in Elasticsearch, no need for a new major version or anything.
</comment><comment author="aschokking" created="2015-03-19T02:03:24Z" id="83264948">Thanks for clarifying @polyfractal, that makes sense. 
</comment><comment author="lukas-vlcek" created="2015-03-25T08:02:36Z" id="85908543">very nice!
</comment><comment author="lewchuk" created="2015-04-08T17:45:06Z" id="90985801">+1 on adding the secondard reduces, will it be limited to a two level aggregation or can more levels be possible?

I'd suggest a modification to "Aggregation to calculate the (mean) average value of the buckets in a given aggregation" to be "Aggregation to calculate the any/all of the extended_stats values of the buckets in a given aggregation, e.g. after a terms aggregation". This allows each bucket to be given an equal weight regardless of the number of documents in the underlying buckets.
</comment><comment author="polyfractal" created="2015-04-08T18:14:04Z" id="90992775">@lewchuk The new functionality should be able to work in multi-level aggregations.  E.g. you can embed these new aggs at multiple levels in the aggregation tree.  

Depending on the agg, they may have certain requirements which must be satisfied (e.g. a `derivative` must be embedded inside a histogram or date_histo, since it expects numerical series of numerical data); you'll receive a validation error if you put it in the wrong place.

Most of these new aggs also support "chaining".  For example, you could calculate acceleration by taking the derivative of a derivative of position.  Or do something like take the moving average of the derivative of the position.  Etc etc :)

&gt; I'd suggest a modification to "Aggregation to calculate the (mean) average value of the buckets in a given aggregation" to be "Aggregation to calculate the any/all of the extended_stats values of the buckets in a given aggregation, e.g. after a terms aggregation".

I believe the plan is to support all the basic "arithmetic" functions, not just mean.  So mean/min/max/sum/etc.  Basically mirroring the existing set of metrics...but for agg values instead of document values.
</comment><comment author="lewchuk" created="2015-04-09T01:42:48Z" id="91087436">@polyfractal Thanks for the clarification! Will be very excited to unleash the power of these new aggregations.
</comment><comment author="Kallin" created="2015-04-22T14:41:13Z" id="95213949">periodicity/seasonality stuff sounds interesting. we would like to do detection of customer attrition, many of whom have seasonal behaviour based on the vertical of their industry. this feature sounds like it could help eliminate false positives.
</comment><comment author="goupeng212" created="2015-05-11T03:14:23Z" id="100752635">+1
</comment><comment author="acarstoiu" created="2015-06-18T20:13:32Z" id="113276732">I've had a close look at the documentation of the upcoming [pipeline aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline.html). Quite an exciting stuff :smiley:

Yet there's a very important, I'd say capital functionality missing. The primary reason for using the server-side post-aggregations is _not_ laziness (at least not in my case), but performance: it might be killing for your application to receive tons of data on the wire and then crunch them a while to finally spit just a few numbers.

All pipeline aggregations should have a `supress_source` parameter (or something alike) that would instruct the coordinating node to prune the buckets used as source data from the returned result. Certainly, an aggregation might be supressed by several pipeline aggregations, but one would suffice to have its buckets removed from the reply.
</comment><comment author="acarstoiu" created="2015-06-18T20:59:10Z" id="113288211">In the meantime I found the `filter_path` parameter which seems a good work-around (although I expect it to be less performant).
</comment><comment author="roytmana" created="2015-07-08T18:20:10Z" id="119686768">Read your blog on pipeline aggregations (https://www.elastic.co/blog/out-of-this-world-aggregations) Really nice thank you

I would be interested in few more pipeline aggregations (or rather transformations)
1. To provide compatibility with huge number of Pivot Table/Chart visualization packages who expect flat dataset and can "pivot" on its columns. Flatten aggregation tree - it would flatten tree of nested aggregations to a list of records capturing keys recursively from top down in the records as well as all metrics. 
2. Overlay transformation that can overlay (with various strategies) aggregation buckets from sibling aggregations. For example if we want to bring _missing aggregation into terms aggregation buckets (I know that 2.0 will support it directly but it is for illustration purposes) or overlaying results produced by aggregations on different fields - for example in case management system they want to count number of cases being opened and closed per month so elastic could collate aggs by two separate fields together based on the bucket key (fiscal year). Even more useful case overlaying with calculation (or script)
3. Arithmetic/expressions to produce derivative metrics based on agg metrics within each bucket. (#2 is probably more generic and more complex case of overlay with calculation) 
</comment><comment author="clintongormley" created="2015-07-09T15:43:05Z" id="120041239">@roytmana I like the idea of (1) flattening aggs into columns

(2) and (3) sound like they could be achieved very easily with https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline-bucket-script-aggregation.html
</comment><comment author="roytmana" created="2015-07-09T15:48:13Z" id="120044170">ok @clintongormley  I will play with script pipeline when the beta is out. If you decide to go ahead with (1) I would be happy to provide some use cases.  
</comment><comment author="clintongormley" created="2015-07-10T16:17:28Z" id="120449382">@roytmana just chatted to @colings86 and apparently (2) isn't supported by the bucket_script agg yet.  But we should definitely add support
</comment><comment author="clintongormley" created="2015-07-10T16:27:03Z" id="120451560">Hmmmm  actually rereading (2) I'm not entirely sure if I understood it correctly.  The examples you provide are quite different, eg:
- moving _missing into the list of buckets.. this seems like a fairly arbitrary transform which would require access to the whole agg tree and could potentially result in weird output
- however the "open" vs "closed" example sounds like those should be two metrics in each bucket of a date histo, then you could use bucket_script to (eg) add a third metric calculated from `open - closed`

Am I missing something?

The bit that I said was unsupported by bucket_script was the ability to access two separate histograms 
</comment><comment author="roytmana" created="2015-07-10T17:06:16Z" id="120461446">let me try to elaborate a bit
moving _missing: currently (and I know it'll be in 2.0) terms agg does not support missing bucket so one way to solve it was to declare a sibling "missing" aggregation (with the same sub aggs as in the terms) next to my say terms aggregation and then move its result into terms agg bucket array. This is just one example of the overlaying. 
Here is another one: imagine that you want to selectively drill down sub aggs (that is not to bring subaggs data for every bucket of a parent agg). Example we aggregate by country and city and we want to show breakdown by country and within countries we want to show breakdown by city but only for Germany and France. So 
One way to do it is to have two sibling aggs one on Country only and the other on Country and City with "include" restricting  countries to Germany and France. then overlay second (two level) agg over the first and you have selective subaggregation. It is very useful in UI where user can freely drill down different path of nested aggregations (and they do not wish to drill down into every bucket) and then could change some global filer or search criteria and I need to reload entire visible tree from new query

As for open/closed. I do not think it could be two metrics in one bucket they are two different fields to bucket on. Here is requirement: I want to calculate number of cases and cost of cases opened and closed in each fiscal year and show them side by side. I have two fields OpenFY and ClosedFY which are pre-calculated. I want to show a chart with two data series one for opened and one for closed (counts and cost). Open an closed are two independent fields (It is even possible that there could be a year when there was no closed at all so there will not be a bucket for this FY in closed)

I want to agg on the first and on the second and then merge results by FY so each bucket will get open and closed metrics together. I do it currently in post processing but I think result tree manipulation support directly in ES would be really useful!

One more question I have is about nested and reverse_nested (same for parent) aggregations. They introduce extra level in result tree which I am not sure is necessary. It only changes calculation scope but should not alter result tree depth. It makes it rather a headache to deal with it in dynamic metadata driven systems where users do not care how data is laid out they just pick how to aggregate and what to calculate and I may have to cross nested back and forth to accommodate it. Right now in post processing I have to transform my results by removing these extra nodes created due to nested/reverse_nested (a royal headache in entirely dynamic system) before passing it to UI level. I was wondering if it would introduce any problem (name clash?) if nested/reverse_nested did not introduce a separate node and all its subaggs emitted their results into agg owning the nested one.  
</comment><comment author="roytmana" created="2015-07-10T17:20:50Z" id="120464413">I want to add that nested/reverse_nested introducing extra levels in result tree is not a trivial matter.
Consider this example. A Case has Customers and Teams (of employees) who work on the case. I want to see 3 level breakdown of cases by customer by team and by employee. While logically my result tree should have 3 levels actual result tree with all the nesting/un-nesting is good deal more complex. I would greatly appreciate if you give it some thought and see if it an option could be added to skip extra nodes in result tree for calculation scope changing aggs
</comment><comment author="tmandry" created="2015-08-07T16:26:10Z" id="128755725">Lag or Timeshift Aggregation: Sort of a generalization of the serial differencing agg which only provides the lag functionality, allowing you to perform operations on values in different buckets (from the same or bucket aggregations.)

Use case: Cohort retention analysis, where I want to see what percentage of users come back the day after their first day. I could do this by bucketing by day and by filtering on both `first_seen_days_ago:0` and `first_seen_days_ago:1`, using the lag aggregation to line up the second filter with the first, and finally dividing values from the same cohort.
</comment><comment author="polyfractal" created="2015-08-07T16:53:44Z" id="128763711">@tmandry hmm, I can see this being useful.  Would you need/want a newly created field to be appended to each bucket, like:

``` js
"buckets": [
    {
        "key_as_string": "2014-07-29T17:00:00.000Z",
        "key": 1406653200000,
        "doc_count": 7,
        "login_today": {  // &lt;-- original, derived from something like an `avg` metric
            "avg": 1
        },
        "login_yesterday": {   // &lt;-- derived and shifted via a `timeshift` agg
            "avg": 1
        }
    },
```

Or would it be sufficient if the `serial_diff` agg allowed arbitrary scripting, so that you could perform any mathematical operation other than just subtraction?

Thinking about it, the advantage of actually appending a new bucket is that you can use something like `bucket_selector` or `bucket_script` to filter / munge the agg, whereas the arbitrary scripting might be a bit more limiting.
</comment><comment author="tmandry" created="2015-08-07T17:57:21Z" id="128778699">@polyfractal For my use case, the `serial_diff` approach would work, but appending a new bucket would allow us to enrich the interface with raw user counts in addition to percentages. (At least, I think appending would be necessary.)
</comment><comment author="polyfractal" created="2015-09-25T16:35:59Z" id="143270932">Been working with pipelines more extensively on a demo project.  A few observations about what is difficult:
- A "sliding histogram" would be _very_ useful.  There are situations where you need to accumulate the results from a range t&lt;sub&gt;i&lt;/sub&gt;..t&lt;sub&gt;i+n&lt;/sub&gt; into a single value...then repeat the process for the next t&lt;sub&gt;i+1&lt;/sub&gt;..t&lt;sub&gt;i+n+1&lt;/sub&gt; time range.  After doing that, you want to treat the output of each time range as a point in a new series and perform metrics on that.
  
  Currently, the only way to do this is execute a search-per-range and index the results, then run a followup agg.  The main downside to this functionality is that it could produce a very large number of buckets.  But I think the usefulness outweighs the downside
- An ability to pick out individual buckets from a series.  E.g. a `first`, `last`, `nth` metric.  For example, you could have a `date_histo` embedded in a `terms`, giving one time series per term.  Then you want to calculate a moving avg and some other stuff for each series, and just want the "final" value from each series, so you could determine the largest "final" value.  Currently there is no way to do that.
  
  Alternatively, pathing could be modified to allow `last` etc as special keywords, so you could do `termsAgg&gt;dateAgg[last].value`.  Would tie in nicely with the ability to ask for specific terms too (`termsAgg['foo']&gt;dateAgg.....`)
- Terms aggs tend to be an impenetrable wall.  It is difficult to access values on either "side" of the terms agg since it is a dynamic multi-value bucket.  And double term aggs basically prevent all access entirely
</comment><comment author="clintongormley" created="2015-09-27T09:46:38Z" id="143533500">&gt; An ability to pick out individual buckets from a series. E.g. a first, last, nth metric. 

could be: `dateAgg[-1].value` for last
</comment><comment author="arivazhagan-jeganathan" created="2015-12-24T12:26:52Z" id="167104275">Query String with Aggregation parameters works fine with JEST client. but with TCP, is it always mandatory to build AggregationBuilder to execute aggregation? Why JSON aggregation query is not supported in TCP? any specific reason for this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>threadpool: Expose the ability the add/remove custom executors to the thread pool infrastructure.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9875</link><project id="" key="" /><description>Useful to expose info and stats of custom threadpools via the stats apis.
</description><key id="58890440">9875</key><summary>threadpool: Expose the ability the add/remove custom executors to the thread pool infrastructure.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label></labels><created>2015-02-25T11:57:00Z</created><updated>2015-05-18T23:26:30Z</updated><resolved>2015-02-27T12:10:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-25T14:26:23Z" id="75967929">left two comments - looks good in general ...
</comment><comment author="martijnvg" created="2015-02-27T12:10:23Z" id="76385486">Closing this issue. It is usually enough to maintain a running flag in the Runnable and to clear the threadpool exeutor's queue. In that case the extra infrastructure in the PR to ThreadPool isn't necessary and only adds complexity.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update dynamic fields in mapping on master even if parsing fails for the rest of the doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9874</link><project id="" key="" /><description>...ils for the rest of doc

The local DocumentMapper is updated while parsing and dynamic fields are added before
parsing has finished. If parsing fails after a dynamic field has been added already
then the field was not added to the cluster state but was present in the local mapper of this
node. New documents with the same field would not necessarily cause an update either and
after restarting the node the mapping for these fields were lost. Instead the new fields
should always be updated.

closes #9851
</description><key id="58886953">9874</key><summary>Update dynamic fields in mapping on master even if parsing fails for the rest of the doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T11:23:33Z</created><updated>2015-05-29T16:31:13Z</updated><resolved>2015-03-26T20:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-05T15:34:39Z" id="77385081">LGTM
</comment><comment author="jpountz" created="2015-03-05T15:36:27Z" id="77385439">@brwe Since this change looks quite significant, I don't think it should be pushed to 1.4.5.
</comment><comment author="brwe" created="2015-03-06T18:27:53Z" id="77609506">That was not a good idea - the change of MapperParsingException seemes to have screwed up bwc for Exception serialization, see for example http://build-us-00.elasticsearch.org/job/es_bwc_1x/8385/
I reverted the commit now.
</comment><comment author="brwe" created="2015-03-17T22:02:22Z" id="82616467">Ok, new plan. Instead of changing the mapper parsing exception we actually can just get the context from the document mapper and ask if the mapping was changed - much easier and I cannot see any downside to it. @jpountz would you mind taking another look?
</comment><comment author="jpountz" created="2015-03-24T15:20:04Z" id="85548991">I'm not too happy with getting back to the cache to check whether mappings have been modified, I liked the previous solution better. But on ther other hand, I don't have a better idea. Can we maybe just use this approach on 1.x and keep the previous approach on master?
</comment><comment author="brwe" created="2015-03-26T14:26:23Z" id="86539942">Ok, I'll do that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] added a note for the default shard_size value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9873</link><project id="" key="" /><description /><key id="58884604">9873</key><summary>[DOCS] added a note for the default shard_size value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T11:01:17Z</created><updated>2015-04-20T14:28:39Z</updated><resolved>2015-02-25T11:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-02-25T11:03:07Z" id="75942160">LGTM thanks a lot @colings86 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kibana 4 with elastic search 1.4.4 gives connect econnrefused</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9872</link><project id="" key="" /><description>Hi,

I'm trying out kibana 4.0.0 and have downloaded it and elasticsearch 1.4.4, but get an error when kibana tries to connect to elasticsearch.

My setup:
OS: Windows 7

I have posted the same question here: https://groups.google.com/forum/?nomobile=true#!topic/elasticsearch/TsOQ6e_fGx4

The only changes I have done is in kibana.yml where I have set host: "0:0:0:0:0:0:0:0", as I saw that elasticsearch is listening on an ipv6 address and so changed it in kibana to do the same. Otherwise no changes in any of the config files. 
Both services start up nicely, and http://localhost:9200 gives this output:
{
  "status" : 200,
  "name" : "Argus",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.4",
    "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
    "build_timestamp" : "2015-02-19T13:05:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
}

Elastic-search console window on startup:
[2015-02-25 10:58:42,339][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-02-25 10:58:42,408][INFO ][node                     ] [Argus] version[1.4.4], pid[8640], build[c88f77f/2015-02-19T13:05:36Z]
[2015-02-25 10:58:42,408][INFO ][node                     ] [Argus] initializing ...
[2015-02-25 10:58:42,412][INFO ][plugins                  ] [Argus] loaded [], sites []
[2015-02-25 10:58:45,563][INFO ][node                     ] [Argus] initialized
[2015-02-25 10:58:45,563][INFO ][node                     ] [Argus] starting ...
[2015-02-25 10:58:45,899][INFO ][transport                ] [Argus] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/139.113.102.160:9300]}
[2015-02-25 10:58:46,455][INFO ][discovery                ] [Argus] elasticsearch/LYg7WrfBSOeONOwbovB0IQ
[2015-02-25 10:58:50,227][INFO ][cluster.service          ] [Argus] new_master [Argus][LYg7WrfBSOeONOwbovB0IQ][NOLD0048][inet[/139.113.102.160:9300]], reason: zen-disco-join (elected_as_master
[2015-02-25 10:58:50,266][INFO ][gateway                  ] [Argus] recovered [0] indices into cluster_state
[2015-02-25 10:58:50,464][INFO ][http                     ] [Argus] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/139.113.102.160:9200]}
[2015-02-25 10:58:50,465][INFO ][node                     ] [Argus] started

Kibana console window on startup:
{"@timestamp":"2015-02-25T09:58:56.064Z","level":"info","message":"Listening on :::5601","node_env":"\"production\""}

But when I go to http://localhost:5601, it tries to load the webpage but crashes with an error message:
 Fatal Error
Kibana: Unable to connect to Elasticsearch
Error: unknown error
    at respond (http://localhost:5601/index.js?_b=5888:81675:15)
    at checkRespForFailure (http://localhost:5601/index.js?_b=5888:81641:7)
    at http://localhost:5601/index.js?_b=5888:80304:7
    at wrappedErrback (http://localhost:5601/index.js?_b=5888:20897:78)
    at wrappedErrback (http://localhost:5601/index.js?_b=5888:20897:78)
    at wrappedErrback (http://localhost:5601/index.js?_b=5888:20897:78)
    at http://localhost:5601/index.js?_b=5888:21030:76
    at Scope.$eval (http://localhost:5601/index.js?_b=5888:22017:28)
    at Scope.$digest (http://localhost:5601/index.js?_b=5888:21829:31)
    at Scope.$apply (http://localhost:5601/index.js?_b=5888:22121:24)

Kibana console spits out this message (Note the connect econnrefused-error message):
{"@timestamp":"2015-02-25T09:59:03.589Z","level":"info","message":"GET / 304 - 6ms","node_env":"\"production\"","request":{"method":"GET","url":"/","headers":{"host":"localhost:5601","connection":"kee
{"@timestamp":"2015-02-25T09:59:03.700Z","level":"info","message":"GET /styles/main.css?_b=5888 304 - 4ms","node_env":"\"production\"","request":{"method":"GET","url":"/styles/main.css?_b=5888","heade
{"@timestamp":"2015-02-25T09:59:03.701Z","level":"info","message":"GET /images/initial_load.gif 304 - 5ms","node_env":"\"production\"","request":{"method":"GET","url":"/images/initial_load.gif","heade
{"@timestamp":"2015-02-25T09:59:03.702Z","level":"info","message":"GET /bower_components/requirejs/require.js?_b=5888 304 - 5ms","node_env":"\"production\"","request":{"method":"GET","url":"/bower_com
{"@timestamp":"2015-02-25T09:59:03.703Z","level":"info","message":"GET /require.config.js?_b=5888 304 - 5ms","node_env":"\"production\"","request":{"method":"GET","url":"/require.config.js?_b=5888","h
{"@timestamp":"2015-02-25T09:59:03.797Z","level":"info","message":"GET /index.js?_b=5888 304 - 1ms","node_env":"\"production\"","request":{"method":"GET","url":"/index.js?_b=5888","headers":{"host":"l
{"@timestamp":"2015-02-25T09:59:03.801Z","level":"info","message":"GET /styles/theme/elk.ico 304 - 1ms","node_env":"\"production\"","request":{"method":"GET","url":"/styles/theme/elk.ico","headers":{"
{"@timestamp":"2015-02-25T09:59:04.200Z","level":"info","message":"GET /config?_b=5888 304 - 3ms","node_env":"\"production\"","request":{"method":"GET","url":"/config?_b=5888","headers":{"host":"local
{"@timestamp":"2015-02-25T09:59:04.479Z","level":"info","message":"GET /images/no_border.png 304 - 2ms","node_env":"\"production\"","request":{"method":"GET","url":"/images/no_border.png","headers":{"
{"@timestamp":"2015-02-25T09:59:05.491Z","level":"error","message":"connect ECONNREFUSED","node_env":"\"production\"","error":{"message":"connect ECONNREFUSED","name":"Error","stack":"Error: connect E
{"@timestamp":"2015-02-25T09:59:05.493Z","level":"info","message":"GET / 502 - 1007ms","node_env":"\"production\"","request":{"method":"GET","url":"/elasticsearch/","headers":{"host":"localhost:5601",
{"@timestamp":"2015-02-25T09:59:05.510Z","level":"info","message":"GET /bower_components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0 304 - 5ms","node_env":"\"production\"","request":{"method":

When I close the kibana.bat - window, I get this from the elasticsearch.bat-window:
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

So from the error message it looks like kibana has been able to contact elastic search, but why it fails beats me. Googling for "connect econnrefused" gives me that the service being looked for is not there, in my case it will mean that kibana can't find elastic search. But then why the error message in elastic search console window?

I'm out of ideas. Any help on this matter would be much appreciated!
</description><key id="58879320">9872</key><summary>Kibana 4 with elastic search 1.4.4 gives connect econnrefused</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ranasingh</reporter><labels /><created>2015-02-25T10:15:44Z</created><updated>2015-02-28T04:36:32Z</updated><resolved>2015-02-28T04:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:52:18Z" id="76506605">@rashidkpc is this something you've seen before?

You may have more luck opening this ticket on the kibana issues list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Expose JTS SelfTouchingRingFormingHoleValid in GeoShape mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9871</link><project id="" key="" /><description>Indexing a polygon with a self touching condition raises an exception. 

For an example, look at this valid geojson (take from natural earth) : https://gist.github.com/clement-tourriere/d544420ebb0490a194d0

It would be great to expose, in geo_shape mapping, the possibility to make this type of polygon valid (set JTS isSelfTouchingRingFormingHoleValid to true).
</description><key id="58873973">9871</key><summary>[GEO] Expose JTS SelfTouchingRingFormingHoleValid in GeoShape mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">clement-tourriere</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2015-02-25T09:31:55Z</created><updated>2015-04-14T13:22:46Z</updated><resolved>2015-04-14T13:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-03-25T17:58:12Z" id="86149486">Similar to #9511 

tldr; The proposed behavior violates the OGC SFA specification - which ES 1.4.3+ is now compliant. Per the spec, self touching polys should be converted to `multipolygon` or multiple `polygon` objects.  

In many areas JTS functionality is overridden for performance and correctness (e.g., winding order) so allowing this behavior not only violates the OGC spec, but is not as trivial as enabling the JTS flag.
</comment><comment author="nknize" created="2015-04-14T13:22:46Z" id="92828497">Resolved in commit 0ed8d47f37b22aa4a2c03ef4c2d8c227d319f332
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_index` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9870</link><project id="" key="" /><description>see #8143
</description><key id="58861938">9870</key><summary>Lock down `_index` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T07:37:57Z</created><updated>2015-06-08T09:03:40Z</updated><resolved>2015-02-25T20:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-02-25T08:20:02Z" id="75921951">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_type` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9869</link><project id="" key="" /><description>see #8143
</description><key id="58856896">9869</key><summary>Lock down `_type` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T06:30:36Z</created><updated>2015-06-08T09:04:15Z</updated><resolved>2015-02-25T06:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-25T06:35:36Z" id="75912840">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce tests.policy a bit more</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9868</link><project id="" key="" /><description>Remove some more wildcard permissions and replace with just what we need.
</description><key id="58855894">9868</key><summary>Reduce tests.policy a bit more</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T06:18:55Z</created><updated>2015-02-25T16:29:02Z</updated><resolved>2015-02-25T16:29:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-25T16:06:59Z" id="75988068">+1 in general - what is Percolation doing here?
</comment><comment author="rmuir" created="2015-02-25T16:16:34Z" id="75990084">I didnt dig into any of these issues. Its important to me to remove the wildcard first, so no more WTFs appear in the meantime unnoticed, and to separate out any long-tail investigations. I planned to open issues for both WTFs.
</comment><comment author="s1monw" created="2015-02-25T16:17:31Z" id="75990298">awesome +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting the following exception after changing the index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9867</link><project id="" key="" /><description>Hi,

I have changed the index name from "sample1" to "sample2", but after making that I am getting the exception while indexing a record in elastic-search:

org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [FORBIDDEN/4/index closed];
    at org.elasticsearch.cluster.block.ClusterBlocks.indexBlockedException(ClusterBlocks.java:158)
    at org.elasticsearch.action.index.TransportIndexAction.checkRequestBlock(TransportIndexAction.java:174)
    at org.elasticsearch.action.index.TransportIndexAction.checkRequestBlock(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:353)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:330)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:101)
    at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:134)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:116)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:203)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:185)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:212)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:109)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)

How should I go about it?
</description><key id="58854482">9867</key><summary>Getting the following exception after changing the index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prachicsa</reporter><labels /><created>2015-02-25T05:56:33Z</created><updated>2015-02-25T06:16:13Z</updated><resolved>2015-02-25T06:16:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-02-25T06:06:48Z" id="75910557">It means your index is closed - http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-open-close.html#indices-open-close

So you have to open it.
</comment><comment author="dadoonet" created="2015-02-25T06:16:13Z" id="75911253">You asked the same question on the mailing list which is the best place for questions.
https://groups.google.com/d/msgid/elasticsearch/6f5179db-9c50-4ac8-ae8e-3083b53855f8%40googlegroups.com?utm_medium=email&amp;utm_source=footer

Let's follow up the discussion there.

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bool query with must query is matching all documents when the query doesn't match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9866</link><project id="" key="" /><description>We're seeing an issue where if a query, such as a `simple_query_string` used in `must` clause (or multiple queries in `should` clauses) of a `bool` query, and the analyzed query string results in an empty query string (such as stop words being filtered out), then the it behaves like the entire query is ignored and all documents are matched. Even if the `simple_query_string` query is inside a `filtered` query, then the filters are ignored.

This is causing major issues for us in a multi-tenant environment as we're getting results not belonging to the correct tenant.

This issue started after we recently upgraded from 1.2.1 to 1.4.3.

This is issue appears to have been introduced in 1.3.3 (from testing various versions) and also affects 1.4.4.

I've simplified our queries below with only query in the `must` clause. Real world, we're searching against multiple types.

Analysis:

``` json
curl -XPUT 'localhost:9200/my_index' -d '
{
  "index": {
    "analysis": {
      "analyzer": {
        "html_strip": {
          "filter": [
            "standard",
            "lowercase",
            "stop",
            "asciifolding",
            "minimal_stemmer"
          ],
          "char_filter": [
            "html_strip"
          ],
          "tokenizer": "standard",
          "type": "custom"
        },
        "default_search": {
          "filter": [
            "standard",
            "lowercase",
            "stop",
            "asciifolding",
            "minimal_stemmer"
          ],
          "tokenizer": "standard",
          "type": "custom"
        },
        "default_index": {
          "filter": [
            "standard",
            "lowercase",
            "stop",
            "asciifolding",
            "minimal_stemmer"
          ],
          "tokenizer": "standard",
          "type": "custom"
        }
      },
      "filter": {
        "minimal_stemmer": {
          "type": "stemmer",
          "name": "minimal_english"
        }
      }
    }
  }
}
'
```

Create `page` type:

``` json
curl -XPUT 'localhost:9200/my_index/page/_mapping' -d '
{
  "page" : {
    "properties" : {
      "title" : {
        "type" : "string"
      },
      "content" : {
        "type" : "string",
        "analyzer": "html_strip"
      },
      "is_visible" : {
        "type" : "boolean"
      },
      "store_id": {
        "index": "not_analyzed",
        "type": "integer"
      }
    }
  }
}
'
```

Create two documents with different `store_id`'s:

``` json
curl -XPOST 'localhost:9200/my_index/page/store-1234-id-1' -d '
{
  "title": "Page for store 1234",
  "content": "The content for this page",
  "is_visible": true,
  "store_id": 1234
}
'

curl -XPOST 'localhost:9200/my_index/page/store-5678-id-1' -d '
{
  "title": "Page for store 5678",
  "content": "The content for another page",
  "is_visible": true,
  "store_id": 5678
}
'
```

Execute a search containing just a stop word `the`:

``` json
curl -XPOST 'localhost:9200/my_index/page/_search' -d '
{
   "explain": true,
    "query": {
        "bool": {
            "must": [
                {
                    "filtered": {
                        "query": {
                            "simple_query_string": {
                                "query": "the",
                                "fields": [
                                    "title",
                                    "content"
                                ]
                            }
                        },
                        "filter": {
                            "and": [
                                {
                                    "term": {
                                        "is_visible": true
                                    }
                                },
                                {
                                    "term": {
                                        "store_id": 1234
                                    }
                                }
                            ]
                        }
                    }
                }
            ]
        }
    }
}'
```

Result:

``` json
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 1,
    "hits": [
      {
        "_shard": 1,
        "_node": "HGgHSCGZTwqLOWwDwSEo6w",
        "_index": "my_index",
        "_type": "page",
        "_id": "store-1234-id-1",
        "_score": 1,
        "_source": {
          "title": "Page for store 1234",
          "content": "The content for this page",
          "is_visible": true,
          "store_id": 1234
        },
        "_explanation": {
          "value": 1,
          "description": "ConstantScore(cache(_type:page)), product of:",
          "details": [
            {
              "value": 1,
              "description": "boost"
            },
            {
              "value": 1,
              "description": "queryNorm"
            }
          ]
        }
      },
      {
        "_shard": 4,
        "_node": "HGgHSCGZTwqLOWwDwSEo6w",
        "_index": "my_index",
        "_type": "page",
        "_id": "store-5678-id-1",
        "_score": 1,
        "_source": {
          "title": "Page for store 5678",
          "content": "The content for another page",
          "is_visible": true,
          "store_id": 5678
        },
        "_explanation": {
          "value": 1,
          "description": "ConstantScore(cache(_type:page)), product of:",
          "details": [
            {
              "value": 1,
              "description": "boost"
            },
            {
              "value": 1,
              "description": "queryNorm"
            }
          ]
        }
      }
    ]
  }
}
```

As you can see both documents are returned.

When the query contains a different search term that _does_ exist in a document (eg `content`):

``` json
curl -XPOST 'localhost:9200/my_index/page/_search' -d '{
    "explain": true,
    "query": {
        "bool": {
            "must": [
                {
                    "filtered": {
                        "query": {
                            "simple_query_string": {
                                "query": "content",
                                "fields": [
                                    "name",
                                    "content"
                                ]
                            }
                        },
                        "filter": {
                            "and": [
                                {
                                    "term": {
                                        "is_visible": true
                                    }
                                },
                                {
                                    "term": {
                                        "store_id": 1234
                                    }
                                }
                            ]
                        }
                    }
                }
            ]
        }
    }
}
'
```

Result:

``` json
{
  "took": 4,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.05626005,
    "hits": [
      {
        "_shard": 1,
        "_node": "HGgHSCGZTwqLOWwDwSEo6w",
        "_index": "my_index",
        "_type": "page",
        "_id": "store-1234-id-1",
        "_score": 0.05626005,
        "_source": {
          "title": "Page for store 1234",
          "content": "The content for this page",
          "is_visible": true,
          "store_id": 1234
        },
        "_explanation": {
          "value": 0.056260053,
          "description": "sum of:",
          "details": [
            {
              "value": 0.056260053,
              "description": "weight(content:content in 0) [PerFieldSimilarity], result of:",
              "details": [
                {
                  "value": 0.056260053,
                  "description": "score(doc=0,freq=1.0), product of:",
                  "details": [
                    {
                      "value": 0.29335263,
                      "description": "queryWeight, product of:",
                      "details": [
                        {
                          "value": 0.30685282,
                          "description": "idf(docFreq=1, maxDocs=1)"
                        },
                        {
                          "value": 0.9560043,
                          "description": "queryNorm"
                        }
                      ]
                    },
                    {
                      "value": 0.19178301,
                      "description": "fieldWeight in 0, product of:",
                      "details": [
                        {
                          "value": 1,
                          "description": "tf(freq=1.0), with freq of:",
                          "details": [
                            {
                              "value": 1,
                              "description": "termFreq=1.0"
                            }
                          ]
                        },
                        {
                          "value": 0.30685282,
                          "description": "idf(docFreq=1, maxDocs=1)"
                        },
                        {
                          "value": 0.625,
                          "description": "fieldNorm(doc=0)"
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      }
    ]
  }
}
```

When the query is executed outside the context of a `bool` query:

``` json
curl -XPOST 'localhost:9200/my_index/page/_search' -d '{
    "explain": true,
    "query": {
        "filtered": {
            "query": {
                "simple_query_string": {
                    "query": "the",
                    "fields": [
                        "name",
                        "content"
                    ]
                }
            },
            "filter": {
                "and": [
                    {
                        "term": {
                            "is_visible": true
                        }
                    },
                    {
                        "term": {
                            "store_id": 1234
                        }
                    }
                ]
            }
        }
    }
}
'
```

We now correctly get no results:

``` json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
```

In 1.3.2 or below you'll get the above empty result when using the original bool query.

Has anything changed in mappings or analysis that's causing this or is this a regression?
</description><key id="58835693">9866</key><summary>Bool query with must query is matching all documents when the query doesn't match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">rayward</reporter><labels /><created>2015-02-25T01:27:55Z</created><updated>2015-06-24T16:07:26Z</updated><resolved>2015-06-24T16:07:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rayward" created="2015-02-25T01:49:18Z" id="75890280">Interestingly, if I change `simple_query_string` to `query_string`, then I get the correct results.

Is this a combination of `simple_query_string` not strictly having this behaviour from `query_string`:

&gt; empty query
&gt; If the query string is empty or only contains whitespaces the query will yield an empty result set.

And this change in 1.3.3 - https://github.com/elasticsearch/elasticsearch/pull/7347 ?

I suspect the inner query parsing results in no query and the clause doesn't get added - https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java#L74 

So previously perhaps we got an empty result set and now it's returning `MatchAll` if there are no clauses. I'm just speculating though..
</comment><comment author="rayward" created="2015-02-25T06:08:18Z" id="75910671">We've confirmed that reverting this particular change - https://github.com/elasticsearch/elasticsearch/pull/7347/files#diff-94095d9e5782634b609ae9a773f11827L134 - resolves the issue for us.

I don't believe that is the "correct" fix in the end though, but it does restore the previous behaviour regarding `simple_query_string`.
</comment><comment author="dakrone" created="2015-02-25T06:36:43Z" id="75912937">Sorry, I didn't mean to close this, I am looking into this :)
</comment><comment author="dakrone" created="2015-02-25T22:37:57Z" id="76076515">As part of the root cause, I've submitted a patch for this to Lucene here: https://issues.apache.org/jira/browse/LUCENE-6298
</comment><comment author="rayward" created="2015-02-25T22:42:52Z" id="76077327">Thanks @dakrone :+1: 
</comment><comment author="clintongormley" created="2015-06-17T11:34:30Z" id="112761568">Closed by #10435
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tribe in es v.1.4.4 crashes when downstream cluster have problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9865</link><project id="" key="" /><description>This bug shows up in ES v1.4.4.   I reverted to running 1.4.3 and did not see this.   One of my downstream cluster suffered problems and the tribe node started hanging also, to where Kibana connecting to it timed out.

The logs were flooded with lines like these and the tribe node pretty much stopped responding.  If I shutdown it down and replaced with 1.4.3.   Then eventhough errors show in log, it still respond ok.

```
[2015-02-24 22:46:17,814][DEBUG][action.admin.cluster.node.stats] [ela4-app11185.prod] failed to execute on node [qZDMHmpgR4qDW8SX7M3ppg]
org.elasticsearch.transport.RemoteTransportException: [ltx1-app13285.prod][inet[/10.149.106.54:9300]][clu
ster:monitor/nodes/stats[n]]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'
        at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:542)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
        at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
```

etc...

```
[2015-02-24 22:46:17,953][DEBUG][action.admin.cluster.node.stats] [ela4-app11185.prod] failed to execute on node [zMwqY3NnR82BgSmdfeGEbQ]
org.elasticsearch.transport.RemoteTransportException: [ltx1-app13578.prod][inet[/10.149.108.37:9300]][cluster:monitor/nodes/stats[n]]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'
        at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:542)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
        at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:212)
        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:156)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:96)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: directory '/export/content/data/data-elasticsearch/nodes/0/indices/abc-log_event-ltx1-2015.02.22/1/index' exists and is a directory, but cannot be listed: list() returned null
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:231)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
        at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:87)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:40)
        at org.elasticsearch.index.store.Store.stats(Store.java:216)
        at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:540)
        ... 12 more
```

etc.

I have the logs, it's fairly large and contain some internal data.  Let me know how you want me to get them to you.
</description><key id="58831704">9865</key><summary>tribe in es v.1.4.4 crashes when downstream cluster have problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TinLe</reporter><labels><label>:Tribe Node</label><label>discuss</label></labels><created>2015-02-25T00:39:56Z</created><updated>2015-12-05T17:59:46Z</updated><resolved>2015-12-05T17:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-05-05T23:07:16Z" id="99256225">Hi @TinLe, can you clarify what you mean when you say that the tribe node has stopped responding?

&gt; The logs were flooded with lines like these and the tribe node pretty much stopped responding.
- Were requests made against the tribe node not returning?
- Were they returning with an exception?
- Specific API calls (e.g., stats as shown above) were failing?
- All API calls were failing?
- Were all failures related to a specific cluster?

Thanks, let us know.
</comment><comment author="TinLe" created="2015-05-06T05:00:43Z" id="99323548">The tribe does exactly as described :-).

. It stop responding to Kibana.  When I tried curl localhost:9200/_nodes/_local on node, it just hangs.   Eventually (30+ minutes, I went to get coffee, do something else), I control-c it :-}.

I posted part of the logs above (those are the common errors I was seeing).  I'd have to scrub it before putting entire log in public.

All API calls were failing, e.g. they all timed out or hang....   I ended up having to bounce tribe node each time, and it would be (maybe) useable for a bit before getting into a bad state again due to downstream cluster mis-behaving/sad.

These problems are due to one or more clusters.   No specific ones, although some clusters tend to "experience" more issues than others.   Due to internal policy, I do not have administrative control over all clusters.   But I do own the tribe nodes and am responsible for aggregating data from all clusters.

So I am caught between a rock and a very hard place.   I need tribe nodes to be highly resistant to misbehaving downstream clusters/nodes.   If a downstream clusters/nodes have issues, tribe should be able to skip/drop from list and check back later to see if that particular cluster or node has recovered.
</comment><comment author="pickypg" created="2015-05-07T03:13:48Z" id="99696826">Well, in that case, let's figure out how to get us those logs. Can you DM me your email address on Twitter ([@pickypg](https://twitter.com/pickypg)) to avoid giving away either of emails to the lovely bots of the world.

We certainly also want to ensure that we are not creating problems for our Tribe clients out there.
</comment><comment author="TinLe" created="2015-05-18T20:43:51Z" id="103204740">I have not had chance to get the logs together.   I'll do so soon.

I wonder if this is related to https://github.com/elastic/elasticsearch/issues/8677
</comment><comment author="pickypg" created="2015-06-29T17:11:46Z" id="116764317">@TinLe Any luck on scrounging those logs together?
</comment><comment author="TinLe" created="2015-06-29T20:25:47Z" id="116832133">Unfortunately I've gotten all my downstream to fix (and add monitoring to) their clusters, so have not seen this lately.

I do have one log from end of Feb that seem to show this problem.   Tried dm @pickypg on twitter, but since you're not following me, I am not allowed to dm you :-)

I'll send the log to you via another route.
</comment><comment author="pickypg" created="2015-06-29T20:49:51Z" id="116842609">@TinLe I sent a follow request to you, assuming the same name. You can probably also guess my email address using my first.last name. :)
</comment><comment author="TinLe" created="2015-06-29T22:35:13Z" id="116869086">@pickypg  I've submitted log file via your support site.  It's issue 10900.
</comment><comment author="clintongormley" created="2015-12-05T17:59:46Z" id="162231124">Closing in favour of https://github.com/elastic/elasticsearch/pull/13948 and https://github.com/elastic/elasticsearch/pull/14993
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for `minimum_should_match` to `simple_query_string`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9864</link><project id="" key="" /><description>This behaves similar to the way that `minimum_should_match` works for
the `match` query (in fact it is implemented in the exact same way)

Fixes #6449
</description><key id="58828675">9864</key><summary>Add support for `minimum_should_match` to `simple_query_string`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-25T00:06:26Z</created><updated>2015-04-06T17:50:18Z</updated><resolved>2015-02-25T19:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-25T18:19:02Z" id="76022248">@rmuir I dug into how this was changing the queries, and I think the current behavior is correct (I pushed more tests):

For these 4 documents:

```
{"body": "foo"} // id: 1
{"body": "bar"} // id: 2
{"body": "foo bar"} // id: 3
{"body": "foo bar baz"} // id: 4
```

I have 3 queries

First, an SQS query with `foo bar` and `minumum_should_match` of 2, this gets
transformed to

```
(_all:foo _all:bar)~2
```

Which is correct

Second an SQS query with `foo bar` for the fields "body" and "body2" (which
doesn't exist) with MSM=2, transformed to:

```
((body2:foo body:foo) (body2:bar body:bar))~2
```

This seems correct to be also, it can occur in either `body` or `body2`, but
more "foo" and "bar" have to be present.

Third, an SQS query with `foo bar baz` on the "body" and "body2" fields with
MSM=70%, which becomes:

```
((body2:foo body:foo) (body2:bar body:bar) (body2:baz body:baz))~2
```

Which seems correct to me also.

Next, I indexed 4 more documents to test the cross-field matching:

```
{"body2": "foo", "other": "foo"} // id: 5
{"body2": "bar", "other": "foo"} // id: 6
{"body2": "foo bar", "other": "foo"} // id: 7
{"body2": "foo bar baz", "other": "foo"} // id: 8
```

Then I issue 3 more queries.

First, a query for `foo bar` on the "body" and "body2" field with MSM=2

```
((body2:foo body:foo) (body2:bar body:bar))~2
```

This correctly matches documents 3, 4, 7, and 8

Second, a query for `foo bar` with no fields set (meaning "_all") and MSM=2

```
(_all:foo _all:bar)~2
```

This matches 3, 4, 6, 7, &amp; 8, which is what I would expect

Finally, a query for `foo bar baz` on "body2" and "other" with MSM=70%

```
((other:foo body2:foo) (other:bar body2:bar) (other:baz body2:baz))~2
```

Matching 6, 7, and 8, which is what I would expect, because to me it is
analogous to saying "find 2 of the following: 'foo, bar, baz' in the fields
'body2' and 'other'".

Can you explain more about what you mean with the unintuitive multi-field
application of the `minimum_should_match`?
</comment><comment author="rmuir" created="2015-02-25T18:30:08Z" id="76024689">I think you are right (and great to have those new tests), in that it works for simple multi-field cases too. My brain must have mixed this up with other parsers. 

I think it still degrades to the behavior I am concerned about when the language doesn't use whitespace to separate words, or when WHITESPACE operator is disabled (useful for query-time multi-word synonym support), or other cases. But in general it works and that is something to just fix about the parser one day.

+1 for this simple approach... sorry for the noise
</comment><comment author="rmuir" created="2015-02-25T18:44:03Z" id="76027638">as far as confusion over some of that stuff, i think it can apply in general with MSM, and will happen when things like stopwords are different across the fields too. Perhaps we should just doc some kind of warning, and keep it simple. I see all kinds of confusion about this e.g. on the solr lists around such things, and i fixed a bug in one of its parsers where it didnt apply it for chinese, which will certainly happen here. This is better than a lot of complexity I think. I always thought MSM was hard to integrate into parsers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix logging a RoutingNode object, log an object with a good .toString instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9863</link><project id="" key="" /><description>Previously this would log:

```
[2015-02-24 11:13:45,105][TRACE][cluster.routing.allocation.allocator] [Poltergeist] Try moving shard [[test][2], node[HFn4dJ7fQAyfSAB8BquaSQ], [R], s[STARTED]] from [org.elasticsearch.cluster.routing.RoutingNode@6df2c498]
```

DiscoveryNode has an actual `toString()` implementation, so log it instead
</description><key id="58826030">9863</key><summary>Fix logging a RoutingNode object, log an object with a good .toString instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T23:46:51Z</created><updated>2015-04-06T17:50:16Z</updated><resolved>2015-02-25T21:05:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-25T06:28:25Z" id="75912257">LGTM. If needed, may be a fix in 1.4 as well?
</comment><comment author="bleskes" created="2015-02-25T07:47:32Z" id="75918830">change looks good but why not add a toString to RoutingNode instead? This way we'll also fix future issues and have easier debugging life. How about `routingNode ([node_t0][xE3t3-VbSmeeCFRgtNGYeg][Boazs-Air.local][local[1]], [5] assigned shards)`
</comment><comment author="dakrone" created="2015-02-25T18:22:54Z" id="76023119">@bleskes yeah, that's a better idea, I will do that
</comment><comment author="dakrone" created="2015-02-25T18:33:59Z" id="76025516">@bleskes I changed this, it looks much better now:

```
[2015-02-25 11:32:45,182][TRACE][cluster.routing.allocation.allocator] [Solarr] Try moving shard [[my_index][2], node[HDhbU4D9Rx27MSr_72bmWQ], [P], s[STARTED]] from [routingNode ([Solarr][HDhbU4D9Rx27MSr_72bmWQ][Xanadu.domain][192.168.0.4], [5 assigned shards)]
```
</comment><comment author="bleskes" created="2015-02-25T19:29:20Z" id="76036680">+1 except for a minor parentheses issue: `[5 assigned shards)]` should probably be `[5] assigned shards)]`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_match cross_fields type does not work with position_offset_gap setting (!?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9862</link><project id="" key="" /><description>Hi,

It seems that the **cross_fields** type **multi_match** query cannot work together with fields that have **position_offset_gap** settings.
Is this a feature or bug?

Reproduction:

```
# Delete old index
curl -XDELETE "http://localhost:9200/testindex"

# Create test index
curl -XPOST "http://localhost:9200/testindex/" -d'
{
  "index": {
    "number_of_replicas": "0",
    "number_of_shards": "1"
  }
}'

# (testcase 1) Create mapping with position offset gap
curl -XPUT "http://localhost:9200/testindex/_mapping/user" -d'
{
  "user": {
    "properties": {
      "first_name": {
        "type": "string",
        "position_offset_gap": 100
      },
      "last_name": {
        "type": "string",
        "position_offset_gap": 100
      }
    }
  }
}'

# (testcase 2) Create mapping without position offset gap
curl -XPUT "http://localhost:9200/testindex/_mapping/user" -d'
{
  "user": {
    "properties": {
      "first_name": {
        "type": "string"
      },
      "last_name": {
        "type": "string"
      }
    }
  }
}'

# Put 1 user
curl -XPUT "http://localhost:9200/testindex/user/1" -d'
{
  "first_name": "John",
  "last_name": "Smith"
}'

# Multi match with cross_fields (try with testcase 1 and 2)
curl -XGET "http://localhost:9200/testindex/user/_search" -d'
{
  "query": {
    "multi_match": {
      "query": "John Smith",
      "type": "cross_fields",
      "fields": [
        "first_name",
        "last_name"
      ],
      "operator": "and"
    }
  }
}'
```

Try it with the first and then with the second mapping, as well. 
Testcase 1 will not produce result hit, while testcase 2 will get the 1 document.
I tried with the latest ES version.

Thanks,
Csaba D.
</description><key id="58823477">9862</key><summary>multi_match cross_fields type does not work with position_offset_gap setting (!?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dezsenyi</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2015-02-24T23:25:29Z</created><updated>2015-12-05T17:52:14Z</updated><resolved>2015-12-05T17:52:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:30:22Z" id="76505605">Nice catch!  Confirmed that this is a bug in 1.4.4

thanks
</comment><comment author="VicsCaba" created="2015-05-18T16:32:28Z" id="103120074">Hi,

Has this issue been solved? I was experiencing also some problems with position_offset_gap when applying analyzers. Field is multivalue and position_offset_gap works properly if I don't define any analyzer for this field. However when applying an analyzer such us asciifolding it seems that position_offset_gap is not applied.

Is this the expected behaviour? I tried to handle it creating a custom analyzer with position_offset_gap but didn't work
https://github.com/elastic/elasticsearch/pull/10934/files#r29594225

Thank you
</comment><comment author="clintongormley" created="2015-05-25T10:48:00Z" id="105204463">Hi @VicsCaba - not fixed yet, still on the list.
</comment><comment author="clintongormley" created="2015-12-05T17:52:14Z" id="162230670">This appears to have been fixed, at least in 2.1.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use en-dash instead of hyphen in license between years</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9861</link><project id="" key="" /><description>English grammar states that an en-dash should be used, not a hyphen between two years.
## Examples of each

**em-dash**: &#8212;
**en-dash**: &#8211;
**hyphen**: -
## Em-dash

An em-dash is typically used as a stand-in for a comma or parenthesis to separate out phrases&#8212;or even just a word&#8212;in a sentence for various reasons (i.e. an appositive). Examples where an em-dash should be used:
- School is based on the three R&#8217;s&#8212;reading, writing, and &#8217;rithmetic.
- Against all odds, Pete&#8212;the unluckiest man alive&#8212;won the lottery.
- I sense something; a presence I've not felt since&#8212;
## En-dash

An en-dash is used to connect values in a range or that are related. A good rule is to use it when you're expressing a "to" relationship. Examples where an en-dash should be used:
- in years 1939&#8211;1945
- pages 31&#8211;32 may be relevant
- New York beat Los Angeles 98&#8211;95
- When American English would use an em-dash &#8211; following British and Canadian conventions.
## Hyphen

A hyphen is used to join words in a compound construction, or separate syllables of a word, like during a line break, or (self-evidently) a hyphenated name.
- pro-American
- cruelty-free eggs
- em-dash
- it's pronounced hos-pi-tal-it-tee
- Olivia Newton-John
</description><key id="58817470">9861</key><summary>Use en-dash instead of hyphen in license between years</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kkirsche</reporter><labels /><created>2015-02-24T22:40:14Z</created><updated>2015-02-28T02:24:56Z</updated><resolved>2015-02-28T02:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:10:56Z" id="76504625">All of that, and you missed the fact that the year is now 2015? :)
</comment><comment author="clintongormley" created="2015-02-28T02:24:56Z" id="76505343">I was about to merge this, then I went through all of the files that needed the copyright year to be updated, and I realised that I'm perfectly happy with using an ASCII hyphen as a range between numbers when viewing and editing docs in the console.  Also, in my console at least, the en-dash displays poorly.

Thanks, but I'm going to close this PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] OOM Error when using QuadPrefixTree with 1m precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9860</link><project id="" key="" /><description>Fixing issue #9691 revealed a deeper problem with the QuadPrefixTree's memory usage.  At 1m precision the example shape in https://gist.github.com/nknize/abbcb87f091b891f85e1 consumes more than 1GB of memory.  This is initially alleviated by using 2 bit encoded quads (instead of 1byte) but only delays the problem.  Moreover, as new complex shapes are added duplicate quadcells are created - thus introducing unnecessary redundant memory consumption (an inverted index approach makes mosts sense - its Lucene!).

For now, if a QuadTree is used for complex shapes great care must be taken and precision must be sacrificed (something that's automatically done with the distance_error_pct without the user knowing - which is a TERRIBLE approach).  An alternative improvement could be to apply a Hilbert R-Tree - which will be explored as a separate issue.  Or to restrict the accuracy to a lower level of precision (something that's undergoing experimentation).
</description><key id="58812257">9860</key><summary>[GEO] OOM Error when using QuadPrefixTree with 1m precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>enhancement</label></labels><created>2015-02-24T21:58:01Z</created><updated>2015-04-21T19:03:55Z</updated><resolved>2015-04-21T19:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] Update parameters and defaults for geo_point mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9859</link><project id="" key="" /><description>`geo_point` mapping currently has 11 parameters (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html) most of which are causing mass confusion for 90% of the elasticsearch user base.

Additionally, for most use-cases, the defaults are consuming large amounts of resources leading to scaling problems over time (unless the user really understands what is going on under the covers - which most don't).  This enhancement will update the default `geo_point` parameter mappings and rework the geohash logic to reduce resource consumption and improve the overall performance for `geo_point`.  

`geo_shape` mapping will be worked as a separate issue (which has a large set of its own issues)
</description><key id="58811098">9859</key><summary>[GEO] Update parameters and defaults for geo_point mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-02-24T21:49:04Z</created><updated>2015-11-13T05:07:25Z</updated><resolved>2015-11-13T05:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-02-25T09:48:00Z" id="75932596">Maybe it's worth also combining the normalise and validate options? Especially since they don't work together. We could have, for example, just the `validate`, `validate_lat` and `validate_lon` options and have the accepted values as `error`, `normalise`, `no`.

Also do we need three settings for validate? Are there really use cases where people need to validate or normalise latitudes and not longitudes? Could we have just a single `validate` option which works on  both lat and lon?
</comment><comment author="clintongormley" created="2015-02-28T02:49:39Z" id="76506483">Is there ever a reason why you would want to accept invalid geopoints?
</comment><comment author="RolphH" created="2015-04-05T06:11:49Z" id="89726543">Is geopoint [null, null] considered to be invalid, then yes, there is a reason. In some occasions we don't have a geopoint (for example when your geoip database is not up-to-date and you cannot resolve an IP to a geopoint ). Result is [null, null] 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to change chart label in Kibana 4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9858</link><project id="" key="" /><description>I have upgraded my application to Kibana 4, and generated few dashboard.
I am looking for option to change the label. by default it is coming as "Count of documents"

Can you please help me to fix this.
</description><key id="58810824">9858</key><summary>How to change chart label in Kibana 4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebadul1981</reporter><labels /><created>2015-02-24T21:47:03Z</created><updated>2015-02-24T21:55:33Z</updated><resolved>2015-02-24T21:52:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-24T21:52:40Z" id="75854501">this is the issue tracker for elasticsearch not kibana. Can you please refer to the mailing lists for questions like this, issue tracker are only for bugs and development discussions
</comment><comment author="ebadul1981" created="2015-02-24T21:54:17Z" id="75854794">Can you please send me the URL/path?
</comment><comment author="s1monw" created="2015-02-24T21:55:33Z" id="75855032">@rashidkpc what's the best place for this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot  / restore module lets you restore an index with alias that conflicts with another index on restore destination.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9857</link><project id="" key="" /><description>I've only tested this on 1.3.5, but after going through the release notes for every version since, including 1.4.x, I don't see it mentioned anywhere.

I had an index named "dev-blog" on my 1.3.5 cluster. I restored a snapshot of a "blog" index from another (1.0.1) cluster. This "blog" index had a dev-blog alias associated with it, and I forgot to tell the restore process to not restore the aliases. 

The restore completed successfully. However, I was unable to delete the dev-blog alias from the "blog" cluster. When trying to delete the alias ( curl -XDELETE 'localhost:9200/blog/_alias/dev-blog' ), this was returned instead:

{
  "error": "RemoteTransportException[[xxxxxxxx:xxxx]][indices/aliases]]; nested: InvalidAliasNameException[[blog] Invalid alias name [dev-blog], an index exists with the same name as the alias]; ",
  "status": 400
}

it was like this restore operation allowed the cluster to get into a state I could not get it out of.  To fix this, I had to snapshot the dev-blog index, delete it completely, and THEN i was able to issue the above delete command to remove the dev-blog alias from the blog index.

Either the restore needs to check if there is a collision between aliases and index names before allowing the restore to happen, or the validation step being done as part of the DELETE needs to let this proceed. 
</description><key id="58807836">9857</key><summary>Snapshot  / restore module lets you restore an index with alias that conflicts with another index on restore destination.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gheppner</reporter><labels /><created>2015-02-24T21:23:51Z</created><updated>2015-02-28T02:03:24Z</updated><resolved>2015-02-28T02:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:03:23Z" id="76504249">@gheppner this looks like it has been fixed in https://github.com/elasticsearch/elasticsearch/pull/7918
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Schedule pending delete if index store delete fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9856</link><project id="" key="" /><description>We try to lock all shards when an index is deleted but likely not
succeeding since shards are still active. To ensure that shards
that used to be allocated on that node get cleaned up as well we have
to retry or block on the delete until we get the locks. This is not desirable
since the delete happens on the cluster state processing thread. Instead of blocking
this commit schedules a pending delete for the index just like if we can't delete shards.
</description><key id="58806079">9856</key><summary>Schedule pending delete if index store delete fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T21:10:44Z</created><updated>2015-06-07T17:35:41Z</updated><resolved>2015-02-25T13:32:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-25T07:49:50Z" id="75919027">@bleskes if you have a minute can you look at this?
</comment><comment author="bleskes" created="2015-02-25T09:09:09Z" id="75927505">LGTM. Left two minor comments.
</comment><comment author="bleskes" created="2015-02-25T10:53:06Z" id="75940929">LGTM, again :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update span-multi-term-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9855</link><project id="" key="" /><description>Added comma - there is no "term range" query
</description><key id="58784208">9855</key><summary>Update span-multi-term-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cgp</reporter><labels /><created>2015-02-24T18:45:11Z</created><updated>2015-02-28T02:06:08Z</updated><resolved>2015-02-28T02:06:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T02:05:57Z" id="76504377">thanks @cgp 
merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Use all found index files instead of static list for static bwc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9854</link><project id="" key="" /><description>It is a pain to add a new static index, and then have to update the old
index test.  This removes the need for the latter step.
</description><key id="58782713">9854</key><summary>Tests: Use all found index files instead of static list for static bwc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T18:33:01Z</created><updated>2015-03-24T03:28:39Z</updated><resolved>2015-02-24T19:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-02-24T19:17:06Z" id="75824998">looks good, just a small suggestion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>For a given list of indices - consecutively iterating thru and setting index.routing.allocation.require gets progressively slower</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9853</link><project id="" key="" /><description>Using the curator tool (or a bash like `for i in list of indices`;do blah;done) rolling thru 1000 indexes the api calls get progressively slower and slower.

The scenario is to set all the indexes to a certain class as we move towards a tiered storage strategy.
we set `class:ssd`

I don't have 1st run logs - but on a 2nd run it looks like

```
curator allocation --older-than 0 --prefix globallogs. --timestring '%Y%m%d' --rule class=ssd
2015-02-24 15:54:29,318 INFO      Job starting...
2015-02-24 15:54:29,320 INFO      Applying allocation/routing tags to indices...
2015-02-24 15:54:29,394 INFO      Updating index setting index.routing.allocation.require.class=ssd
2015-02-24 15:54:33,407 INFO      apply_allocation_rule operation succeeded on globallogs.20110812
2015-02-24 15:54:33,409 INFO      Updating index setting index.routing.allocation.require.class=ssd
2015-02-24 15:54:37,430 INFO      apply_allocation_rule operation succeeded on globallogs.20110813

&lt;snip&gt;

2015-02-24 17:30:46,229 INFO      apply_allocation_rule operation succeeded on globallogs.20141104
2015-02-24 17:30:46,231 INFO      Updating index setting index.routing.allocation.require.class=ssd
2015-02-24 17:30:55,402 INFO      apply_allocation_rule operation succeeded on globallogs.20141105
2015-02-24 17:30:55,403 INFO      Updating index setting index.routing.allocation.require.class=ssd
```

I've tried to do a single shard like `curl -XPUT localhost:9200/globallogs.20150223/_settings -d '{"index.routing.allocation.require.class" : "ssd"}'` and it took the same amount of time to respond.

Querying Hot threads from /_nodes/hot_threads - gives https://gist.github.com/petecheslock/9d27dba1afe6fe0578e4

We are running v1.4.3
</description><key id="58773771">9853</key><summary>For a given list of indices - consecutively iterating thru and setting index.routing.allocation.require gets progressively slower</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">petecheslock</reporter><labels /><created>2015-02-24T17:38:21Z</created><updated>2015-03-15T15:34:48Z</updated><resolved>2015-03-14T22:00:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-24T19:15:28Z" id="75824676">Okay, I spent some time looking into this, here is my theory - 

Every time an index's settings are updated, Elasticsearch first applies the new
settings to the index, and attempts to re-route the cluster (in case settings
like `number_of_replicas` have changed). See this line in
[MetaDataUpdateSettingsService](https://github.com/elasticsearch/elasticsearch/blob/1816951b6b0320e7a011436c7c7519ec2bfabc6e/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java#L313-L315)

So here's what is happening, the first index is having the
`index.routing.allocation.require.class: ssd` setting added to it, so the
setting is added, and then the master nodes reroutes all of the shards. This
means the `BalancedShardsAllocator` will run through shards checking if they can
remain, in your case none of the nodes have the `node.class: ssd` setting, so
the shard for the index can not remain on the current node. The allocator
attempts to move it to any other node, which it can't, because none of the nodes
have that tag yet. Eventually, it fails because there is nowhere to move the
shards for this index and it gives up.

The cluster state in incremented and sent out to all nodes.

Now, the second index has the `index.routing.allocation.require.class: ssd`
setting applied. Again, the master node adds the setting and attempts another
reroute. This time the balanced shards allocator tries to move the second index
away, and it also tries to move all the shards for the first index away. Again,
there are no nodes with the "ssd" tag, so it eventually gives up.

The cluster state is incremented and sent out to all nodes.

This repeats for every index that you have, so you end up with:

`n * (n + 1) / 2` total moving-an-index-away attempts.

For 1000 indices, this means 500500 attempted rebalances for a particular index,
assuming you are applying the setting incrementally.

Fortunately, there is a way to make this better -

Currently, Curator issues requests on an index-by-index basis after resolving
the indices that need to have settings changed, however, after every request, a
new reroute is performed and a new cluster state issued. To prevent this, we can
change:

```
curl -XPUT 'localhost:9200/index1/_settings' -d'{...}'
curl -XPUT 'localhost:9200/index2/_settings' -d'{...}'
curl -XPUT 'localhost:9200/index3/_settings' -d'{...}'
curl -XPUT 'localhost:9200/index4/_settings' -d'{...}'
curl -XPUT 'localhost:9200/index5/_settings' -d'{...}'
```

Into a single call:

```
curl -XPUT 'localhost:9200/index1,index2,index3,index4,index5/_settings' -d'{...}'

... or ...

curl -XPUT 'localhost:9200/index*/_settings' -d'{...}'
```

By doing it in a single request, only a single reroute is attempted after
applying the settings to all the indices.

I've spoken with @untergeek about making this change in Curator, and I think we
both agreed that it should be added. Until it is, as a workaround, you can
manually apply the setting to multiple indices at once, which should prevent so
much churn in the cluster with regard to rerouting and new cluster states.
</comment><comment author="petecheslock" created="2015-02-24T20:52:39Z" id="75843285">I think you're on the right track. For another datapoint - after restarting the cluster to pick up the new node class tags set.  We ran (running) curator again on indices older than x days and seeing a similar things happen even with there being nodes in the cluster with the proper class assigned.   Maybe just due to the sheer number of indices and nodes?  We don't see it on a dev cluster of 100 indices and like 6 systems (yet).  But the workaround can work for me in the meantime.   

Thanks!
</comment><comment author="petecheslock" created="2015-02-24T21:02:27Z" id="75845207">Actually - i take that back - after restarting the cluster - with the tags set, it's cranking thru the indexes at a rate of one a second (which is far better than before) and does not show likes it's slowing down.  So i think mostly related to when you don't have the node applied with that tag yet that may exacerbate the problem.  
</comment><comment author="dakrone" created="2015-03-14T22:00:58Z" id="80739757">@petecheslock okay, curator [3.0.0](https://github.com/elastic/curator/releases) was released, which batches these requests instead of sending them one-at-a-time. This should fix this for the case when the tag is not set on a node and it keeps taking longer and longer, because it will only cause a single cluster state update.

I'm going to close this for now, feel free to re-open or open a new issue!
</comment><comment author="petecheslock" created="2015-03-15T15:34:48Z" id="81103082">Thanks @dakrone &#128077;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score weight of 0.0 returns 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9852</link><project id="" key="" /><description>Any ideas why this would return a score of 1, when the last (bool must_not) matches, even though the weight is set to 0.0?  (BTW - using elasticsearch 1.4.4)

As a bit of background, this provides custom scoring where 
- if one field matches, score at .35, 
- if another more important field matches, score .65, 
- if both match, score 1.0 (the sum)
- if non match, score 0

If I change the no match weight to -1 or something very small, like 0.000001, the weight for the must_not match comes back accurately.
I am thinking this is some kind of divide by 0 error handler in ES, that returns 1 when a score hits 0?

```
GET test/document/_search
{
  "query": {
    "function_score": {
      "filter": { "term": { "type": "animal" } },
      "functions": [ 
        {
          "filter": { 
            "bool" : {
                "must" : {
                    "term" : { "content" : "fish"}
                }}}, 
          "weight": 0.35
        },
        {
          "filter": { 
            "bool" : {
                "must" : {
                    "term" : { "user_description" : "fish"  }
                }}}, 
          "weight": 0.65
        },
        {
          "filter": { 
            "bool" : {
                "must_not" : [
                    {"term" : { "user_description" : "fish" }},
                    {"term" : { "content" : "fish" }}
                    ]
                }
          }, 
          "weight": 0.0
        }
      ],
      "score_mode": "sum"
    }
  }
}
```
</description><key id="58771056">9852</key><summary>function_score weight of 0.0 returns 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffsteinmetz</reporter><labels /><created>2015-02-24T17:20:47Z</created><updated>2015-02-24T23:13:53Z</updated><resolved>2015-02-24T23:13:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeffsteinmetz" created="2015-02-24T23:13:53Z" id="75869478">closed.  Realized that "sum" adds function score to query score.  0 + 1 = 1
use "boost_mode" : "replace"
and score non matches with
"script_score": {
            "script": "0"
          }

I was hoping to do this without scripting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[mappings] partially parsed documents can cause mapping loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9851</link><project id="" key="" /><description>When a document is parsed only halfway but parsing exits with a MapperParsingException, the fields that were parsed till then are still in the local DocumentMapper but never added to the cluster state. Once the nodes are restarted the mapping is gone. I wore a test for it here https://github.com/brwe/elasticsearch/commit/52cd27c5735f678f20fb89c5eea8c746536f5c25#diff-defbaaff93b959a2f9a93e7167f6f345R246

We can potentailly fix this by also update the mapping on MapperParsingExceptions (https://github.com/brwe/elasticsearch/commit/52cd27c5735f678f20fb89c5eea8c746536f5c25#diff-9669e07f0556311d187e534e321a0393R422) but we would probably need to check first if the mapper was actually changed, otherwise we might end with one update for each Exception.
</description><key id="58768849">9851</key><summary>[mappings] partially parsed documents can cause mapping loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-02-24T17:05:02Z</created><updated>2015-03-26T20:06:43Z</updated><resolved>2015-03-26T20:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch converting float numbers when aggregating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9850</link><project id="" key="" /><description>Hello,
I have a index with multiple documents and  columns.
Two of my columns are order number and status code.
I am trying to identify all unique ordernumbers and find the highest status code within that same order_number.
For example: 
Order Number : 123 has 5 documents each with different status codes (1, 2, 3.4,5,7)
Order Number : 125 has 3 documents each with different status codes (3,6,3.3)

I have noticed that when I have a decimal number, the value gets converted to random number. For exmaple:
If a status code was 3350.2, it would get converted to 3350.199951171875

Aggs part of the query looks like:
      "aggs" : {
        "keys" : 
            {
                "terms" : { "field" : "ol.ORDER_LINE_KEY", "size": 2500},
                "aggs" : {
                    "max" : { "max" : { "field" : "ol.STATUS_CODE" } }
                }
            }
        }
    }

In ES_Head, the status code appears to be correct, it only seems to messup when doing aggs.
</description><key id="58768344">9850</key><summary>Elasticsearch converting float numbers when aggregating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ap0091</reporter><labels /><created>2015-02-24T17:01:51Z</created><updated>2015-02-24T17:10:38Z</updated><resolved>2015-02-24T17:10:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-24T17:10:38Z" id="75798837">&gt; If a status code was 3350.2, it would get converted to 3350.199951171875

This is because:
- aggregations always work on doubles, not floats. If you have a float, it will be casted to a double.
- 3350.2 cannot be represented accurately as a floating-point number

Here is a code snippet (in Java but would do the same in other languages) that demonstrates the problem that you are seeing:

``` java
float f = 3350.2f;
System.out.println(f); // prints 3350.2
double d = f;
System.out.println(d); // prints 3350.199951171875
d = 3350.2;
System.out.println(d); // prints 3350.2
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scheduling threads can be leaked on exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9849</link><project id="" key="" /><description>In #9843, HunspellServiceTests leak scheduler and timer thread pools.  In this test class, there are 2 tests which check excepetion cases.  When those 2 tests are @Ignored, there are no more thread leaks.
</description><key id="58759652">9849</key><summary>Scheduling threads can be leaked on exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>bug</label><label>test</label></labels><created>2015-02-24T16:04:08Z</created><updated>2015-03-27T09:12:36Z</updated><resolved>2015-03-27T09:12:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggs: Terms filtering should leverage Terms.intersect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9848</link><project id="" key="" /><description>Our approach at terms filtering today is naive (and slow) since we check the regular expression against every possible term in order to figure out whether is is included or not. We should switch to Lucene's RegExp and terms intersection in order to advance quickly to the next matching term and build the bit that that contains allowed terms ordinals more quickly.

This would be a breaking change since the regexp syntax is not exactly the same, but it would also have the benefit of becoming consistent with regexp queries.
</description><key id="58757945">9848</key><summary>Aggs: Terms filtering should leverage Terms.intersect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>adoptme</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T15:52:38Z</created><updated>2015-05-15T13:15:22Z</updated><resolved>2015-05-15T13:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-24T15:54:58Z" id="75782805">This would likely fix #7526.
</comment><comment author="clintongormley" created="2015-02-28T01:52:46Z" id="76503663">+1
</comment><comment author="jpountz" created="2015-05-15T13:15:21Z" id="102393727">Closed through https://github.com/elastic/elasticsearch/pull/10418
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to search by geo via the querystring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9847</link><project id="" key="" /><description>I was trying to search by one of my geo fields via a query string (search at a single location) and was unable to do so. It would be great to also be able to do a geo search and put a radius (kinda like a number range query) via the query string.

I came across the following post where others were also asking for this: http://stackoverflow.com/questions/26371753/elasticsearch-query-string-query-on-geo
</description><key id="58744365">9847</key><summary>Ability to search by geo via the querystring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niemyjski</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-02-24T14:20:39Z</created><updated>2015-12-05T17:48:20Z</updated><resolved>2015-12-05T17:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="niemyjski" created="2015-03-27T13:59:05Z" id="86947244">I think this would be an awesome feature. I could find all the ice cream shops in a 5 mile radius in 20 characters or less :).
</comment><comment author="clintongormley" created="2015-12-05T17:48:20Z" id="162230086">This would have to be implemented in Lucene, as we use their query parser.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_all field setting should allow for excluding fields by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9846</link><project id="" key="" /><description>The current behavior of the _all field is to be enabled by default and `include_in_all` on all fields by default.

While fields can be individually opted-out of _all, and _all completely disabled for a type, a way to enable _all but use a white-list approach on fields is missing.

E.g. something like

``` json
{
    "person" : {
        "_all" : {"enabled" : true, "include_by_deafult": false},
        "properties" : {
            "simple1" : {"type" : "string", "include_in_all" : true},
            "simple2" : {"type" : "string"}
        }
    }
}
```

So enabled and include_by_default are true by default to match the current behavior, but with a simple change to `include_by_default` you can change _all's behavior to be a white-list approach as opposed to the current black-list approach
</description><key id="58726598">9846</key><summary>_all field setting should allow for excluding fields by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>discuss</label></labels><created>2015-02-24T11:36:42Z</created><updated>2015-12-05T17:47:11Z</updated><resolved>2015-12-05T17:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-24T11:41:28Z" id="75742012">I would really use copy_to feature instead of _all field.
</comment><comment author="synhershko" created="2015-02-24T11:44:17Z" id="75742351">This is mostly a concern for Kibana users who don't want everything searchable by default - real world documents can become rather big, and Kibana still searches on _all by default.

Would copy_to preserve per-field analysis like _all does? if so, that should be specified in the docs.
</comment><comment author="clintongormley" created="2015-02-24T11:52:02Z" id="75743302">@synhershko The `_all` field doesn't preserve per-field analysis.  It has its own analyzer.  
</comment><comment author="synhershko" created="2015-02-24T11:57:00Z" id="75743908">@clintongormley ok, must be confusing this with another feature (preserving boosts maybe?)

Nevertheless, would love an easy way to exclude fields by default from _all - right now I have to use a template or use _default in mapping
</comment><comment author="javanna" created="2015-03-19T15:36:06Z" id="83636663">I second @dadoonet , I would even disable the `_all` field for this usecase and use the `copy_to` to have your own custom catch_all field(s), which will hold only the content that you want it to contain.
</comment><comment author="clintongormley" created="2015-12-05T17:47:11Z" id="162229808">This can be done today:

```
PUT t
{
  "mappings": {
    "t": {
      "include_in_all": false,
      "properties": {
        "foo": {
          "type": "string",
          "include_in_all": true 
        },
        "bar": {
          "type": "string"
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": "one",
  "bar": "two"
}
```

This matches:

```
GET t/_search?q=one
```

This doesn't:

```
GET t/_search?q=two
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggestion doesnt give integer suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9845</link><project id="" key="" /><description>{
"suggest":{"autocomplete":{"text":"80","completion":{"field":"pageAutoComplete","size":5}}}
}

Doesnt give any suggestions. Whereas 

{
"suggest":{"autocomplete":{"text":"st","completion":{"field":"pageAutoComplete","size":5}}}
}

works perfectly.

The field pageAutoComplete is a completion field, using simple analyzer.

Using Elastic Search 1.4.2
</description><key id="58723401">9845</key><summary>Completion Suggestion doesnt give integer suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mathiasnohall</reporter><labels /><created>2015-02-24T11:05:40Z</created><updated>2015-02-28T01:43:14Z</updated><resolved>2015-02-28T01:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T01:43:14Z" id="76503169">Hi @mathiasnohall 

The simple analyzer only keeps letters, it discards anything else (including numbers)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove InternalNode class and use Node directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9844</link><project id="" key="" /><description /><key id="58717194">9844</key><summary>Remove InternalNode class and use Node directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T10:07:58Z</created><updated>2015-06-06T16:12:11Z</updated><resolved>2015-02-24T10:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-24T10:16:14Z" id="75730895">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Remove thread leak filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9843</link><project id="" key="" /><description>Now that the global cluster is gone, we can get rid of this filter that allows leaks across tests.  This change uncovered one thread pool that we never closed, as well as leaks across tests for suite scope. 

There is still one leak, when running HunspellServiceTests (it leaks the `timer` and `scheduler` threadpools). Perhaps someone has ideas on what could be happening there.
</description><key id="58715578">9843</key><summary>Tests: Remove thread leak filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T09:54:21Z</created><updated>2015-02-27T08:17:31Z</updated><resolved>2015-02-24T16:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-24T10:09:57Z" id="75730015">awesome LGTM
</comment><comment author="javanna" created="2015-02-25T08:20:46Z" id="75922045">Shouldn't we remove `ElasticsearchThreadFilter` as well now? It seems unused at this point.
</comment><comment author="javanna" created="2015-02-25T08:22:14Z" id="75922198">Also, @rjernst can you please label this PR?
</comment><comment author="dadoonet" created="2015-02-25T08:25:36Z" id="75922531">If we do remove it as per @javanna suggestion, please update also [Twitter River](https://github.com/elasticsearch/elasticsearch-river-twitter/) project in `master`, `es-1.x` and `es-1.4` branches: https://github.com/elasticsearch/elasticsearch-river-twitter/blob/master/src/test/java/org/elasticsearch/river/twitter/test/TwitterIntegrationTest.java#L84
</comment><comment author="dadoonet" created="2015-02-25T08:32:12Z" id="75923212">My comment was unclear. I meant that we just have to remove it from `TwitterIntegrationTest` and replace:

``` java
@ThreadLeakFilters(defaultFilters = true, filters = {ElasticsearchThreadFilter.class, Twitter4JThreadFilter.class})
```

by:

``` java
@ThreadLeakFilters(defaultFilters = true, filters = {Twitter4JThreadFilter.class})
```
</comment><comment author="rjernst" created="2015-02-27T07:50:49Z" id="76351998">I pushed the removal of ElasticsearchThreadFilter in e221dc2 and 28776e6, and also fixed the twitter river as David noted.
</comment><comment author="dadoonet" created="2015-02-27T08:17:31Z" id="76354289">Interesting. I was not aware of this: https://github.com/elasticsearch/elasticsearch-analysis-icu/blob/master/src/test/java/org/elasticsearch/index/analysis/TestIndexableBinaryStringTools.java#L43

Thank you Jenkins catching it! :) I'm going to fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_id` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9842</link><project id="" key="" /><description>There are two implications to this change.
First, percolator now uses _uid internally, extracting the id portion
when needed. Second, sorting on _id is no longer possible, since you
can no longer index _id. However, _uid can still be used to sort, and
is better anyways as indexing _id just to make it available to
fielddata for sorting is wasteful.

see #8143
</description><key id="58712755">9842</key><summary>Lock down `_id` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T09:26:36Z</created><updated>2015-06-08T09:04:28Z</updated><resolved>2015-02-24T22:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-24T09:32:20Z" id="75724701">LGTM
</comment><comment author="jpountz" created="2015-02-24T09:32:35Z" id="75724737">Maybe it should have an entry in the migration guide?
</comment><comment author="jpountz" created="2015-02-24T10:16:34Z" id="75730957">LGTM^2
</comment><comment author="martijnvg" created="2015-02-24T10:20:32Z" id="75731518">the reason the percolator was using the _id field was that the type part of the uid wouldn't be needed to be loaded in memory via field data and to prevent the splitting of the type and id at percolate time. 

I'm less concerned about the type being loaded in memory, since at some point _uid will be doc values by default. But I'm not a big fan of the type/id splitting. Maybe the percolator should have its own field that can be used for id lookup? But this can potentially be explored in a different issue. 

But LGTM and worry about that later.
</comment><comment author="jpountz" created="2015-02-24T15:48:08Z" id="75781382">There are already at least two reasons why we would need doc values on `_uid`:
- search stability (since we don't always query the same copy of the data)
- random scores with seed

So maybe we should have doc values on _uid (at least by default, maybe enforced). I think this would also fix this percolator concern and it would be fine to work with the _uid instead of _id?
</comment><comment author="martijnvg" created="2015-02-24T22:48:44Z" id="75864220">@jpountz yes that makes sense and fix the concern. I lean towards enforcing doc values on the _uid field.
</comment><comment author="javanna" created="2015-02-25T08:23:35Z" id="75922318">@rjernst labels please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Node#stop() in favor of #close()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9841</link><project id="" key="" /><description>it's confusing and we don't test the `restart` functionality. I think we should just remove it entirely.
</description><key id="58709309">9841</key><summary>Remove Node#stop() in favor of #close()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-24T08:49:06Z</created><updated>2015-02-27T13:04:02Z</updated><resolved>2015-02-27T13:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-24T09:27:50Z" id="75724067">btw. this also means we will need to remove `TransportNodesRestartAction` and friends. IMO that's just fine as we don't rest that either...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>My index is not Mapping in elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9840</link><project id="" key="" /><description>I'm moving my data from mysql to elastic search using jdbc river plugin.

its scheduled to fed data into elastic search every minute.... But when, I try to visualize the data.. Elastic search s not getting indexed with kibana...

JDBC RIVER

curl -XPUT 'localhost:9200/_river/Sample_query/_meta' -d '{
  "type" : "jdbc",
  "jdbc":{
      "url" : "jdbc:mysql://localhost/",
      "user" : "xxx",
      "password" : "xxxx",
      "strategy" : "simple",
      "poll" : "5s",
      "scale" : 0,
      "autocommit" : true,
      "fetchsize" : 10,
      "max_rows" : 0,
      "max_retries" : 3,
      "max_retries_wait" : "10s",
      "schedule": "0 0/1 \* \* \* ?",
      "sql" :"SELECT *from clock desc limit 350;"

Index mapping:

"type_mapping": {
            "mytype" : {
                "properties": {
                        "GroupName": {
                "type": "string",
                "store": true, 
                "index" : "not_analyzed",
                "doc_values": true
            },
                        "HostsName": {
                "type": "string",
                "store": true, 
                "index" : "not_analyzed",
                "doc_values": true
            }, 
                        "DATE": {
                "type": "date",
                "store": true, 
                "doc_values": true
            },
                        "ItemsName": {
                "type": "string",
                "store": true,
                "index" : "not_analyzed",
                "doc_values": true
            },
            "Value": {
                "type": "number",
                "store": true, 
                "doc_values": true
            }
                    }
            }
        }
How to map the index in elasticsearch???
</description><key id="58707650">9840</key><summary>My index is not Mapping in elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Diwahars</reporter><labels /><created>2015-02-24T08:29:29Z</created><updated>2015-02-24T11:44:39Z</updated><resolved>2015-02-24T11:44:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-24T11:44:39Z" id="75742401">You'll get better support on the mailing list or on the JDBC plugin repo about this.

Github is only for issues or feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to append data to exist data in array?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9839</link><project id="" key="" /><description>Hi ,

I have a document  on Elastic search in below format.

{
    "id": 1,
    "name": "A green door",
    "price": 12.50,
    "tags": ["home", "green"]
}

now I want to update the same document with new value on "tags" array. 
new value is "yellow".

after updating "yellow " to exist document, My document should display as below.

{
    "id": 1,
    "name": "A green door",
    "price": 12.50,
    "tags": ["home", "green","yellow"]
}

Question : How to merge new value with existing values ?
Some one please guide me on this?
</description><key id="58700988">9839</key><summary>How to append data to exist data in array?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Praveen82</reporter><labels /><created>2015-02-24T06:57:04Z</created><updated>2015-02-24T07:48:49Z</updated><resolved>2015-02-24T07:48:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-02-24T07:02:01Z" id="75708967">Simply reindex the document, it will then update it. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html
</comment><comment author="Praveen82" created="2015-02-24T07:26:41Z" id="75710926">Hi Markwalkom,

  ReIndex the document is overwriting/update the old values to new values.
I mean,I could see  the following json  after reindexing or calling update and upsert api's.

{
"id": 1,
"name": "A green door",
"price": 12.50,
"tags": ["yellow"]
}

In the output json, I have lost my old values from array tag
 "tags": ["home", "green"]

Java Api's to update the exist doc or create new document if not exit.

IndexRequest indexRequest = new IndexRequest("user", "test","1")
            .source(builder);
            UpdateRequest updateRequest = new UpdateRequest("user", "test","1")
            .doc(builder)
            .upsert(indexRequest);  
            client.update(updateRequest).get();
</comment><comment author="dadoonet" created="2015-02-24T07:48:49Z" id="75712867">This is exactly what is happening behind the scene. Update API get the previous doc, merge values, index the full new doc.

BTW please ask questions on the mailing list. This space is for issues or feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separate reporting for memory usage of ordinals?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9838</link><project id="" key="" /><description>Currently, memory usage for global ordinal maps are reported under the fielddata section.  This can be confusing to end users esp. when they have switched from fielddata to doc values.  Have we considered reporting ordinal maps usage in a separate category?
</description><key id="58693332">9838</key><summary>Separate reporting for memory usage of ordinals?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-02-24T04:49:52Z</created><updated>2015-09-28T10:12:11Z</updated><resolved>2015-09-28T10:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-27T22:52:07Z" id="143599796">My problem with that is that ordinal maps are really an implementation detail of how we implement some operations. So either we want to have a stable format of our reporting APIs so that tools like kopf or marvel can use it, or we can add more information but then it needs to be only for human consumption as it might break every time that we change the way that some things are implemented.
</comment><comment author="clintongormley" created="2015-09-28T10:12:11Z" id="143700215">@jpountz++  ordinals are an implementation detail. I prefer the option of stable reporting APIs 

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add String to the default whitelisted receivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9837</link><project id="" key="" /><description>Fixes #8866
</description><key id="58664419">9837</key><summary>Add String to the default whitelisted receivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T23:01:03Z</created><updated>2015-06-07T17:07:24Z</updated><resolved>2015-02-24T00:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-02-24T00:43:15Z" id="75675306">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down `_uid` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9836</link><project id="" key="" /><description>Also, cleanup writePre20Settings so it is shared across all field mappers.

see #8143
</description><key id="58660447">9836</key><summary>Lock down `_uid` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T22:29:38Z</created><updated>2015-06-08T09:04:43Z</updated><resolved>2015-02-23T23:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-23T23:16:26Z" id="75659344">LGTM, left one trivial subjective comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CSV River strange behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9835</link><project id="" key="" /><description>Hi all, 

I have recently replaced my bulk import mechanism (PHP and bulk API) with river csv. What I have noticed so far is a strange behavior that shows up after a certain index size (around 10.000.000 docs and ~1.5G disk size). So when the index is small everything works fine, I have set the bulk_size=1000, concurrent_requests=4 and bulk_threashold=10. After a couple of hours when index become bigger the whole process slows down and the import of .csv files becomes really slow. I have checked the elastic .log files and I figured out that the execution circle (polling time) of the import is interrupted. For instance here is what I get from the logs
#  logs

[2015-02-23 20:08:55,135][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File has been processed 54eb7eed2bbe9.csv.processing
[2015-02-23 20:08:55,136][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File 54eb7eed2bbe9.csv.processing, processed lines 2300
[2015-02-23 20:08:55,137][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Processing file 54eb81de7e37f.csv
[2015-02-23 20:08:55,146][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:09:52,079][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: [SocketTimeoutException[Read timed out]]
[2015-02-23 20:09:54,170][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:09:54,286][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:10:41,762][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:10:41,911][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:10:52,411][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: SocketTimeoutException[Read timed out]
[2015-02-23 20:11:37,582][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:11:37,758][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File has been processed 54eb81de7e37f.csv.processing
[2015-02-23 20:11:37,759][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File 54eb81de7e37f.csv.processing, processed lines 2985
[2015-02-23 20:11:37,759][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Processing file 54eb807bf351c.csv
[2015-02-23 20:11:37,765][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:12:02,830][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: SocketTimeoutException[Read timed out]
[2015-02-23 20:12:30,479][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:12:30,536][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:13:03,132][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: [SocketTimeoutException[Read timed out]]
[2015-02-23 20:13:24,458][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:13:24,581][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:14:03,423][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: SocketTimeoutException[Read timed out]
[2015-02-23 20:14:12,914][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:14:13,010][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File has been processed 54eb807bf351c.csv.processing
[2015-02-23 20:14:13,010][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] File 54eb807bf351c.csv.processing, processed lines 2924
[2015-02-23 20:14:13,011][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Processing file 54eb7eb509a30.csv
[2015-02-23 20:14:13,032][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:15:11,204][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Executed bulk composed of 1000 actions
[2015-02-23 20:15:11,311][INFO ][org.agileworks.elasticsearch.river.csv.CSVRiver] [Domina] [csv][maxweb] Going to execute new bulk composed of 1000 actions
[2015-02-23 20:15:13,741][ERROR][marvel.agent.exporter    ] [Domina] error sending data to [http://[0:0:0:0:0:0:0:0]:9200/.marvel-2015.02.23/_bulk]: SocketTimeoutException[Read timed out]

As you can see there is no accuracy between time periods. The one circle ends at 2015-02-23 20:13:24 and the next start at 2015-02-23 20:14:12. Next you can find the csv river and index settings
# CSV River

``` json
{
    "type": "csv",
    "csv_file": {
        "folder": "/vagrant/CSV/",
        "filename_pattern": ".*\.csv$",
        "poll": "1m",
        "fields": [
            "serverId",
            "duration",
            "requestTime",
            "responseTime",
            "statementType",
            "isRealQuery",
            "queryFailed",
            "sqlQuery",
            "transactionId",
            "clientName",
            "serverName",
            "serverUniqueName",
            "affectedTables",
            "queryError",
            "canonCommandType",
            "canonicalId"
        ],
        "first_line_is_header": "false",
        "concurrent_requests": "4",
        "charset": "UTF-8"
    },
    "index": {
        "index": "maxweb",
        "type": "queries",
        "bulk_size": 1000,
        "bulk_threshold": 10
    }
}
```
# Index mapping

``` json
{
    "mappings": {
        "queries": {
            "transform": {
                "script": "ctx._source['affectedTables'] = ctx._source['affectedTables']?.tokenize(',')",
                "lang": "groovy"
            },
            "_all": {
                "enabled": false
            },
            "_source": {
                "compress": false
            },
            "properties": {
                "affectedTables": {
                    "type": "string",
                    "index": "not_analyzed",
                    "copy_to": [
                        "suggest_tables"
                    ]
                },
                "canonCommandType": {
                    "type": "integer",
                    "index": "no"
                },
                "canonicalId": {
                    "type": "string",
                    "index": "no"
                },
                "clientName": {
                    "type": "string",
                    "index": "no"
                },
                "duration": {
                    "type": "double",
                    "doc_values": true
                },
                "isRealQuery": {
                    "type": "string"
                },
                "queryError": {
                    "type": "string",
                    "index": "no"
                },
                "queryFailed": {
                    "type": "boolean"
                },
                "requestTime": {
                    "type": "double",
                    "doc_values": true
                },
                "responseTime": {
                    "type": "double",
                    "index": "no"
                },
                "serverId": {
                    "type": "long",
                    "doc_values": true
                },
                "serverName": {
                    "type": "string",
                    "index": "no"
                },
                "serverUniqueName": {
                    "type": "string",
                    "index": "no"
                },
                "sqlQuery": {
                    "type": "string",
                    "norms": {
                        "enabled": false
                    }
                },
                "statementType": {
                    "type": "integer",
                    "doc_values": true
                },
                "suggest_tables": {
                    "type": "completion",
                    "analyzer": "simple",
                    "payloads": false,
                    "preserve_separators": true,
                    "preserve_position_increments": true,
                    "max_input_length": 50
                }
            }
        }
    }
}
```
# elasticasearch.yml

index.refresh_interval: 30s
index.translog.flush_threshold_ops: 50000
index.translog.flush_threshold_size: 512mb
indices.fielddata.cache.size: 20%
indices.cache.filter.size: 20%
indices.memory.index_buffer_size: 40%
index.merge.scheduler.max_thread_count : 1
bootstrap.mlockall: true
# /etc/sysconfig/elasticsearch

MAX_LOCKED_MEMORY=unlimited
MAX_OPEN_FILES=65535
ES_JAVA_OPTS=-server
ES_HEAP_SIZE=512m
# index status

``` json
{
    "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
    },
    "indices": {
        "maxweb": {
            "index": {
                "primary_size_in_bytes": 3413521092,
                "size_in_bytes": 3413521092
            },
            "translog": {
                "operations": 4423
            },
            "docs": {
                "num_docs": 17886624,
                "max_doc": 17886624,
                "deleted_docs": 0
            },
            "merges": {
                "current": 0,
                "current_docs": 0,
                "current_size_in_bytes": 0,
                "total": 29,
                "total_time_in_millis": 28204,
                "total_docs": 262490,
                "total_size_in_bytes": 60109517
            },
            "refresh": {
                "total": 158,
                "total_time_in_millis": 15612
            },
            "flush": {
                "total": 3,
                "total_time_in_millis": 23029
            },
            "shards": {
                "0": [{
                    "routing": {
                        "state": "STARTED",
                        "primary": true,
                        "node": "oYGAJctoTTmSU1wD021byA",
                        "relocating_node": null,
                        "shard": 0,
                        "index": "maxweb"
                    },
                    "state": "STARTED",
                    "index": {
                        "size_in_bytes": 3413521092
                    },
                    "translog": {
                        "id": 1424570700536,
                        "operations": 4423
                    },
                    "docs": {
                        "num_docs": 17886624,
                        "max_doc": 17886624,
                        "deleted_docs": 0
                    },
                    "merges": {
                        "current": 0,
                        "current_docs": 0,
                        "current_size_in_bytes": 0,
                        "total": 29,
                        "total_time_in_millis": 28204,
                        "total_docs": 262490,
                        "total_size_in_bytes": 60109517
                    },
                    "refresh": {
                        "total": 158,
                        "total_time_in_millis": 15612
                    },
                    "flush": {
                        "total": 3,
                        "total_time_in_millis": 23029
                    }
                }]
            }
        }
    }
}
```
# index stats

``` json
{
         "primaries": {
            "docs": {
               "count": 17890687,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 3416809219,
               "throttle_time_in_millis": 669
            },
            "indexing": {
               "index_total": 76407,
               "index_time_in_millis": 9773626,
               "index_current": 2,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 0,
               "query_time_in_millis": 0,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 0,
               "current_docs": 0,
               "current_size_in_bytes": 0,
               "total": 31,
               "total_time_in_millis": 29507,
               "total_docs": 281907,
               "total_size_in_bytes": 64514039
            },
            "refresh": {
               "total": 165,
               "total_time_in_millis": 16431
            },
            "flush": {
               "total": 3,
               "total_time_in_millis": 23029
            },
            "warmer": {
               "current": 0,
               "total": 368,
               "total_time_in_millis": 119
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 32864
            },
            "segments": {
               "count": 26,
               "memory_in_bytes": 9095212,
               "index_writer_memory_in_bytes": 324016,
               "index_writer_max_memory_in_bytes": 103887667,
               "version_map_memory_in_bytes": 22792,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 8246,
               "size_in_bytes": 4267399
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            }
         },
         "total": {
            "docs": {
               "count": 17890687,
               "deleted": 0
            },
            "store": {
               "size_in_bytes": 3416809219,
               "throttle_time_in_millis": 669
            },
            "indexing": {
               "index_total": 76407,
               "index_time_in_millis": 9773626,
               "index_current": 2,
               "delete_total": 0,
               "delete_time_in_millis": 0,
               "delete_current": 0,
               "noop_update_total": 0,
               "is_throttled": false,
               "throttle_time_in_millis": 0
            },
            "get": {
               "total": 0,
               "time_in_millis": 0,
               "exists_total": 0,
               "exists_time_in_millis": 0,
               "missing_total": 0,
               "missing_time_in_millis": 0,
               "current": 0
            },
            "search": {
               "open_contexts": 0,
               "query_total": 0,
               "query_time_in_millis": 0,
               "query_current": 0,
               "fetch_total": 0,
               "fetch_time_in_millis": 0,
               "fetch_current": 0
            },
            "merges": {
               "current": 0,
               "current_docs": 0,
               "current_size_in_bytes": 0,
               "total": 31,
               "total_time_in_millis": 29507,
               "total_docs": 281907,
               "total_size_in_bytes": 64514039
            },
            "refresh": {
               "total": 165,
               "total_time_in_millis": 16431
            },
            "flush": {
               "total": 3,
               "total_time_in_millis": 23029
            },
            "warmer": {
               "current": 0,
               "total": 368,
               "total_time_in_millis": 119
            },
            "filter_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "id_cache": {
               "memory_size_in_bytes": 0
            },
            "fielddata": {
               "memory_size_in_bytes": 0,
               "evictions": 0
            },
            "percolate": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0,
               "memory_size_in_bytes": -1,
               "memory_size": "-1b",
               "queries": 0
            },
            "completion": {
               "size_in_bytes": 32864
            },
            "segments": {
               "count": 26,
               "memory_in_bytes": 9095212,
               "index_writer_memory_in_bytes": 324016,
               "index_writer_max_memory_in_bytes": 103887667,
               "version_map_memory_in_bytes": 22792,
               "fixed_bit_set_memory_in_bytes": 0
            },
            "translog": {
               "operations": 8246,
               "size_in_bytes": 4267399
            },
            "suggest": {
               "total": 0,
               "time_in_millis": 0,
               "current": 0
            },
            "query_cache": {
               "memory_size_in_bytes": 0,
               "evictions": 0,
               "hit_count": 0,
               "miss_count": 0
            }
         }
      }
```

**\* Is store.throttle_time_in_millis: 669 cosidered as an important factor? I am asking since I use doc_values on my mapping so maybe I am pushig too much my little VM :)
# Finally I did notice some high I/O traffic with iotop

![screenshot from 2015-02-23 22 48 22](https://cloud.githubusercontent.com/assets/10655938/6336848/4a30d73a-bbae-11e4-9c80-618bcc6d26a2.png)
# Here is the sys info

Vagrant
OS: CentOS release 6.6
RAM: 2GB
CPU: Intel(R) Core(TM) i5-2430M CPU @ 2.40GHz (2 cores)

Thanks a lot for your time

Regards,
Alex

The proxylab team | http://www.proxylab.io/
</description><key id="58651817">9835</key><summary>CSV River strange behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">proxylab</reporter><labels><label>discuss</label></labels><created>2015-02-23T21:31:25Z</created><updated>2015-09-08T08:26:01Z</updated><resolved>2015-09-08T08:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-08T01:18:43Z" id="138401287">@clintongormley should we close this one as 2.0 does not support rivers ?
</comment><comment author="clintongormley" created="2015-09-08T08:26:00Z" id="138474220">Yes. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disable all filter cache per request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9834</link><project id="" key="" /><description>There is an option to disable query cache per request.
I would like to see an option to disable ALL filter cache per request.

We have some request, we're we know that all the filters should not be cached at all, 
but it would be rather painful to configure this for every setting in these cases.

```
curl 'localhost:9200/my_index/_search?filter_cache=fase' 
```
</description><key id="58651100">9834</key><summary>disable all filter cache per request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels /><created>2015-02-23T21:26:22Z</created><updated>2015-02-28T01:28:08Z</updated><resolved>2015-02-28T01:28:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T01:28:08Z" id="76502325">Hi @julianhille 

The results of the whole query are either cached or not, but filter caching is much more fine-grained, hence the need to specify filter caching per filter.  That said, you can probably forget about it as we'll be moving to automated filter caching based on reuse: https://github.com/elasticsearch/elasticsearch/pull/8573
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>small bug in kibana</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9833</link><project id="" key="" /><description>Hi I found a small bug in kibana 3 and 4,
when i have Brackets in user name field example 
facebook (US)
facebook (DE)
in kibana object is break it to us ,de 
you can see in the picture below
![image](https://cloud.githubusercontent.com/assets/11095042/6337126/5d8dddda-bbb0-11e4-993a-c16195975323.png)
</description><key id="58647635">9833</key><summary>small bug in kibana</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oferoh</reporter><labels><label>non-issue</label></labels><created>2015-02-23T21:04:55Z</created><updated>2015-02-24T10:56:57Z</updated><resolved>2015-02-24T07:37:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-23T23:14:31Z" id="75659040">I'm sorry but I don't really understand what error you are seeing.
The screen capture does not mention the same text you are describing.

That said:
- if you have a Kibana issue, you'd better open it in Kibana project
- I think it's more a question about your mapping but still unsure as I don't understand what is exactly your issue.
</comment><comment author="oferoh" created="2015-02-24T04:48:04Z" id="75699030">Thank you David,

the example wasn't so good,

the ADV_CAMPAIGN_NAME  = "KandCo(FB)" - the table below 
in the table above you can see that for some reason it split the name "KandCo"  , "fb" same count 8034

Is that clear enough?
</comment><comment author="dadoonet" created="2015-02-24T07:37:35Z" id="75711845">Yes. So I confirm it's not a bug.
You should change your mapping and mark this field as not_analyzed.
</comment><comment author="oferoh" created="2015-02-24T07:54:22Z" id="75713402">Dear David 
im not sure you right please see my code,
all string field are not_analyzed,

DELETE atlas_events
DELETE _river
PUT _river/atlas_events/_meta
{
    "type" : "jdbc",
    "jdbc": {
        "url":"jdbc:sqlserver://localhost;databaseName=gaya",
        "user":"elasticsearch",
        "password":"elasticsearch",
        "sql":"select \* from get_data_from_exasol",
        "index" : "atlas_events",
        "type" : "clicks",
            "properties" : {
            "ADV_PROJECT_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    }
  }

```
}
```

}
PUT atlas_events/_mapping/clicks
{
  "properties" : {
    "ADV_PROJECT_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "BROWSER" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "CREATIVE_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "DEVICE_CLASS_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "DEVICE_GROUP_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "OS" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "PRT_CAMPAIGN_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    },
    "PRT_PROJECT_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
    }
  }
}

![image](https://cloud.githubusercontent.com/assets/11095042/6345157/069c88b8-bc0b-11e4-87ca-a7ca542f687e.png)
</comment><comment author="dadoonet" created="2015-02-24T08:10:18Z" id="75714959">PUT atlas_events/_mapping/clicks Is incorrect.

PUT atlas_events/clicks/_mapping

Check your mapping with GET atlas_events/clicks/_mapping
</comment><comment author="oferoh" created="2015-02-24T10:21:42Z" id="75731672">still not working 
I did you what you suggest,

![image](https://cloud.githubusercontent.com/assets/11095042/6346492/01fa66f0-bc19-11e4-94fa-ad71e4164686.png)

![image](https://cloud.githubusercontent.com/assets/11095042/6346399/52f146ce-bc18-11e4-88b1-7bb4591ae219.png)

![image](https://cloud.githubusercontent.com/assets/11095042/6346416/7c99ee40-bc18-11e4-9f25-a1f12d54f216.png)
</comment><comment author="dadoonet" created="2015-02-24T10:26:48Z" id="75732349">What gives `GET atlas_events/clicks/_mapping`
</comment><comment author="oferoh" created="2015-02-24T10:52:57Z" id="75735860">![image](https://cloud.githubusercontent.com/assets/11095042/6347876/0c7cb5e6-bc24-11e4-9e36-23c551ef2819.png)
</comment><comment author="oferoh" created="2015-02-24T10:56:06Z" id="75736306">found the bug,:) 

missed the 
,
     "ADV_CAMPAIGN_NAME" : {
      "type" :    "string",
      "index":    "not_analyzed"
</comment><comment author="oferoh" created="2015-02-24T10:56:17Z" id="75736332">Sorry thank you ,
</comment><comment author="oferoh" created="2015-02-24T10:56:57Z" id="75736421">![image](https://cloud.githubusercontent.com/assets/11095042/6347948/9b3d0c4a-bc24-11e4-9dab-13a9a41d7cf2.png)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing @Override annotations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9832</link><project id="" key="" /><description>These help a lot when refactoring, upgrading lucene, etc, and
can prevent code duplication (as you get a compile error for outdated stuff).
</description><key id="58639734">9832</key><summary>Add missing @Override annotations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T20:01:05Z</created><updated>2015-06-08T13:51:13Z</updated><resolved>2015-02-23T22:08:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-23T20:03:04Z" id="75620172">+1
</comment><comment author="rjernst" created="2015-02-23T20:38:28Z" id="75626715">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>security manager for groovy?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9831</link><project id="" key="" /><description>Was a bummer to see that dynamic groovy support has been knocked out. I read that the groovy team was consulted, but is there really nothing that can be done? I saw this article from a few years back, though I am not familiar with the java SecurityManager:

http://www.chrismoos.com/2010/03/24/groovy-scripts-and-jvm-security/

Would be great to have a way to do arithmetic and basic loops etc. in scripts. I'm using groovy for a scripted metric agg, and I don't see any way to do that anymore other than storing 4 separate groovy scripts on each node (init/map/combine/reduce). I would even say that groovy scripting would be great even if no classes were supported other than a predefined list used for these basic calculations. I understand the dynamic nature might make it impossible to ensure security, but if it could be done with severe limitations, I'd still say it's tremendously valuable.
</description><key id="58634842">9831</key><summary>security manager for groovy?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kallin</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-02-23T19:25:34Z</created><updated>2015-12-05T17:27:51Z</updated><resolved>2015-12-05T17:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T17:27:51Z" id="162225651">The Java security manager is now used extensively in Elasticsearch.  And on top of that, we're writing our own scripting language which can be enabled by default  #13084.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default timezone per index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9830</link><project id="" key="" /><description>I'm doing a lot of time-based reporting using elastic, and have to remember to always set the offset to -5 (EST) whenever I do something like a date-range agg, otherwise I get documents from the wrong day in my buckets. It would be nice if I could specify a default timezone per index, that would be used for all time-based filters/aggs etc, but that could be overridden if necessary.
</description><key id="58634121">9830</key><summary>default timezone per index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kallin</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-02-23T19:19:58Z</created><updated>2015-12-05T17:26:16Z</updated><resolved>2015-12-05T17:26:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T17:26:16Z" id="162225422">Sorry it has taken a while to get to this.  Unfortunately, this wouldn't work on multi-index search.  All timezones need to be the same across all indices, otherwise the results would be meaningless.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modify default shard allocation algorithm to use index for primary only indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9829</link><project id="" key="" /><description>By default, primary shard allocation does not account for an index, only the number of primaries and replicas per node.  This does not cause even load in the following scenario.
1. Allocate 2 indexes, with primary shards only (0 replicas) across a cluster equal to the node count.  I.E 3 nodes, 3 shards.
2. Index 1 receives very little load
3. Index 2 receives very heavy load

In the current scenario, primary shards are not allocated evenly across nodes within an index.  In the primary shard only scenario above, this leads to uneven load distribution across the cluster.  Nodes should own an even, or as close to even distribution of primary shards as possible when no replicas are present.
</description><key id="58624355">9829</key><summary>Modify default shard allocation algorithm to use index for primary only indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tnine</reporter><labels /><created>2015-02-23T18:07:13Z</created><updated>2015-02-23T21:20:55Z</updated><resolved>2015-02-23T21:20:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-02-23T18:45:30Z" id="75604734">The simplest way to resolve this is to set `index.routing.allocation.total_shards_per_node` to 1 on high load indexes so that their shards are distributed evenly.  Its probably better than some fancy weighting algorithms because its pretty simple and creates a hard and fast rule.  My cluster has ~1800 indexes and only about 20 of them produce enough load that I set this on them.  It works well.
</comment><comment author="tnine" created="2015-02-23T21:20:55Z" id="75636299">Awesome, so we need to set this per index then?  Thank you for your help!  Closing the issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs: Fix rounding issue using `date_histogram` with `pre_zone_adjust_large_interval`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9828</link><project id="" key="" /><description>Fixes an issue with using `date_histogram` aggregation for month intervals
in combination with `pre_zone_adjust_large_interval` reported in #8209.

Closes #8209
</description><key id="58621865">9828</key><summary>Aggs: Fix rounding issue using `date_histogram` with `pre_zone_adjust_large_interval`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2015-02-23T17:47:59Z</created><updated>2015-03-19T10:57:32Z</updated><resolved>2015-02-23T18:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-23T17:48:47Z" id="75593535">LGTM
</comment><comment author="cbuescher" created="2015-02-23T17:50:55Z" id="75593931">Great, will merge. I will also put just the tests on master although issue is fixed there by the `date_histogram`cleanup.
</comment><comment author="jpountz" created="2015-02-23T17:57:12Z" id="75595206">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some more simple fs cleanups.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9827</link><project id="" key="" /><description>a few more cleanups.

the isSameFile() sha1 logic is just flat out buggy, in addition to wasting cpu.
</description><key id="58617751">9827</key><summary>Some more simple fs cleanups.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T17:19:37Z</created><updated>2015-06-07T10:17:10Z</updated><resolved>2015-02-23T18:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-23T17:28:57Z" id="75589462">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score: undo "Remove explanation of query score from functions"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9826</link><project id="" key="" /><description>This adds the Explanation to the explainScore() again. It is needed
because the explanation of script functions will otherwise not contain
an explanation of _score if boost mode is set to replace.
</description><key id="58614244">9826</key><summary>function_score: undo "Remove explanation of query score from functions"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.5.1</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T16:55:36Z</created><updated>2015-04-09T08:48:03Z</updated><resolved>2015-03-24T14:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-23T17:37:40Z" id="75591217">LGTM
</comment><comment author="brwe" created="2015-02-25T11:54:20Z" id="75948153">@maxjakob @jpountz thanks for the review! Addressed all comments. Can I push?
</comment><comment author="maxjakob" created="2015-02-25T14:25:21Z" id="75967722">:+1: 
</comment><comment author="maxjakob" created="2015-03-23T11:18:38Z" id="84954852">Can we expect this to be in version 1.4.5?
</comment><comment author="s1monw" created="2015-03-23T12:10:31Z" id="84967238">@brwe LGTM
</comment><comment author="s1monw" created="2015-03-24T09:27:15Z" id="85419688">@brwe please merge this into 1.x,  1.5 and master
</comment><comment author="brwe" created="2015-03-24T14:52:14Z" id="85538884">@maxjakob sorry we did not get this in 1.5 and promised I'll add labels next time to remind me. It will be in 1.5.1. There will be no further 1.4.X release.
</comment><comment author="maxjakob" created="2015-03-25T17:11:50Z" id="86125688">@brwe I see, 1.5.1 then, that's fine. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Define good heuristics to use `collect_mode: breadth_first`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9825</link><project id="" key="" /><description>`breadth_first` does not work when sub aggregations require scores, which is why we never use it by default today. However, we recently added the ability to know whether an aggregator requires scores or not (for now it just assumes that if a script is used then scripts are needed but we can improve it in the future, for instance I think we could have the information with lucene-expressions at least).

This means that we could now use `breadth_first` by default, we just need to figure out a good heuristic of when this is going to make the terms agg faster and/or more memory-efficient.
</description><key id="58607347">9825</key><summary>Define good heuristics to use `collect_mode: breadth_first`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-02-23T16:10:34Z</created><updated>2016-08-26T13:30:29Z</updated><resolved>2016-06-16T12:28:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="speedplane" created="2015-10-29T01:55:42Z" id="152050161">It would be fantastic if the `breadth_first` / `depth_first` choice could be handled on the ES side. 

Short of that, it would be nice to have some insight into whether one should be using `breadth_first` instead of `depth_first`. Right now, it's hard to know which one you should use other than testing them both and seeing which is faster. For example, if a counter returned how many temporary buckets were created in order to build the aggregation, one could get some insight into the internals of the aggregation and better optimize it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>One shard on 1.3.4 -&gt; 1.4.4 gone bad (edit: corrupted index)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9824</link><project id="" key="" /><description>I Installed a new instance of ES 1.4.4 and copied the /data folder into the new instance. It started to get  some errors like: 
`failed to fetch index version after copying it over`(...)`Preexisting corrupted index`(...)`caused by: CorruptIndexException[codec header mismatch: actual header=0 vs expected header=1071082519`

Trying to fix this, I removed this index on the file system and tried a restore...initially the log reported ok
`restore [repo_name_2015-02-23_0000] is done` but it sent more errors right away
(...)`sending failed shard for`(...)`reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[index_name][0] failed recovery]; nested: IndexShardRestoreFailedException[[index_name][0] restore failed]; nested: IndexShardRestoreFailedException[[index_name][0] failed to restore snapshot [repo_name_2015-02-23_0000]]; nested: IndexShardRestoreFailedException[[index_name][0] Failed to recover index]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1aj2bdu actual=pebgem`

I checked for the status of the backup and it was ok:

`"index_name" : {
        "shards_stats" : {
          "initializing" : 0,
          "started" : 0,
          "finalizing" : 0,
          "done" : 1,
          "failed" : 0,
          "total" : 1
        },
        "stats" : {
          "number_of_files" : 1,
          "processed_files" : 1,
          "total_size_in_bytes" : 1529,
          "processed_size_in_bytes" : 1529,
          "start_time_in_millis" : 1424646178596,
          "time_in_millis" : 8659
        },
        "shards" : {
          "0" : {
            "stage" : "DONE",
            "stats" : {
              "number_of_files" : 1,
              "processed_files" : 1,
              "total_size_in_bytes" : 1529,
              "processed_size_in_bytes" : 1529,
              "start_time_in_millis" : 1424646178596,
              "time_in_millis" : 8659
            }
          }
        }
      },`

This happened in only one shard/index.

Any advice?

Other notes: 
1 node
93 indices
102 shards
~150 million documents
~30 GB
</description><key id="58607017">9824</key><summary>One shard on 1.3.4 -&gt; 1.4.4 gone bad (edit: corrupted index)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tostasqb</reporter><labels /><created>2015-02-23T16:08:16Z</created><updated>2015-02-28T01:16:58Z</updated><resolved>2015-02-28T01:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tostasqb" created="2015-02-23T16:39:17Z" id="75578821">I have just re-installed ES 1.3.4 and tried to restore the same backup with the same result.
Guess this index is really corrupted ?. 

My question now is, how come only now this problem is showing? I back up my data daily and it was being shown on a client's application until this upgrade started...not a single error (that I have caught) until now.
</comment><comment author="clintongormley" created="2015-02-28T01:16:58Z" id="76501586">Hi @tostasqb 

The 1.4 branch has many more checksum checks than before, so it is quite possible that a previously undetected corruption is only now being found.  

Sorry that this has happened to you, but given that your backups are also corrupt, I think the only thing left to do is to delete that shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs: breadth_first does not work on several levels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9823</link><project id="" key="" /><description>Probably introduced in #9544 since I can reproduce the issue on master but not 1.x. The following recreation generates an error: `Deferred collectors cannot be collected directly. They must be collected through the recording wrapper`.

```
DELETE test

PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "a": {
          "type": "string",
          "index": "no",
          "doc_values": "yes"
        },
        "b": {
          "type": "string",
          "index": "no",
          "doc_values": "yes"
        },
        "c": {
          "type": "string",
          "index": "no",
          "doc_values": "yes"
        }
      }
    }
  }
}

PUT test/test/1
{
  "a": "foo",
  "b": "bar",
  "c": "baz"
}

GET test/_search
{
  "aggs": {
    "a_terms": {
      "terms": {
        "field": "a",
        "collect_mode": "breadth_first"
      },
      "aggs": {
        "b_terms": {
          "terms": {
            "field": "b",
            "collect_mode": "breadth_first"
          },
          "aggs": {
            "c_terms": {
              "terms": {
                "field": "c"
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="58606536">9823</key><summary>Aggs: breadth_first does not work on several levels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T16:05:02Z</created><updated>2015-04-09T10:09:48Z</updated><resolved>2015-04-09T10:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NoSuchFileException _1gl.si in OldIndexBackwardsCompatibilityTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9822</link><project id="" key="" /><description>OldIndexBackCompatTests fails (rarely, won't repro for me after beasting) sometimes (http://build-us-00.elasticsearch.org/job/es_core_1x_centos/3304/console) with this nasty exception:

```
java.nio.file.NoSuchFileException: _1gl.si in dir=least_used[rate_limited(default(mmapfs(/mnt/jenkins/workspace/es_core_1x_centos/target/J0/data/TEST-ip-10-255-15-175-CHILD_VM=[0]-CLUSTER_SEED=[3276434121362593305]-HASH=[87FD12A7499C81]/nodes/1/indices/test/0/index),niofs(/mnt/jenkins/workspace/es_core_1x_centos/target/J0/data/TEST-ip-10-255-15-175-CHILD_VM=[0]-CLUSTER_SEED=[3276434121362593305]-HASH=[87FD12A7499C81]/nodes/1/indices/test/0/index)), type=MERGE, rate=20.0)]
    at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:603)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:688)
    at org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader.read(Lucene3xSegmentInfoReader.java:106)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:358)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:454)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:906)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:92)
    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:158)
    at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
    at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:648)
    at org.elasticsearch.index.engine.InternalEngine.snapshotIndex(InternalEngine.java:774)
    at org.elasticsearch.test.store.MockFSDirectoryService$1.beforeIndexShardClosed(MockFSDirectoryService.java:93)
    at org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexShardClosed(InternalIndicesLifecycle.java:193)
    at org.elasticsearch.index.IndexService.closeShardInjector(IndexService.java:391)
    at org.elasticsearch.index.IndexService.removeShard(IndexService.java:382)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedShards(IndicesClusterStateService.java:294)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:187)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:466)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:182)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:152)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

It's always with the 0.20.6 index (= Lucene 3.6.2).  The exc is bad: it means Lucene corrupted the 3.x index on first commit with Lucene 4.x, because it failed to write an upgraded .si file yet did reference it.

Digging with @s1monw we uncovered one abuse case where Lucene could do this https://issues.apache.org/jira/browse/LUCENE-6279 (which is a "won't fix") where if a directory is "dirty" (has unrelated index files from a previous index), Lucene will fail to upgrade the .si files and the index will be corrupt.

So ... is ES re-using directories here?  The test make new nodes, sharing the local data dir, to replicate to, and then later closes them before testing the next index.  Each node that starts up will try to find a free (not locked) sub-dir of the data dir, and then that may have leftover files from the last time that index was replicated to that node?

If this is happening ... I think we need to somehow fix ES to never share dirty index directories?
</description><key id="58592531">9822</key><summary>NoSuchFileException _1gl.si in OldIndexBackwardsCompatibilityTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2015-02-23T14:30:59Z</created><updated>2015-12-05T17:24:25Z</updated><resolved>2015-12-05T17:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-02-23T23:15:27Z" id="75659193">One thing that freaks me out a bit about the 0.20.6.zip back compat test index is it has big segment names e.g. _1gz, _1gm, whereas the other indices all have tiny (brand new index) segments like _0, _1, ... not sure it's related but it's sort of weird :)  @rjernst any ideas?
</comment><comment author="mikemccand" created="2015-02-23T23:17:55Z" id="75659561">I turned on logging for all file deletions: https://github.com/elasticsearch/elasticsearch/commit/50875f13a78e03886b27dd84c2e98713a5afbcfd

Maybe it sheds light on the next failure ...
</comment><comment author="rmuir" created="2015-02-23T23:17:58Z" id="75659569">I think i added this one. I altered ryan's code locally to ensure this index had non-cfs segments. means i told it to make a lot more docs. You can see with ls -l that its 10x bigger than the other test indexes.

Particularly, most test indexes are trivially small and only contain cfs segments from flush. I needed this one to have non-cfs segments so that we could test some of the corner cases present there (like old adler32 checksums for term dictionaries that seek'ed on write)
</comment><comment author="mikemccand" created="2015-02-23T23:19:30Z" id="75659823">@rmuir ahh good, thanks for the explanation.  It's good that it's non-trivial ... maybe that's related here (this test only seems to fail with 0.20.6 index even though there are other 3.x indices).
</comment><comment author="rmuir" created="2015-02-23T23:41:35Z" id="75662831">I looked and none of the 0.20.x indexes have any CFS. Yet all of the later ones do (and no non-CFS). None of the indexes have any deletes, or documents in translogs.

So I don't remember what my issue was and why i needed to hack the script to produce more documents, but I am sure its because my expected test would not fail without me doing it... I generally hide from python.

related: #9143
</comment><comment author="mikemccand" created="2015-02-24T17:52:50Z" id="75808629">The "log all file deletions" (test.engine.lucene.iw.ifd logger) we turned on for this test was very helpful!  It revealed a different Lucene bug  that looks to be the cause here: https://issues.apache.org/jira/browse/LUCENE-6287

I suspect because this particular index (0.20.6) had many more segments and therefore needed merging, it tickled this Lucene bug.
</comment><comment author="clintongormley" created="2015-12-05T17:24:25Z" id="162224927">Old issue, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>wrong type for bulk updates and deletes on closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9821</link><project id="" key="" /><description>```
$ curl -XDELETE localhost:9200/test; echo
{"error":"IndexMissingException[[test] missing]","status":404}
$ curl -XPUT localhost:9200/test/test/1 -d '{"title": "test document"}'; echo
{"_index":"test","_type":"test","_id":"1","_version":1,"created":true}
$ curl -XPOST localhost:9200/test/_close; echo
{"acknowledged":true}
$ curl -XPOST localhost:9200/_bulk -d '{"update" : {"_index": "test", "_type": "type1", "_id": "1"}}
&gt; {"doc": {"field1": "value1"}}
&gt; '; echo
{"took":1,"errors":true,"items":[{"index":{"_index":"test","_type":"type1","_id":"1","status":403,"error":"IndexClosedException[[test] closed]"}}]} 
```

The key should be `update` instead of `index`. Similar things happen for deletes. The fix against current master is trivial:

```
diff --git a/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java b/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
index 6e48349..ffd49df 100644
--- a/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
+++ b/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
@@ -168,13 +168,13 @@ public class TransportBulkAction extends HandledTransportAction&lt;BulkRequest, Bul
         } else if (request instanceof DeleteRequest) {
             DeleteRequest deleteRequest = (DeleteRequest) request;
             if (index.equals(deleteRequest.index())) {
-                responses.set(idx, new BulkItemResponse(idx, "index", new BulkItemResponse.Failure(deleteRequest.index(), deleteRequest.type(), deleteRequest.id(), e)));
+                responses.set(idx, new BulkItemResponse(idx, "delete", new BulkItemResponse.Failure(deleteRequest.index(), deleteRequest.type(), deleteRequest.id(), e)));
                 return true;
             }
         } else if (request instanceof UpdateRequest) {
             UpdateRequest updateRequest = (UpdateRequest) request;
             if (index.equals(updateRequest.index())) {
-                responses.set(idx, new BulkItemResponse(idx, "index", new BulkItemResponse.Failure(updateRequest.index(), updateRequest.type(), updateRequest.id(), e)));
+                responses.set(idx, new BulkItemResponse(idx, "update", new BulkItemResponse.Failure(updateRequest.index(), updateRequest.type(), updateRequest.id(), e)));
                 return true;
             }
         } else {
```

However, why is the type reported at all? Is it allowed to differ? Could there be multiple responses to a single action?
</description><key id="58590573">9821</key><summary>wrong type for bulk updates and deletes on closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">robx</reporter><labels><label>:Bulk</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-02-23T14:15:53Z</created><updated>2015-07-07T07:29:31Z</updated><resolved>2015-07-07T07:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T01:05:03Z" id="76500814">Hi @robx 

Thanks for reporting.  Would you be up for sending a PR?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Template mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9820</link><project id="" key="" /><description>Is there a way of dynamically generating index name in index-template mapping? Like we do in logstash by prefixing date for instance.

&gt; curl -XPUT http://localhost:9200/template/mytemplate -d '{
&gt; "template" : "someindex*",
&gt; "order": 1,
&gt; "settings" : {
&gt; "index.store.compress.stored" : true
&gt; },
&gt; "mappings" : {
&gt; "_default" : {
&gt; "_all" : {"enabled" : false}
&gt; }
&gt; }
&gt; }
&gt; '
</description><key id="58590460">9820</key><summary>Template mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-02-23T14:15:01Z</created><updated>2015-02-28T01:00:54Z</updated><resolved>2015-02-28T01:00:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T01:00:54Z" id="76500483">No there isn't - you index into a particular index name, from that we can match templates with wildcards, but we can't change the name of the index itself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type conversion possible in index template?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9819</link><project id="" key="" /><description>Is it possible to do type conversion input documents? For exampled if the document has a field that is in quotes as "100", can it be converted into integer? I can't seem to make it work. 

&gt; curl -XPUT http://localhost:9200/_template/mytemplate -d '{
&gt;   "template" : "someindex*",
&gt;   "order":    1,
&gt;  "settings" : {
&gt;      "index.cache.field.type" : "soft",
&gt;     "index.refresh_interval" : "5s",
&gt;      "index.store.compress.stored" : true
&gt; },
&gt;  "mappings" : {
&gt; "_default_" : {
&gt; "_all" : {"enabled" : false},
&gt; "properties" : {
&gt; "item_id" :{"type": "string", "index": "not_analyzed" },
&gt; "item_name" : {"type": "string", "index": "not_analyzed" },
&gt; "value" : {"type": "integer"},
&gt; "item_location" : {
&gt; "properties" : {
&gt; "location" : {
&gt; "type" : "geo_point"
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }
&gt; '

Input document 

&gt; {
&gt;  "item_someid":"item_somename",
&gt;  "some_value":"1",
&gt;  "item_location":
&gt;      {"location":"37.8098,-122.418"}
&gt; }
</description><key id="58589940">9819</key><summary>Type conversion possible in index template?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-02-23T14:10:46Z</created><updated>2015-02-28T00:59:48Z</updated><resolved>2015-02-28T00:59:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:59:48Z" id="76500380">Geo-points have to be mapped explicitly, or you can use name-based matching to determine which fields should be geo-points.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch is aggregating documents with all versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9818</link><project id="" key="" /><description>I had an index where it contained a field with empty values. I filled them with some logic. Earlier, 358 docs were empty. After indexing again, only 7 empty documents remain, but when I do 

```
GET /zomato/test/_search
{
  "query": {
    "term": {
      "zomatocity": {
        "value": ""
      }
    }
  }
  , "aggs": {
    "NAME": {
      "terms": {
        "field": "zomatocity",
        "size": 1000
      }
    }
  }
}
```

I get this

```
{
   "took": 12,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 358,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "NAME": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 106,
         "buckets": [
            {
               "key": "",
               "doc_count": 358
            },
            {
               "key": "praha",
               "doc_count": 56
            },
            {
               "key": "ncr",
               "doc_count": 50
            },
            {
               "key": "bangalore",
               "doc_count": 35
            },
            {
               "key": "mumbai",
               "doc_count": 28
            },
            {
               "key": "grande-lisboa",
               "doc_count": 24
            },
            {
               "key": "jakarta",
               "doc_count": 18
            },
            {
               "key": "dubai",
               "doc_count": 14
            },
            {
               "key": "brno",
               "doc_count": 12
            },
            {
               "key": "london",
               "doc_count": 8
            }
         ]
      }
   }
}
```

What is happening here? Empty documents are also showing again in the facet count. Please help. I'm using elasticsearch - 1.4.2
</description><key id="58589483">9818</key><summary>elasticsearch is aggregating documents with all versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">confusedsouls</reporter><labels><label>feedback_needed</label></labels><created>2015-02-23T14:07:18Z</created><updated>2015-04-26T19:36:58Z</updated><resolved>2015-04-26T19:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-26T22:46:22Z" id="76291778">Sorry I don't understand the problem that you are seeing. Please use the mailing-list for asking questions about elasticsearch.
</comment><comment author="clintongormley" created="2015-02-28T03:41:53Z" id="76508474">@confusedsouls it's much easier to understand if you show us a complete recreation of the problem, rather than just describing it.
</comment><comment author="clintongormley" created="2015-04-26T19:36:58Z" id="96423674">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix errors reported by error-prone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9817</link><project id="" key="" /><description>I compiled elasticsearch with error-prone
(https://github.com/google/error-prone) and it reported
13 errors. This commit fixes those errors.

The errors were related to the following:
- http://errorprone.info/bugpattern/SuppressWarningsDeprecated
- http://errorprone.info/bugpattern/StaticAccessedFromInstance
- http://errorprone.info/bugpattern/CheckReturnValue
- http://errorprone.info/bugpattern/TryFailThrowable
</description><key id="58589185">9817</key><summary>Fix errors reported by error-prone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">sebastianmonte</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T14:04:34Z</created><updated>2015-03-29T10:31:18Z</updated><resolved>2015-03-29T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T09:17:23Z" id="84285289">HI @sebastianmonte thanks, looks good! Could you please sign our [CLA](https://www.elastic.co/contributor-agreement) so we can pull this in?
</comment><comment author="sebastianmonte" created="2015-03-25T16:15:27Z" id="86101122">Hi, thanks for the feedback. Actually I have already signed it (after the pull request). Should I do it again?
</comment><comment author="javanna" created="2015-03-26T08:22:06Z" id="86394225">Hi @sebastianmonte I am sorry, maybe something went wrong, but I can't find your signed CLA. Would you mind trying again?
</comment><comment author="sebastianmonte" created="2015-03-26T08:43:30Z" id="86406215">Hi @javanna , I signed the agreement few minutes ago.
</comment><comment author="javanna" created="2015-03-29T10:31:18Z" id="87388189">Thanks @sebastianmonte merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect query at start makes some aggregations stop working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9816</link><project id="" key="" /><description>It took me a little while to narrow down this bug.

I have a cluster with various indexes and types, and I have a query that performs aggregations, that only works on one index/type and fails miserably if executed on anything else.

This is the query I use:

```
{
  "aggs": {
    "genre": {
      "scripted_metric": {
        "init_script": "_agg.genre = [:]",
        "map_script": "genres=doc['genre'][0].split('/'); for (i=0; i&lt;genres.size(); i++) _agg.genre.put(genres[i],_agg.genre.get (genres[i],0)+1)",
        "reduce_script": "retval = [:];for (i in 0.._aggs.genre.size()) {for ( e in _aggs.genre[i] ) {retval.put(e.key,retval.get(e.key,0)+e.value)}};return retval;"
      }
    },
    "decade": {
      "scripted_metric": {
        "init_script": "_agg.decade = [:]",
        "map_script": "try {dec = ((doc['releasedYear'][0].toInteger()/10).toInteger()*10).toString();_agg.decade.put(dec,_agg.decade.get(dec,0)+1);} catch (e) {}",
        "reduce_script": "retval = [:];for (i in 0.._aggs.decade.size()) {for ( e in _aggs.decade[i] ) {retval.put(e.key,retval.get(e.key,0)+e.value)}}; retval.remove('0'); return retval;"
      }
    },
    "movieRating": {
      "terms": {
        "field": "movieRating"
      }
    },
    "releasedYear": {
      "terms": {
        "field": "releasedYear"
      }
    }
  },
  "query": {
    "filtered": {
      "filter": {},
      "query": {
        "match_all": {}
      }
    }
  }
}
```

And this is an example of document inside the type

```
{
"_index": "edge",
"_type": "AssetsDetails",
"_id": "10026",
"_score": 1,
"_source": {
"_id": 10026,
"fantasyViolence": false,
"assetType": "FTP",
"castFirstName": [
"James Buchli"
,
" Toni Myers"
],
"closedCaption": true,
"contentId": 10026,
"contentType": "ASSET",
"createDate": 1422669583,
"creditsFirstName": [ ],
"description": "From the unique vantage point of 200 miles above Earth's surface, we see how natural forces - volcanoes, earthquakes and hurricanes - affect our world, and how a powerful new force - humankind - has begun to alter the face of the planet. From Amazon rain forests to Serengeti grasslands, Blue Planet inspires a new appreciation of life on Earth, our only home.",
"distributor": "someone",
"dolbyStereo": false,
"downloadAllowed": true,
"encodingType": "MPEG-2 TS",
"encripted": false,
"endVodDate": 2019686400,
"episodeTitle": "",
"genre": "Documentary/Short",
"hd": true,
"isSubscriptionPart": true,
"language": false,
"madefortv": false,
"modifyDate": 1422669583,
"movieRating": "G",
"movieRatingId": 2,
"price": 199,
"protocol": "HLS",
"providerassetid": "ZVOD2013100819121602",
"reducedDesc": "From the unique vantage point of 200 miles above Earth's surface, we see how natural forces - volcanoes, earthquakes and hurricanes - affect our world, and how a powerful new force - humankind - has begun to alter the face of the planet. From Amazon rain forests to Serengeti grasslands, Blue Planet inspires a new appreciation of life on Earth, our only home.",
"reducedTitle": "IMAX - Blue Planet",
"releasedYear": "1990",
"rentTime": 2,
"resourceId": 144,
"rtsp": true,
"runtime": 2400,
"sexRating": false,
"spFile": "http://192.168.9.12/inside_job/inside_job_36.m3u8",
"sppath": "",
"startVodDate": 1293840000,
"startvodsetdate": 1293840000,
"startvodsetid": 2,
"status": true,
"studioroyaltyflatrate": 0,
"studioroyaltyminimum": 0,
"studioroyaltypercent": 0,
"subtitleLanguage": "",
"suggestiveDialog": false,
"supplierid": -1,
"imageId": 21,
"title": "6_IMAX Blue Planet",
"trickPlay": true,
"tvRatingId": -1,
"tvnAssetId": "6_ZVOD2013100819121601",
"tvnassetname": "IMAX_Blue_Planet_title",
"tvnbillingid": "927278",
"tvnprovider": "",
"tvnprovidercontenttier": 0,
"tvnproviderid": "",
"tvnsuggestedprice": 199,
"tvnversionmajor": 1,
"tvnversionminor": 0,
"customerassettype": "MOD",
"version": 1,
"violence": false,
"vsFile": "http://192.168.9.12/inside_job/inside_job_36.m3u8",
"uri": "",
"serverIp": "hlsurl",
"serverPort": "0",
"serverType": "HLS"
}
}
```

Now the problem is that if I try to run the query on `/`, I will get some exceptions

```
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.06][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,741][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.09][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.09][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,748][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.18][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.18][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,754][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.12][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.12][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,769][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.19][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.19][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,770][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.16][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.16][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,776][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.13][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.13][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,788][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.23][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.23][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,862][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.11][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.11][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,864][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.10][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.10][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:34,869][DEBUG][action.search.type       ] [Dark Beast] [.marvel-2015.02.20][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [.marvel-2015.02.20][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [genre] in mapping with types []]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:35,070][DEBUG][action.search.type       ] [Dark Beast] [edge][0], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [edge][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method split() on null object]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:35,081][DEBUG][action.search.type       ] [Dark Beast] [edge][4], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [edge][4]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method split() on null object]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:35,086][DEBUG][action.search.type       ] [Dark Beast] [edge][1], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [edge][1]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method split() on null object]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:35,088][DEBUG][action.search.type       ] [Dark Beast] [edge][2], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [edge][2]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method split() on null object]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
[2015-02-23 14:41:35,099][DEBUG][action.search.type       ] [Dark Beast] [edge][3], node[OXjTmwuySZSw-isia4Yxbw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e4a3c94]
org.elasticsearch.search.query.QueryPhaseExecutionException: [edge][3]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: NullPointerException[Cannot invoke method split() on null object]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
        at org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.collect(ScriptedMetricAggregator.java:96)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
```

After the exceptions, if I run the query on the correct index/type, the aggregations on the years and decades will work fine, however if I run the query on `/` as first thing after starting elasticsearch, then subsequent correct queries will fail to aggregate years and decades, which will only contain empty data structures in the response.

Let me know if you need more information.
</description><key id="58587592">9816</key><summary>Incorrect query at start makes some aggregations stop working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">ltworf</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-02-23T13:50:54Z</created><updated>2015-12-14T20:09:08Z</updated><resolved>2015-12-14T20:09:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:54:27Z" id="76500016">@colings86 it looks like the script is being cached along with the field datatype lookup?  Please could you take a look.
</comment><comment author="clintongormley" created="2015-12-05T17:23:41Z" id="162224896">@colings86 has this already been fixed?
</comment><comment author="colings86" created="2015-12-08T15:00:24Z" id="162907771">@clintongormley I'm not aware of any fix that has been merged for this. I have also just tried to reproduce it without success.

@ltworf are you still able to reproduce this bug on the latest version of Elasticsearch (currently 2.1)?
</comment><comment author="ltworf" created="2015-12-08T16:08:34Z" id="162930788">I no longer work on that project. I will unsubscribe this.
</comment><comment author="clintongormley" created="2015-12-14T20:09:08Z" id="164545036">OK - I'll close for now as presumed fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed deprecated script parameter names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9815</link><project id="" key="" /><description>This change removes the deprecated script parameter names ('file', 'id', and 'scriptField').
It also removes the ability to load file scripts using the 'script' parameter. File scripts should be loaded using the 'script_file' parameter only.
</description><key id="58586111">9815</key><summary>Removed deprecated script parameter names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T13:37:41Z</created><updated>2015-06-06T17:27:49Z</updated><resolved>2015-02-23T14:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-02-23T13:47:27Z" id="75542904">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range filters, date math and time_zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9814</link><project id="" key="" /><description>Hi guys. 

I've stumbled upon an issue. When specifying the `time_zone` parameters in a range filter where `gte` and `lt` are set using datemath, the time zone is not taken into account when rounding!

The following is part of a filters aggregation, and I expect to be given the week 4 weeks ago, starting on a
sunday at 23:00 UTC time, and ending on sunday 23:00 UTC time with a full week in between. 

```
{
  "filters": {
    "4w": {
      "range": {
        "timestamp": {
          "lt": "now-4w+1w/w",
          "gte": "now-4w/w",
          "time_zone": "+01:00"
        }
      }
    }
  }
}
```

But no, after inspecting the docs that are returned, there are no documents returned between the first sunday 23:00 and monday 00:00 UTC. There are however documents returned between sunday 23:00 and monday 00:00 UTC at the end of the interval.

Is this a bug? Should not timezones be included when doing rounding in date math?
</description><key id="58571463">9814</key><summary>Range filters, date math and time_zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">mewwts</reporter><labels><label>:Dates</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T11:02:57Z</created><updated>2015-03-03T09:03:36Z</updated><resolved>2015-02-25T18:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-02-25T09:03:24Z" id="75926820">Will have a look since this might or might not be related to the rounding issues recently solved in #9790.
</comment><comment author="cbuescher" created="2015-02-25T09:04:02Z" id="75926902">@mewwts Which version are you running?
</comment><comment author="cbuescher" created="2015-02-25T10:29:22Z" id="75937882">From a first glance: the rounding in DateMathParser (which seems to be used in range filter) is not related to the TimeZoneRounding which was subject of #9790, so I was wrong in assuming the two issues are related. Seems like rounding in DateMathParser does not take into account time zone correctly.
</comment><comment author="mewwts" created="2015-02-25T13:21:40Z" id="75958273">Hi @cbuescher, thanks for your reply and quick fix!?
I'm running 1.4.2. I was going to take a look at this myself, but I can see you beat me to it!
</comment><comment author="mewwts" created="2015-02-25T18:08:40Z" id="76020128">Hands down, greatest response ever. Thanks @cbuescher. Now the wait for 1.4.5, I guess?
</comment><comment author="cbuescher" created="2015-02-25T18:11:25Z" id="76020705">So far only on out main branch, but will merge with the current 1.4 branch, so after that's happened it will go into 1.4.5. 
</comment><comment author="mewwts" created="2015-02-25T18:33:30Z" id="76025412">:+1: Thanks, @cbuescher.
</comment><comment author="cbuescher" created="2015-03-02T11:35:53Z" id="76697508">On branch 1.4 with dff19cb and on 1.x with 8391b51
</comment><comment author="mewwts" created="2015-03-03T09:03:25Z" id="76909372">Great @cbuescher!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Don't reuse single node tests node instance across tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9813</link><project id="" key="" /><description>Similar to the shared cluster we should not reuse the node from
singlenodetest across tests.
</description><key id="58567992">9813</key><summary>[TEST] Don't reuse single node tests node instance across tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T10:29:14Z</created><updated>2015-03-19T10:58:29Z</updated><resolved>2015-02-23T20:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-23T16:13:58Z" id="75573639">LGTM
</comment><comment author="rjernst" created="2015-02-23T18:20:46Z" id="75599779">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Default units for Geo Distance Agg is meters not km</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9812</link><project id="" key="" /><description>Docs need changing to reflect that the default distance unit is meters.

https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/aggregations/bucket/geodistance-aggregation.asciidoc
</description><key id="58565712">9812</key><summary>[DOCS] Default units for Geo Distance Agg is meters not km</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>docs</label><label>v1.3.10</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T10:07:22Z</created><updated>2015-02-28T00:45:45Z</updated><resolved>2015-02-28T00:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RecoveryState clean up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9811</link><project id="" key="" /><description>To support the `_recovery` API, the recovery process keeps track of current progress in a class called RecoveryState. This class currently have some issues, mostly around concurrency (see #6644 ). This PR cleans it up as well as other issues around it:
- Make the Index subsection API cleaner:
  - remove redundant information - all calculation is done based on the underlying file map
  - clearer definition of what is what: total files, vs reused files (local files that match the source) vs recovered files (copied over). % based progress is reported based on recovered files only.
  - cleaned up json response to match other API (sadly this breaks the structure). We now properly report human values for dates and other units.
  - Add more robust unit testing
- Detail flag was passed along as state (it's now a ToXContent param)
- State lookup during reporting is now always done via the IndexShard , no more fall backs to many other classes.
- Cleanup APIs around time and move the little computations to the state class as opposed to doing them out of the API

I also improved error messages out of the REST testing infra for things I run into.

Given the BWC nature of the change I'm on the fence whether this should go into 1.4.X - it does fix a concurrency issue and makes things consistent where they weren't.

Closes #6644 
</description><key id="58560932">9811</key><summary>RecoveryState clean up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>breaking</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-23T09:18:22Z</created><updated>2015-06-06T17:21:38Z</updated><resolved>2015-02-25T16:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-23T09:21:42Z" id="75510609">oooh nice I will review...
</comment><comment author="s1monw" created="2015-02-24T10:26:49Z" id="75732351">left some comments.... I think this should not go into 1.4 since it's pretty big though...
</comment><comment author="bleskes" created="2015-02-24T16:37:19Z" id="75791775">@s1monw I pushed another round, with more simplifications.
</comment><comment author="s1monw" created="2015-02-25T14:06:34Z" id="75964588">I had one minor comments - LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>creating index using uri</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9810</link><project id="" key="" /><description>hi,
i am trying to create an index through url without using crul command but it giving bad request,
this is code snippet which i am using               

 String url = "http://localhost:9200/product/apparel/300";
        URL obj = new URL(url);
        HttpURLConnection con = (HttpURLConnection) obj.openConnection();

```
    //add reuqest header
    con.setRequestMethod("POST");
    con.setRequestProperty("Accept-Language", "en-US,en;q=0.5");

    String urlParameters = "type:slide,quantity:2";
```
</description><key id="58554005">9810</key><summary>creating index using uri</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vahith</reporter><labels /><created>2015-02-23T07:56:22Z</created><updated>2015-02-23T09:04:07Z</updated><resolved>2015-02-23T09:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-23T09:04:07Z" id="75508624">Please ask usage questions on the mailing list.
Your code has a lot of errors i'd say:

POST vs PUT
No real payload but parameters (incorrects)
Not a JSON doc

BTW why not using elasticsearch Java API?

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Will doc values support ip type in future?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9809</link><project id="" key="" /><description>Wondering if doc values will support the ip type in the future. Running a query for unique client ip's across large timespans is a fairly common web log query. We could use not_analyzed string fields but we'd then lose searching via cidr notation.

Thanks. 
</description><key id="58553213">9809</key><summary>Will doc values support ip type in future?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">stonith</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2015-02-23T07:50:41Z</created><updated>2015-04-04T15:51:49Z</updated><resolved>2015-04-04T15:51:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-03-16T17:33:48Z" id="81828996">A quick look at the code looks like doc values should already be supported on ip fields. Is it just a documentation issue or did you try to use doc values on an ip field and it failed?
</comment><comment author="stonith" created="2015-03-16T18:17:52Z" id="81850945">I guess it's a documentation issue, I didn't bother trying it as the docs didn't list IP types as a supported type. I can submit a PR to update the doc if it's already supported.

http://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggestion needed for handling renames</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9808</link><project id="" key="" /><description>I have following requirements,
For every project that I index has 2 values
1) Id: Each project has a unique ID
2) Name: Name of the project. But Name of the project can change.

For every project, we index a document with different attributes. Say each document in index will have an Attribute A1 with different values. Each attribute has an Id and Name. Name again can change.

Oveall index would look like
ProjectName ProjectGUID A1Name A1Id File
P1                 P1ID             T1     T1-ID        f11
P1                 P1ID             T1     T1-ID        f12
P1                 P1ID             T1     T1-ID        f13
P1                 P1ID             T2     T2-ID        f21
.
.

post rename of T2 to T3 index will look like,
P1                 P1ID             T3     T2-ID        f21

So Name has changed but not Id.

During query I aggregate on different attributes. 

Problem:
1) User always queries with Project name.
2) During displaying results I want to display results based upon different attributes. Say for attribute A1 with name T1 3 documents exist. For T2 5 documents exist.

If I index document with name query is very simple. But as we support renames, any rename means indexing whole project.
So another option is to store ids in documents and store id to name map separately. For project rename we can use Alias with id as filter. For aggregations, one of the options I could see is term lookup filters. But I need to 
1) read about aggregating on term lookup rather than term itself 
2) cost associated with term lookup.

Are any other alternatives available with elasticsearch? I believe this should be a common problem. Any suggestions,
</description><key id="58549836">9808</key><summary>Suggestion needed for handling renames</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">satishmallik</reporter><labels /><created>2015-02-23T07:04:12Z</created><updated>2015-02-23T07:08:31Z</updated><resolved>2015-02-23T07:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-02-23T07:08:31Z" id="75497274">Please use the google group for these sorts of questions - https://groups.google.com/forum/#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove redundant fs metadata ops.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9807</link><project id="" key="" /><description>Some easy cleanups:
Files.exists(f) &amp;&amp; Files.isDirectory(f) -&gt; Files.isDirectory(f)
if (Files.exists(f)) Files.delete(f) -&gt; Files.deleteIfExists(f)
if (!Files.exists(f)) Files.createDirectories(f) -&gt; Files.createDirectories(f)

In a few places where successive i/o ops are done against the same file, convert
to Files.readAttributes().
</description><key id="58511783">9807</key><summary>Remove redundant fs metadata ops.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-22T18:07:50Z</created><updated>2015-06-08T13:51:38Z</updated><resolved>2015-02-22T22:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-22T18:17:27Z" id="75449037">LGTM
</comment><comment author="kimchy" created="2015-02-22T22:51:04Z" id="75468661">@rmuir should be also backport to 1.x where applicable?
</comment><comment author="rmuir" created="2015-02-22T22:52:36Z" id="75468839">These are NIO.2 cleanups, when i look at 1.x its still using java.io.File. This makes things tricky...
</comment><comment author="kimchy" created="2015-02-22T22:53:24Z" id="75468938">I see....
</comment><comment author="rmuir" created="2015-02-22T22:55:24Z" id="75469114">Lemme look more. we can do a little bit, but it won't be as good.

For example File.isDirectory implies File.exists just like it does for NIO.2 
</comment><comment author="kimchy" created="2015-02-22T22:58:28Z" id="75469256">yea, make sense. My answer was short since maybe to improve the file system API usage in 1.x, we might need to backport the NIO.2 work, so am on the fence as to what is the best path...
</comment><comment author="rmuir" created="2015-02-22T22:59:54Z" id="75469330">I think that is somewhat tricky since on 1.x we have lucene 4, which itself is not using NIO.2. But we can at least remove any redundant exists() for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make restore either incremental or full (recovery too?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9806</link><project id="" key="" /><description>today we might reuse files when we restore a snapshot and make recoveries. In snapshot and restore the restore operation should be full or incremental but if the diff between target and source has files that are different in the target we should force a full restore instead of overwriting existing files. Otherwise we might run into situations where future .si files reference the worng files.
</description><key id="58501251">9806</key><summary>Make restore either incremental or full (recovery too?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-02-22T12:34:28Z</created><updated>2017-07-20T18:37:15Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-26T19:12:10Z" id="263080415">@abeyad can this issue be closed yet?</comment><comment author="abeyad" created="2016-11-27T19:34:35Z" id="263141956">@clintongormley 
&gt; if the diff between target and source has files that are different in the target we should force a full restore instead of overwriting existing files.

We don't currently do this, so this still needs to be done.

&gt; Otherwise we might run into situations where future .si files reference the worng files

@s1monw could you elaborate on what you mean by this?  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>script_score doesn't take a script filename as argument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9805</link><project id="" key="" /><description>The [`script score`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#_script_score) function doesn't allow for the scoring script to be loaded from a file. This is, however, paramount for sensitive applications, as described in [this blogpost](http://www.elasticsearch.org/blog/running-groovy-scripts-without-dynamic-scripting/).

@polyfractal has pointed out that the bug is that [`ScoreFunction`](https://github.com/elasticsearch/elasticsearch/blob/1.x/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java#L80-L84) only uses the default inline `script` argument. It should also accept the `file` argument.

It would be nice if the `file` argument could be accepted consistently by functions that use inline scripts. For security's sake.
</description><key id="58461672">9805</key><summary>script_score doesn't take a script filename as argument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aolieman</reporter><labels /><created>2015-02-21T14:50:03Z</created><updated>2015-02-28T00:39:56Z</updated><resolved>2015-02-28T00:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:34:57Z" id="76498552">Hi @aolieman 

Thanks for reporting.  This has been fixed in #7977
</comment><comment author="aolieman" created="2015-02-28T00:39:56Z" id="76498932">Wonderful! I'll be looking out for the fix in the change log.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fielddata loading for _parent is not lazy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9804</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/fielddata-formats.html#_fielddata_loading

It looks like fielddata loading for _parent is not lazy and is eager by default, which means that the fielddata cache (id cache) is populated as parent/child documents are indexed.  Global ordinals for _parent however do not get loaded eagerly.  Would be nice to document this behavior.
</description><key id="58434800">9804</key><summary>Fielddata loading for _parent is not lazy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-02-21T01:55:59Z</created><updated>2015-02-22T22:17:56Z</updated><resolved>2015-02-22T22:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>support sorting in aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9803</link><project id="" key="" /><description>Would it be possible to support a sort option in an alias.

My problem is that I have a sequence id in my index and I want my clients to default sort by that id.

thanks,
</description><key id="58432074">9803</key><summary>support sorting in aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bhenderson</reporter><labels /><created>2015-02-21T01:09:09Z</created><updated>2015-02-28T00:32:52Z</updated><resolved>2015-02-28T00:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:32:52Z" id="76498411">Hi @bhenderson 

While yes, it would be possible, I don't like the idea of adding this complexity to aliases.  The options available in aliases are intended for enforcing rules to give the illusion of multi-tenancy in a shared index.

A default sort is just a preference (one of many that could be requested) and then raises the question of eg what happens if the user specifies another sort order? should it be allowed or denied? What if they want to apply a secondary sort etc. And who knows what other use-case specific configurations could be requested?

This feels like it should be applied in a proxy layer in front of Elasticsearch instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kernel32LibraryTests creates JNA proxy classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9802</link><project id="" key="" /><description>With the current tests policy in place, this hits SecurityException (e.g. http://build-us-00.elasticsearch.org/job/es_core_master_window-2012/1004/console).

Does the functionality or just the test need to create these proxies? Can we avoid it? Otherwise, we can add this permission, but it is a little bit strange that JNA for e.g. unix mlockall does not need to create proxies but on windows it does.

 Caused by: java.security.AccessControlException: access denied ("java.lang.reflect.ReflectPermission" "newProxyInPackage.org.elasticsearch.common.jna")
  1&gt;     java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
  1&gt;     java.security.AccessController.checkPermission(AccessController.java:884)
  1&gt;     java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
  1&gt;     [...java.lang.reflect.*]
  1&gt;     com.sun.jna.Native.loadLibrary(Native.java:415)
  1&gt;     com.sun.jna.Native.loadLibrary(Native.java:391)
  1&gt;     org.elasticsearch.common.jna.Kernel32Library.&lt;init&gt;(Kernel32Library.java:50)
  1&gt;     org.elasticsearch.common.jna.Kernel32Library.&lt;init&gt;(Kernel32Library.java:36)
  1&gt;     org.elasticsearch.common.jna.Kernel32Library$Holder.&lt;clinit&gt;(Kernel32Library.java:45)
  1&gt;     org.elasticsearch.common.jna.Kernel32Library.getInstance(Kernel32Library.java:60)
  1&gt;     org.elasticsearch.common.jna.Kernel32LibraryTests.testKernel32Library(Kernel32LibraryTests.java:74)
</description><key id="58422388">9802</key><summary>Kernel32LibraryTests creates JNA proxy classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T22:57:57Z</created><updated>2015-03-02T09:09:43Z</updated><resolved>2015-03-02T09:09:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-23T08:53:26Z" id="75507415">this is related to #8993 
</comment><comment author="s1monw" created="2015-02-23T08:53:52Z" id="75507474">@tlrx can you comment on this. I really wonder if the native library we load here is worth the trouble?
</comment><comment author="tlrx" created="2015-02-23T11:10:30Z" id="75524306">@rmuir thanks for spotting it.

&gt; Does the functionality or just the test need to create these proxies? Can we avoid it? Otherwise, we can add this permission, but it is a little bit strange that JNA for e.g. unix mlockall does not need to create proxies but on windows it does.

The functionality makes a native call on Windows to add a console control handler. The current implementation uses proxification which is the best documented way to make a native call on Windows with the JNA library. The other way is to use [Direct Mapping](https://github.com/twall/jna/blob/master/www/DirectMapping.md) like what is done in the unix/mlockall function.

If these proxy creation is really annoying, I think I can change the code to use Direct Mapping in Kernel32Library too. 

Besides this, I didn't see a test for the `Native.tryMlockall()` method. I added one locally and it fails with SecurityManager enabled and the current test policy: 

```
Caused by: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "createSecurityManager")
```

&gt; I really wonder if the native library we load here is worth the trouble?

#8993 relies on the [SetConsoleCtrlHandler function](https://msdn.microsoft.com/en-us/library/windows/desktop/ms683242%28v=vs.85%29.aspx) and was added to ensure that a node is correctly shutdown on Windows when the user closes the console that starts Elasticsearch (either by clicking Close on the console window's window menu, or by clicking the End Task button command from Task Manager, or with a TASKILL command like a stop script would do). I really think that's very important to gracefully stop the node... Without this, it's like kill -9 the JVM.

So I suggest to keep the native library (it will also be used for the mlockall equivalent on Windows) and unify the way the native calls are made in CLibrary/Kernel32Libray and limit the permissions to grant in the policy file, and finally add a test for Native.tryMlockall().
</comment><comment author="rmuir" created="2015-02-23T12:59:31Z" id="75536745">&gt; So I suggest to keep the native library (it will also be used for the mlockall equivalent on Windows) and unify the way the native calls are made in CLibrary/Kernel32Libray and limit the permissions to grant in the policy file, and finally add a test for Native.tryMlockall().

+1 that sounds great. 
</comment><comment author="s1monw" created="2015-02-23T13:42:46Z" id="75542267">cool stuff!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve safety when deleting files from the store</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9801</link><project id="" key="" /><description>Today if we delete files from the index directory we never acquire the
write lock. Yet, for safety reasons we should get the write lock before
we modify / delete any files. Where we can we should leave the deletion
to the index writer and only delete that are necessary to delete ourself.
</description><key id="58414253">9801</key><summary>Improve safety when deleting files from the store</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T21:40:28Z</created><updated>2015-06-07T17:35:48Z</updated><resolved>2015-02-24T20:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-24T11:24:39Z" id="75739840">I updated this to fix other places as well where we need the write lock and added more safety. @mikemccand @rmuir can you take a look please
</comment><comment author="rmuir" created="2015-02-24T13:02:14Z" id="75752221">+1
</comment><comment author="mikemccand" created="2015-02-24T13:43:57Z" id="75757907">Left a couple comments, otherwise LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing quotes breaks stuff</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9800</link><project id="" key="" /><description>The following is allowed:

```
curl -XPUT localhost:9200/test9/test9/1 -d '{ test: 0 }'
```

However it upsets downstream systems as it's not quite JSON, eg. the Perl Search::Elasticsearch module, Kibana, and possibly others. Should the _source be rewritten to make it proper JSON (like when you submit YAML), or should this be banned entirely, maybe in 2.0?
</description><key id="58410984">9800</key><summary>Missing quotes breaks stuff</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:REST</label><label>discuss</label><label>low hanging fruit</label></labels><created>2015-02-20T21:11:26Z</created><updated>2016-05-24T10:05:41Z</updated><resolved>2016-05-24T10:05:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:15:25Z" id="76496972">yeah, i'm torn about it.  i think the idea of relaxed parsing was to make it less onerous to try things out from the command line with curl.  That said, with autocomplete tools like Sense available, it is much easier to type correct JSON.

We're certainly leaning towards being stricter, because it avoids silent errors.
</comment><comment author="GlenRSmith" created="2015-07-07T08:44:14Z" id="119124130">Maybe there should be a configuration to enforce strict json. If so, I think it would make most sense for the default behavior to be "strict", and have it over-ridden in es.yml in the distros. This would make it a little more likely, I think, that people would remember to change the config before deploying to production.
Even with that modification, maybe in "relaxed" mode, ES should do what @jimmyjones2 suggests. There would be some performance overhead, but, if implemented properly, removing the "json relaxed" config would eliminate that.
So, where does the setting belong?
Maybe something on "index." which can be set on index creation. Or, come to think of it, something in the mapping, analogous to "dynamic". I'm hesitant to suggest something really literal, like "require_quoted_keys", because maybe there would be a future case for including other relaxations, like tolerating missing commas between key/value pairs.
</comment><comment author="clintongormley" created="2015-12-05T17:21:19Z" id="162224730">Really this is just a setting in Jackson: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java#L46

Should we allow this to be changed via settings, or even just require quoted field names?
</comment><comment author="jimmyjones2" created="2015-12-05T20:11:34Z" id="162242663">I'd vote for quoted by default, for one this behaviour wasted an hour of my life plus Elasticsearch sends the HTTP header `Content-Type: application/json` in the response, which is incorrect if quotes are missing.
</comment><comment author="rjernst" created="2015-12-05T21:08:50Z" id="162247121">I dont think we should make it configurable or possible, given that it is not part of the json spec, and it would tie us to a particular json library (jackson).
</comment><comment author="clintongormley" created="2015-12-05T22:36:12Z" id="162254172">Actually, the JSON spec requires double quotes around field names.  http://www.json.org/
</comment><comment author="rjernst" created="2015-12-05T22:38:31Z" id="162254255">Exactly, my comment was meant to say the json spec does not allow the absence of quotes. 
</comment><comment author="clintongormley" created="2015-12-05T22:39:50Z" id="162254314">But currently we accept JSON without quotes around field names, so changing this would be a breaking change.
</comment><comment author="clintongormley" created="2015-12-05T22:41:29Z" id="162254394">Honestly, i think the only thing it would break is JSON entered on the command line.  Any clients will be quoting fields correctly already.
</comment><comment author="jimmyjones2" created="2015-12-09T19:23:44Z" id="163364134">Would you accept a patch for 3.0 to change ALLOW_UNQUOTED_FIELD_NAMES to false?
</comment><comment author="dakrone" created="2015-12-09T19:46:37Z" id="163370159">I'm +1 on changing Jackson to be strict (the `ALLOW_UNQUOTED_FIELD_NAMES` change), what do you think @clintongormley?
</comment><comment author="nik9000" created="2015-12-09T19:47:02Z" id="163370264">&gt; Would you accept a patch for 3.0 to change ALLOW_UNQUOTED_FIELD_NAMES to false?

Yes please. Make sure to put it in the breaking changes list in "migrate_3_0.asciidoc". I'm working on reindex now which we could use to ease the pain of anyone who has invalid json in their index.
</comment><comment author="clintongormley" created="2016-05-24T10:05:41Z" id="221224054">Closed by #15351
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodeEnv should lock all shards for an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9799</link><project id="" key="" /><description>Today locking all shards only locks the shards that are present on
the node or that still have a shard directory. This can lead to odd
behavior if another shard that doesn't exist yet is allocated while
all shards are supposed to be locked.
</description><key id="58408482">9799</key><summary>NodeEnv should lock all shards for an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T20:49:43Z</created><updated>2015-06-06T19:12:51Z</updated><resolved>2015-02-20T21:11:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-20T20:56:07Z" id="75317651">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc to fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9798</link><project id="" key="" /><description /><key id="58405972">9798</key><summary>Update percolate.asciidoc to fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Asimov4</reporter><labels><label>docs</label><label>v1.4.5</label><label>v1.5.0</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T20:26:46Z</created><updated>2015-03-21T09:24:12Z</updated><resolved>2015-03-21T09:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-21T09:23:57Z" id="84286860">Thanks @Asimov4 merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle truncated translog gracefully</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9797</link><project id="" key="" /><description>We used to handle truncated translogs in a better manner (assuming that
the node was killed halfway through writing an operation and discarding
the last operation). This brings back that behavior by catching an
`EOFException` during the stream reading and throwing a
`TruncatedTranslogException` which can be safely ignored in
`IndexShardGateway`.

Fixes #9699
</description><key id="58387273">9797</key><summary>Handle truncated translog gracefully</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Translog</label><label>blocker</label><label>enhancement</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T17:52:58Z</created><updated>2015-06-07T17:40:56Z</updated><resolved>2015-03-03T15:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-20T21:03:34Z" id="75318728">@dakrone this looks good, I wonder if in the test we should write a translog and then munge it to remove the last few bytes to simulate transactions? that way we don't have a file that might eventually not support backward comp. on written ops?

I think its a good change since it restores the previous behavior, would be good to work on the per op better translog handling as well eventually that will make this more resilient (can't find the issue now)
</comment><comment author="dakrone" created="2015-02-23T22:09:33Z" id="75647879">@kimchy sounds good, I added another test that tests truncated translog files (for a non-pre-generated translog)
</comment><comment author="kimchy" created="2015-03-03T11:53:05Z" id="76932156">@dakrone it looks great, I would consider adding another test, that reads the data in a similar manner that the gateway does (reading it one by one, not based on locations), and seeing that we get a truncated exception once we work no a truncated translog? other than that, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding tests for TimezoneRounding in 1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9796</link><project id="" key="" /><description>1. Test using the pre_zone_adjust_large_interval option.
2. Test rounding of ambiguous timestamps in local time (after DST switch)
</description><key id="58386387">9796</key><summary>Adding tests for TimezoneRounding in 1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">orenash</reporter><labels><label>:Aggregations</label><label>test</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T17:46:04Z</created><updated>2015-02-25T15:31:11Z</updated><resolved>2015-02-23T11:09:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="orenash" created="2015-02-20T17:50:39Z" id="75286492">Those tests were written while examining issue #9491.
They all pass after the fix in #9790.
</comment><comment author="cbuescher" created="2015-02-23T10:55:11Z" id="75522448">@orenash Thanks, LGTM. Will merge these along with #9790 on 1.x and 1.4 branch.
</comment><comment author="cbuescher" created="2015-02-23T11:09:01Z" id="75524107">Merged commit with 1.4 (4abb3c6) and 1.x (e0c1c1c)
</comment><comment author="javanna" created="2015-02-25T08:35:08Z" id="75923493">@cbuescher can you label the PR please too?
</comment><comment author="cbuescher" created="2015-02-25T15:30:44Z" id="75980632">@javanna These tests are on 1.4 branch after the 1.4.4 release, also on master with 30fd70f. Set labels accordingly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scroll.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9795</link><project id="" key="" /><description>Replaced occurences of scroll_id for _scroll_id, as the former does not work (and requests ignore it silently, so it has been difficult for me to catch)
</description><key id="58382471">9795</key><summary>Update scroll.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">oscarsj</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-02-20T17:16:11Z</created><updated>2015-06-19T15:41:42Z</updated><resolved>2015-06-19T15:41:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-28T00:07:03Z" id="76496244">Hi @oscarsj 

Thanks for fixing the error.  I've added two comments.  Please could you fix those and push again. Also, please would you sign the CLA? http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2015-06-19T15:41:41Z" id="113552742">No feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportService does not raise exceptions correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9794</link><project id="" key="" /><description>Hy

I found a bug in the ES Java client (TransportClient):

After one node gets unavailable, our application takes several minutes to reconnect to ES again.
After some investigations it seems like the TransportService can not raise exceptions to the RequestHolders.
raiseNodeDisconnected() cannot match the disconnected node (Object from listedNodes in TransportClientNodesService) with the RequestHolder node (Object from nodes in TransportClientNodesService).

The nodes get matched by their Id but listedNodes contain dummy ids ("#transport#-1") while nodes contains real ids from the cluster.
</description><key id="58377844">9794</key><summary>TransportService does not raise exceptions correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polgl</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2015-02-20T16:43:46Z</created><updated>2016-11-27T21:05:15Z</updated><resolved>2016-11-27T21:05:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polgl" created="2015-03-09T14:22:27Z" id="77861859">This is a BUG in the Java API: the list with listed nodes never gets updated and thus never contains the right IDs. 
</comment><comment author="clintongormley" created="2015-12-05T17:14:47Z" id="162224339">Hi @polgl 

Sorry it has taken a while to look at this.  Are you still seeing this problem in Elasticsearch 2.0?  If so, any chance you'd like to send a PR?
</comment><comment author="polgl" created="2016-01-11T18:13:46Z" id="170638422">Hi @clintongormley here is the diff of a quick fix
I did not manage to write a test with the right conditions for this (netty client, one slow/ blocking request, killing the server without normal shutdown)
This change ensures that the DiscoveryNode object that was connected before is used for disconnecting.

```
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
@@ -54,6 +54,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
+import java.util.HashMap;
 import java.util.Set;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.CountDownLatch;
@@ -86,6 +87,7 @@ public class TransportClientNodesService extends AbstractComponent {

     private final Object mutex = new Object();

+    private volatile Map&lt;DiscoveryNode, DiscoveryNode&gt; discoveredNodesMap = new HashMap&lt;&gt;();
     private volatile List&lt;DiscoveryNode&gt; nodes = Collections.emptyList();
     private volatile List&lt;DiscoveryNode&gt; filteredNodes = Collections.emptyList();

@@ -372,7 +374,9 @@ public class TransportClientNodesService extends AbstractComponent {
                     } else if (livenessResponse.getDiscoveryNode() != null) {
                         // use discovered information but do keep the original transport address, so people can control which address is exactly used.
                         DiscoveryNode nodeWithInfo = livenessResponse.getDiscoveryNode();
-                        newNodes.add(new DiscoveryNode(nodeWithInfo.name(), nodeWithInfo.id(), nodeWithInfo.getHostName(), nodeWithInfo.getHostAddress(), listedNode.address(), nodeWithInfo.attributes(), nodeWithInfo.version()));
+                        DiscoveryNode discoveredNode = new DiscoveryNode(nodeWithInfo.name(), nodeWithInfo.id(), nodeWithInfo.getHostName(), nodeWithInfo.getHostAddress(), listedNode.address(), nodeWithInfo.attributes(), nodeWithInfo.version());
+                        newNodes.add(discoveredNode);
+                        discoveredNodesMap.put(listedNode, discoveredNode);
                     } else {
                         // although we asked for one node, our target may not have completed initialization yet and doesn't have cluster nodes
                         logger.debug("node {} didn't return any discovery info, temporarily using transport discovery node", listedNode);
@@ -381,6 +385,10 @@ public class TransportClientNodesService extends AbstractComponent {
                 } catch (Throwable e) {
                     logger.info("failed to get node info for {}, disconnecting...", e, listedNode);
                     transportService.disconnectFromNode(listedNode);
+                    // disconnect from the node that was connected before
+                    if (discoveredNodesMap.containsKey(listedNode)) {
+                        transportService.disconnectFromNode(discoveredNodesMap.get(listedNode));
+                    }
                 }
             }
```
</comment><comment author="clintongormley" created="2016-01-11T19:14:24Z" id="170657810">@bleskes could you take a look at this please?
</comment><comment author="bleskes" created="2016-01-27T12:22:29Z" id="175591059">@polgl I looked at this and I'm a bit puzzled. Here is what I  understand:

1) During the issue of the liveness test the target node disconnects.
2) This somehow makes the liveness request not being canceled but just hang.
3) That causes new nodes not to be picked up.

I don't see how the node disconnect event is missed based on the client code alone - the disconnect even should come for both the "official" node and the one that is connected "light" for the liveness call. Can you elaborate? Also - which version of ES are you referring to? it might something subtle I miss by looking at the wrong version.
</comment><comment author="polgl" created="2016-01-27T19:25:00Z" id="175810167">sorry I may went to fast in my explanation:

1) The node disconnects between 2 liveness tests
2) an other request (search request for example) that was started before but is not yet finished hangs
3) causes the search request not to be canceled and/or retried 

I added a graph the show the sequence of events in the client and the server:

![graph](https://docs.google.com/drawings/d/13sqI2AJ67ljEpGYG-wy6qgaibWCxL8IMlZ3imT2RYAc/pub?w=960&amp;amp;h=720)

What I understood from the code:
1) add node at with a given address
`org.elasticsearch.client.transport.TransportClientNodesService#addTransportAddresses`
creates `DiscoveryNode object 1` with id = `#transport#-0` 

2) test liveness
`org.elasticsearch.client.transport.TransportClientNodesService.SimpleNodeSampler#doSample`
tests liveness for `DiscoveryNode object 1` with id = `#transport#-0` 
gets response with real id and creates `DiscoveryNode object 2` with id = `abcd` 

3) send search request
`org.elasticsearch.transport.TransportService.RequestHolder` holds reference to `DiscoveryNode object 2`  with id = `abcd` 

4) connection loss

5) test liveness again
`org.elasticsearch.client.transport.TransportClientNodesService.SimpleNodeSampler#doSample`
tests liveness for `DiscoveryNode object 1` with id = `#transport#-0` 
node is not available
disconnects from `DiscoveryNode object 1`
`org.elasticsearch.transport.TransportService.Adapter#raiseNodeDisconnected` does not match the `RequestHolder` from 3)
request from 3) is not stopped
</comment><comment author="bleskes" created="2016-02-01T13:00:47Z" id="177962568">Thanks for clarifying @polgl . I think I understand where things go wrong. The NodeSampler is mainly in charge of maintaining a connection to the specified nodes (by sending a liveness command) and validating that the nodes on the other side indeed belong to the same cluster. It isn't responsible for maintaining the nodes that the client is currently connected to and cancelling in flight request. Both of those are the responsibility of the TransportService, which should have detected the connection loss (socket break) in step 4 above and failed all inflight requests to the node.  Something else is going on... 
</comment><comment author="clintongormley" created="2016-11-26T19:11:05Z" id="263080363">@bleskes did you ever get anywhere with this?</comment><comment author="bleskes" created="2016-11-27T21:05:14Z" id="263147467">@clintongormley not really - something was wrong but not exactly fitting the discreption. I think we can close it for now and reopen if something similar pops up - it will be hard to continue debugging without a concrete issue. @polgl if this is still happening for you, feel free to reopen.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Awkward syntax when using a file script in a terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9793</link><project id="" key="" /><description>Following the release of Elasticsearch 1.4.3 and the disclosed vulnerability of Groovy, I have disabled the Groovy sandbox.
But in some places I used Groovy in some queries, for example:

``` javascript
{ 
  "aggs": { 
    "result": { 
      "terms": { 
        "script": "if doc['protocol']?.value doc['protocol'].value + '/' + doc['ip_dstport'].value else doc['ip_dstport'].value" 
      } 
    } 
  } 
}
```

I tried to externalize the groovy script, store in in a file under /etc/elasticsearch/scripts/protocol_label.groovy, then use this aggregation instead, but it doesn't work:

``` javascript
{ 
  "aggs": { 
    "result": { 
      "terms": { 
        "file": "protocol_label", 
        "language": "groovy" 
      } 
    } 
  } 
}
```

I found a workaround, but it does not look clean:

``` javascript
{ 
  "script_fields": { 
    "dummy": { 
      "file": "protocol_label", 
      "language": "groovy" 
    } 
  },
   "aggs": { 
    "result": { 
      "terms": { 
        "file": "protocol_label", 
        "language": "groovy" 
      } 
    } 
  } 
}
```
</description><key id="58377345">9793</key><summary>Awkward syntax when using a file script in a terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ludovicc</reporter><labels /><created>2015-02-20T16:40:07Z</created><updated>2015-02-23T10:18:12Z</updated><resolved>2015-02-23T10:18:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-02-23T10:17:50Z" id="75517919">@ludovicc in the aggregations you need to define file_scripts using the `script_file` parameter instead of `file`. https://github.com/elasticsearch/elasticsearch/pull/7977 standardises this across all the scriptable APIs and will be available from version 1.5

Apologies for the confusion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Queue size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9792</link><project id="" key="" /><description>I'm having the following problem : 

"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[rlrweRJAQJqaoKfFB8il5A][stt_prod][3]: EsRejectedExecutionException[rejected execution (queue capacity 1000) on org.elasticsearch.action.search.type.TransportSearchTypeAction

It sounds very strange; when I restarted the server, it worked fine again.

What could happen ? 

Here is my configuration : 
- ES version is 1.0.1
- I have 3 indexes, of respective size 2.5G, 1.7G and 250M, each one has 5 shards
- the cluster is one only one instance (solo)
- the state of the cluster says 15 successful shard, 0 failed shard and 30 total shards (where are the 15 shards missing ?)
- in my settings, mlockall is set to true
- I enabled script.disable_dynamic: false, installed plugins _head and action-updatebyquery
- ES heap size is set 50% which I can confirm using top command : 
  5320 elastic+  20   0  9.918g 4.788g  72980 S   7.6 65.3  29:49.42 java
- I'm using 30% of disk capacity

My traffic is not more than 125 requests per minutes : 
![requests](https://cloud.githubusercontent.com/assets/6875375/6289842/466f8b60-b923-11e4-89bf-4e58dcba4a03.png)

So if I understand well, each request can live 30s, how come I have a queue of 1000 ?!
Can ES save the requests in the queue while the shards have failed ? 
Why do the shards do not come back ?

Thanks for your help (I'm not using ES usually, more Solr or CloudSearch)
</description><key id="58373816">9792</key><summary>Queue size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">christopher5106</reporter><labels /><created>2015-02-20T16:14:01Z</created><updated>2015-02-20T21:03:16Z</updated><resolved>2015-02-20T21:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-02-20T21:03:16Z" id="75318674">Issues here are mostly for code problems, given this is an operational one we will keep it on the mailing list thread you started here - https://groups.google.com/d/msgid/elasticsearch/33bfd4c7-4388-4e02-8e96-d8bb8cdd17ca%40googlegroups.com?utm_medium=email&amp;utm_source=footer
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New type of aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9791</link><project id="" key="" /><description>Hello,
I am using ElasticSearch 1.4.2 and Kibana 4. I am not able to get the correct plots, because the following aggregation does not exists in Elasticsearch and also Kibana.
Given T1, T2, T3 ... Tn time periods. Each period contains the following element: {T1, d1:v1, d2:v2...dn:vn} where d1:v1 keys value pairs. For example: {"09:00","Product":"a","Color":"red","quantity":1}, {"09:01","Product":"c","Color":"blue","quantity":5}.

I wanted to make a plot based on time(X axis) and quantity (Y axis).  The current elasticsearch and kibana return the "wrong" values in my case.

The system automatically splits the T1..Tn time interval into bucket[1],bucket[2],...,bucket[n](bucket[1]&gt;=T1 and bucket[n]&lt;=T[n])

bucket[1] = SUM(T1) + SUM(T2)
bucket[2] = SUM(T1) + SUM(T2)+SUM(T3)
...
bucket[n] = SUM(Tn-1) + SUM(Tn)

This aggregation is wrong for me. I would like to have the following:
bucket[1] = AVG(SUM(T1) + SUM(T2))
bucket[2] = AVG(SUM(T1) + SUM(T2)+SUM(T3))
...
bucket[n] = AVG(SUM(Tn-1) + SUM(Tn))

For example:

![screen shot 2015-02-18 at 18 13 53](https://cloud.githubusercontent.com/assets/981448/6252348/f15a41f4-b799-11e4-8b95-0234ca142dae.png)

The bucket length is 18 minute: we have 09:00,16:12, 16:30 buckets.
09:00 SUM(T1) + SUM(t2) = 1 + 1 + 1 + 1 + 1= 5
16:12  SUM(T3) = 1 + 2 + 2+ 2 = 7
16:20 SUM(T4) = 1 

What I want to have:
09:00 = AVG(SUM(T1) + SUM(T2)) = (2 +  3)/2 = 2.5
16:12  AVG(SUM(T3)) = 7/1 = 7
16:20 AVG(SUM(T4)) = 1 

I provide an example which can easily reproducible using kibana 4 and the latest elasticsearch:
-Insert the following data to elasticsearch(I used sense from marvel):
POST /example/transactions/_bulk
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:00:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "b", "time": "2015-02-09T16:30:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time":"2015-02-09T09:00:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time":"2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
- make the query using kibana 4
  ![screen shot 2015-02-18 at 17 31 43](https://cloud.githubusercontent.com/assets/981448/6251405/13cf497e-b794-11e4-8702-b18ad5960ba8.png)

The query made to the elasticsearch is:

``` json
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "*"
                }
            },
            "filter": {
                "bool": {
                    "must": [{
                        "range": {
                            "time": {
                                "gte": 1423399451544,
                                "lte": 1423631917911
                            }
                        }
                    }],
                    "must_not": []
                }
            }
        }
    },
    "aggs": {
        "3": {
            "date_histogram": {
                "field": "time",
                "interval": "3600000ms",
                "min_doc_count": 1,
                "extended_bounds": {
                    "min": 1423399451544,
                    "max": 1423631917911
                }
            },
            "aggs": {
                "4": {
                    "terms": {
                        "field": "Product",
                        "size": 0,
                        "order": {
                            "1": "desc"
                        }
                    },
                    "aggs": {
                        "1": {
                            "sum": {
                                "field": "quantity"
                            }
                        }
                    }
                }
            }
        }
    },
    "fields": ["*", "_source"],
    "script_fields": {
        "time": {
            "script": "if (doc[\"time\"].value == 0) { null } else { doc[\"time\"].value }"
        }
    }
}
```

I made a simple web page where I can make queries to ES and the result of the query is plotted. I changed the query as follow in order to make the "correct" plot.

``` json
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "*"
                }
            },
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "time": {
                                   "gte": 1423147775940,
                                    "lte": 1423752575940
                                }
                            }
                        }
                    ],
                    "must_not": []
                }
            }
        }
    },
    "aggs": {
        "2": {
            "terms": {
                "size": 0,
                "field": "Product"
            },
            "aggs": {
                "end_data": {
                    "date_histogram": {
                        "field": "time",
                        "interval": "1080000ms",
                        "min_doc_count": 1,
                        "extended_bounds": {
                           "min":1423147775940,
                           "max":1423752575940
                        }
                    },
                    "aggs": {
                        "times": {
                            "terms": {
                                "field": "time"
                            },
                            "aggs": {
                                "total_jobs": {
                                    "sum": {
                                        "field": "quantity"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    "fields": [
        "*",
        "_source"
    ],
    "script_fields": {
        "time": {
            "script": "if (doc[\"time\"].value == 0) { null } else { doc[\"time\"].value }"
        }
    }
}
```

I made the averages in JS and I got the plot what I want:
![screen shot 2015-02-18 at 17 36 35](https://cloud.githubusercontent.com/assets/981448/6251536/b7235e62-b794-11e4-84a7-ea7eaf6ebc80.png)

Is it possible to support this kind of aggregation? 

Thank you,
Zoltan
</description><key id="58373083">9791</key><summary>New type of aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">zmathe</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2015-02-20T16:08:49Z</created><updated>2015-03-18T21:50:26Z</updated><resolved>2015-03-02T15:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zmathe" created="2015-02-20T16:10:30Z" id="75263790">Let me know if you need more information. I can provide you a real example.
</comment><comment author="zmathe" created="2015-02-27T14:05:18Z" id="76398809">I show real example in order to better understand the problem. I would like to replace part of our monitoring system which is based on MySQL and mathplotlib to elasticsearch-kibana. 
First plot is the correct plot generated using MySQL.
![screen shot 2015-02-19 at 18 00 59](https://cloud.githubusercontent.com/assets/981448/6271717/f1c5cd74-b862-11e4-867c-1704c86e1b7d.png)

The equivalent plot in elasticsearch is:
![screen shot 2015-02-19 at 17 57 26](https://cloud.githubusercontent.com/assets/981448/6271741/1fad81a0-b863-11e4-8718-03c7b64d5723.png)

As you can see the number of jobs in the Y axis is ~7 million, which is not correct(the buckets size is 3 hours).

I implemented a simple tool where you execute a query and we can see the result. 
![screen shot 2015-02-19 at 18 18 56](https://cloud.githubusercontent.com/assets/981448/6271841/cfafcbc6-b863-11e4-9684-dc2b020f413a.png)

As you can see if I do an extra step I can have the correct plot. But this means I can not use Kibana. 

Thanks,
Zoltan
</comment><comment author="clintongormley" created="2015-02-28T04:00:33Z" id="76509063">@colings86 this sounds like it would fit into the new aggs features?
</comment><comment author="colings86" created="2015-03-02T15:05:35Z" id="76727128">@clintongormley yep this would fit into the new agg features in https://github.com/elasticsearch/elasticsearch/issues/9876, although it would probably need to be done through the scriptable implementation.

Closing in favour of that issue (#9876)
</comment><comment author="zmathe" created="2015-03-02T15:19:01Z" id="76729786">Thank you very much for adding this to the new aggregation framework!
</comment><comment author="bleskes" created="2015-03-03T10:41:08Z" id="76923049">@zmathe I may be totally missing something but in your aggs examples, I see that the kibana version does a date histogram first, then prodcut terms aggs and then sums the quantity. 

The request under

&gt; I made a simple web page where I can make queries to ES and the result of the query is plotted. I changed the query as follow in order to make the "correct" plot.

is different. It does terms aggs first, then date histogram, then summing the quantity. I wonder if just changing the agg order in kibana will also give you what you want?
</comment><comment author="zmathe" created="2015-03-03T13:44:28Z" id="76947028">@bleskes  Changing the order does not work in Kibana, because there is an extra step (which calculates the averages), which can not be done in ES. I had to do this extra step in the client side (in the JS).
Changing the order will give the correct buckets, but I do not want to sum the quantity I want to have the 
avg. In the following short example I am going to explain the problem.
The dataset I use:

``` json
POST /example/transactions/_bulk
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:00:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "b", "time": "2015-02-09T16:30:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time":"2015-02-09T09:00:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time":"2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 1, "Product": "a", "time": "2015-02-09T09:15:00Z"}
{ "index": {}}
{"Color": "red", "quantity": 2, "Product": "b", "time": "2015-02-09T16:15:00Z"}
```

Kibana is using the following query to plot the quantity over the time, group by product:

``` json
GET /example/_search/?search_type=count
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "*"
                }
            },
            "filter": {
                "bool": {
                    "must": [{
                        "range": {
                            "time": {
                                "gte": 1423399451544,
                                "lte": 1423631917911
                            }
                        }
                    }],
                    "must_not": []
                }
            }
        }
    },
    "aggs": {
        "3": {
            "date_histogram": {
                "field": "time",
                "interval": "3600000ms",
                "min_doc_count": 1,
                "extended_bounds": {
                    "min": 1423399451544,
                    "max": 1423631917911
                }
            },
            "aggs": {
                "4": {
                    "terms": {
                        "field": "Product",
                        "size": 0,
                        "order": {
                            "1": "desc"
                        }
                    },
                    "aggs": {
                        "1": {
                            "sum": {
                                "field": "quantity"
                            }
                        }
                    }
                }
            }
        }
    }
}

The result of this query:

{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 10,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "3": {
         "buckets": [
            {
               "4": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "1": {
                           "value": 5
                        },
                        "key": "a",
                        "doc_count": 5
                     }
                  ]
               },
               "key_as_string": "2015-02-09T09:00:00.000Z",
               "key": 1423472400000,
               "doc_count": 5
            },
            {
               "4": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "1": {
                           "value": 8
                        },
                        "key": "b",
                        "doc_count": 5
                     }
                  ]
               },
               "key_as_string": "2015-02-09T16:00:00.000Z",
               "key": 1423497600000,
               "doc_count": 5
            }
         ]
      }
   }
}

Changing the aggs order the query is:

GET /example/_search/?search_type=count
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "*"
                }
            },
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "time": {
                                   "gte": 1423147775940,
                                    "lte": 1423752575940
                                }
                            }
                        }
                    ],
                    "must_not": []
                }
            }
        }
    },
    "aggs": {
        "2": {
            "terms": {
                "size": 0,
                "field": "Product"
            },
            "aggs": {
                "end_data": {
                    "date_histogram": {
                        "field": "time",
                        "interval": "1080000ms",
                        "min_doc_count": 1,
                        "extended_bounds": {
                           "min":1423147775940,
                           "max":1423752575940
                        }
                    },
                    "aggs": {
                        "times": {
                            "terms": {
                                "field": "time"
                            },
                            "aggs": {
                                "total_jobs": {
                                    "sum": {
                                        "field": "quantity"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

The result of this query is:
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 10,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "2": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "a",
               "doc_count": 5,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T09:00:00.000Z",
                        "key": 1423472400000,
                        "doc_count": 5,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423473300000,
                                 "key_as_string": "2015-02-09T09:15:00.000Z",
                                 "doc_count": 3,
                                 "total_jobs": {
                                    "value": 3
                                 }
                              },
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 2,
                                 "total_jobs": {
                                    "value": 2
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            },
            {
               "key": "b",
               "doc_count": 5,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T16:12:00.000Z",
                        "key": 1423498320000,
                        "doc_count": 4,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423498500000,
                                 "key_as_string": "2015-02-09T16:15:00.000Z",
                                 "doc_count": 4,
                                 "total_jobs": {
                                    "value": 7
                                 }
                              }
                           ]
                        }
                     },
                     {
                        "key_as_string": "2015-02-09T16:30:00.000Z",
                        "key": 1423499400000,
                        "doc_count": 1,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423499400000,
                                 "key_as_string": "2015-02-09T16:30:00.000Z",
                                 "doc_count": 1,
                                 "total_jobs": {
                                    "value": 1
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            }
         ]
      }
   }
}

As you can see in the result we have two buckets 'a' and 'b', in the other example we have: 2015-02-09T09:00:00.000Z and 2015-02-09T16:00:00.000Z.

In the last example in the bucket 'a' I had to make the avgs (because it can not be calculated by ES):
{
               "key": "a",
               "doc_count": 5,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T09:00:00.000Z",
                        "key": 1423472400000,
                        "doc_count": 5,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423473300000,
                                 "key_as_string": "2015-02-09T09:15:00.000Z",
                                 "doc_count": 3,
                                 "total_jobs": {
                                    "value": 3
                                 }
                              },
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 2,
                                 "total_jobs": {
                                    "value": 2
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            }

I have: a-&gt;2015-02-09T09:00:00.000Z and the average of:
 "buckets": [
                              {
                                 "key": 1423473300000,
                                 "key_as_string": "2015-02-09T09:15:00.000Z",
                                 "doc_count": 3,
                                 "total_jobs": {
                                    "value": 3
                                 }
                              },
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 2,
                                 "total_jobs": {
                                    "value": 2
                                 }
                              }
                           ]
3+2/2=2.5.
After the avgs I made the correct buckets by changing the order of the rows:
 2015-02-09T09:00:00.000Z, a, 2.5.
However in the firs example (the current ES implementation):
2015-02-09T09:00:00.000Z, a, 5
You can see the different:

![screen shot 2015-03-03 at 14 27 48](https://cloud.githubusercontent.com/assets/981448/6463479/bca0959e-c1b2-11e4-939b-6fe434bf6563.png)


Because of the avg calculation I can not use Kibana. I think this will be useful not only us.  I am sorry for the long message.
Thanks,
Zoltan
```
</comment><comment author="zmathe" created="2015-03-03T13:45:16Z" id="76947150">I am sorry the screen shot is missing in the previous post:
![screen shot 2015-03-03 at 14 27 48](https://cloud.githubusercontent.com/assets/981448/6463585/d15fb6bc-c1b3-11e4-913d-7ddc9163abbf.png)
</comment><comment author="aschokking" created="2015-03-18T21:50:25Z" id="83197296">We've been running into this exact same problem with summed averages.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs: Fix rounding issues when using `date_histogram` and time zones</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9790</link><project id="" key="" /><description>This fix enhances the internal time zone conversion in the
TimeZoneRounding classes that were the cause of issues with
strange date bucket keys in #9491 and #7673.

Closes #9491
Closes #7673
</description><key id="58372641">9790</key><summary>Aggs: Fix rounding issues when using `date_histogram` and time zones</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2015-02-20T16:05:25Z</created><updated>2015-03-19T10:16:58Z</updated><resolved>2015-02-23T12:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-23T09:09:34Z" id="75509243">LGTM
</comment><comment author="cbuescher" created="2015-02-23T09:59:05Z" id="75515287">@jpountz Thanks, will merge this. Do you think this should this also go on 1.4 branch? 
</comment><comment author="jpountz" created="2015-02-23T09:59:48Z" id="75515383">@cbuescher Yes, please.
</comment><comment author="cbuescher" created="2015-02-23T12:40:14Z" id="75534482">Merged with 1.4 on b7dbf1e and on 1.x with 78f0202
</comment><comment author="javanna" created="2015-02-25T13:17:09Z" id="75957680">@cbuescher can you update labels please? Did this change in 1.x and 1.4 only, not master?
</comment><comment author="cbuescher" created="2015-02-25T15:40:50Z" id="75982725">@javanna This particular PR is just merged with 1.4 and 1.x branch since on master there was a more general refactoring of time zone roundings with #9637. Not sure if this means I should label this PR with 2.0 as well? 
</comment><comment author="javanna" created="2015-02-25T15:41:52Z" id="75982921">No I think this was just missing the `1.5.0` label. If it didn't go into master it shouldn't be marked `2.0.0`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Triple Negative</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9789</link><project id="" key="" /><description>Double negatives are confusing, but a triple negative?!

&gt; no non null

It takes five minutes to understand this little sentence.  Cleaned that up a bit.
</description><key id="58356710">9789</key><summary>Remove Triple Negative</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">speedplane</reporter><labels><label>docs</label></labels><created>2015-02-20T14:03:03Z</created><updated>2015-02-25T08:38:02Z</updated><resolved>2015-02-23T19:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-02-23T17:49:00Z" id="75593578">@speedplane I see one tiny issue noted above.  Could you please fix?

Also, could I please ask that you sign our CLA?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="speedplane" created="2015-02-23T18:12:41Z" id="75598188">I see the issue... but I don't see a way of fixing it without checking out a local copy of the repo and creating a new patch, etc. Is there a way to fix this online without doing that?
</comment><comment author="clintongormley" created="2015-02-23T18:36:08Z" id="75602823">Heya @speedplane 

Thanks for the PR.  Unfortunately it changes the meaning of the sentence incorrectly, as the new wording excludes documents that have no value for the field.

How about:  

```
 Returns all documents except those that have a non-`null` value in the original field.
```
</comment><comment author="speedplane" created="2015-02-23T19:03:20Z" id="75608425">That's still a confusing triple negative: (1) except, (2) non, (3) null. How about:

```
Returns documents that have `null` values or no value in the original field.
```
</comment><comment author="clintongormley" created="2015-02-23T19:09:22Z" id="75609626">Sounds good - merged, thanks @speedplane 
</comment><comment author="javanna" created="2015-02-25T08:38:02Z" id="75923793">@clintongormley can you label this PR please? Not sure which branches it got pushed to ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch analyzer not working with nested query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9788</link><project id="" key="" /><description>I have created an index with the following mapping

```
PUT test1
{
  "mappings": {
    "searchText": {
      "properties": {
        "catalogue_product": {
          "type": "nested",
          "properties": {
            "id": {
              "type": "string",
              "index": "not_analyzed"
            },
            "long_desc": {
              "type": "nested",
              "properties": {
                "translation": {
                  "type": "nested",
                  "properties": {
                    "en-GB": {
                      "type": "string",
                      "anlayzer": "snowball"
                    },
                    "fr-FR": {
                      "type": "string",
                      "anlayzer": "snowball"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

I have put one record using

```
PUT /test1/searchText/1
{
  "catalogue_product": {
    "id": "18437",
    "long_desc": {
      "translation": {
        "en-GB": "C120 - circuit breaker - C120H - 4P - 125A - B curve",
        "fr-FR": "Disjoncteur C120H 4P 125A courbe B 15000A"
      }
    }
  }
}
```

Then if i do a search for the word
"breaker"
inside

"catalogue_product.long_desc.translation.en-GB"
I get the added record

```
POST /test1/searchText/_search
{
  "query": {
    "nested": {
      "path": "catalogue_product.long_desc.translation",
      "query": {
        "match": {
          "catalogue_product.long_desc.translation.en-GB": "breaker"
        }
      }
    }
  }
}
```

if replace the word

"breaker"
with

"breakers"
, I dont get any records in spite of the en-GB field having analyzer=snowball in the mapping

```
POST /test1/searchText/_search
{
  "query": {
    "nested": {
      "path": "catalogue_product.long_desc.translation",
      "query": {
        "match": {
          "catalogue_product.long_desc.translation.en-GB": "breakers"
        }
      }
    }
  }
}
```

I am going crazy with this. Where am I going wrong? I tried a new mapping with analyzer as english instead of snowball, but that did not work either :( Any help is appreciated
</description><key id="58356259">9788</key><summary>ElasticSearch analyzer not working with nested query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cgurjar</reporter><labels /><created>2015-02-20T13:59:20Z</created><updated>2015-03-01T05:38:06Z</updated><resolved>2015-02-27T23:53:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T23:53:09Z" id="76494812">You have misspelled `analyzer`...  I have opened #9927 to warn about such issues.

btw you are seriously overusing nested fields - you don't need this many...
</comment><comment author="cgurjar" created="2015-03-01T05:36:07Z" id="76575353">Why? I actually am keeping this similar to the source. I will change it only if "nested queries" impact performance

Note - I did notice any performance problems with this practically when running nested queries
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to set indices.query.bool.max_clause_count: 2000 or index.query.bool.max_clause_count: 2000</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9787</link><project id="" key="" /><description>Hi

I am unable to set max_clause_count. Even if I add indices.query.bool.max_clause_count: 2000 or index.query.bool.max_clause_count: 2000 in elasticsearch.yml. Still it is getting defaulted to 1024.

Bellow is the error message :

'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=') OR (assignments.assignmentKey = 'PnRQe9ZOl1w6ys39TRYMJg\=\=')\r\n': too many boolean clauses]; nested: TooManyClauses[maxClauseCount is set to 1024]; }]","status":400}

Thanks,
Amalraj
</description><key id="58351367">9787</key><summary>Unable to set indices.query.bool.max_clause_count: 2000 or index.query.bool.max_clause_count: 2000</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amalrajcharles</reporter><labels><label>feedback_needed</label></labels><created>2015-02-20T13:21:29Z</created><updated>2015-03-04T17:41:39Z</updated><resolved>2015-03-04T17:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T23:39:00Z" id="76493341">Hiya

I'm struggling to make it fail with the default settings, but if I set `index.query.bool.max_clause_count` to eg `10`, then i get it to fail with (eg) a `match_phrase_prefix` query.

Perhaps you can demonstrate exactly what it is you are doing?

thanks
</comment><comment author="amalrajcharles" created="2015-03-04T17:41:39Z" id="77206578">Hi,

I had not restarted the elasticsearch service. The settings works as expected.

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow primary promotion on shadow replica without failing the shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9786</link><project id="" key="" /><description>Today we fail the shard if we need to upgrade a replica to a primary on shadow replicas
on shared filesystem. Yet, this commit allows promotion by re-initializing on the master preventing
reallocation of all replicas.
</description><key id="58348491">9786</key><summary>Allow primary promotion on shadow replica without failing the shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T12:54:32Z</created><updated>2015-06-06T19:13:12Z</updated><resolved>2015-02-25T13:50:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-23T20:10:16Z" id="75621527">I like it, it's pretty simple too, left one minor comment, LGTM
</comment><comment author="s1monw" created="2015-02-24T10:29:17Z" id="75732693">@kimchy can you take a look at this just to make sure I am not missing something?
</comment><comment author="kimchy" created="2015-02-25T13:43:31Z" id="75961125">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is this a right way to find in nested type that a user has  four time done event e1 and has not done event e2?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9785</link><project id="" key="" /><description>Below is Mapping of type user with nested event
{
   "beta": {
      "mappings": {
         "user": {
            "dynamic_templates": [
               {
                  "string_template": {
                     "mapping": {
                        "type": "string",
                        "index": "analyzed",
                        "analyzer": "string_lowercase"
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               }
            ],
            "_routing": {
               "required": true,
               "path": "_shardId"
            },
            "properties": {  
               "event": {
                  "type": "nested",
                  "properties": {
                     "_date": {
                        "type": "date",
                        "format": "dateOptionalTime"
                     },
                     "count": {
                        "type": "long"
                     },
                     "first_date": {
                        "type": "date",
                        "format": "dateOptionalTime"
                     },
                     "name": {
                        "type": "string",
                        "analyzer": "string_lowercase"
                     },
                     "last_date": {
                        "type": "date",
                        "format": "dateOptionalTime"
                     }
                  }
               }
            }
         }
      }
   }
}
</description><key id="58345621">9785</key><summary>Is this a right way to find in nested type that a user has  four time done event e1 and has not done event e2?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sandeep-verma</reporter><labels /><created>2015-02-20T12:25:14Z</created><updated>2015-02-27T22:30:16Z</updated><resolved>2015-02-27T22:30:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T22:30:16Z" id="76484915">Hi @sandeep-verma 

Please ask questions like these in the mailing list (and you will need to provide more information than just the mapping).

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Retry if shard deletes fail due to IOExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9784</link><project id="" key="" /><description>Today if a shard deletion fails we simply ignore it and move on. On system like
windows where a virus scanner can hold on to files or any other process ie. the admins
explorer window we fail to delete shards leaving large amout of data behind. We should try
best effort to clean those shards up before we ack the delete.
</description><key id="58340458">9784</key><summary>Retry if shard deletes fail due to IOExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T11:33:51Z</created><updated>2015-06-06T19:16:07Z</updated><resolved>2015-02-20T11:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-02-20T11:48:33Z" id="75226529">LGTM. Left one logging request (warn on fail of retried deletes)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Elasticsearch 1.4.2] String is truncated in the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9783</link><project id="" key="" /><description>If I use "."(dots) in a string, in the index the string is truncated. For example:
I inserted some data using sense:
POST /problem/transactions/_bulk
{ "index": {}}
{"Status": "Running", "Jobs": 1, "Site": "a.b2p3.fr", "time": "2015-02-09T09:00:00Z"}
{ "index": {}}
{"Status": "Running", "Jobs": 3, "Site": "a.b.c", "time": "2015-02-09T09:00:00Z"}

I made a query on this data:

``` json
GET /problem/transactions/_search?search_type=count 
{
    "size": 0,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "*"
                }
            },
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "time": {
                                   "gte": 1423147775940,
                                    "lte": 1423752575940
                                }
                            }
                        }
                    ],
                    "must_not": []
                }
            }
        }
    },
    "aggs": {
        "2": {
            "terms": {
                "size": 0,
                "field": "Site"
            },
            "aggs": {
                "end_data": {
                    "date_histogram": {
                        "field": "time",
                        "interval": "1080000ms",
                        "min_doc_count": 1,
                        "extended_bounds": {
                           "min":1423147775940,
                           "max":1423752575940
                        }
                    },
                    "aggs": {
                        "times": {
                            "terms": {
                                "field": "time"
                            },
                            "aggs": {
                                "total_jobs": {
                                    "sum": {
                                        "field": "Jobs"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

The result is:

``` json
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "2": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "a.b.c",
               "doc_count": 1,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T09:00:00.000Z",
                        "key": 1423472400000,
                        "doc_count": 1,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 1,
                                 "total_jobs": {
                                    "value": 3
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            },
            {
               "key": "a.b2p3",
               "doc_count": 1,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T09:00:00.000Z",
                        "key": 1423472400000,
                        "doc_count": 1,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 1,
                                 "total_jobs": {
                                    "value": 1
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            },
            {
               "key": "fr",
               "doc_count": 1,
               "end_data": {
                  "buckets": [
                     {
                        "key_as_string": "2015-02-09T09:00:00.000Z",
                        "key": 1423472400000,
                        "doc_count": 1,
                        "times": {
                           "doc_count_error_upper_bound": 0,
                           "sum_other_doc_count": 0,
                           "buckets": [
                              {
                                 "key": 1423472400000,
                                 "key_as_string": "2015-02-09T09:00:00.000Z",
                                 "doc_count": 1,
                                 "total_jobs": {
                                    "value": 1
                                 }
                              }
                           ]
                        }
                     }
                  ]
               }
            }
         ]
      }
   }
}
```

As you can see in the result the a.b2p3.fr is split into a.b2p3 and fr!
However the row data is correct:

&gt; &gt; &gt; from elasticsearch import Elasticsearch
&gt; &gt; &gt; client = Elasticsearch()
&gt; &gt; &gt; response = client.search(index="problem",body={})
&gt; &gt; &gt; for i in response['hits']['hits']:
&gt; &gt; &gt; ...   print i['_source']
&gt; &gt; &gt; ...
&gt; &gt; &gt; {u'Status': u'Running', u'Jobs': 1, u'Site': u'a.b2p3.fr', u'time': u'2015-02-09T09:00:00Z'}
&gt; &gt; &gt; {u'Status': u'Running', u'Jobs': 3, u'Site': u'a.b.c', u'time': u'2015-02-09T09:00:00Z'}
&gt; &gt; &gt; 
&gt; &gt; &gt; It seams to me the index is not correctly created.
&gt; &gt; &gt; Could you please have a look?
&gt; &gt; &gt; Thank you,
&gt; &gt; &gt; Zoltan
</description><key id="58340104">9783</key><summary>[Elasticsearch 1.4.2] String is truncated in the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zmathe</reporter><labels /><created>2015-02-20T11:30:39Z</created><updated>2015-02-20T15:57:57Z</updated><resolved>2015-02-20T15:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-02-20T12:11:53Z" id="75229034">I think you need to make sure this is marked as `"not_analyzed" : true` in the mapping
</comment><comment author="zmathe" created="2015-02-20T15:57:57Z" id="75261489">I am sorry for the noise. I have modified the index by adding the not_analyzed to the mapping and it works!

``` json
{'site_type': {'_all': {'enabled': 'false'}, 'properties': {'Site': {'index': 'not_analyzed', 'type': 'string'}}, '_source': {'compressed': 'true'}}}
```

Thank you very much for the quick response!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify term query/filter behavior when _type is not indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9782</link><project id="" key="" /><description>From http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-type-field.html#mapping-type-field:

```
The _type field can also not be indexed, and all the APIs will still work except for specific queries (term queries / filters) or faceting done on the _type field.

{
    "tweet" : {
        "_type" : {"index" : "no"}
    }
}
```

From testing, when _type is configured to not be indexed, a query like the following will still return the documents:

```
{
  "query": {
   "term": {
     "_type": {
       "value": "tweet"
     }
   }
  }
}
```

This seems to be consistent with the other documentation page on _uid (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-uid-field.html) where it says "The _uid field is automatically used when _type is not indexed to perform type based filtering, and does not require the _id to be indexed."

But contradictory to the description provided on the _type doc page (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-type-field.html#mapping-type-field):
"The _type field can also not be indexed, and all the APIs will still work except for specific queries (term queries / filters) or faceting done on the _type field."
</description><key id="58320231">9782</key><summary>Clarify term query/filter behavior when _type is not indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-02-20T07:54:54Z</created><updated>2015-02-27T22:05:59Z</updated><resolved>2015-02-27T22:05:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T22:05:59Z" id="76480875">This is no longer an issue as you can no longer configure the `_type` field (see https://github.com/elasticsearch/elasticsearch/pull/9869)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Remove global shared cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9781</link><project id="" key="" /><description>This was previously attempted in #8854. I revived that branch and did
some performance testing as was suggested in the comments there.

I fixed all the errors, mostly just the rest tests, which
needed to have http enabled on the node settings (the global cluster previously had this always enabled). I also addressed the comments from that issue.

My performance tests involved running the entire test suite on my
desktop which has 6 cores, 16GB of ram, and nothing else was being
run on the box at the time. I ran each set of settings 3 times and
took the average time.

| mode | master | patch | diff |
| --- | --- | --- | --- |
| local | 409s | 417s | +2% |
| network | 368s | 380s | +3% |

This increase in average time is clearly worthwhile to pay to achieve
isolation of tests. One caveat is the way I fixed the rest tests
is still to have one cluster for the entire suite, so all the rest
tests can still potentially affect each other, but this is an
issue for another day.

There were some oddities that I noticed while running these tests
that I would like to point out, as they probably deserve some
investigation (but orthogonal to this PR):
- The total test run times are highly variable (more than a minute between the min and max)
- Running in network mode is on average actually _faster_ than local mode. How is this possible!?
</description><key id="58310060">9781</key><summary>Tests: Remove global shared cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T04:50:22Z</created><updated>2015-03-24T03:22:46Z</updated><resolved>2015-02-23T06:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-02-20T08:02:25Z" id="75202437">I think you should also modify https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/testing/testing-framework.asciidoc
</comment><comment author="rjernst" created="2015-02-20T08:11:19Z" id="75203163">@dadoonet Good catch, I just updated with a fix for those docs.
</comment><comment author="jpountz" created="2015-02-20T13:57:29Z" id="75241263">I don't think the global cluster is worth the speed up that you measured, +1 to remove. I would also like to understand why running tests in network mode is faster, this is counter intuitive.
</comment><comment author="rmuir" created="2015-02-20T14:07:04Z" id="75242574">I think the benefits of having test isolation greatly outweigh any 2%, even if it was 20% or 30% i would feel this way.

Isolating tests means not only that they will be more reproducible, but also that we can enable things like thread leak detection, file handle leak detection, etc. Today it prevents us from using any of lucene's mockfilesystems in general, which can simulate windows behavior and other things that may find a lot of bugs (especially in code not using the Directory api).
</comment><comment author="dakrone" created="2015-02-20T23:13:24Z" id="75336887">LGTM
</comment><comment author="s1monw" created="2015-02-21T12:49:36Z" id="75370265">LGTM +1 to move forward
</comment><comment author="javanna" created="2015-02-25T07:26:56Z" id="75917058">Thanks for running  benchmarks @rjernst . 

That said, this still requires some more work... as mentioned in #8854: 

&gt; the global cluster has the property to be an external one too, and REST tests can run against this external cluster. We use this before releasing, to run REST tests against the distribution that we are going to ship. We might want to move that logic to SUITE clusters, but this needs be done before merging otherwise the release process breaks.

This PR remove the support for -Dtests.cluster and left `ExternalTestCluster` unused...we need to fix this otherwise the release script will not test the generated artifact. As far as I can see the release script will run rest tests against an internal cluster instead, all the time (will not throw any error but we will not be testing what we think we are testing, which is not good).
</comment><comment author="rjernst" created="2015-02-27T07:12:14Z" id="76349042">@javanna Ok I'm attempting to add back the external cluster: #9916
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix field mappers to always pass through index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9780</link><project id="" key="" /><description>Currently many meta field mappers do not take index settings in their
simple constructor that DocumentMapper uses, and instead pass null or
empty settings to the parent abstract mapper.  This change fixes them to
pass through index settings, and adds an assertion in AbstractFieldMapper
that settings are not null.
</description><key id="58297895">9780</key><summary>Fix field mappers to always pass through index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-20T01:18:49Z</created><updated>2015-06-08T09:05:11Z</updated><resolved>2015-02-23T06:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-20T13:31:52Z" id="75237968">LGTM
</comment><comment author="dadoonet" created="2015-02-23T09:52:16Z" id="75514313">Note: this commit is my primary suspect for failures in mapper attachment plugin.
Going to fix that.

```
org.elasticsearch.ElasticsearchIllegalStateException: [index.version.created] is not present in the index settings for index with uuid: [null]
    at __randomizedtesting.SeedInfo.seed([8214BC00B3DEEBCA:9B8FF2F963DAB833]:0)
    at org.elasticsearch.Version.indexCreated(Version.java:446)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.&lt;init&gt;(AbstractFieldMapper.java:351)
    at org.elasticsearch.index.mapper.attachment.AttachmentMapper.&lt;init&gt;(AttachmentMapper.java:358)
    at org.elasticsearch.index.mapper.attachment.AttachmentMapper$Builder.build(AttachmentMapper.java:213)
    at org.elasticsearch.index.mapper.attachment.AttachmentMapper$Builder.build(AttachmentMapper.java:86)
    at org.elasticsearch.index.mapper.object.ObjectMapper$Builder.build(ObjectMapper.java:195)
    at org.elasticsearch.index.mapper.DocumentMapper$Builder.&lt;init&gt;(DocumentMapper.java:185)
    at org.elasticsearch.index.mapper.MapperBuilders.doc(MapperBuilders.java:41)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:232)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:213)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:441)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide additional guidance on upgrade api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9779</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-upgrade.html#indices-upgrade

Would be nice to add some guidance on upgrade api (recommended, optional, required, etc.. ?):

Afaik, running the upgrade API after upgrading is optional.  Segments will be upgraded as part of the regular merge process over time when there is active indexing.  If there is a need to take advantage of some Lucene improvements in newer versions (eg. added checksums, corruption detection, new index/compression format), then it can be useful to run the upgrade API against indices that will not get written to anymore, or if you need to immediately take advantage of the improvements in the newer Lucene version.
</description><key id="58294718">9779</key><summary>Provide additional guidance on upgrade api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>v1.6.0</label></labels><created>2015-02-20T00:43:13Z</created><updated>2015-06-05T15:53:42Z</updated><resolved>2015-06-05T15:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Marvel nullpointer with 1.4.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9778</link><project id="" key="" /><description>```
java.lang.NullPointerException
        at org.elasticsearch.marvel.agent.Utils.extractHostsFromHttpServer(Utils.java:90)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.openAndValidateConnection(ESExporter.java:344)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:212)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:275)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:173)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:305)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:225)
```
</description><key id="58284429">9778</key><summary>Marvel nullpointer with 1.4.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">analytically</reporter><labels /><created>2015-02-19T23:02:10Z</created><updated>2015-12-05T17:13:20Z</updated><resolved>2015-12-05T17:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-02-23T07:40:15Z" id="75499559">@analytically thx for reporting. Does this happen all the time for you? does it help if you increase the agent's data sampling interval? (marvel.agent.interval )

I had similar issue fixed in the yet-to-be released code and I want to make sure it's the same.
</comment><comment author="analytically" created="2015-02-23T23:50:58Z" id="75664356">It does happen all the time. I'm seeing the same with 1.4.4.
</comment><comment author="bleskes" created="2015-02-24T09:56:33Z" id="75728007">@analytically thx. This is a Marvel issue not ES. Can you try increasing the marvel.agent.interval and see it helps?
</comment><comment author="mausch" created="2015-03-03T09:42:40Z" id="76914990">I'm seeing this in 1.4.2 too.
</comment><comment author="clintongormley" created="2015-12-05T17:13:20Z" id="162224265">Marvel 2 has been completely rewritten. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Remove use of globalTempDir() and forbid it from future use.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9777</link><project id="" key="" /><description>Using this temp dir circumvents test isolation and isn't necessary.
</description><key id="58276372">9777</key><summary>Test: Remove use of globalTempDir() and forbid it from future use.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-19T21:54:36Z</created><updated>2015-03-24T03:19:52Z</updated><resolved>2015-02-19T22:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-19T21:58:04Z" id="75147089">LGTM, left one comment
</comment><comment author="rmuir" created="2015-02-19T21:58:52Z" id="75147241">+1
</comment><comment author="s1monw" created="2015-02-19T22:01:35Z" id="75147679">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query against node that is not master get remotetransportexception exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9776</link><project id="" key="" /><description>I have a cluster setup
elasticsearch 1.4.3
Java version 1.7.0_75

when I query the master i get the result fast, but when quering a node on the cluster I get the following exception:

```
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[Lo1poYQOQSyEGHmAx9bt5w][agg_usages][0]: SearchParseException[[agg_usages][0]: from[-1],size[-1]:
```

Exception in log is:

```
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:687)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:543)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchParseException: Expected START_OBJECT but got START_ARRAY null
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:661)
```
</description><key id="58274494">9776</key><summary>query against node that is not master get remotetransportexception exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">omerh</reporter><labels /><created>2015-02-19T21:42:48Z</created><updated>2015-10-16T14:22:00Z</updated><resolved>2015-10-16T14:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-19T21:43:55Z" id="75143062">can you show the request you are sending? seems like the JSON you are building is starting with `[ ...`, and we only support `{ ...` .
</comment><comment author="clintongormley" created="2015-10-16T14:22:00Z" id="148730750">No feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: generating terms from array using script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9775</link><project id="" key="" /><description>This appears to be a defect (running against 1.4.3):

http://stackoverflow.com/questions/28616170/elasticsearch-generating-terms-from-array-using-script
</description><key id="58264763">9775</key><summary>Aggregations: generating terms from array using script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nariman-haghighi</reporter><labels /><created>2015-02-19T20:26:36Z</created><updated>2015-02-19T21:18:02Z</updated><resolved>2015-02-19T21:18:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Debian packaging: JAVA_HOME detection in init script defeats update-alternatives</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9774</link><project id="" key="" /><description>We are using update-alternatives to manage java versions, so we'd like elasticsearch to always use `which java` to select a java version. Unfortunately this doesn't work by default because the elasticsearch init script tries to autodetect `JAVA_HOME` and use that instead of the java in the path: https://github.com/elasticsearch/elasticsearch/blob/544ef8cb17014efb72abb2932d227e4f327b1dcb/src/deb/init.d/elasticsearch#L47

We can override the detected `JAVA_HOME`  in `/etc/default/elasticsearch`, causing the script to fall back to `which java` as desired. Before committing to this approach, we'd like to confirm that this behavior won't accidentally or intentionally change without warning in the future. Can we count on an empty `JAVA_HOME` being respected in future versions?

I'd also like to suggest removing the code that autodetects `JAVA_HOME`, especially for distributions that use update-alternatives. I prefer the behavior of the default elasticsearch binaries: use `JAVA_HOME` if present but don't try to set it.

/cc @spinscale
</description><key id="58260165">9774</key><summary>Debian packaging: JAVA_HOME detection in init script defeats update-alternatives</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">grantr</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-02-19T19:57:50Z</created><updated>2015-09-22T13:53:27Z</updated><resolved>2015-09-22T13:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T21:50:11Z" id="76478368">@spinscale @tlrx any thoughts?
</comment><comment author="tlrx" created="2015-03-02T09:45:19Z" id="76683150">@grantr thanks to share your thoughts.

I agree with you and I also think that the automatic JAVA_HOME detection is maybe not a such good idea. We are currently modifying the scripts to make them more uniform and we will remove this detection loop from the init script. If really needed, one could add it in the `/etc/default/elasticsearch` to set JAVA_HOME.

To me, the right behavior is: use `JAVA_HOME` if defined (in `/etc/default/elasticsearch` or anywhere else) otherwise `which java` will do the job.

&gt; Can we count on an empty JAVA_HOME being respected in future versions?

Yes.

&gt; I prefer the behavior of the default elasticsearch binaries: use JAVA_HOME if present but don't try to set it.

I agree :)
</comment><comment author="grantr" created="2015-03-02T17:36:23Z" id="76758744">Great, thanks @tlrx! I'll follow the release notes to see when I can remove the `JAVA_HOME` override from our `/etc/default/elasticsearch`.
</comment><comment author="tlrx" created="2015-09-22T12:55:57Z" id="142279369">@grantr it should be fixed by #13514
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hot threads should include timestamp and params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9773</link><project id="" key="" /><description>When looking at a hot threads output, it's useful to know the exact time when the hot threads were taken, e.g. to correlate back to Marvel charts or other monitors.  If multiple hot threads were pulled at different times it's useful to know which one was when.

I think it's also useful to know the params that were passed (interval, busiestThreads count, whether idle threads were ignored).

This change just adds a line at the top of each node's hot threads, like this:

  Hot threads at 2015-02-19T10:33:52.333Z, interval=500ms, busiestThreads=2147483647, ignoreIdleThreads=false:
</description><key id="58245835">9773</key><summary>Hot threads should include timestamp and params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2015-02-19T18:08:54Z</created><updated>2015-06-08T13:51:56Z</updated><resolved>2015-02-19T20:10:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-19T18:22:54Z" id="75106721">LGTM, this is a nice addition
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shadow replicas: Mark shards in cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9772</link><project id="" key="" /><description>Currently shadow replicas show up as regular replica shards in the cat API.

Might make sense to have `p`, `r` and `s` as shard types, so they are easy to distinguish.
</description><key id="58245700">9772</key><summary>Shadow replicas: Mark shards in cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:CAT API</label><label>enhancement</label></labels><created>2015-02-19T18:07:54Z</created><updated>2015-03-06T20:45:46Z</updated><resolved>2015-03-06T20:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Need to put 1.44 on the debian repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9771</link><project id="" key="" /><description>It doesn't appear to be available?
</description><key id="58245139">9771</key><summary>Need to put 1.44 on the debian repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikebell90</reporter><labels /><created>2015-02-19T18:03:32Z</created><updated>2015-02-19T18:05:30Z</updated><resolved>2015-02-19T18:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikebell90" created="2015-02-19T18:05:30Z" id="75103571">oops sorry, my bad.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor how restore cleans up files after snapshot was restored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9770</link><project id="" key="" /><description>Today we restore files by running through the directory removeing all files
not in the snapshot. Some files in that direcotry might belong there even though
we remove them. This commit moves the responsiblity of cleaning up pending files
to lucene by utilizing IndexWriter#IndexFileDeleter
</description><key id="58240252">9770</key><summary>Refactor how restore cleans up files after snapshot was restored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-19T17:27:26Z</created><updated>2015-06-07T17:28:02Z</updated><resolved>2015-02-20T10:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-02-19T17:55:38Z" id="75101683">I left a few comments; IW is unfortunately a "blunt tool" to use for this (maybe we should make it easier to "delete unref'd files" in an index), but I do like that it will recompute which files are in fact referenced by the restored segments file.
</comment><comment author="mikemccand" created="2015-02-19T21:54:04Z" id="75146376">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing: RestClient does not escape `source` HTTP parameter correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9769</link><project id="" key="" /><description>This test reproduces the issue. Be aware, that this only happens, when the whole source is put into the `source` HTTP parameter and not sent as body, You can control this behaviour in `RestClient.callApiBuilder()` for testing purposes

``` yaml

---
"Expressions scripting test":

  - do:
      index:
        index:  test123
        type:   test
        id:     1
        body:   { age: 23 }

  - do:
      indices.refresh: {}

  - do: { search: { body: { script_fields : { my_field : { lang: expression, script: 'doc["age"].value + 19' } } } } }
  - match:  { hits.hits.0.fields.my_field: [ 42.0 ] }
```

The problem is, that everything gets encoded correctly, with the exception of the `+` sign, which gets replaced with a space upon decoding - and this makes the script compilation fail.
</description><key id="58240092">9769</key><summary>Testing: RestClient does not escape `source` HTTP parameter correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>test</label></labels><created>2015-02-19T17:26:17Z</created><updated>2015-03-03T09:21:48Z</updated><resolved>2015-03-03T09:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Spelling out the sort order options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9768</link><project id="" key="" /><description>Perhaps this additional section is stating the obvious, but I did find myself checking the `SortOrder` code in order to confirm.
</description><key id="58237723">9768</key><summary>Spelling out the sort order options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">itzg</reporter><labels><label>docs</label></labels><created>2015-02-19T17:08:49Z</created><updated>2015-03-01T20:06:14Z</updated><resolved>2015-03-01T20:06:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="itzg" created="2015-02-19T17:10:01Z" id="75093044">CLA was signed a few minutes ago. Do I need to re-submit the pull request?
</comment><comment author="clintongormley" created="2015-02-28T00:38:45Z" id="76498843">Hi @itzg 

Thanks for the PR.  I left one comment.  I'll merge your PR as soon as that is sorted.

thanks
</comment><comment author="itzg" created="2015-03-01T00:45:09Z" id="76561412">@clintongormley , I have corrected the description of the default sort ordering.
</comment><comment author="clintongormley" created="2015-03-01T20:06:04Z" id="76627435">thanks @itzg - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simple Query String Query with flag PREFIX doesnt work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9767</link><project id="" key="" /><description>Is there something wrong with `flags` option to `simple_query_string` in the 2nd example? Why this query doesn't work as expected?

This works (expected results):

```
flags: "ALL",
query: "1 zloty Parchim*"
```

This doesn't work (no results):

```
flags: "PREFIX",
query: "1 zloty Parchim*"
```

This works (but useless):

```
flags: "PREFIX",
query: "Parchim*"
```

`default_operator` is `AND` in all cases.
</description><key id="58236503">9767</key><summary>Simple Query String Query with flag PREFIX doesnt work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spajak</reporter><labels /><created>2015-02-19T17:00:07Z</created><updated>2015-02-19T19:42:52Z</updated><resolved>2015-02-19T19:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-02-19T17:56:11Z" id="75101792">@spajak do you have an example document so I can try to reproduce this?

Also, it might be that you need to enable whitespace in the second example, using `flags: "WHITESPACE|PREFIX"` so that spaces are allowed.
</comment><comment author="spajak" created="2015-02-19T19:42:52Z" id="75121412">Yes. WHITESPACE is needed.
Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't retrieve stored fields as part of inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9766</link><project id="" key="" /><description>I've been trying out the new 'inner_hits' stuff (see #8153, I built a 1.5 snapshot from commit 23ef4e8 on the 1.x branch).

However, I don't seem to be able to request stored fields from my nested documents. It would be great to be able to specify a 'fields' element as part of the nested hit.

Note that Martijn's comment in https://github.com/elasticsearch/elasticsearch/pull/8153#issuecomment-61798326 suggested it is possible, but I couldn't work out how... 

As a side note, that comment was suggested in relation to disabling the source. In my case I was trying to disable the source for just the nested children, by adding the following to my mapping:

``` json
"_source" : {
    "excludes" : ["nested_field", "nested_field.*"]
}
```

which resulted in a null pointer exception...

```
java.lang.NullPointerException
    at org.elasticsearch.search.fetch.FetchPhase.createNestedSearchHit(FetchPhase.java:295)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:178)
    at org.elasticsearch.search.fetch.innerhits.InnerHitsFetchSubPhase.hitExecute(InnerHitsFetchSubPhase.java:96)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:190)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:501)
    at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:452)
    at org.elasticsearch.search.action.SearchServiceTransportAction$17.call(SearchServiceTransportAction.java:449)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```
</description><key id="58231908">9766</key><summary>Can't retrieve stored fields as part of inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">tstibbs</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-02-19T16:31:55Z</created><updated>2015-03-28T07:49:20Z</updated><resolved>2015-03-28T07:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T21:38:56Z" id="76476686">The NPE sounds like a bug, because inner hits is expecting the source to be there.  You should be able to use the `fields` parameter to retrieve stored fields though (at least according to the docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-request-inner-hits.html#nested-inner-hits )
</comment><comment author="tstibbs" created="2015-03-05T10:20:16Z" id="77339551">@clintongormley yeah, I noticed the docs say you can use fields, but it doesn't tell you where to put the `fields` element within the `inner_hits` element - I've tried a whole bunch of different places but can't make any of them do anything other than give me a parse error. Looking at `InnerHitsParseElement` and `InnerHitsQueryParserHelper` etc I can't see it looking for fields so I think probably the docs are wrong in this case.
</comment><comment author="martijnvg" created="2015-03-24T07:50:59Z" id="85383596">@tstibbs I referred to disabling the entire source and I think in your case you only leave out the nested part of your source. I see how this can result into the NPE you're describing.
</comment><comment author="tstibbs" created="2015-03-24T07:57:38Z" id="85387399">@martijnvg yep, I noticed that, just thought I'd report the NPE as it's presumably a bug (though a pretty minor one).

My original point about not being able to retrieve stored fields (e.g. if you disable the entire source) is still valid though I think.
</comment><comment author="martijnvg" created="2015-03-24T10:08:56Z" id="85433181">@tstibbs I opened #10235 in order to fix the stored field support in inner hits (it was supported, but forgot to add the parsing support for stored fields) and address the NPE you have found.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimize tests.policy a bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9765</link><project id="" key="" /><description>This isn't fine grained or really minimal, its just an improvement, e.g. removing blanket wildcard RuntimePermission, removing execute permission, etc
</description><key id="58222874">9765</key><summary>minimize tests.policy a bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2015-02-19T15:29:33Z</created><updated>2015-02-19T18:38:10Z</updated><resolved>2015-02-19T18:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-02-19T15:31:13Z" id="75071632">LGTM
</comment><comment author="jpountz" created="2015-02-19T15:47:08Z" id="75074810">LGTM
</comment><comment author="s1monw" created="2015-02-19T16:35:18Z" id="75084812">nice +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string is not using default analyzer inside of function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9764</link><project id="" key="" /><description>I have query 

``` javascript
{
  "from": 0,
  "size": 10,
  "fields": [
    "Url",
    "Id",
    "Title",
    "Description",
    "Date",
    "Breadcrumb",
    "FileBlob.title",
    "FileBlob.content_type"
  ],
  "query": {
    "function_score": {
      "query": {
        "query_string": {
          "query": "tip en ven",
          "fields": [
            "Id_analyzed^1.5",
            "Title_analyzed^1.5",
            "Description_analyzed^1",
            "Breadcrumb_analyzed^1",
            "FileBlob^1",
            "netmesterrankingterms"
          ],
          "default_operator": "and"
        }
      },
      "script_score": {
        "script": "double boost = 1;  return boost; ",
        "params": {
          "searchTerm": "tip en ven"
        }
      }
    }
  }
}
```

And i have defined default_search and index analyzer which uses stop words. 
In query which i use "tip en ven",  **en** is danish stop word, and will be removed on index time as my default_index analyzer uses stop words. So when i search i have default_operator and so it expect all words to be found in document but it will not be. 
Question is why query_string is not using default_search by default? because when i set directly analyzer to it it works as expected
</description><key id="58220736">9764</key><summary>query_string is not using default analyzer inside of function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vovikdrg</reporter><labels><label>feedback_needed</label></labels><created>2015-02-19T15:13:56Z</created><updated>2015-04-26T19:36:08Z</updated><resolved>2015-04-26T19:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-27T21:30:12Z" id="76475235">Perhaps you have fields with their own analyzers defined?  Could you provide a full (but simple) recreation of the whole problem (ie including creating the index, indexing the doc, the query, and the demonstration of the problem itself).
</comment><comment author="clintongormley" created="2015-04-26T19:36:07Z" id="96423620">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Would be useful to be able to specify inner_hits at top level without specifying a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9763</link><project id="" key="" /><description>I've been trying out the new 'nested_hits' stuff (see #8153, I built a 1.5 snapshot from commit 75b6d8e on the 1.x branch) and the two options for specifying it, and noticed that my use case seems to fall into a gap between the two options. Basically, I want to specify the inner_hits element at the top level, but I want it to return the inner hits based on the children that actually matched, rather than me specifying a whole new query (note I tried just leaving the query element out of the inner_hits bit, but that just seemed to return all my children regardless of whether they were part of the match).

My use case involves my users entering a query like this:

&gt; find a parent who has a child which is blue and another child which is red.

So my queries end up looking something like this (hopefully the pseudo-query-syntax made up for brevity will make sense to you): 

```
(nested(blue)) AND (nested(red))
```

What would be really useful is a way to say &#8220;give me the three children that contributed most to the score of the overall result&#8221;. But I can&#8217;t see any way to do that with the current ways to specify the inner_hits element.

Option one seems to be to specify inner hits for each of the nested queries. That will give me back three hits for each nested query so it&#8217;s difficult to tell which contributed most overall. I&#8217;d have to start taking the scores from the results for each nested query and combining them to work out which the top three matching children are. At which point I&#8217;m re-implementing a lot of lucene&#8217;s scoring maths. And it gets worse if the above query actually had &#8220;AND NOT&#8221; in it, or when the nested queries are deeply nested within boolean and constantscore queries.

Option two seems to be to specify inner hits for the query as a whole. But I have to give it its own query &#8211; which means I have to somehow transform the query above into a new query. It would end up something like this:

```
(blue) OR (red)
```

Note it has to change from AND to OR because it needs to match the single child.

Transforming the query sounds hard (particularly if I let my users enter an arbitrary query and try to transform it in code), and although I can&#8217;t come up with a compelling example I feel that it could result in highlights from children that didn&#8217;t actually contribute to the match of the parent. If transforming the query really is the right way to go about this, then maybe this logic should be put into elasticsearch (e.g. if you specify a top level inner_hits element but don&#8217;t specify a query, then it runs the main search query through the transformer and uses that for the inner_hits query). Regardless of whether I do this in my own code or whether it is done in elasticsearch, this approach has the problem that dealing with nested queries within nested queries could be tricky, and might require multiple levels of transformation.

So, another approach (sounds simpler but probably isn&#8217;t) might be to simply return the top x children that contributed most strongly to the parent match, regardless of how far down the tree of nested queries they are. So in my example above, I&#8217;d expect that a child containing both &#8216;blue&#8217; and &#8216;red&#8217; would be one of the top matches. This (I think, conceptually at least) is no different to lucene working out which fields matched most strongly in a document that matches a query with many levels of nested boolean queries. If this approach is possible, it sounds like it would be most robust than the transformer approach above.

I&#8217;m not sure how much of what I&#8217;m proposing is even possible, or if my grasp of the situation is at all correct. I'd be interested to hear what people think.
</description><key id="58214795">9763</key><summary>Would be useful to be able to specify inner_hits at top level without specifying a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">tstibbs</reporter><labels><label>:Nested Docs</label><label>discuss</label></labels><created>2015-02-19T14:29:17Z</created><updated>2015-12-05T17:12:46Z</updated><resolved>2015-12-05T17:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T17:12:46Z" id="162224154">I've been playing around with this today - took me a while to figure out what you were asking.  Sorry to deliver the news, but I don't think this can be implemented in any generic way without dramatically changing the way all of Lucene's queries work.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation on non indexed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9762</link><project id="" key="" /><description>I am trying to make aggregations on time series to get fixed-duration buckets of time.
I need to search based on timestamp (`ts` field) but never on values (`value` field) so my mapping specifies to skip indexing on `value`.
However, when I aggregate, it seesms that ES does not find the `value` field and use count zero instead.

To reproduce (marvel syntax):
- define a mapping template as:

```
PUT _template/test
{
  "template": "test",
  "mappings": {
    "metric" : {
      "properties": {
        "ts": { "type": "date"},
        "value": { "type": "double", "index": "no"}
      }
    }
  }
}
```
- index one document:

```
POST test/metric
{
  "ts": 4,
  "value": 42
}
```
- aggregate:

```
GET test/metric/_search?search_type=count
{
  "aggs": {
    "sum_indexed_field": { // non sense, only here to show aggregation work on indexed field
      "sum": {
        "field" : "ts"
      }
    },
    "sum_nonindexed_field" : {
      "sum" : {
        "field": "value"
      }
    }
  }
}
```

Ouput is rather disappointing:

```
{
   "took": 240,
   "timed_out": false,
   "_shards": {
      "total": 4,
      "successful": 4,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "sum_indexed_field": {
         "value": 4 // ok
      },
      "sum_nonindexed_field": {
         "value": 0 // should get 42 here
      }
   }
}
```

Am I doing something wrong?
Is this due to the way aggregations work?

Note 1 : using `not_analyzed` instead of `no` makes it work.
Note 2 : adding `"doc_values": true` makes it work as well.

But I'd like to understand.
</description><key id="58213556">9762</key><summary>Aggregation on non indexed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kamaradclimber</reporter><labels /><created>2015-02-19T14:19:19Z</created><updated>2016-03-17T12:07:41Z</updated><resolved>2015-02-19T14:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-19T14:45:03Z" id="75062924">Elasticsearch duplicates your content depending on what it needs to do with it:
- it has a store which typically stores the `_source` and is useful to give data back
- it has an inverted index that is used for searching/filtering
- and finally "fielddata" which is a columnar view on your data that it uses for aggregations and sorting.

This last thing is what is useful in your case. However elasticsearch does not store fielddata by default but builds it in memory from the inverted index on the first time that you need it. This is why you can only sort and aggregate on fields that are indexed by default.

If you don't need to search but only to aggregate on these fields, you can disable indexing (index: no) but you need to tell elasticsearch to store fielddata at indexing time. The name of this feature is called `doc_values`, see http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/doc-values.html.

Also note that `index: no` will prevent you from not only searching for individual terms, but also running range, prefix, wildcard queries or filters.
</comment><comment author="kamaradclimber" created="2015-02-19T15:17:18Z" id="75068851">thanks a lot for the detailed explaination
</comment><comment author="ivanprostran" created="2016-03-17T12:07:41Z" id="197849954">&gt; If you don't need to search but only to aggregate on these fields, you can disable indexing (index: no) but you need to tell elasticsearch to store fielddata at indexing time. The name of this feature is called doc_values,

I have done that but I'm still having the same visualization problem in Kibana v4.2.2 (elastic 2.2.x).

Some field definition is as follows:

"SomeCounterToSumOrAgregate": {
                    "type": "long","store":"yes","index": "no","doc_values": true
    },

and I am not able to sum/aggregate it because *\* Kibana doesn't show it.*\* as already stated in the subject.

If I'm not wrong "doc_values" feature is an optimization for "fielddata" according to the official documentation --&gt; https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html.

...And there is nothing wrong with that, actually Elastic back-end  returns the data properly as stated in documentation, so  this is clearly Kibana's behavior. 

I would appreciate any suggestion on how to make Kibana show  non-indexed fields (visualization)
To be more specific I'm not even clear about  whether it is a bug or feature.
What I understand is the following: I don't need massive unique data (counters, bytes or something like that) to be indexed. It is clearly unnecessary overhead.

Thanks !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only iterate the files that we recovered from the commit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9761</link><project id="" key="" /><description>Today we use Directory#listAll() to find all the files we recovered. Yet,
this is not accurate since there might be leftovers etc. It's better to
only iterate over the known files from the segments info that we recovered.
</description><key id="58211046">9761</key><summary>Only iterate the files that we recovered from the commit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2015-02-19T13:58:23Z</created><updated>2015-06-07T17:04:15Z</updated><resolved>2015-02-19T16:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-19T14:10:34Z" id="75057241">LGTM
</comment><comment author="bleskes" created="2015-02-19T14:11:40Z" id="75057409">LGTM do we want a test that injects evil files?
</comment><comment author="s1monw" created="2015-02-19T14:58:10Z" id="75065303">&gt; LGTM do we want a test that injects evil files?

that one is going be available at the lucene release next to you soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Engine: close snapshots before recovery counter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9760</link><project id="" key="" /><description>When we clean up after recoveries, we currently close the recovery counter first, followed up by the different snapshots. Since the recovery counter may issue a flush (introduced in #9439) , the snapshot references prevent the flush from deleting the current translog file once it's no longer needed. 

Note: this is not a problem on master, as we moved the translog delete logic, making it kick in if needed 
when the ref counter goes to 0.

relates to https://github.com/elasticsearch/elasticsearch/issues/9226#issuecomment-74873737
</description><key id="58201727">9760</key><summary>Engine: close snapshots before recovery counter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2015-02-19T12:25:42Z</created><updated>2015-03-19T10:49:55Z</updated><resolved>2015-02-27T19:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-02-19T12:36:43Z" id="75045077">can we do next to the code the reason for the importance of close ordering?
</comment><comment author="bleskes" created="2015-02-19T12:45:10Z" id="75046052">@kimchy comments added. 
</comment><comment author="s1monw" created="2015-02-24T08:53:37Z" id="75719678">left a minor comment
</comment><comment author="bleskes" created="2015-02-27T14:17:11Z" id="76400542">@s1monw ping?
</comment><comment author="s1monw" created="2015-02-27T16:24:38Z" id="76422356">ok fair enough LGTM then
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>