<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Only wait for initial state unless we already got a master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16956</link><project id="" key="" /><description>This seems to be an error introduced in refactoring around #16821
where we now wait 30seconds by default if the node already joined
a cluster and got a master. This can slow down tests dramatically
especially on slow boxes and notebooks.

here is a related test failure: http://build-us-00.elastic.co/job/es_core_master_oracle_6/5269/
</description><key id="138519076">16956</key><summary>Only wait for initial state unless we already got a master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>regression</label><label>review</label></labels><created>2016-03-04T16:36:27Z</created><updated>2016-03-06T08:16:10Z</updated><resolved>2016-03-06T08:16:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-04T16:36:46Z" id="192346042">@ywelsch @jasontedor @bleskes  FYI
</comment><comment author="s1monw" created="2016-03-05T11:01:04Z" id="192622804">@jasontedor  I tried to not change the code too much but I guess it's fine... updated and rebased
</comment><comment author="bleskes" created="2016-03-05T11:11:56Z" id="192623498">Thx @s1monw . LGTM

Re latch timeout -  left over from a different iteration of that refactoring. Agreed the observer timeout is simpler. Thx @jasontedor .
</comment><comment author="jasontedor" created="2016-03-05T19:15:07Z" id="192712041">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tests and documentation for using `time_zone` in date range aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16955</link><project id="" key="" /><description>Date range aggregations used to be unable to use a `time_zone` parameter to e.g. be applied int date math roundings like in `now/d` (see #10130 as an example). After the aggregation refactoring, the `time_zone` parameter has been pulled up to ValuesSourceAggregatorBuilder and can now be used in date range aggregations as well. 

This change adds randomized time zone settings to the existing IT tests to verify that the `time_zone` parameter is honored when calculating the bucket boundaries. Also moving the DateRangeTests from module-groovy/messy back to core as DateRangeIT, sharing common script mocks with DateHistogramIT and adding documentation for the `time_zone` parameter in the date range aggregation docs.

Closes #10130
</description><key id="138486813">16955</key><summary>Add tests and documentation for using `time_zone` in date range aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-04T14:19:47Z</created><updated>2016-03-14T11:25:26Z</updated><resolved>2016-03-07T15:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-04T18:45:14Z" id="192404544">@nik9000 thanks, I think I addressed most of your comments. I can also give extending NativeScriptFactory a shot, but need to see where that leads me and if it is simpler than the current mocks.
</comment><comment author="nik9000" created="2016-03-04T18:49:41Z" id="192406642">&gt; but need to see where that leads me and if it is simpler than the current mocks.

I think it'll be simpler but it isn't that important.

LGTM
</comment><comment author="cbuescher" created="2016-03-07T11:59:51Z" id="193220841">@nik9000 I switched the script mocks to native scripts and added Nepal-support for time zones ;-). I know you where already okay with this, but in case you want to take a last look before I merge. 
</comment><comment author="nik9000" created="2016-03-07T14:33:35Z" id="193272803">Left something small on the docs. LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>waste no one help frm this website</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16954</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="138479787">16954</key><summary>waste no one help frm this website</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snkiran</reporter><labels /><created>2016-03-04T13:43:54Z</created><updated>2016-03-04T13:46:15Z</updated><resolved>2016-03-04T13:46:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>1.7.3 with 0.90.12 compatible fields responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16953</link><project id="" key="" /><description /><key id="138468478">16953</key><summary>1.7.3 with 0.90.12 compatible fields responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">euroling-johnjohansson</reporter><labels /><created>2016-03-04T12:53:26Z</created><updated>2016-03-04T12:58:20Z</updated><resolved>2016-03-04T12:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add deprecation logging for mapping transform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16952</link><project id="" key="" /><description>With this commit we log a message to the deprecation log in case
somebody uses the deprecated 'transform' element in their mapping.

Relates #16910
</description><key id="138451010">16952</key><summary>Add deprecation logging for mapping transform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-03-04T11:28:06Z</created><updated>2016-03-04T19:15:58Z</updated><resolved>2016-03-04T11:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-04T11:28:45Z" id="192242622">I have tested the deprecation logging an example based on the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-transform.html):

```
PUT /sample_transform_index
{
   "mappings": {
      "example": {
         "transform": {
            "script": {
               "inline": "if (ctx._source['title']?.startsWith('t')) ctx._source['suggest'] = ctx._source['content']",
               "params": {
                  "variable": "not used but an example anyway"
               },
               "lang": "groovy"
            }
         },
         "properties": {
            "title": {
               "type": "string"
            },
            "content": {
               "type": "string"
            },
            "suggest": {
               "type": "string"
            }
         }
      }
   }
}
```

This produces:

```
[2016-03-04 12:11:28,140][DEBUG][deprecation.index.mapper ] Mapping transform is deprecated and will be removed in the next major version
```
</comment><comment author="dadoonet" created="2016-03-04T11:30:48Z" id="192243038">LGTM
</comment><comment author="danielmitterdorfer" created="2016-03-04T11:51:09Z" id="192252396">Thanks for the review @dadoonet!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document `sum` as supported scoring type for has_child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16951</link><project id="" key="" /><description>the examples all use `sum` for the `"score_mode"` field, but it isn't listed in the list of supported modes.
</description><key id="138449322">16951</key><summary>Document `sum` as supported scoring type for has_child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ccmtaylor</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-03-04T11:21:18Z</created><updated>2016-03-07T09:50:29Z</updated><resolved>2016-03-07T09:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-04T19:15:18Z" id="192422115">Actually, `total` (which is listed) is a synonym for `sum`,  but I much prefer `sum` as it is clearer.  I'd delete `total` and just stick with `sum`.

Also, please could I ask you to sign the CLA so that I can merge it in on the changes are made?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ccmtaylor" created="2016-03-05T21:23:58Z" id="192743198">@clintongormley I signed the CLA and updated the docs as you suggested. 
</comment><comment author="clintongormley" created="2016-03-07T09:50:29Z" id="193184522">thanks @ccmtaylor - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logs old script params use to the Deprecation Logger</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16950</link><project id="" key="" /><description>Relates to #16910
</description><key id="138448639">16950</key><summary>Logs old script params use to the Deprecation Logger</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v2.3.0</label></labels><created>2016-03-04T11:18:57Z</created><updated>2016-03-04T12:20:27Z</updated><resolved>2016-03-04T12:12:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-04T11:27:14Z" id="192242139">Left a comment (feel free to ignore). LGTM
</comment><comment author="colings86" created="2016-03-04T12:12:58Z" id="192257147">@dadoonet thanks for the review :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Discovery Multicast Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16949</link><project id="" key="" /><description>See #16910
</description><key id="138446376">16949</key><summary>Deprecate Discovery Multicast Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Multicast</label><label>deprecation</label><label>v2.3.0</label></labels><created>2016-03-04T11:05:42Z</created><updated>2016-03-04T12:38:37Z</updated><resolved>2016-03-04T12:38:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-04T11:57:11Z" id="192254036">I left one minor comment, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate mapper-attachments plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16948</link><project id="" key="" /><description>See #16910
</description><key id="138444897">16948</key><summary>Deprecate mapper-attachments plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>deprecation</label><label>v5.0.0-alpha1</label></labels><created>2016-03-04T10:58:55Z</created><updated>2016-03-04T12:35:59Z</updated><resolved>2016-03-04T12:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-04T11:53:13Z" id="192252792">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hi,t meet a problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16947</link><project id="" key="" /><description>![image](https://cloud.githubusercontent.com/assets/9464485/13520251/b8cd9d26-e219-11e5-9168-e1aa831200e5.png)
why es parse log level as a int value?
if i change the log level to int value and this problem is resolved,but not my need.
</description><key id="138397075">16947</key><summary>hi,t meet a problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmx1989219</reporter><labels /><created>2016-03-04T07:02:30Z</created><updated>2016-03-04T07:41:07Z</updated><resolved>2016-03-04T07:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-04T07:41:07Z" id="192164694">HI @lmx1989219 please join us at https://discuss.elastic.co/ so we can properly answer any of the questions you might have. We'd like to keep github reserved for bugs and feature requests. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add edits to the ingest doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16946</link><project id="" key="" /><description>@martijnvg @talevy I've made some edits to the Ingest Node doc (sorry to get these in rather late, but Elasticon and the All-Hands got in the way). Can you take a quick look and confirm that the changes are OK?

Summary of changes:
- Added some explanatory text around concepts (mostly in the intro) that I thought needed a bit more context.
- Tried to remove passive voice where it made the sentence harder to read
- Added some headings to break up long bits of text
- Added IDs to sections (otherwise, the generated HTML pages get renamed whenever we change the heading text).
- Fixed sentences that were ambiguous
</description><key id="138396489">16946</key><summary>Add edits to the ingest doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dedemorton</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-03-04T06:57:45Z</created><updated>2016-03-04T10:49:19Z</updated><resolved>2016-03-04T10:49:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-04T10:48:48Z" id="192229729">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining a multi field in a dynamic template matching the new text type doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16945</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0 (master)

**JVM version**: 1.8

**OS version**: OSX 10.11.3

**Description of the problem including expected versus actual behavior**:

While updating the Kibana functional tests to deal with the deprecation of the `string` mapping type, I tried to create a dynamic template matching `text` fields with a multi-field mapping as suggested in https://github.com/elastic/elasticsearch/issues/12394

My dynamic template looks like this:

```
"dynamic_templates": [{
        "text_fields": {
          "mapping": {
            "type": "text",
            "fields": {
              "raw": {
                "type": "keyword"
              }
            }
          },
          "match_mapping_type": "text",
          "match": "*"
        }
      }]
```

When posting documents with text fields to this index, the [fieldName].raw field does not get created.

**Steps to reproduce**:
1. Create an index with the above dynamic template.
2. Index a document that has a text field.
3. Query the field mappings `localhost:9200/&lt;index_name&gt;/_mapping/*/field/*?include_defaults=true`
4. Notice that the [fieldName].raw field is missing

I tried the same dynamic template with `"match_mapping_type": "*"` instead of `text`, and the raw field appeared as expected. I also tried the same multi field mapping on a regular field instead of a template, and that also worked. So this seems to be a problem with the combination of `text` fields and dynamic templates in particular. 
</description><key id="138349704">16945</key><summary>Defining a multi field in a dynamic template matching the new text type doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-03-04T01:46:08Z</created><updated>2016-10-11T13:52:04Z</updated><resolved>2016-10-11T13:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-04T03:38:31Z" id="192082291">I think the match mapping type needs to be string? That is the "type" in json, which hasnt changed. 
</comment><comment author="rjernst" created="2016-03-04T03:40:35Z" id="192082511">Which raises the issue, we should validate the value of that setting for each template. We should spin a separate issue for that (probably generally about dynamic template validation as I dont think we do any right now).
</comment><comment author="jpountz" created="2016-03-04T09:42:54Z" id="192209302">+1 @rjernst 
</comment><comment author="Bargs" created="2016-03-04T15:07:57Z" id="192315139">Confirmed `string` in the match mapping type works. That's pretty confusing for the user though. The `match_mapping_type` docs read: 

&gt; match_mapping_type matches on the datatype detected by dynamic field mapping, in other words, the datatype that Elasticsearch thinks the field should have.

The dynamic field mapping creates `text` fields now, so I'd expect that to be a valid value. If we have to stick with string, we should at least update the match_mapping_type docs to explain this.
</comment><comment author="pickypg" created="2016-03-04T18:28:31Z" id="192397112">I agree with @Bargs, plus it's inconsistent in that we _already_ allow `long` and `double` to be the matching type, which technically don't exist in JSON (which uses `number`).
</comment><comment author="clintongormley" created="2016-03-04T19:35:32Z" id="192431228">@jpountz when we discussed this, I think we decided on using `text` as the detected type?
</comment><comment author="rjernst" created="2016-03-04T20:09:33Z" id="192445353">We actually did use `text`, but there must still be uses of the other template methods I mentioned here: https://github.com/elastic/elasticsearch/pull/16877#discussion_r54749987
</comment><comment author="jpountz" created="2016-03-05T14:20:59Z" id="192657187">@clintongormley Templates have a concept of a match type, which is used to match templates, and a dynamic type, which is the elasticsearch field type to use by default if no type is specified. When we discussed it, we indeed agreed on using text as the default dynamic type but for now we are still using "string" as the match type. I'd be fine to switch to text if we think this makes more sense, but I agree with Ryan that even more importantly we should validate templates better so that adding templates with an unknown match type would be rejected since it means those templates have no chance to be ever used.
</comment><comment author="clintongormley" created="2016-10-11T13:51:58Z" id="252922733">Closed by #17285
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add max map count check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16944</link><project id="" key="" /><description>This commit adds a bootstrap check on Linux for the max map count (max
virtual memory areas) available to the Elasticsearch process.
</description><key id="138316092">16944</key><summary>Add max map count check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha2</label></labels><created>2016-03-03T22:29:09Z</created><updated>2016-04-15T01:04:03Z</updated><resolved>2016-04-15T01:04:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-04T22:10:33Z" id="192491468">&gt; If we are going to do this check, then we should revert that and go back to full lucene performance (pure mmap!)

+1 to default entirely to mmap (this is Lucene's default too!).

People don't realize today that ES's file switch directory sends all .cfs files, which is often the majority of segments in an index!, to NIOFS, which hurts random access performance needed for id lookups, doc values.  We fix this if we just default to mmapfs across the board.

Also, since it's only 1 .cfs file per segment (well, except for deletions), this won't hurt the map count so much ... 
</comment><comment author="dakrone" created="2016-04-14T17:11:09Z" id="210054294">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo in verb tense of "to gather"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16943</link><project id="" key="" /><description /><key id="138313935">16943</key><summary>Typo in verb tense of "to gather"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jameskerr</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-03-03T22:19:15Z</created><updated>2016-03-04T00:48:58Z</updated><resolved>2016-03-03T23:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-03T23:15:16Z" id="192013375">Thanks @jameskerr! Could you please sign the [CLA](https://www.elastic.co/contributor-agreement/) so I can merge this in?
</comment><comment author="dakrone" created="2016-03-03T23:15:40Z" id="192013571">Oh whoops, looks like you already did, I'll merge this :)
</comment><comment author="jameskerr" created="2016-03-04T00:48:58Z" id="192038836">:+1: Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mention path.repo in backup chapter of the guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16942</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/guide/current/backing-up-your-cluster.html), we need to mention `path.repo` as it's a critical setting for getting snapshots happening.

Requested [here](https://twitter.com/bart_read/status/705385983672455168).
</description><key id="138311199">16942</key><summary>Mention path.repo in backup chapter of the guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2016-03-03T22:06:24Z</created><updated>2016-03-03T22:07:39Z</updated><resolved>2016-03-03T22:07:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-03-03T22:07:38Z" id="191988172">Moving to https://github.com/elastic/elasticsearch-definitive-guide/issues/481
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include took time in DocWriteResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16941</link><project id="" key="" /><description>Closes #16641
</description><key id="138299567">16941</key><summary>Include took time in DocWriteResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">camilojd</reporter><labels /><created>2016-03-03T21:16:29Z</created><updated>2017-03-20T19:28:43Z</updated><resolved>2016-03-15T04:04:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-03T21:34:01Z" id="191973331">Thanks for the contribution @camilojd! :smile: 

But can you please add tests? This is adding new functionality without tests, and we've had enough time and unit conversion bugs. 
</comment><comment author="camilojd" created="2016-03-04T17:23:50Z" id="192367927">&gt; But can you please add tests? This is adding new functionality without tests, and we've had enough time and unit conversion bugs.

Hi @jasontedor!

I addressed your review comment and added some asserts to code that already exercises `DocWriteResponse` and subclasses, to ensure `getTookInMillis() &gt;= 0`.

Maybe it's necessary to write tests that verify `getTookInMillis()` is effectively above zero for some predictable costly indexing operations?
</comment><comment author="jasontedor" created="2016-03-04T17:27:05Z" id="192369105">&gt; I addressed your review comment and added some asserts

@camilojd Did you squash and force push? It's easier for the review process if you just let the commits accumulate (unless a reviewer explicitly asks for a rebase/squash). :smile: 
</comment><comment author="camilojd" created="2016-03-04T17:37:15Z" id="192374772">&gt; @camilojd Did you squash and force push? It's easier for the review process if you just let the commits accumulate (unless a reviewer explicitly asks for a rebase/squash).

Yes, I did that. Sorry! Will take it into account for the next time :relieved: 
</comment><comment author="jasontedor" created="2016-03-04T17:48:58Z" id="192377823">&gt; to ensure `getTookInMillis() &gt;= 0`

@camilojd Unfortunately this does not protect against any of the bugs that I'm worried about; there are many ways to change the code that will still satisfy this condition but be wrong.

&gt; Maybe it's necessary to write tests that verify getTookInMillis() is effectively above zero for some predictable costly indexing operations?

Yeah. We likely need at least one test that tests the timing a controlled fashion, and one test that tests the timing with a real clock.
</comment><comment author="jasontedor" created="2016-03-04T17:50:44Z" id="192378265">&gt; Yes, I did that. Sorry! Will take it into account for the next time :relieved:

@camilojd No worries, the basic rule is do not rewrite public history. For this PR it was relatively simple to go back and reread the entire diff, but you can imagine how this would be a problem on much larger change set that requires lots of back-and-forth.
</comment><comment author="olcbean" created="2017-03-19T18:53:58Z" id="287638179">@jasontedor 
I am currently looking at #23131 and I am wondering what tests should I write for it. Looking into the code base, I did not see any tests for the search and bulk 'took' time (maybe I missed something?). Hence I searched for hints in any related issues and saw your comment

&gt; Yeah. We likely need at least one test that tests the timing a controlled fashion, and one test that tests the timing with a real clock.

As I am pretty new to ES, could you please elaborate what tests you have in mind as timing tests are really tricky (and rather unreliable).</comment><comment author="jasontedor" created="2017-03-19T19:00:20Z" id="287638588">There is a test for bulk took time in `TransportBulkActionTookTests`.

A test with a controlled clock could be to ensure that the took calculations are correct. A test with a real clock could be to ensure that at least the took calculations produce sensible numbers.</comment><comment author="olcbean" created="2017-03-20T08:30:03Z" id="287699369">@jasontedor 
Thank you for the pointer. I will have a look at it.

One more question : why is the bulk time measured in nanos while the search time is in millis? 
I myself do not see an obvious benefit of handling the time in nanos (especially when the return value is in millis). But nevertheless shouldn't we consider to unify the time measurement approach for both / all actions?</comment><comment author="nik9000" created="2017-03-20T13:59:29Z" id="287766933">&gt; bulk time measured in nanos while the search time is in millis?

I haven't dug into it but if I had to guess it is because `System.nanoTime` is a relative clock while `System.currentTimeMillis` is an absolute clock. If you want to measure took time then `nanoTime` is better because it won't be affected by clock skew caused by systems like NTP.</comment><comment author="jasontedor" created="2017-03-20T17:01:34Z" id="287826241">They are both reported in millis, but we absolutely should be using a high-precision *relative* clock for both search and bulk. I will fix this for search, bulk is fine.</comment><comment author="jasontedor" created="2017-03-20T19:28:43Z" id="287871257">&gt; They are both reported in millis, but we absolutely should be using a high-precision *relative* clock for both search and bulk. I will fix this for search, bulk is fine.

I opened #23662.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use SleepingWrapper on shared filesytems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16940</link><project id="" key="" /><description>On shared FS / shadow replicas we rely on a lock retry if the lock has
not yet been relesed on a relocated primary. This commit adds this `hack`
for shared filesystems only.

Closes #16936
</description><key id="138292732">16940</key><summary>Use SleepingWrapper on shared filesytems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T20:46:54Z</created><updated>2016-03-03T23:05:43Z</updated><resolved>2016-03-03T20:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-03T23:05:43Z" id="192010490">Thanks for fixing this Simon, I do agree that it would be good if we could get rid of it entirely. I think the primary relocation is the use of it, but I'm not 100% sure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix organization rename in all files in project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16939</link><project id="" key="" /><description>Basically a query-replace of "https://github.com/elasticsearch/" with "https://github.com/elastic/"

We get a lot of PRs to this effect, so I figured doing it all in one swoop would be easiest.
</description><key id="138268896">16939</key><summary>Fix organization rename in all files in project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T19:05:26Z</created><updated>2016-03-03T19:11:46Z</updated><resolved>2016-03-03T19:10:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-03T19:07:46Z" id="191917752">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fingerprinting Ingest Processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16938</link><project id="" key="" /><description>A potentially useful processor is one that can generate one or more "fingerprints" from an incoming document.  This could aid in finding duplicates, detecting plagarism, or clustering similar documents together.

I think there are two realms of fingerprinting: content fingerprinting and structural fingerprinting.
### Content Fingerprinting

Hashes the content of fields to generate a fingerprint-per-field, and optionally, a fingerprint that represents all the fields.  Could use simple hashing, or perhaps something more sophisticated like MinHash, SimHash, [Winnowing](http://igm.univ-mlv.fr/~mac/ENS/DOC/sigmod03-1.pdf) or [Probabilistic Fingerprinting](http://www.cs.cmu.edu/~gveda/reports/IITK/cs497_DocFingerprinting.pdf).  

The API could look something like:

``` json
{
  "fingerprint": {
    "type": "content",
    "fields": ["foo", "bar"],
    "hash": "minhash",
    "hash_all": true
  }
}
```

E.g. specify the type of fingerprinting we want to do (`content`), a list of fields to hash, the style of hashing and if we should also hash all the hashes together.  The output would then be the document + new fingerprint fields:

``` json
{
  "foo": "...",
  "bar": "...",
  "fingerprint_foo": 1283891,
  "fingerprint_bar": 8372038,
  "fingerprint_all": 3817273
}
```
### Structural Fingerprinting

The other mode of fingerprinting could be structural in nature (this is the one I'm more interested in, tbh).  Instead of fingerprinting the content of fields, we are actually fingerprinting the structure of the document itself.  Essentially, we would recursively parse the JSON and hash the keys at each level in the JSON tree.  These hashes then become a fingerprint for the structure of the document.

Importantly, this type of fingerprinting ignores the leaf values...we just want to fingerprint the JSON keys themselves.

``` json
{
  "fingerprint": {
    "type": "structure",
    "root": ["foo"],
    "recursive": true,
    "hash": "murmur3",
    "hash_all": true
  }
}
```
- `root`: defines where to start recursing, in case you only care about a portion of the document.  Could be omitted or set to `"*"` to process the entire document
- `recursive` if you want the processor to fingerprint all the layers.  False if you just want the top-level of keys hashed.
- `hash`: murmur, minhash, etc
- `hash_all`: if all the hashes should be hashed together to build a final fingerprint

And the new document:

``` json
{
  "foo": {
    "bar": {
      "baz": "buzz"
    },
    "beep": {
      "boop": "bop"
    }
  },
  "fingerprint_level1": 001734,
  "fingerprint_level2": 992727,
  "fingerprint_level3": 110293,
  "fingerprint_all": 235240
}
```

Instead of a fingerprint-per-field, we now have one per "level".  

I can think of a number of objections to both of these, but this should at least kick off the discussion :)

/cc @martijnvg 
</description><key id="138265685">16938</key><summary>Fingerprinting Ingest Processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-03-03T18:52:43Z</created><updated>2016-03-16T15:14:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-03T19:05:09Z" id="191917042">I love this!
</comment><comment author="martijnvg" created="2016-03-03T21:26:26Z" id="191971213">:+1:  great idea
</comment><comment author="clintongormley" created="2016-03-04T13:40:19Z" id="192287462">@polyfractal btw have you seen this? https://github.com/elastic/elasticsearch/issues/13325

just waiting to be exposed /cc @markharwood 
</comment><comment author="polyfractal" created="2016-03-04T15:37:05Z" id="192326023">Interesting!  I had not seen that.  That could potentially cover some of the features needed for content fingerprinting (or could be used as one of the "hashes").

The major downside I see to the FingerprintFilter is that large blocks of text will still generate large numbers of tokens, and for some use-cases you really just want a single fingerprint token to represent the whole thing (like minhash, winnowing, etc)
</comment><comment author="markharwood" created="2016-03-04T15:43:51Z" id="192328006">&gt; The major downside I see to the FingerprintFilter is that large blocks of text will still generate large numbers of tokens, 

I was thinking a downstream filter could take care of hashing the stemmed, sorted, deduped etc set of tokens FingerprintFilter and friends produces.
I didn't want to build the hashing into FingerprintFilter because sometimes you might want the readability of the raw tokens.
</comment><comment author="polyfractal" created="2016-03-04T15:56:01Z" id="192332489">Makes sense.  I suppose this goes back to the debate of analyzers vs. ingest pipelines ... e.g how much work should be done by analyzers, vs deferring more complex/expensive computations to dedicated ingest pipelines.  Dunno :)
</comment><comment author="martijnvg" created="2016-03-04T16:05:15Z" id="192335276">&gt; analyzers, vs deferring more complex/expensive computations to dedicated ingest pipeline

If fingerprinting can be done on a per field basis then this should be done via analyzers, because that will perform much better. However if fingerprinting requires several fields or the entire document then this should be done via a pipeline, since there all fields are accessible.
</comment><comment author="polyfractal" created="2016-03-04T16:09:36Z" id="192337222">Ok, so in that case we can probably scratch the entire "content fingerprinting" section.  That could be done with FingerprintFilter + various hashes, and if you want a total hash, you can `copy_to` a new field that is also hashed.

The "structural fingerprinting" stuff would have to be a pipeline, since it requires multiple fields in the doc
</comment><comment author="eskibars" created="2016-03-04T17:56:23Z" id="192381367">Still, I think there's a lot of value.  There's a pretty common use case for multiple-field "document fingerprinting," including in records management and e-mail search use cases.  Usually something like the concatenation of to+from+cc+timestamp+subject and then md5/sha1/md5+sha1 hashes used for deduplication.  It would be most interesting to make sure some of those hash algorithms were available as they're industry standards for those use cases.  I'm a +1 for this overall.
</comment><comment author="bleskes" created="2016-03-07T09:05:20Z" id="193166851">@polyfractal out of curiosity - which you always manage to trigger in me :) - what use cases do you see for the structural fingerprinting? I'm also asking because I presume it will be more useful in the case  where people have many many optional fields , in which case the might want to model their data differently and put the field name as value to avoid mapping explosion. In this case we're back to content fingerprinting? 
</comment><comment author="polyfractal" created="2016-03-07T15:03:33Z" id="193286836">@bleskes For me, the use-case is parsing JSON logs (slow query logs in particular) and fingerprinting that structure.  And the situation is a bit unique, since I actually want _only_ the fingerprint and original _source, but skip the actual doc fields.

The root problem (for me) is that fingerprinting the structure of hierarchal json is much more reliable than treating the JSON as textual content and n-gram'ing it.  I was hoping this could be massaged into a general purpose feature that is used in other contexts, but I'll be 100% honest and say I dunno what else it can be used for.  It may be too special-purpose :)

Since this is essentially locality-sensitive hashing for tree structures, I suppose it could be used to cluster taxonomies, call stacks, lineage trees, etc?

I absolutely see the problem with many fields + mapping explosion.  Don't have a good answer to that :/

I suppose this could be re-imagined as a content fingerprint that expects JSON, and loads/hashes that from a single string field?
</comment><comment author="bleskes" created="2016-03-08T10:04:07Z" id="193700210">&gt; For me, the use-case is parsing JSON logs (slow query logs in particular) and fingerprinting that structure. 

@polyfractal - I see. So the "document" you are interested in is the query type / combination irrespective of the values. I.e., the tree structure, including node names is the document. Indeed the question becomes - do we see more use cases for that? o.w. we can preprocess the tree into a value (by keeping all node names and `{}`) and finger print that.
</comment><comment author="polyfractal" created="2016-03-08T16:25:46Z" id="193849148">Yep, and I'm not sure.  It definitely could be too niche.  Processing the tree into a text value is doable, but introduces a variety of problems.  For example, take this tree:

```
query:
  match:
    baz: abc,
  bool:
    must:
      match:
        bar: xyz
      match:
        baz: abc
```

How do you transform that into a textual value?  Assuming there is still some kind of processor that understand JSON and can emit node names:
- Go depth-first which maintains subtree structure:  `match baz bool must match bar match baz`.  
- Go breadth-first which maintains per-level co-ocurrence:  `match bool baz must match match bar baz`.  
- Go depth-first, but generate a value for each "branch" in the tree:  `["match baz", "bool must match bar", "bool must match baz"]`, and then use a bunch of phrase magic to accomplish per-level co-ocurrence.

I dunno, I'll keep playing.  Maybe one of those above schemes will work nicely when combined with MinHash/SimHash/Winnowing.  I definitely agree it doesn't make sense to have functionality that is special-purpose and not generally useful
</comment><comment author="polyfractal" created="2016-03-15T16:39:54Z" id="196913636">Ok, so I've been mulling this over.  I think we can simplify this proposal down to just "content fingerprinting" with a variety of algorithms.  

In the place of "structural fingerprinting", I think we should have a second ingest processor that is essentially a "recursive JSON flattener" processor.  Given a JSON _string_ (rather than a complete JSON document), it flattens it into one or several fields, according to your configurations (all keys in one field, keys-per-level fields, prepending level to key name, etc).  This also sidesteps the mapping explosion issue, since it requires the JSON to be in string form.

To accomplish structural fingerprinting, you just run it through the "flattener" first to get a set of regular fields, then use content fingerprinting.  More generic, flexible, can be used with entirely different processor chains.  I'll think on what that kind of processor would look like and open another separate proposal ticket for it.
</comment><comment author="bleskes" created="2016-03-16T15:14:23Z" id="197376660">&gt; This also sidesteps the mapping explosion issue, since it requires the JSON to be in string form.

Since all of this happens at ingest time, I don't think there is any risk of running into too many fields - if we remove the source after fingerprinting/collapsing it. This could be an option of that flatner (maybe default?) or we can use another processor. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add -XX+AlwaysPreTouch JVM flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16937</link><project id="" key="" /><description>Enables the touching of all memory pages used by the JVM heap spaces
during initialization of the HotSpot VM, which commits all memory pages
at initialization time. By default, pages are committed only as they are
needed.
</description><key id="138264506">16937</key><summary>Add -XX+AlwaysPreTouch JVM flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T18:46:43Z</created><updated>2016-09-08T13:56:33Z</updated><resolved>2016-03-10T17:12:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-08T16:56:13Z" id="193865846">@rmuir I believe you proposed this change when I brought up JVM flags, do you have any opinion one way or the other about it?
</comment><comment author="rmuir" created="2016-03-08T17:06:29Z" id="193870958">I think its probably a good idea to get into a steady state faster, at the expense of maybe slower startup. touching every page is definitely less invasive than mlockall at least. 

Users should see process RES go to full memory size immediately rather than "growing over time", which could be perceived as more confusing, or less confusing :) 
</comment><comment author="clintongormley" created="2016-03-08T18:00:49Z" id="193892948">if we turn this on, we should probably revisit our OOB defaults for min/max heap size.
</comment><comment author="jasontedor" created="2016-03-09T03:08:10Z" id="194091834">&gt; Enables the touching of all memory pages used by the JVM heap spaces
&gt; during initialization of the HotSpot VM, which commits all memory pages
&gt; at initialization time.
&gt; 
&gt; if we turn this on, we should probably revisit our OOB defaults for min/max heap size.

I read through the VM code; this only effects pages allocated at startup, so it has little effect if `InitialHeapSize &#8810; MaxHeapSize`.
</comment><comment author="jasontedor" created="2016-03-09T03:09:35Z" id="194092167">I tested the impact of this on startup time. I applied the following diff to master:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java b/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
index 107a955..754fb9b 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java
@@ -33,6 +33,7 @@ public final class Elasticsearch {
      * Main entry point for starting elasticsearch
      */
     public static void main(String[] args) throws StartupError {
+        System.exit(0);
         try {
             Bootstrap.init(args);
         } catch (Throwable t) {
```

and then timed startup time with a 32g heap:

``` bash
$ time ES_HEAP_SIZE=32g \
&gt; ~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/bin/elasticsearch                                                                                                                                                                                                                    

real    0m0.200s
user    0m0.103s
sys     0m0.115s
$ time ES_HEAP_SIZE=32g ES_JAVA_OPTS="-XX:+AlwaysPreTouch" \
&gt; ~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/bin/elasticsearch                                                                                                                                                                                 

real    0m8.190s
user    0m0.884s
sys     0m7.327s
```

And then comparing to raw startup time:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 2156590..9ff8bcb 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -194,9 +194,9 @@ final class Bootstrap {
         return InternalSettingsPreparer.prepareEnvironment(EMPTY_SETTINGS, terminal);
     }

-    private void start() {
+    private void start() throws IOException {
         node.start();
-        keepAliveThread.start();
+        stop();
     }

     static void stop() throws IOException {
```

``` bash
$ time ES_HEAP_SIZE=32g \
&gt; ~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/bin/elasticsearch                                                                                                                                                                                                                    

real    0m5.382s
user    0m9.383s
sys     0m0.447s
$ time ES_HEAP_SIZE=32g ES_JAVA_OPTS="-XX:+AlwaysPreTouch" \
&gt; ~/elasticsearch/elasticsearch-5.0.0-SNAPSHOT/bin/elasticsearch &gt; /dev/null

real    0m13.164s
user    0m10.273s
sys     0m7.366s
```

I also ran additional tests with smaller heap sizes and noted that this is linear in the initial size of the heap (matching intuition).

This is on:

``` bash
$ sudo dmidecode --type 4 | grep Version
        Version: Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz
$ sudo dmidecode --type 17 | grep " \+Speed.*MHz" | sort | uniq
        Configured Clock Speed: 2400 MHz
```

I don't think this is worth losing sleep over, but I did want to provide this information in case someone does have a good reason for keeping startup times down.
</comment><comment author="jasontedor" created="2016-03-09T03:10:16Z" id="194092269">@dakrone Can you add a note to the migration docs that startup times will be longer and that the initial resident size will be larger?
</comment><comment author="dakrone" created="2016-03-10T16:42:43Z" id="194943146">@jasontedor pushed a commit to add a note to the migration guide.
</comment><comment author="jasontedor" created="2016-03-10T16:56:59Z" id="194953787">&gt; pushed a commit to add a note to the migration guide.

Left another comment. Feel free to address at your discretion, no need for another review cycle. LGTM.
</comment><comment author="jasontedor" created="2016-09-08T13:56:33Z" id="245605307">Pretouch is going to get parallelized after [JDK-8157952](https://bugs.openjdk.java.net/browse/JDK-8157952); there is a patch for this up for review now targeting Java 9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] IndexWithShadowReplicasIT failures after writeLockTimeout removed from Engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16936</link><project id="" key="" /><description>This reproduces pretty frequently (but not all the time):

```
Caused by: [test][[test][0]] EngineCreationFailureException[failed to create engine]; nested: LockObtainFailedException[Lock held elsewhere: /var/lib/jenkins/jobs/elasticsearch/workspace/core/build/testrun/integTest/J4/temp/org.elasticsearch.index.IndexWithShadowReplicasIT_E1ABD5E13715F0B9-001/tempDir-007/test/0/index/write.lock]; nested: FileAlreadyExistsException[/var/lib/jenkins/jobs/elasticsearch/workspace/core/build/testrun/integTest/J4/temp/org.elasticsearch.index.IndexWithShadowReplicasIT_E1ABD5E13715F0B9-001/tempDir-007/test/0/index/write.lock];
11:46:57   1&gt;    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
11:46:57   1&gt;    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
11:46:57   1&gt;    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1316)
11:46:57   1&gt;    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1302)
11:46:57   1&gt;    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:892)
11:46:57   1&gt;    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:865)
11:46:57   1&gt;    at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:226)
11:46:57   1&gt;    ... 9 more
```

I've marked the suite with `AwaitsFix` (f9d1f95e846e7aae6d431fba3031d61a3bb3b1ee) as @s1monw mentioned it's simple until he gets a chance to fix it.
</description><key id="138256670">16936</key><summary>[CI] IndexWithShadowReplicasIT failures after writeLockTimeout removed from Engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>test</label></labels><created>2016-03-03T18:18:38Z</created><updated>2016-03-03T20:52:22Z</updated><resolved>2016-03-03T20:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-03T18:18:49Z" id="191893542">Related issue: https://github.com/elastic/elasticsearch/pull/16930
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add max size virtual memory check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16935</link><project id="" key="" /><description>This commit adds a bootstrap check on Linux and OS X for the max size of
virtual memory (address space) to the user running the Elasticsearch
process.
</description><key id="138254881">16935</key><summary>Add max size virtual memory check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T18:09:36Z</created><updated>2016-03-22T15:53:39Z</updated><resolved>2016-03-22T15:53:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-18T20:57:49Z" id="198540236">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to configure analyzers on keyword fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16934</link><project id="" key="" /><description>This commit allows an analyzer to be configured on a keyword field. The analyzer
must generate exactly one token for any input string. This is not something that
can be checked easily so the approach I used is to run the analyzer on a couple
test inputs and check that it generates one token as expected.

One expected use-case for this feature is to normalize case, so for ease of use
I made `lowercase` a built-in analyzer which puts a lowercase filter on top of
a keyword tokenizer.
</description><key id="138236281">16934</key><summary>Allow to configure analyzers on keyword fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2016-03-03T16:57:56Z</created><updated>2016-03-18T14:11:39Z</updated><resolved>2016-03-18T14:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-03T17:08:11Z" id="191863318">&gt; The analyzer must generate exactly one token for any input string.

Out of curiosity, what will happen if it generates more than one token? I can easily think of analyzers where the test inputs will be a single token but indexing real docs will provide multiple tokens. Are we going to hard fail documents that do that? Truncate them to the first term?
</comment><comment author="jpountz" created="2016-03-03T17:13:47Z" id="191865746">If that happens, the document will be rejected, similarly to what would happen if you provide a string value for a numeric field. I agree the current validation of analyzers is a bit of a hack.
</comment><comment author="dakrone" created="2016-03-03T17:15:03Z" id="191866406">If the document will be rejected, do we need to pre-test the analyzer with the test inputs? Is that intended to give feedback at the mapping field creation time?
</comment><comment author="jpountz" created="2016-03-03T17:17:55Z" id="191867707">I think it is still useful: given that mappings are append-only (you cannot remove fields) this gives you an error before the field is added to the mapping, so that you have an opportunity to fix the field definition. If we only checked at indexing time, then you could easily create fields that cannot be used and the only way to get rid of them would be to regenerate the whole index.
</comment><comment author="dakrone" created="2016-03-03T17:24:05Z" id="191871297">&gt; I think it is still useful: given that mappings are append-only (you cannot remove fields) this gives you an error before the field is added to the mapping, so that you have an opportunity to fix the field definition.

That makes sense.

How hard would it be to "blacklist" certain token filters from the `keyword` type? I'm thinking we could just blacklist things where the test wouldn't catch it, like decompounders
</comment><comment author="jpountz" created="2016-03-03T17:27:54Z" id="191873235">I think the main issue if we want to implement such a black list is that analyzers do not expose the components that they are made of.
</comment><comment author="jpountz" created="2016-03-03T17:33:14Z" id="191876035">If we want to start totally safe, one option would be to only accept a finite state of predefined analyzers, eg. only `keyword` and `lowercase`.
</comment><comment author="dakrone" created="2016-03-03T17:35:15Z" id="191876835">Personally, I'd rather start strict and (if deemed necesarry) go lenient later (or never), so I vote for a pre-defined number of analyzers (keyword and lowercase, as you said).
</comment><comment author="dakrone" created="2016-03-03T17:35:28Z" id="191876976">It's way harder to remove leniency than it is to add it at a later time.
</comment><comment author="jpountz" created="2016-03-03T17:56:06Z" id="191886824">@rmuir just sent me a note that Lucene has the `MultiTermAwareComponent` marker interface that we could leverage here, but unfortunately elasticsearch does not use the lucene factories, it has its own. So I think starting with a very limited set of supported analyzers makes sense and later we can look into hooking onto this `MultiTermAwareComponent` if it makes sense.
</comment><comment author="dakrone" created="2016-03-03T17:58:57Z" id="191887657">+1, that sounds great!
</comment><comment author="rmuir" created="2016-03-03T18:03:04Z" id="191889009">We can always test things are not out of sync by iterating the list in AnalysisFactoryTests and asserting our list is in sync with ones matching that interface?

This same list could be used to automatically apply a subset of the TokenFilters/CharFilters for "wildcard" etc terms. Basically the subset that won't screw everything up. And then parameters like lowerCaseWildCards can go away.

https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/index/analysis/AnalysisFactoryTests.java#L36
</comment><comment author="jpountz" created="2016-03-18T14:11:34Z" id="198377200">Closing, I will revisit it with the approach that is discussed above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent closing index during snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16933</link><project id="" key="" /><description>Closing an index that is being restored from a snapshot should fail the close operation. The previous behavior would close the index and the snapshot process would continue running indefinitely. 

Relates to #16321 
</description><key id="138224123">16933</key><summary>Prevent closing index during snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T16:20:16Z</created><updated>2016-03-08T13:21:41Z</updated><resolved>2016-03-08T13:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-03-07T20:36:42Z" id="193438004">Left a minor comment. Otherwise, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest pipeline support to reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16932</link><project id="" key="" /><description>Adds ingest pipeline support to reindex.
</description><key id="138221108">16932</key><summary>Add ingest pipeline support to reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T16:10:03Z</created><updated>2016-03-04T19:35:13Z</updated><resolved>2016-03-04T15:10:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-03T21:23:10Z" id="191970032">LGTM
</comment><comment author="nik9000" created="2016-03-04T15:10:07Z" id="192316011">And merged! Thanks @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for legacy checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16931</link><project id="" key="" /><description>Elasticsearch 5.0 doesn't support indices wiht legacy checksums anymore.
The last time we write legacy checksums was in 1.3.0 which was based
on lucene 4.9 already which means that all files have CRC32 checksums.
All indices that Elasticsearch can read today must be written with
lucene version &gt;= 4.8 anyway so we can drop this layer of backwards
compatibility entirely.

Since we are close to upgrading to Lucene 6.0 we should get rid of this
in a more contiained change than the lucene upgrade.
</description><key id="138212874">16931</key><summary>Remove support for legacy checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T15:38:46Z</created><updated>2016-03-04T08:09:19Z</updated><resolved>2016-03-04T08:09:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-03T15:51:13Z" id="191822917">Thanks for doing this separately! It is too hard to do in a lucene upgrade, because really the cleanups are needed. All lucene indexes &gt;= 5.0 have checksums.

We should commit this one first and let jenkins chew on it without mixing it with that.
</comment><comment author="rmuir" created="2016-03-03T21:15:53Z" id="191967700">thanks for this cleanup!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove writeLockTimeout from InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16930</link><project id="" key="" /><description>`writeLockTimeout` has been removed in Lucene 6 completely and since we have
the shard locking mechanism now for quite a while we don't need this anymore.
Shards should only be allocated once all resources are released such that there
can't be any other shard holding the lock to that index in any sane situation.
</description><key id="138208043">16930</key><summary>Remove writeLockTimeout from InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T15:18:57Z</created><updated>2016-03-03T16:21:48Z</updated><resolved>2016-03-03T16:21:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-03T15:19:10Z" id="191811766">@rmuir FYI
</comment><comment author="rmuir" created="2016-03-03T15:49:17Z" id="191822335">+1, this method is removed in indexwriter. locks don't sleep anymore.
</comment><comment author="mikemccand" created="2016-03-03T15:51:43Z" id="191823118">+1, wonderful.

ES is already ensuring there is no contention for this lock, so if there somehow is, it's a bug, and with this change we'll see that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scroll docs don't mention that scroll is cleared once exhausted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16929</link><project id="" key="" /><description>The documentation for `clear_scroll` [0](or scroll in general) doesn't mention that the scroll is cleaned up automatically when scroll is exhausted leading to some confusion whther a call to `clear_scroll` is required after iterating over all documents.

0 - https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html#_clear_scroll_api
</description><key id="138195042">16929</key><summary>Scroll docs don't mention that scroll is cleared once exhausted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>docs</label></labels><created>2016-03-03T14:37:47Z</created><updated>2016-08-05T08:35:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-03T15:12:27Z" id="191807048">The scroll is not cleaned up automatically, it will be removed when the timeout is reached. So `clear_scroll` is not mandatory but it can help to call it explicitly. 
@jpountz  any reason why the scroll is not cleared automatically (I tested on 2.x only) ? 
</comment><comment author="jpountz" created="2016-03-03T15:28:45Z" id="191814693">@jimferenczi I think it makes it more user-friendly: it is easier to remember that the scroll should be always cleared than that the scroll should only be cleared if not all documents were retrieved?
</comment><comment author="nik9000" created="2016-03-03T16:31:21Z" id="191840293">The rule reindex and I follow is "if the last request returned a scroll ID you need to clear it". If the last request didn't return one we wouldn't clear it. I suspect this is pretty common.
</comment><comment author="HonzaKral" created="2016-03-03T16:32:54Z" id="191841016">Well, the purpose of the scroll is to iterate over all the documents so, if all goes well, it should be exhausted in majority of cases. Making it so it is cleared automatically is then definitely more user friendly and should conserve resources.
</comment><comment author="jasontedor" created="2016-03-03T18:50:55Z" id="191910887">&gt; I think it makes it more user-friendly: it is easier to remember that the scroll should be always cleared than that the scroll should only be cleared if not all documents were retrieved?

I like the way that @jpountz is looking at it here. To add, closing automatically upon completion of the scroll would be in opposition to other resources that require explicit close whether or not you exhaust them (file handles, result sets in JDBC, etc.). This is just something that developers should be use to and I do not think that we should go against convention here.
</comment><comment author="rjernst" created="2016-03-03T19:00:29Z" id="191914343">Thinking about it from a developers perspective, it is also easier and less error prone to always clear. Having a clear in something like a finally block is simpler than a user having to check "did i exhaust? if not, also send this clear".
</comment><comment author="HonzaKral" created="2016-03-03T19:06:30Z" id="191917423">That might be so for Java and for interacting with a database over a low-level API, where you could also argue it is very common to prepare statements/queries etc.

Vast majority of elasticsearch users interact with it using the http api where the conventions are dramatically different and very seldom do you care about server-side resources, assuming the server is a black box. Not to mention that many languages people use with elasticsearch don't have a super easy try/finally construct like you are used to.

I see no reason not to clear automatically assuming we have the information we have reached the end of a scroll, we can still recommend it as a best practice to always clear, but it seems a bit weird to not do something that helps the users just to force them to do the right thing.
</comment><comment author="clintongormley" created="2016-03-04T19:03:15Z" id="192413484">&gt; The scroll is not cleaned up automatically, it will be removed when the timeout is reached. So clear_scroll is not mandatory but it can help to call it explicitly. 

Actually, this is not quite correct.  Scrolls ARE cleared automatically when `search_type=scan`, they are not cleared automatically for non scanning searches including `sort:_doc` (the replacement for scanning).

I think that all scrolls should be automatically expired once they are exhausted.  From a client perspective, why should I need to send an extra request over the network after I've finished retrieving all my docs, and wait for the response?  It just slows things down for me.

Clients using scroll helpers (available in some/most? of the clients) don't need to think about whether they should clear the scroll or not - the scroll helper should handle these things automatically.  And if not using a helper, all they need to do is to always send the clear scroll request and ignore 404 responses.
</comment><comment author="kjendrzyca" created="2016-08-05T08:35:05Z" id="237788832">&gt; Scrolls ARE cleared automatically when search_type=scan

I think this info should be added to the `scroll` documentation. This issue is the only place where I could find the infromation why I'm getting `Not Found` error when trying to close the scoll.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix use of apostrophe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16928</link><project id="" key="" /><description /><key id="138192091">16928</key><summary>Fix use of apostrophe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheDeveloper</reporter><labels><label>docs</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T14:26:34Z</created><updated>2016-03-03T18:56:16Z</updated><resolved>2016-03-03T18:56:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TheDeveloper" created="2016-03-03T14:33:05Z" id="191785950">CLA signed.
</comment><comment author="dakrone" created="2016-03-03T18:56:16Z" id="191912492">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove "_all" from RestoreSnapshotRequest#indices javadocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16927</link><project id="" key="" /><description>The comment was misleading. Setting indices to ["_all"] doesn't work, it
results in an IndexMissingException.

The `RestoreService` calls into `ShapshotUtils.filterIndices` to resolve the indices from the RestoreRequest.  And it doesn't contain any code related to `_all`. There are also no tests that indicate that `_all` can be used. (At least I couldn't find any)
</description><key id="138182929">16927</key><summary>remove "_all" from RestoreSnapshotRequest#indices javadocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>discuss</label></labels><created>2016-03-03T13:54:06Z</created><updated>2016-04-28T12:20:49Z</updated><resolved>2016-04-28T12:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-04T13:59:01Z" id="192293442">@mfussenegger i'm thinking we should fix it to work with `_all` instead.  /cc @imotov ?
</comment><comment author="mfussenegger" created="2016-03-04T14:59:43Z" id="192311842">I've actually found this in 2.1 and looking at 1.7 I believe it also doesn't work there. (I can double check that if you want)

Since this doesn't seem to be a new regression I'd rather leave _all out as restoring all indices can be accomplished by passing an empty list into the RestoreRequest. Don't see a reason to have 2 ways to accomplish the same thing.
</comment><comment author="clintongormley" created="2016-03-04T19:20:46Z" id="192425277">Purely for consistency.  We use `_all`, `*`, and sometimes no param (where it doesn't affect URL order) to represent the same thing, so it's kinda annoying to have an API that is different.
</comment><comment author="imotov" created="2016-03-04T19:59:09Z" id="192440625">@clintongormley agree, we should fix it.
</comment><comment author="mfussenegger" created="2016-03-04T20:20:21Z" id="192449137">Okay, I'll update the PR and fix it
</comment><comment author="mfussenegger" created="2016-04-28T12:20:49Z" id="215406867">Closing this - fixed it with https://github.com/elastic/elasticsearch/pull/18025
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up shard balancer by reusing shard model while moving shards that can no longer be allocated to a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16926</link><project id="" key="" /><description>Decommissioning a node or applying a filter inclusion / exclusion can potentially lead to many shards that need to be moved to other nodes. The `BalancedShardsAllocator` deals very poorly with this situation if many shards are affected: For every shard that has to leave a node due to some filter exclusion or other constraint, the whole model that represents currently allocated shards is reconstructed. If 1000 shards need to leave a node, we currently calculate the model a 1000 times.

This PR reuses the model across all shard movements in an allocation round: It calculates the shard model once and simulates the application of all shards that can be moved on this model.

Additional notes:
- I marked the PR as breaking as it makes changes to the `ShardsAllocator` interface, which might be implemented by other balancers. I still think we should backport this to v2.3.0, however.
- For maximum reuse of the model, we should fold `allocateUnassigned`, (the newly introduced) `moveShards` and `rebalance` into a single method as they are only used in AllocationService by being called in succession. I see this as a follow-up PR.
</description><key id="138171126">16926</key><summary>Speed up shard balancer by reusing shard model while moving shards that can no longer be allocated to a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>breaking-java</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T12:55:35Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-04T18:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-03T13:34:16Z" id="191762763">&gt; I marked the PR as breaking as it makes changes to the ShardsAllocator interface, which might be implemented by other balancers. I still think we should backport this to v2.3.0, however. 

++ new versions need to recompile it anyway
</comment><comment author="s1monw" created="2016-03-03T13:41:54Z" id="191764522">left some comments - this change looks great!
</comment><comment author="ywelsch" created="2016-03-04T10:52:59Z" id="192230590">@s1monw I pushed a new commit 975ef83 removing the early return. I won't change the node interleaving iteration order for now as I believe it to be still useful. I might revisit this though in a future PR.
</comment><comment author="s1monw" created="2016-03-04T11:03:37Z" id="192234836">@ywelsch can we at least have a comment on that why we have this odd iteration?
</comment><comment author="ywelsch" created="2016-03-04T11:21:10Z" id="192239586">Added a comment in 6356e08
</comment><comment author="s1monw" created="2016-03-04T16:35:23Z" id="192345662">LGTM thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain param to Analyze API not coerced</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16925</link><project id="" key="" /><description>Boolean params in the REST API are coerced from eg `"true"` to a real boolean.  The `explain` parameter in `_analyze` API fails, eg:

```
GET /_analyze?pretty=1
{
   "attributes" : [
      "keyword"
   ],
   "filters" : [
      "snowball"
   ],
   "tokenizer" : "standard",
   "text" : "&lt;text&gt;This is troubled&lt;/text&gt;",
   "explain" : "true",
   "char_filters" : [
      "html_strip"
   ]
}
```

fails with 

```
{
  "error": {
    "root_cause": [
      {
        "type": "illegal_argument_exception",
        "reason": "Unknown parameter [explain] in request body or parameter is of the wrong type[VALUE_STRING] "
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "Unknown parameter [explain] in request body or parameter is of the wrong type[VALUE_STRING] "
  },
  "status": 400
}
```
</description><key id="138169960">16925</key><summary>Explain param to Analyze API not coerced</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>bug</label><label>low hanging fruit</label><label>v2.3.0</label></labels><created>2016-03-03T12:48:42Z</created><updated>2016-03-08T08:00:19Z</updated><resolved>2016-03-08T08:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Es.index.read.missing.as.empty is not working with multiple indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16924</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_73

**OS version**: Linux 4.1.17-22.30.amzn1.x86_64

**Description of the problem including expected versus actual behavior**:
This Issue has been discussed [here](https://discuss.elastic.co/t/es-index-read-missing-as-empty-seems-not-working-with-multiple-indices/42612/1).
</description><key id="138161580">16924</key><summary>Es.index.read.missing.as.empty is not working with multiple indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DynamicScope</reporter><labels /><created>2016-03-03T12:17:36Z</created><updated>2016-03-03T12:23:13Z</updated><resolved>2016-03-03T12:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-03T12:23:13Z" id="191733151">Wrong project, I think the issue should be raised here instead: https://github.com/elastic/elasticsearch-hadoop . Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check that parent_type in Has Parent Query has child types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16923</link><project id="" key="" /><description>Closes #16692
</description><key id="138145433">16923</key><summary>Check that parent_type in Has Parent Query has child types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T10:58:47Z</created><updated>2016-03-03T16:20:03Z</updated><resolved>2016-03-03T14:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-03T14:19:47Z" id="191781443">Thanks @alexshadow007, this change looks good. I'll merge it in soon.
</comment><comment author="martijnvg" created="2016-03-03T16:20:02Z" id="191834914">backport to 2.x branch: https://github.com/elastic/elasticsearch/commit/551a78e6a4b124338bf1ece2c5666d2b6ac1e093
backport to 2.2 branch: https://github.com/elastic/elasticsearch/commit/7d0e2536e26774ad2ec0d2ab39ae1fe148c646e5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix sporadic error on match query test when a fuzziness of 0s is used&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16922</link><project id="" key="" /><description>&#8230; on a date field.
</description><key id="138144461">16922</key><summary>Fix sporadic error on match query test when a fuzziness of 0s is used&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels /><created>2016-03-03T10:54:55Z</created><updated>2016-03-03T13:29:47Z</updated><resolved>2016-03-03T13:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-03T10:55:12Z" id="191705297">@cbuescher can you take a look ?
</comment><comment author="cbuescher" created="2016-03-03T11:08:32Z" id="191710521">Great, I was also looking at this and this CI failure (http://build-us-00.elastic.co/job/es_core_master_oracle_6/5245/) right now.
</comment><comment author="cbuescher" created="2016-03-03T11:10:25Z" id="191710903">LGTM, left one tiny comment, maybe you want to add that as well.
</comment><comment author="jimczi" created="2016-03-03T11:20:44Z" id="191713489">Thanks @cbuescher. I've changed the fix to include your comment. I agree it's cleaner to check the type of the field instead of the try/catch thingy. 
</comment><comment author="cbuescher" created="2016-03-03T13:10:10Z" id="191754139">@jimferenczi thanks, looks great. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>missing results when aggregating on a long values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16921</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0.72

**OS version**: Debian 7.9 (wheezy)

**Description of the problem including expected versus actual behavior**:
I'm doing a quite simple request, just matching a keyword. Get a certain number of results. 
When doing the same keyword request, but applying an aggregation (see below) on a string field which is possibly long, i got less results, the ones with a long "log_message.raw" value are not being retrieved. 

``` bash
"aggs": {
    "2": {
      "terms": {
        "field": "log_message.raw",
        "size": 0,
        "order": {
          "_count": "desc"
        }
      }
    }
  }
```

Thought it was related to kibana, but requesting directly through CURL give me same results

Thanks !
</description><key id="138140712">16921</key><summary>missing results when aggregating on a long values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arnaudbey</reporter><labels><label>feedback_needed</label></labels><created>2016-03-03T10:37:12Z</created><updated>2016-03-04T12:26:32Z</updated><resolved>2016-03-04T12:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arnaudbey" created="2016-03-03T12:46:03Z" id="191749044">After some others search, it appears that some missing results value are not especially long 
i.e. : "PHP message: PHP Notice: Undefined variable: level in /usr/share/nginx/www/mydomain.net/releases/20160219112954/src/MyBundle/Manager/ScoreManager.php on line 217"

Any ideas ?

EDIT  : if it can help, in JSON results, i got the right number of hits, but they're not there, my hits / buckets arrays is empty... ;

``` json
{
  "took": 5,
  "timed_out": false,
  "_shards": {
    "total": 20,
    "successful": 20,
    "failed": 0
  },
  "hits": {
    "total": 4406,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "2": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": []
    }
  }
}
```
</comment><comment author="jpountz" created="2016-03-03T16:01:23Z" id="191827052">Could you give both requests ard responses so that we can compare?
</comment><comment author="arnaudbey" created="2016-03-04T07:03:26Z" id="192145703">yep, thanks.

So here is the request with the missing results : 

``` json
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "\"PHP Message\"",
          "analyze_wildcard": true
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": 1456786800000,
                  "lte": 1459461599999,
                  "format": "epoch_millis"
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "aggs": {
    "2": {
      "terms": {
        "field": "log_message.raw",
        "size": 5,
        "order": {
          "_count": "desc"
        }
      }
    }
  }
}
```

And the response 

``` json
{
  "took": 7,
  "timed_out": false,
  "_shards": {
    "total": 25,
    "successful": 25,
    "failed": 0
  },
  "hits": {
    "total": 4406,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "2": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": []
    }
  }
}
```

And now the "working" request 

``` json
{
  "size": 0,
  "sort": [
    {
      "@timestamp": {
        "order": "desc",
        "unmapped_type": "boolean"
      }
    }
  ],
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "\"PHP Message\"",
          "analyze_wildcard": true
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": 1456786800000,
                  "lte": 1459461599999,
                  "format": "epoch_millis"
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "highlight": {
    "pre_tags": [
      "@kibana-highlighted-field@"
    ],
    "post_tags": [
      "@/kibana-highlighted-field@"
    ],
    "fields": {
      "*": {}
    },
    "require_field_match": false,
    "fragment_size": 2147483647
  },
  "aggs": {
    "2": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "12h",
        "time_zone": "Europe/Berlin",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": 1456786800000,
          "max": 1459461599999
        }
      }
    }
  },
  "fields": [
    "*",
    "_source"
  ],
  "script_fields": {},
  "fielddata_fields": [
    "@timestamp"
  ]
}
```

and the response (of which I've deleted a big part of the hit part)

``` json
{
  "took": 405,
  "hits": {
    "hits": [
      {
        "_index": "logstash-2016.03.02",
        "_type": "nginx-error",
        "_id": "AVM37jQPM9mikt_pjDGf",
        "_score": null,
        "_source": {
          "message": "PHP message: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217",
          "@version": "1",
          "@timestamp": "2016-03-02T15:24:30.354Z",
          "beat": {
            "hostname": "abe",
            "name": "abe"
          },
          "count": 1,
          "fields": null,
          "input_type": "log",
          "offset": 92522828,
          "source": "/var/log/nginx/mydomain.com_error_log",
          "type": "nginx-error",
          "host": "abe",
          "tags": [
            "nginx",
            "_grokparsefailure"
          ]
        },
        "fields": {
          "@timestamp": [
            1456932270354
          ]
        },
        "highlight": {
          "message": [
            "@kibana-highlighted-field@PHP@/kibana-highlighted-field@ @kibana-highlighted-field@message@/kibana-highlighted-field@: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217"
          ]
        },
        "sort": [
          1456932270354
        ]
      },
      {
        "_index": "logstash-2016.03.02",
        "_type": "nginx-error",
        "_id": "AVM37FSJM9mikt_pjC0m",
        "_score": null,
        "_source": {
          "message": "PHP message: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217",
          "@version": "1",
          "@timestamp": "2016-03-02T15:22:23.331Z",
          "beat": {
            "hostname": "abe",
            "name": "abe"
          },
          "count": 1,
          "fields": null,
          "input_type": "log",
          "offset": 92433494,
          "source": "/var/log/nginx/mydomain.com_error_log",
          "type": "nginx-error",
          "host": "abe",
          "tags": [
            "nginx",
            "_grokparsefailure"
          ]
        },
        "fields": {
          "@timestamp": [
            1456932143331
          ]
        },
        "highlight": {
          "message": [
            "@kibana-highlighted-field@PHP@/kibana-highlighted-field@ @kibana-highlighted-field@message@/kibana-highlighted-field@: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217"
          ]
        },
        "sort": [
          1456932143331
        ]
      },
      {
        "_index": "logstash-2016.03.02",
        "_type": "nginx-error",
        "_id": "AVM37FSJM9mikt_pjC0u",
        "_score": null,
        "_source": {
          "message": "PHP message: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/",
          "@version": "1",
          "@timestamp": "2016-03-02T15:22:23.331Z",
          "beat": {
            "hostname": "abe",
            "name": "abe"
          },
          "count": 1,
          "fields": null,
          "input_type": "log",
          "offset": 92434404,
          "source": "/var/log/nginx/mydomain.com_error_log",
          "type": "nginx-error",
          "host": "abe",
          "tags": [
            "nginx",
            "_grokparsefailure"
          ]
        },
        "fields": {
          "@timestamp": [
            1456932143331
          ]
        },
        "highlight": {
          "message": [
            "@kibana-highlighted-field@PHP@/kibana-highlighted-field@ @kibana-highlighted-field@message@/kibana-highlighted-field@: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/"
          ]
        },
        "sort": [
          1456932143331
        ]
      },
      {
        "_index": "logstash-2016.03.02",
        "_type": "nginx-error",
        "_id": "AVM37FSJM9mikt_pjC0x",
        "_score": null,
        "_source": {
          "message": "PHP message: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217",
          "@version": "1",
          "@timestamp": "2016-03-02T15:22:23.331Z",
          "beat": {
            "hostname": "abe",
            "name": "abe"
          },
          "count": 1,
          "fields": null,
          "input_type": "log",
          "offset": 92435276,
          "source": "/var/log/nginx/mydomain.com_error_log",
          "type": "nginx-error",
          "host": "abe",
          "tags": [
            "nginx",
            "_grokparsefailure"
          ]
        },
        "fields": {
          "@timestamp": [
            1456932143331
          ]
        },
        "highlight": {
          "message": [
            "@kibana-highlighted-field@PHP@/kibana-highlighted-field@ @kibana-highlighted-field@message@/kibana-highlighted-field@: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217"
          ]
        },
        "sort": [
          1456932143331
        ]
      },
      {
        "_index": "logstash-2016.03.02",
        "_type": "nginx-error",
        "_id": "AVM37ETZM9mikt_pjC0P",
        "_score": null,
        "_source": {
          "message": "PHP message: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217",
          "@version": "1",
          "@timestamp": "2016-03-02T15:22:16.330Z",
          "beat": {
            "hostname": "abe",
            "name": "abe"
          },
          "count": 1,
          "fields": null,
          "input_type": "log",
          "offset": 92430810,
          "source": "/var/log/nginx/mydomain.com_error_log",
          "type": "nginx-error",
          "host": "abe",
          "tags": [
            "nginx",
            "_grokparsefailure"
          ]
        },
        "fields": {
          "@timestamp": [
            1456932136330
          ]
        },
        "highlight": {
          "message": [
            "@kibana-highlighted-field@PHP@/kibana-highlighted-field@ @kibana-highlighted-field@message@/kibana-highlighted-field@: PHP Notice:  Undefined variable: level in /usr/share/nginx/www/mydomain.com/releases/20160219112954/src/MyOrg/MyBundle/Manager/ScoreManager.php on line 217"
          ]
        },
        "sort": [
          1456932136330
        ]
      }
    ],
    "total": 4406,
    "max_score": 0
  },
  "aggregations": {
    "2": {
      "buckets": [
        {
          "key_as_string": "2016-03-01T00:00:00.000+01:00",
          "key": 1456786800000,
          "doc_count": 667
        },
        {
          "key_as_string": "2016-03-01T12:00:00.000+01:00",
          "key": 1456830000000,
          "doc_count": 3069
        },
        {
          "key_as_string": "2016-03-02T00:00:00.000+01:00",
          "key": 1456873200000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-02T12:00:00.000+01:00",
          "key": 1456916400000,
          "doc_count": 670
        },
        {
          "key_as_string": "2016-03-03T00:00:00.000+01:00",
          "key": 1456959600000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-03T12:00:00.000+01:00",
          "key": 1457002800000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-04T00:00:00.000+01:00",
          "key": 1457046000000,
          "doc_count": 0
        },
        {
          "key_as_string": "2016-03-04T12:00:00.000+01:00",
          "key": 1457089200000,
          "doc_count": 0
        },
      ]
    }
  }
}
```

Thanks for your time
</comment><comment author="jpountz" created="2016-03-04T08:26:03Z" id="192181715">I can't see any "log_message" field in your documents, maybe this is the problem?
</comment><comment author="arnaudbey" created="2016-03-04T12:26:32Z" id="192260061">Indeed... just realize that we don't have the same structure on all our objects. :confused: 
Sorry !!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query time boost for _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16920</link><project id="" key="" /><description>Index time boost in the _all field is currently done via payload. For each token/position we encode a float in the payload that is used at query time to influence the score of the query. The boost value is extracted from the mapping at index time when the value of a field is copied into _all.
For instance:

```
{
    "mappings": {
        "type1": {
            "properties": {
                "title": {
                    "type": "string",
                    "boost": 3.5
                }
            }
        }
    }
}
```

when querying the `_all` field, words that originated from the `title` field will have their score multiplied by 3.5. Unfortunately you cannot change this value without reindexing your data.
## Proposal

Now that the dynamic boost at index time is prohibited (only field boost mapping is allowed) we could investigate an hybrid approach where instead of directly encoding a boost value in the payload we would encode a small number that refers to a class of score. 
Suppose you have 3 fields in your mapping and you want to be able to score them differently in the _all fields. By assigning 3 classes of score (0, 1, 2) it is easy to remap those classes at query time into different boost values.
For instance a query with index time boost could look like this:

```
{
    "mappings": {
        "test": {
            "properties": {
                "title": {
                    "type": "string",
                    "payload": 0
                },
                "text": {
                    "type": "string",
                    "payload": 1
                },
                "author": {
                    "type": "string",
                    "payload": 2
                }
            }
        }
    }
}

{
   "query": {
     "payload_score_remap": {
         "0": 2.45,
         "1": 3.45,
         "2": 0.75,
         "mode": "max"
      },
      "query_string": {
         "query": "foo bar"
      }
   }
}
```
</description><key id="138124353">16920</key><summary>Query time boost for _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2016-03-03T09:25:37Z</created><updated>2017-03-31T09:34:20Z</updated><resolved>2017-03-31T09:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-03T17:31:59Z" id="191875466">Interesting idea.  Instead of giving the payloads numbers, it'd be nice to just refer to field names, perhaps as part of the _all field configuration.
</comment><comment author="jimczi" created="2016-03-03T17:45:11Z" id="191881296">Yes especially because the boost parameter of the field can be use for query time boosting of the field itself (and not _all field) so it's kind of confusing. The key point is that we need to assign a small unique id to the field when we add it to the _all field so that the payload's encoding stays small. The tricky part is that this unique id cannot be automatically computed for dynamic fields. If it's in the _all field configuration then it's easy to assign one each time the configuration is updated. 
Could be something like:

```
"_all": {
   "payload_fields": ["text", "title"]
}
```

... but I'll need to find a better name for "payload_field" ;)
</comment><comment author="jimczi" created="2017-03-31T09:34:19Z" id="290666209">Index time boost are deprecated in Lucene:
https://issues.apache.org/jira/browse/LUCENE-6819 and the _all field is gone.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add max number of processes check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16919</link><project id="" key="" /><description>This commit adds a bootstrap check on Linux for the max number of
processes available to the user running the Elasticsearch process.
</description><key id="138060434">16919</key><summary>Add max number of processes check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-03-03T02:58:02Z</created><updated>2016-03-08T14:00:55Z</updated><resolved>2016-03-03T16:42:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-03T05:33:29Z" id="191591512">Code here looks good to me, thanks for organizing!

I do worry about why we need to ensure 32k threads are available, it is not efficient to use so many threads and can cause a ton of problems.

I don't know which (mis)configurations this change impacts either, I use ubuntu defaults and mine are already larger than 32k.

Since its linux-specific anyway, it would be nice in the future if the error message told them exactly how to fix the problem, e.g. https://github.com/jasontedor/elasticsearch/blob/max-number-of-processes-check/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java#L96-L101
</comment><comment author="jasontedor" created="2016-03-08T14:00:55Z" id="193795000">&gt; I do worry about why we need to ensure 32k threads are available, it is not efficient to use so many threads and can cause a ton of problems.

@rmuir I opened #17003 to get the necessary number of threads down and reduced the limit to 2048.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticsearchException[failed to create shard]; nested: AccessDeniedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16918</link><project id="" key="" /><description>I keep on getting this after I reboot the service.

[2016-03-03 02:20:15,998][WARN ][indices.cluster          ] [prod-elasticsearch-10-2-25-12] [[search_v2][2]] marking and sending shard failed due to [failed to create shard]
[search_v2][[search_v2][2]] ElasticsearchException[failed to create shard]; nested: AccessDeniedException[/mnt/elasticsearch/index-data/prod_elastic_cluster/nodes/0/indices/search_v2/2/_state];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:389)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:650)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:550)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.AccessDeniedException: /mnt/elasticsearch/index-data/prod_elastic_cluster/nodes/0/indices/search_v2/2/_state
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:426)
    at java.nio.file.Files.newDirectoryStream(Files.java:413)
    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:257)
    at org.elasticsearch.index.shard.ShardPath.loadShardPath(ShardPath.java:122)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:311)
    ... 9 more

A few shards are looping between UNASSIGNED &amp; INITIALIZING.
I've checked the folder permission and it is correct. Any idea how can I fix this?
</description><key id="138055183">16918</key><summary>ElasticsearchException[failed to create shard]; nested: AccessDeniedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">teewhey</reporter><labels /><created>2016-03-03T02:23:28Z</created><updated>2016-03-03T04:22:40Z</updated><resolved>2016-03-03T02:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-03T02:28:34Z" id="191547477">&gt; I've checked the folder permission and it is correct. 

Are you sure? What user are you launching the Elasticsearch process as, and are you sure that that user has permissions for the resource in question? Be sure to check ownership and all of the `rwx` flags for the state file `/mnt/elasticsearch/index-data/prod_elastic_cluster/nodes/0/indices/search_v2/2/_state` and all parent directories.
</comment><comment author="teewhey" created="2016-03-03T02:45:02Z" id="191551496">Yes I am sure... The owners for the folders and groups belongs to "elasticsearch" user that is used to launch the process.

The cluster was previously working until I did some relocation and restarted the cluster, that's when the issue started to happen. But I do notice this : 
On the data folder where one of the failing shard is (/mnt/elasticsearch/index-data/prod_elastic_cluster/nodes/0/indices/search_v2/3/_state)

ls -al
total 8
drwxr-xr-x 2 elasticsearch elasticsearch 4096 Mar  2 10:18 .
drw-r-xr-x 5 elasticsearch elasticsearch 4096 Mar  2 10:18 ..

The folder permission for '..' is not 755. Since there is no reason to manually changing the folder permission, I'd say these are handled by elasticsearch since those that are working has 755 permission on the ".." folder.
</comment><comment author="jasontedor" created="2016-03-03T02:54:23Z" id="191554518">You've shown that the elasticsearch user does not have the executable permission for the parent directory. If the owner doesn't have the executable permission on the parent directory, it will not be able to read a subdirectory via an absolute path through that parent directory.
</comment><comment author="teewhey" created="2016-03-03T03:18:04Z" id="191558693">My point is why ".." is created by elasticsearch as 655 instead of 755 since the creation of these directories are handled by elasticsearch.
</comment><comment author="jasontedor" created="2016-03-03T03:42:12Z" id="191566165">&gt; The cluster was previously working until I did some relocation

Are you sure that you didn't commit an oops when you did the relocation?

&gt; My point is why ".." is created by elasticsearch as 655 instead of 755 since the creation of these directories are handled by elasticsearch.

Elasticsearch uses the default permissions when it creates these directories; do you have a `umask` set?
</comment><comment author="teewhey" created="2016-03-03T03:47:50Z" id="191566988">The umask is set is 0022.
During the relocation, my job queue is still running, so very likely there was some commit ops happened during relocation.
</comment><comment author="jasontedor" created="2016-03-03T04:07:49Z" id="191572255">&gt; During the relocation, my job queue is still running, so very likely there was some commit ops happened during relocation.

Just fix the permissions and you'll be fine. :smile: 
</comment><comment author="teewhey" created="2016-03-03T04:22:40Z" id="191575917">Yeah.. already done that earlier when i presented the finding earlier : ) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes the DiscoveryWithServiceDisruptionsIT#testIndicesDeleted test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16917</link><project id="" key="" /><description>In particular, this test ensures we don't restart the master node until
we know the index deletion has taken effect on master. This overcomes a
current known issue where a delete can return before cluster state
changes take effect.

Closes #16890
</description><key id="137950024">16917</key><summary>Fixes the DiscoveryWithServiceDisruptionsIT#testIndicesDeleted test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T18:15:01Z</created><updated>2016-03-09T16:38:45Z</updated><resolved>2016-03-09T15:29:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-02T22:42:56Z" id="191474999">@bleskes Your feedback would be appreciated. I tried taking the route of registering a cluster state listener (the code is commented out), but there was no way to use it to make assertions for the unit test while executing in the listener's thread.  The current approach check's the cluster state periodically before proceeding with the node restart.  I'm happy to hear your suggestions if there is a better way.
</comment><comment author="bleskes" created="2016-03-03T10:28:02Z" id="191696751">Thx @abeyad . I left some suggestion.

&gt;  This overcomes a current known issue where a delete can return before cluster state
&gt; changes take effect. 

Note that this is not an issue - the change times out due to the disruption that the test add. We report correctly by indicating that the change is not acked in the response, but the test knows that and therefore doesn't check for it. I do have some long term plans to make sure that when the call returns it is at least guaranteed to be processed on the current master, which will make things more intuitive. I don't think we want to always wait on all the nodes (with no timeout) before returning.
</comment><comment author="abeyad" created="2016-03-03T16:28:38Z" id="191838683">&gt; I do have some long term plans to make sure that when the call returns it is at least guaranteed to be processed on the current master, which will make things more intuitive.

That would be awesome.  And I changed the commit comment to reflect what you mentioned above.
</comment><comment author="abeyad" created="2016-03-03T16:42:04Z" id="191845856">@bleskes I made the changes, except for the `PUBLISH_TIMEOUT_SETTING` which caused a `FailedToCommitClusterStateException[timed out while waiting for enough masters to ack sent cluster state. [1] left]` exception.  
</comment><comment author="abeyad" created="2016-03-03T16:43:03Z" id="191846440">Also, do you think the feature itself (i.e. #11665) belongs in 2.3 as well?  I've held off for now until the test issue has been resolved.
</comment><comment author="bleskes" created="2016-03-07T09:27:01Z" id="193177617">&gt; I made the changes, except for the PUBLISH_TIMEOUT_SETTING which caused a FailedToCommitClusterStateException[timed out while waiting for enough masters to ack sent cluster state. [1] left] exception.

I missed the fact that the commit timeout defaults to publish time out - I wanted to create the situation that the commit timeout stays 30s and publish timeout is at 0. This means that the master will continue as soon as the CS as been comitted (and not wait on the isolated data node). 
</comment><comment author="bleskes" created="2016-03-07T09:27:51Z" id="193177810">&gt; Also, do you think the feature itself (i.e. #11665) belongs in 2.3 as well? I've held off for now until the test issue has been resolved.

It's a bug so I think it should go into the 2.x branch. But there is absolutely no rush. Let's get this in and stable on master first.
</comment><comment author="abeyad" created="2016-03-07T15:06:33Z" id="193288588">@bleskes I wasn't aware of the two different settings, thanks for that!  I've included a commit timeout of 30s and publish timeout of 0s.  All tests pass.
</comment><comment author="bleskes" created="2016-03-09T08:11:48Z" id="194174349">LGTM. Let's give it a go!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use and test relative time in TransportBulkAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16916</link><project id="" key="" /><description>This commit modifies TransportBulkAction to use relative time instead of
absolute time when measuring how long a bulk request took to be
processed, and adds tests for this functionality.
</description><key id="137941264">16916</key><summary>Use and test relative time in TransportBulkAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T17:36:51Z</created><updated>2016-03-03T17:01:54Z</updated><resolved>2016-03-03T17:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-02T17:48:28Z" id="191347199">@jasontedor Thanks, this looks great and I like the LongSupplier for relative time! LGTM
</comment><comment author="danielmitterdorfer" created="2016-03-03T08:06:44Z" id="191644096">@jasontedor I left one hint (I don't think that you have to change anything). Looks great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added ingest statistics to node stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16915</link><project id="" key="" /><description>The ingest stats include the following statistics:
- `ingest.ingest_total`\- The total number of document ingested during the lifetime of this node
- `ingest.ingest_time_in_millis` - The total time spent on ingest preprocessing documents during the lifetime of this node
- `ingest.ingest_current` - The total number of documents currently being ingested.
- `ingest.ingest_failed` - The total number ingest preprocessing operations failed during the lifetime of this node

Also the ingest stats contain a break down of the above stats on a per pipeline basis. These stats are automatically updated if pipelines are added or removed. This information is useful to give insight how much nodes spent on ingest related activities.
</description><key id="137932247">16915</key><summary>Added ingest statistics to node stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T17:03:18Z</created><updated>2016-03-10T12:22:38Z</updated><resolved>2016-03-10T12:22:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-05T10:01:32Z" id="192615242">looks good to me in general, maybe @pickypg or @tlrx can have a look as well, as for the new exposed stats and so on.
</comment><comment author="tlrx" created="2016-03-07T09:29:28Z" id="193178216">I left few comments
</comment><comment author="martijnvg" created="2016-03-07T12:48:01Z" id="193235905">Thanks @tlrx, I've updated the PR.
</comment><comment author="tlrx" created="2016-03-10T10:27:37Z" id="194779699">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest tests should be able to wait for running tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16914</link><project id="" key="" /><description>If a task leaks from one test case to another that should fail the test. The tests should have some ability to wait for a task to complete.

Closes #16906
</description><key id="137921409">16914</key><summary>Rest tests should be able to wait for running tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>blocker</label><label>review</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T16:22:08Z</created><updated>2016-03-14T19:05:10Z</updated><resolved>2016-03-08T16:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-02T16:22:26Z" id="191311513">@imotov can you look at this?
</comment><comment author="nik9000" created="2016-03-03T12:56:36Z" id="191751304">@rjernst I did the things!
</comment><comment author="nik9000" created="2016-03-03T16:15:16Z" id="191832155">I had a voice/video conversation with @imotov about this and I'm going to rewrite it a third time. Two things:
- You can't fail tests if there are tasks running between them. Some tasks are run in the background. For now I'm going to convert it to a WARN level log so that if the task **do** cause failure we can see it in the CI logs. This isn't a great solution but creating a task white or blacklist seems like a large maintenance burden.
- I'm going to add a `wait_for_completion` parameter to the task list API (defaults false) that, when true, will wait for the tasks that it found to finish. It'll probably just poll or something -  I have to figure that out. That way we don't need a new REST test section, all the clients can just use it, and users can use it too.
</comment><comment author="nik9000" created="2016-03-03T21:47:03Z" id="191978884">ok - I've taken a thirds stab at this. I'm not super happy with the implementation of the action but it gets the job done. It feels a bit hacky.
</comment><comment author="nik9000" created="2016-03-03T21:47:30Z" id="191979150">Also I couldn't come up with a great way to unit test the new parameter so I didn't. I'm open to suggestions though.
</comment><comment author="nik9000" created="2016-03-04T02:13:44Z" id="192062607">@imotov / @rjernst can you take another other other look at this? 
</comment><comment author="imotov" created="2016-03-07T21:22:44Z" id="193457056">Left some comments. 
</comment><comment author="nik9000" created="2016-03-08T00:00:03Z" id="193514350">@imotov have some more updates
</comment><comment author="imotov" created="2016-03-08T14:31:00Z" id="193804374">Left a small comment about setters signature. Otherwise LGTM. 
</comment><comment author="nik9000" created="2016-03-08T16:59:58Z" id="193867691">Merged! Waiting on https://github.com/elastic/elasticsearch/pull/16959 to backport.
</comment><comment author="nik9000" created="2016-03-14T14:28:53Z" id="196333296">Now that #16959 is in I am starting the backport of this. So far it has been entirely mechanical.
</comment><comment author="nik9000" created="2016-03-14T19:05:10Z" id="196476390">Backported to 2.x with 5b6779ed54b2c628e1fbb4c18518d96b8f6cec4e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Suggestion field name mandatory ctor argument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16913</link><project id="" key="" /><description>The field name is a required argument for all suggesters, but it was specified via a field() setter in SuggestionBuilder so far. This changes field name to being a mandatory constructor argument and suggestion builders to throw an error if field name is missing or the empty string.
</description><key id="137911682">16913</key><summary>Make Suggestion field name mandatory ctor argument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T15:50:21Z</created><updated>2016-03-10T18:57:13Z</updated><resolved>2016-03-03T20:54:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-02T21:18:44Z" id="191436690">@cbuescher LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex should support throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16912</link><project id="" key="" /><description>The last big feature in the "initial" phase of reindex is throttling. This ticket should track it. Right now I have no idea how to implement it or what form it'll take. But we'll need it.
</description><key id="137909235">16912</key><summary>Reindex should support throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label></labels><created>2016-03-02T15:43:17Z</created><updated>2016-04-27T09:02:35Z</updated><resolved>2016-04-27T09:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-26T20:24:36Z" id="214874880">This just missed being closed by the relevant PRs. These are the hashes in master:
da96b6e41d1b44eec8bfa170d8d0d8647e57dfef - basic throttling support
78ab6c5b7ff82da7f7d3c059b4a43d80bad188fb - dynamic rethrottling
</comment><comment author="clintongormley" created="2016-04-27T09:02:25Z" id="215018416">@nik9000 This can be closed then, no?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex's status needs to survive task completion or node termination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16911</link><project id="" key="" /><description>Right now when a reindex finishes if the user has set `wait_for_completion=false` or the http request has timed out then the reindex's status is **gone**. We should store the status on successful completion. We should also store it periodically in case the request dies or the node is `kill -9`ed or something.
</description><key id="137901166">16911</key><summary>Reindex's status needs to survive task completion or node termination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>:Task Manager</label><label>discuss</label></labels><created>2016-03-02T15:13:38Z</created><updated>2016-03-10T17:04:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-02T15:20:39Z" id="191285004">In all probability this will actually be a general feature of task management rather than specific to reindex. Reindex is the first task that isn't always ephemeral (like search) and doesn't create an artifact from which you could read the status (like snapshot and restore). It'd be lovely to have something unified rather than reindex specific. We just have reindex and, partially, snapshot/restore to work from when designing it.
</comment><comment author="nik9000" created="2016-03-09T14:37:42Z" id="194321999">One way to implement this is to write the status to an index both during task execution (to survive kill -9) and after it is completed (so it is available after the task is complete). A couple of (mostly stream of consciousness) points from about that potential implementation:
1. The user can maintain the index - Elasticsearch will write to it but someone else can delete the tasks from it when it is done.
2. The index doesn't need to support much in the way of searching. For the most part people will look up things from it by id.
3. It should be comparatively small.
4. The write after completion should be synchronous with the task finishing so that there is no time when the task's status isn't available.
5. To implement the status write during task execution we can either let the task trigger its own writes, periodically snapshot the status into the index, or both. My instinct is that these writes should be asynchronous. We might want to use something like the current time or time since start as the "version" of the status document.
6. The status will need a timestamp and an overall status enum-ish field. Something like `running` or `done` or `canceled` or something.
</comment><comment author="rjernst" created="2016-03-09T21:39:48Z" id="194519424">Could an alternative be storing the last X reindex jobs in the cluster state, and have this be a rotating buffer? Keeping full history seems like something a user could do themselves, and maintaining an entire index for this seems very heavyweight.
</comment><comment author="nik9000" created="2016-03-09T21:42:32Z" id="194521110">That's certainly an alternative. My understanding is that keeping this in the cluster state is more heavyweight because it has to be pushed to all the nodes. The index only needs to be on a couple of nodes.
</comment><comment author="rjernst" created="2016-03-09T21:46:34Z" id="194523260">&gt; The index only needs to be on a couple of nodes.

Then you get into all kinds of craziness with how many replicas do you have, and other index settings like allocation awareness. This should be small in the cluster state, and with cluster state diffs it's a minor change.

And tasks are stored in the cluster state right? So really there will be an update anyways when the task completes, so moving it to a parallel section for recently completed tasks would happen in the same cluster state update.
</comment><comment author="bleskes" created="2016-03-10T17:04:12Z" id="194957578">I don&#8217;t think we should start storing everything in the cluster state. Last I heard (correct me if I&#8217;m wrong) - the plan is to have progress status as a general feature in the task API. We don&#8217;t know how many that will be. 

The cluster state should store anything that seems like a configuration that need to be available on all nodes and be fast retrievable (like scripts ). Doing that with an index is complicated. All other things should go into an index, which is much simpler and has other advantages (like free search and aggregations). It can be single shard, single replica one. Not a big one.

&gt; On 09 Mar 2016, at 22:46, Ryan Ernst notifications@github.com wrote:
&gt; 
&gt; The index only needs to be on a couple of nodes.
&gt; 
&gt; Then you get into all kinds of craziness with how many replicas do you have, and other index settings like allocation awareness. This should be small in the cluster state, and with cluster state diffs it's a minor change.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add deprecation logging for deprecated functionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16910</link><project id="" key="" /><description>There are a number of things that have been deprecated in 2.x but are not logged in the deprecation logs.  This issue is a meta issue to track these:
- [x] Mapper attachment plugin (5.0.0 only) (@dadoonet) See #16948
- [x] Multicast plugin (@dadoonet) See #16949 
- [x] Mapping `_source` transform (@danielmitterdorfer) See #16952
- [x] Geo mapping params like `validate_*` (@nknize) See #17015
- [x] Use of `scan` search type (@jpountz) See #16980
- [x] Use of old script/template syntax (@colings86) See #16950
</description><key id="137892574">16910</key><summary>Add deprecation logging for deprecated functionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>deprecation</label><label>Meta</label><label>v2.3.0</label></labels><created>2016-03-02T14:47:30Z</created><updated>2016-03-15T09:44:49Z</updated><resolved>2016-03-15T09:44:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-02T15:17:29Z" id="191283241">@clintongormley About mapper attachment plugin there is no alternative before 5.0. So I think we should only use the deprecation logger in master branch for this plugin. WDYT?
</comment><comment author="clintongormley" created="2016-03-02T16:20:25Z" id="191310891">@dadoonet makes sense
</comment><comment author="jpountz" created="2016-03-03T10:14:19Z" id="191693247">@clintongormley warmers should not be necessary: we just ignore warmers when parsing the cluster state in 5.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add mlockall bootstrap check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16909</link><project id="" key="" /><description>This commit adds a boostrap check for the situation bootstrap.mlockall
is enabled, but memory was not able to be locked.
</description><key id="137890589">16909</key><summary>Add mlockall bootstrap check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T14:39:33Z</created><updated>2016-03-03T18:18:30Z</updated><resolved>2016-03-02T14:53:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-02T14:47:14Z" id="191266578">LGTM awesome
</comment><comment author="s1monw" created="2016-03-02T14:47:59Z" id="191266996">thinking about this, should be add this to the migration guide so folks can check ahead of time?
</comment><comment author="jasontedor" created="2016-03-02T14:49:19Z" id="191267590">&gt; thinking about this, should be add this to the migration guide so folks can check ahead of time?

I agree. I'll open a PR that adds the checks that have been recently been added to the migration docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display if node is snapshot build on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16908</link><project id="" key="" /><description>Recent simplifications to org.elasticsearch.Version led to "-SNAPSHOT"
to not be displayed in the node version upon startup. Yet, this is
useful information to have. This simple commit adds back the display of
"-SNAPSHOT" in the version on startup if the build is a snapshot build.
</description><key id="137881043">16908</key><summary>Display if node is snapshot build on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T14:01:00Z</created><updated>2016-03-02T16:10:33Z</updated><resolved>2016-03-02T14:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-02T14:01:18Z" id="191248394">This gives

```
[2016-03-02 09:00:38,072][INFO ][node                     ] [Caiman] version[5.0.0-SNAPSHOT], pid[98009], build[1d60a21/2016-03-02T13:53:40.250Z]
```

on startup instead of

```
[2016-03-02 08:49:25,553][INFO ][node                     ] [Needle] version[5.0.0], pid[96334], build[2e8025f/2016-03-02T13:41:37.049Z]
```
</comment><comment author="tlrx" created="2016-03-02T14:07:59Z" id="191250041">LGTM
</comment><comment author="jasontedor" created="2016-03-02T14:41:51Z" id="191263756">Thanks @tlrx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch monitoring support for Dynatrace Application Monitoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16907</link><project id="" key="" /><description>Dynatrace provides functionality which can be used to fetch health data and monitor multiple Elasticsearch cluster.
</description><key id="137878728">16907</key><summary>Elasticsearch monitoring support for Dynatrace Application Monitoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels><label>docs</label></labels><created>2016-03-02T13:50:42Z</created><updated>2016-03-02T14:25:22Z</updated><resolved>2016-03-02T14:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T14:25:22Z" id="191257804">thanks @centic9 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex's rest tests fail frequently in CI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16906</link><project id="" key="" /><description>https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/487/console
etc
</description><key id="137871495">16906</key><summary>Reindex's rest tests fail frequently in CI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label></labels><created>2016-03-02T13:17:38Z</created><updated>2016-03-08T16:54:47Z</updated><resolved>2016-03-08T16:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-02T13:20:09Z" id="191236081">I'll be fixing these soon but I'll use this ticket to silence them.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix potential NPE in SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16905</link><project id="" key="" /><description>Prevents a NPE when parsing `fields` parameter and internal field name
list has not been initialized.

Closes #16902
</description><key id="137857488">16905</key><summary>Fix potential NPE in SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Java API</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T12:10:01Z</created><updated>2016-03-10T18:56:45Z</updated><resolved>2016-03-02T12:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-02T12:14:33Z" id="191215411">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce the notion of hidden indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16904</link><project id="" key="" /><description>With the setting `index.hidden` an index can be marked as hidden which
prevents the index from being resolved in index expressions or with `_all` / `*`.
Yet, if the index is access explicitly or the expression resolves to an alias pointing
to a hidden index the index is accessible. This is mainly a use case to prevent accidential
access of a metadata or cluster private index from an ordinary search.
</description><key id="137857091">16904</key><summary>Introduce the notion of hidden indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2016-03-02T12:07:58Z</created><updated>2016-09-13T10:10:05Z</updated><resolved>2016-09-13T10:09:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-02T13:33:07Z" id="191239394">left a few comments, one other general question is why do we consider aliases different compared to wildcard expressions? Shall we consider hiding those indices even when expanding aliases?
</comment><comment author="s1monw" created="2016-03-02T13:57:09Z" id="191247385">&gt; Shall we consider hiding those indices even when expanding aliases?

if you do that this stuff get's very hairy. I think aliases are different here they don't happen by accident they are like an explicit index. if you sym-link a hidden directory on disk it shows up too though 
</comment><comment author="javanna" created="2016-03-02T14:08:23Z" id="191250134">&gt; if you sym-link a hidden directory on disk it shows up too though

I totally see this point, I think I was focusing mainly on aliases that point to multiple indices though... that break the directory analogy I believe. Fine with me though!
</comment><comment author="s1monw" created="2016-03-02T15:00:51Z" id="191275124">@javanna @clintongormley I think it's ready
</comment><comment author="clintongormley" created="2016-03-02T16:19:57Z" id="191310722">LGTM - we still need to add all the index options to the specs for all rest api's which support multi-indices, but i can do that in a follow up PR
</comment><comment author="s1monw" created="2016-03-02T19:34:27Z" id="191389892">&gt; LGTM - we still need to add all the index options to the specs for all rest api's which support multi-indices, but i can do that in a follow up PR

@clintongormley I think they are not added anywhere really
</comment><comment author="dakrone" created="2016-09-12T21:26:36Z" id="246499401">@javanna I think this is waiting for your final +1 before being merged?
</comment><comment author="javanna" created="2016-09-13T09:11:41Z" id="246622207">&gt; @javanna I think this is waiting for your final +1 before being merged?

not really...but the PR was opened 6 months ago so who knows why it wasn't merged. I guess we decided not to do it? @s1monw do you remember?
</comment><comment author="lukas-vlcek" created="2016-09-13T09:42:49Z" id="246629793">Will it be possible to learn which indices are hidden in `_cat` API?
</comment><comment author="s1monw" created="2016-09-13T10:09:53Z" id="246636240">I am going to close this - we won't do this any time soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply a wildcard query to a nested object.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16903</link><project id="" key="" /><description>I have the below structure (small part of a very large elastic-search document)

```
sample: {
   {
       "md5sum":"4002cbda13066720513d1c9d55dba809",
       "id":1,
       "sha256sum":"1c6e77ec49413bf7043af2058f147fb147c4ee741fb478872f072d063f2338c5",
       "sha1sum":"ba1e6e9a849fb4e13e92b33d023d40a0f105f908",
       "created_at":"2016-02-02T14:25:19+00:00",
       "updated_at":"2016-02-11T20:43:22+00:00",
       "file_size":188416,
       "type":{
          "name":"EXE"
       },
       "tags":[

       ],
       "sampleSources":[
          {
             "filename":"4002cbda13066720513d1c9d55dba809",
             "source":{
                "name":"default"
             }
          },
         {
             "filename":"4002cbda13066720332513d1c9d55dba809",
             "source":{
                "name":"default"
             }
          }
       ]
    }
}
```

The filter I would like to use is to find by the 'name' contained within `sample.sampleSources.source` using elastic search. I would like to use the wildcard query for the name. Below is the query I am trying.

```
GET /app/sample/_search
{
   "query": {
      "nested": {
         "path": "sampleSources.source",
         "query": {
             "wildcard": {
                "sampleSources.source.name": "def*"
             }
         }
      }
   }
} 
```

Below is the mapping

```
{  
   "app":{  
      "mappings":{  
         "sample":{  
            "sampleSources":{  
               "type":"nested",
               "properties":{  
                  "filename":{  
                     "type":"string"
                  },
                  "source":{  
                     "type":"nested",
                     "properties":{  
                        "name":{  
                           "type":"string"
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```

The elastic search version which I have is 1.5.2, lucene version 4.10.4. I don't see a reason why this not not work, can someone please guide? 

P.S. Utmost apologies, since I feel the issues in here relate to elastic search as a whole not in the actual application of it.
</description><key id="137845687">16903</key><summary>Apply a wildcard query to a nested object.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aghatnekar</reporter><labels /><created>2016-03-02T11:19:10Z</created><updated>2016-03-02T11:43:48Z</updated><resolved>2016-03-02T11:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T11:43:48Z" id="191202572">Hi @aghatnekar 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI failure] NPE in SearchSourceBuilderTests.testFromXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16902</link><project id="" key="" /><description>This happened a few times since yesterday:

```
java.lang.NullPointerException
    at __randomizedtesting.SeedInfo.seed([ECE681D21D988644:CD8EB4CA943F669E]:0)
    at org.elasticsearch.search.builder.SearchSourceBuilder.parseXContent(SearchSourceBuilder.java:782)
    at org.elasticsearch.search.builder.SearchSourceBuilder.fromXContent(SearchSourceBuilder.java:743)
    at org.elasticsearch.search.builder.SearchSourceBuilder.parseSearchSource(SearchSourceBuilder.java:111)
    at org.elasticsearch.search.builder.SearchSourceBuilderTests.assertParseSearchSource(SearchSourceBuilderTests.java:468)
    at org.elasticsearch.search.builder.SearchSourceBuilderTests.testFromXContent(SearchSourceBuilderTests.java:458)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```

Reproduces with e.g. `gradle :core:test -Dtests.seed=136E458E684C15F5 -Dtests.class=org.elasticsearch.search.builder.SearchSourceBuilderTests -Dtests.method="testFromXContent" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=de-AT -Dtests.timezone=America/Coral_Harbour`
</description><key id="137843836">16902</key><summary>[CI failure] NPE in SearchSourceBuilderTests.testFromXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label></labels><created>2016-03-02T11:10:24Z</created><updated>2016-03-02T12:17:50Z</updated><resolved>2016-03-02T12:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add deprecation logging for deprecated queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16901</link><project id="" key="" /><description>This adds deprecation logging for the `and`, `fquery`, `filtered`, `or` an
`query` queries.

Closes #16885
</description><key id="137831659">16901</key><summary>Add deprecation logging for deprecated queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>v2.3.0</label></labels><created>2016-03-02T10:14:44Z</created><updated>2016-03-16T19:36:26Z</updated><resolved>2016-03-03T09:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T11:38:35Z" id="191200449">@jpountz also the `limit` query
</comment><comment author="clintongormley" created="2016-03-02T11:58:43Z" id="191210934">And the `missing` query
</comment><comment author="jpountz" created="2016-03-02T12:37:11Z" id="191224344">Hmm I missed them because they were in a different section of te breaking changes. I'll add deprecation logging for them as well.
</comment><comment author="jpountz" created="2016-03-02T13:08:59Z" id="191232689">I pushed a new commit that adds deprecation logging for `limit` and `missing`.
</comment><comment author="clintongormley" created="2016-03-02T14:47:19Z" id="191266615">thanks @jpountz 

Is it possible to log the original request that used the deprecated queries? Would make them easier to find, but I'm guessing that this would require a much bigger change.
</comment><comment author="jpountz" created="2016-03-02T14:57:02Z" id="191272092">@clintongormley that would indeed be a much larger change :(
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the field mapping index time boost into a query time boost.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16900</link><project id="" key="" /><description>Index time boost will still be applied for indices created before 5.0.0.
</description><key id="137828319">16900</key><summary>Change the field mapping index time boost into a query time boost.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T09:59:10Z</created><updated>2016-03-04T10:50:37Z</updated><resolved>2016-03-04T10:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-02T10:01:28Z" id="191166251">Originated from https://github.com/elastic/elasticsearch/issues/12394#issuecomment-190107719
</comment><comment author="jpountz" created="2016-03-02T13:41:09Z" id="191241927">I think the doc changes might break some links (@clintongormley should be able to confirm) but otherwise it looks good to me. Thanks!
</comment><comment author="jimczi" created="2016-03-03T10:58:42Z" id="191706665">@jpountz I've removed the docBoost from the context and added an entry in the redirects.asciidoc.
Could you take a look ? 
</comment><comment author="jpountz" created="2016-03-03T13:34:37Z" id="191762828">This looks great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store _all payloads on 1 byte instead of 4.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16899</link><project id="" key="" /><description>This changes the `_all` field to store per-field boosts using a single byte
similarly to norms.

For some reason I could not regenerate the bw indices for beta and rc
releases. I still need to look into it.
</description><key id="137825164">16899</key><summary>Store _all payloads on 1 byte instead of 4.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T09:46:44Z</created><updated>2016-03-03T14:02:15Z</updated><resolved>2016-03-03T14:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-02T10:27:25Z" id="191173977">The loss in precision should not affect every user but we should add a small note in the migration docs, don't you think ?
</comment><comment author="jpountz" created="2016-03-02T12:36:30Z" id="191224027">Good point, will do!
</comment><comment author="jpountz" created="2016-03-02T16:34:47Z" id="191316768">@jimferenczi I pushed a new commit.
</comment><comment author="jimczi" created="2016-03-02T17:50:22Z" id="191348084">I don't think we should encode a boost value directly. We've started to change index time boost into query time boost in the other field types so I guess it would be nice to have some sort of query time boost in the _all field as well. For instance we could encode the class of score this payload refers to and change the score of each class at query time. 

```
PUT test
{
    "mappings": {
        "type1": {
            "properties": {
                "title": {
                    "type": "string",
                    "boost": 1
                }
            }
        }
    }
}
```

The boost value is an integer (or a short or even a byte but the key point is to have small numbers so that we can use vint to encode the payload) that refers to the first class score. At query time we could remap the payloads with a value of 1 to any float value we want.

@jpountz I am not saying that we should not merge this PR but other solutions that would let the user change the boost for _all fields at query time should be investigated before changing the format ? 
</comment><comment author="jpountz" created="2016-03-03T08:45:16Z" id="191655932">+1 on exploring this idea but I think we should merge this PR first as this proposal is a larger change and has some challenges.
</comment><comment author="jimczi" created="2016-03-03T08:59:23Z" id="191660301">Ok thanks @jpountz. I'll open a ticket for the index time boosting in _ all fields.
LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DiscoveryNode#shouldConnectTo method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16898</link><project id="" key="" /><description>`DiscoveryNode#shouldConnectTo` was originally introduced to prevent client nodes from connecting to other client nodes directly in the cluster. That said, it worked only if `node.client` was set to `true` and not when `node.master` and `node.data` were both set to `false`. It looks safe to remove then, which allows to solve all kinds of problems around monitoring that happen wherever there are 2 or more clients nodes in the cluster, and one of them gets hit by a request.

I tested manually that all of the problems listed [here](https://github.com/elastic/elasticsearch/issues/16815#issuecomment-190842194) are solved by this fix. Automated testing is not ideal here as we rely on an integration test that uses the java api. Nonetheless the test failed without the fix and is now consistently green. The best way to test this would be to be able to run REST tests against an external cluster with two or more clients nodes, but our test infra does not yet support that.
</description><key id="137824710">16898</key><summary>Remove DiscoveryNode#shouldConnectTo method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-02T09:45:20Z</created><updated>2016-03-02T22:04:31Z</updated><resolved>2016-03-02T22:04:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-02T09:57:07Z" id="191164129">THanks @javanna . Change looks good to me. Left a comment on the test. I will add unit testing for the connection in #16788 
</comment><comment author="javanna" created="2016-03-02T14:14:29Z" id="191252169">@bleskes I pushed a new commit that should make you happy ;)
</comment><comment author="bleskes" created="2016-03-02T14:39:21Z" id="191262928">I'm happy. Thx Luca. We can extend the node selector to cover more cases later (if not already covered by another test..). Lgtm ;)
</comment><comment author="javanna" created="2016-03-02T16:13:48Z" id="191307873">@bleskes out of curiosity, how could we expand the node selector even more, I thought randomizing attributes would make sure we cover every possible node? What have I missed? :)
</comment><comment author="javanna" created="2016-03-02T19:05:45Z" id="191375529">Clarified with Boaz, we were not testing yet the different ways to select nodes as part of the request, I added that as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed create index on 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16897</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**:
java version "1.8.0_72"
Java(TM) SE Runtime Environment (build 1.8.0_72-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.72-b15, mixed mode)

**OS version**:
Darwin imac-2.local 15.3.0 Darwin Kernel Version 15.3.0: Thu Dec 10 18:40:58 PST 2015; root:xnu-3248.30.4~1/RELEASE_X86_64 x86_64

**Description of the problem including expected versus actual behavior**:
I just create index by rest api.
[Failed Index name]
$ curl -XPUT "http://localhost:9200/bulk-indexer-config" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index": {
      "refresh_interval": "5m"
    }
  },
  "mappings": {
    "default": {
      "_all": {
        "enabled": false
      }
    }
  }
}'
and then i got some error.
But It is ok that I changed name of index which is bulk_indexer_config from dash to under bar.

[Successful Index name]
$ curl -XPUT "http://localhost:9200/bulk_indexer_config" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index": {
      "refresh_interval": "5m"
    }
  },
  "mappings": {
    "default": {
      "_all": {
        "enabled": false
      }
    }
  }
}'

**Steps to reproduce**:
1. If you create index with my script, you can show the error.
   2.
   3.

**Provide logs (if relevant)**:
==== error logs ====
{
  "error" : {
    "root_cause" : [ {
      "type" : "invalid_type_name_exception",
      "reason" : "mapping type name [_all] can't start with '_'"
    } ],
    "type" : "mapper_parsing_exception",
    "reason" : "Failed to parse mapping [_all]: mapping type name [_all] can't start with '_'",
    "caused_by" : {
      "type" : "invalid_type_name_exception",
      "reason" : "mapping type name [_all] can't start with '_'"
    }
  },
  "status" : 400
}

**Describe the feature**:
I think, This is error for meta field mapper.
Would you solve this problem?

Thanks.
</description><key id="137784909">16897</key><summary>failed create index on 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HowookJeong</reporter><labels><label>feedback_needed</label></labels><created>2016-03-02T06:18:35Z</created><updated>2016-03-03T16:26:59Z</updated><resolved>2016-03-03T16:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T08:47:43Z" id="191132237">Hi @HowookJeong 

Your recreation of the problem works just fine - it happily creates the index.  The error message you show indicates that your request was incorrect and you had the `_all` field just under `mappings` instead of where you show it in your request.

Please try again and, if you can still create the issue, paste the actual requests that you ran.
</comment><comment author="HowookJeong" created="2016-03-03T09:24:15Z" id="191674087">I tried create index.

Case 1) failure
$ curl -XPUT "http://localhost:49200/hello-world?pretty" -d'

&gt; {
&gt; "settings": {
&gt; "number_of_shards": 1,
&gt; "number_of_replicas": 1,
&gt; "index": {
&gt; "refresh_interval": "5m"
&gt; }
&gt; },
&gt; "mappings": {
&gt; "_all":{
&gt; "enabled":false
&gt; },
&gt; "default": {
&gt; }
&gt; }
&gt; }
&gt; }'
&gt; {
&gt;   "error" : {
&gt;     "root_cause" : [ {
&gt;       "type" : "invalid_type_name_exception",
&gt;       "reason" : "mapping type name [_all] can't start with '_'"
&gt;     } ],
&gt;     "type" : "mapper_parsing_exception",
&gt;     "reason" : "Failed to parse mapping [_all]: mapping type name [_all] can't start with '_'",
&gt;     "caused_by" : {
&gt;       "type" : "invalid_type_name_exception",
&gt;       "reason" : "mapping type name [_all] can't start with '_'"
&gt;     }
&gt;   },
&gt;   "status" : 400
&gt; }

Case 2) success
$ curl -XPUT "http://localhost:49200/hello-world?pretty" -d'

&gt; {
&gt; "settings": {
&gt; "number_of_shards": 1,
&gt; "number_of_replicas": 1,
&gt; "index": {
&gt; "refresh_interval": "5m"
&gt; }
&gt; },
&gt; "mappings": {
&gt; "default": {
&gt; }
&gt; }
&gt; }
&gt; }'
&gt; {
&gt;   "acknowledged" : true
&gt; }

I already read this document which is "https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html#disabling-all-field" and _all setting is declared in the type of inside.
So, i think the below settings is right.
"mappings": {
"default": {
"_all": {
"enabled": false
}

If I was wrong, would you help me how i can set way of _all configure.
Thanks.
</comment><comment author="clintongormley" created="2016-03-03T16:22:28Z" id="191835975">@HowookJeong I copied and pasted your curl commands and they worked correctly on 2.2.0.  I am unable to recreate this problem
</comment><comment author="clintongormley" created="2016-03-03T16:26:08Z" id="191837312">if you are trying to disable the `_all` field by default for all future types in the same index, the syntax is:

```
curl -XPUT "http://localhost:9200/hello-world" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index": {
      "refresh_interval": "5m"
    }
  },
  "mappings": {
    "_default_": {
      "_all": {
        "enabled": false
      }
    }
  }
}'
```

To disable it for a particular type (eg `some_type`) then it'd be:

```
curl -XPUT "http://localhost:9200/hello-world" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index": {
      "refresh_interval": "5m"
    }
  },
  "mappings": {
    "some_type": {
      "_all": {
        "enabled": false
      }
    }
  }
}'
```
</comment><comment author="clintongormley" created="2016-03-03T16:26:59Z" id="191837801">You should head over to the forum to ask questions like these: https://discuss.elastic.co/ - there's a great community of people who will help.

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds more context to the min/max settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16896</link><project id="" key="" /><description>Closes #16115
</description><key id="137750333">16896</key><summary>Adds more context to the min/max settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-03-02T02:14:15Z</created><updated>2016-03-03T16:17:33Z</updated><resolved>2016-03-03T16:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T10:47:42Z" id="191183439">Honestly, this sentence confuses me more than the original.  The min/max settings provide absolute min and max values which can be used to limit the value calculated when size is set to a percentage.  But that's what the docs already say, eg:

&gt; If the `index_buffer_size` is specified _as a percentage_, then this setting can be used to specify an _absolute maximum_
</comment><comment author="markwalkom" created="2016-03-03T07:39:04Z" id="191633029">Ok. I can't really think of another way to phrase it at this time, so feel free to close this off.
</comment><comment author="clintongormley" created="2016-03-03T16:17:33Z" id="191833481">thanks @markwalkom 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Triangle and Hex Grid Encoding for GeoGridAggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16895</link><project id="" key="" /><description>A triangular tessellation based `geo_point` encoding supports aggregating on grids other than basic geohash (e.g., triangular, hex). These aggregations provide better spatial visualization but are also commonly used in GIS analysis applications due to their equal area characteristics. This feature issue adds a `grid_type` parameter to `GeoHashGridParams` (which will likely need to be refactored to `GeoGridParams`) to allow users to specify different grid types such as `geohash`, `triangular`, `hexagonal`. We should also investigate generalizing `geohash_grid` aggregation to a more general `geo_grid` aggregation so we can provide different grid definitions.
</description><key id="137740862">16895</key><summary>Add Triangle and Hex Grid Encoding for GeoGridAggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>discuss</label><label>feature</label></labels><created>2016-03-02T01:09:25Z</created><updated>2017-03-29T13:50:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="djminkus" created="2017-02-07T19:57:09Z" id="278121427">Is a solution for this still desired?</comment><comment author="thomasneirynck" created="2017-03-29T13:50:53Z" id="290096012">Would this include also quad_keys, similar to Bing Maps (https://msdn.microsoft.com/en-us/library/bb259689.aspx)?

The tile-pyramids employed by most mapping services (Web Mercator, 2x2 division) are easier to work with from a front-end perspective than geohash-grids due to the uniform scaling horizontal/vertical across zoom levels.

So something like https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-geohashgrid-aggregation.html, but bucketing by tile in the quad-tree.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe client node settings should be validated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16894</link><project id="" key="" /><description>As part of #13383 and #15300, tribe nodes were changed to explicitly pass relevant settings to the internal client nodes they create. Also, with the new settings validation, the per client settings are validated when the underlying node is created. However, _all_ normal node settings are allowed for that validation. We should instead be explicit about which settings are allowed, and reject others. For example, passing `path.home` (or any of the `path.*` settings) doesn't make sense and is ignored for these client nodes. 
</description><key id="137713621">16894</key><summary>Tribe client node settings should be validated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2016-03-01T23:00:52Z</created><updated>2016-10-06T20:03:53Z</updated><resolved>2016-10-06T20:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-10-06T20:03:52Z" id="252072925">Tribe settings like `path.*` are no longer allowed (as of 5.0).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Passthrough environment and network settings to tribe client nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16893</link><project id="" key="" /><description>In 2.2, the client nodes created internally by a tribe node were changed
to be explicit about which settings the client nodes use, no longer
loading all settings from elasticsearch.yml. However, some settings were
missed, notably network bind settings. This change adds those settings
to be passed through, as well as adds unit tests for building the tribe
client node settings.
</description><key id="137709819">16893</key><summary>Passthrough environment and network settings to tribe client nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Tribe Node</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T22:47:03Z</created><updated>2016-03-09T08:53:13Z</updated><resolved>2016-03-09T08:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TinLe" created="2016-03-02T05:57:52Z" id="191080305">These changes to how tribe nodes settings get passed through is also going to break plugins that has settings in elasticsearch.yml.
</comment><comment author="bleskes" created="2016-03-02T09:43:39Z" id="191158460">Thanks @rjernst for picking this up quickly. Left comments. I think it would be good to get more eyes on this. @javanna can you take a look too?
</comment><comment author="clintongormley" created="2016-03-02T11:01:01Z" id="191188559">&gt; These changes to how tribe nodes settings get passed through is also going to break plugins that has settings in elasticsearch.yml.

@TinLe could you provide an example of what you mean?
</comment><comment author="TinLe" created="2016-03-02T17:04:44Z" id="191327230">@clintongormley I mean any plugins that uses properties setting in elasticsearch.yml is not seeing their properties in when ES is running in tribe mode.

For example, search-guard-ssl.   In tribe mode, none of the "searchguard.*" properties are passed through to the plugin.   The minute I commented out all tribe settings, the plugin see the properties.
</comment><comment author="TinLe" created="2016-03-02T22:31:18Z" id="191466584">Figured it out.   All of the plugin properties must now be copied and put under the tribe section, one per each downstream cluster for them to get passed through.

These bugs are so annoying!
</comment><comment author="clintongormley" created="2016-03-07T17:07:59Z" id="193349345">I had a chat to Ryan about this.  There are essentially two options:
- Pass through no settings from tribe-&gt;node except for an explicit whitelist (plus any settings specified at the node client level in the yml)
- Pass through all settings from tribe-&gt;node, except for an explicit blacklist (plus any settings specified at the node client level in the yml)

The first option has the advantage that any missing settings can be added to node client level in the yml.  The second has the disadvantage that there is no way to STOP settings beings passed down.

I think we have to choose the first option: an explicit whitelist, essentially what we do today.  

For 2.2.1, which is a bug fix release, we should:
- add any missing settings that are obvious whitelist candidates
- document which settings are inherited from the tribe node

Later, we should provide a mechanism for plugins to register a list of settings to add to the whitelist.
</comment><comment author="rjernst" created="2016-03-07T21:00:31Z" id="193446599">@bleskes @javanna I pushed a new commit.
</comment><comment author="bleskes" created="2016-03-08T12:33:10Z" id="193766521">LGTM but to one extra passthrough setting. Thanks @rjernst 
</comment><comment author="rjernst" created="2016-03-09T08:53:13Z" id="194188211">2.x: 66f1709
2.2: 43fd8f0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can we avoid calling File.exists for every green shard on every routing table change?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16892</link><project id="" key="" /><description>When shard a relocated away from nodes, we need to delete the local copy. This is done by the `IndicesStore` class and it's super careful for obvious reasons.

The current logic is to go through the following for every cluster state published with a routing table change (for every shard):
1) If the shard is not green, skip.
2) If the shard is assigned locally, skip.
4) If the shard is green call file.exist to see if there is a local copy. If not skip.
5) Send requests to all nodes holding the shard and ask them if they still have it open.
6) When all nodes respond with no error _and_ nothing has happened in the mean time (i.e. no new cluster state was process), delete the shard after acquiring  the local shard lock.

It turns out that (3) can taking serious time (&gt;30s in aggregate) for people with lots of shards (we saw 13K primaries). This delays cluster state processing and brings the cluster into trouble.

To fix this we can't simply check if the published CS in question actually removes a shard from the local node, because of (6) which relies on any new CS to trigger a new check.

We can be more relaxed in 6 and delete anyway _if_ we can get the lock (we already do it on the cluster state thread).

An alternative is to have a local in memory map that remembers whether a shard was checked locally and no folder was found. That map can be reset whenever a shard is introduced to the node.

Thoughts?
</description><key id="137683016">16892</key><summary>Can we avoid calling File.exists for every green shard on every routing table change?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2016-03-01T20:51:15Z</created><updated>2017-07-14T12:55:59Z</updated><resolved>2017-07-14T12:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-01T22:44:44Z" id="190943677">@bleskes can you link to where in the code we do the check you want to remove?
</comment><comment author="bleskes" created="2016-03-02T10:19:38Z" id="191171943">@dakrone I meant [this](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java#L253) . Note that it's not just removing this check but also checking that there was a change in allocation of the current node (o.w. will keep doing that file exists check).
</comment><comment author="ywelsch" created="2017-07-14T12:55:59Z" id="315352822">Fixed by #21438</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Glossary should contain query and filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16891</link><project id="" key="" /><description>The [glossary](https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html) should contain the terms `query` and `filter`. Both of these are used extensively in the documentation and folks who wander to the glossary aren't going to know the difference. [This](https://www.elastic.co/guide/en/elasticsearch/guide/current/_queries_and_filters.html) page in the guide is helpful but I get the sense people don't always find their way there.
</description><key id="137659418">16891</key><summary>Glossary should contain query and filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2016-03-01T19:11:16Z</created><updated>2016-03-01T19:11:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>DiscoveryWithServiceDisruptionsIT.testIndicesDeleted fails on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16890</link><project id="" key="" /><description>This test was enabled after the completion of #11665, but sporadically fails with the following reproduce:

`gradle :core:integTest -Dtests.seed=4A03B4FF87C5DACC -Dtests.class=org.elasticsearch.discovery.DiscoveryWithServiceDisruptionsIT -Dtests.method="testIndicesDeleted" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=he-IL -Dtests.timezone=Asia/Istanbul`
</description><key id="137657587">16890</key><summary>DiscoveryWithServiceDisruptionsIT.testIndicesDeleted fails on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T19:02:58Z</created><updated>2016-03-09T15:29:24Z</updated><resolved>2016-03-09T15:29:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make search failure cause rest failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16889</link><project id="" key="" /><description>Indexing failures have caused the reindex http request to fail for a while
now. Both search and indexing failures cause it to abort. But search
failures didn't cause a non-200 response code from the http api. This
fixes that.

Also slips in a fix to some infrequently failing rest tests.

Closes #16037
</description><key id="137655770">16889</key><summary>Make search failure cause rest failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T18:54:00Z</created><updated>2016-03-14T20:14:00Z</updated><resolved>2016-03-10T18:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T10:38:18Z" id="191178481">@nik9000 while you're on this, I think this issue would be of interest to you too: https://github.com/elastic/elasticsearch/issues/16555#issuecomment-183676958
</comment><comment author="dakrone" created="2016-03-10T16:27:38Z" id="194936053">LGTM left one minor comment
</comment><comment author="nik9000" created="2016-03-14T20:14:00Z" id="196501813">Backported to 2.x with 7b1d567cd21c7dc3d7f2cd7fad889826125cd627
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecated stuff should show up as deprecated in the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16888</link><project id="" key="" /><description>Right now the index on the right hand side of the docs page contains all the deprecated stuff on the same level as the non-deprecated stuff. It should really be marked as deprecated somehow in the index. See https://imgur.com/qiDPRcn for a nice picture of it.

It'd be nice for someone who is just coming to Elasticsearch for the first time to be able to skip the deprecated stuff entirely but I don't see how that is possible without a crazy maintenance burden. Maybe it is enough to just mark the deprecated stuff in the index as deprecated and sort it to the bottom of the lists. Or something. I dunno.

Reported [here](https://www.reddit.com/r/elasticsearch/comments/48h34n/elasticsearch_rest_api_inconsistent/).
</description><key id="137650418">16888</key><summary>Deprecated stuff should show up as deprecated in the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label><label>docs</label><label>feature</label></labels><created>2016-03-01T18:28:20Z</created><updated>2016-03-01T18:28:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Limit query's docs use filtered query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16887</link><project id="" key="" /><description>The filtered query is deprecated but it is still used in the docs for the limit query:
https://imgur.com/mZSsN0r

I suspect this is a leftover from the great filter/query merged of 2.0.

Reported [here](https://www.reddit.com/r/elasticsearch/comments/48h34n/elasticsearch_rest_api_inconsistent/).
</description><key id="137648708">16887</key><summary>Limit query's docs use filtered query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>docs</label></labels><created>2016-03-01T18:19:55Z</created><updated>2016-03-02T11:41:27Z</updated><resolved>2016-03-02T11:41:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-01T18:23:23Z" id="190841470">Filtered is actually used all over the docs - I imagine we should remove it everywhere but it's page.
</comment><comment author="clintongormley" created="2016-03-02T11:40:44Z" id="191201979">Actually, it is used only on limit/and/or, all of which are deprecated.  In master, these are gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use System.nanoTime() instead of System.currentTimeMillis() for took time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16886</link><project id="" key="" /><description /><key id="137642637">16886</key><summary>Use System.nanoTime() instead of System.currentTimeMillis() for took time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Bulk</label><label>enhancement</label></labels><created>2016-03-01T17:53:34Z</created><updated>2016-03-02T19:38:45Z</updated><resolved>2016-03-02T08:31:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-01T18:05:36Z" id="190835936">LGTM. You may want to mention in the second line of the commit/pr description the whole relative time thing.
</comment><comment author="martijnvg" created="2016-03-02T08:30:52Z" id="191126823">Thanks @nik9000 I've updated the commit description.
</comment><comment author="martijnvg" created="2016-03-02T14:25:08Z" id="191257673">I reverted this PR. `System.nanoTime()` was being called from multiple threads, causing the elapsed time to be incorrect. Fixing this requires a different change, so for now I reverted this PR.
</comment><comment author="nik9000" created="2016-03-02T15:25:29Z" id="191286551">I _think_ we use nanoTime for this kind of thing elsewhere. Are we always on one thread there? @jasontedor would probably know. I use exactly the same pattern in reindex for it's took and I suspect its broken if this is.
</comment><comment author="danielmitterdorfer" created="2016-03-02T15:35:53Z" id="191289983">@martijnvg: Can you elaborate? `System.nanoTime()` uses a monotonic clock under the hood so it should always be safe and more robust than `System.currentTimeMillis()`, even when used from multiple threads. However, `System.nanoTime()` will only work on a single machine, i.e. it reports (totally) different values on different machines.
</comment><comment author="martijnvg" created="2016-03-02T15:47:57Z" id="191295684">&gt; Are we always on one thread there?

We take the start nanotime snapshot on a netty thread and the elapsed time was computed on a different thread.

@danielmitterdorfer This was assumption after checking online (varies per jvm impl?). The took time in bulk response was locally completely off (100000+ ms for indexing a single doc) and this was the only explanation that made sense to me, so I reverted. But maybe I reacted too quickly...
</comment><comment author="jasontedor" created="2016-03-02T15:50:26Z" id="191297033">&gt; We take the start nanotime snapshot on a netty thread and the elapsed time was computed on a different thread.

That should not matter.

&gt; 100000+ ms for indexing a single doc

Sounds like a unit conversion problem?

&gt; But maybe I reacted too quickly...

Do you have a failed build that we can look at? Do you mind if I take a look?
</comment><comment author="martijnvg" created="2016-03-02T15:54:50Z" id="191299592">&gt; Do you have a failed build that we can look at? Do you mind if I take a look?

We don't have tests for `took` in bulk response, but I noticed this locally when testing something else. Let me dig this up again. I think I may have overreacted.
</comment><comment author="jasontedor" created="2016-03-02T15:55:14Z" id="191299743">@martijnvg There was a missed called to [System#currentTimeMillis on line 94](https://github.com/martijnvg/elasticsearch/blob/82567f1bdf578c0f1128f0f4a457b8ba6269e7ce/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java#L94), that's the source of the problem.

This is the fix:

```
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
index 4b53ee2..4ddabe2 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
@@ -91,7 +91,7 @@ public class TransportBulkAction extends HandledTransportAction&lt;BulkRequest, Bul

     @Override
     protected void doExecute(final BulkRequest bulkRequest, final ActionListener&lt;BulkResponse&gt; listener) {
-        final long startTime = System.currentTimeMillis();
+        final long startTime = System.nanoTime();
         final AtomicArray&lt;BulkItemResponse&gt; responses = new AtomicArray&lt;&gt;(bulkRequest.requests.size());

         if (autoCreateIndex.needToCheck()) {
```
</comment><comment author="martijnvg" created="2016-03-02T15:57:25Z" id="191300462">@jasontedor oops... thanks for checking! I clearly overreacted.
</comment><comment author="martijnvg" created="2016-03-02T15:58:20Z" id="191300810">I'll revert the revert commit and change line 94.
</comment><comment author="clintongormley" created="2016-03-02T16:22:12Z" id="191311458">@martijnvg please add the version label back when you re-revert 
</comment><comment author="martijnvg" created="2016-03-02T19:38:45Z" id="191391439">This change will not re-reverted. #16916 will supersede this change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecation logging for query/filter merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16885</link><project id="" key="" /><description>In 5.0, the `filtered` query and the `query` filter have been removed, and the `fuzzy` query deprecated.  We should log any use of these to the deprecation log in 2.3, so that user can spot queries which are using old syntax.
</description><key id="137628015">16885</key><summary>Deprecation logging for query/filter merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-03-01T16:50:28Z</created><updated>2016-03-03T10:15:51Z</updated><resolved>2016-03-03T10:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove http_address from _cat/nodeattrs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16884</link><project id="" key="" /><description>This was recently added with #16770 but it turns out it is not a proper node attribute, so it is enough to print it out as part of `_cat/nodes` only. Turns out that node service attributes which `http_address` belongs to are used only for reporting purposes by nodes info, and may make the output confusing if printed out as part of node attributes, as for instance they cannot be used to select nodes like other node attributes.
</description><key id="137610983">16884</key><summary>Remove http_address from _cat/nodeattrs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:CAT API</label><label>non-issue</label><label>review</label></labels><created>2016-03-01T15:53:19Z</created><updated>2016-03-02T16:34:48Z</updated><resolved>2016-03-02T16:34:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-02T11:17:05Z" id="191193210">@s1monw can you have a look please?
</comment><comment author="s1monw" created="2016-03-02T14:18:25Z" id="191253739">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Port delete by query to reindex's infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16883</link><project id="" key="" /><description>We should port delete by query to reindex's infrastructure. It'll "automatically" get support for cancelation and reading it's status. Since the status includes enough data to calculate a completion percentage, I think this is a win. It will remove support for the timeout parameter, I think. It isn't really needed when you can just cancel the request with task management though.

This also means it'll move into the reindex module and be available in a standard installation.

Sound like a good idea?
</description><key id="137604255">16883</key><summary>Port delete by query to reindex's infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Delete By Query</label><label>:Reindex API</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha3</label></labels><created>2016-03-01T15:27:55Z</created><updated>2016-05-19T14:14:51Z</updated><resolved>2016-05-19T14:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-01T15:28:43Z" id="190770832">I think this sounds like a good idea to me
</comment><comment author="dadoonet" created="2016-03-01T15:29:43Z" id="190771139">++
</comment><comment author="tlrx" created="2016-03-01T16:51:18Z" id="190807501">+1

I'd like to give it a try
</comment><comment author="nik9000" created="2016-03-01T16:59:31Z" id="190813196">Have at it!
</comment><comment author="yehosef" created="2016-05-18T13:39:15Z" id="220028598">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Crank NamingConventionsCheck to 11</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16882</link><project id="" key="" /><description>Tightens lots of the rules around classes that are named like integration
tests. They must actually be integration tests and runnable. To do so it
had to learn that ESRestTestCase subclasses are integration tests.

Introduces NamingConventionsCheckTest which is easier to run and debug
than NamingConventionsCheck --self-test.

Fixes all of the test name violations. To do so this commit had to pull
ESSmokeClientTestCase into :test:framework as ESClientTestCase as the
common superclass for all tests that use an external es cluster only.
</description><key id="137567582">16882</key><summary>Crank NamingConventionsCheck to 11</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>test</label></labels><created>2016-03-01T12:56:51Z</created><updated>2016-03-25T15:37:09Z</updated><resolved>2016-03-23T00:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-02T19:13:34Z" id="191379582">@rjernst I reworked this so the funky change you pointed out is gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport add setFactory permission to GceDiscoveryPlugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16881</link><project id="" key="" /><description>This commit adds a missing permission and a simple test that
ensures we discover other nodes via a mock http endpoint.

Backport of #16860
Relates to #16485
</description><key id="137554186">16881</key><summary>Backport add setFactory permission to GceDiscoveryPlugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2016-03-01T11:59:44Z</created><updated>2016-03-01T13:25:37Z</updated><resolved>2016-03-01T13:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-01T12:02:47Z" id="190688397">looks good.
</comment><comment author="jasontedor" created="2016-03-01T12:08:19Z" id="190689701">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename SearchServiceTransportAction to SearchTransportService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16880</link><project id="" key="" /><description>The suffix TransportAction is misleading as it may make think that it extends TransportAction, but it does not. This class makes accessible the different search operations exposed by SearchService through the transport layer. Also resolved few compiler warnings in the class itself.
</description><key id="137548856">16880</key><summary>Rename SearchServiceTransportAction to SearchTransportService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T11:34:30Z</created><updated>2016-03-01T11:58:28Z</updated><resolved>2016-03-01T11:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-01T11:53:45Z" id="190686567">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch date rangequery affected by TimeZone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16879</link><project id="" key="" /><description>I tried for several times,but still confused.
At first the mapping of date field is 
{
    "createdAt" : {
            "type" : "date",
            "format" : "yyyy-MM-dd HH:mm:ss"
          },
}
when I query with java, I use SimpleDateFormat like 

```
SimpleDateFormat smf = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
String format = smf.format(date);
```

the result is affected by TimeZone.
And I try store the date field with format like "yyyy-MM-dd hh:mm:ss Z"  and without format.In this time , I get the correct result. So what happend in the format thing?
</description><key id="137533476">16879</key><summary>ElasticSearch date rangequery affected by TimeZone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sosoda</reporter><labels /><created>2016-03-01T10:28:57Z</created><updated>2016-03-01T10:37:26Z</updated><resolved>2016-03-01T10:37:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-01T10:37:01Z" id="190655657">This is neither a bug report nor a feature request which is what Elasticsearch GitHub issues are reserved for. There is a community of people at the [Elastic Discourse forums](https://discuss.elastic.co) that would be happy to help you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update index-options.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16878</link><project id="" key="" /><description>Fix typo
</description><key id="137531805">16878</key><summary>Update index-options.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label></labels><created>2016-03-01T10:20:56Z</created><updated>2016-03-02T09:52:17Z</updated><resolved>2016-03-02T09:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T09:52:17Z" id="191162224">thanks @peschlowp - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate string in favor of text/keyword.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16877</link><project id="" key="" /><description>This commit removes the ability to use string fields on indices created on or
after 5.0. Dynamic mappings now generate text fields by default for strings
but there are plans to also add a sub keyword field (in a future PR).

Most of the changes in this commit are just about replacing string with
keyword or text. Some tests have been removed because they existed because of
corner cases of string mappings like setting ignore-above on a text field or
enabling term vectors on a keyword field which are now impossible.

The plan is to remove strings entirely in 6.0.
</description><key id="137529888">16877</key><summary>Deprecate string in favor of text/keyword.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>deprecation</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T10:11:51Z</created><updated>2016-03-03T22:53:22Z</updated><resolved>2016-03-03T09:22:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-02T16:29:11Z" id="191314050">@rjernst could you have a look?
</comment><comment author="rjernst" created="2016-03-02T16:39:40Z" id="191318621">LGTM, just one comment which could be addressed in a followup.
</comment><comment author="dakrone" created="2016-03-03T17:39:24Z" id="191878488">@jpountz I think if this removes `string` type and throws warnings, it should be marked as `breaking` and notes added to the migration guide, right?
</comment><comment author="jpountz" created="2016-03-03T17:45:17Z" id="191881346">I added the breaking label. Regarding documentation, I'm keeping it for later so that the docs can deliver a consistent message about #12394 (I have not documented the text and keyword fields either yet).
</comment><comment author="dakrone" created="2016-03-03T17:52:17Z" id="191885410">&gt; Regarding documentation, I'm keeping it for later so that the docs can deliver a consistent message about #12394 (I have not documented the text and keyword fields either yet).

Makes sense, I just don't want to forget it! :)
</comment><comment author="epixa" created="2016-03-03T20:09:22Z" id="191941450">With regard to documentation/messaging for this, I just want to underscore how impactful this break can be, so we should make sure to highlight it as clearly as possible when we launch 5.0.

This broke the mappings in Kibana, for example, so all CI builds are breaking and Kibana is uninstallable.  We can fix master by updating our mappings for the .kibana index, but in order to support users upgrading their installs after the release, we need to either build into Kibana 5.0 an internal re-indexing feature or change Kibana to use versioned indexes behind a .kibana alias (which is our current consensus).
</comment><comment author="rjernst" created="2016-03-03T20:14:27Z" id="191943252">@epixa This change is backwards compatible for _existing_ indexes, and users will already need to reindex in order to take advantage of the numeric changes that will also happen in 5.0 (which come with Lucene 6). This only affected kibana tests since it creates new indexes, so I don't see it as a problem?
</comment><comment author="Bargs" created="2016-03-03T20:15:20Z" id="191943623">I've come across a problem while trying to update the default mappings for the .kibana index.

The current breaking changes guide states:

&gt; On all types but string, the index property now only accepts true/false instead of not_analyzed/no. The string field still accepts analyzed/not_analyzed/no

After switching our `string` field to `text`, it will now only accept true/false for the `index` property. How do I get an indexed but not_analyzed `text` field?
</comment><comment author="rjernst" created="2016-03-03T20:20:38Z" id="191945399">@Bargs That is what the new `keyword` type is for, see #16589 for the PR which added it, and #12394 for the meta issue describing the split of `string` to `keyword` and `text`.
</comment><comment author="Bargs" created="2016-03-03T22:04:22Z" id="191986351">Thanks @rjernst, just the info I needed!
</comment><comment author="rashidkpc" created="2016-03-03T22:53:22Z" id="192005352">Yeah, we're going to need to build out a reindexing-on-upgrade system in Kibana to support this correctly. We create mappings for Kibana objects on demand, and this is going to cause problems.

For example, if a user has saved dashboards and visualizations, but they've never saved a search they'll have the `dashboard` and `visualization` `title` field set to string. If they then try to save a search we'll try to create a mapping for the `search` type and that will fail due to a conflict across types. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `ingest_took` to bulk response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16876</link><project id="" key="" /><description> The `ingest_took` indicates how much time was spent on ingest preprocessing.

The `ingest_took` is separate from `took`, which keeps track how much time is spent on indexing/deleting/updating. The `ingest_took` is only visible in the rest response if at least one index request has ingest enabled.
</description><key id="137528630">16876</key><summary>Add `ingest_took` to bulk response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-01T10:07:30Z</created><updated>2016-03-02T20:09:11Z</updated><resolved>2016-03-01T17:25:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-01T10:53:19Z" id="190662419">@javanna @bleskes I've updated the PR.
</comment><comment author="javanna" created="2016-03-01T15:48:58Z" id="190778796">LGTM
</comment><comment author="s1monw" created="2016-03-02T20:05:31Z" id="191403789">this PR passes invalid tookInMillis values to serialization. https://github.com/elastic/elasticsearch/pull/16876/files#diff-764ab3948584e67f5cb1d566ef82186aR38 -1 is not allowed for vlong it must be positive. I have a failing test because of this:

```
ERROR   0.14s J1 | DecayFunctionScoreIT.testManyDocsLin &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: UncategorizedExecutionException[Failed execution]; nested: AssertionError;
   &gt;    at __randomizedtesting.SeedInfo.seed([8A21AFDC7C31D644:E577B0C45C2FD983]:0)
   &gt;    at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:87)
   &gt;    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:46)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1417)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1352)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1336)
   &gt;    at org.elasticsearch.search.functionscore.DecayFunctionScoreIT.testManyDocsLin(DecayFunctionScoreIT.java:612)
   &gt;    at java.lang.Thread.run(Thread.java:745)
   &gt; Caused by: java.lang.AssertionError
   &gt;    at org.elasticsearch.common.io.stream.StreamOutput.writeVLong(StreamOutput.java:198)
   &gt;    at org.elasticsearch.action.bulk.BulkResponse.writeTo(BulkResponse.java:141)
   &gt;    at org.elasticsearch.transport.local.LocalTransportChannel.sendResponse(LocalTransportChannel.java:82)
   &gt;    at org.elasticsearch.transport.local.LocalTransportChannel.sendResponse(LocalTransportChannel.java:71)
   &gt;    at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58)
   &gt;    at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:103)
   &gt;    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler$1.onResponse(HandledTransportAction.java:58)
   &gt;    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler$1.onResponse(HandledTransportAction.java:54)
   &gt;    at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:216)
   &gt;    at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:87)
   &gt;    at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:214)
   &gt;    at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:241)
   &gt;    at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:227)
   &gt;    at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:355)
   &gt;    at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:327)
   &gt;    at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:316)
   &gt;    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:91)
   &gt;    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:87)
   &gt;    at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:216)
   &gt;    at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:87)
   &gt;    at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:214)
   &gt;    at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:241)
   &gt;    at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:227)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishOnSuccess(TransportReplicationAction.java:650)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:573)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:559)
   &gt;    at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:759)
   &gt;    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:830)
   &gt;    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:814)
   &gt;    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:804)
   &gt;    at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58)
   &gt;    at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:103)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.doFinish(TransportReplicationAction.java:1107)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.doRun(TransportReplicationAction.java:983)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.finishAndMoveToReplication(TransportReplicationAction.java:794)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.executeLocally(TransportReplicationAction.java:725)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:711)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:292)
   &gt;    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:284)
   &gt;    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:65)
   &gt;    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:361)
   &gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$FilterAbstractRunnable.doRun(EsThreadPoolExecutor.java:191)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    ... 1 more
  2&gt; NOTE: leaving temporary files on disk at: /home/simon/projects/elasticsearch/core/build/testrun/integTest/J1/temp/org.elasticsearch.search.functionscore.DecayFunctionScoreIT_8A21AFDC7C31D644-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene54): {geo.lon=PostingsFormat(name=Asserting), loc=PostingsFormat(name=Asserting), test=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), num=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), num1=PostingsFormat(name=Asserting), geo.lat=PostingsFormat(name=Asserting), _timestamp=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting), num2=PostingsFormat(name=Asserting)}, docValues:{geo.lon=DocValuesFormat(name=Asserting), loc=DocValuesFormat(name=Lucene54), num=DocValuesFormat(name=Lucene54), _type=DocValuesFormat(name=Lucene54), num1=DocValuesFormat(name=Asserting), geo.lat=DocValuesFormat(name=Lucene54), _timestamp=DocValuesFormat(name=Lucene54), _version=DocValuesFormat(name=Lucene54), num2=DocValuesFormat(name=Lucene54)}, sim=RandomSimilarity(queryNorm=true,coord=yes): {}, locale=uk, timezone=Europe/Oslo
  2&gt; NOTE: Linux 3.13.0-35-generic amd64/Oracle Corporation 1.8.0_66 (64-bit)/cpus=12,threads=1,free=353590912,total=523239424
  2&gt; NOTE: All tests run in this JVM: [SimpleVersioningIT, GetIndexIT, DecayFunctionScoreIT]
Completed [17/278] on J1 in 1.70s, 14 tests, 1 error &lt;&lt;&lt; FAILURES!
```
</comment><comment author="s1monw" created="2016-03-02T20:09:11Z" id="191405058">argh scratch that - I misread the line...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update configuration.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16875</link><project id="" key="" /><description>Adjust text explanation to the refresh interval used as an example
</description><key id="137528076">16875</key><summary>Update configuration.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label></labels><created>2016-03-01T10:05:31Z</created><updated>2016-03-02T09:50:25Z</updated><resolved>2016-03-02T09:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-02T09:50:25Z" id="191161788">thanks @peschlowp 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation's Result should have writeToJson method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16874</link><project id="" key="" /><description>**Describe the feature**:
I'm just using aggregation for geo clustering,and when i run the es script on sense,the results like below

```
{
  "took": 297,
  "timed_out": false,
  "_shards": {
    "total": 944,
    "successful": 944,
    "failed": 0
  },
  "hits": {
    "total": 149468,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "grid": {
      "buckets": [
        {
          "key": "dr5ru",
          "doc_count": 65549,
          "cell": {
            "bounds": {
              "top_left": {
                "lat": 40.78121,
                "lon": -74
              },
              "bottom_right": {
                "lat": 40.737305,
                "lon": -73.959984
              }
            }
          }
        },
        {
          "key": "dr5rs",
          "doc_count": 28544,
          "cell": {
            "bounds": {
              "top_left": {
                "lat": 40.7373,
                "lon": -74
              },
              "bottom_right": {
                "lat": 40.693375,
                "lon": -73.96
              }
            }
          }
        },
        {
          "key": "dr5rt",
          "doc_count": 11505,
          "cell": {
            "bounds": {
              "top_left": {
                "lat": 40.737232,
                "lon": -73.95995
              },
              "bottom_right": {
                "lat": 40.693367,
                "lon": -73.916046
              }
            }
          }
        }
      ]
    }
  }
}
```

BUT when i using api it's only support to using 
resp.getAggregations().get("grid").getBuckets() method. it's not flexible kibana sense
so i suggest to add method writeToJson to as same as output with
resp.getAggregations().get("grid").writeToJson()

Thanks.
</description><key id="137491973">16874</key><summary>Aggregation's Result should have writeToJson method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MadeInChina</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2016-03-01T07:05:09Z</created><updated>2016-03-03T09:17:44Z</updated><resolved>2016-03-02T10:52:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-03-01T15:08:36Z" id="190760581">I just want to make sure I'm understanding what you're asking: are you asking for the Elasticsearch Java API to be able to dump the JSON output as a string so you can parse it yourself?
</comment><comment author="MadeInChina" created="2016-03-02T08:40:18Z" id="191129382">@eskibars 
Yes, that's what i want.
It's could be more flexible.
Thanks
</comment><comment author="clintongormley" created="2016-03-02T10:52:59Z" id="191185752">@MadeInChina no idea why you want JSON in Java, but you can just use toXContent()
</comment><comment author="MadeInChina" created="2016-03-03T09:17:44Z" id="191670258">@clintongormley Thanks. toXContent is a way for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change internal representation of suggesters </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16873</link><project id="" key="" /><description>Change internal representation of suggesters to instances of SuggestBuilder instead of raw bytes.
</description><key id="137396739">16873</key><summary>Change internal representation of suggesters </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T22:22:11Z</created><updated>2016-03-11T17:34:35Z</updated><resolved>2016-03-11T17:34:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-01T12:47:48Z" id="190709836">@abeyad I only had a brief look, left a few minor comments. I like passing the suggesters to the parsing methods as additional argument like in the aggregations parsers. Maybe in a future step we might be able to collaps some of those. I didn't have a close look at the SearchSourceBuilder tests, but would also add a bit more randomization to the suggester setup in createSearchSourceBuilder() (maybe add a few more random suggestions, re-use the randomization from the individual tests if that is already possible?)

Also @areek might want to have another look.
</comment><comment author="cbuescher" created="2016-03-07T14:28:09Z" id="193271066">@abeyad I did a round of reviews, left some suggestions. I think you can remove the "WIP" from the title, looks already quiet far to me. After adressing some of the comments I think this only needs to wait for the CompletionSuggestion refactoring and then maybe a final review once all tests pass.
</comment><comment author="cbuescher" created="2016-03-07T16:14:01Z" id="193321619">fyi I locally put an @AwaitsFix on CompletionSuggestSearchIT and ContextCompletionSuggestSearchIT to see if any later tests also break and found that in distribution/integ-test-zip there are several rest tests failing with "Suggestion must have name". Maybe we were to eager in enforcing this, maybe we only need to adapt the tests. 
</comment><comment author="abeyad" created="2016-03-07T16:33:31Z" id="193333980">@cbuescher I think the tests you are referring to (the `RestIT` ones), those seem to fail with regards to the completion suggester only, no?  
</comment><comment author="cbuescher" created="2016-03-07T16:38:36Z" id="193336934">Not sure, to me it looks more general, but maybe you are right. Just wanted to point it out.
</comment><comment author="abeyad" created="2016-03-07T16:41:36Z" id="193337910">Its possible, I started looking at it and showed some of the failures to @areek who thought they were related to the completion suggester only. I figured once that's in, we can iron out any remaining test failures further, because there is also the `SuggestBuilderTests` which only retrieve a random Term or Phrase SuggestionBuilder now, with the Completion commented out.  So we probably still have some work to do with respect to the tests regardless.
</comment><comment author="abeyad" created="2016-03-07T16:57:47Z" id="193344136">I pushed my latest changes based on @cbuescher comments
</comment><comment author="cbuescher" created="2016-03-07T17:25:48Z" id="193358583">@abeyad great, left two more minor comments.
</comment><comment author="abeyad" created="2016-03-10T05:15:43Z" id="194672940">@cbuescher @areek I made some changes based on Areek's feedback.
</comment><comment author="cbuescher" created="2016-03-10T13:12:48Z" id="194835354"> @abeyad looks good, unfortunately getting some compile errors bc. `CompletionSuggesterBuilderTests.randomCompletionSuggestionBuilder()` is not there. Also a few test are failing for me locally after I fix that.
</comment><comment author="abeyad" created="2016-03-10T15:40:56Z" id="194910977">Fixed and pushed changes.

On Thu, Mar 10, 2016 at 8:13 AM, Christoph B&#252;scher &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; @abeyad https://github.com/abeyad looks good, unfortunately getting
&gt; some compile errors bc.
&gt; CompletionSuggesterBuilderTests.randomCompletionSuggestionBuilder(), and
&gt; a few test are failing for me when I fix that.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16873#issuecomment-194835354
&gt; .
</comment><comment author="cbuescher" created="2016-03-11T12:01:43Z" id="195340348">@abeyad thanks, still getting test errors (and later some compile issues in reindex module). I'm running `gradle clean check` in the meantime and fixing one by one, will let you know what I find (and maybe paste the diff somewhere). After that passes I think this PR should be merged with the feature branch.
</comment><comment author="cbuescher" created="2016-03-11T14:51:09Z" id="195398063">These are the tests (and minor compile thingies) that I found: https://gist.github.com/cbuescher/082435d20ddd279e3db4
</comment><comment author="abeyad" created="2016-03-11T15:18:49Z" id="195410221">LGTM

On Fri, Mar 11, 2016 at 9:52 AM, Christoph B&#252;scher &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; These are the tests (and minor compile thingies) that I found:
&gt; https://gist.github.com/cbuescher/082435d20ddd279e3db4
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16873#issuecomment-195398063
&gt; .
</comment><comment author="cbuescher" created="2016-03-11T17:33:33Z" id="195468465">All tests pass now, LGTM, lets get this in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple the TransportService and ClusterService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16872</link><project id="" key="" /><description>Currently, the cluster service is tightly coupled to the transport service by both managing node connections and requiring the bound address in order to create the local disco node. This commit introduces a new NodeConnectionsService which is in charge of node connection management and makes it possible to remove all network related calls from the cluster service. The local DiscoNode is now created by DiscoveryNodeService and is set both the cluster service and the transport service during node start up.

Closes #16788

Note that this simplifies constructing the InternalClusterService making a viable option for testing. I already moved some of its testing to a unit test. As a follow up, I will attempt removing all test implementations of ClusterService and move InternalClusterService to be the one and only ClusterService.
</description><key id="137359467">16872</key><summary>Decouple the TransportService and ClusterService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T20:15:13Z</created><updated>2016-03-10T10:52:53Z</updated><resolved>2016-03-10T10:52:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-29T20:16:28Z" id="190365735">@s1monw I still want to add a unit test for the new NodeConnectionsService but I think we can start the review process and get feedback..
</comment><comment author="s1monw" created="2016-03-01T13:43:15Z" id="190728563">left some intial review comments
</comment><comment author="bleskes" created="2016-03-03T10:30:37Z" id="191697321">@s1monw I rebased, added a unit test and addressed the comments. I think it's ready for review..
</comment><comment author="bleskes" created="2016-03-07T10:56:31Z" id="193203394">@s1monw thanks for the feedback. I pushed some more commits and merged from master. I think the main discussion still open is regarding listeners. Sadly github folded it away - but please check my answer [here](https://github.com/elastic/elasticsearch/pull/16872#discussion_r55187553)
</comment><comment author="s1monw" created="2016-03-08T15:36:32Z" id="193826858">&gt; @s1monw thanks for the feedback. I pushed some more commits and merged from master. I think the main discussion still open is regarding listeners. Sadly github folded it away - but please check my answer here

I agree with your plan here lets fix it in several steps and be explicit for now...
</comment><comment author="s1monw" created="2016-03-09T10:17:32Z" id="194224010">lets go with what we have I added some minors but LGTM otherwise, it's progress
</comment><comment author="bleskes" created="2016-03-09T13:18:10Z" id="194293509">@s1monw thx. pushed another commit
</comment><comment author="s1monw" created="2016-03-10T10:11:06Z" id="194771616">LGTM thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove implicit support for Cygwin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16871</link><project id="" key="" /><description>This commit makes explicit that we do not support Cygwin.
</description><key id="137352058">16871</key><summary>Remove implicit support for Cygwin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>non-issue</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T19:47:15Z</created><updated>2016-03-16T19:45:05Z</updated><resolved>2016-02-29T20:00:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-29T19:56:48Z" id="190355600">LGTM. Maybe a note in testing.asciidoc or contributing.md?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds a rewrite phase to queries on the shard level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16870</link><project id="" key="" /><description>This change adds a rewrite phase to the queries on the shard before they are assessed for caching or executed. This allows the opportunity to rewrite queries as faster running simpler queries based on attributes known to only the shard itself. The first query to implement this is the RangeQueryBuilder which will rewrite to a MatchAllQueryBuilder if the range of terms on the shard is a subset of the query and rewrites to a MatchNoneQueryBuilder if the range of terms on the shard is completely outside the query.

Closes #9526
</description><key id="137334797">16870</key><summary>Adds a rewrite phase to queries on the shard level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T18:40:38Z</created><updated>2016-07-28T08:54:05Z</updated><resolved>2016-03-17T08:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-29T19:30:15Z" id="190347284">@colings86 I left some comments, looks great so far
</comment><comment author="polyfractal" created="2016-02-29T19:46:03Z" id="190352195">Just a note, and absolutely not a blocker, but I think this rewrite will "escape" the profiler.  Rewrite timings are only recorded when they go through the ContextIndexSearcher, so this sidesteps that ability.  Also, we don't know if we should even profile until a little later when the source is parsed.

Perhaps slap a _"TODO: escapes profiling"_ in there somewhere so that I can revisit this in the future?  I imagine the profiler will need a bit of massaging to accommodate this new style of rewrites (which will only become more common in the future).
</comment><comment author="s1monw" created="2016-02-29T19:50:45Z" id="190353906">&gt; Just a note, and absolutely not a blocker, but I think this rewrite will "escape" the profiler. Rewrite timings are only recorded when they go through the ContextIndexSearcher, so this sidesteps that ability. Also, we don't know if we should even profile until a little later when the source is parsed.

this is a different rewrite than lucene rewrite I think we are fine here?
</comment><comment author="polyfractal" created="2016-02-29T20:02:43Z" id="190358811">&gt; this is a different rewrite than lucene rewrite I think we are fine here?

Yeah, which is why it doesn't get timed :)  I assumed we would want any rewrite (ours or Lucene) to be included in the final rewrite timing, since it's still work that must be done and adds to latency? 
</comment><comment author="s1monw" created="2016-02-29T20:04:16Z" id="190359219">&gt; Yeah, which is why it doesn't get timed :) I assumed we would want any rewrite (ours or Lucene) to be included in the final rewrite timing, since it's still work that must be done and adds to latency?

compared to lucene I think we can ignore this rewriting here the one that is costly is MTQ rewrite this one is rather cheap.
</comment><comment author="jpountz" created="2016-02-29T22:23:07Z" id="190425645">+1 to ignore
</comment><comment author="polyfractal" created="2016-02-29T22:45:42Z" id="190433791">Works for me! :)
</comment><comment author="colings86" created="2016-03-02T12:25:52Z" id="191220134">Currently fails with seed -Dtests.seed=B4AEDE8445BFF1A5
</comment><comment author="s1monw" created="2016-03-02T14:11:50Z" id="191251200">this looks pretty awesome I left one minor comment... I love the simplification to the `RangeQueryBuilderTests` but I think we now need a dedicated unittest for `FieldStatsProvider` can you do that? Other than that LGTM
</comment><comment author="s1monw" created="2016-03-02T21:24:23Z" id="191438881">@colings86 I think this issue would close https://github.com/elastic/elasticsearch/issues/9526
</comment><comment author="s1monw" created="2016-03-16T10:04:36Z" id="197240470">@colings86 this looks awesome - pretty close, I left some comments
</comment><comment author="colings86" created="2016-03-16T11:08:19Z" id="197263733">@s1monw I pushed another commit addressing most of your comments
</comment><comment author="s1monw" created="2016-03-16T11:32:36Z" id="197275842">LGTM
</comment><comment author="colings86" created="2016-03-16T19:01:00Z" id="197491448">@s1monw had to push one more commit to fix SimpleSearchIT.testSimpleDateRange().
</comment><comment author="s1monw" created="2016-03-17T08:23:58Z" id="197756404">LGTM
</comment><comment author="agirbal" created="2016-07-19T05:27:11Z" id="233535628">quick note that based on a few production search use cases I've seen, supporting regular queries with hits would also be highly valuable. A typical example is if a dataset both has a "created time" (which drives the index name) and a "last updated time". Ideally querying on a window of "last updated time" that does not match anything in a given older index would make use of this optimization.
</comment><comment author="s1monw" created="2016-07-19T07:58:45Z" id="233558620">@agirbal see https://github.com/elastic/elasticsearch/pull/19472
</comment><comment author="clintongormley" created="2016-07-19T13:43:51Z" id="233636794">also @agirbal regardless of caching, all range queries will make use of the optimization of checking whether the range intersects with the values in the shard.
</comment><comment author="agirbal" created="2016-07-19T20:07:51Z" id="233750105">@clintongormley right I don't see what caching has to do with this. I think the query cache will be off like default. Great to hear there will be a "fail-fast" for shards that don't have the range.
</comment><comment author="asafm" created="2016-07-27T16:31:53Z" id="235642475">Is it possible to backport this to 2.x?
</comment><comment author="colings86" created="2016-07-27T16:42:05Z" id="235645426">@asafm Unfortunately not. This PR is relatively small but there was a huge amount of work that lead up to this change which involved refactoring every part of the search request (every query type, aggregation, highlighter, suggester, etc.) so that it could be parsed into an internal Elasticsearch QueryBuilder object on the coordinating node to allow it to be in a form where it can be rewritten. Unfortunately the changes are just too extensive to be able to backport.
</comment><comment author="asafm" created="2016-07-27T17:10:57Z" id="235653625">@colings86 Ok, thanks. When you do estimate Elastic 5.x is production worthy?
</comment><comment author="colings86" created="2016-07-28T08:54:05Z" id="235837997">5.x will be production worthy with the release of the 5.0.0 GA. When that will be is not easy to estimate as it depends on feedback we get from the alpha and beta versions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setup logging in package tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16869</link><project id="" key="" /><description>We need to do this now because elasticsearch doesn't start if it fails to
set up logging.
</description><key id="137331304">16869</key><summary>Setup logging in package tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>test</label><label>v2.3.0</label></labels><created>2016-02-29T18:23:50Z</created><updated>2016-02-29T18:39:37Z</updated><resolved>2016-02-29T18:39:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-29T18:32:59Z" id="190323489">LGTM thanks Nik
</comment><comment author="nik9000" created="2016-02-29T18:39:37Z" id="190325385">Thanks for reviewing! Lets see if that makes the build happier.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest: Move name of suggestion to SuggestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16868</link><project id="" key="" /><description>Currently each suggestion keeps track of its own name. This has
the disadvantage of having to pass down the parsed name property
in the suggestions fromXContent() and the serialization methods
as an argument, since we need it in the ctor.

This change moves the naming of the suggestions to the surrounding
SuggestBuilder and by this eliminates the need for passind down
the names in the parsing and serialization methods. By making
`name` a required argument in SuggestBuilder#addSuggestion() we
also make sure it is always set and prevent using the same name twice,
which wasn't possible before.
</description><key id="137318487">16868</key><summary>Suggest: Move name of suggestion to SuggestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T17:28:15Z</created><updated>2016-03-10T18:57:20Z</updated><resolved>2016-03-01T17:09:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-29T19:23:26Z" id="190344266">@cbuescher I really like this change. It makes it a lot clearer to follow the code, allows us to remove the `name` parameter on the `SuggestionBuilder` that couldn't be `final`, and separates the name of the suggestion from its definition. ++ on this change
</comment><comment author="abeyad" created="2016-03-01T14:33:31Z" id="190743198">LGTM
</comment><comment author="areek" created="2016-03-01T16:22:09Z" id="190792800">@cbuescher This looks great! LGTM, I added a few minor comments.
</comment><comment author="cbuescher" created="2016-03-01T17:09:56Z" id="190818054">Thanks, addressed your last comments and merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds Node Ingest indicator (i) to _cat/nodes API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16867</link><project id="" key="" /><description>This commit adds Node ingest information to the `_cat/nodes` endpoint.

Note that this implementation is probably wrong because it will print `i` iif the node is not a client node nor a data node.

May be it would be better to add a new column instead `ingest.processors` and write the number of available processors once #16865 will be merged?

@martijnvg WDYT?
</description><key id="137311965">16867</key><summary>Adds Node Ingest indicator (i) to _cat/nodes API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-02-29T17:06:17Z</created><updated>2016-02-29T18:10:26Z</updated><resolved>2016-02-29T18:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-29T17:36:17Z" id="190303968">I am doing this as part of #16565 where client nodes are gone and we list all of the different node types. PR will come soon.
</comment><comment author="martijnvg" created="2016-02-29T18:02:10Z" id="190312543">&gt; May be it would be better to add a new column instead ingest.processors and write the number of available processors once #16865 will be merged?

The number of processors should be the same on all nodes, otherwise the cluster is badly configured. The put pipeline API will fail if processors don't exist on all nodes. So if the number of processors is unequal between nodes it is a sign on misconfiguration. I think beyond that it doesn't give us much? Maybe instead of showing the number of processors per node, we should list the number of installed plugins? This number of installed plugins should be the same across all nodes.
</comment><comment author="javanna" created="2016-02-29T18:06:05Z" id="190314336">I agree, I don't think knowing the number of processors is that useful, and for plugins don't we already have a specific endpoint?
</comment><comment author="dadoonet" created="2016-02-29T18:09:38Z" id="190315318">Interesting. @javanna So I'm closing my PR as it will be done with yours. (don't forget the `_cat` API please :) )

@martijnvg Indeed. That's interesting. I think that showing the number of processors than would make sense so one should see immediately that something is wrong.
For plugins, I agree that it's better if the number of plugin is consistent across the cluster. Not sure it's mandatory though. Scripting plugins might be useful only on client nodes or data nodes. Discovery plugins should be installed everywhere. Snapshot and restore can be skipped on client nodes... Still, I think it could be a good indicator.

That being said, the cat nodes API does not expose today any plugins indicator. It's not a node attribute.
But this information is available in `_cat/plugins` API. I don't think we need more here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use HttpInfo in cat/_nodes instead of node attribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16866</link><project id="" key="" /><description>The node attribute also renders the type of the inet address
as well as the host name which is unneeded in the cat action.
</description><key id="137306091">16866</key><summary>Use HttpInfo in cat/_nodes instead of node attribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>bug</label><label>review</label></labels><created>2016-02-29T16:46:39Z</created><updated>2016-03-02T20:48:38Z</updated><resolved>2016-03-02T20:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-29T16:58:01Z" id="190287532">LGTM, can you add a note to the 5.0 migration guide about this change also?
</comment><comment author="spalger" created="2016-02-29T20:38:08Z" id="190376772">LGTM, fixes previously failing yaml test https://github.com/elastic/elasticsearch/blob/3c15200f6f6c01872fee71b57634458d97285c9c/rest-api-spec/src/main/resources/rest-api-spec/test/cat.nodes/10_basic.yaml#L52-L59
</comment><comment author="s1monw" created="2016-03-02T19:59:51Z" id="191401811">@dakrone @spalger I found a non-breaking way to fix this... will push soon
</comment><comment author="javanna" created="2016-03-02T20:03:27Z" id="191403052">LGTM thanks Simon I had in mind to make the same change at some point too ;)
</comment><comment author="s1monw" created="2016-03-02T20:14:57Z" id="191407188">&gt; LGTM thanks Simon I had in mind to make the same change at some point too ;)

no idea why I didn't use that in the first place
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest info to node info API, which contains a list of available processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16865</link><project id="" key="" /><description>Internally the put pipeline API uses this information in node info API to validate if all specified processors in a pipeline exist on all nodes in the cluster.
</description><key id="137305795">16865</key><summary>Add ingest info to node info API, which contains a list of available processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T16:45:30Z</created><updated>2016-03-07T14:20:37Z</updated><resolved>2016-03-07T14:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-05T09:58:23Z" id="192615095">left some questions, LGTM otherwise
</comment><comment author="javanna" created="2016-03-05T09:59:54Z" id="192615171">one more question, should the simulate api do validation as well like put pipeline does? or not?
</comment><comment author="martijnvg" created="2016-03-05T15:35:57Z" id="192673067">thx @javanna 

&gt; one more question, should the simulate api do validation as well like put pipeline does?

The simulate doesn't store the pipelines, it just executes on the first node that receives the request. So I think it isn't needed to add this validation there. If someone decides to store the pipeline via the put pipeline API then they will figure it out that one ore more nodes may not have installed a non default/custom processor.
</comment><comment author="javanna" created="2016-03-05T18:43:42Z" id="192705030">&gt; The simulate doesn't store the pipelines, it just executes on the first node that receives the request. So I think it isn't needed to add this validation there.

Agreed, I guess I was more wondering what happens in that case, hopefully the simulate api fails nicely :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can not extract text from Office documents (`.docx` extension)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16864</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0
**Description of the problem including expected versus actual behavior**: Mapper attachments plugin (or ingest-attachment) works with Text of PDF, but not with the Office formats.
**Steps to reproduce**:
1. Install mapper-attachments plugin
2. Index a Word (`.docx` document)
3. Look at logs `DEBUG` level.

**Logs**:

```
[2016-02-29 16:43:39,341][DEBUG][mapper.attachment ] Failed to extract [100000] characters of text for [null]: [Unexpected RuntimeException from org.apache.tika.parser.microsoft.ooxml.OOXMLParser@51667d8a]
...
Caused by: java.lang.IllegalStateException: access denied ("java.lang.RuntimePermission" "getClassLoader")
at org.apache.xmlbeans.XmlBeans.getContextTypeLoader(XmlBeans.java:336)
...
Caused by: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "getClassLoader")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
```

**Analysis**:

As recent Office documents are now `xml` based (`.docx`,  `.xlsx`...), Tika can not read them anymore in the context of elasticsearch because `getClassLoader` call is forbidden.

Reported by many users at https://discuss.elastic.co/t/no-hits-when-do-a-text-search-in-an-attachment-for-docx-file/41779

Switching to `.doc` legacy format works well.
</description><key id="137303644">16864</key><summary>Can not extract text from Office documents (`.docx` extension)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Ingest Attachment</label><label>:Plugin Mapper Attachment</label><label>regression</label></labels><created>2016-02-29T16:37:28Z</created><updated>2016-03-11T01:27:02Z</updated><resolved>2016-03-11T01:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T11:50:00Z" id="190685853">@rjernst any idea what we can do here? Is this a matter of allowing Tika to use getClassLoader?
</comment><comment author="rjernst" created="2016-03-09T23:34:03Z" id="194567481">&gt; Is this a matter of allowing Tika to use getClassLoader?

No, it's not that simple. Adding any permissions still requires executing code needing those permissions inside a doPrivileged block. At a quick glance, it looks like xmlbeans is trying to get the context class loader, but it is hard to tell from the log snippet where this is called from within tika.

@dadoonet Do you have the full stack trace, and/or can you post the base64 of the sample doc which fails? A full recreation would make it easier to investigate where/how to fix the problem, preferably in this case as an addition to the tests (at worst rest tests).
</comment><comment author="rmuir" created="2016-03-10T02:26:59Z" id="194626363">it is not "at worst rest tests", in this case an integration test is a must. Because fantasyland (unit test environment) has everything on a gigantic classpath: you won't hit failures for violations of classloader boundaries.

once you have a good test, then its easy to "fix". Add getClassLoader to this plugin's security policy file and then add it to tika's sandbox: https://github.com/elastic/elasticsearch/blob/master/plugins/ingest-attachment/src/main/java/org/elasticsearch/ingest/attachment/TikaImpl.java#L120

Tika must unfortunately have this special stuff in TikaImpl too, because of the nature of what it does, and because global permissions (core/ security file) are too generous/lenient. So by restricting it, it can only use /tmp and other things, this easily stops e.g. a directory traversal from an xml parser flaw, which is the kind of thing we should expect exist. Plus its just a shit-ton of different jars/code and we gotta keep some kind of leash on that :)

At the same time its not good to just keep adding more exceptions for bad code, without doing something about it. Later as a followup we should look at upgrading tika, they released recently and I know @uschindler also fixed a bunch of problems in some of the parsers themselves like apache POI, those fixes might be in that release and allow some cleanups here.
</comment><comment author="dadoonet" created="2016-03-10T10:01:28Z" id="194766252">I do agree with @rmuir. So I created a branch here: https://github.com/elastic/elasticsearch/tree/fix/16864-attachment-doctypes where you can add also your changes.

It adds a new REST test which test `.doc`, `.docx` docs in the `ingest-attachment` plugin which replaces the `mapper-attachments` plugin.
We can copy our findings then to `mapper-attachments` if we are willing to.

The `doc` test works. The `docx` test fails.

Let me know if I can help on something else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use https for rpm packages, add dnf instructions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16863</link><project id="" key="" /><description>Packages should be downloaded over https instead of http, so change the repo configuration to reflect that.

I tested that this works on Fedora 23.
</description><key id="137298649">16863</key><summary>Use https for rpm packages, add dnf instructions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>docs</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T16:17:48Z</created><updated>2016-02-29T16:38:05Z</updated><resolved>2016-02-29T16:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-29T16:18:59Z" id="190274428">LGTM
</comment><comment author="jasontedor" created="2016-02-29T16:22:28Z" id="190275476">I also tested this on Fedora 23. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bump Elasticsearch version to 5.0.0-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16862</link><project id="" key="" /><description>This commit bumps the Elasticsearch version to 5.0.0-SNAPSHOT in line
with the alignment of versions across the stack.
</description><key id="137297437">16862</key><summary>Bump Elasticsearch version to 5.0.0-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T16:12:56Z</created><updated>2016-03-02T10:40:30Z</updated><resolved>2016-03-01T22:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-01T22:00:45Z" id="190924766">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge reindex to master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16861</link><project id="" key="" /><description>Merges feature/reindex to master.
</description><key id="137281980">16861</key><summary>Merge reindex to master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T15:10:55Z</created><updated>2016-03-01T17:00:40Z</updated><resolved>2016-03-01T15:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-29T17:49:50Z" id="190308023">I'm not entirely clear on why the SearchSourceBuilder and related Search code in core needs changing for the reindex API to be implemented. Whilst I don't have any big objections to the changes I am concerned when we make such changes hidden inside adding a feature that is a plugin when the change is not necessary for that plugin. The reason for this is two-fold: 1) it makes it difficult to see that a commit changes core code since the commit message will only say something like 'adds Reindex API' and 2) it adds a lot of risk to this change because it changes critical which is not immediately apparent mainly dues to 1). If we do go ahead with changing the search code in this PR can you please make it very obvious in both the PR description and the merge commit the changes that were made in core so if someone has to go through commits/PRs to find a bug in one of these core classes it can be seen that this change could be a cause and is not skips as unrelated?
</comment><comment author="colings86" created="2016-02-29T18:44:13Z" id="190326912">I only reviewed the Request handling bits but it makes sense to me now so LGTM for those bits :)
</comment><comment author="nik9000" created="2016-03-01T12:59:17Z" id="190713931">OK! Its time. I'm going to merge this to master locally and run the tests one last time. I think we're ok with the review we got because most of this was already reviewed when it went into the feature branch. The parts @colings86 reviewed were the bits that had to react to master changes the most when we merged master to the branch and so had the least review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add setFactory permission to GceDiscoveryPlugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16860</link><project id="" key="" /><description>This commit adds a missing permission and a simple test that
ensures we discover other nodes via a mock http endpoint.

Closes #16485 

@rjernst I had to make this integ test a ordinary testcase since it runs against an internal cluster. I didn't explore the test fixture path yet since it also requires a test plugin etc. I wonder if we can make this one test run against the internal cluster while others runs against external clusters? I am not super happy with the test but it's a major step forward since it really tests the integration of this plugins while everything else isn't really testing it. It also would have caught the security issue though
</description><key id="137273962">16860</key><summary>Add setFactory permission to GceDiscoveryPlugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugin Discovery GCE</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T14:44:27Z</created><updated>2016-03-01T07:55:46Z</updated><resolved>2016-03-01T07:55:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-29T14:54:33Z" id="190245296">Interesting. I like the idea of using an HTTP server to serve the mock responses.
Thanks @s1monw for fixing this issue.

LGTM
</comment><comment author="rjernst" created="2016-02-29T16:38:08Z" id="190280657">This looks ok to me. It is good to have some kind of test for GCE. I'm apprehensive about checking in the keyfile, but I guess it is just for a test...

At another time, I am happy to look at moving this to a fixture, as well as generating the keyfile, but this PR at least gets us into a better state than we are now.
</comment><comment author="rmuir" created="2016-02-29T16:52:52Z" id="190285793">+1
</comment><comment author="jasontedor" created="2016-02-29T16:57:58Z" id="190287508">I have the same apprehension as @rjernst but given 

&gt; At another time, I am happy to look at moving this to a fixture, as well as generating the keyhole

I'm good with the change too. Thanks for picking this up @s1monw.
</comment><comment author="s1monw" created="2016-02-29T21:03:21Z" id="190390863">@rjernst @jasontedor I pushed a new commit generating the keystore from gradle
</comment><comment author="rjernst" created="2016-02-29T21:22:12Z" id="190402323">Looks good, thanks!
</comment><comment author="jasontedor" created="2016-02-29T22:34:23Z" id="190430387">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move migrate_3_0 to migrate_5_0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16859</link><project id="" key="" /><description>5.0 is the version now.
</description><key id="137271701">16859</key><summary>Move migrate_3_0 to migrate_5_0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T14:36:54Z</created><updated>2016-02-29T14:59:11Z</updated><resolved>2016-02-29T14:44:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-29T14:38:49Z" id="190235755">Left a comment. Otherwise LGTM.
</comment><comment author="jasontedor" created="2016-02-29T14:41:59Z" id="190236950">This is fine, but you'll need to merge in my latest change since I just made a change to the document in #16858.
</comment><comment author="nik9000" created="2016-02-29T14:59:11Z" id="190247017">&gt; This is fine, but you'll need to merge in my latest change since I just made a change to the document in #16858.

Got it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note on Groovy dependencies to migration docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16858</link><project id="" key="" /><description>This commit adds a note to the migration docs regarding the reduction of
the Groovy dependencies from the groovy-all artifact to the groovy
artifact that was previously done in
180ab2493e96223479c2d5efd9fdd0f28fd12fee.

Closes #14787, supersedes #16808 
</description><key id="137269472">16858</key><summary>Add note on Groovy dependencies to migration docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Scripting</label><label>docs</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T14:28:32Z</created><updated>2016-03-02T09:12:26Z</updated><resolved>2016-02-29T14:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-29T14:29:40Z" id="190231771">The backport of this to 2.x will add the same note to the migration docs for 2.3, and will reduce the Groovy dependency to `groovy` from `groovy-all` (this is already reduced in master).
</comment><comment author="s1monw" created="2016-02-29T14:32:35Z" id="190232995">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add breaking change notes for logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16857</link><project id="" key="" /><description>ba5be0332dbf28a3fa4b0e73d4222a1da39b5dfe removed support for degrading
to slf4j and j.u.l but didn't document this as a breaking change because
it is only breaking for folks using Elasticsearch's jar as a java client.
People do that so this counts as a breaking change.

Also, if anyone was brave enough to try and replace log4j on an installed
version of Elasticsearch that will no longer work and this documents that
as well. It doens't get a full heading and instead lives with the java
client notes. Mostly because I can't imagine it worked consistently enough
for anyone to actually do it in the first place. We just never tested it
well enough to make sure we didn't break it after it was implemented.
</description><key id="137268812">16857</key><summary>Add breaking change notes for logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T14:26:05Z</created><updated>2016-02-29T16:29:07Z</updated><resolved>2016-02-29T16:29:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-29T16:06:54Z" id="190270582">LGTM
</comment><comment author="jasontedor" created="2016-02-29T16:07:17Z" id="190270711">LGTM.
</comment><comment author="nik9000" created="2016-02-29T16:12:00Z" id="190272204">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove QueryInnerHits from HasChildQuery, HasParentQuery and NestedQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16856</link><project id="" key="" /><description>After the `inner_hits` section has been refactored to be parsed on the coordinating node (see #10217) we need to replace QueryInnerHits from HasChildQueryBuilder, HasParentQueryBuilder and NestedQueryBuilder. Currently this still keeps the `inner_hits` part of the search source as a byte array that is parsed later on the shard. After the refactoring of `inner_hits`, we should be able to remove this.
</description><key id="137265339">16856</key><summary>Remove QueryInnerHits from HasChildQuery, HasParentQuery and NestedQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label></labels><created>2016-02-29T14:10:36Z</created><updated>2016-03-30T21:28:00Z</updated><resolved>2016-03-30T21:28:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-30T21:28:00Z" id="203642285">This was adressed in #17291.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query on a field that is also the index type name can search on any other field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16855</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.\* (seem fixed from 2)

**JVM version**: Java 1.8

**OS version**: Any

**Description of the problem including expected versus actual behavior**: If you do a query against a field (term, match) that is also the type name of the index, another field is matched. Query on a multi-field field. (Hard to explain, see the gist)

**Steps to reproduce**: I made a simple gist: https://gist.github.com/ebuildy/72fade2e26dea91256d6
1. Create an index, with a multi-field field (https://www.elastic.co/guide/en/elasticsearch/reference/current/_multi_fields.html)
2. Add a document with a single field
3. Query against a field that is the index type name (in my GIST: "title", I query against title.simple)
4. The document is matched but does not contain any field "title"

**Provide logs (if relevant)**:

Explains debug shows "weight(alias.simple:coca in 0)" whereas I specify the field "title" in the query.

It seems fixed from 2.0, but I am curious to know the reason!
</description><key id="137261094">16855</key><summary>Query on a field that is also the index type name can search on any other field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebuildy</reporter><labels /><created>2016-02-29T13:52:19Z</created><updated>2016-02-29T18:48:25Z</updated><resolved>2016-02-29T18:48:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-29T18:48:25Z" id="190329533">In 1.x, there was a feature that attempted to disambiguate fields of the same name in different types. That is the feature you are hitting here, and it was removed in 2.0. See #8872.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch side changes so Elastic can run vagrant tests in its continuous integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16854</link><project id="" key="" /><description>**Describe the feature**:
It'd be super helpful if we could run the vagrant tests in continuous integration at Elastic ([here](http://build.elastic.co/), search from "ES server master"). That way we know when we break scripts or the deb or rpm or something. I'm going to use this ticket to track effort:
- [x] Get the tests passing again. We need to do this to make sure we don't break anything in the next step. #17152 
- [x] Switch the boxes to those that Elastic has on Vagrant Atlas ([here](https://atlas.hashicorp.com/elastic)). We need to do this because those are easier to host in Elastic's CI - mostly because they have lxc versions which can be spun up in AWS. #17195 
- [x] Add sles-11 checks. Elastic added a sles-11 box to vagrant atlas. We should use it! 
</description><key id="137259720">16854</key><summary>Elasticsearch side changes so Elastic can run vagrant tests in its continuous integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>Meta</label><label>test</label></labels><created>2016-02-29T13:48:16Z</created><updated>2016-03-21T12:27:47Z</updated><resolved>2016-03-21T12:27:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-16T19:23:46Z" id="197500527">The first task

&gt; Get the tests passing again. We need to do this to make sure we don't break anything in the next step.

is being tracked in #17152.
</comment><comment author="jasontedor" created="2016-03-18T17:35:34Z" id="198464826">The second task

&gt;  Switch the boxes to those that Elastic has on Vagrant Atlas (here). We need to do this because those are easier to host in Elastic's CI - mostly because they have lxc versions which can be spun up in AWS.

is being tracked in #17195.
</comment><comment author="jasontedor" created="2016-03-18T17:46:17Z" id="198467705">The third task

&gt; Add sles-11 checks. Elastic added a sles-11 box to vagrant atlas. We should use it!

[There does not appear to be a SLES 11 package for OpenJDK 8](https://build.opensuse.org/package/repositories/Java:Factory/java-1_8_0-openjdk), so I think that we should drop this box from support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias creation operation goes very slow when we have more than 100000 aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16853</link><project id="" key="" /><description>I've developed a snippet of code so that it tries to create 100000 Aliases on Elasticsearch. I found that as long as number of Aliases was increasing the time takes for creation an Alias is raising as well. It seems complexity of this operation is O(n) that **makes no sense**.
</description><key id="137232445">16853</key><summary>Alias creation operation goes very slow when we have more than 100000 aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malaki12003</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2016-02-29T11:51:29Z</created><updated>2016-03-02T10:32:21Z</updated><resolved>2016-02-29T15:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ebuildy" created="2016-02-29T13:10:12Z" id="190203776">Hmmmmmm

Can you post your method and machine configuration please?
</comment><comment author="jasontedor" created="2016-02-29T15:39:58Z" id="190261060">&gt; It seems complexity of this operation is O(n) that makes no sense.

Every alias change requires a cluster state update. Cluster state updates are published using cluster state diffs. The diff is [calculated](https://github.com/elastic/elasticsearch/blob/b83b9d69c9187ed969853fdcec3cadc7d67be56f/core/src/main/java/org/elasticsearch/cluster/DiffableUtils.java#L228-L232) by checking every existing alias the before cluster state, and whether or not it has been removed in the after cluster state. This is clearly linear, so it makes perfect sense. The benefit of sending cluster state diffs vastly outweighs the advantages of not executing this linear time operation, especially since having 100000 aliases is an anti-pattern. What is more, even if this linear diff operation was removed, the published cluster state updates grow linearly in the number of aliases.
</comment><comment author="malaki12003" created="2016-03-01T08:25:33Z" id="190609825">In my snippet of code I make requests in serialized form (one by one). For the first request it takes around 10 ms and in a linear growth, it goes up to 170 ms for 40000th request. I need to create requests on demand, this is why I can't use bulk format. Please note that in the simple test case, I dedicated a machine that provided with 16 gig ram and a fast SSD hard without any Cluster configuration and other loads. So, it  seems that the problem is not related to clustering. In real situation, it event can be worse.I have 2 clustered servers in production and more than 100000 Alias that are made on them. I found for some requests, latency is up to 40s. I really think it is not acceptable.
The elastic version is 2.1.1 with 100 shards. The problem is just for Alias operations (create/delete) and other operations like indexing&amp; searching work well. In fact, I have a web application with more than 500000 users. I make a routing policy through aliasing for each user. For each user request I check that wheather the alias is create for the user or not. If there is no alias for the user, I make an alias request. What is your suggestion? 
</comment><comment author="jasontedor" created="2016-03-01T13:16:46Z" id="190720261">&gt; The elastic version is 2.1.1 with 100 shards.

On two nodes, for a single index? If my reading of this situation is correct, that strikes me as another anti-pattern.

&gt; What is your suggestion?

I think that you should use filters directly on each request. With the information that you've given so far, I doubt that the routing is necessary.
</comment><comment author="malaki12003" created="2016-03-01T13:52:31Z" id="190731065">I have 1 index, 100 shards and a routing policy based on user ids like the following example:
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html#_examples_2
There are more than 500000 users in my system. So, absolutely I need routing parameter. According to this information,what is your suggestion?
</comment><comment author="jasontedor" created="2016-03-01T17:05:58Z" id="190816936">&gt; I have 1 index, 100 shards and a routing policy based on user ids like the following example: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html#_examples_2

That is way too many shards for a single index on one or two machines. You'll be fine if you drop this to something more reasonable like two shards. As a bonanza, you can almost surely drop the custom routing.

I'm also sorry that the documentation has led you astray here. :(

&gt; There are more than 500000 users in my system. So, absolutely I need routing parameter.

I don't think that you do, but I could be wrong. Why do you think that you do?

&gt; According to this information,what is your suggestion?

I think that you should drop the number of shards to two, I think that you do not need to use custom routing, and I think that instead of using aliases you should use a filter.
</comment><comment author="clintongormley" created="2016-03-02T10:32:21Z" id="191175495">&gt; There are more than 500000 users in my system.

@malaki12003 we have long spoken about faking an index-per-user using aliases, but this phrasing was ill chosen as it makes people believe that aliases are free and will scale infinitely.

Unfortunately the truth is more prosaic.  As you have found, alias creation scales linearly.  Frankly, I'm impressed that you got to 100k!  In earlier versions we struggled to get over 10k.

This model works with small numbers of "users" (perhaps we should talk about index-per-tenant instead?) but at the scale you're talking about, you'll have to take a different approach.  The problem with aliases is that they are held in memory all the time on every node.  But you don't have all 500k users on your site at the same time.  Instead, you could move this logic client side and use the user_id to add a routing value and filter to every request.

Regarding the number of shards you have, ie 100.  Think about this carefully.  You're saying that you plan to grow to a cluster with 100 nodes, just for the primary shards.  Another 100-200 nodes for the replicas. Do you really plan on a cluster of 300 nodes?  And you're sure that you will never reindex (eg to change your mappings) while growing from two nodes to 300?

My suggestion is to start with a more realistic number of shards like 5 or 10.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES client node may get OOM if the data node lags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16852</link><project id="" key="" /><description>**Elasticsearch version**:  2.2.0

**JVM version**:  Oracle JDK 1.8.0_60-b27

**OS version**: CentOS 6.6

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1.  get some ES client nodes (node.client: true) in front of some ES data nodes(node.data: true)
2.  ingest documents in a very high traffic volume to ES client nodes
3.  sometimes the data nodes lag (maybe doing index merge), but ES client nodes still receive documents and may get OOM,  the expected behaviour is ES client nodes refuse new requests or just disable channel read for a while.

In my use case, I allocate 10G heap for ES client node,  it can accumulate about 6G requests and then gets OOM soon, here is the jhat analysis on the heap dump(it's almost OOM but not yet, sorry for the bad typesetting below),  I confirmed most "class [B" instances are referenced by org.elasticsearch.http.netty.NettyHttpRequest.

```
Class   Instance Count  Total Size
class [B    955327  6514870246
class [C    40860058    1838467108
class java.lang.String  40852528    490230336
class org.elasticsearch.action.index.IndexResponse  6605067     429329355
class org.elasticsearch.action.bulk.BulkItemResponse    9517699     266495572
class [Ljava.lang.Object;   249443  165445416
class org.elasticsearch.action.bulk.BulkItemResponse$Failure    2912632     116505280
class [I    953017  100724932
class org.elasticsearch.action.index.IndexRequest   468109  68812023
class org.jboss.netty.buffer.BigEndianHeapChannelBuffer     2079219     49901256
class java.lang.StackTraceElement   1206188     33773264
class [Ljava.util.HashMap$Node;     113947  18542872
class java.util.HashMap     345776  16597248
class org.jboss.netty.buffer.SlicedChannelBuffer    475522  15216704
class [Lorg.jboss.netty.buffer.ChannelBuffer;   45252   12156088
class [Ljava.lang.StackTraceElement;    88263   11062296
class org.elasticsearch.action.bulk.BulkItemRequest     472770  9928170
class java.util.HashMap$Node    350723  9820244
class [S    117658  9414512
class org.elasticsearch.common.bytes.ChannelBufferBytesReference    477579  3820632
class [Lorg.elasticsearch.action.bulk.BulkItemRequest;  4706    3805056
class org.elasticsearch.transport.RemoteTransportException  58822   3764608
class com.chenlb.mmseg4j.CharNode$TreeNode  248024  2728264
class org.jboss.netty.buffer.CompositeChannelBuffer     42523   1913535
class java.util.ArrayList   115491  1847856
class java.net.InetAddress$InetAddressHolder    67734   1625616
class com.fasterxml.jackson.core.json.UTF8StreamJsonParser  7477    1622509
class java.net.InetSocketAddress$InetSocketAddressHolder    75698   1513960
class org.elasticsearch.common.util.concurrent.EsRejectedExecutionException     29411   1441139
class [Lorg.elasticsearch.action.ActionWriteResponse$ShardInfo$Failure;     87248   1395968
class org.elasticsearch.action.ActionWriteResponse$ShardInfo    87247   1395952
class com.fasterxml.jackson.core.json.JsonReadContext   22435   1256360
class java.net.Inet4Address     67734   1083744
class org.jboss.netty.buffer.TruncatedChannelBuffer     38175   1068900
class org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext   19147   957350
class org.jboss.netty.handler.codec.http.DefaultHttpHeaders$HeaderEntry     14257   741364
class java.lang.Class   7129    655868
class org.elasticsearch.cluster.routing.ShardRouting    5902    625612
class java.net.InetSocketAddress    75698   605584
class sun.nio.ch.SocketChannelImpl  4035    589110
class org.jboss.netty.channel.socket.nio.NioAcceptedSocketChannel   2944    562304
class com.fasterxml.jackson.core.io.IOContext   7477    545821
class java.util.concurrent.ConcurrentHashMap$Node   19442   544376
class com.fasterxml.jackson.core.sym.ByteQuadsCanonicalizer     7480    508640
class java.lang.Character   248083  496166
class org.elasticsearch.common.transport.InetSocketTransportAddress     60871   486968
class com.fasterxml.jackson.core.util.TextBuffer    7477    486005
class [D    16  369768
class org.elasticsearch.cluster.routing.PlainShardIterator  17418   348360
class [Ljava.util.concurrent.ConcurrentHashMap$Node;    60  327680
class com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper    7477    314034
class [Lorg.jboss.netty.handler.codec.http.DefaultHttpHeaders$HeaderEntry;  2044    310688
class org.elasticsearch.action.bulk.BulkShardRequest    4706    310596
class org.elasticsearch.cluster.ClusterStateObserver    4705    305825
class java.util.Collections$UnmodifiableRandomAccessList    18381   294096
class org.elasticsearch.cluster.routing.IndexShardRoutingTable  2951    286247
class org.elasticsearch.action.bulk.TransportBulkAction$2   4705    263480
class com.carrotsearch.hppc.ObjectObjectHashMap     5264    257936
class com.chenlb.mmseg4j.CharNode   12726   254520
class sun.nio.cs.US_ASCII$Decoder   5260    241960
class org.jboss.netty.channel.socket.nio.NioClientSocketChannel     1092    235872
class org.jboss.netty.channel.AbstractChannel$ChannelCloseFuture    4038    218052
class org.jboss.netty.channel.socket.nio.DefaultNioSocketChannelConfig  4034    193632
class org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase  4705    188200
class [Ljava.io.Closeable;  7690    184560 
```
</description><key id="137224649">16852</key><summary>ES client node may get OOM if the data node lags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Dieken</reporter><labels /><created>2016-02-29T11:19:06Z</created><updated>2016-03-05T12:11:07Z</updated><resolved>2016-02-29T13:07:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Dieken" created="2016-02-29T11:23:41Z" id="190163202">I investigated a little on the source code, the ES class "HttpPipeliningHandler" has a limit(10000 by default) on the in-flight **responses** **per** ingester connection,  but seems no limit on the **total** in-flight requests from all ingester connections,  after a glance at class org.elasticsearch.rest.action.bulk.RestBulkAction and its related classes in the call chain, the document ingesting **on ES client node** seems not go to "bulk.queue".

```
# the ES client nodes and data nodes run on same machines, that's why each IP occurs twice.
$ curl -s es-cluster-hostname/_cat/thread_pool | sort -k1,1
172.20.122.59  172.20.122.59            0          0             0            0           0              0             0            0               0 
172.20.122.59  172.20.122.59           32        492        299489            0           0              0             0            0               0 
172.20.122.60  172.20.122.60            0          0             0            0           0              0             0            0               0 
172.20.122.60  172.20.122.60           32        108       1334741            0           0              0             0            0               0 
172.20.122.61  172.20.122.61            0          0             0            0           0              0             0            0               0 
172.20.122.61  172.20.122.61           32        400        452529            0           0              0             0            0               0 
172.20.122.62  172.20.122.62            0          0             0            0           0              0             0            0               0 
172.20.122.62  172.20.122.62           11          0        856437            0           0              0             0            0               0 
......
```
</comment><comment author="danielmitterdorfer" created="2016-02-29T13:07:40Z" id="190202887">@Dieken: Thanks for the report! We currently work on limiting request sizes in #16011 so I am closing this one in favor of the other ticket.
</comment><comment author="Dieken" created="2016-03-05T09:08:15Z" id="192609965">Just for the record,  I got a rude patch based on v2.2.0 for my use case: https://github.com/Dieken/elasticsearch/commit/f2d487eaf1213daa6ad25f13fbccdd1d6b5930f4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make TermsQuery considered costly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16851</link><project id="" key="" /><description>This will be fixed in Lucene 6. This commit aims at backporting the change to
elasticsearch 2.3 which will be on Lucene 5.

See https://issues.apache.org/jira/browse/LUCENE-7050.
</description><key id="137215099">16851</key><summary>Make TermsQuery considered costly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-02-29T10:38:55Z</created><updated>2016-03-01T15:58:42Z</updated><resolved>2016-03-01T15:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2016-02-29T13:26:04Z" id="190210592">This is somewhat surprising - I've worked on plenty of systems where TermsQuery are perfectly cachable and a low hanging fruit when it comes to optimisations.

To my understanding, this will prevent TermsQuery from being cached many times - based on heuristics, yes, but still biased towards not caching.

Can we have a way to opt-in for caching a query manually, similar to pre 2.x ES?
</comment><comment author="jpountz" created="2016-02-29T13:37:21Z" id="190215035">Sorry maybe my explanation was not clear but the goal is to make TermsQuery cached more aggressively, not less. The queries that are classified as "costly" in that case are queries that need to evaluate documents on the whole index, regardless of whether they are intersected with a selective query or not (eg. TermsQuery, PrefixQuery, NumericRangeQuer).
</comment><comment author="jimczi" created="2016-02-29T13:59:58Z" id="190222948">LGTM though I wonder if we could check the size of the terms query in order to decide if it's a costly query or not. If the number of terms is below 16 then the query is rewritten into a "cheap" boolean query. 
I wonder if we should also add GeoPointTermQueryConstantScoreWrapper (not on this PR of course).
IMO we should add a simple check for any query that would be able to tell if the query will be executed on the whole index beforehand. This could make the caching more efficient, we could directly extract the bitset created by this query instead of relying on advance to build the bitset for the cache.
</comment><comment author="jpountz" created="2016-02-29T14:37:50Z" id="190235199">&gt; If the number of terms is below 16 then the query is rewritten into a "cheap" boolean query. 

I think this is fine: the query cache only sees rewritten queries so if a TermsQuery that contains less than 16 terms is passed to IndexSearcher, the query cache will actually see a BooleanQuery.

&gt; IMO we should add a simple check for any query that would be able to tell if the query will be executed on the whole index beforehand.

I have been avoiding it so far as I would really like queries to be unaware of caching as much as possible.
</comment><comment author="jimczi" created="2016-02-29T15:39:50Z" id="190260989">&gt; I think this is fine: the query cache only sees rewritten queries so if a TermsQuery that contains less than 16 terms is passed to IndexSearcher, the query cache will actually see a BooleanQuery.

No the rewrite is noop, the resolution is done in the weight when the scorer is created. It's the same in any MultiTermQuery, the rewrite does not check anything just returning a new class which has the heuristic to create a scorer based on a boolean query or on a doc id set already built.

&gt; I have been avoiding it so far as I would really like queries to be unaware of caching as much as possible.

I agree but we should have a way to detect if a costly query is really costly ;). Maybe we could have a post query callback which checks the cost of the query, if the cost is low then no need to add the query in the frequency tracking ring buffer ?
</comment><comment author="jpountz" created="2016-03-01T10:35:18Z" id="190654970">TermsQuery actually rewrites both in rewrite and per segment: rewrite generates a BooleanQuery when there are less than 16 terms. If there are more than 16 terms, it is also rewritten per segment when less than 16 terms from the query are also contained in the terms dict. So I think that's fine?

Regarding an API for detecting costly queries, don't get me wrong, I understand the need and wish we had one. But having such an API only for the purpose of caching feels wrong to me, so I'd rather wait and see if we can have other use-cases for it like better query execution (regardless of caching).
</comment><comment author="jimczi" created="2016-03-01T11:26:03Z" id="190675807">&gt; So I think that's fine?

Sorry I checked MultiTermQuery and TermsQuery does not inherit from it.

Anyway this PR should help a lot for use cases like https://github.com/elastic/elasticsearch/issues/16031 so again LGTM.
</comment><comment author="synhershko" created="2016-03-01T11:29:43Z" id="190678341">@jpountz :+1: , thanks for explaining

As a side note, and as a power user with many requirements for low latency querying abilities, the lack of some manual control of caching aches to me quite a bit. While you are doing great work here, there will always be those corner cases and bugs, and for those it'd be really helpful to have a "force_cache":true option for queries in the filter context.
</comment><comment author="jpountz" created="2016-03-01T14:00:37Z" id="190733614">If the common filters are known beforehand, wouldn't it be better to ad a flag at index time to know whether those queries match in order to be able to run fast term queries at search time?

For your low-latency requirements, I think that something we should explore would be to automatically warm up the cache for new segments based on the popular filters from the history (we don't to it today and these popular filters will likely only get cached on the first time that they are used on these new segments, potentially increasing the latency if the filter is costly or matches many docs).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added file name to exceptions when failing to read index state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16850</link><project id="" key="" /><description>Closes #16713
</description><key id="137211208">16850</key><summary>Added file name to exceptions when failing to read index state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T10:22:35Z</created><updated>2016-02-29T10:44:28Z</updated><resolved>2016-02-29T10:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-29T10:39:49Z" id="190147785">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations returns wrong results </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16849</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.3

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

Aggregations returns wrong results on one of my indices - running avg or max or min on a small set of double field called "time", it looks like it's ignoring the floating point 

**the index has the following mapping:**

Please note that we have two "time" fields on the same index, One under the "root" object witch is mapped as double, And one under the "event.properties"  witch is mapped as long.

```
{
    indexname: {
        mappings: {
            web-api: {
                properties: {
                    ....
                    event: {
                        properties: {
                            time: {
                                type: "long"
                            },
                            ....
                        }
                    },
                    .....
                    time: {
                        type: "double"
                    },
                    .....
                }
            }
        }
    }
}
```

**when i run the following query and aggs i get wrong results:**

```
curl -XGET elasticsearch:9200/indexname/web-api/_search -d '
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "_exists_: time"
        }
      },
      "filter": {
        "range": {
          "@timestamp": {
            "gte":  1456733280000,
            "lte":  1456733306000
          }
        }
      }
    }
  }
  , "size": 2
  , "aggs": {
    "min-agg": {
      "min": {
        "field": "time"
      }
    },
    "max-agg": {
      "max": {
        "field": "time"
      }
    },
    "avg-agg": {
      "avg": {
        "field": "time"
      }
    }
  }
}
' 
```

**Results:**

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 8,
      "successful": 8,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "indexname",
            "_type": "web-api",
            "_id": "AVMsEjtIrdN06BhGvXc-",
            "_score": 1,
            "_source": {
               ....
               "@timestamp": "2016-02-29T08:08:13.929+00:00",
               ....
               "time": 1741.3100000023842
            }
         },
         {
            "_index": "indexname",
            "_type": "web-api",
            "_id": "AVMsEjvO9e5_IXiSdhbm",
            "_score": 1,
            "_source": {
               ....
               "@timestamp": "2016-02-29T08:08:14.388+00:00",
               ....
               "time": 2377.314999997616
            }
         }
      ]
   },
   "aggregations": {
      "max-agg": {
         "value": 4657446186044490000
      },
      "min-agg": {
         "value": 4655373177816613000
      },
      "avg-agg": {
         "value": 4656409681930551000
      }
   }
}
```

Any idea, why we are getting those results ??

Thanks
</description><key id="137187383">16849</key><summary>Aggregations returns wrong results </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofer-velich</reporter><labels /><created>2016-02-29T08:35:52Z</created><updated>2016-02-29T09:49:26Z</updated><resolved>2016-02-29T08:57:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T08:57:23Z" id="190108673">Hi @ofer-velich 

The `time` field is being resolved to different fields at query time: one to `event.time` and one to `time`.  This ambiguity has been cleared up in 2.x (see #8870) but cannot be resolved in 1.x.

You can try prepending the type name to to disambiguate: `web-api.time` vs `web-api.event.time`

(Prepending the type name is no longer supported in 2.x because it creates other ambiguities, but should work for your case).

i highly recommend upgrading.
</comment><comment author="ofer-velich" created="2016-02-29T09:49:26Z" id="190128603">Thanks! 
We are currently upgrading our clusters to 2.X.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce additional frequency metric other than term frequency </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16848</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.7.2

**JVM version**:  1.8

**OS version**: 14.0 Ubuntu

**Description of the problem including expected versus actual behavior**:
I want to introduce additional frequency metric on top of term frequency at index time. The frequency metric is calculated based on the term-document matrix.

I tried overriding DefaultSimilarity.Java. The class contains the term frequency, document frequency information. But, the mapping of the term frequency to the term and document is missing in that class.

I tried using ScoreFunctionBuilder.Java to modify the scoring at query time. But, the score is independent of term-frequency and only depends on the modified scoring logic. 

I want the exact class which calculates the term frequency and document frequency and can we override the class using a plugin. Kindly help me out on this issue.

Also, the inference from Mastering Elasticsearch Guide book  is that term frequency is calculated and stored at the time of Indexing. If this is correct, please let me know the class which calculates the term frequency at the time of Indexing. Also correct me if I am wrong.
</description><key id="137184094">16848</key><summary>Introduce additional frequency metric other than term frequency </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mahathi-Bhagavatula</reporter><labels /><created>2016-02-29T08:26:56Z</created><updated>2016-02-29T08:53:42Z</updated><resolved>2016-02-29T08:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T08:53:42Z" id="190107986">Hi @Mahathi-Bhagavatula 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use diffs for ingest metadata in cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16847</link><project id="" key="" /><description /><key id="137171113">16847</key><summary>Use diffs for ingest metadata in cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-29T07:11:55Z</created><updated>2016-03-01T13:02:51Z</updated><resolved>2016-03-01T13:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-01T11:51:01Z" id="190686043">left one minor comment on testing, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Escaping params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16846</link><project id="" key="" /><description /><key id="137037803">16846</key><summary>Escaping params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-28T10:48:27Z</created><updated>2016-03-18T13:15:52Z</updated><resolved>2016-02-28T16:18:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-28T16:17:05Z" id="189898638">Thanks @synhershko! I'll apply this patch against master (on the discovery-ec2 docs) and then backport to the 2.x series.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Deprecated Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16845</link><project id="" key="" /><description>Currently, when we deprecate settings, it's only via the documentation. This PR enables the ability to create actual Setting objects that warn on usage to notify the user that they are deprecated without having to litter the code with checks surrounding them.

I am waiting to add tests to ensure that we like the direction that this is going in.
</description><key id="137019745">16845</key><summary>Support Deprecated Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Settings</label></labels><created>2016-02-28T07:14:21Z</created><updated>2016-03-03T18:56:55Z</updated><resolved>2016-03-03T18:56:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-02-28T07:16:54Z" id="189809564">I wonder if we should add a `Version` field to the deprecated setting object so that it can tell users when it was deprecated to make it easier to find it in the docs.
</comment><comment author="pickypg" created="2016-02-29T17:36:56Z" id="190304143">I removed the `replacementKey`, which was optional anyway, and added the `version` as required. This should make it easier to find why the warning appears as well as sending people to the breaking changes lists, which is always a good thing to read over.
</comment><comment author="dadoonet" created="2016-02-29T18:01:19Z" id="190312077">@pickypg If we also remove the version, then may be with the coming #16629 we can add a new enum `Deprecated` and we can just "tag" a setting as deprecated.

For example, write from:

``` java
public static final Setting&lt;String&gt; TRANSPORT_TYPE_SETTING = Setting.simpleString("transport.type", SettingsProperty.ClusterScope);
```

to 

``` java
public static final Setting&lt;String&gt; TRANSPORT_TYPE_SETTING = Setting.simpleString("transport.type", SettingsProperty.ClusterScope, SettingsProperty.Deprecated);
```

Would that make sense?
</comment><comment author="pickypg" created="2016-02-29T18:08:11Z" id="190314922">@dadoonet Love it. It also removes the entire need for a proxy, which is a constant nuisance.
</comment><comment author="pickypg" created="2016-02-29T18:09:23Z" id="190315246">@dadoonet I accidentally clicked comment too soon. The only thing that I want to be sure that we do is to consistently log warnings for deprecated setting usage. I will miss the idea of having the version published so that it makes it easy to find in the docs.
</comment><comment author="pickypg" created="2016-03-01T18:45:09Z" id="190848144">@dadoonet I'm happy to close this PR once yours adds `Deprecated`.
</comment><comment author="dadoonet" created="2016-03-02T09:08:14Z" id="191143831">@pickypg I added this c103e40e726c69fa093bb22e2574499dbce0282c to my branch.
</comment><comment author="clintongormley" created="2016-03-02T10:54:22Z" id="191186465">@pickypg this should be using the deprecation logger https://github.com/elastic/elasticsearch/pull/11285
</comment><comment author="dadoonet" created="2016-03-02T14:28:44Z" id="191259329">@clintongormley You're right. I missed the DeprecationLogger infra was there already. I applied it on my branch: https://github.com/dadoonet/elasticsearch/commit/e4031932edaea00b5ec06f6153aa7e02e8f0e74e
</comment><comment author="pickypg" created="2016-03-03T18:56:18Z" id="191912499">This is no longer needed with @dadoonet's changes in https://github.com/elastic/elasticsearch/pull/16629.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor bootstrap checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16844</link><project id="" key="" /><description>This commit refactors the bootstrap checks into a dedicated class. The
refactoring provides a model for different limits per operating system,
and provides a model for unit tests for individual checks.

Relates #16733, relates #16835 
</description><key id="137002735">16844</key><summary>Refactor bootstrap checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-02-28T03:56:16Z</created><updated>2016-03-10T18:35:16Z</updated><resolved>2016-03-02T13:48:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-28T09:05:42Z" id="189825629">I guess for me what is broken, is the implication that if i want to bind to some address other than the default, that this means i'm gonna overload my machine with thousands of shards.

I don't change things like file descriptors limits on my computer because i really don't want to have non-default values here as a developer. Now what to do... I suppose this effectively stops me from working on the networking code at the very least.
</comment><comment author="s1monw" created="2016-02-29T16:33:41Z" id="190279228">@jasontedor  I guess we should go with an explicit setting for now? I think it's less controversial? the other option is to move to an opt out setting?
</comment><comment author="jaymode" created="2016-02-29T16:38:22Z" id="190280791">&gt; the other option is to move to an opt out setting?

+1. If the error message tells you that you can disable this check with setting `foo` but never do it in production, I think that would be a good compromise?
</comment><comment author="nik9000" created="2016-02-29T17:07:20Z" id="190292456">&gt; +1. If the error message tells you that you can disable this check with setting foo but never do it in production, I think that would be a good compromise?

I prefer opt out as well. You have to intentionally think "I don't care about safe stuff, just do things". And `gradle run` can just set it all the time.
</comment><comment author="jaymode" created="2016-02-29T17:47:18Z" id="190306983">I'll caveat my earlier comment, **if** we are adding a setting then I prefer opt-out. I think its a fine line between preventing users from shooting themselves in the foot and making it harder to do development when a change is made like setting `network.host` so two developers could collaborate on a change together.
</comment><comment author="s1monw" created="2016-03-01T12:15:00Z" id="190691610">to be honest I don't like either way. The fact that OSX is a pain to get this configured correctly is concerning and given the fact that OSX is unlikely a production env we can maybe opt out automatically if we run on OSX? I mean all these checks are best effort so I guess that's not that much of a problem. what do you think @jasontedor  just use this constant: `Constants.MAC_OS_X`
</comment><comment author="jasontedor" created="2016-03-01T12:42:49Z" id="190707994">&gt; The fact that OSX is a pain to get this configured correctly is concerning and given the fact that OSX is unlikely a production env we can maybe opt out automatically if we run on OS X?

I am not in favor of opting out automatically on OS X, but I am in favor of reduced limits for the file descriptor check on OS X. I think that future checks can be handled on a case-by-case basis.
</comment><comment author="jasontedor" created="2016-03-01T21:43:49Z" id="190919604">@rmuir @s1monw I pushed 783fbd2a773f62435c33dc3208b49f05a8282e42 which drops the file descriptor check on OS X to the default setting for that OS, and removed the different checks between snapshot and release builds. 
</comment><comment author="s1monw" created="2016-03-02T13:06:41Z" id="191231587">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>S3 client side encryption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16843</link><project id="" key="" /><description>Closes https://github.com/elastic/elasticsearch/issues/13673

I'm picking up a unfinished PR here https://github.com/elastic/elasticsearch-cloud-aws/pull/118. Most of the implementation keeps the same.

This PR add s3 client side encryption functionality. It uses the envelope encryption of the AWS ADK for Java (AES 256 in CBC mode). You case use keys of 128, 192 or 256 bits but you have to install the JCE because the envelope encryption will use 256bits keys.

I'm not sure about this: I remove the MD5 checking part in the `doUpload` method. I think AWS S3 SDK is doing this for us. 
More here: https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L1449-L1469. 
</description><key id="136995125">16843</key><summary>S3 client side encryption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Repository S3</label></labels><created>2016-02-28T02:21:31Z</created><updated>2017-02-13T15:54:23Z</updated><resolved>2016-03-24T19:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2016-02-28T02:22:31Z" id="189766011">@dadoonet would you like to take a look, if you have time? :-)
</comment><comment author="dadoonet" created="2016-03-05T12:43:11Z" id="192638618">I gave a first look (not complete yet) and it's a really good start.

About removing MD5 check, I do agree that it sounds already here in the AWS SDK. Any chance you can test manually that it goes through this line in debug mode? https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L1455
</comment><comment author="xuzha" created="2016-03-07T05:07:34Z" id="193101687">Thx @dadoonet, I just updated the PR ;-)

About the debugging, I confirmed that AWS SDK is checking the MD5 for us, the screen shot: 
![screen shot 2016-03-05 at 14 48 48](https://cloud.githubusercontent.com/assets/1799964/13561231/692fc222-e3df-11e5-91e2-c20489b90875.png)
</comment><comment author="xuzha" created="2016-03-15T05:57:28Z" id="196676818">@dadoonet can you take another look when you have time? :-)
</comment><comment author="dadoonet" created="2016-03-15T17:05:38Z" id="196925329">I left a note which we can discuss in another issuer later. The current PR looks good to me.
Not sure if we want to backport this to 2.x branch as well...
</comment><comment author="xuzha" created="2016-03-24T19:59:01Z" id="200994291">Merged into master. Thanks @dadoonet  for the review and sorry for the delay.
</comment><comment author="xuzha" created="2016-03-24T20:01:52Z" id="200996433">@clintongormley I have noticed @NicolasTr did not signed the CLA at that commit. Is there any problem here?
</comment><comment author="GlenRSmith" created="2016-03-24T20:13:41Z" id="201001022">@xuzha if it helps, @NicolasTr signed the CLA in:
https://github.com/elastic/elasticsearch-cloud-aws/pull/118
</comment><comment author="jasontedor" created="2016-03-24T20:22:06Z" id="201003787">I don't think that this pull request should have been merged. The CLA doesn't pass on one of the commits, and one of the [commits](https://github.com/elastic/elasticsearch/pull/16843/commits/ea78fd6560624968fada645c6358eea84e4afb7f#diff-9df9f5bfe09e4d36bf31d31c387a3c94R22) has merge conflict markers in it.
</comment><comment author="xuzha" created="2016-03-24T20:27:31Z" id="201005129">Thanks @jasontedor 

The first two commits should be together, I picked up a really old commit from elastic/elasticsearch-cloud-aws#118, and fixed in the later commit. 
</comment><comment author="jasontedor" created="2016-03-24T20:30:04Z" id="201005784">&gt; The first two commits should be together, I picked up a really old commit from [elastic/elasticsearch-cloud-aws#118](https://github.com/elastic/elasticsearch-cloud-aws/pull/118), and fixed in the later commit.

We should never knowingly commit code that doesn't compile.
</comment><comment author="billxinli" created="2016-05-19T21:36:37Z" id="220459148">Curious if we will ever see this feature implemented or merged in?
</comment><comment author="hoffoo" created="2016-06-21T23:51:45Z" id="227605958">Any hope that we will see this?
</comment><comment author="dchang-novotec" created="2017-02-13T15:54:23Z" id="279432887">^^^ ditto, would definitely like to see this feature in the next release</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor spelling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16842</link><project id="" key="" /><description>Old: the terms that is has in a field.
Fixed: the terms that it has in a field.
</description><key id="136973459">16842</key><summary>Fix minor spelling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhlqn</reporter><labels><label>docs</label></labels><created>2016-02-27T21:58:40Z</created><updated>2016-02-29T02:54:18Z</updated><resolved>2016-02-29T00:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T00:32:18Z" id="189980353">thanks @anhlqn 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain what the simple analyzer does</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16841</link><project id="" key="" /><description /><key id="136973380">16841</key><summary>Explain what the simple analyzer does</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thesmart</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-02-27T21:57:30Z</created><updated>2016-05-07T14:39:40Z</updated><resolved>2016-05-07T14:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T00:31:41Z" id="189980306">Hi @thesmart 

Same comments apply as in https://github.com/elastic/elasticsearch/pull/16840

thanks
</comment><comment author="clintongormley" created="2016-05-07T14:39:39Z" id="217641661">CLA not signed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain what the whitespace analyzer does</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16840</link><project id="" key="" /><description /><key id="136973359">16840</key><summary>Explain what the whitespace analyzer does</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thesmart</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-02-27T21:57:09Z</created><updated>2016-05-07T14:39:36Z</updated><resolved>2016-05-07T14:39:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T00:30:50Z" id="189980249">Thanks for the PR.  I've left a couple of comments, also please could I ask you to sign the CLA so that I can merge it in once it has been updated?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2016-05-07T14:39:36Z" id="217641659">CLA not signed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove suppressAccessChecks permission for Groovy script plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16839</link><project id="" key="" /><description>Closes #16527
</description><key id="136951746">16839</key><summary>Remove suppressAccessChecks permission for Groovy script plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-27T18:24:12Z</created><updated>2016-02-27T18:52:43Z</updated><resolved>2016-02-27T18:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-27T18:51:41Z" id="189701542">LGTM
</comment><comment author="jasontedor" created="2016-02-27T18:52:05Z" id="189701561">Yay! LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort term aggregation with nested aggregation in order path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16838</link><project id="" key="" /><description>Right now the below aggregation is not possible even though the 'nested_agg' does return a single bucket and nested aggregation is a single bucket aggregation. 

Below sample aggregation generates an error message that says 'nested_agg' is not a single bucket aggregation and can not be in the order path.

```
{
buckets: {
terms: {
  field: 'docId',
  order: {'nested_agg&gt;sum_value': 'desc'}
},
aggs: {
  nested_agg: {
    nested: {
      path: 'my_nested_object'
    },
    aggs: {
      sum_value: {
        sum: {field: 'my_nested_object.value'}
      }
    }
   }
  }
 }
}
```
</description><key id="136912400">16838</key><summary>Sort term aggregation with nested aggregation in order path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aresn</reporter><labels><label>:Aggregations</label><label>bug</label><label>discuss</label><label>high hanging fruit</label></labels><created>2016-02-27T12:17:44Z</created><updated>2017-07-18T14:06:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T00:24:56Z" id="189979873">@colings86 any thoughts on this?
</comment><comment author="colings86" created="2016-02-29T13:25:24Z" id="190210463">I managed to reproduce this on the master branch and now know why this is happening but I don't have a solution as to how we can fix it short of just documenting that you can't order by an aggregation within a nested aggregation.

The issue is that the `NestedAggregatorFactory.createInternal()` method calls `AggregatorFactory.asMultiBucketAggregator()`. This creates a wrapper around the `NestedAggregator` that will create a separate instance of `NestedAggregator` for each parent bucket. We do this in the `NestedAggregator` to ensure the doc ids are delivered in order because with a single instance and multi-valued nested fields we could get documents not in order. Some of the aggregations rely on the fact that documents are collected in order. For example, we could collect (doc1, bucket1), (doc2, bucket1), (doc1, bucket2), (doc2, bucket2) which would be out of order, so by having separate instances we are guaranteeing docId order since each instance will only collect one bucket.

I tried to change the `AggregationPath.validate()` method to use the underlying aggregator (the first instance of it at least) but then it fails later because we need to retrieve the value from the aggregator and there is no way of getting the value from a particular instance form the wrapper.
</comment><comment author="aresn" created="2016-04-03T01:40:14Z" id="204847403">I managed to somehow face this issue again. The "path" parameter in moving average can not point to a nested aggregation because nested aggregation is not a single bucket aggregation.
</comment><comment author="byronvoorbach" created="2016-04-29T13:34:17Z" id="215712571">Is there any way to get around this 'issue'?? I'm running into the same issues
</comment><comment author="jambanagar" created="2016-06-28T13:54:08Z" id="229055699">@clintongormley @colings86 Any update on this please? We are badly stuck without this...
</comment><comment author="sylcjl" created="2016-06-29T06:29:25Z" id="229268173">+1

my question on stackoverflow
http://stackoverflow.com/questions/38089711/how-can-i-sort-aggregations-buckets-by-metric-results
</comment><comment author="sawyercade" created="2016-07-06T17:28:10Z" id="230844915">+1 for this as well. Very big use case scenario for us.
</comment><comment author="kagrela" created="2016-07-12T07:57:32Z" id="231966043">+1 as this is showstoper for us to upgrade from es v1  to es v2
</comment><comment author="whizz" created="2016-07-12T08:12:59Z" id="231969158">We were bitten by the same thing. FWIW, we worked around it temporarily by ordering the aggregations in the application code after they are returned from ES.
</comment><comment author="voldern" created="2016-08-26T13:55:36Z" id="242742060">This missing feature is preventing us from upgrading to ES 2.X. Is there any plans to support this in the near future?
</comment><comment author="jambanagar" created="2016-08-26T14:03:59Z" id="242744288">@clintongormley Nested architecture is an important functionality in ES. Most companies build atleast at minimum some sort of functionality with nested mappings. This bug renders ES useless. Any updates?
</comment><comment author="calvinps" created="2016-10-06T19:12:09Z" id="252060263">+1 sorting after the fact in our application isn't a viable option due to number of results. 
</comment><comment author="idozorenko" created="2016-12-20T19:31:26Z" id="268335539">I have made a fix to sort which has nested aggregations in path. Also you might have multi-value buckets in path (you should just specify bucket key in path like "colors.red&gt;stats.variance").
I might create a pull request or just give a link to the commit in fork of ES 5.1.2 if anyone is interested. </comment><comment author="RubieV" created="2016-12-20T19:34:27Z" id="268336381">That would be great, or link in your fork?

Op di 20 dec. 2016 20:32 schreef idozorenko &lt;notifications@github.com&gt;:

&gt; I have made a fix to sort which has nested aggregations in path. Also you
&gt; might have multi-value buckets in path (you should just specify bucket key
&gt; in path like "colors.red&gt;stats.variance").
&gt; I might create a pull request or just give a link to the commit in fork of
&gt; ES 5.1.2 if anyone is interested.
&gt;
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/16838#issuecomment-268335539&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AH4yOnC_e-o0zAsgVPxT6MWYR8jdUBwNks5rKC03gaJpZM4HkeIN&gt;
&gt; .
&gt;
</comment><comment author="kagrela" created="2016-12-21T06:43:02Z" id="268449151">&gt;I might create a pull request

:+1:</comment><comment author="idozorenko" created="2016-12-21T12:46:34Z" id="268514930">As I'm not a contributor, I will just share my commit to ES 5.1 branch here. Please let me know if you have any questions.

https://github.com/elastic/elasticsearch/commit/8f601a3c241cb652a889870d93fd32b3d226ef41
</comment><comment author="clintongormley" created="2016-12-21T12:48:50Z" id="268515330">@idozorenko feel free to submit a PR so that we can review the code - thanks</comment><comment author="IdanWo" created="2017-05-06T02:04:12Z" id="299609004">Does this problem also occur in reverse_nested aggs? (not direct nested)</comment><comment author="colings86" created="2017-05-06T07:49:00Z" id="299622931">yes</comment><comment author="akashmpatel91" created="2017-05-08T11:00:35Z" id="299837273">+1 for this issue. We have exact same problem and same query is running with AWS ES 1.5. 

Can any one tell us about the status for this bug fix? This is very critical feature for us and can not move forward without this functionality? Does any one suggest to use ES 1.5 instead of 5.X version? (Personally i do not think we should do this)</comment><comment author="brettahale" created="2017-05-08T17:45:16Z" id="299938097">We didn't want to wait for a fix or to upgrade so we ended up restructuring
our data to be a parent/child relationship vs nested. So far so good.

On Mon, May 8, 2017 at 5:01 AM, akashmpatel91 &lt;notifications@github.com&gt;
wrote:

&gt; +1 for this issue. We have exact same problem and same query is running
&gt; with AWS ES 1.5.
&gt;
&gt; Can any one tell us about the status for this bug fix? This is very
&gt; critical feature for us and can not move forward without this
&gt; functionality? Does any one suggest to use ES 1.5 instead of 5.X version?
&gt; (Personally i do not think we should do this)
&gt;
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/16838#issuecomment-299837273&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/ABAejttN4mvEjco7X7BmwNBNVOOoopaQks5r3vX7gaJpZM4HkeIN&gt;
&gt; .
&gt;
</comment><comment author="akashmpatel91" created="2017-05-08T19:18:59Z" id="299963988">@brettahale, Please note that "parent-child relations can make queries hundreds of times slower" as per ES documentation https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-search-speed.html. 

We did POC and it is correct, we are handling 5-6 billions documents and query is taking 6-7 sec to return results. With nested document query is returning results in 400 ms.</comment><comment author="brettahale" created="2017-05-08T23:46:15Z" id="300021449">Agreed, wasn't ideal but we were able to deliver our feature. I'd like to
see a fix here as well but after a chat with ES support, it sounded like a
foundational change that wasn't likely going to get fixed anytime soon.

On Mon, May 8, 2017 at 1:19 PM, akashmpatel91 &lt;notifications@github.com&gt;
wrote:

&gt; @brettahale &lt;https://github.com/brettahale&gt;, Please note that
&gt; "parent-child relations can make queries hundreds of times slower" as per
&gt; ES documentation https://www.elastic.co/guide/en/elasticsearch/reference/
&gt; master/tune-for-search-speed.html.
&gt;
&gt; We did POC and it is correct, we are handling 5-6 billions documents and
&gt; query is taking 6-7 sec to return results. With nested document query is
&gt; returning results in 400 ms.
&gt;
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/16838#issuecomment-299963988&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/ABAeju3EHsROM1hWyII36f5pPpPeVK0bks5r32rQgaJpZM4HkeIN&gt;
&gt; .
&gt;
</comment><comment author="akashmpatel91" created="2017-05-18T18:43:13Z" id="302506302">Hello team, any update on this?</comment><comment author="IdanWo" created="2017-06-12T18:12:47Z" id="307872436">Our workaround was to copy some fields from the parent docs into its nested docs, so we can still make terms aggregation on the nested docs but sort by fields "found" in the parent docs. This allowed us to omit the reverse_nested aggregation between the bucket and the sub bucket. Not ideal, but works in our cases. Of course, a fix would be much appreciated.

Edit:
Apparently, there was no need for a workaround in my case.
See the comment below.</comment><comment author="IdanWo" created="2017-06-13T18:34:06Z" id="308208189">@colings86: Actually I did managed to use Terms aggregation, a sub Reverse_Nested aggregation and a sub Cardinality aggregation for ordering. Something like that:
```
{
    "aggs" : {
        "AllMovieNames" : {
            "terms" : { "field" : "MovieName" },
            "order": {
            "backToActors&gt;distinctActorsCount":"desc"
            },
           "size": 10
        },
		"aggs":
		{
			"backToActors":{		
				"reverse_nested":{},
				"aggs":{
					"distinctActorsCount":{
						"cardinality":{
							"field":"ActorName"
						}
					}
				}			
			}	
		}			
    }	
}
```
No exception message was thrown, and the order was just as expected.

Are you sure the problem occur in both Nested and Reverse_Nested sub aggregation? 
I'm using ElasticSearch 5.3.2.</comment><comment author="colings86" created="2017-06-20T10:58:33Z" id="309718632">@IdanWo sorry, actually you are right, this problem doesn't occur on the `reverse_nested` aggregation, the only single bucket aggregation it should affect is the `nested` aggregation because thats the only single bucket aggregation that uses `AggregatorFactory.asMultiBucketAggregator()`</comment><comment author="ggarciao" created="2017-07-18T14:04:04Z" id="316073796">I'm another victim of this insidious bug:

Situation:

- Document ROOT with two nested documents NESTED1 and NESTED2.
- Term aggregation over a field in ROOT.NESTED1 (nested aggregation -to NESTED1- then term aggregation)
- Sum aggregation over a field in ROOT.NESTED2 (inside the previous term aggregation, reverse nested aggregation -back to ROOT-, nested aggregation -to NESTED2- then sum aggregation)

I cannot use the sum aggregation to sort the term aggregation because an error is thrown saying that the nested aggregation -to NESTED2- does not returns a single-bucket aggregation

**Can someone update us with the status of this bug?**

(I'm using ElasticSearch 5.4)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update MaxMind geoip2 version to 2.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16837</link><project id="" key="" /><description>Update to align with #16801 jackson 2.7.1
</description><key id="136910589">16837</key><summary>Update MaxMind geoip2 version to 2.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">slachiewicz</reporter><labels><label>:Ingest</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2016-02-27T11:57:55Z</created><updated>2016-02-29T08:39:28Z</updated><resolved>2016-02-28T12:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-28T12:24:35Z" id="189856810">@slachiewicz thanks, merged it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refuse to start if logging doesn't configure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16836</link><project id="" key="" /><description>Right now if Elasticsearch fails to configure logging log4j just outputs some message and we plow ahead. We need to notice this and fail hard. We've already removed some code that allows us to move forward if we fail to configure log4j (https://github.com/elastic/elasticsearch/commit/9c259aca413f904fcc31d959ab54a6ea8432404a) but we need tests that Elasticsearch dies when the log configuration is totally broken. Without a test that we run consistently we're as likely as not to still allow some broken log configuration to make it through. This feels like a fairly strait forward thing to test in the vagrant infrastructure or in some smoke test.
</description><key id="136871655">16836</key><summary>Refuse to start if logging doesn't configure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2016-02-27T03:16:50Z</created><updated>2016-02-29T00:23:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Disable production limits on snapshot builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16835</link><project id="" key="" /><description>This commit disables the production limits checks on snapshot
builds. This is at a minimum short-term relief for developers that do in
fact bind to external network interfaces, and is possibly a long-term
fix as well. The situation with using the JVM flag MaxFDLimit is far too
complicated.

Closes #16813
</description><key id="136855665">16835</key><summary>Disable production limits on snapshot builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-27T00:49:55Z</created><updated>2016-03-10T18:35:33Z</updated><resolved>2016-02-27T00:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-27T00:52:59Z" id="189541936">LGTM
</comment><comment author="rmuir" created="2016-02-27T01:26:53Z" id="189546913">Basically this means we do not test what users run in production?
</comment><comment author="jasontedor" created="2016-02-27T01:52:01Z" id="189552151">&gt; Basically this means we do not test what users run in production?

@rmuir I plan to refactor a bit and add some tests early next week. I'll add an issue tomorrow so that this isn't lost.
</comment><comment author="rmuir" created="2016-02-27T02:22:14Z" id="189559163">Thanks @jasontedor 
</comment><comment author="jasontedor" created="2016-02-28T03:56:51Z" id="189778600">&gt; I plan to refactor a bit and add some tests early next week. I'll add an issue tomorrow so that this isn't lost.

@rmuir @s1monw I skipped straight to a pull request and opened #16844.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove log4j exception hiding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16834</link><project id="" key="" /><description>At some time in the distant past, Bootstrap could be used by those
embedding elasticsearch. However, it is no longer allowed (now package
private), and so any errors (for example, log4j missing, or any other
exception while initializing) should be exposed directly and cause
elasticsearch to fail to start. This change removes hiding of logging
initialization exceptions.
</description><key id="136837891">16834</key><summary>Remove log4j exception hiding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Core</label><label>enhancement</label><label>PITA</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T22:55:59Z</created><updated>2016-02-29T00:12:56Z</updated><resolved>2016-02-26T23:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-26T22:58:49Z" id="189517496">LGTM. Now that we don't support loggers other than log4j this is reasonably obvious.
</comment><comment author="nik9000" created="2016-02-26T22:59:47Z" id="189517953">Are you going to backport to 2.x? The log4j is non-option reasoning is less obvious there. Its probably still the right thing to do though.
</comment><comment author="rjernst" created="2016-02-26T23:00:15Z" id="189518166">@nik9000 Did we ever truly support slf4j _instead of_ log4j in a server setting? I thought slf4j was there for the client case. I do think we should backport to 2.x.
</comment><comment author="nik9000" created="2016-02-26T23:03:52Z" id="189519475">&gt; I thought slf4j was there for the client case.

in the client case they'd just use the log4j -&gt; slf4j bridge. I think it is safe to backport because its super unlikely that someone is using another logger. Much less likely than someone badly configuring logging. If they exist we can talk to them when and if they complain.
</comment><comment author="rjernst" created="2016-02-26T23:15:21Z" id="189522017">2.x commit: de508e3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhanced lat/long error handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16833</link><project id="" key="" /><description>Closes #16137.
</description><key id="136828037">16833</key><summary>Enhanced lat/long error handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbertouch</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-02-26T22:05:25Z</created><updated>2016-04-06T11:49:07Z</updated><resolved>2016-04-06T11:47:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jbertouch" created="2016-02-26T22:26:10Z" id="189508141">Thanks @jasontedor, I hadn't familiarized myself with the `ElasticsearchException` hierarchy.
</comment><comment author="jasontedor" created="2016-02-28T16:28:04Z" id="189899336">@jbertouch Can you add tests in `GeoPointFieldMapperTests#testValidateLatLonValues` that test failing cases for the new code as well? The tests that are currently there do something that I do not like which is not validate that the contents of the exception are as expected, so let's not follow that pattern (but leave them how they are). Let me know if what I mean requires clarification.
</comment><comment author="jbertouch" created="2016-03-01T21:11:19Z" id="190907174">Hi @jasontedor, I added additional test failing cases to `GeoPointFieldMapperTests#testValidateLatLonValues`, which validate the wrapped exception type as a `NumberFormatException`, and I didn't alter the existing tests.
</comment><comment author="jbertouch" created="2016-03-02T04:43:11Z" id="191058005">FWIW, I ran a quick regex over the ES test packages - these are the tests with unvalidated/empty catch clauses for exceptions if you want to review them in future:

`BulkRequestTests#testBulkAllowExplicitIndex`
`TransportReplicationActionTests#shardOperationOnReplica`
`JarHellTests#testInvalidVersions`
`AllocationCommandsTests#testAllocateCommand`
`AllocationCommandsTests#testCancelCommand`
`Base64Tests#assertInvalidBase64`
`IteratorsTests#testNull`
`IteratorsTests#testNullIterator`
`IteratorsTests#assertNoSuchElementException`
`HppcMapsTests#testIntersection`
`SimpleJodaTests#testThatRootObjectParsingIsStrict`
`SimpleJodaTests#assertDateFormatParsingThrowingException`
`CommonGramsTokenFilterFactoryTests#testDefault`
`KeepFilterFactoryTests#testLoadOverConfiguredSettings`
`KeepFilterFactoryTests#testKeepWordsPathSettings`
`ShadowEngineTests#testShadowEngineIgnoresWriteOperations`
`DynamicMappingTests#testTypeNotCreatedOnIndexFailure`
`GeoPointFieldMapperTests#testValidateLatLonValues`
`BoolQueryBuilderTests#testIllegalArguments`
`StoreTests#testCanReadOldCorruptionMarker`
`TypesExistsIT#testSimple`
`IndicesOptionsIntegrationIT#testAllMissingStrict`
`IndicesOptionsIntegrationIT#verify`
`CloseIndexDisableCloseAllIT#testCloseAllRequiresName`
`IndexStatsIT#testSimpleStats`
`DestructiveOperationsIntegrationIT#testDestructiveOperations`
`PercolatorIT#testSimple1`
`PercolatorIT#testPercolatingExistingDocs_versionCheck`
`PercolatorIT#testCountPercolation`
`SignificanceHeuristicTests#basicScoreProperties`
`SignificanceHeuristicTests#testScoreMutual`
`ChildQuerySearchIT#testParentFieldToNonExistingType`
`GeoFilterIT#testShapeBuilders`
`SearchScrollIT#testClearIllegalScrollId`
`SimpleSearchIT#testSearchNullIndex`
`SuggestSearchTests#testPhraseBoundaryCases`
`AbstractS3SnapshotRestoreTest#testRepositoryWithCustomCredentialsIsNotAccessibleByDefaultCredentials`
`AbstractS3SnapshotRestoreTest#testRepositoryInRemoteRegionIsRemote`
`BlockClusterStateProcessing#startDisrupting`
`IntermittentLongGCDisruption#BackgroundWorker#run`
`SlowClusterStateProcessing#BackgroundWorker#run`
</comment><comment author="jasontedor" created="2016-03-03T04:11:06Z" id="191572979">This is looking good; can you add one more thing? In particular, assertions on the exception message?
</comment><comment author="jbertouch" created="2016-03-07T15:11:25Z" id="193291154">Hi Jason, I added/tested the extra assertions.
</comment><comment author="jasontedor" created="2016-03-07T15:16:00Z" id="193293607">&gt; Hi Jason, I added/tested the extra assertions.

@jbertouch Thanks, I left a comment that applies to all of the assertions. Getting close. :)
</comment><comment author="jbertouch" created="2016-03-31T18:53:01Z" id="204075754">@jasontedor Done!
</comment><comment author="jasontedor" created="2016-03-31T18:54:34Z" id="204076557">&gt; Done!

Great, thank you so much! I took a quick peek and it looks good. I'll take a final closer look later and wrap this up accordingly. 
</comment><comment author="jasontedor" created="2016-04-02T15:06:24Z" id="204731176">@jbertouch The change looks great, thank you! There is one line that has to be fixed to meet the line-length limits or it will break the build on integration. There are also some lines where the indentation doesn't match our code style. I marked them in the PR review; do you mind fixing those too when you hit the line-length limit? It's fine if not, I'll handle them with an extra commit upon integration but since you have to push one more commit anyway I think you can hit them then? :smile:

Finally, do you mind if I squash all of these commits into one when I integrate this PR?

Thanks again!
</comment><comment author="jbertouch" created="2016-04-04T16:16:41Z" id="205372552">@jasontedor Love your attention to detail! Sorry I missed those formatting issues, have been road testing the Atom text editor, think I will go back to Eclipse. Please go ahead and squash the commits.
</comment><comment author="jasontedor" created="2016-04-06T11:49:07Z" id="206333000">Thank you for your hard work and contribution to Elasticsearch; it is truly appreciated! I squashed this into a single commit and pushed 3651854bf6d0b1e9fb4ad8a954f476ba9f6df367 to master. &#128516; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolation with Bounding Box GeoLocation throws NullPointerException in Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16832</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_72

**OS version**: Linux 851a91a0d5ad 4.0.7-boot2docker #1 SMP Wed Jul 15 00:01:41 UTC 2015 x86_64 GNU/Linux

Hi, we are currently running a cluster of 2.1.2 and looking to upgrade to 2.2.0, but we are having some issues when it comes to testing our code around percolators. With 2.1.2 everything works fine, but when testing against 2.2.0 we get a return of internal server error with the reason being a null pointer exception. I have tested this with two clean Docker images from the official Elasticsearch images to ensure there's no configuration issues.

The only thing I could find about this issue is an unanswered posting on Stack Overflow here, http://stackoverflow.com/questions/35451052/elastic-search-percolation-with-bounding-box-geolocation-throws-nullpointerexcep

**Steps to reproduce**:
 1 Create Percolator Index

``` bash
curl -XPOST http://192.168.99.101:32770/test -d '{"mappings": {".percolator": {"dynamic": true,"properties": {"id": {"type" : "integer"}}},"test": {"properties": {"location": {"type": "geo_point","lat_lon": true }}}}}'
```

 2 Create Percolator Query

``` bash
curl -XPUT http://192.168.99.101:32770/test/.percolator/alert-1 -d '{"query":{"filtered":{"filter":{"bool":{"must":[{"geo_bounding_box":{"location":{"top_left":[-71.09,42.36],"bottom_right":[-71.085,42.355]},"type":"indexed"}}]}}}}}'
```

 3 Percolator a Document

``` bash
curl -XGET http://192.168.99.101:32772/test/test/_percolate?percolate_format=ids&amp;percolate_index=test -d '{"doc":{"location":{"lon":-71.0875,"lat":42.3575}}}'
```

**Output from 2.1.2**

``` json
{"took":3,"_shards":{"total":5,"successful":5,"failed":0},"total":1,"matches":["alert-1"]}
```

**Output from 2.2.0**

``` json
{"took":6,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"shard":3,"index":"test","status":"INTERNAL_SERVER_ERROR","reason":{"type":"null_pointer_exception","reason":null}}]},"total":0,"matches":[]}
```

**Provide logs (if relevant)**:

```
RemoteTransportException[[Anelle][127.0.0.1:9301][indices:data/read/percolate[s]]]; nested: PercolateException[failed to percolate]; nested: PercolateException[failed to execute]; nested: NullPointerException;
Caused by: PercolateException[failed to percolate]; nested: PercolateException[failed to execute]; nested: NullPointerException;
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:180)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:268)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:264)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: PercolateException[failed to execute]; nested: NullPointerException;
        at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:583)
        at org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:254)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:177)
        ... 8 more
Caused by: java.lang.NullPointerException
        at org.apache.lucene.search.GeoPointTermQueryConstantScoreWrapper$1.getDocIDs(GeoPointTermQueryConstantScoreWrapper.java:86)
        at org.apache.lucene.search.GeoPointTermQueryConstantScoreWrapper$1.scorer(GeoPointTermQueryConstantScoreWrapper.java:126)
        at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
        at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:280)
        at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
        at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:280)
        at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
        at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:280)
        at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:628)
        at org.elasticsearch.common.lucene.Lucene.exists(Lucene.java:248)
        at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:571)
        ... 10 more
```
</description><key id="136807717">16832</key><summary>Percolation with Bounding Box GeoLocation throws NullPointerException in Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">amscotti</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2016-02-26T20:33:54Z</created><updated>2017-06-28T12:32:28Z</updated><resolved>2017-06-28T12:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-02-27T04:18:22Z" id="189574377">Thanks for the report @amscotti. A couple follow up questions:
1. did you reindex your data?
2. can you post your `geo_point` mapping?
</comment><comment author="amscotti" created="2016-02-27T21:47:24Z" id="189735033">@nknize Thanks for the reply! This is a fully new Elasticsearch setup using Docker to isolate this issue. Following the steps to reproduce we'll create a new index and attempt to percolate a document. Using 2.1.2, the percolation will succeed where in version 2.2.0 you will get a NullPointerException error from the server. Also, the issue isn't just in the Docker image, it also happens on my MacBook running OS X 10.11.3. So I don't think the issue is related to the set up.

The mapping is included in the steps to reproduce as part of the curl command, but here it is pulled out.

``` json
{
   "mappings":{
      ".percolator":{
         "dynamic":true,
         "properties":{
            "id":{
               "type":"integer"
            }
         }
      },
      "test":{
         "properties":{
            "location":{
               "type":"geo_point",
               "lat_lon":true
            }
         }
      }
   }
}
```
</comment><comment author="Schartoym" created="2016-02-28T11:01:46Z" id="189840471">Confirm same issue on Windows
**Elasticsearch version**: 2.2.0
**OS version**: Windows Server 2012
Mappings:

``` javascript
{
   "mappings":{
      ".percolator":{
         "properties": {
          "query": {
            "type": "object",
            "enabled": false
          }
        }
      },
      "test":{
         "properties":{
            "location":{
               "type":"geo_point",
            }
         }
      }
   }
}
```

Percolation Query:

``` javascript
{
  "_index": "test",
  "_type": ".percolator",
  "_id": "550858",
  "_version": 1,
  "found": true,
  "_source": {
    "query": {
      "bool": {
        "must": [          
          {
            "geo_polygon": {
              "location": {
                "points": [
                  {
                    "lat": 55.8118391906254,
                    "lon": 37.526867226289
                  },
                  {
                    "lat": 55.8081657829831,
                    "lon": 37.5251506260451
                  },
                  {
                    "lat": 55.8081326519671,
                    "lon": 37.5251464834297
                  },
                  {
                    "lat": 55.8023318307023,
                    "lon": 37.5263481363022
                  },
                  {
                    "lat": 55.8022898138136,
                    "lon": 37.5263769973219
                  },
                  {
                    "lat": 55.7981320290787,
                    "lon": 37.5316985066514
                  },
                  {
                    "lat": 55.7981018643375,
                    "lon": 37.5317651671351
                  },
                  {
                    "lat": 55.7929765434144,
                    "lon": 37.5530511410252
                  },
                  {
                    "lat": 55.792969394719,
                    "lon": 37.5531051836136
                  },
                  {
                    "lat": 55.7924858389984,
                    "lon": 37.568554672078
                  },
                  {
                    "lat": 55.792516839275,
                    "lon": 37.5686842702771
                  },
                  {
                    "lat": 55.7982298947502,
                    "lon": 37.5774733801441
                  },
                  {
                    "lat": 55.7968776363261,
                    "lon": 37.5790737397055
                  },
                  {
                    "lat": 55.7969071958654,
                    "lon": 37.5793619663257
                  },
                  {
                    "lat": 55.7994213421139,
                    "lon": 37.5803919440669
                  },
                  {
                    "lat": 55.7994746916239,
                    "lon": 37.580384677184
                  },
                  {
                    "lat": 55.8009250884466,
                    "lon": 37.5793547135494
                  },
                  {
                    "lat": 55.8009637661386,
                    "lon": 37.5793021078072
                  },
                  {
                    "lat": 55.8080215965125,
                    "lon": 37.5624792790697
                  },
                  {
                    "lat": 55.8080344294469,
                    "lon": 37.5624365225948
                  },
                  {
                    "lat": 55.8138344258045,
                    "lon": 37.5329107092988
                  },
                  {
                    "lat": 55.8138269869868,
                    "lon": 37.5327768801379
                  },
                  {
                    "lat": 55.8118937516748,
                    "lon": 37.5269403857461
                  },
                  {
                    "lat": 55.8118391906254,
                    "lon": 37.526867226289
                  }
                ]
              }
            }
          }
        ]
      }
    }
  }
}
```

Document percolation

``` javascript
GET test/test/_percolate
{
  "doc":{
    "location":{
      "lon": -71.0875,
      "lat": 42.3575
    }
  }
}
```

I've tested same query on Elastic 2.1.2 and it's works correctly
</comment><comment author="nknize" created="2016-03-04T16:46:40Z" id="192352297">So the problem is Lucene's `MemoryIndex` does not support DocValues, and the new GeoPointField requires DocValues for post filtering.  @martijnvg will open a separate issue to add DocValue support and we will reference it on this issue.
</comment><comment author="martijnvg" created="2016-03-15T08:22:34Z" id="196712971">This is the issue that adds doc values support to the MemoryIndex: https://issues.apache.org/jira/browse/LUCENE-7091
</comment><comment author="martijnvg" created="2016-03-16T13:29:00Z" id="197329289">Doc values support to the MemoryIndex has been added and will be included in the Lucene 6.0 release (which hasn't been released yet). Elasticsearch 5.0 will depend on this, so from 5.0 this issue is fixed.

I opened #17105 to backport / fork the fix in 2.x, but there are concerns with forking Memory index and we should not do that.

Luckily there is a workaround for this issue which can help anyone running into this issue. This problems mentioned in this issue only happen if the new geo query implementing are used. We still use the old geo query implementations for indices created before 2.2.0. There is an index setting (`index.version.created`) that we can use to let ES think an index was created on a cluster that ran with an older ES version. If we set it to a version &lt; 2.2.0 then ES uses the older geo query implementations (only for the index we set this created version for).

Note1: You shouldn't set this setting on already created indices before the upgrade to &gt; 2.2. Only for new percolator indices (after to upgrade to &gt; 2.2).
Note2: Only set this setting on indices that hold your percolator queries. Using this setting on all indices, would mean you can't benefit from geo improvements when running regular searches. If your percolator queries co-exist in the same index holding your data then you should move your percolator queries into a dedicated index.

I this case we can set `index.version.created` to `2010299` (version id of version 2.1.2) and `geo_bounding_box`, `geo_polygon` and other geo queries will work as they did before ES 2.2 in the percolator. Example based on the initial post:

```
curl -XPUT "http://localhost:9200/test" -d'
{
  "settings": {
    "index.version.created" : 2010299
  }, 
  "mappings": {
    ".percolator": {
      "dynamic": true,
      "properties": {
        "id": {
          "type": "integer"
        }
      }
    },
    "test": {
      "properties": {
        "location": {
          "type": "geo_point",
          "lat_lon": true
        }
      }
    }
  }
}'
```
</comment><comment author="martijnvg" created="2017-06-28T12:32:28Z" id="311645919">Closing this issue as it has been fixed from 5.0 and beyond.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Investigate removing interning in ESLoggerFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16831</link><project id="" key="" /><description>**Describe the feature**:
While reviewing #16703 we noticed that ESLoggerFactory interns the logger name and prefix. We're not quite sure why because it didn't have any comments. It probably doesn't hurt anything but it doesn't seem all that useful either. This is a note that we should look into removing that interning.
</description><key id="136802148">16831</key><summary>Investigate removing interning in ESLoggerFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2016-02-26T20:12:26Z</created><updated>2016-02-29T00:07:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade groovy dependency in lang-groovy module to version 2.4.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16830</link><project id="" key="" /><description>First step to resolve #16527.

I will open a second PR to remove `supressAccessChecks`.
</description><key id="136793364">16830</key><summary>Upgrade groovy dependency in lang-groovy module to version 2.4.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Scripting</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T19:40:04Z</created><updated>2016-02-29T00:26:41Z</updated><resolved>2016-02-27T16:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-26T19:43:09Z" id="189448718">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add start time and duration to tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16829</link><project id="" key="" /><description>Tasks now contain timestamps indicating when the tasks were created and current run time
</description><key id="136787792">16829</key><summary>Add start time and duration to tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T19:18:12Z</created><updated>2016-02-27T23:45:02Z</updated><resolved>2016-02-27T23:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-26T19:50:31Z" id="189452764">Hurray! I left some minor comments, all of which you can feel free to say "no, I like it the way it is, thank you". I think it'd tell a great story if you added "get me tasks that have been running longer than X seconds" to the task list API in this PR. It'd make it super obvious why this is important. But this PR is fine as is if you want.
</comment><comment author="nik9000" created="2016-02-26T19:50:56Z" id="189453154">To make it official, LGTM.
</comment><comment author="imotov" created="2016-02-26T21:45:57Z" id="189493283">@nik9000 I pushed changes to address your comments. What do you think?
</comment><comment author="nik9000" created="2016-02-26T22:47:39Z" id="189514945">Fair enough. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sharding based on shard size or index size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16828</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

It would be nice to be able to specify a target shard size in ES config file, and when a shard approaches that size, ES automatically create a new "shard" for that index. Our use case is a dynamic log size for our indices affected by organic traffic pattern changes and traffic shifts from one datacenter to a different one. These changes result in shards with undesired size, even up to 150 Gb, being relocated frequently, and causing instability to the entire cluster itself.  Tuning the shard size is not an straight forward process and there may not be a single solution to the equation.
</description><key id="136780075">16828</key><summary>Sharding based on shard size or index size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">sherry-ger</reporter><labels><label>:Core</label><label>feature</label><label>Meta</label></labels><created>2016-02-26T18:48:34Z</created><updated>2016-02-29T00:01:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-26T19:43:23Z" id="189448777">A few of the Elasticsearch committers have been kicking this idea around for a while. I wasn't the one who originated it but I'm online now so I'll describe it:

Right now if you have time based data like logs then we (Elastic the company, Elasticsearch committers, helpful people on discuss.elastic.co) recommend you go with an index per X time period. It makes you chose a few things:
- What time period should you use? Daily was pretty traditional its not always right.
- How many shards should these indexes have? More shards has higher write performance, sometimes lower search performance, and always puts more overhead on the cluster.
- What do I do if my time periods don't have a predictable volume? I end up with some big time periods and some small ones and that is a pain.

So we have a plan to make all of these choices simpler! The basics go like this: create an index with enough shards to get maximal write performance. Something like `(num_nodes - 1)/num_replicas` shards. Eventually some even will occur (index gets to be a certain size probably) and we'll make a new index just like the old one automatically. Then we smash the old one down to one shard. Because the number of shards changes after the triggering event you get to live in the best of both worlds with regards to bullet number 2 above. Because this is dynamic bullets number 1 and 3 shouldn't be a problem either.

Now, you may ask lots of interesting questions. Like:

&lt;dl&gt;
  &lt;dt&gt;Why just one shard?&lt;/dt&gt;
  &lt;dd&gt;It's simpler to implement one shard given routing. I suppose you could go to more shards if you were careful to pick a number that doesn't break the routing but for this use case it doesn't seem worth it. From where I sit right now it seems fine just to have more time based indexes rather than more shards on a single time based index. Better, even.&lt;/dd&gt;
  &lt;dt&gt;How do you make this invisible to the client application? What if it wants to write to this magical growing index?&lt;/dt&gt;
  &lt;dd&gt;For now I don't want to make this invisible. I think its safer not to think of this as a nifty indexing behavior that you can opt in to rather than some perfect abstraction over and index. So when these new time based indexes are created they are just indexes. You can write to them if you want to, though I imagine most folks won't want to.&lt;/dd&gt;
  &lt;dt&gt;What if you want to change the mapping on a new index? This was a way that we recommended people roll changes into their time based indexes. Things like turning on doc_values or new analysis configuration.&lt;/dt&gt;
  &lt;dd&gt;This'll have to be supported somehow. Probably by creating the new write-optimized indexes using a template. The storage-optimized single shard indexes have to have the same mapping as the write-optimized indexes or the shard merging operation would be more non-trivial than it already is.&lt;/dd&gt;
  &lt;dt&gt;What about _optimize aka _force_merge?&lt;/dt&gt;
  &lt;dd&gt;We'll probably make that an optional part of the creation of the storage-optimized indexes from the write-optimized indexes. It might even be on by default. I dunno. It is a reasonable choice if you aren't going to be writing to the storage-optimized indexes. And I think that is the normal use case.&lt;/dd&gt;
  &lt;dt&gt;What rollover rules will you support at first?&lt;/dt&gt;
  &lt;dd&gt;Certainly index size and document count. Anything else is probably phase 2.&lt;/dd&gt;
  &lt;dt&gt;Will the rollover rules be exact or approximate?&lt;dt&gt;
  &lt;dd&gt;This is a leading question! They'll be approximate because they'll be calculated asynchronously on the shard level. So both their async nature and their shard by shard nature make them far from exact.&lt;/dd&gt;
&lt;/dl&gt;

I'll add to this list/change it as I think of more things.

Does that cover it?
</comment><comment author="nik9000" created="2016-02-26T19:44:30Z" id="189449040">&gt; target shard size in ES config file

Almost certainly we'll do this as a part of the index's configuration, probably near where you'd set number_of_shards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modify path of Servlet Transport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16827</link><project id="" key="" /><description>Modify URL to point to correct Servlet Transport link.
</description><key id="136779171">16827</key><summary>Modify path of Servlet Transport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ayushsangani</reporter><labels><label>docs</label></labels><created>2016-02-26T18:43:32Z</created><updated>2016-02-28T23:59:05Z</updated><resolved>2016-02-28T23:59:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T23:59:05Z" id="189972909">thanks @ayushsangani 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[WIP] MultiField_Stats Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16826</link><project id="" key="" /><description>re: issue #16817 this is a WIP PR (for initial feedback and review) to add the first Multi-Field metric agg that computes the Pearson product-moment correlation coefficient for a given list of numeric fields.

It's a pretty simple single pass calculation that I simulated in Matlab validating against Matlab's `corr` method. This PR includes a validation of the actual aggregation results (computed across shards) against the expected value computed using a large single set (see CorrelationIT). Initial rough calculations over a set of 1M distributed observations give at least 1e-7 accuracy.

UPDATE: This aggregation is now called `multifield_stats`. It takes a list of multiple fields and computes the following statistics:
1. counts
2. means
3. variances
4. skewness
5. kurtosis
6. covariance
7. correlation
</description><key id="136776880">16826</key><summary>[WIP] MultiField_Stats Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>discuss</label><label>feature</label><label>review</label></labels><created>2016-02-26T18:33:05Z</created><updated>2016-05-13T11:43:51Z</updated><resolved>2016-05-13T11:43:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wamuir" created="2016-02-28T04:17:45Z" id="189782218">Define correlation matrix for x1...x4:
`corr = numpy.array(`
`[[1.00, .397, .214, .569],`
`[.397, 1.00, .448, .439],`
`[.214, .448, 1.00, .404],`
`[.569, .439, .404, 1.00]])`

Generate some data (10M obs):
`numpy.random.seed(seed=1)`
`mtx = numpy.random.randn(4, 10000000)`
`chol = numpy.linalg.cholesky(corr)`
`data = numpy.dot(chol, mtx)`

`numpy.set_printoptions(precision=17)`
`print(numpy.corrcoef(data))`
`[[ 1.                   0.39650687829250175  0.21417973223669709
   0.56896721641827785]`
 `[ 0.39650687829250175  1.                   0.44797611026090212
   0.43881323671321049]`
 `[ 0.21417973223669709  0.44797611026090212  1.                   0.40411226138655432]`
 `[ 0.56896721641827785  0.43881323671321049  0.40411226138655432  1.                 ]]`

numpy.matrix.transpose(data) -&gt; index as doubles -&gt; es query: "pearson_correlation":
`[[ 1.                                 ]`
`[ 0.3965068782924764  1.                                             ]`
`[ 0.21417973223672318  0.44797611026089856   1.          ]`
`[ 0.5689672164183015  0.43881323671322076  0.40411226138653933  1.        ]]`

Very good
</comment><comment author="colings86" created="2016-02-29T10:43:30Z" id="190148926">@nknize I left a couple of comments but I like it so far
</comment><comment author="wamuir" created="2016-02-29T14:32:42Z" id="190233029">Consider implementing pairwise deletion and making this the default.  For one potential gotcha w/ listwise deletion, using the example provided by @nknize in #16817, consider replacing `FOO_Corp_Stock_Value` with `FB` or `TWTR`:

``` javascript
"pearson_correlation" : {
    "correlation" : {
        "field" : [ "FOO_Corp_Stock_Value", "AAPL", "INDEXSP" ],
    }
}
```

Also consider if the ticker symbol changed for "FOO_Corp_Stock" during this period, and the tickers are indexed separately.  This throws an exception, but the best we could get back with listwise deletion is null output.  More importantly, consider if data aren't missing at random and missingness on A moderates the relationship beween B and C.

Also related, @polyfractal mentions tests for correlation in #16817:

&gt; Time-series data isn't i.i.d. (current value is usually related to last value in some fashion), so you'd likely need other pipeline aggs to first remove inter-series dependence before **testing for correlation**

If this agg cannot accept sub-aggregations (e.g., extended stats) then consider returning the appropriate stats with the agg.  I'd argue that the appropriate stats are extended stats (incl. counts, means and variance) for variables, by pair.  Combined with an option for listwise deletion, one should be able to produce a variance-covariance matrix and perform many statistical tests.

@polyfractal also mentions serial correlation.  Down the road, there probably should be a way to examine residuals if this agg is extended to test hypotheses about a population beyond the bivariate case.  But the ouput now seems sound: unless I am mistaken, the sample correlation coefficient alone (as output now) should be an unbiased estimate of the population parameter to the extent that the items are normally distributed and were measured without error; it is the standard error of this estimate (and therefore test statistics) that is impacted.  Now, this isn't to say that the coefficient won't change after partialling out one or more lags, but that might be best viewed as a specification issue on the part of the user.

For example, in the case of OLS (which would be a nice implementation, down the road), the test statistic makes distributional assumptions of the residual such that all elements along the diagonal of the residual var/cov matrix equal a constant (homogeneity) and elements off the diagonal are zero (independence).  In the case that this i.i.d. assumption is violated, beta estimates are still unbiased and consistent but variance estimates of OLS estimators are both biased and inconsistent---standard errors are not trustworthy.
</comment><comment author="nknize" created="2016-03-03T23:01:54Z" id="192008447">This is some wonderful feedback! I've had a chance to revisit this a bit and should have an updated commit in a day or so. 

&gt; Consider implementing pairwise deletion and making this the default.

Will be addressing this shortly. Though I think there's plenty of time to get this as an enhancement in another PR?

&gt; consider if the ticker symbol changed for "FOO_Corp_Stock" during this period

Here we're assuming the stock ticker is the fieldname. So if the ticker symbol changes the application will need to map that to the field name. 

&gt; consider if data aren't missing at random and missingness on A moderates the relationship beween B and C.

I think this is a biggie. I'm looking at addressing this by way of count based weights. But I'm just getting started on this part. Again, might hold off to another PR?

Based on the feedback I'm thinking of making the following changes:
1. The agg will be changed to be more than just correlation. So I'll be refactoring to `multifield_stats`. Any objection to the name? The `multifield_stats` will provide
   - `count` - per field count to (eventually) communicate missing values
   - `mean` - per field means
   - `covariance` - upper triangle matrix (to include diagonals for variance) of the variance-covariance
2. In addition to the above, a sister `extended_muitifield_stats` (not really liking the name so I'm open to suggestions) will provide the following:
   - `correlation` - upper triangle matrix (to include diagonals for autocorrelation) of the pearson product-moment correlation coefficients
   - `kurtosis` - vector describing the shape of the per-field distributions
   - `skewness` - vector describing the symmetry of the per-field distribution about the means

I've completed counts, mean, covariance, and correlation. Before I split into 2 separate aggs and add kurtosis and skewness I wanted to solicit feedback for the usefulness of the additional measurements and for having 2 separate aggregations.

Update: Might be nice to just merge this with `stats` and `extended_stats` where they can accept either single or multiple fields.
</comment><comment author="nknize" created="2016-03-05T05:31:20Z" id="192582431">Updated the PR to do the following:
- rename aggregation to `multifield_stats`
- adds `mean`, `variance`, `skewness`, `kurtosis`, `covariance`
- updates `singleFieldTest` to include testing all generated statistics

Still todo in this PR:
- add testing for missing and multi values
- incorporate @colings86 feedback re: script names
- update javadocs

Interested in feedback @wamuir, @colings86, @polyfractal  
</comment><comment author="nknize" created="2016-03-10T21:42:38Z" id="195062458">@colings86 If you get a chance can you have a look at commit 61db3e8? Make sure I didn't miss anything while trying to parse the Scripts as a Map.
</comment><comment author="nknize" created="2016-03-11T16:19:38Z" id="195436694">So the PR has gotten pretty big, but everything should be ready. This does not yet handle pairwise missing values. As discussed above that can be handled in a separate PR.

Here is an example of the output:

``` javascript
  "aggregations" : {
    "multifieldstats" : {
      "counts" : {
        "randVal1" : 100000,
        "value" : 100000,
        "randVal2" : 100000,
        "values" : 100000
      },
      "means" : {
        "randVal1" : 0.5007814114859065,
        "value" : 50000.499999999134,
        "randVal2" : 0.5004831083181183,
        "values" : 50001.999999999134
      },
      "variances" : {
        "randVal1" : 0.08361899306430848,
        "value" : 8.333416666666644E8,
        "randVal2" : 0.0832774520285396,
        "values" : 8.333416666666644E8
      },
      "skewness" : {
        "randVal1" : -0.0030206367840570187,
        "value" : 7.154127608560924E-17,
        "randVal2" : -7.692826885603065E-4,
        "values" : 7.154127608560924E-17
      },
      "kurtosis" : {
        "randVal1" : 1.8005677156780868,
        "value" : 1.799999999760016,
        "randVal2" : 1.8008100300780934,
        "values" : 1.799999999760016
      },
      "covariance" : {
        "randVal1" : {
          "value" : 58.078219752650746,
          "randVal2" : -5.485860518247862E-4,
          "values" : 58.078219752650746
        },
        "value" : {
          "randVal2" : -7.158019911939834,
          "values" : 8.333416666666662E8
        },
        "randVal2" : {
          "values" : -7.158019911939815
        }
      },
      "correlation" : {
        "randVal1" : {
          "value" : 0.006957436968003296,
          "randVal2" : -0.006573983049459838,
          "values" : 0.006957436968003296
        },
        "value" : {
          "randVal2" : -8.59246237772021E-4,
          "values" : 1.000000000000002
        },
        "randVal2" : {
          "values" : -8.592462377720186E-4
        }
      }
    }
  }
```

It might be nice (in another PR) to specify which stats the user would like returned. It may not change what is computed, but simplifies the output if the application only cares about certain fields.

/cc @polyfractal @colings86 @wamuir @markharwood 
</comment><comment author="polyfractal" created="2016-03-11T19:15:02Z" id="195504603">@nknize Left some comments, I think it's looking good!  I think my main concern is `MultiFieldStatsResults` being mutable ... that dirty flag scares me :)  It might be cleaner split into a class that accumulates and calculates the per-shard results, then converts itself into an immutable result class which is serialized / publicly consumed?  I'd request a second opinion from someone more experienced though :)

Otherwise, just general questions around handling missing values and various `NaN`'s.  Don't have a strong opinion about those, but it's something that I found chronically affecting the pipeline work we did, so wanted to raise the issues.

Future thought:  it might be nice if there was an option to control how multi-values are accumulated per-field (e.g. min/max/sum/avg, etc).  But definitely not important now, this PR is big enough :)
</comment><comment author="polyfractal" created="2016-03-14T14:38:20Z" id="196339830">Oh, also, probably needs some reference docs?
</comment><comment author="nknize" created="2016-03-15T23:46:08Z" id="197070771">Updated the PR based on feedback. I also changed the output to list the stats by field. I think this will be easier to parse than returning the output by stat name. The following is an example:
UPDATE: Providing an example based on real world data (US Census and Infant Mortality)

``` javascript
{
    "aggregations": {
        "multifieldstats": {
            "income": {
                "count": 50,
                "mean": 51985.1,
                "variance": 7.383377037755103E7,
                "skewness": 0.5595114003506483,
                "kurtosis": 2.5692365287787124,
                "covariance_correlation": {
                    "doctors": {
                        "covariance": 324930.1575510204,
                        "correlation": 0.5875210320121014
                    },
                    "mortality": {
                        "covariance": -5620.564285714285,
                        "correlation": -0.4885863807807459
                    },
                    "crime": {
                        "covariance": -79235.02857142863,
                        "correlation": -0.05021539843485891
                    },
                    "poverty": {
                        "covariance": -21093.65836734694,
                        "correlation": -0.8352655256272504
                    }
                }
            },
            "doctors": {
                "count": 50,
                "mean": 260.276,
                "variance": 4142.649208163264,
                "skewness": 1.2822859781434495,
                "kurtosis": 4.572810390406977,
                "covariance_correlation": {
                    "income": {
                        "covariance": 324930.1575510204,
                        "correlation": 0.5875210320121014
                    },
                    "mortality": {
                        "covariance": -28.15702040816327,
                        "correlation": -0.3267657853533306
                    },
                    "crime": {
                        "covariance": -1112.624979591838,
                        "correlation": -0.09413626080307078
                    },
                    "poverty": {
                        "covariance": -80.98493061224492,
                        "correlation": -0.42811979313186255
                    }
                }
            },
            "mortality": {
                "count": 50,
                "mean": 6.830000000000001,
                "variance": 1.7923469387755107,
                "skewness": 0.5540291708637753,
                "kurtosis": 2.933457707288507,
                "covariance_correlation": {
                    "income": {
                        "covariance": -5620.564285714285,
                        "correlation": -0.4885863807807459
                    },
                    "doctors": {
                        "covariance": -28.15702040816327,
                        "correlation": -0.3267657853533306
                    },
                    "crime": {
                        "covariance": 105.19959183673471,
                        "correlation": 0.42790782954941514
                    },
                    "poverty": {
                        "covariance": 2.2208571428571418,
                        "correlation": 0.5644295679341517
                    }
                }
            },
            "crime": {
                "count": 50,
                "mean": 407.48,
                "variance": 33721.39755102041,
                "skewness": 0.45739941691696784,
                "kurtosis": 2.2451622175923696,
                "covariance_correlation": {
                    "income": {
                        "covariance": -79235.02857142863,
                        "correlation": -0.05021539843485891
                    },
                    "doctors": {
                        "covariance": -1112.624979591838,
                        "correlation": -0.09413626080307078
                    },
                    "mortality": {
                        "covariance": 105.19959183673471,
                        "correlation": 0.42790782954941514
                    },
                    "poverty": {
                        "covariance": 148.45575510204083,
                        "correlation": 0.2750707603781055
                    }
                }
            },
            "poverty": {
                "count": 50,
                "mean": 12.732000000000001,
                "variance": 8.637730612244896,
                "skewness": 0.4516049811903419,
                "kurtosis": 2.8615929677997767,
                "covariance_correlation": {
                    "income": {
                        "covariance": -21093.65836734694,
                        "correlation": -0.8352655256272504
                    },
                    "doctors": {
                        "covariance": -80.98493061224492,
                        "correlation": -0.42811979313186255
                    },
                    "mortality": {
                        "covariance": 2.2208571428571418,
                        "correlation": 0.5644295679341517
                    },
                    "crime": {
                        "covariance": 148.45575510204083,
                        "correlation": 0.2750707603781055
                    }
                }
            }
        }
    }
}
```
</comment><comment author="nknize" created="2016-03-16T16:16:13Z" id="197405223">/cc @rashidkpc @spalger The output format for the `multifield_stats` aggregation is up in the air. Would love your feedback from the Kibana Viz perspective.
</comment><comment author="rashidkpc" created="2016-03-16T20:42:13Z" id="197539160">@nknize Have you seen this sort of result visualized somewhere before? I'm trying to figure out how this data would be best presented. I suspect that our existing charts wouldn't do it justice.
</comment><comment author="nknize" created="2016-03-16T21:51:08Z" id="197565343">@rashidkpc I spoke with @spalger a little bit about this at the all hands. The tabular heatmap visualization is typical for a correlation matrix. Strong positive correlation (closer to +1.0) is usually depicted as a red cell with weaker correlation (closer to 0.0) colored white and strong negative correlation (closer to -1.0) colored blue.

Here's a good example:

![network_correlations](https://cloud.githubusercontent.com/assets/830187/13829864/42daf0aa-eb97-11e5-9776-7ba4647340c3.png)
</comment><comment author="spalger" created="2016-03-16T22:12:49Z" id="197578803">Yeah, I'm fine with that structure. We'll have to transform it from object to array, but as long as the fields aren't in a meaningful order this is fine.
</comment><comment author="rashidkpc" created="2016-04-11T16:03:34Z" id="208425656">@nknize Can you add a simplified example of the current request and response format?
</comment><comment author="nknize" created="2016-04-11T16:20:06Z" id="208432676">@rashidkpc Sure thing. Here's a simple 2 variable example using the same data as above:
- poverty
- income

Request:

``` javascript
"aggs": {
    "multifieldstats": {
        "multifield_stats": {
            "field": ["poverty", "income"]
        }
     }
}
```

Response:

``` javascript
"aggregations": {
    "multifieldstats": {
        "income": {
            "count": 50,
            "mean": 51985.1,
            "variance": 7.383377037755103E7,
            "skewness": 0.5595114003506483,
            "kurtosis": 2.5692365287787124,
            "covariance_correlation": {
                "poverty": {
                    "covariance": -21093.65836734694,
                    "correlation": -0.8352655256272504
                }
            }
        },
        "poverty": {
            "count": 50,
            "mean": 12.732000000000001,
            "variance": 8.637730612244896,
            "skewness": 0.4516049811903419,
            "kurtosis": 2.8615929677997767,
            "covariance_correlation": {
                "income": {
                    "covariance": -21093.65836734694,
                    "correlation": -0.8352655256272504
                }
            }
        }
    }
}
```
</comment><comment author="nknize" created="2016-04-12T02:51:16Z" id="208681471">@clintongormley how about making this feature experimental for 5.0.0?
</comment><comment author="clintongormley" created="2016-05-13T11:43:51Z" id="219021011">Superseded by https://github.com/elastic/elasticsearch/pull/18300
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index deletes not applied when cluster UUID has changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16825</link><project id="" key="" /><description>If a node was isolated from the cluster while a delete was happening, the node will ignore the deleted operation when rejoining as we couldn't detect whether the new master genuinely deleted the indices or it is a new fresh "reset" master that was started without the old data folder. We can now be smarter and detect these reset masters and actually delete the indices on the node if its not the case of a reset master.

Note that this new protection doesn't hold if the node was shut down. In that case it's indices will still be imported as dangling indices.

Closes #11665
</description><key id="136757791">16825</key><summary>Index deletes not applied when cluster UUID has changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T17:18:27Z</created><updated>2016-03-01T14:58:25Z</updated><resolved>2016-03-01T14:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-26T17:21:21Z" id="189379773">@bleskes @dakrone @brwe feedback would be appreciated :)
</comment><comment author="bleskes" created="2016-02-29T13:52:56Z" id="190220719">I left some very minor comments. Looks good. I thought we had a test somewhere about index deletion with master change (which we should be able to enable now), but maybe I'm wrong. @brwe do you remember?
</comment><comment author="bleskes" created="2016-02-29T14:10:21Z" id="190225514">also re the pr description:

&gt; This commit fixes the issue by only reimporting indices from data nodes if and only
&gt; if the cluster UUID on the master node is different from the cluster
&gt; UUID of the previous cluster state on the data node

This inaccurate - on disk dangling indices will still be imported - for example if a node was off while the index was deleted. This change only deals with the case where a node was isolated from the cluster while a delete happened (but was up!) and where a master failure happened during a delete operation (but after the delete was already  committed).   I think we should something like:  

If a node was isloated from the cluster while a delete was happening, the node will ignore the deleted operation when rejoining as we couldn't detect whether the new master genuinely deleted the indices or is a new fresh "resetted" master that was started without the old data folder. We can now be smarter and detect these resetted masters and actually delete the indices if this is not the case. 

Note that this new protection doesn't hold if the node was shut down. In that case it's indices will still be imported as dangling indices.
</comment><comment author="brwe" created="2016-02-29T14:25:20Z" id="190229983">&gt; @brwe do you remember?

I think that is the one that @abeyad enabled here: https://github.com/elastic/elasticsearch/pull/16825/files#diff-c454159f60a0127854028ccb5502500eL1076 I know of no other one.
</comment><comment author="bleskes" created="2016-02-29T14:27:01Z" id="190230510">@brwe I missed that one line in the white space fixing. Jet lag... :( thanks!
</comment><comment author="abeyad" created="2016-02-29T16:38:49Z" id="190280992">I fixed the issues raised by @bleskes and @dakrone and all tests pass.  I also updated the PR description and will change the commit message as well once the PR is approved.
</comment><comment author="bleskes" created="2016-03-01T13:43:31Z" id="190728661">LGTM. Thanks @abeyad 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store synonym definition file in an index instead on the FS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16824</link><project id="" key="" /><description>Today, you have two ways to define a list of synonyms:
- within the index settings (mapping) which could cause a very big index metadata (so big cluster state - imagine daily indices)
- on the file system itself but this is hard to maintain then

This feature request is about storing that document within an index and be able to retrieve it when the index is created/loaded.

See also discussion on https://discuss.elastic.co/t/recommendation-for-large-synonym-file/42740/5
</description><key id="136745889">16824</key><summary>Store synonym definition file in an index instead on the FS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2016-02-26T16:30:48Z</created><updated>2016-02-28T23:53:41Z</updated><resolved>2016-02-28T23:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="frankkoornstra" created="2016-02-26T16:47:20Z" id="189366331">+1
</comment><comment author="Timvd" created="2016-02-26T16:47:59Z" id="189366496">+1
</comment><comment author="andrestc" created="2016-02-26T18:13:03Z" id="189404661">Definetly +1, we implemented something like this in our application and would be great to have this kind of feature natively.
</comment><comment author="clintongormley" created="2016-02-28T23:53:41Z" id="189972499">Closing as duplicate of https://github.com/elastic/elasticsearch/issues/5124
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>transport client document insert with version 1.5  string is splitting with space.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16823</link><project id="" key="" /><description>elastic search transport client document insert is splitting the word with space.
e.g., Sachin Tendulkar is splitted as Sachin and Tendulkar two words. When we query I am getting the first word only.

Please let me know how to avoid this?
</description><key id="136725232">16823</key><summary>transport client document insert with version 1.5  string is splitting with space.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Shashi-GS</reporter><labels /><created>2016-02-26T15:20:28Z</created><updated>2016-02-26T15:46:57Z</updated><resolved>2016-02-26T15:33:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-26T15:33:03Z" id="189324263">Please join us on our [discuss forum](https://discuss.elastic.co/) where we can properly answer any question you might have. We like to keep github reserved for bugs or feature requests rather than questions. Thanks for your cooperation!
</comment><comment author="Shashi-GS" created="2016-02-26T15:46:57Z" id="189331871">Sure, thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add tests.jarhell.check to properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16822</link><project id="" key="" /><description>Otherwise it will not be picked up when running the tests from
command line.

relates to #16174
</description><key id="136723032">16822</key><summary>add tests.jarhell.check to properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.2.1</label></labels><created>2016-02-26T15:11:02Z</created><updated>2016-03-09T10:38:31Z</updated><resolved>2016-02-29T13:39:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-26T16:09:56Z" id="189346195">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DiscoveryService and reduce guice to just Discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16821</link><project id="" key="" /><description>DiscoveryService was a bridge into the discovery universe. This is unneeded and we can just access discovery directly or do things in a different way.

One of those different way, is not having a dedicated discovery implementation for each our discovery plugins but rather reuse ZenDiscovery.

UnicastHostProviders are now classified by discovery type, removing unneeded checks on plugins.
</description><key id="136718756">16821</key><summary>Remove DiscoveryService and reduce guice to just Discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T14:53:10Z</created><updated>2016-02-29T19:57:09Z</updated><resolved>2016-02-29T19:54:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-26T14:54:38Z" id="189312322">@jasontedor / @ywelsch can you take a look?
</comment><comment author="ywelsch" created="2016-02-29T09:52:18Z" id="190129925">Left one suggestion about clusterblock handling and one question.
</comment><comment author="bleskes" created="2016-02-29T15:53:07Z" id="190266239">@javanna @ywelsch thx. responded to comments and pushed a very minor clean up commit. Can you take a look?
</comment><comment author="ywelsch" created="2016-02-29T16:31:14Z" id="190278366">LGTM. Thanks @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch logging improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16820</link><project id="" key="" /><description>Hello,

Right now if you enable Elasticsearch slow log from config - elasticsearch.yml or through api call, it will generate the logs on the data node for queries that take more then 1 second (if set like this) in ES_index_search_slowlog.log.
The problem is that it will generate these logs on the data nodes where the query hits, so you have to check each nodes to see them If you have 50 data nodes this can be quite painfully.
What we need is for the these logs to be generated at the node the request is sent to ( client node for example)  and the logs should also include the ip of the machine the call was made from.

I believe this should be implemented as it will make it much easier to debug.

**_I know you could manually collect all the logs and use nginx proxy in front to catch the ip and then concatenate all of them.  But why do all of this ? ***_
</description><key id="136718715">16820</key><summary>Elasticsearch logging improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gorjdesign</reporter><labels /><created>2016-02-26T14:53:02Z</created><updated>2016-03-08T17:30:43Z</updated><resolved>2016-02-26T16:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-26T16:06:47Z" id="189344390">Duplicates #9669
</comment><comment author="gorjdesign" created="2016-03-08T13:17:34Z" id="193781387">If you could please read my issue you will see that I ask more than in the topic you pointed me to.  I have also sent you an e-mail 10 days go and still have not received an answer ...
</comment><comment author="clintongormley" created="2016-03-08T17:30:43Z" id="193880250">@Revan007 please do not email people directly.  It is really bad form
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest: Small documentation problem for grok processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16819</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
3.0.0-SNAPSHOT

The documentation for grok processor in ingest documentation has an error in the sample:

```
{
  "description" : "...",
  "processors": [
    {
      "grok": {
        "match_field": "message",
        "match_pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}"
      }
    }
  ]
}
```

The fields match_field and match_pattern should be _field_ and _pattern_

The _Table 6. Grok Options_ has the same problem, the field names need to be changed the same way.

This is the file I am talking about:
https://github.com/elastic/elasticsearch/blob/master/docs/reference/ingest/ingest-node.asciidoc
</description><key id="136703512">16819</key><summary>Ingest: Small documentation problem for grok processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jettro</reporter><labels><label>:Ingest</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-02-26T13:54:25Z</created><updated>2016-02-26T14:41:28Z</updated><resolved>2016-02-26T14:40:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-26T14:41:11Z" id="189308314">@jettro Thanks for raising this documentation error. It is fixed now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.7 - failed to read local state, exiting... org.elasticsearch.ElasticsearchException </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16818</link><project id="" key="" /><description>*_Getting following error on running -  *_

`elasticsearch --config=/usr/local/opt/elasticsearch17/config/elasticsearch.yml`

Error - (can't use ES 2.2 because of some application constraints)
aunchctl unload ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch17.plist

```
MYMAC:etc mukesh$ launchctl load ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch17.plist
MYMAC:etc mukesh$ elasticsearch --config=/usr/local/opt/elasticsearch17/config/elasticsearch.yml
[2016-02-26 17:01:13,895][INFO ][node                     ] [Mikado] version[1.7.5], pid[30437], build[00f95f4/2016-02-02T09:55:30Z]
[2016-02-26 17:01:13,896][INFO ][node                     ] [Mikado] initializing ...
[2016-02-26 17:01:13,964][INFO ][plugins                  ] [Mikado] loaded [], sites []
[2016-02-26 17:01:14,002][INFO ][env                      ] [Mikado] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [5.2gb], net total_space [111.8gb], types [hfs]
[2016-02-26 17:01:16,050][ERROR][gateway.local.state.shards] [Mikado] failed to read local state (started shards), exiting...
org.elasticsearch.ElasticsearchException: unexpected field in shard state [index_uuid]
    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:297)
    at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.loadShardStateInfo(LocalGatewayShardsState.java:188)
    at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.loadShardsStateInfo(LocalGatewayShardsState.java:173)
    at org.elasticsearch.gateway.local.state.shards.LocalGatewayShardsState.&lt;init&gt;(LocalGatewayShardsState.java:66)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
```
</description><key id="136672949">16818</key><summary>ES 1.7 - failed to read local state, exiting... org.elasticsearch.ElasticsearchException </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mukeshkdangi</reporter><labels /><created>2016-02-26T11:36:50Z</created><updated>2016-04-01T00:26:23Z</updated><resolved>2016-02-26T15:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-26T15:14:23Z" id="189318408">The index_uuid field was added to the shard state in 2.0  (https://github.com/elastic/elasticsearch/pull/10093 ) . It seems you have installed ES 2.x and then downgrade to ES1.7. 1.7 can not read the state file made by 2.x (and many other files). This is expected and is by design. You sadly have two options - go back to 2.x or delete the data and reindex on 1.7
</comment><comment author="mukeshkdangi" created="2016-02-26T15:30:01Z" id="189323207">i have already deleted the whole ES folder and purged ES 2.x, installed 1.7 ..still same error ..any suggestion ?
</comment><comment author="bleskes" created="2016-02-29T14:13:25Z" id="190226215">@mukeshkdangi I don't know - this exception comes from reading too new state files...
</comment><comment author="edwinlunando" created="2016-03-16T05:58:56Z" id="197165676">I got this problem too. I delete the elasticsearch data directory and fix the issue.
</comment><comment author="nakamichikun" created="2016-04-01T00:25:34Z" id="204187297">Thanks @edwinlunando that's work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correlation Metric Aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16817</link><project id="" key="" /><description>This first Multi-Field metric aggregation will compute the [Pearson product-moment correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) for a given list of numeric fields. 

Example usage (correlating stock value against S&amp;P 500):

``` javascript
"pearson_correlation" : {
    "correlation" : {
        "field" : [ "FOO_Corp_Stock_Value", "AAPL", "INDEXSP" ],
    }
}
```

Example result:

The result provides just the upper triangle (excluding the auto-correlated diagonal which would just be 1.0)

``` javascript
"pearson_correlation" : {
    "correlation" : {
        "FOO_Corp_Stock_Value" : {
            "AAPL" : 0.015411381889249296,
            "INDEXSP" : 0.028740529828528586
        },
        "AAPL" : {
            "INDEXSP" : 0.429228885527728843
        }
    }
}
```

This hypothetical example shows a small positive correlation (0.0154) between Apple and FOO Corp. stock value. A small positive correlation (0.0287) between FOO Corp. stock and the S&amp;P 500, and a strong (fake) positive correlation (0.4292) between Apple Stock and the S&amp;P 500.

While a made up example with contrived data, this correlation provides a useful statistical tool for drawing correlations between all kinds of data (medical, finance, log trends) and provides a foundation for much more advanced statistics (e.g., spatial statistics and econometrics).
</description><key id="136601947">16817</key><summary>Correlation Metric Aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2016-02-26T05:33:23Z</created><updated>2016-06-23T12:58:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wamuir" created="2016-02-26T14:31:54Z" id="189304643">Measures of covariation would be useful.  However, a more general solution would be to estimate a variance/covariance matrix for two or more fields.
</comment><comment author="wamuir" created="2016-02-26T15:14:20Z" id="189318397">Consider also when the number of fields &gt;= 3 offering an option to select pairwise or listwise deletion of missing values.   For pairwise, this would complicate the output as the query should return unique counts for each element of the matrix (if the data contain missing values).
</comment><comment author="nknize" created="2016-02-26T16:09:49Z" id="189346131">@wamuir awesome getting this feedback!

&gt; a more general solution would be to estimate a variance/covariance matrix

This got me thinking. For a more general purpose agg, perhaps this should be renamed `multi_stats` (in the same manner that we have `stats` and `extended_stats`)? `multi_stats` would give us the ability to compute and output a handful of multi-field statistics? e.g., to start it would provide covariance and pearson correlation. What do you think? /cc @polyfractal @colings86 

&gt; offering an option to select pairwise or listwise deletion of missing values.

To keep things simple the first implementation (PR shortly) is listwise omission of documents containing missing values. We need to think a bit more about how to handle missing values in future enhancements. EM would be nice, but I don't think it will be able to scale unless there is a single pass approach? Pairwise is easy in the implementation but (as you pointed out) the output is complicated. Would love thoughts/examples for an efficient output using pairwise deletion, I could get that in a follow up PR.
</comment><comment author="nknize" created="2016-02-26T16:10:51Z" id="189346955">Also would love @brwe thoughts
</comment><comment author="markharwood" created="2016-02-26T16:37:26Z" id="189362633">This feels like it should be a pipeline agg? The examples given are stocks but presumably the variables that go into this are:
1) Set keys (in this case FOO_Corp_Stock_Value:["INDEXSP", "AAPL"] )
2) Value source field (Presumably avg/max/last value for field STOCK_PRICE)
3) Sequence grouping (presumably a time based group e.g. YYYYYMMDD or YYYYMMDDHH)

Each of these variables are a config option but given the distributed nature of the data and our single-pass approach to aggs we couldn't leave 1) as an open-ended set (e.g. compare ALL stock prices for correlations) as we can't trim candidates locally on each shard to only the most-correlated stocks - we don't have all the data in one shard to answer this question. For this reason we would have to insist on a small number of set keys.
</comment><comment author="markharwood" created="2016-02-26T17:06:57Z" id="189374883">For reference, I used Pearson's correlation on www.hivemindmap.com to time-correlate Twitter hashtags.
This can be used to auto-detect events which are shown on a calendar:

![hivemindmap](https://cloud.githubusercontent.com/assets/170925/13359085/42adb960-dcaa-11e5-817b-23350457a56e.jpg)

This calendar actually combines elements of anomaly detection, graph and correlation to provide this automated "event detection":

1) Anomaly detection identifies the hashtags that "spiked" in time
2) Graph analytics identified strongly co-occurring hashtags in tweets and groups them using community detection algos (this gives the choice of colour in the diagram).
3) Pearson correlation identifies those tags in the above that shared a similar popularity timeline.

Together these spot an "event" and the related concepts.
</comment><comment author="polyfractal" created="2016-02-26T17:51:34Z" id="189392073">If your data is "well distributed" and i.i.d. I think a single-pass over the data would give a good estimate of correlation (I think?).  If we assume that each shard has a reasonable sampling of your data, we can pretend each shard can be correlated in isolation and merge the results into a pseudo-approximate answer.

It's effectively taking the same set of tradeoffs that search makes.  We can't search everything together, so just pretend the data is distributed the same across all shards and run the calculations in isolation.  If your data is distributed wildly differently between shards, all bets are off (but they are with search ranking too).

That said, I was thinking about this yesterday and we'd probably need a pipeline version too.  Time-series data _isn't_ i.i.d. (current value is usually related to last value in some fashion), so you'd likely need other pipeline aggs to first remove inter-series dependence before testing for correlation.  Didn't occur to me until I started playing around with TS correlations yesterday =/
</comment><comment author="nknize" created="2016-02-26T18:35:20Z" id="189415844">&gt; That said, I was thinking about this yesterday and we'd probably need a pipeline version too.

I agree, a pipeline version would be wonderful. This is a "progress not perfection" single pass approach that can serve as a utility for initial multi valued stats aggregations.

For review, comments, suggestions, the PR is posted at #16826.  I also had the version mislabeled. The aggregation refactor in 5.0 is needed for the multi ValueSource. 
</comment><comment author="clintongormley" created="2016-06-23T12:58:40Z" id="228041782">Closed by https://github.com/elastic/elasticsearch/pull/18300
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modified ExampleConfiguration to use Settings instead of parsing yaml file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16816</link><project id="" key="" /><description /><key id="136581667">16816</key><summary>Modified ExampleConfiguration to use Settings instead of parsing yaml file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sarwarbhuiyan</reporter><labels><label>:Plugins</label><label>non-issue</label><label>v5.0.0-beta1</label></labels><created>2016-02-26T03:03:19Z</created><updated>2016-08-26T12:22:31Z</updated><resolved>2016-08-25T23:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:49:43Z" id="206461256">@sarwarbhuiyan LGTM, maybe we want to add an `assert` in there that the settings were loaded correctly just so if they aren't loaded in tests it'll fail?
</comment><comment author="sarwarbhuiyan" created="2016-04-12T13:15:32Z" id="208899709">@dakrone in the source code itself or did you mean write a unit test?
</comment><comment author="dakrone" created="2016-04-12T15:33:00Z" id="208963844">@sarwarbhuiyan I meant in the code itself, in our tests I believe we run things with `-ea` so it's mostly just sanity for us
</comment><comment author="clintongormley" created="2016-05-07T14:40:08Z" id="217641685">@dakrone please could you review again
</comment><comment author="dakrone" created="2016-05-09T20:58:40Z" id="217987738">@sarwarbhuiyan I think the plugins/jvm-example/generated-resources/plugin-descriptor.properties file got accidentally added to this? I don't think it's required for this?
</comment><comment author="rjernst" created="2016-05-15T03:39:33Z" id="219264461">&gt; I think the plugins/jvm-example/generated-resources/plugin-descriptor.properties file got accidentally added to this? 

This seems like it would have been caused by updating to latest master from an old checkout, but not regenerating the IDE. We no longer use a generated-resources directory (we know have IDE specific dirs for builds). @dakrone is correct that it should be removed from this change.
</comment><comment author="dakrone" created="2016-06-13T21:23:48Z" id="225713088">@sarwarbhuiyan this no longer compiles, can you fix the compilation error and ensure that `gradle check` passes before it's merged?
</comment><comment author="dakrone" created="2016-08-25T21:16:21Z" id="242543158">LGTM, feel free to squash and merge
</comment><comment author="sarwarbhuiyan" created="2016-08-25T23:52:12Z" id="242582033">Fixed by b0ceecc3ebe2d5d2a4e2abc94a89785ac6ca2d6a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportNodesAction: Getting stats from all nodes via client node does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16815</link><project id="" key="" /><description>So, I am not sure if this is a feature or a bug, but we may want to discuss this. if you try to get node stats from a client node and other client nodes are part of the cluster, then the stats will only contain stats from this client node and all master and data nodes.

The reason for this is in `TransportNodesAction`, where we check, if the code where this is executed on is a client node as well as the receiving node, see [here](https://github.com/elastic/elasticsearch/blob/99a7d8e41f4a0dfa14f64d9b9d080626c558c5e3/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java#L161-L165)

I understand that this is intentional in the TransportNodesAction, but there might be actions, where we actually want to allow this (clearing caches i.e.)
</description><key id="136573924">16815</key><summary>TransportNodesAction: Getting stats from all nodes via client node does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Stats</label><label>bug</label></labels><created>2016-02-26T02:08:29Z</created><updated>2016-03-02T22:05:25Z</updated><resolved>2016-03-02T22:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-27T12:23:00Z" id="189627895">thanks for opening this @spinscale I had the same on my list to open. I stumbled upon it while working on #16565, cluster stats node counts are off due to this depending on which node the request hit, in case you have client nodes in your cluster.  Also related to #3617 and most likely affects other apis too.

Note also that this will happen only if `node.client` is set to true, not in case `node.master` and `node.data` are both set to false, which I am fixing as part of my upcoming PR for #16565...so while removing the `node.client` setting the question is: should the `shouldConnectTo` method be adapted or go away?
</comment><comment author="bleskes" created="2016-02-29T12:37:18Z" id="190193775">&gt;  should the shouldConnectTo method be adapted or go away?

Imo this check stems from seeing Client nodes as a client and expecting many of them (possibly hundreds ). This meant that not connecting them will save something significant. It was OK to do because for all _data_ related requests we will never send something from one client node to another. The downside is of course that things like Nodes  Stats and nodes info do not report them.  Since we are moving away from client nodes  we should just keep things simple and connect every node to every node.
</comment><comment author="clintongormley" created="2016-03-01T13:09:52Z" id="190717087">Related to https://github.com/elastic/elasticsearch/issues/16105
</comment><comment author="javanna" created="2016-03-01T18:25:45Z" id="190842194">Did some digging around this, this seems to affect the following apis:
- nodes info (missing client nodes)
- nodes stats (missing client nodes)
- nodes hot threads (missing client nodes)
- cluster stats (nodes count off, both total and client count)
- _cat/nodes (missing info and stats for clients nodes, but the nodes are listed)
- _cat/nodeattrs (missing process id for clients nodes)
- _cat/threadpool (missing threadpool info and stats for clients nodes, but the nodes are listed)
- _cat/plugins (throws NPE)

The issue manifests if you have two or more clients nodes in your cluster and you call any of the above api hitting directly one of the clients nodes. When the client node acts as a coordinating node it cannot connect to other client nodes hence the other client nodes are skipped.
</comment><comment author="javanna" created="2016-03-02T22:05:25Z" id="191454275">Closed via #16898.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to deserialize exception response from stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16814</link><project id="" key="" /><description>*_Getting following error - *_

```
failed to get node info for [#transport#-1][MY-Lap.local][inet[localhost/127.0.0.1:9300]], disconnecting...
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
...................
and     ... 23 more
Exception in thread "main" org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []
```

**I'm getting this error while running this command .**

  `java -Dfile.encoding=utf-8 -jar foxtrot-server/target/foxtrot-server-0.1.jar initialize config/local.yml
`
*_Here is my local.yml - *_

&gt; http:
&gt;   port: 17000
&gt;   adminPort: 17001
&gt;   connectorType: nonblocking
&gt; 
&gt; elasticsearch:
&gt;   hosts:
&gt; - localhost
&gt;   cluster: foxtrot
&gt; 
&gt; hbase:
&gt;   secure : false
&gt;   tableName: foxtrot
&gt; 
&gt; cluster:
&gt;   name: foxtrot
&gt;   disableMulticast: true
&gt;   members: ["localhost:9201", "localhost:9200"]
&gt; 
&gt; logging:
&gt;   level: ERROR
&gt; 
&gt; deletionconfig:
&gt;   active: true
&gt;   interval: 86400
&gt;   initialdelay: 60

*_and http://localhost:9200/_nodes/jvm?pretty output - *_ all versions are same

&gt; {
&gt; cluster_name: "elasticsearch_mukesh",
&gt; nodes: {
&gt; CySFQu8vRg2_FRcohq_csA: {
&gt; name: "Scarlet Spiders",
&gt; transport_address: "127.0.0.1:9300",
&gt; host: "127.0.0.1",
&gt; ip: "127.0.0.1",
&gt; version: "2.2.0",
&gt; build: "8ff36d1",
&gt; http_address: "127.0.0.1:9200",
&gt; jvm: {
&gt; pid: 98584,
&gt; version: "1.8.0_60",
&gt; vm_name: "Java HotSpot(TM) 64-Bit Server VM",
&gt; vm_version: "25.60-b23",
&gt; vm_vendor: "Oracle Corporation",
&gt; start_time_in_millis: 1456407783249,
&gt; mem: {
&gt; heap_init_in_bytes: 268435456,
&gt; heap_max_in_bytes: 1038876672,
&gt; non_heap_init_in_bytes: 2555904,
&gt; non_heap_max_in_bytes: 0,
&gt; direct_max_in_bytes: 1038876672
&gt; },
&gt; gc_collectors: [
&gt; "ParNew",
&gt; "ConcurrentMarkSweep"
&gt; ],
&gt; memory_pools: [
&gt; "Code Cache",
&gt; "Metaspace",
&gt; "Compressed Class Space",
&gt; "Par Eden Space",
&gt; "Par Survivor Space",
&gt; "CMS Old Gen"
&gt; ],
&gt; using_compressed_ordinary_object_pointers: "true"
&gt; }
&gt; },
&gt; 5Wl3g25MStKUGt8ECnDLVg: {
&gt; name: "Quasar",
&gt; transport_address: "127.0.0.1:9301",
&gt; host: "127.0.0.1",
&gt; ip: "127.0.0.1",
&gt; version: "2.2.0",
&gt; build: "8ff36d1",
&gt; http_address: "127.0.0.1:9201",
&gt; jvm: {
&gt; pid: 2525,
&gt; version: "1.8.0_60",
&gt; vm_name: "Java HotSpot(TM) 64-Bit Server VM",
&gt; vm_version: "25.60-b23",
&gt; vm_vendor: "Oracle Corporation",
&gt; start_time_in_millis: 1456427039800,
&gt; mem: {
&gt; heap_init_in_bytes: 268435456,
&gt; heap_max_in_bytes: 1038876672,
&gt; non_heap_init_in_bytes: 2555904,
&gt; non_heap_max_in_bytes: 0,
&gt; direct_max_in_bytes: 1038876672
&gt; },
&gt; gc_collectors: [
&gt; "ParNew",
&gt; "ConcurrentMarkSweep"
&gt; ],
&gt; memory_pools: [
&gt; "Code Cache",
&gt; "Metaspace",
&gt; "Compressed Class Space",
&gt; "Par Eden Space",
&gt; "Par Survivor Space",
&gt; "CMS Old Gen"
&gt; ],
&gt; using_compressed_ordinary_object_pointers: "true"
&gt; }
&gt; }
&gt; }
&gt; }

any help please ?
</description><key id="136573439">16814</key><summary>Failed to deserialize exception response from stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mukeshkdangi</reporter><labels /><created>2016-02-26T02:04:29Z</created><updated>2016-02-26T03:49:52Z</updated><resolved>2016-02-26T02:08:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-26T02:08:01Z" id="189078441">I'm sorry, but we do not support foxtrot; you might have better luck on [their repository](https://github.com/Flipkart/foxtrot).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>max file descriptor issues on OS X</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16813</link><project id="" key="" /><description>With the recent change in #16733 we introduced the notion of running in a production mode, which is detected by inspecting whether `network.host` is set. On OS X this is problematic with the Oracle JDK, since it [limits the java process](https://developer.apple.com/library/mac/documentation/Java/Reference/Java_VMOptionsRef/Articles/JavaVirtualMachineOptions.html) to `10240` file descriptors by default unless the `-XX:-MaxFDLimit` VM option is passed. This limit will cause elasticsearch to fail to start (see below).

The [JDK 7 documentation](http://www.oracle.com/technetwork/articles/java/vmoptions-jsp-140102.html) states that this option is only relevant to solaris, but according to this [issue](https://bugs.openjdk.java.net/browse/JDK-8010126) it is relevant to all platforms but the documentation won't be updated. The JDK8 documentation does not list this option anymore even though it is still in use.

Some output from a terminal:

```
$ java -version
java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
$ ulimit -n
65536
$ launchctl limit
    cpu         unlimited      unlimited      
    filesize    unlimited      unlimited      
    data        unlimited      unlimited      
    stack       8388608        67104768       
    core        0              unlimited      
    rss         unlimited      unlimited      
    memlock     unlimited      unlimited      
    maxproc     2048           2048           
    maxfiles    65536          65536  
$ bin/elasticsearch
Exception in thread "main" java.lang.IllegalStateException: max file descriptors [10240] for elasticsearch process likely too low, increase it to at least [65536]
    at org.elasticsearch.bootstrap.Bootstrap.enforceOrLogLimits(Bootstrap.java:401)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:192)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:283)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:37)
Refer to the log for complete error details.
$ export ES_JAVA_OPTS=-XX:-MaxFDLimit
$ bin/elasticsearch
[2016-02-25 15:00:33,638][INFO ][node                     ] [Hector] version[3.0.0], pid[30941], build[c9c4cac/2016-02-25T13:06:48.503Z]
[2016-02-25 15:00:33,638][INFO ][node                     ] [Hector] initializing ...
[2016-02-25 15:00:33,988][INFO ][plugins                  ] [Hector] modules [lang-mustache, lang-painless, ingest-grok, lang-expression, lang-groovy], plugins []
[2016-02-25 15:00:34,008][INFO ][env                      ] [Hector] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [123.2gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-02-25 15:00:34,008][INFO ][env                      ] [Hector] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-02-25 15:00:35,338][INFO ][node                     ] [Hector] initialized
[2016-02-25 15:00:35,338][INFO ][node                     ] [Hector] starting ...
[2016-02-25 15:00:35,414][INFO ][transport                ] [Hector] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2016-02-25 15:00:35,420][INFO ][discovery                ] [Hector] elasticsearch/pqUOEZAZQtGy_Sbax5uLIg
[2016-02-25 15:00:38,453][INFO ][cluster.service          ] [Hector] new_master {Hector}{pqUOEZAZQtGy_Sbax5uLIg}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-02-25 15:00:38,473][INFO ][http                     ] [Hector] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
[2016-02-25 15:00:38,474][INFO ][node                     ] [Hector] started
[2016-02-25 15:00:38,492][INFO ][gateway                  ] [Hector] recovered [0] indices into cluster_state
```

In the above scenario, the only change to elasticsearch.yml is setting `network.host` to `localhost` and I am running a build from master.

We may want to consider adding this option to the `elasticsearch` script or documenting it.
</description><key id="136495131">16813</key><summary>max file descriptor issues on OS X</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>discuss</label></labels><created>2016-02-25T20:04:24Z</created><updated>2016-02-27T14:36:19Z</updated><resolved>2016-02-27T00:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-27T00:50:05Z" id="189541620">This setting is weird. Note that it's enabled by default and the documentation says

&gt; Bump the number of file descriptors to max.

So why do we want to _disable_ it to increase the number of file descriptors past 10240 on OS X?

I dove into the OpenJDK code to understand this flag and how it interacts with each of the major operating systems. When this flag is set, all of them basically delegate to `getrlimit` with the resource `RLIMIT_NOFILE` and then the JVM tries to set the soft limit to the hard limit (it silently ignores failure). The one exception to this is OS X which takes the minimum of `OPEN_MAX` and the soft limit as the new soft limit.

The reason for the exception on OS X is due to this from `man setrlimit`:

&gt; `setrlimit() now returns with errno set to EINVAL in places that historically succeeded.  It no longer accepts "rlim_cur = RLIM_INFINITY" for RLIM_NOFILE.  Use "rlim_cur = min(OPEN_MAX, rlim_max)".`

The constant `OPEN_MAX` is defined as:

`#define OPEN_MAX                10240   /* max open files per process - todo, make a config option? */`

in `/usr/include/sys/syslimits.h`. A todo. :disappointed: This explains the 10240 number that we are seeing in the output.

When the flag is disabled, the number of file descriptors is equal to the soft limit. This is why if the soft limit is increased on OS X, then the 10240 limit can be avoided. Sneaky and rather counterintuitive.

The situation with OS X gets weird though. If you look at `int fdalloc(proc_t p, int want, int *result)` in [`bsd/kern/kern_descrip.c`](http://www.opensource.apple.com/source/xnu/xnu-3248.20.55/bsd/kern/kern_descrip.c) there is this code:

```
    lim = min((int)p-&gt;p_rlimit[RLIMIT_NOFILE].rlim_cur, maxfiles);
```

The limit is the minimum of the soft limit and `maxfiles`. What is `maxfiles`? It's a global variable defined in [`bsd/conf/param.c`](http://www.opensource.apple.com/source/xnu/xnu-3248.20.55/bsd/conf/param.c):

```
#define MAXFILES (OPEN_MAX + 2048)
int maxfiles = MAXFILES;
```

Wait, so is the max files still only 12288?

I ran the dtrace 

```
$ dtrace -n 'BEGIN { trace(`maxfiles); exit(0); }&#8217;
```

which gives 

```
dtrace: description 'BEGIN ' matched 1 probe
CPU     ID                    FUNCTION:NAME
  0      1                           :BEGIN         12288
```

This then leads us to `kern.maxfiles` and `kern.maxfilesperproc` which need to be increased via `sysctl` followed by a reboot. 

Of course, none of this makes sense on Windows (insert joke about how Windows is not a major operating system) where `getrlimit` doesn't even exist. On Windows, `MaxFDLimit` has no effect (the flag is recognized at start up, but has no impact on the runtime behavior of the JVM).

I note that there is a comment on the OpenJDK bug that @jaymode linked to that this this flag is going to be deprecated, but it still appears in the JDK9 sources, so I'm skeptical of that at this time.

My conclusion for all of this is that this is _way_ too complicated. I think we should take a different route than working around the limit via this JVM flag and all these others dances on OS X. Instead, I think that we should just disable the flag if the build is a snapshot build.

I opened #16835.
</comment><comment author="jasontedor" created="2016-02-27T00:56:06Z" id="189542261">Closed by #16835
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unexpected `array_index_out_of_bounds_exception`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16812</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 8.0.???

**OS version**: CentOs

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:

```
PUT t/t/_bulk
{"index":{}}
{"a":"x","b":"m","v":2}
{"index":{}}
{"a":"x","b":"n","v":3}
{"index":{}}
{"a":"x","b":null,"v":5}
{"index":{}}
{"a":"y","b":"m","v":7}
{"index":{}}
{"a":"y","b":"n","v":11}
{"index":{}}
{"a":"y","b":null,"v":13}
{"index":{}}
{"a":null,"b":"m","v":17}
{"index":{}}
{"a":null,"b":"n","v":19}
{"index":{}}
{"a":"x","b":"m","v":27}
{"index":{}}
{"a":"y","b":"n","v":39}

GET t/t/_search
{
  "aggs": {
    "_match": {
      "aggs": {
        "_missing": {
          "aggs": {
            "v": {
              "extended_stats": {
                "field": "v"
              }
            }
          },
          "missing": {
            "field": "a"
          }
        }
      },
      "terms": {
        "field": "b",
        "size": 10
      }
    }
  },
  "size": 0
}
```

Get an error (and some results)

```
{
    "took":1,
    "timed_out":false,
    "_shards":{
        "total":3,
        "successful":1,
        "failed":2,
        "failures":[{
            "shard":0,
            "index":"testing_e18cf562b420160225_183510",
            "node":"QlVbQTlvT5O_fe9x4LxvWQ",
            "reason":{
                "type":"array_index_out_of_bounds_exception",
                "reason":null
            }
        }]
    },  
    // results clipped
```

The `stats` aggregate does not have this problem.
</description><key id="136474519">16812</key><summary>Unexpected `array_index_out_of_bounds_exception`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klahnakoski</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-02-25T18:44:35Z</created><updated>2016-03-09T10:38:11Z</updated><resolved>2016-03-07T09:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="scrotty" created="2016-03-03T16:07:11Z" id="191828942">Our team has run into this issue too (ES 2.2, RedHat). In one of the failure modes the entire ES cluster operation is impaired and requires the cluster be fully stopped and restarted to recover. The workaround on our end has been to drop the batch size dramatically (to 25!).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check test naming conventions on all modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16811</link><project id="" key="" /><description>The big win here is catching tests that are incorrectly named and will
be skipped by gradle, providing a false sense of security.

The whole thing takes about 10 seconds on my Macbook Air, not counting
compiling the test classes, which seems worth it. Because this runs as
a gradle task with propery UP-TO-DATE handling it can be skipped if the
tests haven't been changed which should save some time.

I chose to keep this in test:framework rather than a new subproject of
buildSrc because ESIntegTestCase and doesn't inroduce any additional
dependencies.
</description><key id="136467044">16811</key><summary>Check test naming conventions on all modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>build</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-25T18:13:29Z</created><updated>2016-02-29T21:42:17Z</updated><resolved>2016-02-29T21:41:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-25T23:08:25Z" id="189025492">OK! I've worked some to clean up the gradle side. It still has some bits that confuse me - namely why I need afterEvaluate AND doFirst to setup the arguments. I know I need one of the two so the classpath is ready, but I haven't a clue why gradle fails without both.
</comment><comment author="nik9000" created="2016-02-29T20:37:47Z" id="190376674">@rjernst can you have another look?
</comment><comment author="rjernst" created="2016-02-29T20:58:36Z" id="190386839">Looks good
</comment><comment author="nik9000" created="2016-02-29T21:42:17Z" id="190408626">Hurray! Thanks for the review @rjernst ! I've merged it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointer Exception when starting embedded elastic </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16810</link><project id="" key="" /><description>**Elasticsearch version**: 2.2

**JVM version**: 1.7

**OS version**:Win10

**Description of the problem including expected versus actual behavior**:

I use embedded (servlet) elastic via transport wares when moving to 2.2 I started getting exception on startup. they do not preven elastic from functioning but I wanted to let you know anyways.
I have just one embedded node

I have 0 replicas and I have 

`discovery.zen.ping.multicast.enabled: false`

it could be something specific to my workstation because explicetly putting my IPv4 address instead default loopback addresses into `discovery.zen.ping.unicast.hosts` fixes it but I decided to report NPE anyway so you can handle it better perhaps.

``` text
INFO  02/25/16 13:02:36 elasticsearch.transport - [es_alexr_ems_n1] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
INFO  02/25/16 13:02:36 elasticsearch.discovery - [es_alexr_ems_n1] es_alexr_ems/EcvRVvceQea3Evecm7O5Fg
WARN  02/24/16 22:11:24 transport.netty - [es_alexr_ems_n1] exception caught on transport layer [[id: 0x710c9066, /0:0:0:0:0:0:0:1:59963 =&gt; /0:0:0:0:0:0:0:1:9301]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
WARN  02/24/16 22:11:24 transport.netty - [es_alexr_ems_n1] exception caught on transport layer [[id: 0xa927d869, /127.0.0.1:59967 =&gt; /127.0.0.1:9301]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)

```
</description><key id="136460914">16810</key><summary>NullPointer Exception when starting embedded elastic </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Discovery</label></labels><created>2016-02-25T17:49:09Z</created><updated>2016-02-29T18:22:11Z</updated><resolved>2016-02-29T18:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-02-25T18:19:44Z" id="188917063">@roytmana you're likely interested in https://github.com/elastic/elasticsearch/issues/12721 .  The transport-wares plugin is aged and we're not seeing a lot of real usage of it out in the community
</comment><comment author="roytmana" created="2016-02-25T18:25:22Z" id="188918659">@eskibars Yeah I seen it and I am not happy it is being discontinued as there are enough people using it. I had to make some changes in the code for it to work with 2.2

I do not think the is related to transport-wares I am pretty sure I will get the same if running standalone ES. I think it is some connectivity issue on my workstation that's causing it and wanted the development team to see the exception because NPE is never a good thing
</comment><comment author="jasontedor" created="2016-02-25T19:20:16Z" id="188938679">@roytmana Can you provide a minimal reproduction _without_ transport wares? I am not able to reproduce on master nor 2.2 currently (I successfully started an embedded node with TRACE logging turned on and there were not any NPEs in the logs).
</comment><comment author="roytmana" created="2016-02-25T21:08:49Z" id="188989994">@jasontedor it does not need any special reproduction. I just unzipped standard elastic distro and it started it with the same exceptions. I think it is something specific to my workstation when connecting to from localhost to localhost (maybe from ipv6 to ipv4) but in any case elastich should not fail with NPE but handle it better. Attached is log with TRACE level 

[elasticsearch.log.txt](https://github.com/elastic/elasticsearch/files/147275/elasticsearch.log.txt)
</comment><comment author="jasontedor" created="2016-02-25T21:14:23Z" id="188991572">@roytmana Can you give any more details about your system and config? I'm also running IPv4 and IPv6 and I do not see this issue. While avoiding the NPE is easy here, I want to understand the root cause before addressing.
</comment><comment author="roytmana" created="2016-02-25T21:21:14Z" id="188993916">I wish I could - I am not a networking guru but I think it is the same
issue I had with some development tool that start jetty on localhost and
then tries to connect to it. It fails. I had to force to use my real ipv4
address to work. I think it started couple of months ago with one windows
routine updates and I think my co-worked is having the same issue with that
tool after that update. It is only loopback connections that seem to be
failing. If you can tell me what to check I will gladly do. I have already
tried   JDK 7, 8, 32 and 64 bit - no difference I tried to turn ipv6 off
but it does not affect loopback it still resolves to ::1 and did not fix
the problem ...
</comment><comment author="roytmana" created="2016-02-25T22:10:28Z" id="189007551">here seems to be similar problem http://superuser.com/questions/962293/cant-connect-to-socket-listening-on-127-0-0-1-but-works-for-localhost-in-wind but no solution
</comment><comment author="eskibars" created="2016-02-25T23:16:29Z" id="189030232">@roytmana just FYI, I just tried it on my win10 machine as well (with all the latest MS updates and both ipv6/ipv4) and am not getting this.  Can you give some more detail about your networking setup?
</comment><comment author="roytmana" created="2016-02-25T23:39:26Z" id="189036367">@eskibars it could be the antivirus/local firewall. but I stopped it it did not help. I spent half a couple of months ago trying to figure why service listening on loopback can't be connected to on my workstation (the tool I was using could not be configured on which address to listen and to which to connect) but could not figure it out. Hopefully it is not a widespread issue since your workstation worked fine and my notebook too.

I could send you my ipconfig and anything else you may need but I would prefer to do it privately
please let me know what else besides ipconfig /all you would want and where to send it

For my purposes I just set my node to local mode since it will be a single node embedded deployment and the issue went away of course 
</comment><comment author="jasontedor" created="2016-02-26T07:27:24Z" id="189146373">I also attempted to reproduce this on a Windows 10 system with the latest updates and it did not reproduce for me. Again, it's really easy to make this NPE go away (without even understanding where it's coming from) but I really want to understand where it's coming from in case it's indicative of a bigger issue and making the NPE go away would just be papering over it.
</comment><comment author="roytmana" created="2016-02-26T14:12:23Z" id="189295790">Once again let me know what info to collect and where to send it
</comment><comment author="jasontedor" created="2016-02-26T20:58:19Z" id="189481087">&gt; Once again let me know what info to collect and where to send it

@roytmana If I made available a custom build of Elasticsearch (or pointed you to a branch that you could build from source (and so see the diffs from v2.2.0)), would you be able to run it and send me the logs? I want to sprinkle additional logging in a couple targeted places to find the source of the channel error that does not have an exception associated with it.
</comment><comment author="roytmana" created="2016-02-26T22:23:37Z" id="189507322">Yes sure please send me the link to download it
</comment><comment author="jasontedor" created="2016-02-27T16:47:06Z" id="189680873">&gt; Yes sure please send me the link to download it

@roytmana Thanks. I've created a branch [issue-16810](https://github.com/jasontedor/elasticsearch/tree/issue-16810) in my repository that you can [diff](https://github.com/jasontedor/elasticsearch/commit/d1755be587b6c59d6531cc19d647a6411c1f6ebf) against the [v2.2.0 tag](https://github.com/elastic/elasticsearch/tree/v2.2.0) in the elastic repository. 

The salient changes are some extra logging statements that I hope will give us enough information to track down the root cause of the problem.

I've [uploaded](https://drive.google.com/file/d/0B4tiSXOP8wheN2xVY1g3Slg2a3M/view?usp=sharing) a packaged zip distribution of Elasticsearch from commit [d1755be](https://github.com/jasontedor/elasticsearch/commit/d1755be587b6c59d6531cc19d647a6411c1f6ebf) in that issue-16810 branch. It has the following SHA-256 hash:

```
d76fe7251b48644fa741b5a3682e0b02b56c539a77fc1fc8984e496d0cb315dc
```

When you start Elasticsearch from the distribution in this zip, it will print a log line that matches

```
^.*version\[2.2.1-SNAPSHOT\], pid\[[0-9]+\], build\[d1755be/2016-02-27T16:30:37Z\]$
```

Can you run Elasticsearch from this distribution with `TRACE` logging enabled, and after the NPE reproduces, share the output here?
</comment><comment author="roytmana" created="2016-02-27T19:59:09Z" id="189714923">please see attached. it is a zip file but I had to rename it because for some reason github did not want to accept zip file

[elasticsearch.zip.txt](https://github.com/elastic/elasticsearch/files/149530/elasticsearch.zip.txt)
</comment><comment author="jasontedor" created="2016-02-28T01:51:06Z" id="189763319">@roytmana I think that you have a 1.x node running on your system and that's what is going on here. Can you confirm?
</comment><comment author="roytmana" created="2016-02-28T14:41:01Z" id="189884716">@jasontedor not at the same time and in any case all my embedded elastic instances have unique cluster name.
I am pretty sure no 1.x was running at the time I did the test but 2.2 could have. I will double-check everything again make sure no elastic is running and run it again
</comment><comment author="jasontedor" created="2016-02-28T14:45:35Z" id="189885106">&gt; @jasontedor not at the same time and in any case all my embedded elastic instances have unique cluster name.

@roytmana The discovery process between the nodes will still take place, which combined with incompatible serialization is exactly what I think is going on here.

&gt; I am pretty sure no 1.x was running at the time I did the test but 2.2 could have. I will double-check everything again make sure no elastic is running and run it again

I'm fairly convinced this situation can only happen if a 1.x node was running, not if another 2.x node was running, because I think the root cause is incompatible serialization.
</comment><comment author="roytmana" created="2016-02-29T16:44:46Z" id="190282962">I stopped every JVM on my computer and no elastic of any version is running on my computer or network. same issue. logs attached
[elasticsearch.zip.txt](https://github.com/elastic/elasticsearch/files/151320/elasticsearch.zip.txt)
</comment><comment author="jasontedor" created="2016-02-29T16:53:38Z" id="190286043">There is no `NullPointerException` in the logs that you attached, just `ConnectTransportException`s which are completely expected here. Now that you're sure that there are no other JVMs running on your system, this only confirms my theory even more. :)

``` bash
11:52:38 2d [jason:~/Downloads] $ unzip elasticsearch.zip.txt
Archive:  elasticsearch.zip.txt
  inflating: elasticsearch.log       
11:52:39 2d [jason:~/Downloads] $ grep NullPointerException elasticsearch.log
11:52:40 2d [jason:~/Downloads] 1 $ 
```
</comment><comment author="roytmana" created="2016-02-29T18:00:57Z" id="190311884">but why all the exceptions connection?
let me start another 2.2 instance and run it again see if NPEs would come back
</comment><comment author="jasontedor" created="2016-02-29T18:09:30Z" id="190315281">&gt; but why all the exceptions connection?

Because when using the default network configuration, Elasticsearch tries to discover nodes also bound to localhost on ports 9301--9304. Since you don't have any other instances running, those discovery attempts will hit connection exceptions.
</comment><comment author="roytmana" created="2016-02-29T18:12:51Z" id="190316393">No NPEs with 2.2 started 1.7.2 and NPEs are back. I guess you would need to look into it. Two separate clusters should not interfere with each other.

Why do you says connection exceptions are expected? They go away if I specify non loopback address for my node
</comment><comment author="jasontedor" created="2016-02-29T18:18:19Z" id="190318012">&gt; No NPEs with 2.2 started 1.7.2 and NPEs are back.  

I consider this confirmation of my theory. :)

&gt; I guess you would need to look into it. Two separate clusters should not interfere with each other.

They are not interfering with each other, the `NullPointerException` here ends up being harmless. If you do have mixed-versions running on the same network, you can avoid the two clusters even talking to each other at all by configuring unicast with host lists.

Eventually the node handshake will be extended by #15971 but there are no plans to make any changes to 1.x and 2.x along these lines. 

&gt; Why do you says connection exceptions are expected? They go away if I specify non loopback address for my node

Because the attempted discovery only occurs if you are using the _default network configuration_.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove some out of date readme files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16809</link><project id="" key="" /><description>We have one in the project root, I imagine we can just use that one.
</description><key id="136419457">16809</key><summary>Remove some out of date readme files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>non-issue</label><label>review</label></labels><created>2016-02-25T15:24:56Z</created><updated>2016-03-08T13:12:29Z</updated><resolved>2016-03-08T13:12:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-25T16:12:49Z" id="188857994">I think you need to modify the distribution config in gradle so it copies the readme from the root? Or we won't have one in distributions anymore?
</comment><comment author="nik9000" created="2016-02-25T16:13:34Z" id="188858445">&gt; I think you need to modify the distribution config in gradle so it copies the readme from the root? Or we won't have one in distributions anymore?

I'll poke at it. I was wondering why we had a NOTICE and LICENSE in that directory too.
</comment><comment author="nik9000" created="2016-03-08T13:12:29Z" id="193779909">I don't have time to work on the packaging changes now. I'll close this so some other person can pick it up if they do have time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security permissions for Groovy JsonSlurper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16808</link><project id="" key="" /><description>This commit adds the necessary class permissions and property
permissions for basic Groovy JsonSlurper functionality.

Closes #14787 
</description><key id="136414454">16808</key><summary>Security permissions for Groovy JsonSlurper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2016-02-25T15:09:09Z</created><updated>2016-04-20T16:04:29Z</updated><resolved>2016-02-29T14:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-25T17:43:35Z" id="188898894">why is json parsing necessary in scripts? I don't think we need to support this. Also this kind of serialization/deserialization tends to rely on `suppressAccessChecks`. 

I really do not think we should do this. Our scripts do not need to be parsing json.
</comment><comment author="rmuir" created="2016-02-25T17:54:30Z" id="188903541">Separately, this PR is broken 80 ways from sunday.

Lets try to enumerate everything wrong with it:
- adding a third party dependency to _parse json in scripts_
- wrong NOTICE.txt/licensing data for said third party dependency.
- wrong thirdPartyExcludes: don't document these classes as missing, they arent missing, they use sun.misc.Unsafe!!!!

But again, we really should not make this change. Totally the wrong direction. No need for anybody to be serializing/deserializing json in scripts. Bringing in additional third party dependencies to do that, that's pure insanity.
</comment><comment author="jasontedor" created="2016-02-25T18:19:59Z" id="188917211">&gt; adding a third party dependency to parse json in scripts

This [dependency already exists in the 2.x line](https://github.com/elastic/elasticsearch/blob/2.2/modules/lang-groovy/pom.xml#L27) and I'm not sure if the fact that it was [removed](https://github.com/elastic/elasticsearch/pull/15688/files#diff-95b11eb218737a024629f7e02e610a37L26) received any attention and certainly not the attention that it deserves. It did not even make the breaking changes doc. And I'm not adding `groovy-all` (which is what is in 2.2) back, just `groovy-json`. Please do not exaggerate the issue.

&gt; wrong NOTICE.txt/licensing data for said third party dependency.

Can you please explain to me why it's wrong?

&gt; wrong thirdPartyExcludes: don't document these classes as missing, they arent missing, they use sun.misc.Unsafe!!!!

This is not grounded by the facts. In particular, the use of unsafe is _disabled_ by default, it is only enabled [if some system properties are intentionally set to true](https://github.com/apache/groovy/blob/c6c709795cc47b87ac6f2705c23731e2521c2dca/subprojects/groovy-json/src/main/java/groovy/json/internal/FastStringUtils.java#L37-L38). The user has to go out of their way to enable these system properties.

&gt; No need for anybody to be serializing/deserializing json in scripts.

Clearly there are users that _think_ there is a need or there wouldn't be issues on GitHub and posts on Discourse about how it's broken in 2.2 right now.

&gt; Bringing in additional third party dependencies to do that, that's pure insanity.

I think that you're mischaracterizing this. Again, it's a dependency that was removed without broad (any?) awareness.
</comment><comment author="rmuir" created="2016-02-25T18:24:34Z" id="188918462">I'm not mischaracterizing anything. This is a scripting engine, not a full blown programming language. What is the need to serialize/deserialize json? 

I'm strongly opposed to this change.
</comment><comment author="rmuir" created="2016-02-25T18:26:45Z" id="188919052">&gt; Clearly there are users that think there is a need or there wouldn't be issues on GitHub and posts on Discourse about how it's broken in 2.2 right now

Its one person basically. Its important to be able to distinguish non-usecases and esoteria from real use cases.
</comment><comment author="jasontedor" created="2016-02-26T07:14:52Z" id="189143307">&gt; &gt; wrong NOTICE.txt/licensing data for said third party dependency.
&gt; 
&gt; Can you please explain to me why it's wrong?

I see why they are wrong, but note that the [license](https://github.com/elastic/elasticsearch/blob/180ab2493e96223479c2d5efd9fdd0f28fd12fee/modules/lang-groovy/licenses/groovy-LICENSE.txt) and [notice](https://github.com/elastic/elasticsearch/blob/180ab2493e96223479c2d5efd9fdd0f28fd12fee/modules/lang-groovy/licenses/groovy-NOTICE.txt) files are also wrong for the groovy artifact and have been since these [license](https://github.com/elastic/elasticsearch/commit/180ab2493e96223479c2d5efd9fdd0f28fd12fee#diff-ecf1be4ab40a53b5dee4df963daad11a) and [notice](https://github.com/elastic/elasticsearch/commit/180ab2493e96223479c2d5efd9fdd0f28fd12fee#diff-f1b9a32d7f95c8a5b1d88ee3ce49230c) files were committed in 180ab2493e96223479c2d5efd9fdd0f28fd12fee. I've pushed 76e8a1151b23cea1443e90d3ee724fd241a7e5eb to fix this for all of these files.
</comment><comment author="fs-chris" created="2016-02-26T08:21:41Z" id="189158430">Just a few comments from the one esoteric guy (there is at least one more: #14787):

In my case, I decide in the Groovy update-script whether to process the JSON document or process another argument. Only in the first case - which is very rare - the document needs to be parsed. Decision is based on the existing document being updated.

Besides that,
- Why is it ok to pass a JSON string to an indexing request (Java-API) but esoteric to pass that same string to a Groovy script in an update request?
- Why is there the documented option of a java.policy file if it's not meant to be used?
- Why are entries added to that policy file not working as expected?
- Why is there support for a fully featured scripting language like Groovy (and even the other supported languages) if only very simple things should be done there?
</comment><comment author="rmuir" created="2016-02-26T12:08:46Z" id="189248136">There isn't support for stuff like this. Only just a few classes like `java.lang.Integer`.

Nobody needs to be parsing JSON here. Its not something we need to support, or add security holes or extra third party libraries for. I will do everything I can to prevent this.
</comment><comment author="clintongormley" created="2016-02-28T23:42:51Z" id="189969574">I'm siding with Robert here.  Parsing JSON in a script just seems wrong.  eg we don't support loading python modules in the python scripting plugin. We're certainly not going to support such things in Painless.  I'd say that scripts that need to do stuff like this should be implemented in Java rather than Groovy.  

@fs-chris mentions in https://github.com/elastic/elasticsearch/issues/14787#issuecomment-189322501 that:

&gt; We need to process several thousands of such requests per second. So performance is essential.

... which indicates that this should be implemented in Java as well.
</comment><comment author="s1monw" created="2016-02-29T10:46:19Z" id="190150386">I also agree with @clintongormley - yet the question is if we allow this backwards break in a minor release. On my end that's a case by case decision and for this one I am on the pro-break end. Especially since afaik @bleskes and @kimchy discussed some simple workaround related to passing the json string as a parameter but I don't recall the details. maybe @bleskes can elaborate?
</comment><comment author="bleskes" created="2016-02-29T11:07:28Z" id="190157703">&gt;  some simple workaround related to passing the json string as a parameter but I don't recall the details

That was the option to parse the json client side and pass it as a map-of-maps parameter to the script. I [mentioned it on the ticket](https://github.com/elastic/elasticsearch/issues/14787#issuecomment-189070783)
</comment><comment author="jasontedor" created="2016-02-29T14:30:26Z" id="190232154">Superseded by #16858.
</comment><comment author="fs-chris" created="2016-02-29T14:31:42Z" id="190232765">I see where this discussion leads to.
It's really annoying that a solution working well with previous releases gets broken without need.
Now we will have to implement that as a plugin which needs to be installed and maintained on all the nodes in a cluster, or stick to an old Elastic version that supports the script.
This change comes along with so many other changes since 2.0 that makes life harder.
</comment><comment author="jasontedor" created="2016-02-29T14:55:15Z" id="190245624">&gt; It's really annoying that a solution working well with previous releases gets broken without need.

@fs-chris I feel your pain, but this breaking change was not made without need (see #16858) and was clearly not made lightly.
</comment><comment author="PeteyPabPro" created="2016-03-01T15:51:27Z" id="190779576">If someone is using the Java API and wants to pass in json as a parameter (e.g. for updating the value of a nested object), using JsonSlurper seemed to be the accepted solution. Is there a workaround for this?
</comment><comment author="kimchy" created="2016-03-01T16:40:39Z" id="190802488">@PeteyPabPro see @bleskes note here: https://github.com/elastic/elasticsearch/issues/14787#issuecomment-189070783, you can pass a "map of maps" or the actual params in the parameters to the script, so you won't need to pass in JSON as a value within JSON.
</comment><comment author="jasontedor" created="2016-03-01T16:43:50Z" id="190804553">@PeteyPabPro: Have you tried the [previous suggestion](https://github.com/elastic/elasticsearch/pull/16808#issuecomment-190157703) of parsing the JSON client side and passing it to the script as a map of maps?
</comment><comment author="PeteyPabPro" created="2016-03-01T17:00:24Z" id="190813777">@kimchy @jasontedor Thanks for the suggestion, let me try that.
</comment><comment author="jasontedor" created="2016-03-01T23:43:00Z" id="190970343">&gt; Thanks for the suggestion, let me try that.

Thanks @PeteyPabPro; please let us know how it turns out?
</comment><comment author="PeteyPabPro" created="2016-03-01T23:58:22Z" id="190976113">@jasontedor Worked perfectly, thanks.
</comment><comment author="jasontedor" created="2016-03-03T15:05:33Z" id="191803978">&gt; Worked perfectly, thanks.

@PeteyPabPro: Outstanding, thanks for reporting back! This means that we can consider this a viable replacement for the previous functionality.
</comment><comment author="disbrain" created="2016-04-20T15:49:08Z" id="212485004">Excuse me, but this basically means that modifying _nested_ data in place (without previously loading the content into the client) is no longer possible? Previously, always talking about nested data, with a script I could alter the content of a nested record and reassign it without knowing what was the whole content (an array of nested objects for instance), now seems that since the payload has to be supplied as a param to the script I have to load the whole array into the client and reassign it whole through the map trick
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove mention of mvn from java source files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16807</link><project id="" key="" /><description>Switch with gradle.
</description><key id="136409813">16807</key><summary>Remove mention of mvn from java source files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>non-issue</label><label>review</label></labels><created>2016-02-25T14:55:57Z</created><updated>2016-02-25T16:05:46Z</updated><resolved>2016-02-25T16:05:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-25T15:11:25Z" id="188828670">Good. Can you get [`tests.maven`](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/buildSrc/src/main/groovy/org/elasticsearch/gradle/BuildPlugin.groovy#L403) too? It's read in [`BootstrapForTesting`](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java#L123-L127) and looks like a simple addition to this PR.
</comment><comment author="nik9000" created="2016-02-25T15:25:07Z" id="188837716">&gt; Can you get tests.maven too? It's read in BootstrapForTesting and looks like a simple addition to this PR.

Sure.
</comment><comment author="nik9000" created="2016-02-25T15:27:31Z" id="188839546">@jasontedor I've renamed the property. I'm running the tests now just to be extra paranoid.
</comment><comment author="jasontedor" created="2016-02-25T15:29:48Z" id="188841258">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>use elasticsearch 2.2.0 and Create NativeScript in java and install successfully but when restart or start elasticsearch it show error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16806</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**:1.8

**OS version**:Linux Mint 17.2 Cinnamon 64-bit (Cinnamon 2.6.13)

**Description of the problem including expected versus actual behavior:**
Hi,
i created nativescript in java and create .zip file with (jar and plugin-descriptor.properties) and install it but when i restart or start elasticsearch then it show error

Steps to reproduce:
1.create nativescript in java
2.install nativescript as plugin
3.restart or start elasticsearch

**Provide logs (if relevant):

this is log for successfully install nativescript as plugin in elasticsearch 2.2.0**

-&gt; Installing from file:/home/ajay/Desktop/DEVAJAY/Untitled Folder/CustomScriptFactory1.0.zip...
Trying file:/home/ajay/Desktop/DEVAJAY/Untitled Folder/CustomScriptFactory1.0.zip ...
Downloading .DONE
Verifying file:/home/mindarray/Desktop/DEVAJAY/Untitled Folder/CustomScriptFactory1.0.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed CustomScriptFactory1.0 into /home/mindarray/Desktop/elasticsearch-2.2.0/plugins/CustomScriptFactory1.0

**when restart or start the elasticsearch it show error**

Exception in thread "main" java.lang.ClassCastException: class org.spacevatican.elasticsearchexample.CustomScriptFactory
at java.lang.Class.asSubclass(Class.java:3404)
at org.elasticsearch.plugins.PluginsService.loadPluginClass(PluginsService.java:463)
at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:431)
at org.elasticsearch.plugins.PluginsService.(PluginsService.java:129)
at org.elasticsearch.node.Node.(Node.java:146)
at org.elasticsearch.node.Node.(Node.java:128)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.

**Describe the feature:**

![7eab302a-dbd1-11e5-8eba-14fede94d5a2](https://cloud.githubusercontent.com/assets/5938907/13320321/10b64b46-dbee-11e5-9ec9-7be1bfb343c2.jpg)
</description><key id="136382198">16806</key><summary>use elasticsearch 2.2.0 and Create NativeScript in java and install successfully but when restart or start elasticsearch it show error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ervivekmehta</reporter><labels /><created>2016-02-25T13:02:27Z</created><updated>2016-02-25T13:52:13Z</updated><resolved>2016-02-25T13:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ervivekmehta" created="2016-02-25T13:02:44Z" id="188777734">yes we have already extended Plugin class.

Here is the code overview:

public class TraceOrgCustomScriptFactory extends Plugin implements NativeScriptFactory
{

public ExecutableScript newScript(@Nullable Map&lt;String, Object&gt; params)
{
    return new CustomScript(params);
}

public boolean needsScores() {
    return true;
}

@Override
public String name() {
    return "es-plugin";
}

@Override
public String description() {
    return "es-plugin description";
}

}

public class CustomScript extends AbstractSearchScript
{
public static final short SCRIPT_UPPER_CASE =0;

public static final short SCRIPT_LOWER_CASE =1;

public static final short SCRIPT_REPLACE =2;

public static final short SCRIPT_SUBSTRING =3;

public static final short SCRIPT_FORMAT_DATE =4;

public static final short SCRIPT_ROUND_VALUE =5;

private final Map&lt;String,Object&gt; m_params;

public CustomScript(@Nullable Map&lt;String,Object&gt; params)
{
    m_params =params;
}

public Object run()
{
    Object result =null;

```
short scriptType = Short.parseShort(m_params.get("type").toString());

String field = m_params.get("field").toString();

ScriptDocValues&lt;Object&gt; value = (ScriptDocValues&lt;Object&gt;) doc().get(field);

if (value != null &amp;&amp; !value.isEmpty())
{

    switch (scriptType)
    {
        case SCRIPT_UPPER_CASE:

            result =value.get(0).toString().toUpperCase();

            break;


    }
}

return  result;
```

}

}
</comment><comment author="ervivekmehta" created="2016-02-25T13:04:00Z" id="188777975">please guide me. do not close issue directly. it was working with ES1.7 but not working with ES2.2.0

give at least a proper reply and guide us.
</comment><comment author="jasontedor" created="2016-02-25T13:48:21Z" id="188790262">Duplicates #16803 
</comment><comment author="ervivekmehta" created="2016-02-25T13:51:28Z" id="188791050">it's duplicate because you closed old one without any proper comment.

please provide a proper comment otherwise I have to create another same issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add note about size being per shard here</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16805</link><project id="" key="" /><description>It seems the current note via the comment is too less intrusive for people to notice and it leads to unexpected high number of search-hits per scroll if not explained a bit more.
</description><key id="136370956">16805</key><summary>Add note about size being per shard here</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2016-02-25T12:13:26Z</created><updated>2016-02-29T16:09:04Z</updated><resolved>2016-02-29T16:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-29T16:09:04Z" id="190271288">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proper handling exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16804</link><project id="" key="" /><description>**Elasticsearch version**:

2.1.1

**JVM version**:

 1.8.0_60

**OS version**:

Mac OS yosemite

**Description of the problem including expected versus actual behavior**:

If I'm testing undefined analyser the request will hang, instead it should return an error.

`curl -XGET 'localhost:9200/_analyze' -d ' {"analyzer" : "path_analyzer", "text" : "config/initialize/gitlab.yml" }'`

**Steps to reproduce**:
run `curl -XGET 'localhost:9200/_analyze' -d ' {"analyzer" : "path_analyzer", "text" : "config/initialize/gitlab.yml" }'`

**Provide logs (if relevant)**:

```
[2016-02-25 12:11:20,699][ERROR][transport                ] [Tomazooma] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@1da2dd9e]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

```

**Describe the feature**:
</description><key id="136340656">16804</key><summary>Proper handling exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vsizov</reporter><labels /><created>2016-02-25T10:14:56Z</created><updated>2016-02-25T11:18:59Z</updated><resolved>2016-02-25T11:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-25T11:18:59Z" id="188736503">Duplicates #15148
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>use elasticsearch 2.2.0 and Create NativeScript in java and  install successfully but when restart or start elasticsearch it show error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16803</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**:1.8

**OS version**:Linux Mint 17.2 Cinnamon 64-bit (Cinnamon 2.6.13)

**Description of the problem including expected versus actual behavior**:
Hi,
      i created nativescript in java and create .zip file with (jar and plugin-descriptor.properties) and install it but when i restart or start elasticsearch then it show error 

**Steps to reproduce**:
 1.create nativescript in java
 2.install nativescript as plugin 
 3.restart or start elasticsearch

**Provide logs (if relevant)**:

**this is log for successfully install nativescript as plugin in elasticsearch 2.2.0**

sudo /home/mindarray/Desktop/elasticsearch-2.2.0/bin/plugin  install file:///home/mindarray/Desktop/DEVAJAY/mindarray-elasticsearch-plugin-2.0.zip
-&gt; Installing from file:/home/mindarray/Desktop/DEVAJAY/mindarray-elasticsearch-plugin-2.0.zip...
Trying file:/home/mindarray/Desktop/DEVAJAY/mindarray-elasticsearch-plugin-2.0.zip ...
Downloading .DONE
Verifying file:/home/mindarray/Desktop/DEVAJAY/mindarray-elasticsearch-plugin-2.0.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Installed mindarray-elasticsearch-plugin into /home/mindarray/Desktop/elasticsearch-2.2.0/plugins/mindarray-elasticsearch-plugin

**when restart or start the elasticsearch it show error**  @jasontedor 

mindarray@mindarray-pc6 ~/Desktop/elasticsearch-2.2.0/bin $ ./elasticsearch
[2016-02-26 11:37:07,662][INFO ][node                     ] [Arides] version[2.2.0], pid[7986], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-26 11:37:07,663][INFO ][node                     ] [Arides] initializing ...
[2016-02-26 11:37:08,812][INFO ][plugins                  ] [Arides] modules [lang-expression, lang-groovy], plugins [head, mindarray-elasticsearch-plugin], sites [head]
Exception in thread "main" java.lang.NullPointerException
    at org.elasticsearch.plugins.PluginsService.updatedSettings(PluginsService.java:248)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:147)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.

**Describe the feature**:

![screenshot from 2016-02-26 11 39 15](https://cloud.githubusercontent.com/assets/2405218/13344621/ac2b2bfc-dc7d-11e5-89a1-96f814b35341.png)
</description><key id="136332831">16803</key><summary>use elasticsearch 2.2.0 and Create NativeScript in java and  install successfully but when restart or start elasticsearch it show error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingaj</reporter><labels /><created>2016-02-25T09:40:12Z</created><updated>2016-03-18T12:25:41Z</updated><resolved>2016-02-25T12:18:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-25T12:18:01Z" id="188757969">Sounds like your `CustomScriptFactory` class does not extend `Plugin`.

Can you ask such questions on discuss.elastic.co please? We keep this place only for confirmed issues.
</comment><comment author="kingaj" created="2016-02-25T12:47:34Z" id="188773150">https://discuss.elastic.co/ take lot of time to open
can you please tell me the way how to solve my problem please
</comment><comment author="kingaj" created="2016-02-25T12:49:09Z" id="188774117">and its possible to transfer elasticserach 1.7 indexing data into 2.2.0 elasticsearch 
</comment><comment author="ervivekmehta" created="2016-02-25T12:59:41Z" id="188777181">yes we have already extended Plugin class.

Here is the code overview:

public class TraceOrgCustomScriptFactory extends Plugin implements NativeScriptFactory
{

```
public ExecutableScript newScript(@Nullable Map&lt;String, Object&gt; params)
{
    return new CustomScript(params);
}

public boolean needsScores() {
    return true;
}

@Override
public String name() {
    return "es-plugin";
}

@Override
public String description() {
    return "es-plugin description";
}
```

}

---

public class CustomScript extends AbstractSearchScript
{
    public static final short SCRIPT_UPPER_CASE =0;

```
public static final short SCRIPT_LOWER_CASE =1;

public static final short SCRIPT_REPLACE =2;

public static final short SCRIPT_SUBSTRING =3;

public static final short SCRIPT_FORMAT_DATE =4;

public static final short SCRIPT_ROUND_VALUE =5;

private final Map&lt;String,Object&gt; m_params;

public CustomScript(@Nullable Map&lt;String,Object&gt; params)
{
    m_params =params;
}

public Object run()
{
    Object result =null;

    short scriptType = Short.parseShort(m_params.get("type").toString());

    String field = m_params.get("field").toString();

    ScriptDocValues&lt;Object&gt; value = (ScriptDocValues&lt;Object&gt;) doc().get(field);

    if (value != null &amp;&amp; !value.isEmpty())
    {

        switch (scriptType)
        {
            case SCRIPT_UPPER_CASE:

                result =value.get(0).toString().toUpperCase();

                break;


        }
    }

    return  result;
}
```

}
</comment><comment author="ervivekmehta" created="2016-02-25T13:52:35Z" id="188791414">please provide a proper comment otherwise I have to create another same issue.
</comment><comment author="jasontedor" created="2016-02-25T14:06:30Z" id="188797471">&gt; please provide a proper comment otherwise I have to create another same issue.

A [proper comment](https://github.com/elastic/elasticsearch/issues/16803#issuecomment-188757969) was already given. Your issue is neither stating that there is a bug in Elasticsearch nor is it a feature request. GitHub issues are reserved for bug reports and feature requests; you were directed to the [Elastic Discourse forums](https://discuss.elastic.co) where there is a community that would be more than delighted to help you; this was made clear in the issue template when you filed this issue.

There are unit tests in the Elasticsearch codebase that install and execute native script plugins and those tests pass on the 2.2 branch and the v2.2.0 tag. I note further that your stack trace states that `CustomScriptFactory` can not be cast to an instance of `Plugin` but the code that you pasted here doesn't even show the `CustomScriptFactory` class. It seems clear that you're trying to load a plugin that is not a `Plugin`.

I'm sorry to tell you that there is no evidence of a bug in Elasticsearch here. If you have additional questions, please use the [Elastic Discourse forums](https://discuss.elastic.co) and only use GitHub if you uncover evidence of an actual bug in Elasticsearch.
</comment><comment author="kingaj" created="2016-02-26T06:27:07Z" id="189129809">i change the class and also extend plugin you can see the error which i put above

and my*\* elasticsearch.yml file **

 script.inline: on

 script.indexed: on

 script.file: true

 script.default_lang: native

 resource.reload.interval: 120s
</comment><comment author="jasontedor" created="2016-02-26T06:29:36Z" id="189130074">Please note that dramatically editing your initial post like you have done just creates confusion.

It looks to me like you are overriding `Plugin#additionalSettings` and returning `null` and that's why you're getting the NPE.

Finally, unless you have a bug or a feature request, please take any future conversation to the [Elastic Discourse forums](https://discuss.elastic.co).
</comment><comment author="kingaj" created="2016-02-26T08:00:52Z" id="189154307">i create native search plugin in java and try to use as plugin
@jasontedor  sir you have any example or any link which i follow to create native script and use it 
please sir. 
</comment><comment author="dadoonet" created="2016-02-26T10:26:13Z" id="189207656">Please @kingaj

&gt; Finally, unless you have a bug or a feature request, please take any future conversation to the Elastic Discourse forums.
</comment><comment author="kingaj" created="2016-03-18T12:25:41Z" id="198329074">thank you my script working
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explore TinyLFU cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16802</link><project id="" key="" /><description>When removing Guava as a dependency, @jasontedor wrote an 256-way [LRU cache](https://github.com/elastic/elasticsearch/commit/aa8bfeb88c704206a87e35ed69b485dbdbfbce6a) using a Guava-like API. The design uses a read/write lock per table segment and a lock around the global linked for LRU ordering.

There are a few concerns that might be worth addressing. First is that a r/w lock is expensive, often much more than an exclusive lock, and its usage per read is probably best replaced with a ConcurrentHashMap. Second is that the LRU lock is a throttling point, as all accesses contend on it. Third is that `computeIfAbsent` is racy by allowing redundant computations with last insert wins. Fourth is that LRU is sub-optimal for search workloads, where frequency is a more insightful metric.

The easiest (and biased) solution is to adopt [Caffeine](https://github.com/ben-manes/caffeine). Alternatively a big win would come from integrating [TinyLFU](https://github.com/ben-manes/caffeine/wiki/Efficiency#search) into the existing cache. This would increase the hit rate, thereby reducing I/O and GC allocations, resulting in improved latencies.
</description><key id="136292763">16802</key><summary>Explore TinyLFU cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ben-manes</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2016-02-25T06:13:50Z</created><updated>2016-10-31T02:11:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-25T17:11:51Z" id="188882441">@ben-manes Thanks for the post.

&gt; First is that a r/w lock is expensive, often much more than an exclusive lock, and its usage per read is probably best replaced with a ConcurrentHashMap. Second is that the LRU lock is a throttling point, as all accesses contend on it.

It's difficult to say how impactful these are without search latency benchmarks here (to be clear since I don't want this to read like a dodge, I suspect the LRU lock is impactful under heavy search loads with large search thread pools). Unfortunately, we do not have benchmarks here that would stress the lock contention issues. These sorts of benchmarks are a work-in-progress.

&gt; Third is that computeIfAbsent is racy by allowing redundant computations with last insert wins.

This was the case for the initial commit that you linked to, but this has been [addressed](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/core/src/main/java/org/elasticsearch/common/cache/Cache.java#L345-L408) and (unless I'm mistaken) is no longer the case.

&gt; Fourth is that LRU is sub-optimal for search workloads, where frequency is a more insightful metric.

Do you have any literature on this specific point that you can share?

&gt; The easiest (and biased) solution is to adopt Caffeine. Alternatively a big win would come from integrating TinyLFU into the existing cache.

I'll take a look. I have some [ideas regarding the LRU lock contention as well](https://github.com/elastic/elasticsearch/blob/70396d4243e89dbe2194499cc942bff17bc695aa/core/src/main/java/org/elasticsearch/common/cache/Cache.java#L51-L61), but I really don't want to go stabbing in the dark until we can get valid multi-threaded search latency benchmarks in place. One goal we had with the cache implementation was to keep it simple.
</comment><comment author="ben-manes" created="2016-02-25T18:07:54Z" id="188907811">&gt; It's difficult to say how impactful these are without search latency benchmarks here

Yep, I'd much rather have data to work from than a hypothesis too. I know from [micro-benchmarks](https://github.com/ben-manes/caffeine/wiki/Benchmarks#read-100-1) and past experiences that it can be a problem, but it is too application dependent to draw a conclusion. The hit rate generally has a bigger impact than concurrency, given how expensive the miss penalty often is. I'm more interested in seeing if we can improve that for ES.

&gt; This was the case for the initial commit that you linked to, but this has been addressed

Oh, nice. Sorry that I missed that when skimming over the code.

&gt; Do you have any literature on this specific point that you can share?

Zipf-like distributions are commonly discussed in information retrieval literature ([1](https://hughewilliams.com/tag/zipfs-law/), [2](https://books.google.com/books?id=mDI72_9-bw0C&amp;pg=PA99&amp;lpg=PA99&amp;dq=search+engine+zipf&amp;source=bl&amp;ots=yVpgus4rCv&amp;sig=9-LGzMmmLGZ9JGX5SH_bHqljjAw&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjy6arxt5PLAhUB9WMKHcxdDuoQ6AEIPjAG#v=onepage&amp;q=search%20engine%20zipf&amp;f=false), [3](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&amp;rep=rep1&amp;type=pdf), [4](http://nlp.stanford.edu/IR-book/pdf/05comp.pdf), [5](http://ils.unc.edu/~losee/multzipf.pdf)). Similarly the distribution holds as a model for caching ([6](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0B2D54724B9411CC893B5F0097CCF426?doi=10.1.1.12.2253&amp;rep=rep1&amp;type=pdf), [7](http://www.cas.mcmaster.ca/~gk/papers/effcache.pdf), [8](http://ftp.cs.wisc.edu/pub/techreports/1998/TR1371.pdf)). This is fairly intuitive because the popularity of items is a good signal, but some aging based on recency is logically necessary to avoid holding stale data for too long. The [TinyLFU paper](http://arxiv.org/pdf/1512.00727.pdf) includes charts with web search traces from IBM and UMass. This policy uses recency and historic frequency (present and absent entries) to predict whether an item should be retained.

&gt; I have some ideas regarding the LRU lock contention as well

This [article](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html) on HighScalability describes the approach that I took. The concurrency model was proven out in my prior libraries (CLHM, Guava). The former is used by Cassandra (among others) and the latter is of course popular but underperforms due to [important optimizations](https://github.com/google/guava/issues/2063#issuecomment-107169736) not being ported over. This has very predictable latencies by being O(1) and borrows from database / filesystem theory where similar approaches are used to scale writes.
</comment><comment author="ben-manes" created="2016-04-29T04:28:18Z" id="215623848">@jasontedor 
I added ElasticSearch's cache to Caffeine's benchmark suite. The results were not positive.

The [micro-benchmarks](https://github.com/ben-manes/caffeine/wiki/Benchmarks) were run on my laptop. These results are slower than a synchronized LinkedHashMap in LRU mode. I ran Caffeine a few times since this isn't a good (isolated) testbed machine and it consumes all the available CPU resources.

| Benchmark | ElasticSearch | LinkedHashMap | Caffeine |
| :-: | :-: | :-: | :-: |
| read-only | 2.5 M/s | 7.4 M/s | 130 - 150 M/s |
| read-write | 2.3 M/s | 7.7 M/s | 100 - 115 M/s |
| write-only | 1.6 M/s | 7.7 M/s | 50 - 65 M/s |

The simulator compares the hit rate of the caching policies. The current cache stays very close to a pure LRU, which validates the design. Unfortunately LRU is poor for search workloads, which are biased towards frequency instead of recency.  The [efficiency](https://github.com/ben-manes/caffeine/wiki/Efficiency) using the Univ. of Massachusetts `WebSearch1` workload was,

| Benchmark | ElasticSearch | Caffeine | Optimal |
| :-: | :-: | :-: | :-: |
| WS1 @ 2M | 6.09 % | 23.06 % | 41.28 % |
| WS1 @ 4M | 21.60 % | 41.32 % | 57.80 % |
| WS1 @ 6M | 45.74 % | 55.02 % | 65.85 % |

It may very well be that the cache is not a critical bottleneck for ElasticSearch. Unfortunately it appears to leave a lot of performance on the table, which I bet a little profiling would dramatically improve. If the size of the cache is large, there is an opportunity it either achieve the same hit rate at a reduced capacity or increase the hit rate. In either case there should be reduced latencies (less GC / fewer misses).
</comment><comment author="ben-manes" created="2016-10-31T02:11:01Z" id="257199726">Congrats on 5.0! Its an impressive release and very timely for me. I was about to start migrating from Algolia to Elastic, and was considering Druid for analytical functions. The new features in Elastic are a wonderful alternative, so I'm looking forward to using them instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16801</link><project id="" key="" /><description>Closes #16294.

The previous duplicate key test assertion in JsonSettingsLoaderTests was failing to catch the exception. The background - Jackson 2.7.0 altered the logic for the getTokenLocation method in the JSON parser, it now points to the first character that is logically part of the token. See [https://github.com/FasterXML/jackson-core/issues/229](https://github.com/FasterXML/jackson-core/issues/229).
</description><key id="136218223">16801</key><summary>Upgrade to Jackson 2.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jbertouch</reporter><labels><label>:Core</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2016-02-24T23:09:33Z</created><updated>2016-02-28T23:02:11Z</updated><resolved>2016-02-26T21:18:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-25T21:07:19Z" id="188989321">@jbertouch thanks for the PR! I think we may want to bump it to Jackson 2.7.1 while we're at it: https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.7.1

Do you want to make that change?
</comment><comment author="jbertouch" created="2016-02-25T21:16:09Z" id="188992388">Hi @dakrone, done!
</comment><comment author="dakrone" created="2016-02-26T17:02:35Z" id="189373166">@jbertouch thanks, for these we also need to run `gradle updateSHAs`, do you want to do that? Or I can do it when I merge this
</comment><comment author="jbertouch" created="2016-02-26T17:16:45Z" id="189378589">Hi @dakrone, I ran the task and committed the updated hash values.
</comment><comment author="dakrone" created="2016-02-26T21:19:05Z" id="189486634">Merged, thanks @jbertouch !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update authors.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16800</link><project id="" key="" /><description>Removing link to pom file as it no longer exists.
If it is moved please provide a correct link.
</description><key id="136200526">16800</key><summary>Update authors.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ayushsangani</reporter><labels><label>docs</label><label>v2.2.1</label></labels><created>2016-02-24T22:05:37Z</created><updated>2016-02-27T01:07:51Z</updated><resolved>2016-02-27T01:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-25T14:12:21Z" id="188799544">@ayushsangani Yes, it should just point to 2.2 instead of master.

If you want to sign the CLA and update the pull request to just fix the link, that's fine, otherwise I'll just push a fix. Please let me know either way.
</comment><comment author="ayushsangani" created="2016-02-25T18:30:52Z" id="188920189">@jasontedor Just fixed couple of links, I believe I have signed CLA.
Please merge, if it looks good. Otherwise you can push a fix.
</comment><comment author="jasontedor" created="2016-02-26T19:55:24Z" id="189455472">&gt; Please merge, if it looks good.

@ayushsangani I will squash your commits into one before integrating into 2.2. Thanks for the contribution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid settings index level settings on a node level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16799</link><project id="" key="" /><description>Today we allow to set all kinds of index level settings on the node level which is error prone and difficult to get right in a consistent manner. For instance if some analyzers are setup in a yaml config file some nodes might not have these analyzers and then index creation fails. For settings that should have node level defaults like the default codec we should add dedicated settings.
</description><key id="136197808">16799</key><summary>Forbid settings index level settings on a node level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-02-24T21:54:15Z</created><updated>2016-03-17T19:36:30Z</updated><resolved>2016-03-17T14:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-24T22:40:51Z" id="188492689">+1
</comment><comment author="johtani" created="2016-02-24T23:39:48Z" id="188518519">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moved dynamic field handling in doc parsing to end of parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16798</link><project id="" key="" /><description>Currently dynamic mappings propgate through call semantics, where deeper
dynamic mappings are merged into higher level mappings through
return values of recursive method calls. This makese it tricky
to handle multiple updates in the same method, for example when
trying to create parent object mappers dynamically for a field name
that contains dots.

This change makes the api for adding mappers a simple list
of new mappers, and moves construction of the root level mapping
update to the end of doc parsing.
</description><key id="136180580">16798</key><summary>Moved dynamic field handling in doc parsing to end of parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-24T20:39:33Z</created><updated>2016-03-10T18:03:54Z</updated><resolved>2016-03-10T18:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-26T09:20:53Z" id="189183240">Looks good in general. My only concern would be about the creation of te mapping update at the root level.
</comment><comment author="rjernst" created="2016-03-09T21:31:03Z" id="194514616">@jpountz I pushed an update. While working on simplifying creating the root update, I found the algorithm ended up allowing many fields in the update which were not actually changed from the original mapping. For example, if you have `a.b` and `a.x` existing, and you add `a.c`, `a.x` would be in the update. In order to handle these cases, I needed to only add objects to the stack once an update has been created. This means we must create a temporary stack if there are intermediate objects needed for a dynamic mapper, and this turns out to be exactly what we need in order to create the root update. I think it looks simpler now, although a little less efficient (but correctness outweighs efficiency!).
</comment><comment author="jpountz" created="2016-03-10T09:15:48Z" id="194747296">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch : Filter does not work with wildcard query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16797</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:2.2

**JVM version**:1.8

**OS version**:Win 7

Here is my query

```
POST indexName/test/_search
{
  "fields" : ["Col1"],
  "query":{
                        "filtered":{

                        "query":{


                        "wildcard": {

                            "Col1": {

                                  "value": "*zxc*"
                               }
                            }

                          },
                        "filter":
                {
                  "term": { 

                    "Col2": "val" 

                  }
                }
                    }

            }
}
```

so i want to do a wildcard on `Col1` for the value `*zxc*` and i want only those values where `Col2` is `val` this returns 0 hits. is there anything wrong with my syntax?

The wildcard query works independently if i remove the filter.

**Edit**

this works

```
POST indexName/test/_search
{
  "fields" : ["Col1"],
  "query":{
                        "filtered":{


                        "filter":
                {
                  "term": { 

                    "Col2": "val" 

                  }
                }
                    }

            }
}
```

sample document returned

```
{
        "_index": "indexName",
        "_type": "test",
        "_id": "oainfubgvwurebetrnjt",
        "_score": 1,
        "fields": {
          "Col1": [
            "uyiwebvybg iuowenbgubnrwev  uirbgvuire3bv  vuirbg"
          ]
        }
      }
```

and so does this

```
POST indexName/test/_search
{
  "fields" : ["Col1"],
  "query":{
                        "wildcard": {

                            "Col1": {

                                  "value": "*zxc*"
                               }
                            }

            }
}
```

sample document returned

```
{
        "_index": "indexName",
        "_type": "test",
        "_id": "aofnhuiwegbnweu",
        "_score": 1,
        "fields": {
          "Col1": [
            "idasihid huwbgbuiwb iuohfuweb zxc oifjhwgbu"
          ]
        }
      }
```

so both of these queries work independently. How do i combine them?

Is this a bug in elasticsearch?

**Steps to reproduce**:
1. Create sample dataset with two columns
2. Run wildcard and filter queries separately to check if everything works 
3. Combine wildcard and filter queries

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="136086200">16797</key><summary>Elasticsearch : Filter does not work with wildcard query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2016-02-24T15:00:57Z</created><updated>2016-02-28T22:25:40Z</updated><resolved>2016-02-28T22:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T22:25:40Z" id="189957651">Hi @abtpst 

This works for me:

```
PUT t/t/1
{
  "Col1": "idasihid huwbgbuiwb iuohfuweb zxc oifjhwgbu",
  "Col2": "val"
}

POST t/t/_search
{
  "query": {
    "filtered": {
      "query": {
        "wildcard": {
          "Col1": {
            "value": "*zxc*"
          }
        }
      },
      "filter": {
        "term": {
          "Col2": "val"
        }
      }
    }
  }
}
```

But I think you are using Elasticsearch in a very efficient way.  A wildcard query with a leading wildcard leads to a full index scan which is very inefficient.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrases in span_near</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16796</link><project id="" key="" /><description>I am using `span_near` queries and while I can put in `span_terms` and even `span_or`, I am missing a `span_phrase` clause in the query dsl.

As I understand it, a regular `phrase` clause, is very similar (at least conceptually) to a `span_near`, where the slop is 0 and the terms are the phrase that has been run through the same analyzer as the field uses.

However: while I could construct that `span_near` and its `span_terms` by hand, I would have to be very careful making the terms (getting it close is easy; getting it right is hard) and I would only be able to cover one analyzer at a time, which is hardly ideal.
Alternatively, I could also ask elasticsearch to analyze the phrase for me (according to the fields analyzer), and then use that in the query, but that would cost me an extra round trip. Not to mention that the query construction is actually in a library that is currently blissfully unaware of the actual elasticsearch nodes, it just builds the query.

So having elasticsearch take the phrase and construct a lucene `SpanNearQuery` or something akin to that, would be very nice, and save me a lot of trouble.

Maybe the dsl could look something like:

``` json
{
    "span_phrase": {
        "&lt;field&gt;": "&lt;phrase&gt;"
    }
}
```

Which I could then embed in my `span_near`, like any other span clause.
</description><key id="136072003">16796</key><summary>Phrases in span_near</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cknv</reporter><labels><label>:Query DSL</label><label>discuss</label><label>feature</label><label>high hanging fruit</label></labels><created>2016-02-24T14:09:48Z</created><updated>2017-06-01T08:03:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T22:19:18Z" id="189956610">Hi @cknv 

All of the span queries are term-level queries, ie the query string needs to be analyzed before the resulting terms are used with span queries.  A phrase query would behave completely differently as it would include analysis.  Given that you already need to deal directly with terms, I'm not sure what a `span_phrase` query would buy you?
</comment><comment author="cknv" created="2016-02-29T13:53:15Z" id="190220902">I know that it would include analyzis of the submitted text. That is the whole point. The tl;dr of it is that I have a DSL where I want to add the ablility to search for phrases in near clauses, something like:
- `[term "some phrase"]~4`, somewhat similar to your own [proximity searches](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_proximity_searches) - although I had to change the grammar a bit.
- `term NEAR/4 "some phrase"`, sometimes called a near group.

These two examples is not the hardest to break down, but it is not hard to imagine worse cases.

Now, the problem is that I am reluctant to implement the logic to break down the phrase into terms in my own (python) code, as I can currently spot two options, both of which are not very good:
- Ask Elasticsearch what those terms are before making my query, costing me an extra roundtrip (and in my specific case rewrite how I translate from my DSL to yours).
- Mimick what the analyzer of the specific field does. This would probably be brittle because of subtle differences in the two analyzers.

My problem boils down to the fact that while I can make a DSL on top of yours that support phrases, terms, wildcards, and etc. in the normal clauses that do not care much about positioning. It becomes difficult when I want to add phrases to clauses that translate into span clauses.

However, inside Elasticsearch, you know what [analyzer to use for a given field](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/indices-analyze.html) and can just use that directly, you can perhaps even figure out what to do if that analyzer differs across multiple indices (or maybe that is something lucene would do better).
</comment><comment author="clintongormley" created="2016-03-02T09:09:06Z" id="191144005">Hi @cknv 

The bit I'm missing is this: you're already using span clauses, which are term based, so you already need to do the analysis to convert text to terms.  e.g. to take your example:

```
 term NEAR/4 "some phrase"
```

Imagine you're using the `english` analyzer and `terms` has been stemmed to `term`.  It is quite possible that your user will enter `terms NEAR/4 "some phrase"` which will find nothing unless you analyze `terms` -&gt; `term`.

So you already have to deal with analysis for the words outside the phrase.  Why would the words inside the phrase be any different? 

I wonder if you shouldn't be looking at creating a plugin based on the surround query parser available in Lucene: https://lucene.apache.org/core/5_4_0/queryparser/index.html?org/apache/lucene/queryparser/surround/parser/package-summary.html
</comment><comment author="mcuelenaere" created="2016-03-10T14:50:03Z" id="194883158">I have a similar request.
Given the query "Foo-BAR" and dataset ["bar foo bar", "foo bar", "foo bar foo"] I want "foo bar" to rank the highest.

This can be implemented with a `span_first` wrapping a `span_near` with two terms, however I need to perform the analysis part client-side as there is no ES clause AFAIK that does this (except for `match_phrase`, which cannot be wrapped in a `span_first` though).

http://grokbase.com/t/gg/elasticsearch/12bv1ee7ah/forcing-analysis-of-terms-and-span-terms describes pretty much the same.
</comment><comment author="cknv" created="2016-03-15T10:45:01Z" id="196761944">It's true that I can deal with terms, but actually now that I think of it, it will break down if I ever decide to use stemming or smiliar modification of words that go into the index.
Currently I am mostly saved by my DSL that enables me to restrict the text I accept as terms, thus my need to change them is minimal, but actually I would need to do something and I realize now that my DSL is actually incomplete in certain edge cases.

I still think that part of the problem is having to replicate exactly what the different analyzers are doing, not to mention custom ones. As @mcuelenaere pointed out, I think this can provide a lot of help to provide the correct analysis of text into tokens. I am not sure how hard this is in Elasticsearch, but I hope that it could give the span queries a little more ease of use. Allowing developers to focus on whatever product we base upon Elasticsearch instead of having to figure out how to do text analysis.
</comment><comment author="clintongormley" created="2016-03-15T17:46:35Z" id="196944507">The span queries are low level term-oriented queries.  They are building blocks that can be used to implement a custom query syntax, similar to the `query_string` query syntax, but more position aware. This isn't going to change.  

Really exposing them via the query DSL is a bit of an anomaly.  Normally they'd be used by a query parser written in Java and living on the server.  Analysis is a vital part of the construction of queries which use span queries.  

I think the solution here is to look for (or write) a custom query parser that supports operators like `NEAR/4` etc.
</comment><comment author="speedplane" created="2017-03-30T07:05:03Z" id="290321257">@clintongormley I agree that this is likely the solution. I use ES and did exactly this, see [here for an example](https://www.docketalarm.com/search/documents/?q=%22personal+injury%22+w%2F10+boat).</comment><comment author="clintongormley" created="2017-06-01T08:03:02Z" id="305420069">Related to https://github.com/elastic/elasticsearch/issues/11328</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GetMappingsResponse dosn't work with alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16795</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2

**JVM version**: 1.7

**OS version**: Linux

**Description of the problem including expected versus actual behavior**:

Read a mapping from GetMappingsResponse failed when using the alias instead of the index name. Because the index-name is used as key, not the alias name.

**Steps to reproduce**:

GetMappingsResponse response = client.admin().indices()
                .prepareGetMappings(esIndex) //use an alias here
                .setTypes(esType)
                .execute().actionGet();

response.mappings().get(esIndex); //return null when using alias, here are only index-names available

Using REST API it works fine:
.../esIndex/_mapping/esType/

When using an alias name, I got a response with the alias-name instead of the real index name.
</description><key id="136063044">16795</key><summary>GetMappingsResponse dosn't work with alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreVirtimo</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2016-02-24T13:34:18Z</created><updated>2016-02-28T22:37:19Z</updated><resolved>2016-02-28T22:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-24T15:56:32Z" id="188317069">Hi @AndreVirtimo I am trying to reproduce this with no luck. I always get the index name back on both java api and REST layer, which is the expected behaviour as aliases get resolved to concrete indices as part of api execution. It is expected that with the java api the lookup for the alias in the map fails, better to iterate over the map entries instead. Can you double check your REST api output and post any output that may contain the alias name in it rather than the index name?
</comment><comment author="AndreVirtimo" created="2016-02-25T07:40:47Z" id="188655695">Hey @javanna , I'm sorry. I have tested again the REST Call via alias and I get the index-name in the response.

But never the less I would expect the alias name, because the usage of an alias should be transparent.
</comment><comment author="javanna" created="2016-02-25T11:31:18Z" id="188741986">No problem @AndreVirtimo . As for your comment:

&gt; But never the less I would expect the alias name, because the usage of an alias should be transparent.

This is how all the elasticsearch apis work, responses contain the resolved concrete indices, not the alias (or aliases) used in the request. That said I am marking this issue for discussion so we can discuss this internally and see whether we want to change this or not.
</comment><comment author="clintongormley" created="2016-02-28T22:37:19Z" id="189959681">&gt; This is how all the elasticsearch apis work, responses contain the resolved concrete indices, not the alias (or aliases) used in the request. That said I am marking this issue for discussion so we can discuss this internally and see whether we want to change this or not.

We can't change this as an alias may point to multiple indices, which may have different mappings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add index time "_boost" property which automatically affects the score per document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16794</link><project id="" key="" /><description>This can now be achieved using function_score but that makes it very explicitly and in many cases complicates the queries unnecessarily.

The common use case for this would be positive popularity bias. This is where one would want popular search items to be raising in search results. Implementing popularity in ES entirely would be defeated with reindexing but if the popularity value was provided by the indexing service (business application) at index time then ES would only need to consider it while scoring.

It is important in my head to be able to hide this complexity behind regular _score so that it "just works" when enabled.
</description><key id="136027029">16794</key><summary>Add index time "_boost" property which automatically affects the score per document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pawpro</reporter><labels /><created>2016-02-24T11:05:52Z</created><updated>2016-02-24T11:31:25Z</updated><resolved>2016-02-24T11:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pawpro" created="2016-02-24T11:31:25Z" id="188209103">Addressed by field_value_factor. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2.0 wrongfully marks shard as unassigned / corrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16793</link><project id="" key="" /><description>Hi I had a red state on one of the shards in my index : 

I tried a force relocation to reassign it, this was the output:
https://gist.github.com/jrots/da71bc8b0ad86a27636a
force allocation from previous reason ALLOCATION_FAILED, failed recovery,
 failure IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: 
 CorruptIndexException[failed engine (reason: [search execution corruption failure]) (resource=preexisting_corruption)]; nested: NotSerializableExceptionWrapper[failed engine (reason: [search execution corruption failure])]; 
 nested: CorruptIndexException[Corrupted: docID=3412736, docBase=53, chunkDocs=50, numDocs=4460365
There was indeed a "corrupted_.." file present, 

I did a check with lucene CheckIndex, this was the output : 
https://gist.github.com/jrots/e816c24b17ef4ae6fe29

all segments showed up as valid...  

I manually removed the corrupt_ file and did a restart of that node &amp; all was assigned again and ES had a green state again.. but there might be a bug in ES that generates a "corrupted_" file too soon because there weren't any issues when doing a check afterwards.. 
</description><key id="136026528">16793</key><summary>ES 2.2.0 wrongfully marks shard as unassigned / corrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jrots</reporter><labels><label>non-issue</label></labels><created>2016-02-24T11:03:39Z</created><updated>2016-03-02T20:44:00Z</updated><resolved>2016-03-02T19:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jrots" created="2016-02-24T11:18:41Z" id="188201783">Ok seems like #16789 is related and I'm not the only one experiencing this
</comment><comment author="s1monw" created="2016-02-24T17:09:35Z" id="188356614">Hey @jrots thanks for reporting this, lemme ask some questions:
-  do you still have that "corrupt_" file by any chance? 
- Can you also check you logs to tell what lead to this corruption in the first place? 
- Did you have replicas before and one of them got corrupted? 
- Do you know when that corruption file was written vs. when the index files were written?
- have you done a restore by any chance?

thanks!
</comment><comment author="jrots" created="2016-02-24T20:20:19Z" id="188439052">Hi @s1monw 
- afraid I have deleted the corrupt file :/ If it would reoccur I'll make sure I have a copy of it. 
- log file of when it occurred : https://gist.github.com/MassiveMedia/18014fc75c7fbd6cf0eb
- I didn't have any replicas enabled on that index.. so can't answer on that one 
- no afraid I didn't do an `ls -lsha` at that time, just a simple `ls` .. :/ so can't provide you that info.. will keep this in mind when it would happen again. 
- no I didn't do a restore 
</comment><comment author="s1monw" created="2016-02-24T21:48:03Z" id="188467093">@jrots I have the suspicion that this is a compression bug in lucene. Do you see any chance to upload this shards index somewhere so we can investigate.
</comment><comment author="s1monw" created="2016-02-24T21:49:04Z" id="188467416">@jpountz @rmuir @mikemccand just pinging you here to get your attention as well
</comment><comment author="jpountz" created="2016-02-24T22:01:54Z" id="188471830">Hmm that looks like a bug in the index on top of stored fields. Maybe an overflow?
</comment><comment author="jrots" created="2016-02-24T22:39:59Z" id="188492496">Ok I've uploaded the index, but I rather not post the link here as it contains some sensitive data. Is there an email adres I can sent it to?
</comment><comment author="s1monw" created="2016-02-24T22:48:01Z" id="188494634">you can send it to simon at elastic dot co
</comment><comment author="mikemccand" created="2016-02-24T23:17:08Z" id="188509754">It does sounds like maybe a Lucene bug, especially if `CheckIndex` can't find anything wrong with the segment, but a search thread did.  The JVM is modern (1.8.0_66).
</comment><comment author="mikemccand" created="2016-02-25T08:57:45Z" id="188675387">Hi @jrots, I ran `CheckIndex` from both 5.4.x and 5.5.x Lucene on your index, but it did in fact detect a checksum mismatch on that one segment (`_69l`): https://gist.github.com/mikemccand/1be773920096ede16481

Also, this index has 33 segments but your `CheckIndex` output shows 19.

Are you sure you copied up exactly the same index you had successfully run `CheckIndex` on?  Otherwise I can't explain why your `CheckIndex` runs fine but when I run it, it detects corruption.
</comment><comment author="jrots" created="2016-02-25T11:03:53Z" id="188730094">Hi, 
Well the index data was from almost 10 hours after I removed the _corrupted file.. and there have been updates to the data in that index in the meanwhile. So could be that the segment was merged internally already. I also copied the data with a simple `cp` command while elastic was still running, which probably was not the best idea (need to shutdown the node first). 
If I run `CheckIndex` on that copied data I also get the same warning, https://gist.github.com/jrots/c4087c8fab52a81d346d
I made a new backup now of this shard, while ES was shutdown and `CheckIndex` also ran fine on it.. 
but it probably doesn't make sense to send you that data over? 
If I ever hit this issue again, I'll make sure I make a backup of the data at that point. Because atm it's probably hard to investigate what exactly was corrupt. 
</comment><comment author="s1monw" created="2016-03-02T19:38:02Z" id="191391191">I think it turns out that the index is actually corrupted @mikemccand checkindex failed for you on the shard no? I am closing for now
</comment><comment author="mikemccand" created="2016-03-02T20:44:00Z" id="191421337">&gt; @mikemccand checkindex failed for you on the shard no

Yeah.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sharing parsed documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16792</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

While reviewing elasticsearch code I noticed that documents are parsed twice.
1) in prepareIndexOnPrimary
2) in prepareIndexOnReplica
I understand that two different document mapper instances are used for parsing process. But isn't it possible to refactor it out, so document mappers would have a shared parsed document instance somehow? The cause for this change is simple - parsing of documents maybe twice faster than it is now. More than that, we currently use luwak percolator on our project and it uses same lucene Documents so we'd like to share these parsed documents even for outside world and this could be a good addition. These documents seems to be immutable so it shouldn't be difficult. Please let me know, what do you think about this.
Thank, Edward.
</description><key id="136025329">16792</key><summary>sharing parsed documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edvorg</reporter><labels /><created>2016-02-24T10:59:09Z</created><updated>2016-02-25T01:52:43Z</updated><resolved>2016-02-25T01:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-25T01:52:43Z" id="188557767">prepareIndexOnPrimary &amp; prepareIndexOnReplica are never called on the same node. The reason being that a primary shard is never on the same node as a replica node. As such, I don't believe this is a problem and I'm going to close the issue. If I missed something please reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify parsing of command-line arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16791</link><project id="" key="" /><description>The purpose of this commit is to simplify the parsing of command-line
options. With it, the following breaking changes are made
- double-dash properties are no longer supported
- setting properties should be done via -E
- only the prefix "es." for setting properties is supported now

This pull request also removes setting properties to pass information
about the command line arguments daemonize and pidfile from the CLI
parser to the bootstrap process.

As an added win, the environment is not prepared twice.

Closes #16579, closes #11564 
</description><key id="136011039">16791</key><summary>Simplify parsing of command-line arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>review</label></labels><created>2016-02-24T09:55:32Z</created><updated>2016-03-14T11:35:38Z</updated><resolved>2016-03-14T00:10:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-24T09:58:19Z" id="188171755">This pull request is complete, but the work here is not. In particular, the eventual end game is to remove the use of setting system properties to get command line arguments from the CLI parser down to the bootstrap process. The pieces are in place for this to be a simple extension of this pull request, but there is a lot of time-consuming tedious work to be done to change all the places where we are setting system properties (tests, docs, etc.). This can be done in a follow-up pull request.

Finally, the long end game is to apply the same work to the plugin CLI parser. Again, this can be done in a follow-up pull request.
</comment><comment author="dakrone" created="2016-02-25T20:58:54Z" id="188983869">Can you add an example of using the `-E` option to the help message:

```
&#187; bin/elasticsearch -h
 -E                   : configure an Elasticsearch setting (default: {})
 -H (--path.home) VAL : Elasticsearch path.home
 -d (--daemonize)     : daemonize Elasticsearch (default: false)
 -h (--help)          : print this message (default: false)
 -p (--pidfile) VAL   : pid file location
```

I think users will assume that `bin/elasticsearch -Ees.node.name=foo` will work (but it doesn't)
</comment><comment author="dakrone" created="2016-02-25T21:03:11Z" id="188986543">Yikes, this (not your change, me testing) has revealed other problems:

```
130 &#187; bin/elasticsearch -H /tmp/bar
Failed to configure logging...
ElasticsearchException[Failed to load logging configuration]; nested: NoSuchFileException[/tmp/bar/config];
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:158)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:107)
        at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:200)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:252)
        at org.elasticsearch.bootstrap.Elasticsearch$StartCommand.execute(Elasticsearch.java:156)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:62)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:55)
Caused by: java.nio.file.NoSuchFileException: /tmp/bar/config
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
        at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
        at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
        at java.nio.file.Files.readAttributes(Files.java:1737)
        at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:225)
        at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322)
        at java.nio.file.Files.walkFileTree(Files.java:2662)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:142)
        ... 6 more
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Exception in thread "main" java.lang.SecurityException: exit(0) not allowed by system policy
        at org.elasticsearch.SecureSM.checkExit(SecureSM.java:176)
        at java.lang.Runtime.exit(Runtime.java:107)
        at java.lang.System.exit(System.java:971)
        at org.elasticsearch.bootstrap.Elasticsearch.exit(Elasticsearch.java:107)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:56)
^Cbin/elasticsearch -H /tmp/bar  6.49s user 0.32s system 78% cpu 8.645 total
~/scratch/elasticsearch-3.0.0-SNAPSHOT 
130 &#187; bin/elasticsearch -H /tmp/bar
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Exception in thread "main" java.lang.SecurityException: exit(0) not allowed by system policy
        at org.elasticsearch.SecureSM.checkExit(SecureSM.java:176)
        at java.lang.Runtime.exit(Runtime.java:107)
        at java.lang.System.exit(System.java:971)
        at org.elasticsearch.bootstrap.Elasticsearch.exit(Elasticsearch.java:107)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:56)
^Cbin/elasticsearch -H /tmp/bar  6.95s user 0.32s system 54% cpu 13.321 total
```

Firstly, we should be able to exit if the `path.home` is bad/incorrect and not throw the exception about not being able to exit.

Second, if ES fails to properly start up, it still creates the `/tmp/bar/config/scripts` directory! I think if we fail to start entirely we shouldn't create directories if possible.
</comment><comment author="rjernst" created="2016-02-25T21:15:52Z" id="188992276">I thought we didn't support setting `path.home`? This shouldn't be done by a user, it requires having bin, lib and modules. It is really a setting just for passing between bin/elasticsearch and bootstrap so environment can know where to relatively base defaults for eg config, scripts, plugins, data, etc.
</comment><comment author="jasontedor" created="2016-02-25T21:25:35Z" id="188995112">&gt; I thought we didn't support setting path.home?

@rjernst It's set by `bin/elasticsearch` via a command line argument, it is not settable from the config file (although I think we silently ignore that right now).
</comment><comment author="rjernst" created="2016-02-25T21:28:23Z" id="188995785">Right, I know it is set by bin/elasticsearch, but I thought we did _not_ allow it anymore with --path.home, which is what it looks like you have in your help message (ie @dakrone's problem is artificial because it is not something that should be allowed)
</comment><comment author="jasontedor" created="2016-02-25T21:29:43Z" id="188996114">&gt; Right, I know it is set by bin/elasticsearch, but I thought we did not allow it anymore with --path.home, which is what it looks like you have in your help message (ie @dakrone's problem is artificial because it is not something that should be allowed)

@rjernst You can set it on master right now as long as you provide an actually valid home.
</comment><comment author="rjernst" created="2016-02-25T21:31:55Z" id="188996691">But that is silly. That means someone is starting bin/elasticsearch, but trying to point it at a home installation for _another_ home, that would have a different bin/elasticsearch? We should just not allow this craziness!
</comment><comment author="dakrone" created="2016-02-25T21:34:44Z" id="188997526">&gt; That means someone is starting bin/elasticsearch, but trying to point it at a home installation for another home, that would have a different bin/elasticsearch? We should just not allow this

That is how the service files for Linux distributions work? The `elasticsearch` script in `$PATH` sets up an ES_HOME directory that may or may not be where the script is stored?
</comment><comment author="jasontedor" created="2016-02-25T21:41:36Z" id="188999383">&gt; But that is silly. That means someone is starting bin/elasticsearch, but trying to point it at a home installation for another home, that would have a different bin/elasticsearch? We should just not allow this craziness!

@rjernst Whether or not it's crazy is independent of this pull request.
</comment><comment author="rjernst" created="2016-02-25T21:49:46Z" id="189001781">&gt; Whether or not it's crazy is independent of this pull request.

I don't think it is. This PR is about simplifying, but this `-H` parameter is adding new "official" (ie documented) crazy functionality that was undocumented before, which should stay internal to how bin/elasticsearch passes information to bootstrap.
</comment><comment author="rjernst" created="2016-02-25T21:55:36Z" id="189003667">&gt; That is how the service files for Linux distributions work? The elasticsearch script in $PATH sets up an ES_HOME directory that may or may not be where the script is stored?

`bin`, `lib` and `modules` live under the `ES_HOME` dir, and starting bin/elasticsearch finds that home dir by looking upwards, in order to pass to bootstrap. There is no reason this needs to be a parameter which the script accepts. We should remove `-Des.default.path.home=$ES_HOME` from the start scripts for rpm/deb (bin/elasticsearch always passes -Des.path.home, so the default does not matter).
</comment><comment author="dakrone" created="2016-02-25T21:55:41Z" id="189003689">Okay, sounds like it's a discussion between:

`-Des.path.home=&lt;dir&gt;` or adding a special-purpose flag for doing that (`-H`).

I think it might be cleaner if we only supported a single one, in this case, `-E` (what `-D` is turning into), what do you think @jasontedor @rjernst ?
</comment><comment author="rjernst" created="2016-02-25T21:57:28Z" id="189004169">&gt; I think it might be cleaner if we only supported a single one, in this case, -E (what -D is turning into), what do you think ?

We shoudln't support it at all when calling the script. It is completely something internal to communication between bin/elasticsearch and bootstrap.
</comment><comment author="dakrone" created="2016-02-25T21:59:05Z" id="189004624">&gt; We shoudln't support it at all when calling the script. It is completely something internal to communication between bin/elasticsearch and bootstrap.

We need to have a way for `bin/elasticsearch` to tell Java what `path.home` should be (regardless of if it's internal or not) right?
</comment><comment author="rjernst" created="2016-02-25T21:59:58Z" id="189004861">Yes, we do, but it doesn't have to be something that a user can ever set.
</comment><comment author="dakrone" created="2016-02-25T22:01:22Z" id="189005210">&gt; Yes, we do, but it doesn't have to be something that a user can ever set.

It sounds like you're proposing that the `elasticsearch` script can never be outside of the `path.home` directory, is that correct? (I just want to make sure I understand)
</comment><comment author="rjernst" created="2016-02-25T22:06:05Z" id="189006421">That's my opinion, yes. :)
</comment><comment author="dakrone" created="2016-02-25T22:19:26Z" id="189009891">Okay, in the interest of not blocking the improvements within this PR (like not setting system properties), what do you think about this:
- Remove `-H` from this PR (maintains status quo of being able to set `path.home` like we currently have)
- Remove `-p` from this PR (can be set with `-E pidfile=/var/run/es.pid`)
- Open followup issue for discussion/removal of the `path.home` setting

? 
</comment><comment author="jasontedor" created="2016-02-25T22:28:15Z" id="189012900">&gt; Remove `-H` from this PR (maintains status quo of being able to set `path.home` like we currently have)

I can easily make this hidden in the usage, and remove the `-H` flag maintaining the exact behavior that we have today.

&gt; Remove `-p` from this PR (can be set with `-E pidfile=/var/run/es.pid`)

I think this should also be a separate issue as this impacts the startup scripts too and just increases the surface area of this change. I'd prefer to keep this PR focused on merely changing the _parsing_ of the command line arguments (i.e., getting rid of `BootstrapCLIParser`, and getting rid of the confusion between `-D` for system properties and Elasticsearch settings (thus the change to `-E`), and getting rid of many different ways to set settings (different prefixes, double dash, etc.)). The goal was simplification and setting the stage for getting rid of the usage of system properties.

&gt; Open followup issue for discussion/removal of the `path.home` setting? 

I'm +1 on @rjernst's proposal, I would just prefer that it not be part of this PR.
</comment><comment author="rjernst" created="2016-02-25T22:29:12Z" id="189013129">+1 to just removing `-H` from this PR. Thanks @jasontedor 
</comment><comment author="dakrone" created="2016-02-25T22:30:16Z" id="189013412">&gt; &gt; Remove -p from this PR (can be set with -E pidfile=/var/run/es.pid)
&gt; 
&gt; I think this should also be a separate issue as this impacts the startup scripts too and just increases the surface area of this change. I'd prefer to keep this PR focused on merely changing the parsing of the command line arguments (i.e., getting rid of BootstrapCLIParser, and getting rid of the confusion between -D for system properties and Elasticsearch settings (thus the change to -E)).

Whoops, I hadn't realized we support `-p` already, I'm +1 to what @rjernst said
</comment><comment author="jasontedor" created="2016-02-25T22:53:17Z" id="189019150">&gt; +1 to just removing -H from this PR. Thanks @jasontedor

@rjernst I pushed c097644609a8c596dc2bf45436aad5b70a89b019.
</comment><comment author="clintongormley" created="2016-02-28T23:12:15Z" id="189965799">@jasontedor can you explain the reasoning behind using `-E foo` instead of `--foo`?  I don't like the fact that it is going to break settings for everybody, and I find `--` easier to read.  Is this really required or do we just need a smarter parser?
</comment><comment author="jasontedor" created="2016-02-28T23:18:02Z" id="189967579">@clintongormley I'm eventually going to close this pull request in favor of an iteration that @rjernst and I are working on together that _should_ support both `--foo` and `-E foo` syntax. This was a limitation of the parser that this pull request was based on but we are now going to move to the [jopt library](https://pholser.github.io/jopt-simple/) (think of it as [`getopt`](http://man7.org/linux/man-pages/man3/getopt.3.html) for Java) that _should_ enable suppose for both. The `-D` syntax _must_ be replaced because of the confusion between `-D` arguments for system properties to the JVM and `-D` arguments to Elasticsearch. Finally, it doesn't have to be `-E` but I feel very strongly that it must _not_ be `-D`. Does that answer your question @clintongormley?
</comment><comment author="clintongormley" created="2016-02-29T08:49:24Z" id="190107177">sounds good @jasontedor, thanks
</comment><comment author="rashidkpc" created="2016-03-12T00:14:10Z" id="195612036">@jasontedor just noticed the other pull was merged. Did you want to close this one?
</comment><comment author="jasontedor" created="2016-03-12T00:17:14Z" id="195612381">@rashidkpc The other PR was prep work that @rjernst and I felt would be best to have done in advance of this PR. This PR will either need to be rebased on master (now that that PR is in) or (more likely) closed in favor of a new PR that redoes this work on top of that PR.

To be clear, things are _still_ broken, and we have no CI detecting that.
</comment><comment author="rashidkpc" created="2016-03-12T04:17:40Z" id="195656847">Sorry, my mistake.

/me stops meddling :-)
</comment><comment author="jasontedor" created="2016-03-14T00:10:36Z" id="196086415">Superseded by #17088.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin install script not honouring JAVA_OPTS anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16790</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0.1 (yum repo)

**JVM version**: openjdk version "1.8.0_71"

**OS version**: CentOS 7.2

**Description of the problem including expected versus actual behavior**:

The elasticsearch plugin install script is not honouring the JAVA_OPTS environment variables anymore. this is needed to provide a http proxy for puppet runs.

**Steps to reproduce**:
1. export JAVA="-Dhttp.proxyHost=contentproxy.example.com -Dhttp.proxyPort=3128 -Dhttps.proxyHost=contentproxy.example.com -Dhttps.proxyPort=3128"
2.  /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf
3. -&gt; timeout

please change the last line of the plugin script from:

```
eval "$JAVA"  -client -Delasticsearch -Des.path.home="\"$ES_HOME\"" $properties -cp "\"$ES_HOME/lib/*\"" org.elasticsearch.plugins.PluginManagerCliParser $arg
```

to

```
eval "$JAVA" $JAVA_OPTS -client -Delasticsearch -Des.path.home="\"$ES_HOME\"" $properties -cp "\"$ES_HOME/lib/*\"" org.elasticsearch.plugins.PluginManagerCliParser $arg
```
</description><key id="135995057">16790</key><summary>plugin install script not honouring JAVA_OPTS anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">schlitzered</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-02-24T08:47:45Z</created><updated>2016-05-04T16:48:40Z</updated><resolved>2016-05-04T16:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-15T20:19:24Z" id="197004547">Relates #17121. We will support `ES_JAVA_OPTS` for this script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CorruptIndexException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16789</link><project id="" key="" /><description>**ELK version**: 
Elasticsearch: 2.2.0
Logstash: 2.2.2-1
Kibana: 4.4.1

**JVM version**:
java version "1.8.0_72"
Java(TM) SE Runtime Environment (build 1.8.0_72-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.72-b15, mixed mode)

**OS version**:
Debian 8.3 Jessie (clean install)

**Description of the problem**:
I have problem with my new elasticsearch, my instance crash after few minutes (http://pastie.org/10735193). I tried use lucane manually like here: https://github.com/elastic/elasticsearch/issues/16083 but no problems were detected with this index (http://pastie.org/10735194).
</description><key id="135974278">16789</key><summary>CorruptIndexException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rgolab</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2016-02-24T06:46:46Z</created><updated>2016-04-06T16:17:05Z</updated><resolved>2016-04-06T16:17:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-24T17:22:29Z" id="188360575">your checkindex says there are no segments in this index, can you check that you are running this on the right datapath / node?
</comment><comment author="rgolab" created="2016-02-25T06:34:13Z" id="188634884">I think i'm running it on the right datapath / node. List of files in that index directory: http://pastie.org/10736728
</comment><comment author="clintongormley" created="2016-02-28T22:33:57Z" id="189958609">Could you upload the corrupted file please?
</comment><comment author="clintongormley" created="2016-02-28T22:34:06Z" id="189958658">Or even the whole directory
</comment><comment author="s1monw" created="2016-02-29T16:51:59Z" id="190285475">you have to call commit first before you run checkindex I guess otherwise it will not be able to check it.
</comment><comment author="s1monw" created="2016-02-29T20:01:24Z" id="190358117">&gt; you have to call commit first before you run checkindex I guess otherwise it will not be able to check it.

err... I mean `_flush` sorry
</comment><comment author="clintongormley" created="2016-04-06T16:17:05Z" id="206446892">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple our ClusterService and TransportService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16788</link><project id="" key="" /><description>During the discussion around  #16746, some suggestion were made on how to decouple the dependency we have between ClusterSerivce and TransportService. This issue captures these:
- move the LocalNode stuff into TransportService
- add the reconnect task into a seperate class that is started seperately and depends on both transportservice and ClusterService
- for the connect stuff we do before we notify listeners should be maybe a listener too? I think it's a messed up dependency in ClusterService?
</description><key id="135928012">16788</key><summary>Decouple our ClusterService and TransportService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-02-24T02:09:00Z</created><updated>2016-03-10T10:52:53Z</updated><resolved>2016-03-10T10:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Testing: workaround HttpClient issue with IPv6 hostname verification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16787</link><project id="" key="" /><description>This commit works around an issue with hostname verification in HttpClient when using IPv6
addresses in URLs. When an IPv6 address is used in a URL it is typically wrapped with square
brackets. The hostname verifier for HttpClient does not recognize these as valid IPv6 addresses
and instead treats them as a DNS name. We wrap the default hostname verifier for this version
of HttpClient and strip brackets if we need to.

The corresponding issue in HttpClient is https://issues.apache.org/jira/browse/HTTPCLIENT-1698
but the fix has not been released yet in a stable version.
</description><key id="135917624">16787</key><summary>Testing: workaround HttpClient issue with IPv6 hostname verification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-24T01:06:17Z</created><updated>2016-02-25T13:12:29Z</updated><resolved>2016-02-25T13:12:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-24T01:27:42Z" id="188000079">@jaymode I left some comments.
</comment><comment author="jaymode" created="2016-02-24T15:03:00Z" id="188293169">@jasontedor @dakrone addressed the feddback
</comment><comment author="s1monw" created="2016-02-24T17:23:47Z" id="188361000">can you add a note to the http-client dependency in the gradle file to ensure we remove the hacks once we upgrade ?
</comment><comment author="jaymode" created="2016-02-25T11:39:56Z" id="188745995">added the comment in the version.properties file
</comment><comment author="jasontedor" created="2016-02-25T11:43:41Z" id="188747836">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove es.useLinkedTransferQueue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16786</link><project id="" key="" /><description>This commit removes the system property "es.useLinkedTransferQueue" that
defaulted to false and was used to control the queue implementation used
in a few places.
</description><key id="135915217">16786</key><summary>Remove es.useLinkedTransferQueue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-24T00:52:57Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-02-24T01:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-24T01:02:37Z" id="187994874">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix python script filename extension</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16785</link><project id="" key="" /><description>Python scripts need to be saved as *.py, not *.python
</description><key id="135900898">16785</key><summary>Fix python script filename extension</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsem</reporter><labels><label>docs</label></labels><created>2016-02-23T23:36:05Z</created><updated>2016-02-28T22:03:49Z</updated><resolved>2016-02-28T22:03:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T22:03:49Z" id="189954493">thanks @dsem 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort with a closure comparator doesn't work from groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16784</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**:openjdk version "1.8.0_66"
OpenJDK Runtime Environment (build 1.8.0_66-b17)
OpenJDK 64-Bit Server VM (build 25.66-b17, mixed mode)

**OS version**: FreeBSD 10

**Description of the problem including expected versus actual behavior**: I would like to sort a list from a groovy script with a custom comparator. It fails.
I've already tried to disable groovy sandbox by setting
script.groovy.sandbox.enabled: false

**Steps to reproduce**:
Put this into a groovy script and call it:
`list=[1,2,3,4,5]
list.sort{ a, b -&gt;
    if (a&gt;b)
        return -1
    else if (a&lt;b)
        return 1
    else
        return 0

}`

**Provide logs (if relevant)**:
`2016-02-2321:52:33,259][INFO` ][rest.suppressed          ] /a/b/c/_update Params: {index=a, id=b, type=c}
RemoteTransportException[[dev00][192.168.0.170:9301][indices:data/write/update[s]]]; nested: IllegalArgumentException[failed to execute script]; nested: ScriptException[failed to run file script [report] using lang [groovy]]; nested: NoClassDefFoundError[sun/reflect/MethodAccessorImpl]; nested: ClassNotFoundException[sun.reflect.MethodAccessorImpl];
Caused by: java.lang.IllegalArgumentException: failed to execute script
    at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:256)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:196)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:79)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:164)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:65)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:249)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:245)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ScriptException[failed to run file script [report] using lang [groovy]]; nested: NoClassDefFoundError[sun/reflect/MethodAccessorImpl]; nested: ClassNotFoundException[sun.reflect.MethodAccessorImpl];
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:318)
    at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:251)
    ... 12 more
Caused by: java.lang.NoClassDefFoundError: sun/reflect/MethodAccessorImpl
    at sun.misc.Unsafe.defineClass(Native Method)
    at sun.reflect.ClassDefiner.defineClass(ClassDefiner.java:63)
    at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:399)
    at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:394)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:393)
    at sun.reflect.MethodAccessorGenerator.generateMethod(MethodAccessorGenerator.java:75)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:53)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019)
    at groovy.lang.Closure.call(Closure.java:426)
    at groovy.util.ClosureComparator.compare(ClosureComparator.java:39)
    at java.util.TimSort.countRunAndMakeAscending(TimSort.java:356)
    at java.util.TimSort.sort(TimSort.java:234)
    at java.util.Arrays.sort(Arrays.java:1512)
    at java.util.ArrayList.sort(ArrayList.java:1454)
    at java.util.Collections.sort(Collections.java:175)
    at org.codehaus.groovy.runtime.DefaultGroovyMethods.sort(DefaultGroovyMethods.java:8476)
    at org.codehaus.groovy.runtime.DefaultGroovyMethods.sort(DefaultGroovyMethods.java:8438)
    at org.codehaus.groovy.runtime.dgm$566.doMethodInvoke(Unknown Source)
    at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:218)
    at ae5d43c5eafaae31062b2722aa92b85ba22c61c7.run(ae5d43c5eafaae31062b2722aa92b85ba22c61c7:43)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:311)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:308)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: sun.reflect.MethodAccessorImpl
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:677)
    at groovy.lang.GroovyClassLoader$InnerLoader.loadClass(GroovyClassLoader.java:425)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:787)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:775)
    ... 42 more

`
</description><key id="135863264">16784</key><summary>Sort with a closure comparator doesn't work from groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bra-fsn</reporter><labels /><created>2016-02-23T20:59:29Z</created><updated>2016-02-23T22:33:12Z</updated><resolved>2016-02-23T22:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bra-fsn" created="2016-02-23T22:33:00Z" id="187944817">Ah, the upcoming 2.2.1 has the needed fix:
https://github.com/elastic/elasticsearch/commit/e63c87c14a201522a5b2290f6ac435c2fb791817
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement stronger external versioning scheme</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16783</link><project id="" key="" /><description>Per https://www.elastic.co/guide/en/elasticsearch/guide/current/optimistic-concurrency-control.html: "Elasticsearch checks that the current _version is less than the specified version". This still leaves room for concurrency issues. Eg:
1. Client A reads a document with version V0
2. Client B reads the same document, version V0
3. Client A writes back the document as version V1
4. Client B writes back the document as version V2

In the above example, the last write can overwrite the change made by client A. This does not happen with the regular (non-external) versioning scheme. In order to do full versioning check, the system needs to know both the previous version and the new version. 

So, can we allow optionally specifying the previous version in the PUT request? Eg:

PUT /website/blog/2?version=5&amp;prev_version=3&amp;version_type=external

If prev_version is unspecified then ES behaves as it does today. But if it is specified then ES should check if the current _version matches the specified prev_version, as opposed to checking if the current _version is less than the new version.
</description><key id="135833908">16783</key><summary>Implement stronger external versioning scheme</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">koenoki</reporter><labels /><created>2016-02-23T19:14:29Z</created><updated>2016-02-24T19:59:56Z</updated><resolved>2016-02-24T19:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-24T06:27:31Z" id="188100936">Maybe this means you should not use external versioning? External versioning helps with keeping elasticsearch in sync with another system that is responsible for producing the version numbers. So overriding a document whenever the version increases is the right thing to do.
</comment><comment author="koenoki" created="2016-02-24T19:59:56Z" id="188431552">Thinking about this some more, what you wrote makes sense. In the scenario that I had in mind, the last write was data transformation which didn't increment the version in the master DB, so the last write should continue to use V0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Fix extra config files to work with more than one file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16782</link><project id="" key="" /><description>The extra config copy task for integ tests was previously overwriting
the source and destination for each file to be copied, leaving only the
last. This fixes it to create sub copy specs (which is what was
originally intended) for each file.
</description><key id="135777545">16782</key><summary>Build: Fix extra config files to work with more than one file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-23T15:55:08Z</created><updated>2016-02-29T14:01:02Z</updated><resolved>2016-02-25T19:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-25T17:09:28Z" id="188881733">Left minor comment. LGTM
</comment><comment author="tlrx" created="2016-02-29T08:44:00Z" id="190104910">@rjernst Thanks a lot
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch systemd configuration values getting replaced on update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16781</link><project id="" key="" /><description>Hello,

We're running Elasticsearch 2.2 on RHEL7, with the `http://packages.elasticsearch.org` Yum repository.

**The problem:**
The Elasticsearch systemd configuration file seems to be getting overridden / replaced with default values on update. Fixing this requires manual intervention after each version change. Keeping these default values causes our Elasticsearch instance to crash after a while.

We're using the following configuration in /usr/lib/systemd/system/elasticsearch.service:
LimitNOFILE=256000
LimitMEMLOCK=infinity

After an update, these values are replaced with the following:
LimitNOFILE=65535
# LimitMEMLOCK=infinity

Repository path:
elasticsearch/distribution/src/main/packaging/systemd/elasticsearch.service

Is it supposed to be like this? Are we doing something wrong?

Thanks,

Regards,
Robert
</description><key id="135709844">16781</key><summary>Elasticsearch systemd configuration values getting replaced on update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rbw0</reporter><labels><label>:Packaging</label><label>bug</label><label>stalled</label></labels><created>2016-02-23T11:23:53Z</created><updated>2016-12-18T17:21:54Z</updated><resolved>2016-12-18T17:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-25T21:08:58Z" id="188990071">I believe we may need to change the way the RPM is built to ensure `%config(noreplace)` is in the file, @rjernst I pinged you about this, do you know the gradle invocation to do that?
</comment><comment author="dakrone" created="2016-02-25T21:09:56Z" id="188990362">Related: http://www-uxsup.csx.cam.ac.uk/%7Ejw35/docs/rpm_config.html
</comment><comment author="dakrone" created="2016-02-25T22:07:34Z" id="189006836">Also related: https://bugzilla.redhat.com/show_bug.cgi?id=1296308
</comment><comment author="rbw0" created="2016-03-01T09:18:28Z" id="190628952">I can help look into this
</comment><comment author="rbw0" created="2016-03-01T09:19:03Z" id="190629059">if you want that is, let me know :)
</comment><comment author="dakrone" created="2016-03-02T16:08:06Z" id="191305279">@rbw0 that would be great, if I recall @rjernst saying, we need to change the gradle calling of the rpm package to include the noreplace. I think this should be a pretty straightforward change, the hardest part is figuring out where it should go :)
</comment><comment author="rjernst" created="2016-03-03T00:24:35Z" id="191506647">I have a simple change here (6dffaba) that I have confirmed "works" as far as the attributes are set on deb and rpm. However, I don't want to put up a PR until we can actually test with vagrant. So this is blocked on #16854.
</comment><comment author="jasontedor" created="2016-03-18T19:34:46Z" id="198512711">@rjernst I picked 6dffaba2483a445845266255cab9c584c3681d28 into #17197 where I included a test.
</comment><comment author="Xylakant" created="2016-03-22T16:51:52Z" id="199902246">The proper way to solve this is to use systemd's override capabilities which are designed to solve this issue. Do not modify the rpm-provided file, but provide an override snippet in /etc/systemd/system/elasticsearch.service.d/&lt;pick a filename&gt;. The file should look somewhat like this

```
[Service]
LimitNOFILE=256000
LimitMEMLOCK=infinity
```

Since the override snippet is not part of the package it won't get overwritten on update. See the systemd docs and the excellent digitalocean tutorial at https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files
</comment><comment author="jasontedor" created="2016-03-25T00:45:07Z" id="201086692">&gt; The proper way to solve this is to use systemd's override capabilities which are designed to solve this issue.

Yes, that's the plan, we just need to add documentation and tests for this. Thanks for the link, I think that it will be helpful for some.
</comment><comment author="jasontedor" created="2016-03-25T00:50:27Z" id="201087335">&gt; I picked 6dffaba2483a445845266255cab9c584c3681d28 into #17197 where I included a test.

@rjernst I backed this out of #17197; I ran into some discrepancies between RPM 4.8.x on CentOS 6 and RPM 4.11.y on CentOS 7 in how it handles `/etc/elasticsearch/scripts` and I think it'd be better to more carefully investigate the cause of the discrepancies in a separate pull request.
</comment><comment author="seang-es" created="2016-12-02T21:06:30Z" id="264562651">java.options is affected by this as well.  Will fixes taking advantage of systemd's override capability affect that file?</comment><comment author="jasontedor" created="2016-12-18T17:21:54Z" id="267833853">This is covered in the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.0/setting-system-settings.html#systemd) now.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Three data nodes die</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16780</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:v1.7.3

**JVM version**:v1.7.0_55

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

hs_err_pid27639.log&#65306;https://github.com/jqsl2012/helloGit/blob/master/es_die_log.zip

 A fatal error has been detected by the Java Runtime Environment:

  SIGSEGV (0xb) at pc=0x00002b233569732b, pid=22429, tid=47464825222912

 JRE version: Java(TM) SE Runtime Environment (7.0_55-b13) (build 1.7.0_55-b13)
 Java VM: Java HotSpot(TM) 64-Bit Server VM (24.55-b03 mixed mode linux-amd64 compressed oops)
 Problematic frame:
 J  org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock()V

 Core dump written. Default location: /home/yimr/lamp/elasticsearch/bin/core or core.22429 (max size 1 kB). To ensure a full core dump, try "ulimit -c unlimited" before starting Java again

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="135674597">16780</key><summary>Three data nodes die</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jqsl2012</reporter><labels /><created>2016-02-23T08:53:23Z</created><updated>2016-02-24T03:47:58Z</updated><resolved>2016-02-23T23:22:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-23T23:22:21Z" id="187961963">JVM crashes are almost always due to JVM bugs. I took a look at your logs, I see the segfaults and nothing jumps out at me as far as dangerous JVM flags or the like. I recommend upgrading your JVM at a minimum, at least to the latest 7u release but preferably to the latest 8u release.
</comment><comment author="jqsl2012" created="2016-02-24T03:47:58Z" id="188049632">Thanks for your help
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CANON_EQ - PatternSyntaxException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16779</link><project id="" key="" /><description>Hi all.

I have a problem about CANON_EQ.
When i using CANON_EQ flag in aggregation with Turkish character like : "&#246;&#287;retmen"
I always get this error: "nested: PatternSyntaxException[Unclosed group near index 34\n(?:o&#776;g|&#246;g|og&#776;(?:g&#774;r|&#287;r|gr&#774;)etmen"

This is my agg:
      "aggs": {
        "JobCategory_original": {
          "terms": {
            "field": "jobCategories.nameWithId",
            "size": 3,
            "include": {
              "pattern": "._&#246;&#287;retmen._",
              "flags": "CANON_EQ|CASE_INSENSITIVE|UNICODE_CASE"
            }
          }
        }
      }

If a delete CANON_EQ in flags then everything's ok. Because this character ")" is missing and i think that reason i get this error. How can i handle this issue? 
</description><key id="135662131">16779</key><summary>CANON_EQ - PatternSyntaxException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GokGokalp</reporter><labels /><created>2016-02-23T07:43:02Z</created><updated>2016-02-25T19:37:53Z</updated><resolved>2016-02-25T19:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-02-25T17:55:26Z" id="188903904">I tried a very basic case with ES 2.2 and wasn't able to replicate.

To help isolate this, can you:
1. Identify what version you're using
2. Post a gist `curl` example with a document and full query that has the issue?
</comment><comment author="GokGokalp" created="2016-02-25T19:19:08Z" id="188938305">Thanks for reply. 
We're currently using 1.7.1. 
We started to upgrade process yesterday. I hope this issue and others will fix with 2.2.
</comment><comment author="eskibars" created="2016-02-25T19:37:53Z" id="188946303">OK, I'm going to go ahead and close the issue.  If you're having issues after your upgrade, we can reopen.  Just post your `curl` reproduction at that time if that's the case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2 posting highlighter does not work when highlighting fields different than searched</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16778</link><project id="" key="" /><description>**Elasticsearch version**:2.2

**JVM version**:1.7

**OS version**:Win10

**Description of the problem including expected versus actual behavior**: I search on a composite field populated with copy_to while highlight in individual fields contributing to my composite field. I use posting highlighter and set `require_field_match` to false. It used to work with 1.7 and it still works with plain highlighter in 2.2 but not with posting highlighter. My individual fields which I am highlighting have `"index_options": "offsets"` while my composite field do not. 

Postings highlighter works if I highlight and search the same field

Here is an example of my highlight options on a query:

``` javascript
{
  "number_of_fragments": 0,
  "require_field_match": false,
  "type": "postings",
  "fields": {
    "additionalInfo": {},
    "advisor.team.acronym": {},
    "comments": {},
    "description": {},
    "engagements.title": {},
    "summary": {},
    "teams.team.acronym": {}
  }
}
```
</description><key id="135629037">16778</key><summary>2.2 posting highlighter does not work when highlighting fields different than searched</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Highlighting</label></labels><created>2016-02-23T04:30:47Z</created><updated>2016-02-23T23:08:40Z</updated><resolved>2016-02-23T23:08:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-23T07:38:47Z" id="187585464">can you please post your queries, what works and what doesn't?
</comment><comment author="roytmana" created="2016-02-23T15:42:53Z" id="187754433">@javanna here is one query that used to work in 1.x but does not work now. 

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "search": {
              "query": "fraud",
              "operator": "and"
            }
          }
        }
      ],
      "should": [
        {
          "match": {
            "search": {
              "query": "fraud",
              "type": "phrase",
              "slop": 1,
              "boost": 1.2
            }
          }
        },
        {
          "match": {
            "search_shingle": {
              "query": "fraud",
              "operator": "or",
              "boost": 1.2
            }
          }
        },
        {
          "match": {
            "search_keys": {
              "query": "fraud",
              "operator": "or",
              "boost": 10
            }
          }
        }
      ]
    }
  },
  "from": 0,
  "size": 20,
  "highlight": {
    "number_of_fragments": 0,
    "type": "postings",
    "require_field_match": false,
    "fields": {
      "additionalInfo": {},
      "advisor.team.acronym": {},
      "comments": {},
      "description": {},
      "engagements.jobCode.text": {},
      "engagements.title": {},
      "engagements.productNumber.text": {},
      "mandateAuthority.authorityNumber.text": {},
      "mandateAuthority.title": {},
      "notes.content": {},
      "parties.position.member.name": {},
      "parties.position.orgUnit.acronym": {},
      "parties.position.orgUnit.compositeName": {},
      "sourceIdentifier.text": {},
      "summary": {},
      "teams.stakeholders.appointment.team.acronym": {},
      "teams.team.acronym": {}
    }
  },
  "suggest": {
    "text": "fraud",
    "phrase": {
      "phrase": {
        "field": "search_shingle",
        "size": 4,
        "max_errors": 3,
        "gram_size": 2,
        "direct_generator": [
          {
            "field": "search",
            "suggest_mode": "popular",
            "min_word_length": 3
          }
        ],
        "highlight": {
          "pre_tag": "&lt;em&gt;",
          "post_tag": "&lt;/em&gt;"
        }
      }
    }
  },
  "sort": [
    {
      "_score": {
        "order": "desc",
        "missing": "_last"
      }
    },
    {
      "sourceDate": {
        "order": "desc",
        "missing": "_last"
      }
    },
    {
      "id": {
        "order": "desc",
        "missing": "_last"
      }
    }
  ]
}
```

if I switch to `plain` highlighter it works
if is switch field in first match statement from `search` (my composite copy_to field without offsets) to `summary` (one of the fields contributing to the `search` field) it will highlight that one field

is it possible that postings does not honor `require_field_match`?

please let me know if it all looks fine to you and you can't pinpoint the issue and I will try to find time to have a recreation today (pain to do on windows with curl parameters not working properly)

Alex
</comment><comment author="javanna" created="2016-02-23T16:29:19Z" id="187774369">&gt; is it possible that postings does not honor require_field_match?

Yes that is definitely the problem. Here is the original PR that caused this breaking change: https://github.com/elastic/elasticsearch/pull/11077#issuecomment-102494447 . We also listed the breaking change in the migration guide here: https://github.com/elastic/elasticsearch/blob/master/docs/reference/migration/migrate_2_0/search.asciidoc#postings-highlighter-doesnt-support-match_phrase_prefix .

I suggest either searching and highlighting the same fields, or providing a specific highlight query that differs from the search one. Otherwise use the plain highlighter instead, which still supports `require_field_match`.
</comment><comment author="roytmana" created="2016-02-23T16:49:59Z" id="187785877">@javanna I created a recreation https://gist.github.com/roytmana/a2d6efd307c5aa86210e but I see it is a feature not a bug :-( Any chance it will be relaxed on Lucene side or it is pretty much done deal and I need to look for a work around? I guess I could add these fields to should portion of my query using fastest type of match query keeping everything else intact

this is such a common case when searching "most" of your data :-( when you need to search over dozens of fields and combine it with shingles etc queries get very heavy and keeping track of all fileds involved labor intensive and spread across several layers. 

plain highlighter is good deal slower 

not being able to highlight make copy_to even less useful (no boosting of contributing fields, no control over offset gaps for contributing fields, no highlighting ...) 
</comment><comment author="javanna" created="2016-02-23T22:10:35Z" id="187938996">hi @roytmana I am sorry to bring you bad news, but I don't see `require_field_match` being supported by the lucene postings highlighter. I think it was wrong to support in the first place in the elasticsearch postings highlighter, it made things slow and a lot more complicated. There are easy workarounds though, either search and highlight the same fields, or provide a specific highlight query against the fields that need to be highlighted. Can you let us know if these are good enough please?
</comment><comment author="roytmana" created="2016-02-23T23:08:40Z" id="187957370">Thank you @javanna I am closing it then...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add example for require_field_match to highlighting docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16777</link><project id="" key="" /><description>I think showing how this configuration option is used explicitly is better than inferring that it's another option that can be passed.
</description><key id="135613637">16777</key><summary>Add example for require_field_match to highlighting docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radar</reporter><labels><label>docs</label></labels><created>2016-02-23T02:52:57Z</created><updated>2016-02-28T20:34:01Z</updated><resolved>2016-02-28T20:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T20:34:01Z" id="189942075">thanks @radar 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use System#lineSeparator and not system property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16776</link><project id="" key="" /><description>This commit replaces a use of the system property "line.separator" and
replaces it with a built-in dedicated method that provides the same
value.
</description><key id="135595941">16776</key><summary>Use System#lineSeparator and not system property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-23T01:12:01Z</created><updated>2016-02-25T17:46:56Z</updated><resolved>2016-02-25T17:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-25T17:17:55Z" id="188885618">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document cpu usage in _cat/nodes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16775</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/15302 added cpu usage to the _cat/nodes endpoint, but it wasn't documented, so I added that.
</description><key id="135589707">16775</key><summary>Document cpu usage in _cat/nodes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels><label>:CAT API</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-02-23T00:34:29Z</created><updated>2016-02-24T00:43:22Z</updated><resolved>2016-02-24T00:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-24T00:43:22Z" id="187990148">Thanks @henrikno, I've integrated this into master in a996e218590134694638efc9807e5e40c6043daf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ability to disable Netty gathering writes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16774</link><project id="" key="" /><description>Java NIO has the notion of gathering writes. These are writes that
gather data from multiple buffers into a single channel. These gathering
writes in Netty have been enabled by default with the possibility to
disable them using "es.netty.gathering". This flag was added in case
having gathering writes on by default did not work out. We have not
published this ability and sufficient time has passed to render
judgement that using gathering writes is okay.

Relates #7811
</description><key id="135578299">16774</key><summary>Remove ability to disable Netty gathering writes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Network</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T23:30:26Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-02-23T00:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-22T23:45:24Z" id="187441093">LGTM, can you also mark this as breaking and add a blurb to the migration 5.0 guide?
</comment><comment author="jasontedor" created="2016-02-23T00:11:53Z" id="187446302">&gt; LGTM, can you also mark this as breaking and add a blurb to the migration 5.0 guide?

@dakrone Added in e432e09fff3ec763292e081867b7629dfe994dc6; can you take a look?
</comment><comment author="dakrone" created="2016-02-23T00:14:02Z" id="187446701">+1
</comment><comment author="s1monw" created="2016-02-23T00:15:10Z" id="187446922">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add issue template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16773</link><project id="" key="" /><description>This pull requests adds an issue template for contributors that open bug
reports or feature requests.

Additionally, this pull request adds a `.github` subdirectory to the project
and moves the `CONTRIBUTING.md` file to that directory.
</description><key id="135572913">16773</key><summary>Add issue template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T23:01:19Z</created><updated>2016-03-10T18:35:46Z</updated><resolved>2016-02-22T23:06:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-22T23:01:39Z" id="187428744">Thanks to @dakrone for early feedback and iteration on this. Let's get _something_ posted and see how it works out.
</comment><comment author="dakrone" created="2016-02-22T23:01:53Z" id="187428870">+1, LGTM
</comment><comment author="bleskes" created="2016-02-22T23:06:58Z" id="187430276">I don't know the intricacies of how this will influence the github UI , but +1 - it looks great. Let's just see.
</comment><comment author="shreejay" created="2016-02-22T23:37:09Z" id="187439075">Link to `TESTING.asciidoc` is broken. Move it to `.github` folder or change the path? 
</comment><comment author="jasontedor" created="2016-02-23T00:09:43Z" id="187445938">@shreejay The `.github` folder is meant _only_ for `CONTRIBUTING.md`, `ISSUE_TEMPLATE.md`, and `PULL_REQUEST_TEMPLATE.md` as this is a feature provided by GitHub. I'll fix the broken link in the `CONTRIBUTING.md` doc; thank you for reporting.
</comment><comment author="shreejay" created="2016-02-23T17:58:32Z" id="187818597">Cool. Did not know about .github folder. Thanks. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consider adding a quorum_timeout setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16772</link><project id="" key="" /><description>The functionality of this setting will cover the use cases where one would like for the indexing operation to fail faster in case of shards quorum not being met: `Not enough active copies to meet write consistency of [QUORUM] (have X, needed Y)`.

In a case when most of the shards are allocated with their replicas most of the documents can be indexed immediately. But the documents that go to the shards without replicas allocated (there is no quorum) just have to wait for the general timeout to expire which makes it hard on the client side to manage the indexing process.

Setting `timeout=1ms` on the indexing request will potentially interfere with a normal, slower indexing process and could timeout sooner for nothing.
</description><key id="135569131">16772</key><summary>Consider adding a quorum_timeout setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2016-02-22T22:44:48Z</created><updated>2016-03-08T12:40:26Z</updated><resolved>2016-03-08T12:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2016-02-22T22:48:33Z" id="187423790">Similar discussion:  https://github.com/elastic/elasticsearch/issues/4739
</comment><comment author="bleskes" created="2016-02-23T00:20:11Z" id="187447987">The timeout parameter actually controls how long you wait for the quorum  to be available.  We do not have a timeout on the request to the replicas/primary. Can you elaborate a bit more ? 
</comment><comment author="astefan" created="2016-03-08T12:40:26Z" id="193768113">Ok for now to use the `timeout` parameter with a very low value. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make extraConfig task works with multiple files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16771</link><project id="" key="" /><description>This commit fixes the `configureExtraConfigFilesTask` so that it works when multiple files are specified in a build.gradle file like in:

```
extraConfigFile nodeKeystore.name, nodeKeystore
extraConfigFile clientTruststore.name, clientTruststore
```

Right now, only the latest processed file is copied over.

Not sure this is the best way to fix this, but it seems that a `Copy` task must be created for every extra config file to copy + keep a reference to the file in the `node.config.extraConfigFiles` (and not a ref to the `Map.Entry` object).
</description><key id="135568199">16771</key><summary>Make extraConfig task works with multiple files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>test</label></labels><created>2016-02-22T22:41:28Z</created><updated>2016-02-29T08:42:45Z</updated><resolved>2016-02-23T16:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-23T07:44:49Z" id="187586717">Copy tasks can handle many files fine. I think this is just a misconfiguration because we keep calling into at the top level of the copy config. We need to instead call from first, then into.
</comment><comment author="tlrx" created="2016-02-23T08:12:39Z" id="187596471">&gt; Copy tasks can handle many files fine.

I agree. But it didn't work.

&gt; I think this is just a misconfiguration

Can you please then provide a good configuration? I'll be happy to use it.
</comment><comment author="rjernst" created="2016-02-23T15:55:29Z" id="187761725">I opened a PR: #16782
</comment><comment author="tlrx" created="2016-02-23T16:54:28Z" id="187788543">closed in favor of #16782 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose http address in cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16770</link><project id="" key="" /><description>We expose a lot of information like IP address and port but never
expose the http address/ip:port in the CAT API. It's nice to have it
there too since otherwise json parsing is required to get this information
</description><key id="135548556">16770</key><summary>Expose http address in cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T21:25:37Z</created><updated>2016-03-02T16:35:07Z</updated><resolved>2016-02-22T22:20:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-22T21:57:13Z" id="187404551">LGTM
</comment><comment author="javanna" created="2016-03-01T14:33:52Z" id="190743308">I have second thoughts about exposing the http address as part of `_cat/nodeattrs`. I am not sure the http address belongs to the node attributes here:

```
curl 'localhost:9200/_cat/nodeattrs'
Redneck     127.0.0.1 127.0.0.1 client       true
Redneck     127.0.0.1 127.0.0.1 data         false
Redneck     127.0.0.1 127.0.0.1 http_address 127.0.0.1:9201
Steel Raven 127.0.0.1 127.0.0.1 client       true
Steel Raven 127.0.0.1 127.0.0.1 data         false
Steel Raven 127.0.0.1 127.0.0.1 http_address 127.0.0.1:9202
Phage       127.0.0.1 127.0.0.1 http_address 127.0.0.1:9200
```

I think exposing the http address as part of `_cat/nodes` is good enough. Thoughts?
</comment><comment author="clintongormley" created="2016-03-01T15:02:27Z" id="190757639">@javanna makes sense.  httpaddr could be exposed in nodeattrs in the same way as host and port, but i wouldn't expose it in the same way as the other attributes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Score Test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16769</link><project id="" key="" /><description>Added a test for the use of the variable _score in Painless.
</description><key id="135501543">16769</key><summary>Score Test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>test</label></labels><created>2016-02-22T18:30:00Z</created><updated>2016-04-07T08:37:44Z</updated><resolved>2016-04-06T17:00:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-22T18:37:27Z" id="187310616">Now that this is a module, could we make this a REST test instead since this is essentially an integration test? (and rest tests are a good place for those)
</comment><comment author="rjernst" created="2016-02-22T18:56:52Z" id="187319762">@dakrone I don't think this should be a rest test. It is testing the internals/java api of painless in how it supports _score. Really this should be a pure unit test, it just needs some more work to have the correct mocks. In the meantime, this is a single node test case (which unfortunately a large number of our unit tests are, but that is still better than having high level tests like fantasy land or rest tests for something as low level as this).
</comment><comment author="nik9000" created="2016-02-25T17:28:00Z" id="188891853">I think this is fine as is. It'd be better as a unit test but I can live with this and I like having _score tested.
</comment><comment author="dakrone" created="2016-02-25T17:57:11Z" id="188904435">Sure, I'm fine either way, LGTM
</comment><comment author="dakrone" created="2016-04-06T16:50:28Z" id="206461630">@jdconrad want to merge this in?
</comment><comment author="jdconrad" created="2016-04-06T17:00:20Z" id="206464742">Oops, forgot to close this.  It actually got merged in with (https://github.com/elastic/elasticsearch/pull/17428).  Closing now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing format parameter in RangeFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16768</link><project id="" key="" /><description>The 1.7 documentation for Range Filter states that it accepts a 'format' parameter: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/query-dsl-range-filter.html#_date_options_2

However, the format parameter cannot be found on the RangeFilterBuilder:  https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java
</description><key id="135479688">16768</key><summary>Missing format parameter in RangeFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seallison</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-02-22T17:11:44Z</created><updated>2016-03-02T09:48:41Z</updated><resolved>2016-03-01T09:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-01T09:32:52Z" id="190632890">@seallison thanks for bringing this up. The `format` parameter is supported by the json parser in 1.7, so using the query DSL should work here. The 2.x branch also already has a setter for the `format` parameter in the RangeQueryBuilder (FilterBuilders are merged with QueryBuilders starting with 2.x). @clintongormley I don't think we still add non-critical changes to the 1.7 branches, so I think this can be closed, but please reopen if I'm missing something.
</comment><comment author="clintongormley" created="2016-03-02T09:48:41Z" id="191160997">@cbuescher agreed - thanks for looking at it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed propagation of autoGeneratedId on document creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16767</link><project id="" key="" /><description>canHaveDuplicates was used as autoGeneratedId and so no DocumentAlreadyExists exception was thrown (optimization introduced here: https://github.com/elastic/elasticsearch/commit/f45e6ae3f9643161424a4169d272bfa8588c202e) even when the id was not auto generated.
</description><key id="135426146">16767</key><summary>fixed propagation of autoGeneratedId on document creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v2.3.0</label></labels><created>2016-02-22T14:06:09Z</created><updated>2016-03-02T09:14:04Z</updated><resolved>2016-02-29T15:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-22T16:37:43Z" id="187256970">Good catch. Note though that it wasn't an optimization but rather a protection against throw an DocumentAlreadyExists where we shouldn't. This entire construction is too brittle and was removed in master. 

It also seems we have a gap in our testing as this wasn't caught. We should fix that as well.. Looking the test that was added in the commit you mention, I think the easierst would be to replcae the code [here](https://github.com/elastic/elasticsearch/commit/f45e6ae3f9643161424a4169d272bfa8588c202e#diff-38d197a0e6b168e3ca10ca0553b238adR94), up and including the refersh command with 

```
        ArrayList&lt;IndexRequestBuilder&gt; requests = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt; numDocs; i++) {
            XContentBuilder doc = jsonBuilder().startObject().field("foo", "bar").endObject();
            requests.add(client().prepareIndex("index", "type").setSource(doc));
        }

        indexRandom(true, false, requests);

```

Can you modify the test and see it fails without your fix? 
</comment><comment author="seut" created="2016-02-24T10:38:20Z" id="188187024">@bleskes 
The test you mention is using autoGeneratedId's so this test will never hit the bug.
Also the bug was problematic for us in a different use-case, it was not happening during network failures like the fix tries to protect one from (retrying without bubbling the exception). In our case it was happening on the 2.1 release during concurrent inserts on a shard which was in post_recovery state so this code was run (retry due to illegal shard state): https://github.com/elastic/elasticsearch/blob/2.1/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java#L585

Seems like this code was removed on 2.x so I'm not able to write a test to reproduce this issue.

In case of network failures, I don't really get why retrying without exception bubbling is only done for records with autoGeneratedId's. I think if something like this exists, it should kick in always.
But as you mentioned (and I already knew) this was removed at the master so we don't have to discuss about this anymore ;)

If you have another suggestion how to test this fix at 2.x, I'm trying to implement it for sure ;)
</comment><comment author="bleskes" created="2016-02-25T02:07:05Z" id="188561599">Oh, I see. To add a test for retrying documents that already exist, you'd need to add some documents before the partition and add create operations to the indexing for those. Last you need to check the response for those documents and test that they indeed fail. Does that make sense?
</comment><comment author="seut" created="2016-02-25T11:43:48Z" id="188747890">I've added a simple test which tests my fix. (will of course squash my fixup commit after review is done)

btw. yeah I know how to create docs with custom id's ;)
the ExceptionRetryIT test is a bit flaky, the exception is not always thrown (e.g. if the unlucky node has no shard, the unlucky node is the client node and so no remote transport, etc.) and also the edge-case is not always hit (document must be created before exception is triggered). This flakiness is maybe ok if the test is expecting no failures, but if another test should expecting failures, the test scenario must not be flaky. I wasn't able to create a non-flaky scenario in an easy way, and also the test I added is more concrete for the fix, no matter where the request flags are coming from.
</comment><comment author="bleskes" created="2016-02-29T14:20:39Z" id="190228414">&gt; document must be created before exception is triggered

That's why I suggested creating some docs in advance. Anyway, I'm good with the test and I'll pull it in. Thanks again..
</comment><comment author="bleskes" created="2016-02-29T14:24:01Z" id="190229468">@seut the new file misses a license header which make our test fail. If you would like to add it (and rebase/squash on top on 2.x) that would be great. If not I'm happy to do it for you, but that will make git hub not see this PR as merged (but rather closed). If I don't hear otherwise by tomorrow I'll assume the latter and pull this in...
</comment><comment author="seut" created="2016-02-29T14:38:36Z" id="190235654">&gt; That's why I suggested creating some docs in advance.

Ah damn didn't thought about that and read this over..
</comment><comment author="seut" created="2016-02-29T14:41:31Z" id="190236663">@bleskes added license header and squashed commits
</comment><comment author="bleskes" created="2016-02-29T15:42:56Z" id="190262008">merged. Thanks @seut 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix grammar in warning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16766</link><project id="" key="" /><description /><key id="135410227">16766</key><summary>fix grammar in warning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rstruber</reporter><labels><label>docs</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T13:02:08Z</created><updated>2016-02-22T16:45:39Z</updated><resolved>2016-02-22T16:39:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-22T16:44:37Z" id="187259240">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to delete the document from script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16765</link><project id="" key="" /><description>According to [this discussion](https://discuss.elastic.co/t/is-it-possible-to-delete-a-document-from-a-script/42343), elasticsearch currently doesn't support deleting the document from the script, which is called on it.
It would make some stuff easier (just like scripts do, without the hassle of handling versions, changes and retries on the client side), so I create this issue, hoping that somebody will find this eventually useful enough to implement.
Thanks. 
</description><key id="135394567">16765</key><summary>Allow to delete the document from script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bra-fsn</reporter><labels /><created>2016-02-22T11:47:37Z</created><updated>2016-02-28T19:39:29Z</updated><resolved>2016-02-28T19:39:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T19:39:29Z" id="189931354">Actually it is possible to delete a document from an update script, and it is documented:

```
PUT t/t/1
{}

POST t/t/1/_update
{
  "script": "ctx.op='delete'"
}

GET t/t/1  # returns 404
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>min_doc_freq overwritten in direct generator for phrase suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16764</link><project id="" key="" /><description>The direct generator part of the phrase suggester documentation states that it's possible to define a `min_doc_freq` parameter in order to "improve quality by only suggesting high frequency terms". This makes a lot of sense as a direct generator is simply a term suggester that generate suggestions for the phrase suggester.

Unfortunately, though, this value is overwritten when invoking the underlying Lucene `DirectSpellChecker` from [DirectCandidateGenerator](https://github.com/elastic/elasticsearch/blob/2.2/core/src/main/java/org/elasticsearch/search/suggest/phrase/DirectCandidateGenerator.java#L115):

```
spellchecker.setThresholdFrequency(this.suggestMode == SuggestMode.SUGGEST_ALWAYS ? 0 : thresholdFrequency(frequency, dictSize));
```

ignoring the parameter provided by the user.

To add to the confusion, the other frequency parameter `max_term_freq` works as expected.

It seems to be wrong to overwrite the `thresholdFrequency` on the spellchecker because the spellchecker is provided with the `suggest_mode` and already contains the logic to return the right terms according to the mode's rules.
</description><key id="135390939">16764</key><summary>min_doc_freq overwritten in direct generator for phrase suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>:Suggesters</label><label>bug</label></labels><created>2016-02-22T11:31:12Z</created><updated>2017-05-19T07:06:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T19:36:11Z" id="189930926">@cbuescher has this already been dealt with in the suggester refactoring?
</comment><comment author="cbuescher" created="2016-02-29T13:01:21Z" id="190200941">@clintongormley no, this is internal logic in the DirectCandidateGenerator that hasn't been changes by the refactoring, so it should be treated as a separate issue. The line pointed out was added by this commit (8235b89e9cd4f222ee689d47ca2ef4686b7a6971), so I assume there is a reason in overwriting the threshold when the suggest mode is ALWAYS.
</comment><comment author="micpalmia" created="2016-02-29T14:06:03Z" id="190224464">The line has been _edited_ in the commit to have a separate logic when the suggest mode is `ALWAYS`, but shouldn't have been there in the first place, because whatever the suggest mode, that logic is already implemented at Lucene level (I think).
</comment><comment author="cbuescher" created="2016-02-29T15:33:39Z" id="190258473">@micpalmia you are right. Looking at the code, the spellchecker inside DirectCandidateGenerator seems to get reused to calculate suggestions for each term in the phrase. To me it looks like the overwrite of the original value is needed to take into account the different term frequencies of all tokens in the phrase. The original `min_doc_freq` still comes into play via `frequencyPlateau`, so I'm not sure this is a bug. Maybe @s1monw can help here. Do you experience a particular problem with the `min_doc_freq` parameter for a particular setting?  
</comment><comment author="micpalmia" created="2016-02-29T15:44:26Z" id="190262479">Yes, I noticed a problem trying to tune a phrase suggester (that's why I realized something was fishy). I'm going to try producing a working sample showing the issue.
</comment><comment author="s1monw" created="2016-02-29T16:20:26Z" id="190274861">@micpalmia we store the original frequency in the `DirectSpellchecker` ctor and then use it as the `frequencyPlateau` which is then in-turn returned from `thresholdFrequency(frequency, dictSize)` so it looks fine to me?
</comment><comment author="kaglowka" created="2016-06-08T14:18:39Z" id="224603645">I've been playing around with a perfectly usual suggester case and no matter what value I use as min_doc_freq (either ridiculously high integer or any float value), I get equal results.

I'm rather new to ElasticSearch so I don't exactly understand its inner mechanisms, but if the way min_doc_freq works is much subtler than that, maybe the documentation should be improved?

I'm quoting my exact query so all parameters are explicitly set, but the same occurs for default parameters.

```

{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "simple_phrase": {
      "phrase": {
        "field": "text_bigram",
        "confidence": 2.0,
        "smoothing": {
          "stupid_backoff": {
            "discount": 0.1
          }
        },
        "gram_size": 3,
        "direct_generator": [
          {
            "min_doc_freq": 1000000,
            "max_term_freq": 0.0001,
            "field": "text_raw",
            "suggest_mode": "always",
            "min_word_length": 1,
            "max_edits": 2
          }
        ],
        "real_word_error_likelihood": 0.95,
        "max_errors": 2,
        "size": 3
      },
      "text": "some text"
    }
  }
}
```
</comment><comment author="s1monw" created="2016-06-08T14:58:39Z" id="224616646">as said above with `"suggest_mode": "always",` the `"min_doc_freq": 1000000,` will be ignored since we always suggest!
</comment><comment author="azhuchkov" created="2017-05-19T07:06:51Z" id="302626470">I use `"suggest_mode": "missing"` and it seems that `min_doc_freq` is totally ignored.
Played with `accuracy` which i couldn't find documentation for: for values greater `0.9` the suggester stops advise anything.
I'm using Elasticsearch `v2.4.4` .</comment></comments><attachments /><subtasks /><customfields /></item><item><title>very uneven distribution of empty shards on multiple data paths if the free spaces are different</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16763</link><project id="" key="" /><description>I met this issue on ES-2.2.0,  all ES data nodes have multiple hard disks and no RAID scheme is utilized, they are just a bunch of disks. The usable spaces of those disks on same host are different due to some strange implementation details in XFS (even two XFS fs have same capacity, same size directories and files, they don't always have same free space, I guess it's produced by file creations and deletions).

I pre-create many indicies with replica set to 0,  50 shards for 50 data nodes. The initial empty shard is very small, just several KB disk usage,  ES surprisingly creates a lot of shards on the least used disks,  for example, if one of ten disks on a data node has 10KB more free space,  then it will contain about 8 more shards than other disks.  The least used disks will be run out because I use hourly index pattern,  all shards for a continuous hour range will go to the same disk.  But other disks on that host still have a lot of free space.

I checked a little the code, https://github.com/elastic/elasticsearch/blob/v2.2.0/core/src/main/java/org/elasticsearch/index/IndexService.java#L327, the IndexService instance is for per index, not a singleton, so "dataPathToShardCount" will always be empty due to zero replica for this index, so at https://github.com/elastic/elasticsearch/blob/v2.2.0/core/src/main/java/org/elasticsearch/index/shard/ShardPath.java#L231,  "count" will always be null,  class "ShardPath" always selects the least used disk to allocate shard,  but unluckily the empty shard is very small,  so a lot of shards will be allocated to the least used disk.

This issue isn't limited to zero replica use case,  the current ES logic will always prefer the least used disk and the very small empty shard always produces uneven distribution.

I feel the "dataPathToShardCount" map should be global to all indices on a node, not local to a single index,  but maybe there is better solution.

My workaround is to create RAID 0 for those disks on a single node, but RAID 0 is too risky if a host has about 10 disks, maybe I should choose RAID 6.
</description><key id="135359929">16763</key><summary>very uneven distribution of empty shards on multiple data paths if the free spaces are different</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Dieken</reporter><labels><label>:Allocation</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-22T09:17:39Z</created><updated>2016-05-18T07:39:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-22T16:53:17Z" id="187262539">I suspect that pre-allocating the indices is not working as well with the prediction of shard sizes added in https://github.com/elastic/elasticsearch/pull/11185, @mikemccand does that sound like a good culprit to you?
</comment><comment author="dakrone" created="2016-02-22T16:54:03Z" id="187263205">https://github.com/elastic/elasticsearch/pull/12947 may also be at work here too
</comment><comment author="mikemccand" created="2016-02-22T18:59:23Z" id="187320823">&gt; @mikemccand does that sound like a good culprit to you?

Yes.

&gt; the IndexService instance is for per index
&gt; 
&gt; I feel the "dataPathToShardCount" map should be global to all indices on a node, not local to a single index, but maybe there is better solution.

Oh this is bad: I agree, the `dataPathToShardCount` should (I and I intended it to be originally in #11185!) be across all indices, not just this one index.  Hrmph.

However, I think another fix by @s1monw allowed ES shard balancing to "see" individual `path.data` on a single node, and move shards off on of a node's `path.data` that was filling up even if the other `path.data`s on the node had plenty of free space?
</comment><comment author="Dieken" created="2016-02-23T02:28:03Z" id="187489212">@mikemccand,  the ability to move shards among different disks on a node is good to have,  we always can't assure different indices have similar sizes,  in fact they usually vary a lot due to hourly or daily traffic pattern.

But an initial even distribution is still very nice,  it can avoid as more shard moving as possible,  I wouldn't like to see ES node suddenly competes disk access with itself for shard moving, segment refresh, segment merge.

Oh, maybe I should just go to RAID :-)
</comment><comment author="mikemccand" created="2016-02-23T17:00:53Z" id="187791832">@Dieken Yeah I understand ... if you have ideas on how to fix the `dataPathToShardCount` to be across all indices instead, that would be great too ;)
</comment><comment author="danopia" created="2016-05-18T07:38:57Z" id="219949647">Any updates on this? I'm working with a write-heavy cluster and I value distributing disk throughput over file size. New indexes were just allocated with all shards a single path.disk out of the 4 due to unbalance in disk usage.

An API to relocate shards between path.datas would be enough to deal with this. Right now it looks like I have to shut down nodes and manually move folders.

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recollocation shard is not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16762</link><project id="" key="" /><description>Hello, 

I have a problem with Elasticsearch, one of my nodes failed and after this I wake up it and it is my cluster again and it is recognized by the other nodes inside the cluster. 

Aparently all is working correctly but the problem is that the shards don&#180;t allocate in the new node so in this way I only have two of three nodes in my cluster with data, and the cluster state is yellow because I have a replica shard that it is initializing all time and it is not posible to allocate in any node. 

---

| Node 1  with data |                 |  Node 2 with data |       | **Node 3 NO DATA** |

---

I have a replica shard that it is initializing too and It is imposible for me recollocate in any node with the directive reroute for the replica shard that it is initializing:

```
curl -XPOST 'localhost:9200/_cluster/reroute' -d '{
     "commands": [
        {
            "allocate": {
                "index": "'$INDEX'",
                "shard": '$SHARD',
                "node": "'$NODE'",
                "allow_primary": true
          }
        }
    ]
  }'
```

The configuration file that I have in this cluster for all nodes is:

```
cluster.name: elastic_16/12/2015

bootstrap.mlockall: true

gateway.type: local
gateway.recover_after_nodes: 1
gateway.recover_after_time: 5m
gateway.expected_nodes: 3


indices.recovery.max_bytes_per_sec: 100mb
discovery.zen.minimum_master_nodes: 2

discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["IP  node 1", "IP  node 2"]


path.logs: elasticsearch/logs
path.data: elasticsearch/data
path.work: elasticsearch/work
path.plugins: elasticsearch/plugins

http.cors.enabled: true
http.cors.allow-origin: /.*/
http.cors.allow-credentials: true 


index.routing.allocation.disable_allocation: false


```

The elasticsearch version that I have is the 1.4.4.
</description><key id="135347452">16762</key><summary>Recollocation shard is not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juandasgandaras</reporter><labels /><created>2016-02-22T08:11:58Z</created><updated>2016-02-25T18:05:22Z</updated><resolved>2016-02-25T18:05:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-02-25T18:05:22Z" id="188906560">Hi @juandasgandaras a ton has changed since 1.4.4, especially with respect to resiliency.  I really recommend you upgrade.  Additionally, feel free to open a thread over on our forums https://discuss.elastic.co/ if you want to discuss your specific current cluster issues, which is better suited for that type of situation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI Failure] IndexWithShadowReplicasIT.testIndexWithFewDocuments fails in ensureGreen after rolling restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16761</link><project id="" key="" /><description>See: http://build-us-00.elastic.co/job/es_core_master_window-2008/3150/

The seed is: 

```
gradle :core:integTest -Dtests.seed=4504E6449CC50308 -Dtests.class=org.elasticsearch.index.IndexWithShadowReplicasIT -Dtests.method="testIndexWithFewDocuments" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops" -Dtests.locale=pt-PT -Dtests.timezone=America/Guayaquil
```

However, it doesn't reproduce (for me). Failure looks like:

```
  1&gt; [2016-02-22 04:35:56,315][INFO ][org.elasticsearch.index  ] ensureGreen timed out, cluster state:
  1&gt; version: 15
  1&gt; state uuid: u2OTUHr5SsCTRt1n-ca8Hg
  1&gt; from_diff: false
  1&gt; meta data version: 7
  1&gt; nodes:
  1&gt;    {node_t2}{W6pxsvJnSv6IXHjWKb7rkQ}{local}{local[13]}[add_id_to_custom_path=&gt;false, mode=&gt;local]
  1&gt;    {node_t0}{ZIqZfWvXRfy2KIDS-fpZLQ}{local}{local[11]}[mode=&gt;local, add_id_to_custom_path=&gt;false], master
  1&gt;    {node_t1}{ahGc-_sERu-Jfi_-lXDCdw}{local}{local[12]}[add_id_to_custom_path=&gt;false, mode=&gt;local]
  1&gt; routing_table (version 9):
  1&gt; -- index [[test]]
  1&gt; ----shard_id [test][0]
  1&gt; --------[[test]][0], node[null], [P], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:24.666Z], details[node_left[SIl3slIcS7mTGRGrVjXXxA]]]
  1&gt; --------[[test]][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:23.410Z], details[node_left[ne2HfUu6Q8uW8bVUdJWXlA]]]
  1&gt; --------[[test]][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:24.058Z], details[node_left[o3SK-9vnQ76pRmWAEAsA3w]]]
  1&gt; routing_nodes:
  1&gt; -----node_id[W6pxsvJnSv6IXHjWKb7rkQ][V]
  1&gt; -----node_id[ahGc-_sERu-Jfi_-lXDCdw][V]
  1&gt; -----node_id[ZIqZfWvXRfy2KIDS-fpZLQ][V]
  1&gt; ---- unassigned
  1&gt; --------[[test]][0], node[null], [P], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:24.666Z], details[node_left[SIl3slIcS7mTGRGrVjXXxA]]]
  1&gt; --------[[test]][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:23.410Z], details[node_left[ne2HfUu6Q8uW8bVUdJWXlA]]]
  1&gt; --------[[test]][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2016-02-22T04:35:24.058Z], details[node_left[o3SK-9vnQ76pRmWAEAsA3w]]]
  1&gt; tasks: (0):
  1&gt;
```
</description><key id="135341902">16761</key><summary>[CI Failure] IndexWithShadowReplicasIT.testIndexWithFewDocuments fails in ensureGreen after rolling restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>jenkins</label><label>test</label></labels><created>2016-02-22T07:38:43Z</created><updated>2017-05-26T18:05:58Z</updated><resolved>2017-05-26T18:05:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2017-05-26T18:05:58Z" id="304351040">Shadow replicas have been removed</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How should I optimize cardinality aggregation usage using v2.1?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16760</link><project id="" key="" /><description>I only more than 7000 data, the deployment of four nodes, cardinality aggregation query will be down.

Here's an example of my  aggs:
{
    "from": 0,
    "size": 0,
    "aggregations": {
        "CreateDay": {
            "terms": {
                "field": "CreateDay",
                "size": 200
            },
            "aggregations": {
                "PV": {
                    "value_count": {
                        "field": "_index"
                    }
                },
                "UV": {
                    "cardinality": {
                        "field": "DeviceID",
                        "precision_threshold": 40000
                    }
                },
                "IpCount": {
                    "cardinality": {
                        "field": "IPAddress",
                        "precision_threshold": 40000
                    }
                }
            }
        }
    }
}
</description><key id="135333459">16760</key><summary>How should I optimize cardinality aggregation usage using v2.1?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeanLGr</reporter><labels /><created>2016-02-22T06:48:18Z</created><updated>2016-02-22T20:21:05Z</updated><resolved>2016-02-22T20:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-22T20:21:05Z" id="187367562">GitHub is reserved for bug reports and feature requests, but it looks like you just have an end-user question. Can you please repost this on the [Elastic Discourse forums](https://discuss.elastic.co)? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove obsolete optimize_single_shard settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16759</link><project id="" key="" /><description>I just bumped into some settings that I didn't know existed, which also slipped out the settings migration to the new settings infra. As part of the search api execution we have an optimization in place for executions against a single shard, that allow to execute query and fetch in one go (#547), as well as sorting. Both optimization are on by default but can disabled through static `action.search.optimize_single_shard` &amp; `search.controller.optimize_single_shard` settings. I couldn't find any docs around those, and I think these optimizations are always useful and should never be disabled, thus I am proposing to remove support for these settings.
</description><key id="135325707">16759</key><summary>Remove obsolete optimize_single_shard settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label></labels><created>2016-02-22T06:07:09Z</created><updated>2016-02-22T18:44:23Z</updated><resolved>2016-02-22T18:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-22T18:38:53Z" id="187311416">Agreed. I opened #15978 for it a while ago to remove this option. 
</comment><comment author="javanna" created="2016-02-22T18:44:15Z" id="187313561">sorry I had missed the PR, will review, thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup search sub transport actions and collapse o.e.action.search.type package into o.e.action.search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16758</link><project id="" key="" /><description>TransportSearchTypeAction and subclasses are not actually transport actions, but just support classes useful for their inner async actions that can easily be extracted out so that we get rid of one too many level of abstraction.

Same pattern can be applied to TransportSearchScrollQueryAndFetchAction &amp; TransportSearchScrollQueryThenFetchAction which we could remove in favour of keeping only their inner classes named SearchScrollQueryAndFetchAsyncAction and SearchScrollQueryThenFetchAsyncAction.

Remove org.elasticsearch.action.search.type package, collapsed remaining classes into existing org.elasticsearch.action.search package

Make also ParsedScrollId ScrollIdForNode and TransportSearchHelper classes and their methods package private.

Closes #11710
</description><key id="135324545">16758</key><summary>Cleanup search sub transport actions and collapse o.e.action.search.type package into o.e.action.search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T05:56:55Z</created><updated>2016-03-31T12:18:54Z</updated><resolved>2016-02-29T13:52:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-02-26T12:43:48Z" id="189260108">left a minor comment about formatting, but other than that LGTM
</comment><comment author="jpountz" created="2016-02-26T15:34:38Z" id="189324998">+1 I like it
</comment><comment author="javanna" created="2016-02-29T20:07:23Z" id="190360678">I pushed this to master and backported to 2.x as it fixes #11710. The backport required some work given that scan and count search type were removed in master, plus some other conflict. I haven't backported to 2.2, @clintongormley should I look into it or is 2.3 good enough?
</comment><comment author="clintongormley" created="2016-03-02T08:57:30Z" id="191137112">I'm good with 2.3 - it's too big a change for a bugfix release I think
</comment><comment author="pciccarese" created="2016-03-30T20:50:45Z" id="203631879">Hi, I see #11710 is not going to be fixed in 2.2.0 but is it fixed in 2.3.0? 
It looks like I am suffering the ActionFilter.Simple double invocation on 2.2.0.
Thank you!
</comment><comment author="javanna" created="2016-03-31T07:43:44Z" id="203801993">Hi @pciccarese you are correct. This PR was the fix for #11710 and will go out with 2.3. We decided not to include the fix in a bugfix release as it involved a refactoring that touched quite some classes, too big to be considered just a bugfix.
</comment><comment author="pciccarese" created="2016-03-31T12:07:43Z" id="203902143">Thank you @javanna , do you know if, besides 2.2.0, other previous versions were effected by that bug?  
</comment><comment author="javanna" created="2016-03-31T12:18:54Z" id="203904500">&gt; do you know if, besides 2.2.0, other previous versions were effected by that bug?

Every single version before 2.3.0 is affected by it, from the very first one, sorry! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove es.max-open-files flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16757</link><project id="" key="" /><description>This commit removes the es.max-open-files flag as the same information
can be obtained from the cluster nodes info API, and is warn logged on
startup if it's set too low anyway.

Relates #483, relates #16506 
</description><key id="135307628">16757</key><summary>Remove es.max-open-files flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-02-22T03:50:37Z</created><updated>2016-02-28T19:32:27Z</updated><resolved>2016-02-22T05:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-22T03:51:40Z" id="186993513">LGTM, can you add a blurb in the 5.0 migration asciidoc?
</comment><comment author="jasontedor" created="2016-02-22T04:00:11Z" id="186994518">&gt; LGTM, can you add a blurb in the 5.0 migration asciidoc?

@dakrone Done in ad35c6c5ab764ec1004473089a8adf98f08cfd6e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe client connects directly to client node over transport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16756</link><project id="" key="" /><description>This is observed in a setup where the tribe node does not have firewall access over the transport port to a client node of the downstream cluster:

```
[2016-02-14 20:52:16,243][WARN ][cluster.service          ] [tribe_node_name/t1] failed to connect to node [{client_node}{kH2yVx_WQ22qHmthaX_NHA}{10.8.17.130}{host_name/IP:9300}{data=false, master=false}]
ConnectTransportException[[client_node][host_name/IP:9300] connect_timeout[30s]]; nested: ConnectTimeoutException[connection timed out: host_name/IP:9300];
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:951)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:884)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:857)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:474)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out: host_name/IP:9300
    at org.jboss.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:139)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more
```

The message is benign since there is no reason for the tribe node to connect directly to a downstream cluster's client node.  The behavior is likely due to how client nodes work (in general) where they will connect to all nodes in the cluster (and with tribe node being just a specialized client node, it just behaves the same way).   Perhaps we can add an exclusion for tribe node so it will not attempt to connect to the client nodes in the downstream clusters, etc..
</description><key id="135276885">16756</key><summary>Tribe client connects directly to client node over transport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Tribe Node</label><label>docs</label></labels><created>2016-02-21T23:45:01Z</created><updated>2017-01-24T08:42:23Z</updated><resolved>2016-03-23T09:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-01T15:07:32Z" id="190759992">This seems related to some other issue around client nodes connecting to other client nodes: #16815, #3617, #16105. Looking at the linked issue though, it seems like any client node does not even try to connect to other client nodes, while it should. The problem here seems to be the opposite, an attempt of connection that may not be desirable.

I am not sure about the proposal. Why shouldn't the tribe node connect to the client nodes that are part of the cluster? I think every node should rather be able to connect to whichever other node in the cluster. I understand that the tribe node is already a client of its own, and it doesn't need to connect to other clients nodes when it comes to operations that involve data, but there are apis, like monitoring ones, that do need to have access to client nodes too. My reasoning goes along this other [comment](https://github.com/elastic/elasticsearch/issues/16815#issuecomment-190193775).
</comment><comment author="javanna" created="2016-03-02T22:08:20Z" id="191455113">I think with #16898 we made it clear that each node connects to every other node in the cluster, client nodes should not be treated differently, I don't think we should make exceptions for tribe nodes either. Are you ok with this @ppf2 ?
</comment><comment author="ppf2" created="2016-03-02T22:32:53Z" id="191468055">Thanks @javanna , sounds good.  It will be nice though if we document this behavior also in the tribe node documentation - will be helpful for admins out there who have to figure out what ports to open between the tribe node and the other nodes.
</comment><comment author="ppf2" created="2016-04-01T01:06:43Z" id="204196297">Reopening this ticket for a follow up discussion.  One side effect of the current behavior is that the tribe node log file gets filled up with heaps of exceptions like the one noted at the beginning of this issue.  For instance, within a 16 hour period (&lt; 1 day) with just 1 client node in a downstream cluster, the tribe node ends up logging 176Mb of log entries, pretty much filling up the log file with 21K instances of these exception stacks.

While we do not intend to change the design that the tribe node will try to connect to all nodes in the cluster, it can be helpful if we can move these exceptions (when a tribe node attempts to connect to a client node) to the trace level.  Thoughts?
</comment><comment author="javanna" created="2016-04-01T05:06:25Z" id="204251093">@ppf2 do you mean the log line that's part of the description of this issue or some other log line?
</comment><comment author="ppf2" created="2016-04-01T07:28:50Z" id="204289867">Here you go :)  We are seeing a ton of these indicating that the tribe node is trying to connect to a client node.

```
[2016-03-31 03:20:04,750][DEBUG][action.admin.cluster.node.info] [tribe_node] failed to execute on node [A-cIPrviSUiCmoiIzj_GAw]
SendRequestTransportException[[client_node][tribe_node/IP:9300][cluster:monitor/nodes/info[n]]]; nested: NodeNotConnectedException[[client_node][tribe_node/IP:9300] Node not connected];
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
    at org.elasticsearch.shield.transport.ShieldServerTransportService.sendRequest(ShieldServerTransportService.java:75)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:147)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$100(TransportNodesAction.java:94)
    at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:68)
    at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:44)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:101)
    at org.elasticsearch.shield.action.ShieldActionFilter.apply(ShieldActionFilter.java:113)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:99)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:77)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
    at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:845)
    at org.elasticsearch.clclient_nodeient.support.AbstractClient$ClusterAdmin.nodesInfo(AbstractClient.java:925)
    at org.elasticsearch.rest.action.admin.cluster.node.info.RestNodesInfoAction.handleRequest(RestNodesInfoAction.java:102)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
    at org.elasticsearch.rest.RestController$RestHandlerFilter.process(RestController.java:281)
    at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java:262)
    at org.elasticsearch.shield.rest.ShieldRestFilter.process(ShieldRestFilter.java:77)
    at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java:265)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:385)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.ipfilter.IpFilteringHandlerImpl.handleUpstream(IpFilteringHandlerImpl.java:154)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[client_node][tribe_node/IP:9300] Node not connected]
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1132)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:819)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
    ... 75 more
```
</comment><comment author="javanna" created="2016-04-01T08:56:32Z" id="204315345">Thanks @ppf2! I am not sure we can change log level only when the log line comes from a tribe node. Seems like working around the problem. I think every node should get access to all the other nodes instead, including the client ones.

This specific log line comes from calling nodes info from the tribe node. The tribe node will gather the info from all the nodes, as simple as that. Another way to work around it would be to not use the tribe node for monitoring calls, or filter out some of the nodes from this call (e.g. using node attributes).
</comment><comment author="javanna" created="2016-04-05T06:58:12Z" id="205684592">What I previously provided are workarounds, assuming that the firewall config stays the same. But given that we removed support for the `node.client` setting in master, and we are moving away from using client nodes with the java api, I wonder why those "client" nodes need to be treated differently. I think they should have their ports accessible, cause that's what the cluster requires.
</comment><comment author="bleskes" created="2016-04-05T07:01:56Z" id="205686060">what @javanna said. The tribe node should be able to connect to any node in the clusters it connects to. Agreed that it's confusing with the current way we treat client nodes as clients, but that's what we're changing... 
</comment><comment author="amazinganshul" created="2017-01-24T08:42:23Z" id="274742194">With this issue are we resolving whether tribe nodes in the federation cluster should connect to each other or not? We have a federation cluster with two tribe nodes, but no api output shows that tribe nodes have connected to each other. Is this an expected behaviour? Also is there any documentation on scaling tribe nodes?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Painless a Module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16755</link><project id="" key="" /><description>Moved Painless from a plugin to a module.  Fixed a few style violations.
</description><key id="135275777">16755</key><summary>Make Painless a Module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T23:30:50Z</created><updated>2016-04-05T11:07:15Z</updated><resolved>2016-02-22T00:51:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-22T00:41:14Z" id="186953292">this looks awesome, since you fixed the line length I think you should also remove the suppression [here](https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/resources/checkstyle_suppressions.xml#L1575) and [here](https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/resources/checkstyle_suppressions.xml#L10) 

LGTM otherwise
</comment><comment author="jdconrad" created="2016-02-22T00:52:01Z" id="186954925">@s1monw Thanks, Simon.  Updated the suppression list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings with complex matchers should not overlap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16754</link><project id="" key="" /><description>It is possible to register multiple settings with complex matchers that could both match
a given key. The behavior when this occurs can lead to issues and depends on the
number of settings that have been registered. In order to identify the setting for a given
key, we iterate over the values in a map to find the first setting that matches the given key
and iteration order of a map should not be relied upon.

This commit checks complex settings when adding them and if the keys for these overlap,
an IllegalArgumentException is now thrown.
</description><key id="135269199">16754</key><summary>Settings with complex matchers should not overlap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T22:28:04Z</created><updated>2016-02-28T23:28:59Z</updated><resolved>2016-02-26T12:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-21T22:33:32Z" id="186934835">LGTM
</comment><comment author="danielmitterdorfer" created="2016-02-26T10:47:33Z" id="189217804">Left a few minor comments, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add setting to disable importing dynamic indices on shared filesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16753</link><project id="" key="" /><description>The `gateway.local.dangling.import_shared_fs` defaults to
`false` (meaning don't import them).

Relates to #16358

(also cleans up and removes the checkstyle linelength suppression for DanglingIndicesState)
</description><key id="135268882">16753</key><summary>Add setting to disable importing dynamic indices on shared filesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>breaking</label></labels><created>2016-02-21T22:26:16Z</created><updated>2016-02-26T16:46:28Z</updated><resolved>2016-02-26T16:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-26T16:44:03Z" id="189365015">Closing this, @bleskes brought up a different idea on #16358 that should solve this in a more elegant way.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add LifecycleRunnable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16752</link><project id="" key="" /><description>I've noticed throughout some of the code that we have a need to remove the boilerplate lifecycle check when starting/rescheduling certain runnables. This provides a simpler implementation to get this functionality without duplicating it.

It also helps to avoid constantly rescheduling in some cases.
</description><key id="135264512">16752</key><summary>Add LifecycleRunnable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T21:50:41Z</created><updated>2016-03-01T23:04:59Z</updated><resolved>2016-03-01T23:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-26T10:55:17Z" id="189220833">Left a few minor comments, otherwise LGTM.
</comment><comment author="pickypg" created="2016-03-01T23:03:31Z" id="190953109">Merged / Closed by f2aaa28362b22ba325bc846e633ee5e41b32135f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor NumberFieldType to use Lucene 6.0 Point types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16751</link><project id="" key="" /><description>Numeric Fields are deprecated in Lucene 6.0 so ES 5.0 should cut over to the new BKD based numeric types.
</description><key id="135255889">16751</key><summary>Refactor NumberFieldType to use Lucene 6.0 Point types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-02-21T20:40:11Z</created><updated>2016-04-14T15:57:09Z</updated><resolved>2016-04-14T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Memory lock lost on Windows Server 2012R2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16750</link><project id="" key="" /><description>I'm running my nodes with 
`bootstrap.mlockall: true`
and I have verified that the memory is allocated using task manager and if I run 
`curl http://localhost:9200/_nodes/process?pretty`
it's confirmed the lock's in place.

I've experienced that a node might "loose" some of the previously allocated memory, where instead I see that memory has been allocated to lucene index files as mapped files.

As soon as there's load on the node (assuming it's trying to use the expected available heap memory) the cluster turns red, shows all shards assigned to the node as unassigned. Nothing is logged when it happens.
</description><key id="135249684">16750</key><summary>Memory lock lost on Windows Server 2012R2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xorandor</reporter><labels /><created>2016-02-21T19:56:02Z</created><updated>2016-02-28T19:21:38Z</updated><resolved>2016-02-28T19:21:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T19:20:53Z" id="189926139">Hi @xorandor 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only. If you find a bug then feel free to open an issue with the details.

thanks
</comment><comment author="clintongormley" created="2016-02-28T19:21:38Z" id="189926548">Ah windows, didn't see that.  I think this is a duplicate of https://github.com/elastic/elasticsearch/issues/13648
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add experimental geo field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16749</link><project id="" key="" /><description>The [JTS](https://www.locationtech.org/projects/technology.jts) license changed from LGPL to dual BSD/EDL and Lucene 6 introduces the new BKD tree structure. With these updates, `geo_point` and `geo_shape` functionality can now be combined, and BKD geo support can be added, to a new experimental `geo` field.
</description><key id="135240351">16749</key><summary>Add experimental geo field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Mapping</label><label>feature</label><label>v6.0.0</label></labels><created>2016-02-21T18:35:46Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16748</link><project id="" key="" /><description /><key id="135236865">16748</key><summary>mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pawwaso</reporter><labels /><created>2016-02-21T18:08:37Z</created><updated>2016-02-21T18:09:27Z</updated><resolved>2016-02-21T18:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix Unknown [repository] type [azure] error with 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16747</link><project id="" key="" /><description>This regression has been introduced in 2.2.0 by #13779 where we removed support for `cloud.azure.storage.account` and replaced by `cloud.azure.storage.my_account.account`.
But Azure plugin tries to detect when it starts if all needed settings to use an `azure` repository have been set. If not the case, the plugin does not expose `azure` as an available repository.

Sadly, this check has been badly updated in 2.2.0 so it can never find the expected settings to start correctly.

This gives the following effect:

``` yaml
cloud:
    azure:
        storage:
            my_account:
                account: your_azure_storage_account
                key: your_azure_storage_key
```

When you try to execute the following API you get Unknown [repository] type [azure]:

``` sh
[msimos@msi-gs60 elasticsearch-2.2.0]$ curl -XPUT http://localhost:9200/_snapshot/mybackup?pretty -d '{
"type": "azure"
}'
```

``` js
{
  "error" : {
    "root_cause" : [ {
      "type" : "repository_exception",
      "reason" : "[mybackup] failed to create repository"
    } ],
    "type" : "repository_exception",
    "reason" : "[mybackup] failed to create repository",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Unknown [repository] type [azure]"
    }
  },
  "status" : 500
}
```

``` sh
[msimos@msi-gs60 elasticsearch-2.2.0]$ curl -XGE http://localhost:9200/_cat/plugins?v
name   component   version type url
node01 cloud-azure 2.2.0   j
node01 license     2.2.0   j
node01 shield      2.2.0   j
```

In the elasticsearch log file you see this error:

```
[2016-02-19 10:54:47,357][DEBUG][cloud.azure              ] [node01] starting azure services
[2016-02-19 10:54:47,357][DEBUG][cloud.azure              ] [node01] azure repository is not set using [repositories.azure.account] and [cloud.azure.storage.key] properties
```

Closes #16734

Note that this does not happen in master branch because we split the azure plugin and we always define `azure` as a repository when the `repository-azure` plugin starts.
</description><key id="135222473">16747</key><summary>Fix Unknown [repository] type [azure] error with 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>regression</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-21T16:06:59Z</created><updated>2016-02-29T02:29:50Z</updated><resolved>2016-02-25T12:47:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-21T16:07:19Z" id="186850800">@imotov Any chance you could look at it?
</comment><comment author="imotov" created="2016-02-24T22:04:06Z" id="188472384">Left a minor comment. Otherwise, LGTM.
</comment><comment author="clintongormley" created="2016-02-28T22:42:13Z" id="189961343">@dadoonet is this fix not required in master too?
</comment><comment author="dadoonet" created="2016-02-29T02:29:50Z" id="190007250">No. Master is already working due to some changes we did in master only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only accept transport requests after node is fully initialized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16746</link><project id="" key="" /><description>At the moment we open up the transport service before the local node has been fully initialized. This causes bug as some data structures are not fully initialized yet. See for example #16723. 

Sadly,  we can't just start the TransportService last (as we do with the HTTP server) because the ClusterService needs to know the bound published network address for the local DiscoveryNode. This address can only be determined by actually binding (people may use, for example, port 0). Instead we start the TransportService as late as possible but block any incoming requests until the node has completed initialization. 

A couple of other cleanup during start time:
1) The gateway service now starts before the initial cluster join so we can simplify the logic to recover state if the local node has become master.
2) The discovery is started before the transport service accepts requests, but we only start the join process later using a dedicated method.

Closes #16723
</description><key id="135221070">16746</key><summary>Only accept transport requests after node is fully initialized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Network</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T15:54:06Z</created><updated>2016-02-24T02:10:07Z</updated><resolved>2016-02-24T02:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-21T18:11:37Z" id="186875909">@bleskes after looking at `InternalClusterService` I would love to explore detaching `TransportSerivce` I think we have a good chance to get it fixed if we:
- move the LocalNode stuff into TransportService
- add the reconnect task into a seperate class that is started seperately and depends on both transportservice and ClusterService
- for the connect stuff we do before we notify listeners should be maybe a listener too? I think it's a messed up dependency in ClusterService?

WDYT
</comment><comment author="bleskes" created="2016-02-21T21:52:42Z" id="186925879">@s1monw I think these are excellent ideas and I will explore them - I totally agree we should decouple the cluster state service from the transport service.  There are some issues to figure out in order to solve the issue where the transport service needs to bind the socket in order to make a DiscoNode , which is in turn used in the very first cluster state. I pushed another commit simplifying things here even further. I think it starts to be simple enough and we can push this without doing a bigger refactoring. It's a judgement call and I defer to your judgement. I'm good with any decision.
</comment><comment author="s1monw" created="2016-02-22T22:42:54Z" id="187421862">@bleskes I think we should do what we have for 2.x and open a followup for master. I mean we can push this to master and make the followup removing this one?
</comment><comment author="bleskes" created="2016-02-22T23:14:26Z" id="187431916">@s1monw will do. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Assert that we can write in all data-path on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16745</link><project id="" key="" /><description>Today we might start a node and some of the paths might not have the
required permissions. This commit goes through all data directories as
well as index, shard and state directories and ensures we have write access.
To make this work across all OS etc. we are trying to write a real file
and remove it again in each of those directories
</description><key id="135166582">16745</key><summary>Assert that we can write in all data-path on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T06:09:53Z</created><updated>2016-02-28T20:13:04Z</updated><resolved>2016-02-22T18:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-21T07:23:37Z" id="186766416">LGTM, awesome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Combine node name and task id into single string task id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16744</link><project id="" key="" /><description>This commit changes the URL for task operations from `/_tasks/{nodeId}/{taskId}` to `/_tasks/{taskId}`, where `{taskId}` has a form of `nodeid:id`
</description><key id="135157555">16744</key><summary>Combine node name and task id into single string task id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T04:18:15Z</created><updated>2016-02-28T22:28:09Z</updated><resolved>2016-02-24T22:22:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-21T22:37:55Z" id="186936091">@imotov so what seems weird about this to me is the way it reads: `request.taskId().getId()`, do you think the `taskId()` method should be renamed? It feels like overloading the term "id" in this case
</comment><comment author="imotov" created="2016-02-21T23:16:09Z" id="186941745">@dakrone I agree, do you have a suggestion for the name?
</comment><comment author="dakrone" created="2016-02-22T19:04:37Z" id="187322280">Left some comments that are mostly minor :)
</comment><comment author="imotov" created="2016-02-22T19:36:51Z" id="187339142">@dakrone pushed the changes to address your comments.
</comment><comment author="dakrone" created="2016-02-22T21:47:56Z" id="187401179">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cat/recovery] Make recovery time a TimeValue()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16743</link><project id="" key="" /><description>Revisiting #9209

@drewr Could you review the documentation change I added to you initial commit please?
</description><key id="135138417">16743</key><summary>[cat/recovery] Make recovery time a TimeValue()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-21T00:33:57Z</created><updated>2016-02-28T19:13:23Z</updated><resolved>2016-02-22T20:38:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-22T20:38:08Z" id="187372812">I'm merging it as what I added is only about documentation and we already got a LGTM for the code part.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 5.5.0 official release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16742</link><project id="" key="" /><description>Lucene 5.5.0 was just released; this upgrades ES 2.3.0 and 5.0.0 to the official release bits.
</description><key id="135129470">16742</key><summary>Upgrade to Lucene 5.5.0 official release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-20T22:37:33Z</created><updated>2016-02-21T10:27:30Z</updated><resolved>2016-02-21T10:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-21T00:38:54Z" id="186708712">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update aggregations.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16741</link><project id="" key="" /><description>Changed number of families from 2 to 3.
</description><key id="135127225">16741</key><summary>Update aggregations.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blachniet</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-02-20T22:08:43Z</created><updated>2016-03-02T09:57:34Z</updated><resolved>2016-03-02T09:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T19:12:42Z" id="189924074">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="blachniet" created="2016-03-01T12:51:06Z" id="190710590">Signed
</comment><comment author="clintongormley" created="2016-03-02T09:57:34Z" id="191164368">thanks @blachniet - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove SNAPSHOT notion from code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16740</link><project id="" key="" /><description>Today we have the notion of a snapshot inside Version.java which makes
releasing complicated since to do a release Version.java must be changed.
This commit removes all notions of snapshot from the code and allows to
switch between snapshot and release build by specifying a system property on
the build. For instance running:

```
gradle run -Dbuild.snapshot=false
```

will build and package a release build while the default always
builds snapshots. Calls to the main rest action will still get the snapshot
information rendered out with the response.
</description><key id="135114821">16740</key><summary>Remove SNAPSHOT notion from code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>PITA</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-20T20:00:02Z</created><updated>2016-02-21T01:21:01Z</updated><resolved>2016-02-21T01:21:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-21T00:43:39Z" id="186709995">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to connect with Elasticsearch with JavaConfig</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16739</link><project id="" key="" /><description>I've found [this](http://www.scriptscoop.net/t/24321e0e3f75/spring-data-elasticsearch-transportclient-java-config.html) example on how to do it in JavaConfig. It should be something like this.

``` java
package be.wewv.spring;

import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.common.transport.TransportAddress;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.PropertySource;
import org.springframework.context.support.PropertySourcesPlaceholderConfigurer;
import org.springframework.core.env.Environment;
import org.springframework.core.io.ClassPathResource;
import org.springframework.core.io.Resource;
import org.springframework.data.elasticsearch.core.ElasticsearchOperations;
import org.springframework.data.elasticsearch.core.ElasticsearchTemplate;
import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories;
import org.springframework.web.servlet.config.annotation.EnableWebMvc;

import javax.annotation.PostConstruct;

/**
 * Created by Glenn on 17/01/2016.
 */
@EnableWebMvc
@Configuration
@PropertySource("classpath:properties/application.properties")
@EnableElasticsearchRepositories(basePackages = "org/springframework/data/elasticsearch/repositories")
@ComponentScan(basePackages = "be.wewv")
public class AppConfig {

    @Value("${esearch.port}")
    int port;
    @Value("${esearch.host}")
    String hostname;

    @Bean
    @PostConstruct
    public static PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer() {
        PropertySourcesPlaceholderConfigurer configurer = new PropertySourcesPlaceholderConfigurer();
        Resource resource = new ClassPathResource("properties/application.properties");
        configurer.setLocation(resource);
        return configurer;
    }

    @Bean
    public ElasticsearchOperations elasticsearchTemplate() {
        return new ElasticsearchTemplate(client());
    }

    @Bean
    public Client client(){
        TransportClient client= new TransportClient(); // Error:(80, 33) 
        TransportAddress address = new InetSocketTransportAddress(hostname, port); // Error:(81, 67)
        client.addTransportAddress(address);
        return client;
    }
}
```

But this gives me the following errors.

&gt; Error:(80, 33) java: constructor TransportClient in class org.elasticsearch.client.transport.TransportClient cannot be applied to given types;
&gt;   required: org.elasticsearch.common.inject.Injector
&gt;   found: no arguments
&gt;   reason: actual and formal argument lists differ in length
&gt; 
&gt; Error:(81, 67) java: incompatible types: java.lang.String cannot be converted to java.net.InetAddress
</description><key id="135109084">16739</key><summary>Unable to connect with Elasticsearch with JavaConfig</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlennVanSchil</reporter><labels /><created>2016-02-20T18:40:33Z</created><updated>2016-02-20T20:01:44Z</updated><resolved>2016-02-20T20:01:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-20T20:01:44Z" id="186674200">I think it's an incompatibility with elasticsearch 2.x but unsure.
I think you should open your issue in spring data project as I don't believe it's an elasticsearch issue here.

You can also ask on discuss.elastic.co for help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to serial differencing aggregation doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16738</link><project id="" key="" /><description>Hi,

`thirtieth_difference` should use `the_sum` metric as the `buckets_path`.
</description><key id="135062169">16738</key><summary>Update to serial differencing aggregation doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertlyson</reporter><labels><label>docs</label></labels><created>2016-02-20T11:13:39Z</created><updated>2016-02-28T19:23:04Z</updated><resolved>2016-02-28T19:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T19:09:24Z" id="189923854">thanks @robertlyson 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add G1GC check on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16737</link><project id="" key="" /><description>This commit adds a check on startup for G1 GC while running on early
versions of HotSpot version 25. This is to prevent potential data
corruption issues that can occur on those versions.

Closes #10740
</description><key id="135035727">16737</key><summary>Add G1GC check on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-20T05:27:33Z</created><updated>2016-03-10T18:51:36Z</updated><resolved>2016-02-23T00:52:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-23T00:22:45Z" id="187449385">@s1monw Do you mind reviewing?
</comment><comment author="s1monw" created="2016-02-23T00:31:32Z" id="187452301">LGTM just some bikeshedding on naming and javadocs
</comment><comment author="jasontedor" created="2016-02-23T00:42:24Z" id="187454995">&gt; LGTM just some bikeshedding on naming and javadocs

@s1monw Addressed in afffb3550770b42f9cedaef7be6d81457642e8b3 and 68db3bd654a3e79b6fe49718d67e5de1b282e115. Thanks for reviewing.
</comment><comment author="jasontedor" created="2016-02-23T00:51:05Z" id="187456569">This is the message printed on startup:

```
Exception in thread "main" java.lang.RuntimeException: Java version: Oracle Corporation 1.8.0_31 [Java HotSpot(TM) 64-Bit Server VM 25.31-b07] can cause data corruption when used with G1GC.
Please upgrade the JVM, see http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html for current recommendations.
        at org.elasticsearch.bootstrap.JVMCheck.check(JVMCheck.java:205)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:278)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:37)
Refer to the log for complete error details.
```
</comment><comment author="jprante" created="2016-02-23T08:10:37Z" id="187596110">Will there be a check for 32bit? JVM crashes with G1GC occur only on 32bit, on tiny heaps, no?
</comment><comment author="jasontedor" created="2016-02-23T23:56:39Z" id="187973495">&gt; Will there be a check for 32bit? JVM crashes with G1GC occur only on 32bit, on tiny heaps, no?

@jprante While the [famous Lucene byte slice reader assert](https://issues.apache.org/jira/browse/LUCENE-5168) appeared to only trip on 32-bit, the problem is that no one actually understands that failure and the cause of it. What is more, we've seen hard JVM crashes with G1 GC on 64-bit JVMs (with the problematic frame being a G1 GC method) and [spooky assertions trip](https://issues.apache.org/jira/browse/LUCENE-6098) with G1 GC on 64-bit JVMs (these are the worst because of the potential for corruption).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there a way to find avg without outliers?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16736</link><project id="" key="" /><description>I think that it can be s great feature
</description><key id="134987038">16736</key><summary>Is there a way to find avg without outliers?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">liorg2</reporter><labels /><created>2016-02-19T22:02:48Z</created><updated>2016-02-20T15:42:49Z</updated><resolved>2016-02-19T23:08:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-19T23:08:17Z" id="186444024">Closing: this looks too specific to me for us to have a dedicated aggregation.

As a side-note, you can already do it with two requests:
- a first one with a percentiles aggregation to figure out what the outliers are
- a second one that would filter outliers out and run an average aggregation on the desired field
</comment><comment author="liorg2" created="2016-02-20T15:42:49Z" id="186631246">Outliers detection has several algorithms, and percentiles filtering can be
naive..
In my opinion it can be really cool and usable feature

https://en.m.wikipedia.org/wiki/Outlier

On Saturday, 20 February 2016, Adrien Grand notifications@github.com
wrote:

&gt; Closing: this looks too specific to me for us to have a dedicated
&gt; aggregation.
&gt; 
&gt; As a side-note, you can already do it with two requests:
&gt; - a first one with a percentiles aggregation to figure out what the
&gt;   outliers are
&gt; - a second one that would filter outliers out and run an average
&gt;   aggregation on the desired field
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16736#issuecomment-186444024
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More powerful index pattern matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16735</link><project id="" key="" /><description>Was speaking to @rashidkpc at the AMA booth at Elastic{ON} about the deprecation of date stamped index matching in Kibana. To allow more granular filtering of indices it would be great if index pattern matching supported more than just `*` wildcards. Rashid mentioned using a `?` for single character matching but basic regex patterns would also be awesome. He also said there could be and existing issue for this but I wasn't able to find it.

See also: https://discuss.elastic.co/t/index-filtering-on-more-than-a-wildcard/40802
</description><key id="134981228">16735</key><summary>More powerful index pattern matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pemontto</reporter><labels><label>:Index APIs</label><label>feedback_needed</label></labels><created>2016-02-19T21:38:22Z</created><updated>2016-05-24T10:34:21Z</updated><resolved>2016-05-24T10:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T18:58:10Z" id="189922520">Hi @pemontto 

Given that we're moving away from index names with date stamps, what kind of matching would you like to do?
</comment><comment author="pemontto" created="2016-02-29T05:21:02Z" id="190046632">Using the example from discuss, we have users that append tags to their index names for various reasons e.g.

```
finance-logs-2016.02.03
finance-logs-2016.02.03-temp
finance-logs-2016.02.03-pre-change
```

Not having the ability to specify a pattern that can exclude trailing characters means they're matching our generic wildcard patterns and giving us incorrect documents and document counts in Kibana. Not to mention it would useful wherever we use the multi-index syntax like snapshots.
</comment><comment author="clintongormley" created="2016-02-29T08:51:23Z" id="190107533">@pemontto Date math support in index names may be sufficient for your use case? See https://www.elastic.co/guide/en/elasticsearch/reference/current/date-math-index-names.html
</comment><comment author="pemontto" created="2016-03-03T01:21:22Z" id="191525060">Thanks Clinton! That must be new in ES 2?
For this use case it works perfectly, though I can still see basic regex being very beneficial.

Heading on over to Kibana issues to get this supported.
</comment><comment author="pemontto" created="2016-03-03T01:46:41Z" id="191534784">Perhaps jumped the gun a little bit there, I didn't realise the date math is for specific dates/times not actual pattern matching so it won't easily work with Kibana's index pattern setup.

Also I tried the examples and wasn't able to get it working on my 2.2 cluster, am I missing something?

``` shell
[root@dev-logstore1 ~]# curl localhost:9200
{
  "name" : "es2beta-1",
  "cluster_name" : "es2beta",
  "version" : {
    "number" : "2.2.0",
    "build_hash" : "8ff36d139e16f8720f2947ef62c8167a888992fe",
    "build_timestamp" : "2016-01-27T13:32:39Z",
    "build_snapshot" : false,
    "lucene_version" : "5.4.1"
  },
  "tagline" : "You Know, for Search"
}
[root@dev-logstore1 ~]# date
Thu Mar  3 12:45:51 AEDT 2016
[root@dev-logstore1 ~]# curl -XPUT localhost:9200/logstash-2015.03.03
{"acknowledged":true}[root@dev-logstore1 ~]# curl -XPUT localhost:9200/logstash-2015.03.02
{"acknowledged":true}[root@dev-logstore1 ~]# curl -XPUT localhost:9200/logstash-2015.03.01
{"acknowledged":true}[root@dev-logstore1 ~]# curl -XPUT localhost:9200/logstash-2015.02.29
{"acknowledged":true}[root@dev-logstore1 ~]# curl -XPUT localhost:9200/logstash-2015.03.04
[root@dev-logstore1 ~]# curl -XGET 'localhost:9200/&lt;logstash-{now%2Fd-2d}&gt;,&lt;logstash-{now%2Fd-1d}&gt;,&lt;logstash-{now%2Fd}&gt;/_mapping'
{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"logstash-now/d-2d","index":"logstash-now/d-2d"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"logstash-now/d-2d","index":"logstash-now/d-2d"},"status":404}
```
</comment><comment author="clintongormley" created="2016-03-03T16:12:49Z" id="191830992">@pemontto there are a few things wrong:
1. You created indices for 2015 but we're in 2016
2. Your date formats are not default, so you have to specify them
3. You have to turn off globbing in curl because of the curlies

This example: 

```
curl -XPUT "http://localhost:9200/logstash-2016.02.29"
curl -XPUT "http://localhost:9200/logstash-2016.03.01"
curl -XPUT "http://localhost:9200/logstash-2016.03.02"
curl -XPUT "http://localhost:9200/logstash-2016.03.03"
curl -XPUT "http://localhost:9200/logstash-2016.03.04"


curl -g -XGET "http://localhost:9200/&lt;logstash-{now%2Fd-2d{yyyy.MM.dd}}&gt;,&lt;logstash-{now%2Fd-1d{yyyy.MM.dd}}&gt;/_mapping?pretty"
```

Returns (today):

```
{
  "logstash-2016.03.02" : {
    "mappings" : { }
  },
  "logstash-2016.03.01" : {
    "mappings" : { }
  }
}
```
</comment><comment author="clintongormley" created="2016-05-24T10:34:21Z" id="221230434">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unknown [repository] type [azure] error with Elasticsearch 2.2 &amp; cloud-azure plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16734</link><project id="" key="" /><description>When using Elasticsearch 2.2.0 and the cloud-azure plugin. When you use the following settings:

https://www.elastic.co/guide/en/elasticsearch/plugins/2.2/cloud-azure-repository.html

``` yaml
cloud:
    azure:
        storage:
            my_account:
                account: your_azure_storage_account
                key: your_azure_storage_key
```

When you try to execute the following API you get Unknown [repository] type [azure]:

``` sh
[msimos@msi-gs60 elasticsearch-2.2.0]$ curl -XPUT http://localhost:9200/_snapshot/mybackup?pretty -d '{
"type": "azure"
}'
```

``` js
{
  "error" : {
    "root_cause" : [ {
      "type" : "repository_exception",
      "reason" : "[mybackup] failed to create repository"
    } ],
    "type" : "repository_exception",
    "reason" : "[mybackup] failed to create repository",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Unknown [repository] type [azure]"
    }
  },
  "status" : 500
}
```

``` sh
[msimos@msi-gs60 elasticsearch-2.2.0]$ curl -XGE http://localhost:9200/_cat/plugins?v
name   component   version type url 
node01 cloud-azure 2.2.0   j        
node01 license     2.2.0   j        
node01 shield      2.2.0   j        
```

In the elasticsearch log file you see this error:

```
[2016-02-19 10:54:47,357][DEBUG][cloud.azure              ] [node01] starting azure services
[2016-02-19 10:54:47,357][DEBUG][cloud.azure              ] [node01] azure repository is not set using [repositories.azure.account] and [cloud.azure.storage.key] properties
```
</description><key id="134953700">16734</key><summary>Unknown [repository] type [azure] error with Elasticsearch 2.2 &amp; cloud-azure plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">msimos</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>bug</label><label>regression</label></labels><created>2016-02-19T19:35:58Z</created><updated>2016-02-25T12:49:59Z</updated><resolved>2016-02-25T12:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-25T12:49:59Z" id="188774613">Closed with 7ffd6aa (2.3.0) and 571f425 (2.2.1)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce node level limits if node is started in production env</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16733</link><project id="" key="" /><description>This commit tries to 'guess' if a user starts a node in production by
checking if any network host is configured. If that is the case soft-limits
that are only logged otherwise are enforced like number of open file descriptors.

Closes #16727
</description><key id="134953300">16733</key><summary>Enforce node level limits if node is started in production env</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-19T19:33:48Z</created><updated>2016-02-28T20:12:17Z</updated><resolved>2016-02-22T18:30:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-20T04:00:54Z" id="186502107">LGTM. Great!
</comment><comment author="s1monw" created="2016-02-21T17:36:09Z" id="186865527">@jasontedor @bleskes I added some tests and I wonder if we should also switch to strict mode when folks configure profiles?
</comment><comment author="rmuir" created="2016-02-26T23:05:26Z" id="189520085">This commit is broken on OS X.
</comment><comment author="rmuir" created="2016-02-26T23:07:32Z" id="189520631">The referenced workaround #16813 does not work either. We should revert this commit.
</comment><comment author="rmuir" created="2016-02-26T23:08:04Z" id="189520725">I tried to revert it myself, but github does not allow it. I'm too lazy to revert it manually, but this should be reverted, and only committed when actually tested.
</comment><comment author="jasontedor" created="2016-02-26T23:25:34Z" id="189524341">@rmuir It does. But you have to `sysctl kern.maxfiles`, and `kern.maxfilesperproc` to higher limits, reboot, and set the JVM flag in both `GRADLE_OPTS` (if you're running tests) and `ES_JAVA_OPTS` (the `GRADLE_OPTS` is necessary if you're running tests because a forked process will not be granted more descriptors than its parent).
</comment><comment author="rmuir" created="2016-02-26T23:27:59Z" id="189524838">yeah thats intuitive. we should revert this. 
</comment><comment author="jasontedor" created="2016-02-26T23:48:27Z" id="189529452">@rmuir I agree, the situation is complicated. I spent some time last night reading JVM and Darwin sources (and running `dtrace`s) to understand the implications. The short version is that this flag is weird, especially on OS X. I'll post the longer version of my notes later tonight or early tomorrow on the issue #16813 that @jaymode filed.

An immediate idea I have for temporary relief is to disable the check on snapshot builds.
</comment><comment author="rmuir" created="2016-02-26T23:48:34Z" id="189529472">Lets enumerate the problems here.

I sit here with my brother, who works statistics in elasticsearch, because its a nice opportunity to test nick's PR (https://github.com/elastic/elasticsearch/issues/16817), since my brother actually does this stuff, you know, the hard way right now.

Anyway, lets enumerate the shitty experience we had here:
1. he installed ES on his operating system, and of course its "log4j not configured properly", but continues to run. so logging isnt working, and garbage leniency just lets it run. Fucking awesome guys.
2. attempts to index anything with threads &gt; 1 results in RejectedExecutionException/TransportReplicationAction/nonsense. If me, my brother, mike, and ryan cant figure the shit out, users have no fucking hope.
3. after discussing with ryan and nik, the idea is, lets try master, it might be better. master neither runs nor compiles on freebsd, because gradle 2.4 just fails in strange ways. doesnt fail with "you need at least gradle 2.x", just fails in a strange way. fucking fantastic.
4. download latest version of gradle, fails also because "missing native libraries". Guess setAccessible only carries you so far groovy guys! wonderful.
5. try to run master on my mac, binding to the ethernet port, since my mac is "already setup", then the idea is, my brother can just index to it, and we can get shit done. This check fails.
6. Try passing the linked workaround from jay, this also fails, it seems to just make ES think there is then a limit of only 256 file descriptors.
</comment><comment author="s1monw" created="2016-02-27T00:15:53Z" id="189534184">I have no problem reverting this if it causes problems. It didn't cause problems in my test runs. 
I have problems with statements like this:

&gt; At this point, I am officially embarrassed to be affiliated with this company or this software in any way. This is fucking shit trash guys.

I tried to make things better and preserve the OOB experience. If there are problems then lets reiterate but with statements like this I loose any kind of enjoyment to work with you @rmuir and I am sure  am not the only one. I will revert this check now.
</comment><comment author="jasontedor" created="2016-02-27T00:51:17Z" id="189541760">&gt;  I will revert this check now.

I chatted with @s1monw via another channel, and we are going to try the approach of disabling the check on snapshot builds. I opened #16835.
</comment><comment author="jasontedor" created="2016-02-27T00:58:55Z" id="189542541">&gt; I'll post the longer version of my notes later tonight or early tomorrow on the issue #16813 that @jaymode filed.

I posted my [notes](https://github.com/elastic/elasticsearch/issues/16813#issuecomment-189541620).
</comment><comment author="rmuir" created="2016-02-27T01:17:44Z" id="189545879">As usual, i'm not attacking anyone personally. Just the code! I do attack the code, and i believe it needs some attacking! But lets be clear, its not about any persons. Just a bunch of crap we should clean up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed a few names that could be misleading; the original one I spotted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16732</link><project id="" key="" /><description>was this:

[node                     ] [Fatale] started

which I thought was a fatal error or something. The Terror node name would
yield an interesting message too...
</description><key id="134854626">16732</key><summary>Removed a few names that could be misleading; the original one I spotted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels><label>discuss</label></labels><created>2016-02-19T12:29:27Z</created><updated>2016-07-26T13:21:41Z</updated><resolved>2016-07-26T13:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:52:08Z" id="206462165">@clintongormley personally I don't think we should remove any (or else we are basically inviting anyone to critique the list of names), but what do you think?
</comment><comment author="dweiss" created="2016-04-06T19:37:54Z" id="206526358">The 'fatale' is confusing, it's not just a personal bias. It actually caused me to check the log looking for errors.
</comment><comment author="clintongormley" created="2016-04-07T09:29:15Z" id="206781031">Yeah, while I understand your reaction @dweiss, I'm not crazy about setting a precedent of editing the nodes list either.  We did remove `Master`, which was seriously confusing, but I'm not sure `Fatale`, `Terror`,  and `Nuke - Frank Simpson` are quite the same level. There are lots of Marvel names that are quite edgy and, with the precedent in place, someone somewhere will insist on those being removed. 

There's an easy solution for this: set your own meaningful node names.
</comment><comment author="dweiss" created="2016-04-07T09:47:48Z" id="206789680">Sure, it's debatable what's appropriate, confusing, etc. But I seriously don't understand the resistance to accept hints (and pull requests) that would modify those entries. I mean: if you don't care about the node names then why prevent people who do care from changing them?

Also, ask @s1monw -- we've edited node names before... there are some hidden gems in there that really best be avoided.

Finally, sure, you can override but I think the default list should make some sense without being potentially confusing.
</comment><comment author="danielmitterdorfer" created="2016-07-26T13:18:38Z" id="235264352">@clintongormley What about this PR given that #19456 is now in master?
</comment><comment author="elasticmachine" created="2016-07-26T13:18:39Z" id="235264358">Can one of the admins verify this patch?
</comment><comment author="nik9000" created="2016-07-26T13:21:41Z" id="235265131">&gt; What about this PR given that #19456 is now in master?

Since the names are gone I think this isn't needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include closed indices as well for filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16731</link><project id="" key="" /><description>Fix for #16419
</description><key id="134835615">16731</key><summary>Include closed indices as well for filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">debarshiraha</reporter><labels><label>:Index APIs</label><label>Awaiting CLA</label><label>enhancement</label></labels><created>2016-02-19T10:52:58Z</created><updated>2016-05-25T08:18:35Z</updated><resolved>2016-05-25T08:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-19T15:15:10Z" id="186255199">@debadair you get that CLA check to pass if you sign the [CLA](https://www.elastic.co/contributor-agreement/) with the same email you use for github and that you used for the commit. Then you post a comment on the issue and the checker rechecks automatically.
</comment><comment author="dadoonet" created="2016-02-19T15:29:56Z" id="186260388">@nik9000 You meant @debarshiraha right? :)
</comment><comment author="nik9000" created="2016-02-19T15:49:25Z" id="186270417">&gt; @nik9000 You meant @debarshiraha right? :)

Yup. I'm not doing a good job using the internet this morning....
</comment><comment author="clintongormley" created="2016-02-28T18:56:06Z" id="189922099">@debarshiraha I'm unable to find your signed CLA
</comment><comment author="debarshiraha" created="2016-02-28T23:55:08Z" id="189972594">Our legal mailed elastic to figure out how to add me to the CLA (that they have already done before). He is still waiting for the response.
</comment><comment author="debarshiraha" created="2016-04-14T21:48:48Z" id="210167306">@clintongormley Can someone help to add me to the CCLA? Our open source team has contacted Elastic regarding this.
</comment><comment author="ywelsch" created="2016-05-25T08:18:35Z" id="221504728">superseded by #18545
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16730</link><project id="" key="" /><description /><key id="134810534">16730</key><summary>Fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kdelchev</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-19T09:10:02Z</created><updated>2016-02-26T18:41:57Z</updated><resolved>2016-02-26T18:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-19T15:21:18Z" id="186257814">Change looks good to me. If you sign the [CLA](https://www.elastic.co/contributor-agreement), wait 5 minutes or so, and then make a comment on this issue then the failing check should clear.

The CLA is required for all changes, even single letter ones.
</comment><comment author="kdelchev" created="2016-02-19T21:54:16Z" id="186425757">Signed for CLA, having a :beer: (e.g. wait 5 min), then put this comment. Cheers!
</comment><comment author="nik9000" created="2016-02-26T18:33:24Z" id="189414802">Thanks! I'm finally getting around to merging this. Flying is hard..... Anyway! I'll get this into all the branches that it matters!
</comment><comment author="nik9000" created="2016-02-26T18:36:29Z" id="189416668">2.0: 60d886a0a191f7461c1d419aae6a89229b01d01f
2.1: 83de2dd3820f41b82e31005c5b96ba653807278e
2.2: 5765d27955b7a648623fd5bb45a8cd5966f8030b
2.x: 50e8e6063f23940f2e5c3a452ee9e03cd1bcfdff
master: b451d5eb07b09ea04daef738bbfa6100514e8409
</comment><comment author="nik9000" created="2016-02-26T18:37:24Z" id="189417120">That might have been overkill to go back to 2.0 but it is a documentation fix so it'll show up on the docs for everyone soon-ish.
</comment><comment author="nik9000" created="2016-02-26T18:41:57Z" id="189418789">All done! Thanks @kdelchev !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why es cluster stop to work until i delete the old index?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16729</link><project id="" key="" /><description>In [es document](https://www.elastic.co/guide/en/elasticsearch/guide/master/_coping_with_failure.html),it introduce that,`If we restart Node 1,If Node 1 still has copies of the old shards, it will try to reuse them, copying over from the primary shard only the files that have changed in the meantime`.

So I did an experiment.

Here are 5 nodes in my cluster,Primary shards `1` is saved in node `1`,and replica shards `1` is saved in node `2`.When i restart node `1` and node `2`,Primary shards `1`'s state become `UNASSIGNED`,and replica shards `1`'s state become `UNASSIGNED` too,the health of the cluster become `red`,and the health never become `green`.And the cluster stop to work until i delete the old index.
</description><key id="134752327">16729</key><summary>Why es cluster stop to work until i delete the old index?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhuqunzhou</reporter><labels /><created>2016-02-19T02:12:25Z</created><updated>2016-02-19T04:41:07Z</updated><resolved>2016-02-19T04:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2016-02-19T04:41:07Z" id="186049435">This kind of issue should best be handled in our [forum](https://discuss.elastic.co/c/elasticsearch), or in [stackoverflow](http://stackoverflow.com/questions/35496113/why-es-cluster-stop-to-work-until-i-delete-the-old-index) (where I see you already asked the same question).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation is misleading regarding write consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16728</link><project id="" key="" /><description>The [guide](https://www.elastic.co/guide/en/elasticsearch/guide/master/distrib-write.html) says:

&gt; Here is the sequence of steps necessary to successfully create, index, or delete a document on both the primary and any replica shards:
&gt; 1. The client sends a create, index, or delete request to Node 1.
&gt; 2. The node uses the document&#8217;s _id to determine that the document belongs to shard 0. It forwards the request to Node 3, where the primary copy of shard 0 is currently allocated.
&gt; 3. Node 3 executes the request on the primary shard. If it is successful, it forwards the request in parallel to the replica shards on Node 1 and Node 2. **Once _all_ of the replica** shards report success, Node 3 reports success to the coordinating node, which reports success to the client.

However according to the latest resiliency talk from Elastic{ON} 2016, this is not the case, otherwise #7572 would not happen. According to the presenter (and the [comment](https://github.com/elastic/elasticsearch/issues/7572#issuecomment-59893514) in the issue), the actual situation is as follows:
1. Primary shard receives indexing request
2. Primary shard checks that all/quorum replicas are online and fails request otherwise
3. Primary shard performs index operation on itself
4. Primary shard acks success to the client
5. Primary shard sends indexing request to replicas

The documentation is very misleading making a reader to believe that by the time he gets ack, its data already exists on all/quorum replicas, i.e. safe; and that's not the case.

I think that this behavior should be clearly outlined, until this issue is fixed in 5.x, both in the guide link above and in the [reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-consistency).
</description><key id="134749963">16728</key><summary>Documentation is misleading regarding write consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haizaar</reporter><labels><label>docs</label></labels><created>2016-02-19T01:56:38Z</created><updated>2016-02-28T22:56:24Z</updated><resolved>2016-02-28T22:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-02-25T18:29:59Z" id="188919953">We're aware of a number of issues with the definitive guide, which wasn't updated for 2.x.  Unfortunately, this is just one exampke of quite a number of pages in that book which are not accurate currently.  So we're actively in the process of rewriting it, as noted at the top of all pages in that guide.  Thanks for pointing this one out specifically.
</comment><comment author="haizaar" created="2016-02-25T18:35:06Z" id="188921367">Will it help if I submit pull request that clarifies the issue?
On 25 Feb 2016 8:31 pm, "Shane Connelly" notifications@github.com wrote:

&gt; We're aware of a number of issues with the definitive guide, which wasn't
&gt; updated for 2.x. Unfortunately, this is just one exampke of quite a number
&gt; of pages in that book which are not accurate currently. So we're actively
&gt; in the process of rewriting it, as noted at the top of all pages in that
&gt; guide. Thanks for pointing this one out specifically.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16728#issuecomment-188919953
&gt; .
</comment><comment author="clintongormley" created="2016-02-28T22:56:24Z" id="189963178">Hi @haizaar 

Actually, your list is not correct:

&gt; Primary shard receives indexing request
&gt; Primary shard checks that all/quorum replicas are online and fails request otherwise

It waits for the `timeout` period for the replica shards to become available before failing.

&gt; Primary shard performs index operation on itself
&gt; Primary shard acks success to the client

This only happens once all active shards have either successfully indexed the document or have been marked as failed shards by the primary.

&gt; Primary shard sends indexing request to replicas

You're welcome to submit a PR, but the repo for the guide is here: https://github.com/elastic/elasticsearch-definitive-guide/issues
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce limits like max file descriptors and fail to start if node is using non-localhost network</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16727</link><project id="" key="" /><description>today we only log messages if certain limits like max number of file-descriptors is too low. This is critical in several production environments and we do the non-strict check only to not punish the out of the box experience. Today we only listen to the localhost by default which is a good indicator that we are not running in production. If we run in production ie. there is actually some non-default network interface configured we should enforce limits and also require minimum master nodes set etc. This can happen case by case but the OOB experiences should not punish the users running in production.
</description><key id="134735273">16727</key><summary>Enforce limits like max file descriptors and fail to start if node is using non-localhost network</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-19T00:08:53Z</created><updated>2016-02-22T18:30:36Z</updated><resolved>2016-02-22T18:30:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-19T01:58:10Z" id="186010464">Great idea
</comment><comment author="jasontedor" created="2016-02-19T02:51:42Z" id="186023124">:heart:
</comment><comment author="jasontedor" created="2016-02-19T03:27:01Z" id="186030138">This is tricky because of situations where the node is bound to localhost but publishes to an external interface and sits behind a reverse proxy.
</comment><comment author="s1monw" created="2016-02-19T17:09:09Z" id="186308095">@jasontedor this is what I had in mind https://github.com/s1monw/elasticsearch/commit/1109e29004002e30bc0cfaacdb5f4f838722db5e
</comment><comment author="bleskes" created="2016-02-19T18:15:34Z" id="186342916">&gt; This is tricky because of situations where the node is bound to localhost but publishes to an external interface and sits behind a reverse proxy.

Our protection may not work in this case but treating non-localhost bound ES as production will improve things for most (if not almost all) of the deployments. I think we should do this? (And also the other thing we discussed - i.e., requiring minimum master nodes , in a follow up change).
</comment><comment author="jasontedor" created="2016-02-19T19:13:40Z" id="186365266">&gt; Our protection may not work in this case but treating non-localhost bound ES as production will improve things for most (if not almost all) of the deployments. I think we should do this?

@bleskes Note my [earlier :heart:](https://github.com/elastic/elasticsearch/issues/16727#issuecomment-186023124); I'm 100% in agreement we should do this and other similar checks, and only pointing out there are cases that we won't catch if we _only_ look at what address we are bound to.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JarHell: Check the parent classloader rather than the system classloader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16726</link><project id="" key="" /><description>Currently, JarHell checks the system classloader to find issues with jar collisions.

In order to support OSGi environments for **testing** embedded nodes, it's effectively a requirement that the parent classloader be checked instead.

Pros:
- The ability to embed ES into an OSGi environment, generally for testing capabilities in integration tests.

Cons:
- We currently do not support OSGi environments, which is why we do not test against them. Doing this type of check explicitly
- Embedding nodes is not a recommended or supported in any environment.
- This introduces the potential for unexpected issues "just" for the sake of supporting OSGi.
- Supporting Java 9's jigsaw modularity may require different semantics.
</description><key id="134726486">16726</key><summary>JarHell: Check the parent classloader rather than the system classloader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2016-02-18T23:14:21Z</created><updated>2016-08-30T22:04:06Z</updated><resolved>2016-08-30T22:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-18T23:36:33Z" id="185980657">There isn't a way to check an arbitrary classloader. If its a URLClassLoader, then you can, but this is not the case anymore since jigsaw. That's why `java.class.path` is used: it works everywhere when elasticsearch is run the way we run it (which is not OSGI).

Adding a special hook for running in an OSGI container, e.g. an `instanceof URLClassLoader` might seem like a good idea, but then we lose important checks for everyone on java 8 that happen in the parsing itself (these detect e.g. stale shell scripts that add empty classpath elements, which convert to surprising locations).

trying to be even smarter and conditionally do something "only when its a URLClassLoader and we are in a container" is fairly sneaky, but might work. But we have to add this "container detection" logic (which likely needs special privileges for CL.getSystemClassLoader call) during bootstrap and expose it in BootStrapInfo... and somehow know that all this is working properly when we don't test running inside any containers. And there is really no guarantee containers are using URLClassLoaders.
</comment><comment author="s1monw" created="2016-02-18T23:55:18Z" id="185984719">there is also a setting in 2.2 that allows you to disable jarhell checks for testing `
tests.jarhell.check=false` this might be the better solution for testing?
</comment><comment author="pickypg" created="2016-02-19T00:15:50Z" id="185989254">@s1monw It ends up being bypassed when testing with plugins because the plugin manager bypasses the check.
</comment><comment author="rjernst" created="2016-02-19T14:54:08Z" id="186244297">@pickypg Not if you construct a Node the new way. In master that would be using the Node constructor directly which takes plugin classes, or on 2.x using MockNode (as our tests do).
</comment><comment author="s1monw" created="2016-02-21T18:32:22Z" id="186881880">@pickypg is this solving you issue, can we close?
</comment><comment author="jasontedor" created="2016-08-30T22:03:58Z" id="243596168">We [do not support](https://www.elastic.co/blog/elasticsearch-the-server) embedded Elasticsearch nodes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hot inlined methods in your area</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16725</link><project id="" key="" /><description>An important optimization that a compiler can make is to inline
methods. The JVM JIT compiler makes decisions about inlining methods
based on method size and how hot the method is. For example, small
methods (less than 35 bytes by default, but tunable via
`-XX:MaxInlineSize=N` ) are automatically inlined. And hot methods are
inlined only if they are not large (less than 325 bytes by default, but
tunable via `-XX:MaxFreqInlineSize=N`). Hot methods that the JVM could
not inline in Elasticsearch can be found by enabling the JVM to trace
method inlining via `-XX:+UnlockDiagnoticVMOptions -XX:+PrintInlining`
and then looking for "hot method too big" for any methods in a
sub-package of `org.elasticsearch`. It is generally recommend to not
tune the inlining flags. Instead, we can enable the JVM to inline these
hot methods by breaking these methods down into smaller methods.

The methods that are hot but could not be inlined while benchmarking
Elasticsearch are:

``` text
org.elasticsearch.action.bulk.TransportShardBulkAction::shardOperationOnPrimary (1918 bytes)   hot method too big
org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase::doRun (344 bytes)   hot method too big
org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase::&lt;init&gt; (440 bytes)   hot method too big
org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase::doRun (739 bytes)   hot method too big
org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$DateMathExpressionResolver::resolveExpression (740 bytes)   hot method too big
org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver::resolve (1110 bytes)   hot method too big
org.elasticsearch.cluster.metadata.MetaData::resolveIndexRouting (351 bytes)   hot method too big
org.elasticsearch.common.Base64::decode (389 bytes)   hot method too big
org.elasticsearch.common.Base64::decode4to3 (350 bytes)   hot method too big
org.elasticsearch.common.Base64::encodeBytesToBytes (456 bytes)   hot method too big
org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker::addEstimateBytesAndMaybeBreak (372 bytes)   hot method too big
org.elasticsearch.common.xcontent.XContentBuilder::writeValue (1203 bytes)   hot method too big
org.elasticsearch.http.netty.NettyHttpChannel::getStatus (388 bytes)   hot method too big
org.elasticsearch.http.netty.NettyHttpChannel::sendResponse (562 bytes)   hot method too big
org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$OrdinalsStore::addOrdinal (348 bytes)   hot method too big
org.elasticsearch.index.mapper.DocumentParser::parseObject (687 bytes)   hot method too big
org.elasticsearch.rest.support.RestUtils::decodeComponent (387 bytes)   hot method too big
org.elasticsearch.search.aggregations.AggregationPhase::execute (558 bytes)   hot method too big
org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator::buildAggregation (540 bytes)   hot method too big
org.elasticsearch.search.aggregations.bucket.terms.InternalTerms::doReduce (687 bytes)   hot method too big
org.elasticsearch.search.controller.SearchPhaseController::sortDocs (673 bytes)   hot method too big
org.elasticsearch.search.fetch.FetchPhase::execute (645 bytes)   hot method too big
org.elasticsearch.search.internal.InternalSearchHit::toXContent (890 bytes)   hot method too big
org.elasticsearch.search.query.QueryPhase::execute (1174 bytes)   hot method too big
```

This commit refactors most of the methods that are hot but the JVM could
not inline because they are too large so that they JVM can now inline
them. Simple micro-benchmarking on a few (but not all) of these methods
showed modest performance gains on the order of 5%. This refactoring has
the additional advantage that by breaking these large methods into
smaller methods, they are simpler to understand and maintain.

The methods not addressed by this pull request are:

```
org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$DateMathExpressionResolver::resolveExpression (740 bytes)   hot method too big
org.elasticsearch.search.aggregations.AggregationPhase::execute (558 bytes)   hot method too big
org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator::buildAggregation (540 bytes)   hot method too big
org.elasticsearch.search.aggregations.bucket.terms.InternalTerms::doReduce (687 bytes)   hot method too big
org.elasticsearch.search.controller.SearchPhaseController::sortDocs (673 bytes)   hot method too big
org.elasticsearch.search.fetch.FetchPhase::execute (645 bytes)   hot method too big
org.elasticsearch.search.internal.InternalSearchHit::toXContent (890 bytes)   hot method too big
org.elasticsearch.search.query.QueryPhase::execute (1174 bytes)   hot method too big
```

All other methods are addressed.
</description><key id="134721956">16725</key><summary>Hot inlined methods in your area</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-18T22:50:48Z</created><updated>2017-01-20T14:04:17Z</updated><resolved>2016-02-24T00:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-19T15:23:02Z" id="186258444">@jasontedor out of curiosity -  do you have any numbers about the performance delta between current code and your PR?
</comment><comment author="jasontedor" created="2016-02-19T22:55:23Z" id="186441151">&gt; do you have any numbers about the performance delta between current code and your PR?

@bleskes I do, and there are issues which I've discussed with @danielmitterdorfer. The issue is that there is variance in the benchmark results which I found to be due to a large variance in the number of segments; this hides the impact of this change (which is expected to be small but positive). 
</comment><comment author="jasontedor" created="2016-02-23T00:23:22Z" id="187449897">@danielmitterdorfer Is there any chance of getting this reviewed before the code base starts moving rapidly again?
</comment><comment author="danielmitterdorfer" created="2016-02-23T21:51:21Z" id="187929878">I left a few minor comments here and there. I also ran the benchmarks multiple times but the changes hide in the run-to-run variance. Nevertheless, I think it is great to have shorter methods and it improves maintainability. Btw, I've also liked very much how you've split your commits for this PR. 

Apart from the minor comments, LGTM.
</comment><comment author="jasontedor" created="2016-02-24T00:34:36Z" id="187987539">Thanks for the review @danielmitterdorfer; I integrated your feedback into the [version that was pushed to master](https://github.com/elastic/elasticsearch/compare/a9eb668497dc...bd5c7f088974).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node does not fall back if `on_conflict_ is set to prefer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16724</link><project id="" key="" /><description>I was reported an issue that if a tribe node is connected to two clusters, and if `tribe.on_conflict` is configured to prefer a cluster for a particular index, then if this cluster is down, the tribe node fails to fall back to the other cluster and the tribe node just fails search requests with a shard-not-available exception.
</description><key id="134696882">16724</key><summary>Tribe node does not fall back if `on_conflict_ is set to prefer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>bug</label></labels><created>2016-02-18T21:06:50Z</created><updated>2016-02-28T18:47:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in InternalClusterService.add() using TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16723</link><project id="" key="" /><description>We are using a TransportClient to connect to ES 2.2.0, and getting the following NullPointerException:

! java.lang.NullPointerException: null
! at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:281) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:154) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.cluster.ClusterStateObserver.waitForNextChange(ClusterStateObserver.java:99) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.retry(TransportMasterNodeAction.java:190) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.doStart(TransportMasterNodeAction.java:164) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.start(TransportMasterNodeAction.java:121) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:93) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:50) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:45) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:41) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[netty-3.10.5.Final.jar:na]
! at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75) ~[elasticsearch-2.2.0.jar:2.2.0]
! at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[netty-3.10.5.Final.jar:na]
! at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[netty-3.10.5.Final.jar:na]
! at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_65]
! at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_65]
! at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_65]

We can reproduce this fairly often in our system tests, which starts an ES instance right before starting our client application.  The client application will create a TransportClient, then immediately do a health status check and wait for a yellow status.  If we put a sleep() between creating the TransportClient and the health check, the problem doesn't manifest.

InternalClusterService has an updateTasksExecutor that gets populated by the doStart() method.  The InternalClusterService, however, only appears to be started via Node.start(), which is started via Bootstrap.start(), which is only called by Elasticsearch.main().  In other words, the updateTasksExecutor is only populated when the ES server is run, not when an ES client is used.

The stacktrace above appears to be related to the Transport starting up and not finding a master node.  It then creates a listener to watch for changes to the master node, and tries to add this to the updateTasksExecutor of InternalClusterService, which based on the above paragraph, is null.

I believe we are running into this because we are starting ES immediately before starting our client app, and connecting before a master node has been determined.

Regards,
Ed Howe
Nexidia, Inc.
</description><key id="134693572">16723</key><summary>NullPointerException in InternalClusterService.add() using TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EdHowe</reporter><labels /><created>2016-02-18T20:54:37Z</created><updated>2016-02-24T02:07:36Z</updated><resolved>2016-02-24T02:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-21T17:44:47Z" id="186870876">The problem doesn't lie in the transport client, but rather at the node that first receives the request. This stems from the problem that the cluster service is only started after the transport service, meaning that the node accepts requests before it is fully ready to receive them.  

I've opened #16746 . Thanks for reporting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves GCE settings to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16722</link><project id="" key="" /><description>Closes #16720.
</description><key id="134640263">16722</key><summary>Moves GCE settings to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-18T17:17:31Z</created><updated>2017-01-20T14:04:26Z</updated><resolved>2016-02-20T01:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-18T17:17:47Z" id="185820898">@danielmitterdorfer Could you review it please?
</comment><comment author="danielmitterdorfer" created="2016-02-18T19:19:44Z" id="185872391">@dadoonet I've left a few comments.
</comment><comment author="dadoonet" created="2016-02-18T20:15:03Z" id="185896477">Thank you @danielmitterdorfer! I added a new commit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High disk usage with nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16721</link><project id="" key="" /><description>I have posted this question on the ES forum a while ago:
https://discuss.elastic.co/t/nesting-high-disk-usage/40626
What makes my index use such high amounts of disk space, when it consumes so little without nesting?
</description><key id="134640087">16721</key><summary>High disk usage with nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abulhol</reporter><labels /><created>2016-02-18T17:16:45Z</created><updated>2016-02-28T19:35:04Z</updated><resolved>2016-02-28T19:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ebuildy" created="2016-02-19T13:03:58Z" id="186205531">Hello,

Nested data type creates a separate document when indexing, whereas object will result as flattens object hierarchies into a simple list of field names and values

As you can read on the doc: 

https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html#_using_literal_nested_literal_fields_for_arrays_of_objects

So there is some overhead because document has an uuid, version fields etc.. 
</comment><comment author="abulhol" created="2016-02-22T08:40:38Z" id="187074029">Hi Thomas,
Thank you for the explanation. I have made a calculation:
I have indexed 615 docs, once with and once without nesting.
With nesting, I have 17,123,628 docs and 15.4 GB.
Without nesting, it is 615 docs and 1.6 GB.
(using curl -XGET '127.0.0.1:9200/_my_index_/_stats/store,docs?pretty&amp;human')

So the overhead per doc seems to be approximately (15.4GB-1.6GB)/(17123628-615) = 0.85 KByte

Is that what you would expect as well?
</comment><comment author="clintongormley" created="2016-02-28T19:35:04Z" id="189930434">Hi @abulhol 

You can get more info about what is using disk space via the segments API with the verbose flag.  Either way, this discussion is better suited to the forum rather than her, so i'll close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves GCE settings to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16720</link><project id="" key="" /><description>As we did for other discovery plugins, we can move GCE settings to the new Settings infra.
</description><key id="134623550">16720</key><summary>Moves GCE settings to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>v5.0.0-alpha1</label></labels><created>2016-02-18T16:11:24Z</created><updated>2016-02-20T01:08:17Z</updated><resolved>2016-02-20T01:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>marvel show nodes Incorrectly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16719</link><project id="" key="" /><description>I have five nodes in my cluster.

Firstly,marvel show the number of nodes correctly,But when i restart a node,marvel will add an offline  node. For example,if i restart a node(named `es-1`) N times,then marvel will show N+1 nodes named `es-1`,only one node's state is online,others are offline.
</description><key id="134556655">16719</key><summary>marvel show nodes Incorrectly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhuqunzhou</reporter><labels /><created>2016-02-18T11:39:08Z</created><updated>2016-02-29T09:05:18Z</updated><resolved>2016-02-28T18:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T18:40:26Z" id="189920113">Hi @zhuqunzhou 

This problem is fixed in v2.3.0 (correct @tlrx ?)
</comment><comment author="tlrx" created="2016-02-29T09:05:18Z" id="190111741">Yes, it has been fixed and will be released in v2.3.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix waiting for pidfile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16718</link><project id="" key="" /><description>Closes #16717
</description><key id="134547091">16718</key><summary>Fix waiting for pidfile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">biolds</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-18T11:01:50Z</created><updated>2016-02-29T20:34:45Z</updated><resolved>2016-02-29T20:02:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T14:14:10Z" id="189881539">@nik9000 could you take a look at this please
</comment><comment author="nik9000" created="2016-02-28T15:23:13Z" id="189891481">Will do on Monday.
On Feb 28, 2016 9:14 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 could you take a look at this please
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16718#issuecomment-189881539
&gt; .
</comment><comment author="nik9000" created="2016-02-29T19:26:39Z" id="190345754">Ok - reviewed. We should totally do this. Saying you wait for the pid and not waiting is not cool. The reason this hasn't come up before is that it usually shows up quickly anyway so this is hard to test. I'll see if I can put something together.
</comment><comment author="nik9000" created="2016-02-29T20:02:27Z" id="190358735">Ok - the vagrant based package tests aren't in working order at the moment (we don't run them in CI because we haven't done enough pre work) so I'll merge this sans test because its obviously better.
</comment><comment author="nik9000" created="2016-02-29T20:05:14Z" id="190359576">Thanks @biolds ! I'm going to backport this to the 2.x branch as well so it'll go out with 2.3.
</comment><comment author="nik9000" created="2016-02-29T20:34:45Z" id="190375189">Backported to 2.x with afbc98a47fc9b102c121c63f19581f5e74476776
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deb's init script not waiting for pid before exiting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16717</link><project id="" key="" /><description>In the debian init script, here:
https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L174

The check does not work as expected and exits before the pid file is filled.
xargs exit successfully because nothing is sent to it's stdin:

```
$ cat /dev/null | xargs kill -0 ; echo $?
0
```
</description><key id="134546777">16717</key><summary>deb's init script not waiting for pid before exiting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">biolds</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-02-18T11:00:25Z</created><updated>2016-02-29T20:02:29Z</updated><resolved>2016-02-29T20:02:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Shard relocations keep failing after node restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16716</link><project id="" key="" /><description>Hi,

We're running ES version 2.1.0. Cluster consits of three nodes (all of them have `node.data` and `node.master` set to true). When doing yesterday rolling restart of nodes (glibc patching), the first server was restarted, shard allocation was set to all and all went great. When second node was restarted, after coming back it was stuck with some shards in 'RELOCATING' state for a few hours. When checked logs, they were constatly flooded with the exception below. I have tried to shut down the node, remove whole data directory and start it again, so that it has fresh state and gets all shard replicas from another nodes. What's strange, the situation repeated:

```
[2016-02-18 10:31:06,249][WARN ][indices.cluster          ] [psz-ses-02] [products_spryker-2016-02-17-21-39-29] failed to add mapping [product], source [{"product":{"_all":{"analyzer":"main_analyzer"},"pr
java.lang.IllegalArgumentException: Mapper for [_all] conflicts with existing mapping in other types:
[mapper [_all] cannot be changed from type [_all] to [string]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:418)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:372)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:177)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

I do not understand why to mapping problem appears, as the node was fresh and didn't have any indices / mappings.

We have applied a intermediate workaround during the night - recreate indexes we use (with another name) and drop the old ones. This went smooth - shards (primaries/replicas) were allocated fine on all three nodes and no exceptions since deleting old indices.

Today we wanted to apply the security patches to the third node and repeated situation - restarted third node (according to procedure described on https://www.elastic.co/guide/en/elasticsearch/guide/current/_rolling_restarts.html) - again, when node came back, some shards are stuck in RELOCATING state, which never changes and logs are flooded with the strange exception above.
Never experienced this situation on earlier ES versions. Not sure if it's a bug, but please check if this could be related to any internals of Eleasticsearch - the operation we executed was very easy rolling cluster restart, which resulted in unstability in two out of three tries.
</description><key id="134545019">16716</key><summary>Shard relocations keep failing after node restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">marek-obuchowicz</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-02-18T10:54:34Z</created><updated>2016-04-06T10:25:11Z</updated><resolved>2016-04-06T10:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marek-obuchowicz" created="2016-02-18T10:57:56Z" id="185655104">Soma additional outputs:

```
GET /_cat/shards:
products_spryker-2016-02-17-21-39-29 2     r      STARTED    107042 240.8mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 2     p      STARTED    107042 240.9mb 10.22.112.98 psz-ses-00
products_spryker-2016-02-17-21-39-29 1     p      STARTED    106425 241.9mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 1     r      RELOCATING 106425 241.9mb 10.22.112.98 psz-ses-00 -&gt; 10.22.112.64 6t4CEQiNTRaltfhWsMkTJA psz-ses-02
products_spryker-2016-02-17-21-39-29 5     r      STARTED    105494 226.2mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 5     p      STARTED    105494 226.2mb 10.22.112.98 psz-ses-00
products_spryker-2016-02-17-21-39-29 4     r      STARTED    105446 229.9mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 4     p      STARTED    105446 244.8mb 10.22.112.98 psz-ses-00
products_spryker-2016-02-17-21-39-29 3     r      STARTED    105700 226.3mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 3     p      STARTED    105700   232mb 10.22.112.98 psz-ses-00
products_spryker-2016-02-17-21-39-29 0     r      STARTED    104388 217.5mb 10.22.113.51 psz-ses-01
products_spryker-2016-02-17-21-39-29 0     p      RELOCATING 104388 235.6mb 10.22.112.98 psz-ses-00 -&gt; 10.22.112.64 6t4CEQiNTRaltfhWsMkTJA psz-ses-02
```

```
GET /_cat/recovery:
index                                shard time    type       stage    source_host  target_host  repository snapshot files files_percent bytes     bytes_percent total_files total_bytes translog translog_percent total_translog
products_spryker-2016-02-17-21-39-29 0     235958  replica    done     10.22.112.98 10.22.113.51 n/a        n/a      1     100.0%        130       100.0%        1           130         22436    100.0%           22436
products_spryker-2016-02-17-21-39-29 0     1554689 relocation translog 10.22.112.98 10.22.112.64 n/a        n/a      1     100.0%        130       100.0%        1           130         0        0.0%             22444
products_spryker-2016-02-17-21-39-29 0     90      replica    done     10.22.113.51 10.22.112.98 n/a        n/a      1     100.0%        130       100.0%        1           130         0        100.0%           0
products_spryker-2016-02-17-21-39-29 1     1831    replica    done     10.22.112.98 10.22.113.51 n/a        n/a      31    100.0%        37723981  100.0%        184         253227154   0        100.0%           0
products_spryker-2016-02-17-21-39-29 1     1554690 replica    translog 10.22.113.51 10.22.112.64 n/a        n/a      0     0.0%          0         0.0%          0           0           0        0.0%             3
products_spryker-2016-02-17-21-39-29 1     398     replica    done     10.22.113.51 10.22.112.98 n/a        n/a      0     0.0%          0         0.0%          0           0           2        100.0%           2
```

(numbers in _cat/recovery do not change over time, it's frozen with this status)
</comment><comment author="Lavode" created="2016-02-18T13:39:30Z" id="185723112">Potentially related to the issue mentioned above, we also have some of the nodes occasionally spawning a few thousand threads, which ultimately leads to crashes caused by OOM.

Right now we're slightly above 6.5k threads, with an additional ~2 being spawned each second.

```
root@psz-ses-00:~# ps -Lf -p 570 | wc -l
6502
```

The threads in question:

```
elastic+   570     1   570  0 5702 Feb17 ?        00:00:00 /usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-production
elastic+   570     1   642  0 5702 Feb17 ?        00:00:08 /usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-production
elastic+   570     1   673  0 5702 Feb17 ?        00:11:52 /usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-production
elastic+   570     1   674  0 5702 Feb17 ?        00:11:52 /usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-production
elastic+   570     1   675  0 5702 Feb17 ?        00:11:52 /usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-productio
```

Here an example of a 'healthy' and a 'misbehaving' node's memory usage &amp; thread count.
![elasticsearch_memory](https://cloud.githubusercontent.com/assets/1827358/13144848/3ec4e804-d64d-11e5-889c-afedcf48b1bd.jpg)
</comment><comment author="s1monw" created="2016-02-18T22:03:02Z" id="185942221">can you provide your mappings please and your settings?
</comment><comment author="marek-obuchowicz" created="2016-02-19T00:27:44Z" id="185992118">I have sent mappings, settings and configuration to e-mail address associated with your github account
</comment><comment author="marek-obuchowicz" created="2016-02-19T13:25:55Z" id="186212641">Some update - instead of shutting down / starting nodes, I tried to move shards manually using API (in order to move all shards from a host before shutting it down).
By doing manual operations `_cluster/reroute` - commands, move - i have not observed any problems with relocating primary shards. However, the issue mentioned above happens when trying to relocate replica shards:

```
[2016-02-19 13:23:24,888][WARN ][cluster.action.shard     ] [psz-ses-00] [products_spryker-2016-02-19-11-56-53][0] received shard failed for [products_spryker-2016-02-19-11-56-53][0], node[8ae9oqRCS9SjVepIgJ_OYg], relocating [WNL6ds1EQcKs2n3bwwB01w], [P], v[14], s[INITIALIZING], a[id=fR1O4e-vTDKmxIeZ91QMew, rId=TiQmjZaRRlykJHE3P0thog], expected_shard_size[201664505], indexUUID [YtxdtlD5TtGPcV2K_Anm0g], message [failed to update mappings], failure [IllegalArgumentException[Mapper for [_all] conflicts with existing mapping in other types:
[mapper [_all] cannot be changed from type [_all] to [string]]]]
java.lang.IllegalArgumentException: Mapper for [_all] conflicts with existing mapping in other types:
[mapper [_all] cannot be changed from type [_all] to [string]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:418)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:372)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:177)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-19 13:23:24,888][WARN ][cluster.action.shard     ] [psz-ses-00] [products_spryker-2016-02-19-11-56-53][0] received shard failed for [products_spryker-2016-02-19-11-56-53][0], node[8ae9oqRCS9SjVepIgJ_OYg], relocating [WNL6ds1EQcKs2n3bwwB01w], [P], v[14], s[INITIALIZING], a[id=fR1O4e-vTDKmxIeZ91QMew, rId=TiQmjZaRRlykJHE3P0thog], expected_shard_size[201664505], indexUUID [YtxdtlD5TtGPcV2K_Anm0g], message [master {psz-ses-00}{WNL6ds1EQcKs2n3bwwB01w}{10.22.112.98}{10.22.112.98:25005}{max_local_storage_nodes=1, master=true} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
```
</comment><comment author="marek-obuchowicz" created="2016-02-19T15:16:38Z" id="186255602">Process startup parameters:

```
/usr/lib/jvm/java-7-openjdk-amd64//bin/java -Xms16384m -Xmx16384m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/var/log/elasticsearch-production/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch-production.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/data/logs/production/elasticsearch --default.path.data=/data/shop/production/shared/elasticsearch --default.path.conf=/etc/elasticsearch-production
```

Host info:

```
32242 MB RAM, Debian GNU/Linux 8.3 (jessie), kernel 3.16.0-4, AWS instance m4.2xlarge, ami-02b78e1f
java version "1.7.0_95"
OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-1~deb8u1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)
```

Storage

```
EBS volumes
/dev/xvda1 on / type ext4 (rw,relatime,data=ordered)
/dev/xvdf on /data type ext4 (rw,noatime,nodiratime,nobarrier,data=ordered)
```

Changed sysctl's:

```
vm.swappiness = 5
```

Plugins:

```
Installed plugins in /usr/share/elasticsearch/plugins:
    - elasticsearch-analysis-decompound ver. 2.1.0.0
    - head 
    - cloud-aws
```

Mapping:

```
curl localhost:15005/products_spryker-2016-02-17-21-39-29/_mapping?pretty
{
  "products_spryker-2016-02-17-21-39-29" : {
    "mappings" : {
      "product" : {
        "_all" : {
          "analyzer" : "main_analyzer"
        },
        "properties" : {
          "_all" : {
            "type" : "string",
            "analyzer" : "main_analyzer"
          },
          "brand_name" : {
            "type" : "string",
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "suggest" : {
                "type" : "completion",
                "analyzer" : "simple",
                "payloads" : true,
                "preserve_separators" : true,
                "preserve_position_increments" : true,
                "max_input_length" : 50
              }
            }
          },
          "brand_suggest" : {
            "type" : "completion",
            "analyzer" : "simple",
            "payloads" : true,
            "preserve_separators" : true,
            "preserve_position_increments" : true,
            "max_input_length" : 50
          },
          "de_CH" : {
            "properties" : {
              "attributes" : {
                "properties" : {
                  "productScore" : {
                    "type" : "long"
                  },
                  "tags" : {
                    "type" : "string",
                    "fields" : {
                      "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                      }
                    }
                  }
                }
              },
              "categories" : {
                "type" : "nested",
                "properties" : {
                  "dump" : {
                    "type" : "string",
                    "index" : "not_analyzed"
                  },
                  "id" : {
                    "type" : "long"
                  },
                  "name" : {
                    "type" : "string",
                    "fields" : {
                      "decomp" : {
                        "type" : "string",
                        "analyzer" : "decomp_analyzer",
                        "search_analyzer" : "main_analyzer"
                      },
                      "english_words" : {
                        "type" : "string",
                        "analyzer" : "english_main_analyzer"
                      },
                      "front" : {
                        "type" : "string",
                        "analyzer" : "edgengram_analyzer"
                      },
                      "ngrams" : {
                        "type" : "string",
                        "analyzer" : "ngram_analyzer"
                      },
                      "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                      },
                      "shingles" : {
                        "type" : "string",
                        "analyzer" : "analyzer_shingle"
                      },
                      "suggest" : {
                        "type" : "completion",
                        "analyzer" : "simple",
                        "payloads" : true,
                        "preserve_separators" : true,
                        "preserve_position_increments" : true,
                        "max_input_length" : 50
                      }
                    }
                  }
                }
              },
              "long_description" : {
                "type" : "string",
                "analyzer" : "main_analyzer",
                "fields" : {
                  "correction" : {
                    "type" : "string",
                    "analyzer" : "suggestion_analyzer"
                  },
                  "decomp" : {
                    "type" : "string",
                    "analyzer" : "decomp_analyzer",
                    "search_analyzer" : "main_analyzer"
                  },
                  "english_words" : {
                    "type" : "string",
                    "analyzer" : "english_main_analyzer"
                  },
                  "front" : {
                    "type" : "string",
                    "analyzer" : "edgengram_analyzer"
                  },
                  "keywords" : {
                    "type" : "string",
                    "analyzer" : "keywords_analyzer"
                  },
                  "ngrams" : {
                    "type" : "string",
                    "analyzer" : "ngram_analyzer"
                  },
                  "prefix" : {
                    "type" : "string",
                    "analyzer" : "prefix_analyzer"
                  },
                  "shingles" : {
                    "type" : "string",
                    "analyzer" : "analyzer_shingle"
                  },
                  "suggest" : {
                    "type" : "completion",
                    "analyzer" : "simple",
                    "payloads" : true,
                    "preserve_separators" : true,
                    "preserve_position_increments" : true,
                    "max_input_length" : 50
                  }
                }
              },
              "name" : {
                "type" : "string",
                "analyzer" : "main_analyzer",
                "fields" : {
                  "correction" : {
                    "type" : "string",
                    "analyzer" : "suggestion_analyzer"
                  },
                  "decomp" : {
                    "type" : "string",
                    "analyzer" : "decomp_analyzer",
                    "search_analyzer" : "main_analyzer"
                  },
                  "english_words" : {
                    "type" : "string",
                    "analyzer" : "english_main_analyzer"
                  },
                  "front" : {
                    "type" : "string",
                    "analyzer" : "edgengram_analyzer"
                  },
                  "keywords" : {
                    "type" : "string",
                    "analyzer" : "keywords_analyzer"
                  },
                  "ngrams" : {
                    "type" : "string",
                    "analyzer" : "ngram_analyzer"
                  },
                  "prefix" : {
                    "type" : "string",
                    "analyzer" : "prefix_analyzer"
                  },
                  "shingles" : {
                    "type" : "string",
                    "analyzer" : "analyzer_shingle"
                  },
                  "suggest" : {
                    "type" : "completion",
                    "analyzer" : "simple",
                    "payloads" : true,
                    "preserve_separators" : true,
                    "preserve_position_increments" : true,
                    "max_input_length" : 50
                  }
                }
              },
              "parent_categories" : {
                "type" : "nested",
                "properties" : {
                  "id" : {
                    "type" : "long"
                  },
                  "name" : {
                    "type" : "string",
                    "fields" : {
                      "decomp" : {
                        "type" : "string",
                        "analyzer" : "decomp_analyzer",
                        "search_analyzer" : "main_analyzer"
                      },
                      "english_words" : {
                        "type" : "string",
                        "analyzer" : "english_main_analyzer"
                      },
                      "front" : {
                        "type" : "string",
                        "analyzer" : "edgengram_analyzer"
                      },
                      "ngrams" : {
                        "type" : "string",
                        "analyzer" : "ngram_analyzer"
                      },
                      "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                      },
                      "shingles" : {
                        "type" : "string",
                        "analyzer" : "analyzer_shingle"
                      },
                      "suggest" : {
                        "type" : "completion",
                        "analyzer" : "simple",
                        "payloads" : true,
                        "preserve_separators" : true,
                        "preserve_position_increments" : true,
                        "max_input_length" : 50
                      }
                    }
                  }
                }
              },
              "short_description" : {
                "type" : "string",
                "analyzer" : "main_analyzer",
                "fields" : {
                  "correction" : {
                    "type" : "string",
                    "analyzer" : "suggestion_analyzer"
                  },
                  "decomp" : {
                    "type" : "string",
                    "analyzer" : "decomp_analyzer",
                    "search_analyzer" : "main_analyzer"
                  },
                  "english_words" : {
                    "type" : "string",
                    "analyzer" : "english_main_analyzer"
                  },
                  "front" : {
                    "type" : "string",
                    "analyzer" : "edgengram_analyzer"
                  },
                  "keywords" : {
                    "type" : "string",
                    "analyzer" : "keywords_analyzer"
                  },
                  "ngrams" : {
                    "type" : "string",
                    "analyzer" : "ngram_analyzer"
                  },
                  "prefix" : {
                    "type" : "string",
                    "analyzer" : "prefix_analyzer"
                  },
                  "shingles" : {
                    "type" : "string",
                    "analyzer" : "analyzer_shingle"
                  },
                  "suggest" : {
                    "type" : "completion",
                    "analyzer" : "simple",
                    "payloads" : true,
                    "preserve_separators" : true,
                    "preserve_position_increments" : true,
                    "max_input_length" : 50
                  }
                }
              },
              "top_features" : {
                "type" : "nested",
                "properties" : {
                  "name" : {
                    "type" : "string",
                    "fields" : {
                      "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                      }
                    }
                  },
                  "value" : {
                    "type" : "string",
                    "fields" : {
                      "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                      }
                    }
                  }
                }
              },
              "url" : {
                "type" : "string"
              }
            }
          },
          "images" : {
            "properties" : {
              "highres" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "lowres" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "thumbnails" : {
                "type" : "string",
                "index" : "not_analyzed"
              }
            }
          },
          "is_active" : {
            "type" : "boolean"
          },
          "is_top_seller_category_l1" : {
            "type" : "boolean"
          },
          "is_top_seller_category_l2" : {
            "type" : "boolean"
          },
          "is_top_seller_category_l3" : {
            "type" : "boolean"
          },
          "is_top_seller_home" : {
            "type" : "boolean"
          },
          "is_top_seller_theme_world" : {
            "type" : "boolean"
          },
          "key" : {
            "type" : "string"
          },
          "localizedAttributes" : {
            "type" : "object"
          },
          "merchants" : {
            "properties" : {
              "id" : {
                "type" : "long"
              },
              "is_active" : {
                "type" : "boolean"
              },
              "name" : {
                "type" : "string",
                "fields" : {
                  "raw" : {
                    "type" : "string",
                    "index" : "not_analyzed"
                  }
                }
              },
              "price" : {
                "type" : "long"
              },
              "sku" : {
                "type" : "string"
              },
              "stock_quantity" : {
                "type" : "long"
              },
              "uuid" : {
                "type" : "string"
              }
            }
          },
          "min_price" : {
            "type" : "long"
          },
          "minimum_stock" : {
            "type" : "long"
          },
          "search-result-data" : {
            "properties" : {
              "abstract_name" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "abstract_sku" : {
                "type" : "string",
                "index" : "not_analyzed"
              },
              "id_abstract_product" : {
                "type" : "long"
              },
              "id_touch" : {
                "type" : "long"
              },
              "url" : {
                "type" : "string",
                "index" : "not_analyzed"
              }
            }
          },
          "sku" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "tags" : {
            "type" : "string"
          }
        }
      },
      "marker" : {
        "_all" : {
          "analyzer" : "main_analyzer"
        },
        "properties" : {
          "timestamp" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
```

Settings via API:

```
curl localhost:15005/_settings?pretty
{
  "products_spryker-2016-02-17-21-39-29" : {
    "settings" : {
      "index" : {
        "creation_date" : "1455745169452",
        "number_of_replicas" : "1",
        "max_result_window" : "100000",
        "uuid" : "djrjnzRyQ-OqhoWMdvqBpA",
        "analysis" : {
          "filter" : {
            "decomp" : {
              "type" : "decompound"
            },
            "german_stemmer" : {
              "type" : "stemmer",
              "language" : "light_german"
            },
            "german_stop" : {
              "type" : "stop",
              "stopwords" : "_german_"
            },
            "edgengram_filter" : {
              "max_gram" : "15",
              "type" : "edgeNGram",
              "min_gram" : "4",
              "side" : "front"
            },
            "ngram_filter" : {
              "type" : "nGram",
              "min_gram" : "4",
              "max_gram" : "15"
            },
            "filter_shingle" : {
              "type" : "shingle",
              "min_shingle_size" : "2",
              "max_shingle_size" : "3",
              "output_unigrams" : "false"
            },
            "synonym_filter" : {
              "type" : "synonym",
              "synonyms" : [ "bvlgari,bulgari", "box,kiste,beh&#228;lter", "Handy,natel,mobile phone" ],
              "ignore_case" : "true"
            },
            "filter_stop" : {
              "type" : "stop"
            }
          },
          "analyzer" : {
            "english_main_analyzer" : {
              "type" : "english",
              "filter" : [ "lowercase", "synonym_filter" ],
              "tokenizer" : "standard"
            },
            "prefix_analyzer" : {
              "filter" : "lowercase",
              "tokenizer" : "keyword"
            },
            "main_analyzer" : {
              "type" : "custom",
              "filter" : [ "lowercase", "synonym_filter", "german_stop", "german_normalization", "german_stemmer" ],
              "tokenizer" : "standard"
            },
            "decomp_analyzer" : {
              "filter" : [ "lowercase", "decomp", "unique", "synonym_filter", "german_stop", "german_normalization", "german_stemmer" ],
              "tokenizer" : "decomp"
            },
            "analyzer_shingle" : {
              "type" : "custom",
              "stopwords" : "_german_",
              "char_filter" : [ "html_strip" ],
              "filter" : [ "standard", "lowercase", "synonym_filter", "filter_shingle" ],
              "tokenizer" : "standard"
            },
            "suggestion_analyzer" : {
              "filter" : [ "lowercase" ],
              "tokenizer" : "standard"
            },
            "keywords_analyzer" : {
              "char_filter" : [ "html_strip" ],
              "filter" : [ "german_stop" ],
              "tokenizer" : "standard"
            },
            "ngram_analyzer" : {
              "type" : "custom",
              "char_filter" : [ "html_strip" ],
              "filter" : [ "standard", "lowercase", "synonym_filter", "asciifolding", "edgengram_filter", "ngram_filter" ],
              "tokenizer" : "lowercase"
            },
            "edgengram_analyzer" : {
              "type" : "custom",
              "char_filter" : [ "html_strip" ],
              "filter" : [ "standard", "lowercase", "synonym_filter", "asciifolding", "edgengram_filter" ],
              "tokenizer" : "lowercase"
            }
          },
          "tokenizer" : {
            "decomp" : {
              "type" : "standard",
              "filter" : [ "decomp" ]
            }
          }
        },
        "number_of_shards" : "6",
        "version" : {
          "created" : "2010099"
        }
      }
    }
  },
  "ch_production_catalog" : {
    "settings" : {
      "index" : {
        "creation_date" : "1449005895395",
        "uuid" : "0Wmnq0XNR-OBI20Ts3eNSw",
        "number_of_replicas" : "1",
        "number_of_shards" : "4",
        "version" : {
          "created" : "2010099"
        }
      }
    }
  },
  "stats" : {
    "settings" : {
      "index" : {
        "creation_date" : "1455008875587",
        "uuid" : "vIEGmhOXQ5abPpSVcedwiw",
        "number_of_replicas" : "1",
        "number_of_shards" : "6",
        "version" : {
          "created" : "2010099"
        }
      }
    }
  }
}
```

curl localhost:15005/_cluster/settings?pretty

```
{
  "persistent" : { },
  "transient" : {
    "logger" : {
      "_root" : "INFO"
    },
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "all"
        }
      }
    }
  }
}
```

elasticsearch.yml:

```
cat /etc/elasticsearch-production/elasticsearch.yml  | sed 's/#.*//' | grep -v '^ *$'
cluster.name: spryker-production
node.name: psz-ses-00
node.master: true
node.data: true
node.max_local_storage_nodes: 1
index.number_of_shards: 6
index.number_of_replicas: 1
bootstrap.mlockall: true
network.bind_host: 0.0.0.0
network.publish_host: 10.x.x.x
http.port: 15005
transport.tcp.port: 25005
transport.tcp.compress: false
gateway.recover_after_nodes: 2
gateway.recover_after_time: 3m
gateway.expected_nodes: 3
cluster.routing.allocation.node_initial_primaries_recoveries: 4
cluster.routing.allocation.node_concurrent_recoveries: 2
cluster.routing.allocation.disk.watermark.low: 90%
cluster.routing.allocation.disk.watermark.high: 95%
indices.recovery.max_bytes_per_sec: 50mb
indices.recovery.concurrent_streams: 5
indices.store.throttle.max_bytes_per_sec: 100mb
indices.cache.filter.size: 100M
indices.fielddata.cache.size: 100M
indices.fielddata.cache.expire: 30m
indices.memory.index_buffer_size: 15%
index.query.bool.max_clause_count: 10000
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.timeout: 5s
discovery.zen.ping.multicast.enabled: false
discovery.type: ec2
cloud.aws.region: eu-central-1
discovery.ec2.groups: sg-xxxxxxxx
plugin.mandatory: "cloud-aws"
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s
monitor.jvm.gc.young.warn: 1000ms
monitor.jvm.gc.young.info: 700ms
monitor.jvm.gc.young.debug: 400ms
monitor.jvm.gc.old.warn: 10s
monitor.jvm.gc.old.info: 5s
monitor.jvm.gc.old.debug: 2s
action.disable_delete_all_indices: true
```
</comment><comment author="rjernst" created="2016-02-19T19:55:13Z" id="186378228">Do you have an index template set up?
</comment><comment author="marek-obuchowicz" created="2016-02-19T20:36:34Z" id="186395696">No, I'm not using it. Just double checked, `GET /_template` returns empty response.
</comment><comment author="rjernst" created="2016-02-19T20:40:49Z" id="186396885">What request do you use to create the index? Do you do a put mappings after the index is created?
</comment><comment author="marek-obuchowicz" created="2016-02-19T21:13:44Z" id="186409755">When setting up a new index, we call:

```
PUT /index_name
```

with body like

```
{
   "settings": {
      "index": {
         "max_result_window": 100000
      },
      "analysis": {
         ....
      }
  },
  "mappings": {
    ...
  }
}

And then we put data into new index.
When we need to adjust mappings, we create a new index, point `writer` alias there, put data and when it's done - we point alias `reader` there, so we don't have downtimes. So the answer is - no, we don't update mappings afterwards. 
```
</comment><comment author="rjernst" created="2016-02-19T21:30:48Z" id="186415674">And the mappings are exactly what you posted earlier in your get mappings output? You have _all there under properties as a string, which is what causes the conflict. Bit I am trying to understand how this got through mapping conflict checks, but then triggers them later. 
</comment><comment author="marek-obuchowicz" created="2016-02-19T21:54:02Z" id="186425703">The initial definition for mapping, when creating index, is following:

```
{
   "mappings": {
      "product": {
         "properties": {
            "_all": {
               "type": "string",
               "analyzer": "main_analyzer"
            },
            "is_active": {
               "type": "boolean"
            },
            "sku": {
               "type": "string",
               "index": "not_analyzed"
            },
            "min_price": {
               "type": "long"
            },
            "merchants": {
               "properties": {
                  "id": {
                     "type": "long"
                  },
                  "name": {
                     "type": "string",
                     "fields": {
                        "raw": {
                           "type": "string",
                           "index": "not_analyzed"
                        }
                     }
                  },
                  "is_active": {
                     "type": "boolean"
                  },
                  "price": {
                     "type": "long"
                  }
               }
            },
            "brand_name": {
               "type": "string",
               "fields": {
                  "raw": {
                     "type": "string",
                     "index": "not_analyzed"
                  },
                  "suggest": {
                     "type": "completion",
                     "analyzer": "simple",
                     "search_analyzer": "simple",
                     "payloads": true
                  }
               }
            },
            "brand_suggest": {
               "type": "completion",
               "analyzer": "simple",
               "search_analyzer": "simple",
               "payloads": true
            },
            "de_CH": {
               "properties": {
                  "short_description": {
                     "type": "string",
                     "analyzer": "main_analyzer",
                     "fields": {
                        "english_words": {
                           "type": "string",
                           "analyzer": "english_main_analyzer"
                        },
                        "shingles": {
                           "type": "string",
                           "analyzer": "analyzer_shingle"
                        },
                        "decomp": {
                           "type": "string",
                           "analyzer": "decomp_analyzer",
                           "search_analyzer": "main_analyzer"
                        },
                        "ngrams": {
                           "type": "string",
                           "analyzer": "ngram_analyzer"
                        },
                        "correction": {
                           "type": "string",
                           "analyzer": "suggestion_analyzer"
                        },
                        "suggest": {
                           "type": "completion",
                           "analyzer": "simple",
                           "search_analyzer": "simple",
                           "payloads": true
                        },
                        "front": {
                           "type": "string",
                           "analyzer": "edgengram_analyzer"
                        },
                        "prefix": {
                           "analyzer": "prefix_analyzer",
                           "type": "string"
                        },
                        "keywords": {
                           "type": "string",
                           "analyzer": "keywords_analyzer"
                        }
                     }
                  },
                  "long_description": {
                     "type": "string",
                     "analyzer": "main_analyzer",
                     "fields": {
                        "english_words": {
                           "type": "string",
                           "analyzer": "english_main_analyzer"
                        },
                        "shingles": {
                           "type": "string",
                           "analyzer": "analyzer_shingle"
                        },
                        "decomp": {
                           "type": "string",
                           "analyzer": "decomp_analyzer",
                           "search_analyzer": "main_analyzer"
                        },
                        "ngrams": {
                           "type": "string",
                           "analyzer": "ngram_analyzer"
                        },
                        "correction": {
                           "type": "string",
                           "analyzer": "suggestion_analyzer"
                        },
                        "suggest": {
                           "type": "completion",
                           "analyzer": "simple",
                           "search_analyzer": "simple",
                           "payloads": true
                        },
                        "front": {
                           "type": "string",
                           "analyzer": "edgengram_analyzer"
                        },
                        "prefix": {
                           "analyzer": "prefix_analyzer",
                           "type": "string"
                        },
                        "keywords": {
                           "type": "string",
                           "analyzer": "keywords_analyzer"
                        }
                     }
                  },
                  "name": {
                     "type": "string",
                     "analyzer": "main_analyzer",
                     "fields": {
                        "english_words": {
                           "type": "string",
                           "analyzer": "english_main_analyzer"
                        },
                        "shingles": {
                           "type": "string",
                           "analyzer": "analyzer_shingle"
                        },
                        "decomp": {
                           "type": "string",
                           "analyzer": "decomp_analyzer",
                           "search_analyzer": "main_analyzer"
                        },
                        "ngrams": {
                           "type": "string",
                           "analyzer": "ngram_analyzer"
                        },
                        "correction": {
                           "type": "string",
                           "analyzer": "suggestion_analyzer"
                        },
                        "suggest": {
                           "type": "completion",
                           "analyzer": "simple",
                           "search_analyzer": "simple",
                           "payloads": true
                        },
                        "front": {
                           "type": "string",
                           "analyzer": "edgengram_analyzer"
                        },
                        "prefix": {
                           "analyzer": "prefix_analyzer",
                           "type": "string"
                        },
                        "keywords": {
                           "type": "string",
                           "analyzer": "keywords_analyzer"
                        }
                     }
                  },
                  "categories": {
                     "type": "nested",
                     "properties": {
                        "id": {
                           "type": "long"
                        },
                        "name": {
                           "type": "string",
                           "fields": {
                              "raw": {
                                 "type": "string",
                                 "index": "not_analyzed"
                              },
                              "suggest": {
                                 "type": "completion",
                                 "analyzer": "simple",
                                 "search_analyzer": "simple",
                                 "payloads": true
                              },
                              "english_words": {
                                 "type": "string",
                                 "analyzer": "english_main_analyzer"
                              },
                              "shingles": {
                                 "type": "string",
                                 "analyzer": "analyzer_shingle"
                              },
                              "decomp": {
                                 "type": "string",
                                 "analyzer": "decomp_analyzer",
                                 "search_analyzer": "main_analyzer"
                              },
                              "ngrams": {
                                 "type": "string",
                                 "analyzer": "ngram_analyzer"
                              },
                              "front": {
                                 "type": "string",
                                 "analyzer": "edgengram_analyzer"
                              }
                           }
                        }
                     }
                  },
                  "parent_categories": {
                     "type": "nested",
                     "properties": {
                        "id": {
                           "type": "long"
                        },
                        "name": {
                           "type": "string",
                           "fields": {
                              "raw": {
                                 "type": "string",
                                 "index": "not_analyzed"
                              },
                              "suggest": {
                                 "type": "completion",
                                 "analyzer": "simple",
                                 "search_analyzer": "simple",
                                 "payloads": true
                              },
                              "english_words": {
                                 "type": "string",
                                 "analyzer": "english_main_analyzer"
                              },
                              "shingles": {
                                 "type": "string",
                                 "analyzer": "analyzer_shingle"
                              },
                              "decomp": {
                                 "type": "string",
                                 "analyzer": "decomp_analyzer",
                                 "search_analyzer": "main_analyzer"
                              },
                              "ngrams": {
                                 "type": "string",
                                 "analyzer": "ngram_analyzer"
                              },
                              "front": {
                                 "type": "string",
                                 "analyzer": "edgengram_analyzer"
                              }
                           }
                        }
                     }
                  },
                  "top_features": {
                     "type": "nested",
                     "properties": {
                        "name": {
                           "type": "string",
                           "fields": {
                              "raw": {
                                 "type": "string",
                                 "index": "not_analyzed"
                              }
                           }
                        },
                        "value": {
                           "type": "string",
                           "fields": {
                              "raw": {
                                 "type": "string",
                                 "index": "not_analyzed"
                              }
                           }
                        }
                     }
                  },
                  "attributes": {
                     "properties": {
                        "tags": {
                           "type": "string",
                           "fields": {
                              "raw": {
                                 "type": "string",
                                 "index": "not_analyzed"
                              }
                           }
                        }
                     }
                  }
               }
            },
            "images": {
               "properties": {
                  "highres": {
                     "type": "string",
                     "index": "not_analyzed"
                  },
                  "lowres": {
                     "type": "string",
                     "index": "not_analyzed"
                  },
                  "thumbnails": {
                     "type": "string",
                     "index": "not_analyzed"
                  }
               }
            },
            "search-result-data": {
               "properties": {
                  "abstract_name": {
                     "type": "string",
                     "index": "not_analyzed"
                  },
                  "abstract_sku": {
                     "type": "string",
                     "index": "not_analyzed"
                  },
                  "id_abstract_product": {
                     "type": "long"
                  },
                  "id_touch": {
                     "type": "long"
                  },
                  "url": {
                     "type": "string",
                     "index": "not_analyzed"
                  }
               }
            },
            "localizedAttributes": {
               "properties": {
               }
            }
         }
      }
   }
}
```

(i removed `analysys` section from the snippet above, as it seems to be not important in this issue)
</comment><comment author="rjernst" created="2016-02-22T22:43:45Z" id="187422279">@marek-obuchowicz The issue is you specify a metadata mapper (`_all`) inside mapping properties, as well as try to set the type to `string`.  `_all` is a special field, and the type is an internal mapper, not just a normal string. I'm still not sure how 2.1 allows setting it this way without an error and does not find the issue until a recovery is done, but this should be fixed in 2.2 (where we have additional checks that metadata mappers are not recreated inside mappings).

You need to change your mappings from this:

```
{
   "mappings": {
      "product": {
         "properties": {
            "_all": {
               "type": "string",
               "analyzer": "main_analyzer"
            },
...
```

To this:

```
{
   "mappings": {
      "product": {
        "_all": {
          "analyzer": "main_analyzer"
        },
        "properties": {
...    
```
</comment><comment author="clintongormley" created="2016-04-06T10:25:11Z" id="206293978">I think this issue can be closed now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It appears that we have not received any data for this cluster?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16715</link><project id="" key="" /><description>I install `elastic` and `kibana`,and they all installed `marvel` plugin,but when i open the `marvel` tab on `kibana`,the response is `It appears that we have not received any data for this cluster`.

`Kibana` can add `es`'index, and `marvel` created `.marvel-es-2016-02-18` and `.marvel-es-data` index already.

In `kibana` status tab,show `plugin:marvel   Marvel index ready`

I open the chrome debug mode,and find `/api/marvel/v1/clusters` returns `[]`

es version is 2.2.0.
kibana version is 4.4.1.
marvel version is 2.2.0.
</description><key id="134539106">16715</key><summary>It appears that we have not received any data for this cluster?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhuqunzhou</reporter><labels /><created>2016-02-18T10:27:49Z</created><updated>2016-02-18T11:21:24Z</updated><resolved>2016-02-18T11:21:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zhuqunzhou" created="2016-02-18T11:21:22Z" id="185669970">I restarted all node,and it became ok.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2 java security feature breaks OSS plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16714</link><project id="" key="" /><description>This problem was introduced in 2.2, works fine with 2.1.1 and lower.

I have an Elasticsearch opensource plugin that needs reflection to inspect the originating address of HTTP requests.

In ES 2.2 they introduced security permissions for plugins, following the instructions, I added a grant in the plugin-security.policy file with the following:

```
grant {
  permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
};
```

Now upon installation of the plugin, I can see this (as anticipated in the docs). So I assume the request for permission is successfully kicking in.

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.reflect.ReflectPermission suppressAccessChecks
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.
Installed readonlyrest into /elasticsearch/plugins/readonlyrest
```

However, from my function that actually uses reflection, still see this error...

```
java.security.AccessControlException: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.reflect.AccessibleObject.setAccessible(AccessibleObject.java:128)
    at org.elasticsearch.plugin.readonlyrest.acl.blocks.rules.impl.HostsRule.getAddress(HostsRule.java:76)
    at org.elasticsearch.plugin.readonlyrest.acl.blocks.rules.impl.HostsRule.match(HostsRule.java:130)
    at org.elasticsearch.plugin.readonlyrest.acl.blocks.Block.check(Block.java:104)
    at org.elasticsearch.plugin.readonlyrest.acl.ACL.check(ACL.java:48)
    at org.elasticsearch.plugin.readonlyrest.ReadonlyRestAction$1.process(ReadonlyRestAction.java:60)
    at org.elasticsearch.rest.RestController$ControllerFilterChain.continueProcessing(RestController.java:265)
```

Is there something missing? I have no clue.
</description><key id="134522974">16714</key><summary>ES 2.2 java security feature breaks OSS plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sscarduzio</reporter><labels /><created>2016-02-18T09:17:11Z</created><updated>2016-05-07T11:55:13Z</updated><resolved>2016-02-18T11:17:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sscarduzio" created="2016-02-18T11:17:17Z" id="185665813">Nevermind, I needed to wrap the privileged  action in AccessController.doPrivileged.
</comment><comment author="zaakiy" created="2016-05-07T11:31:27Z" id="217630139">Seeing the same issue on Elasticsearch 2.3.1

```
[root@ip-10-11-1-181 elasticsearch]# /usr/share/elasticsearch/bin/plugin install cloud-aws
-&gt; Installing cloud-aws...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/cloud-aws/2.3.1/cloud-aws-2.3.1.zip ...
Downloading .............................................................................................................................................................................................................................................................................................................................................DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/cloud-aws/2.3.1/cloud-aws-2.3.1.zip checksums if available ...
Downloading .DONE
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission getClassLoader
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]
```
</comment><comment author="nik9000" created="2016-05-07T11:55:13Z" id="217631345">&gt; Seeing the same issue on Elasticsearch 2.3.1

This message is intentional. That plugin wants more permissions than Elasticsearch wants so you get the message and you can decide if you trust the plugin and the permission.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Truncated Cluster State exception should show path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16713</link><project id="" key="" /><description>This was in ES 2.1.0.

When trying to debug an issue where the local cluster state had been truncated (due to running out of disk space), this made it very hard to debug:

```
[2016-02-18 03:52:46,147][ERROR][gateway                  ] [my-node] failed to read local state, exiting...
ElasticsearchException[class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards]; nested: IllegalStateException[class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards];
    at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
    at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:99)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:225)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalStateException: class org.apache.lucene.store.BufferedChecksumIndexInput cannot seek backwards
    at org.apache.lucene.store.ChecksumIndexInput.seek(ChecksumIndexInput.java:49)
    at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:448)
    at org.elasticsearch.gateway.MetaDataStateFormat.read(MetaDataStateFormat.java:177)
    at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:299)
    ... 32 more
```

/cc @bleskes
</description><key id="134507519">16713</key><summary>Truncated Cluster State exception should show path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Exceptions</label><label>:Logging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-02-18T08:00:02Z</created><updated>2016-08-30T08:37:17Z</updated><resolved>2016-02-29T10:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T18:42:14Z" id="189920685">@pickypg What made this hard to debug? surely this is just the data path?
</comment><comment author="bleskes" created="2016-02-28T19:11:00Z" id="189923959">This came from a late night debugging session. I asked Chris to open it as I think it should be clear from the message which file is corrupted - is it an index meta file (which?) or a cluster state file. It can be figured out by correlating stack traces with code but i think a little extra info will help.

On 28 feb. 2016 7:42 PM +0100, Clinton Gormleynotifications@github.com, wrote:

&gt; @pickypg(https://github.com/pickypg)What made this hard to debug? surely this is just the data path?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/16713#issuecomment-189920685).
</comment><comment author="clintongormley" created="2016-02-29T08:45:13Z" id="190105231">ah ok - i thought it was referring to the path which was reporting disk full
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.2 plugin developement with java and installtion not working. shows jar format not supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16712</link><project id="" key="" /><description>Hello,

 I have migrated to Elastic search 2.2 from 1.7.3, I have done all API changes but only one thing is remain. you guys have changed the script native plug-in development also why? in previous version I am just have to create one jar using NativeScript put it in to lib and set its path to elasticsearch.yml and plugin successfully work. 

for the new version there is no proper documentation no any example nothing ? why I am not able to use my old plugin in new version even if a have done changes as per new api.

I wanted to do Upper-case on one filed by using ES script plug-in. Please show me a proper documentation to develop native script plug-in in Java-framework and install it in ES with proper steps.

you should have to mention proper steps in ES documentation brother. It's really hard to understand the documentation too.
</description><key id="134482759">16712</key><summary>Elasticsearch 2.2 plugin developement with java and installtion not working. shows jar format not supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ervivekmehta</reporter><labels /><created>2016-02-18T05:04:52Z</created><updated>2016-02-18T05:53:48Z</updated><resolved>2016-02-18T05:53:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-18T05:53:48Z" id="185555176">Can you please open a discussion on discuss.elastic.co?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator not supporting nested aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16711</link><project id="" key="" /><description>I wanted to aggregate my percolation results with a nested aggregation. However, it appears that code path isn't implemented. Martijn (@martijnvg) you appeared to know the issue.

PercolateContext.java

```
@Override
public MapperService.SmartNameObjectMapper smartNameObjectMapper(String name) {
throw new UnsupportedOperationException();
}
```

Is there support for this coming soon? It would be nice utility that would help avoid duplicating percolators.

Here is the script to reproduce the problem

```
PUT /classify
{
  "mappings":{
    ".percolator":{
      "properties": {
        "query" : {
          "type" : "object",
          "enabled" : false
        },
        "isa":{
          "type": "nested",
          "properties":{
            "feature":{
              "type": "string"
            },
            "id": {
              "type": "string"
            }
          }
        }
      }
    },
    "thing":{
      "properties":{
        "name":{
          "type":"string"
        }
      }
    }
  }
}

POST classify/.percolator/1234
{
  "query": {
    "match": {
      "name": "arts"
    }
  },
  "type": "thing",
  "isa":[{
    "feature": "Category",
    "id": "theatre"
  },
  {
    "feature": "Ambience",
    "id": "classy"
  }
  ]
}

GET /classify/thing/_percolate
{
  "doc": {
    "name": "Academy of Arts"
  },
  "aggs":{
    "isa":{
      "nested": {
        "path": "isa"
      },
      "aggs": {
        "features": {
          "terms": {
            "field": "isa.feature"
          },
          "aggs": {
            "kinds": {
              "terms": {
                "field": "isa.id"
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="134461019">16711</key><summary>Percolator not supporting nested aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarass</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2016-02-18T02:11:18Z</created><updated>2016-03-24T15:23:15Z</updated><resolved>2016-03-24T15:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-18T03:12:16Z" id="185523801">@tarass thanks for reporting. This caused by a bug.
</comment><comment author="martijnvg" created="2016-03-24T15:23:15Z" id="200884509">@tarass This issue will be fixed in the upcoming 5.0 release. The bug has been fixed by the refactoring done in #16349.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node unable to join cluster using client node for discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16710</link><project id="" key="" /><description>`waited for 30s and no initial state was set by the discovery` when trying to join a tribe node to a cluster using a unicast client node for the discovery endpoint.  This works fine with master and data nodes, and tested with 2.2.0, 1.7.5, 1.5.2.
</description><key id="134455061">16710</key><summary>Tribe node unable to join cluster using client node for discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jpcarey</reporter><labels><label>:Tribe Node</label></labels><created>2016-02-18T01:35:47Z</created><updated>2016-04-05T21:25:09Z</updated><resolved>2016-04-05T15:58:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T21:58:05Z" id="189952646">Please provide your tribe node config
</comment><comment author="jpcarey" created="2016-02-29T18:31:51Z" id="190323185">@clintongormley here is a quick test config for repo:

Cluster 1

```
bin/elasticsearch -Des.node.data=false -Des.node.name=t1_master
bin/elasticsearch -Des.node.data=true -Des.node.master=false -Des.node.name=t1_data
bin/elasticsearch -Des.node.data=false -Des.node.master=false -Des.node.name=t1_client
```

Cluster 2

```
bin/elasticsearch -Des.cluster.name=nothing
bin/elasticsearch -Des.cluster.name=nothing -Des.node.data=false -Des.node.master=false
```

Tribe

```
bin/elasticsearch -Des.config=/path.../elasticsearch.yml
```

Tribe config, adjust ports as needed. Tribe node will not join if using a client node for discovery.

```
tribe:
  t1:
    cluster.name: elasticsearch
    discovery.zen.ping.multicast.enabled: false
    discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300"]
  t2:
    cluster.name: nothing
    discovery.zen.ping.multicast.enabled: false
    discovery.zen.ping.unicast.hosts: ["127.0.0.1:9303"]
```
</comment><comment author="javanna" created="2016-04-05T15:58:54Z" id="205869591">I looked into this. It isn't a tribe node problem specifically. Client nodes cannot be used as unicast seeds for discovery in general, as they are filtered out by default, unless the `discovery.zen.master_election.filter_client` is set to `false` (but it defaults to `true`). This has been changed though as part of a recent PR in master: #17329 , along with the removal of the `node.client` setting meaning that from 5.0 on it will be possible to use coordinating only nodes for unicast discovery, and filter out non master nodes from the ping process if needed (all nodes will be pinged by default). I do not think we can/want to backport this change to 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting fields that didn't match the query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16709</link><project id="" key="" /><description>Please, see https://discuss.elastic.co/t/phrase-highlighting-works-incorrectly-on-es-2-1-1-and-2-2-0/39971
</description><key id="134445864">16709</key><summary>Highlighting fields that didn't match the query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-02-18T00:43:36Z</created><updated>2016-02-28T21:57:03Z</updated><resolved>2016-02-28T21:57:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T21:57:03Z" id="189952330">Duplicate of #16705
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Unable to Assign Shards, Marvel or otherwise.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16708</link><project id="" key="" /><description>Hello, I'm trying to start a new cluster of elasticsearch, but I can't seem to get the shards to allocate correctly. I upgrade to the latest marvel, and elasticsearch 2.2.0 and the cluster won't register the marvel shards.  I cant figure out why it wont register. I can't even manually register because it tells me the shard is disabled.

I then created a custom index with a few shards and the shards remain unassigned as well.

```
curl -XPUT http://localhost:9200/test -d '
{
   "settings" : {
      "number_of_shards" : 3,
      "number_of_replicas" : 1
   }
}

'
```

 In the logs I get the following error:

```
[2016-02-17 20:04:52,458][ERROR][marvel.agent             ] [i-11a6decb] background thread had an uncaught exception
ElasticsearchException[failed to flush exporter bulks]
    at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)
    at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)
    at java.lang.Thread.run(Thread.java:745)
    Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution:
[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLw1O4Ctq-FZ8CmFK_-], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]];
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:106)
        ... 3 more
    Caused by: ElasticsearchException[failure in bulk execution:
[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLw1O4Ctq-FZ8CmFK_-], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]]
        at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)
        ... 3 more

```

Then when I try to run a re-route I get:

```
Kenzans-MacBook-Pro-39:~ grantzukel$ curl -XPOST http://localhost:9200/_cluster/reroute?pretty -d '{ "commands" : [ { "allocate" : { "index" : ".marvel-es-data", "shard" : 0, "node" :"i-e098e03a" } } ] }' 
{
  "error" : {
    "root_cause" : [ {
      "type" : "remote_transport_exception",
      "reason" : "[i-169f4cce][10.194.35.20:9300][cluster:admin/reroute]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "[allocate] trying to allocate a primary shard [.marvel-es-data][0], which is disabled"
  },
  "status" : 400
}

```

Here is my elasticsearch config where i enable rebalance and rerouting and primaries to true.

```

my settings:

---

cluster.name: infra_elastic_cluster_3

index.number_of_shards: 3
index.store.throttle.type: none

action.auto_create_index: true

index.number_of_replicas: 1
index.requests.cache.enable: true
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.trace: 500ms
index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.trace: 200ms

index.refresh_interval: 1

cloud:
    aws:
      region: us-west-2
    node:
      auto_attributes: true

discovery:
    type: ec2
    ec2:
      groups: infra_elastic_cluster_3
      any_group: false
      ping_timeout: 60s
    zen:
      minimum_master_nodes: 2

node:
  data: true
  master: false
  name: i-29fd85f3

http:
  max_content_length: 1000mb
  cors.allow-origin: "/.*/"
  cors.enabled: true

bootstrap.mlockall: true

script.inline: on 
script.indexed: on 

tr.logging.maxlength: 500000

indices.memory.index_buffer_size: 30%
indices.store.throttle.max_bytes_per_sec: 1000mb
indices.store.throttle.type: Merge
indices.fielddata.cache.size:  40%

threadpool.bulk.type: fixed
threadpool.bulk.size: 100
threadpool.bulk.queue_size: 10000

network.host: _eth0_

query.bool.max_clause_count: 10240

cluster.routing.allocation.enable: all
cluster.routing.allocation.disable_new_allocation: false
cluster.routing.allocation.disable_allocation: false

cluster.routing.allocation.allow_primary: true
cluster.routing.allocation.allow_rebalance: always

```

Trace log output

```
2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]
[2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] observer: sampled state rejected by predicate (version [50], status [APPLIED]). adding listener to ClusterService
[2016-02-17 21:57:03,686][TRACE][action.bulk              ] [i-29fd85f3] observer: postAdded - predicate rejected state (version [50], status [APPLIED])
[2016-02-17 21:57:43,104][DEBUG][org.apache.http.impl.conn.PoolingClientConnectionManager] Closing connections idle longer than 60 SECONDS
[2016-02-17 21:57:43,104][DEBUG][com.amazonaws.internal.SdkSSLSocket] shutting down output of ec2.us-west-2.amazonaws.com/205.251.235.5:443
[2016-02-17 21:57:43,105][DEBUG][com.amazonaws.internal.SdkSSLSocket] closing ec2.us-west-2.amazonaws.com/205.251.235.5:443
[2016-02-17 21:57:43,106][DEBUG][org.apache.http.impl.conn.DefaultClientConnection] Connection 0.0.0.0:38289&lt;-&gt;205.251.235.5:443 closed
[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]
[2016-02-17 21:58:03,687][TRACE][action.bulk              ] [i-29fd85f3] operation failed. action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}]
UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.retryBecauseUnavailable(TransportReplicationAction.java:555)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:431)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$2.onTimeout(TransportReplicationAction.java:520)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:239)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:794)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-17 21:58:03,687][ERROR][marvel.agent             ] [i-29fd85f3] background thread had an uncaught exception
ElasticsearchException[failed to flush exporter bulks]
    at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)
    at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)
    at java.lang.Thread.run(Thread.java:745)
    Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution:
[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLxPI5GwpZSvKDdhqNh], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]];
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:106)
        ... 3 more
    Caused by: ElasticsearchException[failure in bulk execution:
[0]: index [.marvel-es-2016.02.17], type [node_stats], id [AVLxPI5GwpZSvKDdhqNh], message [UnavailableShardsException[[.marvel-es-2016.02.17][0] primary shard is not active Timeout: [1m], request: [shard bulk {[.marvel-es-2016.02.17][0]}]]]]
        at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)
        ... 3 more
[2016-02-17 21:58:13,693][TRACE][action.bulk              ] [i-29fd85f3] primary shard [[.marvel-es-2016.02.17][0]] is not yet active, scheduling a retry: action [indices:data/write/bulk[s]], request [shard bulk {[.marvel-es-2016.02.17][0]}], cluster state version [50]
[2016-02-17 21:58:13,693][TRACE][action.bulk              ] [i-29fd85f3] observer: sampled state rejected by predicate (version [50], status [APPLIED]). adding listener to ClusterService
[2016-02-17 21:58:13,694][TRACE][action.bulk              ] [i-29fd85f3] observer: postAdded - predicate rejected state (version [50], status [APPLIED])


```
</description><key id="134413365">16708</key><summary>Cluster Unable to Assign Shards, Marvel or otherwise.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zukeru</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2016-02-17T22:01:09Z</created><updated>2016-06-22T10:28:02Z</updated><resolved>2016-05-09T12:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-17T23:42:39Z" id="185461365">Can you try the reroute command again by setting `allow_primary` to true (see https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html)? This allows the allocate command to also allocate primary shards (Note that this loses all existing data for that shard):

```
curl -XPOST http://localhost:9200/_cluster/reroute?pretty -d '{ "commands" : [ { "allocate" : { "index" : ".marvel-es-data", "shard" : 0, "node" :"i-e098e03a", "allow_primary": "true" } } ] }' 
```
</comment><comment author="clintongormley" created="2016-05-09T12:40:58Z" id="217853301">No further feedback. Closing
</comment><comment author="portante" created="2016-06-22T04:54:40Z" id="227643312">@clintongormley, I encountered the same problem, and the above `reroute` fixed that instance.  How do I fix this so that all new marvel indices don't have this problem? Do I need to add a template that addresses this?
</comment><comment author="clintongormley" created="2016-06-22T10:28:02Z" id="227704709">@portante the important thing to figure out is why the index is not being allocated - we never got to the bottom of the story here.  Possibly to do with allocation settings?  Feel free to open a new issue so we can delve into it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Statically check that number of placeholders in log message matches number of parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16707</link><project id="" key="" /><description>This PR checks that the number of placeholders in log message string matches number of parameters passed to logging call.

Good:

```
logger.warn("Hello {}", name)
logger.warn("Hello {}", throwable, name)
```

Bad:

```
logger.warn("Hello {}", firstName, lastName)
logger.warn("Hello {}", name, throwable)
```

Checks are run using the new Gradle task `loggerUsageCheck`.

To fully check all occurrences, the checker requires that log messages are constant strings. In case where a logging usage should be ignored, an annotation `@SuppressLoggerChecks` can be put on the respective method or class where logging occurrences are to be ignored.
</description><key id="134399655">16707</key><summary>Statically check that number of placeholders in log message matches number of parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-17T21:07:07Z</created><updated>2016-03-11T10:46:46Z</updated><resolved>2016-03-11T09:33:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-17T23:54:29Z" id="185463682">Hooray for this feature, thanks @ywelsch!
</comment><comment author="nik9000" created="2016-02-20T16:44:10Z" id="186649037">I talked to @ywelsch about these in person because we had both flown to San Francisco to Elastic{ON}:
1. The check is its own project so it can depend on test-framework and elasticsearch core which makes it much much easier to implement.
2. Embedding it into the gradle process forces you to use their version of byte code library who's version doesn't match. Or you could shade it. Either way that complexity isn't worth it when you can just fork.
3. It does indeed feel weird to have it in `qa`. Maybe move it to the `test` directory or something? Once you accept that it should be a subproject then exactly where you put it is simpler change after the fact so its ok to pay less attention to during the PR.

I'm super excited to get this in though!
</comment><comment author="ywelsch" created="2016-02-22T06:55:29Z" id="187040879">@nik9000 I've updated the PR with suggested changes (last 3 commits). I've also modified all logger calls where the log message was not a constant string. This ensures that ALL logger calls are correct. In case where people want to opt-out of the checking mechanism (e.g. `NettyInternalESLogger` which delegates to `ESLogger`), I've added the `@SuppressLoggerChecks` annotation.

Note that log output is of the following form (each error being represented by two lines):

```
Bad usage of org.elasticsearch.common.logging.ESLogger#debug: Expected 4 arguments but got 5
    org.elasticsearch.action.ingest.IngestActionFilter.lambda$processBulkIndexRequest$198(IngestActionFilter.java:106)
Bad usage of org.elasticsearch.common.logging.ESLogger#trace: Expected 1 arguments but got 0
    org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.nodeIndexDeleted(NodeIndexDeletedAction.java:84)
Bad usage of org.elasticsearch.common.logging.ESLogger#trace: Expected 2 arguments but got 3
    org.elasticsearch.cluster.action.shard.ShardStateAction$2.onNewClusterState(ShardStateAction.java:154)
Bad usage of org.elasticsearch.common.logging.ESLogger#debug: Expected 0 arguments but got 1
    org.elasticsearch.common.geo.builders.PolygonBuilder.component(PolygonBuilder.java:317)
Bad usage of org.elasticsearch.common.logging.ESLogger#debug: Expected 0 arguments but got 1
    org.elasticsearch.common.geo.builders.PolygonBuilder.buildCoordinates(PolygonBuilder.java:395)
Bad usage of org.elasticsearch.common.logging.ESLogger#warn: Expected 2 arguments but got 3
    org.elasticsearch.common.lucene.Lucene.parseVersion(Lucene.java:114)
```
</comment><comment author="rjernst" created="2016-02-22T08:23:13Z" id="187065709">I don't think we should do this as a project under test. It is ultimately a gradle task. If we need to separate the asm version, it can still be a separate subproject within buildSrc. 
</comment><comment author="ywelsch" created="2016-02-22T18:06:33Z" id="187298019">@rjernst I also want to have real tests for the checker (see `ESLoggerUsageTests`). This means:
- I need access to the `core` project from the tests
- I want to make use of the testing infrastructure provided by `test:framework`

How would I do that while having the sources of the checker under `buildSrc`?
Also, there is no real infrastructure under `buildSrc` right now to run tests...

You can compare this checker a bit to the JarHell task, which also lives under core...
</comment><comment author="rjernst" created="2016-02-22T18:20:57Z" id="187304145">JarHell is something we use in core at runtime, which is exposed with a main method to allow using from the command line like we do before tests. But this here is strictly a static check for builds.

Adding a subproject in buildSrc is easy. And I don't think we should be using the test framework for testing it, since that is a huge overkill for testing whether this simple utility works. You can set up tests that use the normal gradle test runner with junit, and in the test src add ESLogger so the tests can work (ie find the ESLogger class, just with a mock of it).
</comment><comment author="nik9000" created="2016-02-25T17:40:35Z" id="188897931">&gt; Adding a subproject in buildSrc is easy

I'll have a look at it this afternoon-ish and see if I can send a PR to ywelsch:enhance/elasticsearch-proper-logging-usage that does that.
</comment><comment author="ywelsch" created="2016-02-26T16:26:32Z" id="189356044">@rjernst The issue with subprojects of buildSrc is that they are not well supported:
- They do not properly integrate into IDE's. IntelliJ is unable to detect buildSrc subprojects, requiring a separate project import of buildSrc as top-level project to work on the buildSrc subprojects.
- They are not added to the buildscript classpath, requiring the actual Gradle task to be in buildSrc (or some hacks as workaround, see http://stackoverflow.com/questions/23990513/how-to-import-multiproject-build-in-buildsrc-dir-in-gradle)

Also:
- If one of the tests were to fail, the project cannot be imported into / refreshed in IDEA. Any gradle command would fail (as `buildSrc:build` is run first). This behavior would be ok if one of the core parts of the build infrastructure would be broken, but probably not for a helper tool that checks logging lines.
- Making the tests work requires mocking ESLogger in the test src. This requires essentially duplicating ESLogger class and introducing a check somewhere that that we are compatible with the actual ESLogger in core.

The current approach does not have to deal with these issues, is simple and integrates nicely with the build. Another advantage of the current approach is that the classpath of the tool is clean (not polluted with gradle api).
</comment><comment author="rjernst" created="2016-02-29T19:45:02Z" id="190351875">Thanks @ywelsch, I did not realize the problems intellij has with buildSrc containing subprojects. I left a couple thoughts about the gradle side. I didn't look through the actual checking code that closely, but the tests look pretty good so I am happy enough there for now.
</comment><comment author="ywelsch" created="2016-03-01T16:38:50Z" id="190801482">@rjernst I've pushed a commit that addresses your comments and enables forbiddenApis for the `logger-usage` project. As this project does not rely on `core`, I had to split the forbiddenApis signatures:
- all-signatures is split into jdk-signatures and es-all-signatures. The reason this is needed is that forbiddenApis tries to load the classes that are in the signatures file (which fails for the logging-usage project as it does not rely on es/core)
- For consistency, I then renamed core-signatures to es-core-signatures and test-signatures to es-test-signatures.

If you don't like the splitting, we can simply simply disable forbiddenApis for `logger-usage`.
</comment><comment author="rjernst" created="2016-03-09T21:59:43Z" id="194528401">Thanks @ywelsch. LGTM.
</comment><comment author="nik9000" created="2016-03-10T16:29:58Z" id="194937718">I left a few small notes. I don't know ASM so I'm going to trust you there but I suspect it is fine. It found a lot of things! My vote is to merge it as soon as you can resolve the conflicts so it starts catching errors.
</comment><comment author="ywelsch" created="2016-03-11T09:33:18Z" id="195287065">@nik9000 Addressed all your comments. I will go ahead and push.
Thanks @nik9000 and @rjernst for the reviews!
</comment><comment author="ywelsch" created="2016-03-11T10:46:46Z" id="195314792">I've cherry-picked wrong usage of placeholders in log statements (718876a) to 2.x (b24d6b3). I've NOT cherry-picked all the logging changes that were made to make log messages string constants.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Aggregation return wrong terms if terms contains dashes "-"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16706</link><project id="" key="" /><description>I'm having an issue using ElasticSearch v2.2.0 on Windows.

**Given the following index data:**

```
PS D:\&gt; (Invoke-WebRequest -Uri "http://localhost:9200/pingbeat-2016.02.16/_search?size=10000&amp;pretty=true").Content.Split("`n")
|Select-String target_name |Sort -Unique

        "target_name" : "hwd-u2b-01f.domain.tld",
        "target_name" : "hwd-u2b-02f.domain.tld",
        "target_name" : "hwd-u2b-03f.domain.tld",
        "target_name" : "pon-u2b-01f.domain.tld",
        "target_name" : "pon-u2b-02f.domain.tld",
        "target_name" : "pon-u2b-03f.domain.tld",
        "target_name" : "timeserv.domain.tld",
```

**The following terms aggregation:**

```
{
"size": 0,
"aggs" : {
    "targets" : {
        "terms" : { "field" : "target_name" }
    }
}}
```

**Returns wrong values for targets:**

```
PS D:\&gt; (Invoke-WebRequest -Uri "http://localhost:9200/pingbeat-2016.02.16/_search?pretty=true" -Method POST -InFile .\pb-targets.txt).Content
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 26712,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "targets" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "u2b",
        "doc_count" : 22896
      }, {
        "key" : "hwd",
        "doc_count" : 11448
      }, {
        "key" : "pon",
        "doc_count" : 11448
      }, {
        "key" : "01f.domain.tld",
        "doc_count" : 7632
      }, {
        "key" : "02f.domain.tld",
        "doc_count" : 7632
      }, {
        "key" : "03f.domain.tld",
        "doc_count" : 7632
      }, {
        "key" : "timeserv.domain.tld",
        "doc_count" : 3816
      } ]
    }
  }
}
```

As you can see, the aggregation returns the following terms:
- u2b
- hwd
- pon
- 01f.domain.tld
- 02f.domain.tld
- 03f.domain.tld
- timeserv.domain.tld

Please let me know if you need more details to help resolve this issue.

Thanks!
</description><key id="134385559">16706</key><summary>Terms Aggregation return wrong terms if terms contains dashes "-"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mponton</reporter><labels /><created>2016-02-17T20:04:20Z</created><updated>2016-02-18T19:11:43Z</updated><resolved>2016-02-18T19:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xavierfacq" created="2016-02-17T22:14:01Z" id="185431010">Hi,

Your field must be "not_analyzed" if you want to aggregate unique values, use a mapping with something like : 

```
              "target_name" : {
                "type" : "string",
                "index": "not_analyzed",
                "include_in_all": false
              },

```

Else it aggregates distinct terms.

Xavier
</comment><comment author="mponton" created="2016-02-18T19:11:43Z" id="185867512">My apologies.

I'm just getting started with ES and apparently I need to read quite a bit more. I used a default index template from https://github.com/joshuar/pingbeat and indeed, the `target_name` is `analyzed`. I reindexed and all is good now.

I will open a ticket or send Joshua a pull request as I don't see the benefit in having a hostname analyzed. It does not look very good on a dashboard :-)

A huge thank you for replying so quickly and helping me with this!

Cheers,

Marco
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighters highlight filter parts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16705</link><project id="" key="" /><description>As reported at https://discuss.elastic.co/t/2-0-question-about-whats-returningw-ithin-the-highlighter/34065 highlighters mistakenly highlight based on query parts that are only used for filtering
</description><key id="134376246">16705</key><summary>Highlighters highlight filter parts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">igor-kupczynski</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-02-17T19:27:43Z</created><updated>2016-10-25T16:06:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-18T07:15:59Z" id="185575722">I replied on the mailing list, I am not sure it's a bug, Did we verify that the behaviour was different before? As far as I can remember, if you had a filtered query in the past, filter matches would be highlighted?
</comment><comment author="javanna" created="2016-02-18T14:33:58Z" id="185747468">I stand corrected, I double checked and indeed this has changed as a side effect of query/filter merging. The same query (using a filtered query) would highlight only the query part in 1.7 and previous versions, while now filters get highlighted as well with 2.0+. Using a specific `highlight_query` without the filters is a valid workaround until we get this fixed.
</comment><comment author="igor-kupczynski" created="2016-02-18T17:10:26Z" id="185818524">Copying @jpountz as well
</comment><comment author="nik9000" created="2016-02-19T05:04:27Z" id="186052499">I guess we could get the old behavior back by trying to figure out if the query is used in a filter context. I thought this side effect of the filter-&gt;query conversion was known/expected though. Stuff like #15793 snuck in but the rest of the filter terms being highlighted was expected?

The whole terms extraction process has always been a bit hacky and when I was managing a production installation I always relied on good old highlight_query to save me.
</comment><comment author="gmoskovicz" created="2016-10-04T12:13:44Z" id="251370920">++ on fixing this
</comment><comment author="jpountz" created="2016-10-04T21:01:03Z" id="251512294">I have been thinking a bit more about it today and I could not come up with a rule about what should be highlighted:
- should filters really not be highlighted?
- if yes then do we still agree that what lives under a `constant_score` should not be highlighted? (it is technically a filter)
- isn't it mostly due to the use of `"require_field_match":false`? in that case maybe the actual fix is for users to specify a `highlight_query` to be more specific about what needs to be highlighted.
</comment><comment author="gmoskovicz" created="2016-10-04T21:08:30Z" id="251514169">@jpountz thanks for looking at it! My comments go inline:

&gt; should filters really not be highlighted?

Maybe we should be able to specify whether the filters should be higlighted or not? Shoudn't this be a configuration? Depending on the use case, people would want to higlight this or not

&gt; if yes then do we still agree that what lives under a constant_score should not be highlighted? (it &gt;is technically a filter)

This could be addressed with my statement in [1]

&gt; isn't it mostly due to the use of "require_field_match":false? in that case maybe the actual fix is for &gt;users to specify a highlight_query to be more specific about what needs to be highlighted.

I think that when require_field_match is false, a highlight query should be required, otherwise what should be highlighted? 
</comment><comment author="jpountz" created="2016-10-04T21:38:21Z" id="251521524">&gt; Maybe we should be able to specify whether the filters should be higlighted or not?

This is something I'd like to avoid if possible as settings/options increase the complexity of our APIs.

&gt; I think that when require_field_match is false, a highlight query should be required, otherwise what should be highlighted?

The current default is to highlight the main query and highlighters will try to highlight across fields. It  is indeed quite a tricky option as weird things can happen if fields have different analyzers.
</comment><comment author="dexterama" created="2016-10-25T16:04:00Z" id="256080085">jpountz: Hello, I am the original poster of this issue on your Elastic forum (https://discuss.elastic.co/t/2-0-question-about-whats-returning-within-the-highlighter/34065) and I also spoke with you about this at Elasticon 2016 at your info booth. My suggestion, please, is to NOT include the filters in the highlighter, and here is my motivation:

What is the premise of the filter? To include, or exclude, given data that matches a given pattern.

What is the premise of the highlighter? To show the user where their search terms hit within the indexed documents, to provide context of the search result they are viewing. 

This is my understanding of the distinction between the two. 

As an engineer, the filter is used to control the return of data.
As a user (e.g. using an aggregation selection to filter down return data), the aggregation/UI interaction might have spawned the filter's creation, but does not mean they need to see a hit upon their aggregation within the highlight. E.g. They selected an aggregation result of document type, so we apply a filter upon it, it is a data limiter in the query, not part of why it hit their search criteria, per se.

But, most importantly, if we ever wish to filter data OUT of a users return set, due to security/authorizations, we NEVER want those filters as part of the highlight. (e.g. user A can only search upon documents of type X, so a filter is applied to the search.)

To this engineer, it seems like the highlighter should highlight **search term matches**, but not the filters that might control data that the search terms query against.

Thanks for listening. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose total number of unique terms per field in field stats api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16704</link><project id="" key="" /><description>It can be useful to report on the total number of unique terms per field in the field stats api, this can be used to get a sense of which fields tend to use more FTSs in memory.  Discussed briefly with @mikemccand earlier, he mentioned that Lucene does record this, so it will be nice for ES to expose this in the field stats api as well.
</description><key id="134361815">16704</key><summary>Expose total number of unique terms per field in field stats api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-17T18:31:34Z</created><updated>2016-02-28T21:45:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-17T18:38:28Z" id="185341459">This can only be provided per-segment in an efficient way: aggregation of this is expensive.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>One log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16703</link><project id="" key="" /><description>Removes all Elasticsearch logger wrappers exception log4j. There was some discussion in #16585 around removing all but j.u.l so that we could remove the log4j dependencies, and we may still do that, but I think that we should do this first. My reasoning is that after this it should be reasonably easy to switch the backing logger. The hard work of cutting over to j.u.l would be supporting the logging configuration we support now. Or making a breaking change to support whatever we can get out of j.u.l.

Closes #16585
</description><key id="134355747">16703</key><summary>One log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-17T18:08:57Z</created><updated>2016-02-29T14:07:11Z</updated><resolved>2016-02-26T21:42:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-25T20:16:19Z" id="188960761">LGTM, we should continue down this path of simplifying ES logging....
</comment><comment author="nik9000" created="2016-02-26T20:14:52Z" id="189463382">I'm going to squash/rebase and merge this if the rebase is clean. If we want j.u.l support one day we can resurrect it from that commit this becomes.
</comment><comment author="nik9000" created="2016-02-26T20:28:14Z" id="189470484">Rebase was clean (one merge error in the imports...). Rerunning tests now.
</comment><comment author="clintongormley" created="2016-02-29T00:12:15Z" id="189976457">@nik9000 i'm assuming this is a breaking change? If so, please add a note to breaking changes (and label this PR)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update snapshots.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16702</link><project id="" key="" /><description>Simplify shared file system repository setup
</description><key id="134348309">16702</key><summary>Update snapshots.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikanderson</reporter><labels /><created>2016-02-17T17:35:25Z</created><updated>2016-02-22T03:07:22Z</updated><resolved>2016-02-22T03:07:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Internal: TransportSearchTypeAction shouldn't extend TransportAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16701</link><project id="" key="" /><description>All the actions that extend TransportSearchTypeAction are subactions of the main TransportSearchAction. The main one is and should be a transport action, register request handlers, support request and response filtering etc. but the subactions shouldn't as that becomes just double work. At the moment each search request goes through validation and filters twice, one as part of the main action, and the second one as part of the subaction execution. The subactions don't need to extend TransportAction, but can be simple support classes, as they are always executed on the same node as their main action.

This commit modifies TransportSearchTypeAction to not extend TransportAction but simply AbstractComponent.

Closes #11710
</description><key id="134292619">16701</key><summary>Internal: TransportSearchTypeAction shouldn't extend TransportAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-02-17T14:21:44Z</created><updated>2016-02-26T14:38:16Z</updated><resolved>2016-02-26T14:38:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-17T18:48:56Z" id="185345252">@jaymode this is for you ;) 

we may want to rename `TransportSearchTypeAction` and all its subclasses at this point as they are not transport actions anymore. Suggestions are welcome, naming is hard.
</comment><comment author="jaymode" created="2016-02-21T22:10:29Z" id="186929344">Thank you @javanna 

I agree that naming these is hard and that they shouldn't be misleading with `Transport*Action` naming. My initial thought is to drop both `Transport` and `Action` from the name. The issue I see with that, is there is already have a `SearchType` enum that could make it confusing but they are in different packages so we could get away with the naming.

For 5.0, we could combine the `SearchType` enum into what is the `TransportSearchTypeAction` now and rename it to `SearchType`. What do you think?
</comment><comment author="javanna" created="2016-02-22T05:59:07Z" id="187025066">@jaymode I played around a bit, as I think we don't need these intermediate classes at all, so I removed them with #16758. I think this PR can be closed in favour of #16758. Let me know what you think.
</comment><comment author="jaymode" created="2016-02-26T12:21:11Z" id="189254335">+1 to closing this in favor of #16758 
</comment><comment author="javanna" created="2016-02-26T14:38:07Z" id="189306751">thanks for the feedback, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16700</link><project id="" key="" /><description>hi,

im created one native script using java,
im not understand how to install my native script as plugin so any one my friend give me example 

ajay.rathod39@gmail.com
</description><key id="134279193">16700</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingaj</reporter><labels /><created>2016-02-17T13:23:56Z</created><updated>2016-02-17T13:34:00Z</updated><resolved>2016-02-17T13:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-17T13:30:55Z" id="185204611">I believe this PR was opened accidentally so I'm going to close it.
</comment><comment author="nik9000" created="2016-02-17T13:34:00Z" id="185205362">&gt; how to install my native script as plugin

In your Plugin class declare an `onModule(ScriptModule module)` method and in its implementation call `registerScript` with your implementation of `NativeScriptFactory`. That is the way to do it in the master branch (unreleased) and I think it is the same in 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Template mapping Default index Analyzer not respected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16699</link><project id="" key="" /><description>Hi,

I'll try to explain as well as possible the problem.
Since I migrate from 1.7.5 to 2.2.0, I have troubles with default analyzers.

1&#176;/ I have template mapping with analysis : 

```
                "human": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": [ "lowercase", "asciifolding", "bypath-words" ]
                },
                "default": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": [ "lowercase", "asciifolding", "bypath-words", "bypath-elision", "bypath-synonyms" ]
                },

```

Some fields have the "human" analyzer specified, else others have the "default" one. No problem here. Everything works well.

2&#176;/ Before the migration in 2.2.0 I had queries like this : 

`QueryBuilders.matchQuery(fieldname, s).operator(Operator.AND).analyzer("default_search");
`

As specifications changed, I modifed analyzer's names to "default" , so in my code have now : 

`QueryBuilders.matchQuery(fieldname, s).operator(Operator.AND).analyzer("default");
`

3&#176;/ Here is the problem !

The .analyzer("default") produce this json : 

```
          "match": {
            "roles_hierarchy": {
              "query": "assistante",
              "type": "boolean",
              "operator": "AND"
              "analyzer": "default"
            }
          }

```

But the query **do not use my "default" analyzer**, it seems to use another "default" one, because I don't have the good analyzing. If I remove the "analyzer": "default", it's OK, the result is good and the term "assistante" is well analyzed.

So I have to remove .analyzer("default") every where in my code because there is a big side effect. 
I think there is a little problem here.

Xavier
</description><key id="134270408">16699</key><summary>Template mapping Default index Analyzer not respected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xavierfacq</reporter><labels><label>:Analysis</label><label>feedback_needed</label></labels><created>2016-02-17T12:49:59Z</created><updated>2016-03-25T16:39:14Z</updated><resolved>2016-03-25T16:39:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T21:38:45Z" id="189950003">Hi @xavierfacq 

The `default_index` setting has been removed. Now we have `default` and `default_search` which, if set, overrides the `default` at search time.

You don't actually have to specify these analyzers - they are used as defaults. I've tested out various configurations and all seem to work as expected.  Could you provide a simple recreation (preferably with curl) that demonstrates the problem?
</comment><comment author="xavierfacq" created="2016-03-25T07:55:41Z" id="201190189">Hi @clintongormley

I tried to reproduce many times on a fresh new instance, but I can't have the problem again. Maybe there was something misconfigured after the migration from 1.7.x to 2.x.

You can close this issue.
</comment><comment author="clintongormley" created="2016-03-25T16:39:13Z" id="201357374">thanks @xavierfacq 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index template create problem with logstash 2.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16698</link><project id="" key="" /><description>I am trying to upgrade from logstash 1.4.2 to logstash 2.1.1 and I have a problem when a new index gets created. I tried to reduce all configs to the minimum and now I have the following:

The most important piece is my index_template.json:

```
{
  "template" : "logstash-*",
  "settings" : {
    "index.refresh_interval" : "5s",
    "index.number_of_shards" : 5,
    "index.number_of_replicas" : 1
  },
  "mappings" : {
    "_default_" : {
       "_all" : {"enabled" : true},
       "dynamic_templates" : [ {
         "string_fields" : {
           "match" : "*",
           "match_mapping_type" : "string",
           "mapping" : {
             "type" : "string", "index" : "analyzed", "omit_norms" : true,
               "fields" : {
                 "raw" : {"type": "string", "index" : "not_analyzed", "ignore_above" : 256}
               }
           }
         }
       } ],
       "properties" : {
         "@version": { "type": "string", "index": "not_analyzed" },
         "receive_time" : {
           "type" : "string"
         }
       }
    }
  }
}
```

I am forcing the "receive_time" to be a string. However with logstash 2.1.1 this field is _sometimes_ a "string" and _sometimes_ a "date"

My logstash config:

```
input {
    file {
        path =&gt; "/tmp/logfile"
        sincedb_path =&gt; "/tmp/logfile.sincedb"
        start_position =&gt; "end"
    }
}
filter {
   mutate {
       #copy timestamp. In the index_template this field is forced to be a "string"
       add_field =&gt; [ "receive_time", "%{@timestamp}" ]
   }
}
output {
        elasticsearch {
          hosts =&gt; [ "elasticsearch:9200" ]
          flush_size =&gt; 100
          workers =&gt; 2
          template =&gt; "index_template.json"
          template_overwrite =&gt; true
        }
}
```

How to reproduce:
- install elasticsearch 1.5.1 e.g. "docker pull elasticsearch:1.5.1" and run it
- run logstash 2.1.1 with the above mentioned config and index_template
- create log entries:
  for i in $(seq 1000); do echo "line number $i"; done &gt;&gt;/tmp/logfile

Now check the type of the field "receive_time". This should be a string.
- stop elasticsearch and delete all data (e.g. remove the container and create a new one)
- repeat creating log entries

Check the type of the field "receive_time". Now this field is of type "date".

If you do the same steps with logstash 1.4.2 this never happens. Maybe because "receive_time" is always a string in this version.

Obviously I don't delete all my elasticsearch data daily. In my stage environment this happens when a new index gets created. Sometimes it is a "string" and sometimes it's a "date". In the latter case I get errors in elasticsearch like this:

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [receive_time]
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [2016-02-03 12:40:32 +0100], tried both date format [dateOptionalTime], and timestamp number with locale []
Caused by: java.lang.IllegalArgumentException: Invalid format: "2016-02-03 12:40:32 +0100" is malformed at " 12:40:32 +0100"
```
</description><key id="134230313">16698</key><summary>Index template create problem with logstash 2.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paltryeffort</reporter><labels><label>:Index Templates</label><label>feedback_needed</label></labels><created>2016-02-17T09:42:58Z</created><updated>2016-02-28T23:32:50Z</updated><resolved>2016-02-28T23:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-18T21:19:34Z" id="185926835">Hi @paltryeffort 

When you delete all your data, you're also deleting the index template.  My suspicion is that logstash is taking time to re-add the index, which means that documents get indexed before the template has been received.  Could you rerun your tests and ensure that the template exists in Elasticsearch before you index any docs?
</comment><comment author="paltryeffort" created="2016-02-26T14:52:07Z" id="189311224">Hi @clintongormley 

You are right. Logstash sends the template to elasticsearch and it works as expected. If I delete elasticsearch and restart a new one, Logstash never sends the template again or makes sure that the template is in place although it had to do a reconnect.

I can reproduce this like this:
- start logstash. It will print an error that it couldn't connect to elasticsearch but keeps running
- start elasticsearch. Logstash automatically connects to elasticsearch once it's there and inserts data. The template never gets written!

It looks like the template only gets send **once** to elasticsearch and only if elasticsearch is reachable upon startup. I never observed that logstash would send or verify the template at any later stage.

I am not sure if this is by design but I would expect logstash to make sure that the template is in place if it couldn't send it on startup and maybe also if the connection to elasticsearch dropped and it had to do a reconnect. I set template_overwrite to true but I guess this only has affect if the template on the logstash local disk changed but not on the elasticsearch side...
</comment><comment author="clintongormley" created="2016-02-28T23:32:50Z" id="189968616">@paltryeffort thanks for getting back with this info. I'd suggest bringing up this discussion on the logstash list instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running multiple instances as a service on one linux server?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16697</link><project id="" key="" /><description>Before `es 2.0`,i can use `servicewrapper` to run an instances as a service.But now,`servicewrapper` don't support `es 2.*`

I want to run multiple instances on one server,so i can't use `RPM package`.And i want to set `java opts` on each instance.
</description><key id="134208923">16697</key><summary>Running multiple instances as a service on one linux server?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhuqunzhou</reporter><labels /><created>2016-02-17T07:52:10Z</created><updated>2016-02-17T15:03:14Z</updated><resolved>2016-02-17T09:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-17T15:03:14Z" id="185242677">&gt; I want to run multiple instances on one server,so i can't use RPM package.And i want to set java opts on each instance

If you want to run multiple instances on one server you'll either have to use RPM and hack together a second init script/systemd descriptor or use the TAR distribution and build your own init scripts. The rpm and deb don't support more than one Elasticsearch instance per server, sadly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separate logging streams for tribe node clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16696</link><project id="" key="" /><description>Suggestion from the field.  If a tribe node is connected to many clusters, it will be helpful to separate the tribe node client logging for each cluster into different files. That will help isolate issues not having to review one large file.
</description><key id="134190492">16696</key><summary>Separate logging streams for tribe node clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>:Tribe Node</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-17T06:23:29Z</created><updated>2016-02-28T21:11:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T21:11:33Z" id="189946814">Hmm I'm not sure this makes sense.  Separating the logging then means you have to mentally combine perhaps several files to see things that were happening across several clusters at the same time... Perhaps we just need to make these logs more easily grep'able.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ALLOCATION_FAILED Field [user] is defined as a field in mapping [syslog]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16695</link><project id="" key="" /><description>Hello,
I am receiving this issue on elasticsearch.log:
[2016-02-16 16:50:41,109][WARN ][indices.cluster          ] 256}}},&lt;CLIP&gt;,&lt;CLIP&gt;,&lt;CLIP&gt;,&lt;CLIP&gt;,"user":{"type":"string","norms":{"enabled":false},"fielddata":{"format":"enabled"},"fields":{"raw":{"type":"string","index":"not_analyzed","ignore_above":256}}}}}}]
java.lang.IllegalArgumentException: **Field [user] is defined as a field in mapping [syslog] but this name is already used for an object in other types**

FROM Head:
"unassigned_info": {
"reason": "ALLOCATION_FAILED",
"at": "2016-02-16T21:38:01.695Z",
"details": "failed to update mappings, failure IllegalArgumentException[Field [user] is defined as a field in mapping [syslog] but this name is already used for an object in other types]"

I imagine that I have to modify a template somewhere, but not sure how to do so. Any guidance would be appreciated.
I am running Elasticsearch 2.2 and logstash 2.1.
Best,
~Jai
</description><key id="134109979">16695</key><summary>ALLOCATION_FAILED Field [user] is defined as a field in mapping [syslog]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jairagoo</reporter><labels /><created>2016-02-16T22:06:44Z</created><updated>2016-05-03T09:47:57Z</updated><resolved>2016-03-02T10:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="saravanp" created="2016-02-25T22:57:49Z" id="189020982">I am seeing the same issue as well

-Saravan
</comment><comment author="palmerabollo" created="2016-03-01T15:54:05Z" id="190780881">See https://discuss.elastic.co/t/how-to-solve-field-in-mapping-but-already-used-in-other-types/43067/3
They suggest to use different indexes, but I'd like to know a better solution, too.
</comment><comment author="clintongormley" created="2016-03-02T10:07:29Z" id="191168433">Hi all

As of 2.0.0, fields with the same name in different types in the same index must have the same mapping.  This was part of a bigger change (see #8870) to clean up numerous problems with mappings.  Your two alternatives are: use separate indices or rename one of the fields.
</comment><comment author="codemariner" created="2016-05-02T21:24:42Z" id="216369008">fwiw, this problem only recently cropped up for us after updating from 2.1 to 2.3.  We didn't update the mappings for the problem fields and have been running fine on 2.1 for some months now.
</comment><comment author="clintongormley" created="2016-05-03T09:47:57Z" id="216482513">@codemariner there were various bugs in earlier 2.x versions which still allowed conflicting mappings to be added.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Path not working when using Context Suggester with Geo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16694</link><project id="" key="" /><description>// CREATE INDEX
curl -X PUT -d '{
    "number_of_shards" : 1,
    "number_of_replicas" : 1
}' "http://localhost:9200/services"

// MAPPING
curl -X PUT -d '{
  "service": {
    "properties": {
      "name": {
        "type": "string",
        "index": "not_analyzed"
      },
      "pin": {
        "type":"geo_point"
      },
      "currentstatus": {
        "type": "string",
        "index": "not_analyzed"
      },
      "suggest_field": {
        "type": "completion",
        "context": {
          "status": {
            "type": "category",
            "default": "bad",
            "path": "currentstatus"  
          },
          "location": {
            "type": "geo",
            "precision": "100km",
            "neighbors": true,
            "path":"pin",
            "default": {
                "lat": 0.0,
                "lon": 0.0
            }
          }
        }
      }
    }
  }
}' "http://localhost:9200/services/service/_mapping"

// Insert record
curl -X PUT -d '{
  "name": "knapsack",
  "pin": {
      "lat": 47.620499,
      "lon": -122.350876
  },
  "currentstatus" : "good",
  "suggest_field": {
    "input": [
      "knacksack",
      "backpack",
      "daypack"
    ]
  }
}' "http://localhost:9200/services/service/1"

// Suggest
curl -X POST -d '{
  "suggest": {
    "text": "k",
    "completion": {
      "field": "suggest_field",
      "size": 10,
      "context": {
        "location": {
            "lat": 47.482880,
            "lon": -122.217064,
            "precision":"100km"
        },
        "status": "good"
      }
    }
  }
}' "http://localhost:9200/services/_suggest"

You'd expect that we'd get a suggestion here. When I do the category completion alone, it works correctly but it doesn't seem to get the lat/lon from the path but rather always uses the default. I don't want to have to duplicate data and this is not working as expected. What is going wrong here?
</description><key id="134050905">16694</key><summary>Path not working when using Context Suggester with Geo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemad</reporter><labels /><created>2016-02-16T17:52:26Z</created><updated>2016-02-28T22:15:37Z</updated><resolved>2016-02-28T21:08:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-28T21:08:26Z" id="189946434">Hi @mikemad 

I'm afraid the current context suggester has a long list of bugs.  It has been completely rewritten in 5.0 and I've checked that it works correctly there. Sorry we're not going to fix the existing suggester, but it'd be a bit like putting lipstick on a pig :)
</comment><comment author="mikemad" created="2016-02-28T22:15:37Z" id="189956240">LOL. At least getting an answer is better than not knowing if I was going crazy!!! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>s3 support for ceph radosgw</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16693</link><project id="" key="" /><description>I would like to see the s3 archive support the s3 as implemented in ceph / radosgw.

Not sure what all is involved however I was not able to make it work
</description><key id="134025747">16693</key><summary>s3 support for ceph radosgw</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bilsch</reporter><labels><label>:Plugin Repository S3</label><label>discuss</label><label>feature</label></labels><created>2016-02-16T16:23:00Z</created><updated>2016-03-03T17:48:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-18T21:03:52Z" id="185920192">I'm unsure if we can have it but may be you could tell more about the error you are seeing.
</comment><comment author="clintongormley" created="2016-02-28T18:47:16Z" id="189921431">&gt; Not sure what all is involved however I was not able to make it work

@bilsch neither am I.  You want to start by doing some research?
</comment><comment author="bilsch" created="2016-03-01T14:28:45Z" id="190742021">@clintongormley sorry for going quiet there. Yes I do want to help out and dig in wrt research. I will look in to a few things and at least collect notes of what is / is not working and at least attempt to understand why. I'm less strong on dev sides of things but reading APIs and such to get a general understanding should be within my ability.
</comment><comment author="bilsch" created="2016-03-01T15:58:52Z" id="190783512">For reference, radosgw [api](http://docs.ceph.com/docs/master/radosgw/s3/). I'll have to figure out how to dissect the packets to see what is going on. I'll get the plugin set up and dump the debug output and post that back Probably later today, maybe tomorrow.
</comment><comment author="bilsch" created="2016-03-01T17:40:00Z" id="190827408">Request

```
curl -XPUT -u admin 'https://elshost.foo.net:9200/_snapshot/s3_repository' -d '{
    "type": "s3",
    "settings": {
        "bucket": "foo_bucket",
        "endpoint": "https://radosgw.foo.net/foo_bucket",
        "access_key": "xxxx",
        "secret_key": "xxxx",
        "compress": true
    }
}' | python -m json.tool
```

Response

```
Enter host password for user 'admin':
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1144  100   858  100   286     98     32  0:00:08  0:00:08 --:--:--   125
{
    "error": {
        "caused_by": {
            "caused_by": {
                "caused_by": {
                    "reason": "foo_bucket.xxxx.foo",
                    "type": "unknown_host_exception"
                },
                "reason": "Unable to execute HTTP request: foo_bucket.xxxx.foo",
                "type": "amazon_client_exception"
            },
            "reason": "Guice creation errors:\n\n1) Error injecting constructor, com.amazonaws.AmazonClientException: Unable to execute HTTP request: foo_bucket.xxxx.foo\n  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)\n  while locating org.elasticsearch.repositories.s3.S3Repository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error",
            "type": "creation_exception"
        },
        "reason": "[s3_repository] failed to create repository",
        "root_cause": [
            {
                "reason": "[s3_repository] failed to create repository",
                "type": "repository_exception"
            }
        ],
        "type": "repository_exception"
    },
    "status": 500
}
```

We are looking to use [path style](http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html) s3 resources. It looks like the s3 client is trying to use bucket named host aliasing. Is there a toggle option to honor the endpoint path?
</comment><comment author="bilsch" created="2016-03-01T18:31:58Z" id="190843991">Also, reference the ceph [bucket and hostname](http://docs.ceph.com/docs/master/radosgw/s3/commons/#bucket-and-host-name)
</comment><comment author="bilsch" created="2016-03-03T17:48:42Z" id="191883191">Looking over the java docs, [S3ClientOptions](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/S3ClientOptions.html) - setting `withPathStyleAccess` to true should do it. I believe somewhere in [onModule ](https://github.com/elastic/elasticsearch/blob/master/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java#L92) but exposed through a config toggle somewhere/somehow.

Hope this helps - not sure what else I can research here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException on hasParentQuery when parent does not exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16692</link><project id="" key="" /><description>Elastic version 2.2.0

&gt; http://localhost:9200/my_index/parent_mapping/

I accidentally wrote *_has_parent *_instead of *_has_child *_and got NullPointerException.

 _Parent type does not have any parent of itself._

When I wrote has_child instead of has_parent query works as expected.

This query causes exception

```
{
  "size": 5,
  "query": {
    "bool": {
      "must": [
        {
          "query": {
            "has_parent": {
              "type": "child_mapping",
              "score_mode": "max",
              "query": {
                "match_all": {}
              },
              "inner_hits": {
                "from": 0,
                "size": 5
              }
            }
          }
        },
        {
          "query": {
            "match_all": {}
          }
        }
      ]
    }
  },
  "sort": [
    {
      "name": "asc"
    }
  ]
}

```

Here is the exception log I received:

```
RemoteTransportException[[Workit.Local][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerExceptio
n;
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData.getOrdinalMap(ParentChildIndexFieldData.java:566)
        at org.elasticsearch.index.query.HasChildQueryParser$LateParsingQuery.rewrite(HasChildQueryParser.java:259)
        at org.apache.lucene.search.ConstantScoreQuery.rewrite(ConstantScoreQuery.java:55)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:836)
        at org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:81)
        at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:232)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:103)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:674)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:618)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:461)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:392)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:389)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-02-16 16:54:02,014][DEBUG][action.search.type       ] [Workit.Local] All shards failed for phase: [query_fetch]
RemoteTransportException[[Workit.Local][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerExceptio
n;
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData.getOrdinalMap(ParentChildIndexFieldData.java:566)
        at org.elasticsearch.index.query.HasChildQueryParser$LateParsingQuery.rewrite(HasChildQueryParser.java:259)
        at org.apache.lucene.search.ConstantScoreQuery.rewrite(ConstantScoreQuery.java:55)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:836)
        at org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:81)
        at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:232)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:103)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:674)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:618)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:461)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:392)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:389)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-02-16 16:54:02,024][INFO ][rest.suppressed          ] /my_index/type/_search Params: {index=my_index, type=type}
Failed to execute phase [query_fetch], all shards failed; shardFailures {[F6pFRctSR4CSS1stcYFiyA][my_index][0]: RemoteTranspor
tException[[Workit.Local][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerException; }
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAc
tion.java:228)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.ja
va:174)
        at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
        at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: ; nested: NullPointerException;
        at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
        at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
        at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
        at java.lang.Throwable.printStackTrace(Throwable.java:665)
        at java.lang.Throwable.printStackTrace(Throwable.java:721)
        at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
        at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
        at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
        at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.log(Category.java:856)
        at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo(Log4jESLogger.java:125)
        at org.elasticsearch.common.logging.support.AbstractESLogger.info(AbstractESLogger.java:90)
        at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:131)
        at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:96)
        at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:87)
        at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAct
ion.java:316)
        ... 10 more
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData.getOrdinalMap(ParentChildIndexFieldData.java:566)
        at org.elasticsearch.index.query.HasChildQueryParser$LateParsingQuery.rewrite(HasChildQueryParser.java:259)
        at org.apache.lucene.search.ConstantScoreQuery.rewrite(ConstantScoreQuery.java:55)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:252)
        at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:836)
        at org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:81)
        at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:232)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:103)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:674)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:618)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:461)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:392)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchSer
viceTransportAction.java:389)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        ... 3 more
```
</description><key id="134004420">16692</key><summary>NullPointerException on hasParentQuery when parent does not exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">srhtcn</reporter><labels><label>:Parent/Child</label><label>:Query DSL</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-02-16T15:04:55Z</created><updated>2016-03-03T14:59:46Z</updated><resolved>2016-03-03T14:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-19T19:31:48Z" id="186370053">Simple recreation:

```
PUT t
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT t/parent/1
{}

PUT t/child/2?parent=1
{}

GET _search
{
  "query": {
    "has_parent": {
      "parent_type": "child",
      "query": {"match_all": {}}
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-02-19T19:32:12Z" id="186370159">This is also broken in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>snapshots doc: Repository Verification `verify` example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16691</link><project id="" key="" /><description>Replace `my_backup` in the example line

```
POST /_snapshot/my_backup/_verify
```

by `s3_repository` to match the repository name in above example : 

```
PUT /_snapshot/s3_repository?verify=false
{
  "type": "s3",
  "settings": {
    "bucket": "my_s3_bucket",
    "region": "eu-west-1"
  }
}
```
</description><key id="133980457">16691</key><summary>snapshots doc: Repository Verification `verify` example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gaelL</reporter><labels><label>docs</label></labels><created>2016-02-16T13:20:01Z</created><updated>2016-02-19T19:12:31Z</updated><resolved>2016-02-19T19:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-19T19:12:31Z" id="186364981">thanks @gaelL 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Informative Logging from Cluster Health API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16690</link><project id="" key="" /><description>When making a cluster health call, it would be helpful if the log message gave some idea about what the actual call was.

For instance, consider this call:

`$ curl -XGET 'http://localhost:9200/_cluster/health?wait_for_status=yellow&amp;timeout=50s'`

If this were to timeout because yellow never happened, it would be nice to have the condition that wasn't met in the logged timeout.  This can separate a call that fails because the cluster really didn't answer in the allotted time (which is good information to have) and a timeout because whatever condition wasn't met.
</description><key id="133976915">16690</key><summary>More Informative Logging from Cluster Health API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylerfontaine</reporter><labels><label>:Cluster</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-16T13:03:53Z</created><updated>2016-02-17T16:18:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>BUG: CORS only works with valid User-Agent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16689</link><project id="" key="" /><description>CORS only works with valid User-Agent. 
This is kind of a problem for special/unknown Browser Types.
![image](https://cloud.githubusercontent.com/assets/3963394/13074653/3bb4bb06-d4a6-11e5-988e-e633daf736ed.png)
</description><key id="133951691">16689</key><summary>BUG: CORS only works with valid User-Agent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qoomon</reporter><labels><label>:REST</label><label>bug</label><label>discuss</label></labels><created>2016-02-16T11:11:08Z</created><updated>2016-04-29T08:47:18Z</updated><resolved>2016-04-28T04:30:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jlroettger" created="2016-03-23T02:06:58Z" id="200130799">I just spent several hours troubleshooting why I couldn't get CORS to work correctly behind AWS CloudFront. This bug was the problem.

Turns out "Amazon CloudFront" is not a valid User-Agent string when it comes to Elasticsearch wanting to respond with an "Access-Control-Allow-Origin" header. Dang :(
</comment><comment author="nikoncode" created="2016-04-27T21:06:15Z" id="215229267">Not reproduces in master.

&lt;pre&gt;
$ curl -H "User-Agent: Mozilla" -H "Origin: http://example.com" -i localhost:9200
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 288
{
  "name" : "Arsenic",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha2",
    "build_hash" : "c2c4ed3",
    "build_date" : "2016-04-27T19:55:30.279Z",
    "build_snapshot" : true,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}
$ curl -H "Origin: http://example.com" -i localhost:9200
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 288
{
  "name" : "Arsenic",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha2",
    "build_hash" : "c2c4ed3",
    "build_date" : "2016-04-27T19:55:30.279Z",
    "build_snapshot" : true,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}
&lt;/pre&gt;
</comment><comment author="qoomon" created="2016-04-28T04:30:40Z" id="215307534">I used v1.7.1
</comment><comment author="nikoncode" created="2016-04-28T07:43:58Z" id="215337634">Not reproduces in 2.3.2 release. Please update to avoid described problems.

&lt;pre&gt;
$ curl -H "User-Agent: Mozilla" -H "Origin: http://example.com" -i localhost:9200
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 317
{
  "name" : "Enforcer",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
$ curl -H "Origin: http://example.com" -i localhost:9200
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Content-Type: application/json; charset=UTF-8
Content-Length: 317
{
  "name" : "Enforcer",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.2",
    "build_hash" : "b9e4a6acad4008027e4038f6abed7f7dba346f94",
    "build_timestamp" : "2016-04-21T16:03:47Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
&lt;/pre&gt;
</comment><comment author="qoomon" created="2016-04-28T07:53:03Z" id="215339445">Was 1.7.1 sorry my bad
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER configurable for ES-1.7.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16688</link><project id="" key="" /><description>In my use case, I setup hourly index for ELK, unluckily the logs come from everywhere and don't have consistent pace,  often hours ago old logs suddenly comes in, that leads to big trouble due to 500kb index buffer size for inactive shard.

For example, my machines have 32 CPUs,  "index.concurrency" is max(8, 0.65*cpus), that's 20 Lucene thread states, also means 20 concurrent segment flushes.  And "indices.memory.interval" is 30s by default (this is 5s in ES-2.x),  so if a single shard has 10MB/s documents suddenly being ingested, that will produce a lot of (500kb/20)=25kb tiny segments:

30s / (500kb / 10240 kb) \* 20 concurrency = 12288 segments.

The huge amount of small segments will trigger many merging, exceed the max_merge_count limit, thus indexing is throttled, unluckily this throttling is 100% stop, not  base on some probability,  then ES data node will accumulate a lot of IndexRequest or ReplicatedIndexRequest, the ES gateway also accumulates a lot of Request,  they will run out of memory and just hang,  the ES cluster will be down.

If I pre-create ES indices for future hours, this issue also kicks in,  the inactive future ES index will suddenly become active and produce a lot of segment,  but this scenario is less critical than old logs suddenly goes into old indices because the volume of new log usually gradually increases.

I made a rude patch to increase INACTIVE_SHARD_INDEXING_BUFFER to 120MB (I set min_shard_index_buffer_size to 128MB),   it works well for me, I don't worry the memory waste because Lucene allocates  memory lazily.

I also checked ES-2.x code,  it totally removes the logic of inactive buffer size and doesn't average total index buffer among active shards,  this is much better, but I'm not sure whether it's easy to port that change to ES-1.7.x.

My rude patch against ES-1.7.5, just FYI:

```
diff --git a/src/main/java/org/elasticsearch/index/engine/EngineConfig.java b/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
index d907082..9504a89 100644
--- a/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
+++ b/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
@@ -138,11 +138,15 @@ public final class EngineConfig {
      */
     public static final String INDEX_VERSION_MAP_SIZE = "index.version_map_size";

+    /**
+     * The indexing buffer size reserved for inactive shard.
+     */
+    public static final String INDEX_INACTIVE_SHARD_INDEX_BUFFER_SIZE = "index.inactive_shard_index_buffer_size";

     public static final TimeValue DEFAULT_REFRESH_INTERVAL = new TimeValue(1, TimeUnit.SECONDS);
     public static final TimeValue DEFAULT_GC_DELETES = TimeValue.timeValueSeconds(60);
     public static final ByteSizeValue DEFAUTL_INDEX_BUFFER_SIZE = new ByteSizeValue(64, ByteSizeUnit.MB);
-    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb");
+    public static ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb");

     public static final String DEFAULT_VERSION_MAP_SIZE = "25%";

@@ -182,6 +186,7 @@ public final class EngineConfig {
         failOnMergeFailure = indexSettings.getAsBoolean(INDEX_FAIL_ON_MERGE_FAILURE_SETTING, true);
         gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES_SETTING, EngineConfig.DEFAULT_GC_DELETES).millis();
         versionMapSizeSetting = indexSettings.get(INDEX_VERSION_MAP_SIZE, DEFAULT_VERSION_MAP_SIZE);
+        INACTIVE_SHARD_INDEXING_BUFFER = indexSettings.getAsBytesSize(INDEX_INACTIVE_SHARD_INDEX_BUFFER_SIZE, ByteSizeValue.parseBytesSizeValue("500kb"));
         updateVersionMapSize();
     }

diff --git a/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index 4ff1b06..27354fa 100644
--- a/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -1073,7 +1073,7 @@ public class IndexShard extends AbstractIndexShardComponent {
             // its inactive, make sure we do a refresh / full IW flush in this case, since the memory
             // changes only after a "data" change has happened to the writer
             // the index writer lazily allocates memory and a refresh will clean it all up.
-            if (shardIndexingBufferSize == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER &amp;&amp; preValue != EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) {
+            if (shardIndexingBufferSize.equals(EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) &amp;&amp; ! preValue.equals(EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER)) {
                 logger.debug("updating index_buffer_size from [{}] to (inactive) [{}]", preValue, shardIndexingBufferSize);
                 try {
                     refresh("update index buffer");
```
</description><key id="133880103">16688</key><summary>make EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER configurable for ES-1.7.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Dieken</reporter><labels><label>:Core</label><label>discuss</label><label>v1.7.5</label></labels><created>2016-02-16T04:06:51Z</created><updated>2016-02-28T21:59:45Z</updated><resolved>2016-02-28T21:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Dieken" created="2016-02-16T04:22:51Z" id="184510678">During the investigation,  I found ES-1.7 doesn't throttle request according to response,  and ES strangely has many net I/O threads:  
- "http_server_worker" thread pool (2 \* cpus threads)
- "http_server_worker.default" thread pool(2 \* cpus threads)
- "transport_client_worker" thread pool (2\* cpus threads)
- "transport_client_worker.default" thread pool (2 \* cpus threads)

I explicitly set "http.port" and "transport.tcp.port" in  elasticsearch.yml,  no idea why ES-1.7 creates two thread pool for http and transport.

If ES is blocked or just slow on indexing,  those I/O threads will still crazily receive requests and lead to OOM (Notice replicated index request doesn't follow the bulk.queue size limit),  this makes me feel ES is a little fragile.
</comment><comment author="clintongormley" created="2016-02-17T16:11:42Z" id="185276596">Hi @Dieken 

While we're still releasing bug fixes for 1.7, this particular change feels way too big to backport to a stable branch which is supposed to see maintenance releases only.

I would suggest that it is time to upgrade.

I'll leave this open for now in case anybody else disagrees.  @mikemccand ?
</comment><comment author="mikemccand" created="2016-02-17T21:38:50Z" id="185418757">Thanks @Dieken ... what you hit is actually a bug (that it takes up to 30s for ES to increase the indexing buffer off of INACTIVE).  It was fixed in 2.x releases in #13918.

Also, in 3.0, we've completely changed how we manage indexing buffer heap, so it's a single shared pool across all shards: #14121.

+1 to upgrade.
</comment><comment author="Dieken" created="2016-02-18T05:52:31Z" id="185555048">Yes, we have just upgraded to 2.2.0 yesterday,  maybe it's worth documenting this issue in 1.7.x reference manual.

The new strategy in 3.0 looks perfect,  it's a great pity not in 2.x  :-)

Thank you for the information!
</comment><comment author="clintongormley" created="2016-02-28T21:59:45Z" id="189953163">Looks like nothing left to do here.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting broken with has_child query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16687</link><project id="" key="" /><description>if you have a parent/child relationship doing a query that contains a `has_child` will result in error:

```
curl localhost:9200/_search -d '{
    "highlight": {"fields": {"title": {}}},
    "query": {
        "has_child": {
            "query": {"match": {"body": "drip"}},
            "type": "answer"
        }
    }
}'
```

Error returned:

```
{
    "type":"illegal_state_exception",
    "reason":"can't load global ordinals for reader of type: class org.apache.lucene.search.highlight.WeightedSpanTermExtractor$DelegatingLeafReader must be a DirectoryReader"
}
```

Works fine on 2.0.0, fails on 2.1.0 and later.
</description><key id="133873067">16687</key><summary>Highlighting broken with has_child query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2016-02-16T03:13:15Z</created><updated>2016-02-16T05:43:14Z</updated><resolved>2016-02-16T05:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-16T05:43:14Z" id="184530567">Duplicate of #14999. We should just as before ignore the `has_child` during highlighting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use MappedFieldType.termQuery to generate simple_query_string queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16686</link><project id="" key="" /><description>For Numeric types, if the query's text is passed to create a boolean
query, the 'Long' analyzer can return an exception similar to:

```
IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647
```

This change looks up the `MappedFieldType` for the specified fields (if
available) and uses the `.termQuery` function to create the query from
the string, instead of analyzing it by creating a new boolean query by
default.

Resolves #16577

This is fixed in 2.3+ by Lucene 5.5
</description><key id="133869858">16686</key><summary>Use MappedFieldType.termQuery to generate simple_query_string queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.2.1</label></labels><created>2016-02-16T02:52:30Z</created><updated>2016-04-07T02:17:53Z</updated><resolved>2016-02-16T03:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cobbzilla" created="2016-04-05T10:06:21Z" id="205738953">this patch doesn't seem to have made it into the 2.3.x code. I ran into the very same bug. Reverting to 2.2.2 fixed it. I looked at SimpleQueryParser in 2.3.1 and sure enough it was missing this bit of smartness to avoid the needless exception.
</comment><comment author="clintongormley" created="2016-04-06T10:58:13Z" id="206309678">According to the original description:

&gt; This is fixed in 2.3+ by Lucene 5.5

@dakrone could you take a look please?
</comment><comment author="dakrone" created="2016-04-06T16:22:17Z" id="206448720">Sure, I'll try to reproduce with 2.3.1 and see.
</comment><comment author="dakrone" created="2016-04-06T16:26:32Z" id="206449931">@cobbzilla I just ran the test (that would reproduce the issue) on the 2.x branch of ES and it passes, this wasn't forward ported to 2.3 because it should have been fixed by the Lucene upgrade. If you are seeing the issue, can you open a separate issue for it? (it may be a new issue)
</comment><comment author="cobbzilla" created="2016-04-07T02:17:53Z" id="206658887">@dakrone Thanks. I will wait for the next 2.3.x release -- the 2.3.1 release was using Lucene 5.5, and I read that this bug was supposed to have been fixed in Lucene 5.5 and 6.x, but I'm going to guess that the specific version of 5.5 bundled with 2.3.1 is missing the fix, but that a future version would include it. If I still see this bug with ES 2.3.2 or 2.3.3, then I will reopen a new issue about it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Confusing error when using has_child with inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16685</link><project id="" key="" /><description>When inner_hits has mis-configured `highlight` key the error returned is wrong:

```
curl localhost:9200/stack/question/_search -d '{
    "query": {
        "has_child": {
            "inner_hits": {
                "highlight": {"body": {}}
            },
            "type": "answer",
            "query": {
                "match": {"body": "drip"}
            }
        }
    }
}'
```

complaints with:

```
            {
                "col": 66,
                "index": "stack",
                "line": 1,
                "reason": "[has_child] requires 'query' field",
                "type": "query_parsing_exception"
            }
```
</description><key id="133844675">16685</key><summary>Confusing error when using has_child with inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-02-15T23:48:30Z</created><updated>2017-04-26T11:55:18Z</updated><resolved>2017-04-26T11:55:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-17T15:44:09Z" id="185263394">In current master, the bad highlight block just gets ignored completely.  I think this will probably change when the search refactoring for inner_hits gets merged in?  @cbuescher @colings86 ?
</comment><comment author="cbuescher" created="2016-02-29T13:59:00Z" id="190222731">@clintongormley the parser that parses the bad highlight block will throw an exception when it runs into an object name it doesn't know (here "body"). Since we haven't refactored `inner_hits` yet, this part still gets parsed on the shard, so it will raise the error there, but once we have refactored `inner_hits` this should also be raised already on the coordinating node. We can keep this open until then for verification, the tests for throwing an error on the bad highlight element are already there. 
</comment><comment author="colings86" created="2017-04-26T11:24:48Z" id="297366406">@cbuescher is this issue now solved since all of the search request has now been refactored?</comment><comment author="cbuescher" created="2017-04-26T11:55:18Z" id="297378470">@colings86 yes, this can be close now. I just checked this on 5.3, the error we get now is:
```
"type": "parsing_exception",
    "reason": "[inner_hits] failed to parse field [highlight]",
    "line": 4,
    "col": 31,
    "caused_by": {
      "type": "illegal_argument_exception",
      "reason": "[highlight] unknown field [body], parser not found"
    }
```
Which correctly points to the cause of the error (the unknown "body" field).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc and script are mutually exclusive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16684</link><project id="" key="" /><description>Consider this :

```
POST /website/pageviews/1/_update?retry_on_conflict=5 
{
   "doc_as_upsert" : "true",
   "script" : "ctx._source.views+=1",
   "doc": {
       "views": 1
   }
}
```

The above should in theory: 
- create a new doc if non-existant
- update existing doc if the id already exists
- increase views by 1 if the id already exists

Currently there is an error saying that `can't provide both script and doc`

```
org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: can't provide both script and doc;
    at org.elasticsearch.action.bulk.BulkRequest.validate(BulkRequest.java:499)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:62)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:428)
```

Thanks
</description><key id="133827994">16684</key><summary>doc and script are mutually exclusive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salimane</reporter><labels /><created>2016-02-15T22:05:16Z</created><updated>2016-02-17T15:39:11Z</updated><resolved>2016-02-17T15:39:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-17T15:39:11Z" id="185260101">This is how you should write it:

```
POST /website/pageviews/1/_update?retry_on_conflict=5 
{
  "script": "ctx._source.views+=1",
  "upsert": {
    "views": 1
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>no known master node, scheduling a retry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16683</link><project id="" key="" /><description>For some reason my data nodes is not being able to find my master node. any help would be appreciate it.

data node:
cluster.name: security-siem
node.name: elastic1
node.master: false
node.data: true
discovery.zen.ping.unicast.hosts: ["elastic-kibana"]

&gt; [2016-02-15 15:18:15,321][INFO ][node                     ] [elastic1] version[2.2.0], pid[14790], build[8ff36d1/2016-01-27T13:32:39Z]
&gt; [2016-02-15 15:18:15,322][INFO ][node                     ] [elastic1] initializing ...
&gt; [2016-02-15 15:18:15,768][INFO ][plugins                  ] [elastic1] modules [lang-expression, lang-groovy], plugins [], sites []
&gt; [2016-02-15 15:18:15,782][INFO ][env                      ] [elastic1] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [11.9gb], net total_space [14.6gb], spins? [possibly], types [ext4]
&gt; [2016-02-15 15:18:15,787][INFO ][env                      ] [elastic1] heap size [1015.6mb], compressed ordinary object pointers [true]
&gt; [2016-02-15 15:18:17,940][INFO ][node                     ] [elastic1] initialized
&gt; [2016-02-15 15:18:17,941][INFO ][node                     ] [elastic1] starting ...
&gt; [2016-02-15 15:18:18,001][INFO ][transport                ] [elastic1] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
&gt; [2016-02-15 15:18:18,042][INFO ][discovery                ] [elastic1] security-siem/KxFzgm9TTviihCbO16B9nw
&gt; [2016-02-15 15:18:48,043][WARN ][discovery                ] [elastic1] waited for 30s and no initial state was set by the discovery
&gt; [2016-02-15 15:18:48,057][INFO ][http                     ] [elastic1] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
&gt; [2016-02-15 15:18:48,057][INFO ][node                     ] [elastic1] started
&gt; [2016-02-15 15:23:52,009][DEBUG][action.admin.cluster.state] [elastic1] no known master node, scheduling a retry
&gt; [2016-02-15 15:24:22,010][DEBUG][action.admin.cluster.state] [elastic1] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])
&gt; [2016-02-15 15:24:22,015][INFO ][rest.suppressed          ] /_cat/indices Params: {v=}
&gt; MasterNotDiscoveredException[null]
&gt;         at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$5.onTimeout(TransportMasterNodeAction.java:205)
&gt;         at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:239)
&gt;         at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:794)
&gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;         at java.lang.Thread.run(Thread.java:745)

Master node:
cluster.name: security-siem
node.name; elastic-kibana
node.master: true
node.data: false
discovery.zen.ping.unicast.hosts: ["elastic1"]

&gt; [2016-02-15 15:17:58,874][INFO ][node                     ] [elastic-kibana] version[2.2.0], pid[4060], build[8ff36d1/2016-01-27T13:32:39Z]
&gt; [2016-02-15 15:17:58,878][INFO ][node                     ] [elastic-kibana] initializing ...
&gt; [2016-02-15 15:17:59,350][INFO ][plugins                  ] [elastic-kibana] modules [lang-expression, lang-groovy], plugins [], sites []
&gt; [2016-02-15 15:17:59,376][INFO ][env                      ] [elastic-kibana] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [11.7gb], net total_space [14.6gb], spins? [possibly], types [ext4]
&gt; [2016-02-15 15:17:59,384][INFO ][env                      ] [elastic-kibana] heap size [1015.6mb], compressed ordinary object pointers [true]
&gt; [2016-02-15 15:18:01,563][INFO ][node                     ] [elastic-kibana] initialized
&gt; [2016-02-15 15:18:01,569][INFO ][node                     ] [elastic-kibana] starting ...
&gt; [2016-02-15 15:18:01,668][INFO ][transport                ] [elastic-kibana] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
&gt; [2016-02-15 15:18:01,681][INFO ][discovery                ] [elastic-kibana] security-siem/QZF-VFfjTDy95-iHQQWm8A
&gt; [2016-02-15 15:18:04,736][INFO ][cluster.service          ] [elastic-kibana] new_master {elastic-kibana}{QZF-VFfjTDy95-iHQQWm8A}{127.0.0.1}{127.0.0.1:9300}{data=false, master=true}, reason: zen-disco-join(elected_as_master, [0] joins received)
&gt; [2016-02-15 15:18:04,776][INFO ][http                     ] [elastic-kibana] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
&gt; [2016-02-15 15:18:04,777][INFO ][node                     ] [elastic-kibana] started
&gt; [2016-02-15 15:18:04,795][INFO ][gateway                  ] [elastic-kibana] recovered [1] indices into cluster_state
</description><key id="133812601">16683</key><summary>no known master node, scheduling a retry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andr3w77</reporter><labels /><created>2016-02-15T20:42:55Z</created><updated>2016-02-15T20:56:38Z</updated><resolved>2016-02-15T20:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-15T20:55:50Z" id="184389869">Nodes bind to localhost by default since 2.0.0; you need to set `network.host` (or various related settings) to have them bind to an external interface. You can read more about this is in the [network settings docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#modules-network).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining a global default similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16682</link><project id="" key="" /><description>Attempts to address #16594.
</description><key id="133809962">16682</key><summary>Defining a global default similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">gpstathis</reporter><labels><label>:Similarities</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-15T20:33:17Z</created><updated>2016-03-18T13:27:23Z</updated><resolved>2016-03-08T14:30:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-24T21:57:19Z" id="188470268">@gpstathis I left some feedback on the code
</comment><comment author="gpstathis" created="2016-02-25T06:38:46Z" id="188637011">@s1monw thanks for the feedback. See https://github.com/elastic/elasticsearch/pull/16682#discussion_r54053648.
</comment><comment author="s1monw" created="2016-03-02T14:55:23Z" id="191271098">@gpstathis this looks awesome - thanks for fixing this I left a minor comment but LGTM otherwise 
</comment><comment author="gpstathis" created="2016-03-05T00:36:34Z" id="192537147">@s1monw fixed the spacing across the board and added the assert. Are you sure we are all set though? As it stands, the `IndexScopedSettings` validation does not take the index version into account so it will fail for pre v3 indices while `SimilarityService` tries to be more lenient them.
</comment><comment author="s1monw" created="2016-03-08T13:23:29Z" id="193783169">&gt; @s1monw fixed the spacing across the board and added the assert. Are you sure we are all set though? As it stands, the IndexScopedSettings validation does not take the index version into account so it will fail for pre v3 indices while SimilarityService tries to be more lenient them.

yes that is fine. We only apply IndexScopedSettings for new indices so I think it's intended to fail then.
</comment><comment author="s1monw" created="2016-03-08T15:08:04Z" id="193817110">@gpstathis merged thanks!!
</comment><comment author="gpstathis" created="2016-03-16T19:11:52Z" id="197495750">Nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Observing 6x slowdown while indexing documents on upgrading from 1.7 to 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16681</link><project id="" key="" /><description>The slowdown occurred while doing a benchmark of index rate of documents with a single string field. 

example doc: '{ "message": "this is a random sentence" }'

The benchmark is for a single node(AWS EC2 c4.2xlarge). The index has 1 shard and no replicas. The mapping is generated automatically. A 250GB general purpose SSD EBS volume was used for data. 

On 1.7, I observed an index rate of 2.2k/sec, but on 2.2 I just see a rate of around 360/sec. 

The code used for the benchmark is here: [https://github.com/sacheendra/es_benchmark](https://github.com/sacheendra/es_benchmark)
The master branch is for 2.2 and the 1.7 branch is for 1.7. The benchmark script was run on a separate machine. (EC2 c3.8xlarge)

Elasticsearch was deployed using docker with data volume mapped to disk. 

The CPU usage is around 10%. About 5GB/250GB is used after an 8hr run. The RAM usage is as seen in the chart below. 

&lt;img width="857" alt="screen shot 2016-02-15 at 11 51 47 pm" src="https://cloud.githubusercontent.com/assets/5353969/13057037/7b7689b6-d43f-11e5-9ad2-e872ac936cb7.png"&gt;

What could be the reason for this slowdown? 

Also any idea on why the memory usage id increasing so fast?
</description><key id="133786000">16681</key><summary>Observing 6x slowdown while indexing documents on upgrading from 1.7 to 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sacheendra</reporter><labels /><created>2016-02-15T18:27:36Z</created><updated>2016-02-16T08:41:02Z</updated><resolved>2016-02-16T08:41:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-15T20:01:05Z" id="184365537">this is very likely the same as https://github.com/elastic/elasticsearch/issues/16676
</comment><comment author="sacheendra" created="2016-02-16T08:41:02Z" id="184577012">Thank you, that seems to be the change causing the slowdown. Didn't know that es fsynced on every request. Using bulk requests, performance looks good. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cloud-aws plugin not working and seem to be an older version </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16680</link><project id="" key="" /><description>I have Elasticsearch 1.7.x running with aws cloud plugin 2.7.1 in the Elk Stack, everything works fine. I wanted to upgrade so I decided to put it on a new instance to make sure all is working ok. I cannot seem to get it working and the version of the cloud-aws plugin seem to be older than 2.7.1? I would have though keeping with [convention it would be higher](https://github.com/elastic/elasticsearch-cloud-aws). 

the plugin doesn't seem to work can you check if I am doing something wrong, here is the set up commands for each.

Set-up for Elasticsearch1.7.2

```
sudo su
yum update -y &amp;&amp; cd /root
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.7.2.noarch.rpm
yum install elasticsearch-1.7.2.noarch.rpm -y
rm -f elasticsearch-1.7.2.noarch.rpm
cd /usr/share/elasticsearch/
./bin/plugin -install mobz/elasticsearch-head
./bin/plugin -install lukas-vlcek/bigdesk
./bin/plugin install elasticsearch/elasticsearch-cloud-aws/2.7.1
cd /etc/elasticsearch
cp elasticsearch.yml elasticsearch.yml.bak
nano elasticsearch.yml
```

Set-up for Elasticsearch 2.2.0

```
sudo su
yum update -y
cd /root
wget  https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.2.0/elasticsearch-2.2.0.rpm
yum install elasticsearch-2.2.0.rpm -y
rm -f elasticsearch-2.2.0.rpm
cd /usr/share/elasticsearch/
bin/plugin install mobz/elasticsearch-head
bin/plugin install lukas-vlcek/Bigdesk
bin/plugin install cloud-aws 
cd /etc/elasticsearch
cp elasticsearch.yml elasticsearch.yml.bak
nano elasticsearch.yml
```

see a [screen shot](https://www.dropbox.com/s/b1tlejuc8tnaugl/cloud-aws.jpg?dl=0) from installing &#224;nd the difference in the version for the cloud-aws plugins

I had to revert back to the working Elasticsearch 1.7.2 with the `elasticsearch-cloud-aws/2.7.1` but would love to know if i'm doing something wrong or this is a bug.

G
</description><key id="133777570">16680</key><summary>cloud-aws plugin not working and seem to be an older version </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Gmanweb</reporter><labels /><created>2016-02-15T17:49:47Z</created><updated>2016-02-17T15:17:40Z</updated><resolved>2016-02-17T15:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-15T17:54:02Z" id="184323838">It's not an older version. We changed the Maven groupId.
</comment><comment author="clintongormley" created="2016-02-17T15:17:40Z" id="185248310">The groupId is now `org.elasticsearch.plugin`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change docs on "node client" to not use an in-memory node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16679</link><project id="" key="" /><description>Currently we suggesting users create a Node (using NodeBuilder in 2.x) to have a client that is capable of keeping up-to-date information. This is generally a bad idea as it means elasticsearch has no control over eg max heap size or gc settings, and is also problematic for users because they must deal with dependency collisions (and in 2.x+ dependencies of elasticsearch itself).

A better alternative, and what we should document, is to run a local elasticsearch server using bin/elasticsearch, and then use the transport client to connect to that local node. This local connection is virtually free, and allows the client code to be completely isolated from the elasticsearch process. Plugins are then also easy to deal with: just install them in elasticsearch as usual.

Closes #15383
</description><key id="133758372">16679</key><summary>Change docs on "node client" to not use an in-memory node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-15T16:32:56Z</created><updated>2016-02-29T15:17:19Z</updated><resolved>2016-02-29T15:17:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-15T16:33:11Z" id="184286486">@rjernst Is it what you meant?
</comment><comment author="dadoonet" created="2016-02-27T12:51:27Z" id="189632621">@rjernst Do you agree with the changes?
</comment><comment author="rjernst" created="2016-02-28T00:40:12Z" id="189754364">The changes look good except for the one comment I left.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClusterHealthResponse is always red</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16678</link><project id="" key="" /><description>I came across an interesting situation while testing some scenarios with Java API version 2.2.0

ClusterHealthResponse returns always red when I want to get the status of the cluster(immediately after the node starts) that has one or more indices but If you get the status of the cluster after creating an index the state of the cluster does not return red in the next request even if execute immediately after the node starts.

Please look at the [simple gist.](https://gist.github.com/hakdogan/e541e52a96b0e16ea8f9)

When you run this standalone application sequentially you will get the following log:

&gt; 2016-02-15 18:18:11,150 [main] INFO  com.kodcu.JavaAPIMain - *************\* GREEN **************
&gt; 2016-02-15 18:18:11,153 [main] INFO  com.kodcu.JavaAPIMain - Index does not exist. Creating...
&gt; 
&gt; 2016-02-15 18:22:18,500 [main] INFO  com.kodcu.JavaAPIMain - *************\* RED **************
&gt; 2016-02-15 18:22:18,502 [main] INFO  com.kodcu.JavaAPIMain - Index already exists.

Afterwards, delete the data path directory and comment the line 67 and then run this code. The result will be as below:

&gt; 2016-02-15 18:28:43,049 [main] INFO  com.kodcu.JavaAPIMain - *************\* GREEN **************
&gt; 2016-02-15 18:28:43,055 [main] INFO  com.kodcu.JavaAPIMain - Index does not exist. Creating...
&gt; 2016-02-15 18:28:43,910 [main] INFO  com.kodcu.JavaAPIMain - *************\* YELLOW **************
&gt; 
&gt; 2016-02-15 18:29:28,472 [main] INFO  com.kodcu.JavaAPIMain - *************\* YELLOW **************
&gt; 2016-02-15 18:29:28,475 [main] INFO  com.kodcu.JavaAPIMain - Index already exists.

What could be the source of this inconsistency?
</description><key id="133757898">16678</key><summary>ClusterHealthResponse is always red</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hakdogan</reporter><labels><label>:Allocation</label><label>:Core</label><label>feedback_needed</label></labels><created>2016-02-15T16:31:23Z</created><updated>2017-03-31T14:37:33Z</updated><resolved>2017-03-31T14:37:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2017-03-21T14:08:36Z" id="288089197">Are you still seeing this issue on you Elasticsearch cluster? If so, do you know the steps to reproduce the bug?</comment><comment author="hakdogan" created="2017-03-22T19:36:34Z" id="288514475">Hi.

&gt; Are you still seeing this issue on you Elasticsearch cluster?

Yes in the same version`(2.2.0)`

&gt; If so, do you know the steps to reproduce the bug?

Please run the above code I'm sharing: [code](https://gist.github.com/hakdogan/e541e52a96b0e16ea8f9)

If you run the standalone application sequentially, you will always see red the state of the cluster after the second time. However, for example, you will see green the state when you are refresh to the created index after creating.</comment><comment author="abeyad" created="2017-03-22T19:45:14Z" id="288516813">In 2.x and prior, when an index was initially created, the cluster would temporarily go `RED` while shards are being allocated and created.  Once all the primary shards have been created on their respective nodes for the newly created index, then the cluster moved to `YELLOW`.  We changed this behavior in the 5.x series so that when an index is created, instead of temporarily going to `RED` while its primary shards are being allocated, we move straight to the `YELLOW` state for that index.</comment><comment author="hakdogan" created="2017-03-23T09:14:28Z" id="288659368">@abeyad Thanks for the explanation</comment><comment author="colings86" created="2017-03-31T14:37:33Z" id="290729923">Sounds like this has been resolved. If I have that wrong please reopen</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Mapper Attachment Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16677</link><project id="" key="" /><description>Now that we have the ingest-attachment plugin (https://github.com/elastic/elasticsearch/pull/16490)  we should deprecate the mapper-attachment plugin.

Closes #16650.

Backport of #16669 in 2.x branch
</description><key id="133751282">16677</key><summary>Deprecate Mapper Attachment Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v2.3.0</label></labels><created>2016-02-15T16:06:01Z</created><updated>2016-03-02T14:32:20Z</updated><resolved>2016-03-02T14:32:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Slow performance on HDD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16676</link><project id="" key="" /><description>I've been seeing a very important slowdown on HDD since Elasticsearch 2.0.0 (I tested 1.7.5 and it doesn't have this problem).

My test:

```
curl -s -w %{time_connect}:%{time_starttransfer}:%{time_total}\\n -XPUT 'http://localhost:9200/twitter/'
for i in {1..200}
do
    curl -s -w %{time_connect}:%{time_starttransfer}:%{time_total}\\n -XPUT "http://localhost:9200/twitter/tweet/$i" -d '{
        "user" : "kimchy",
        "post_date" : "2009-11-15T14:12:12",
        "message" : "trying out Elasticsearch"
    }'
done
```

That's done on a clean elasticsearch freshly extracted from the tarball.

Result from 2.0.0 (average about 160ms):

```
{"_index":"twitter","_type":"tweet","_id":"187","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.235:0.235
{"_index":"twitter","_type":"tweet","_id":"188","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.220:0.220
{"_index":"twitter","_type":"tweet","_id":"189","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.127:0.127
{"_index":"twitter","_type":"tweet","_id":"190","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.112:0.112
{"_index":"twitter","_type":"tweet","_id":"191","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.161:0.161
{"_index":"twitter","_type":"tweet","_id":"192","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.138:0.138
{"_index":"twitter","_type":"tweet","_id":"193","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.153:0.153
{"_index":"twitter","_type":"tweet","_id":"194","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.128:0.129
{"_index":"twitter","_type":"tweet","_id":"195","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.144:0.144
{"_index":"twitter","_type":"tweet","_id":"196","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.138:0.138
{"_index":"twitter","_type":"tweet","_id":"197","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.161:0.161
{"_index":"twitter","_type":"tweet","_id":"198","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.152:0.152
{"_index":"twitter","_type":"tweet","_id":"199","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.137:0.137
{"_index":"twitter","_type":"tweet","_id":"200","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}0.004:0.152:0.152
```

Result from 1.7.5 (average about 6ms):

```
{"_index":"twitter","_type":"tweet","_id":"164","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"165","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"166","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"167","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"168","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"169","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"170","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"171","_version":1,"created":true}0.004:0.007:0.007
{"_index":"twitter","_type":"tweet","_id":"172","_version":1,"created":true}0.004:0.007:0.007
{"_index":"twitter","_type":"tweet","_id":"173","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"174","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"175","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"176","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"177","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"178","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"179","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"180","_version":1,"created":true}0.004:0.007:0.007
{"_index":"twitter","_type":"tweet","_id":"181","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"182","_version":1,"created":true}0.004:0.006:0.006
{"_index":"twitter","_type":"tweet","_id":"183","_version":1,"created":true}0.004:0.006:0.006
```

This log line seems relevant:

```
[2016-02-15 10:39:40,689][INFO ][env] [Set] using [1] data paths, mounts [[/home (/dev/sdc)]], net usable_space [347.9gb], net total_space [931.5gb], spins? [possibly], types [btrfs]
```
</description><key id="133746776">16676</key><summary>Slow performance on HDD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lewisdiamond</reporter><labels /><created>2016-02-15T15:49:00Z</created><updated>2016-02-17T15:16:46Z</updated><resolved>2016-02-15T16:07:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-15T16:07:30Z" id="184274012">The transaction log is now fsync'ed after every request.  Your benchmark is measuring a single threaded load with an fsync after every request.  A more realistic benchmark would use multiple threads to send requests to Elasticsearch and (preferably) use bulk requests.  With these changes you'll see much better throughput.
</comment><comment author="s1monw" created="2016-02-15T16:18:27Z" id="184278182">if you wanna compare the two side-by-side you can change the durability for that index by setting `"index.translog.durability" : "async"`on 2.x otherwise you have to use bulk requests to compare. Note if you use `async` you are subject to loose documents if you kill a node without fsync / commit with a window of 5sec by default
</comment><comment author="lewisdiamond" created="2016-02-15T17:45:20Z" id="184321644">Ah finally! index.translog.durability is what I needed!
Thanks
</comment><comment author="s1monw" created="2016-02-15T20:01:29Z" id="184365746">&gt; Ah finally! index.translog.durability is what I needed!

no you need to use bulk. 
</comment><comment author="lewisdiamond" created="2016-02-17T14:31:43Z" id="185229715">@s1monw No for my use case I need to use `index.translog.durability: async`. I just don't want to spend 200ms to simply index because it result in the webservice having ~400ms response time which is too long. It's not for a bulk index, I already use bulk when appropriate.
</comment><comment author="nik9000" created="2016-02-17T14:40:37Z" id="185233312">&gt; I just don't want to spend 200ms to simply index because it result in the webservice having ~400ms response time which is too long

I guess it depends on your use case. If you are willing to rebuild replay some index operations to elasticsearch then it is probably ok. Are the other ~200ms coming from a relational database on the same spinning disk?
</comment><comment author="lewisdiamond" created="2016-02-17T15:16:46Z" id="185247612">@nik9000 mostly processing, saving to the database is about 5ms.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk api: fail deletes when routing is required but not specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16675</link><project id="" key="" /><description>As part of #10136 we removed the transport action for broadcast deletes in case routing is required but not specified. Bulk api worked differently though and kept on doing the broadcast delete internally in that case. This commit makes sure that delete items are marked as failed in such cases. Also the check has been moved up in the code together with the existing check for the update api, and we now make sure that the exception is the same as the one thrown for single document apis (delete/update).

Note that the failure for the update api contained the wrong optype (the type of the document rather than "update"), that's been fixed too and tested.

Closes #16645
</description><key id="133741713">16675</key><summary>Bulk api: fail deletes when routing is required but not specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Bulk</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-15T15:31:58Z</created><updated>2016-02-27T18:03:34Z</updated><resolved>2016-02-27T18:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-22T06:00:32Z" id="187025180">@s1monw  can you have a look please?
</comment><comment author="jpountz" created="2016-02-26T16:19:16Z" id="189350973">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes serialisation of Ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16674</link><project id="" key="" /><description>Range aggregation tests were failing (e.g. http://build-us-00.elastic.co/job/es_core_master_metal/12385/testReport/junit/org.elasticsearch.messy.tests/IPv4RangeTests/testPartiallyUnmapped/) sometimes because both the string and number versions of form and to were being serialised. This meant that the range aggregator builder objects would not serialise and deserialise to the same bytes before and after the builder had been used. This change makes Range object immutable so the builder doesn't need to worry about the range changing under its feet.
</description><key id="133720830">16674</key><summary>Fixes serialisation of Ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-15T14:06:34Z</created><updated>2016-02-15T15:17:24Z</updated><resolved>2016-02-15T15:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-15T15:11:40Z" id="184247761">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermPosition new method request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16673</link><project id="" key="" /><description>Currently, class TermPosition has three methods:

```
    public String payloadAsString()
    public float payloadAsFloat(float defaultMissing)
    public int payloadAsInt(int defaultMissing)
```

This feature request is to add a fourth method, with the following signature:

```
    public byte[] payloadAsBytes()
```

This feature adds additional flexibility when working with payload data.  Motivating use case:  I am working with an index that has specialized binary payloads that need to be accessed from a plugin class derived from AbstractDoubleSearchScript, for custom scoring.  Iterating over payloads leads to TermPosition objects.  The binary payloads in this case may be wider than 32 bits, and so will not fit into `int` or `float`.  And I do not want to use `String`, because the payload data does not represent characters and so I do not want/need character set encoding.  Having ruled out getting this payload as String, float or int leads to the idea of the new method proposed above.
</description><key id="133714694">16673</key><summary>TermPosition new method request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">steinar-flatland</reporter><labels><label>:Core</label><label>:Plugins</label><label>discuss</label></labels><created>2016-02-15T13:35:53Z</created><updated>2016-02-18T15:29:15Z</updated><resolved>2016-02-18T15:29:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="steinar-flatland" created="2016-02-18T15:29:15Z" id="185774956">When I opened this issue, I did not notice the public member, BytesRef payload, on class TermPosition.  This public member gives me what I need.  This request is not needed and can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify text about date format range</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16672</link><project id="" key="" /><description>Older than a certain year-range was a bit unclear in my opinion.
</description><key id="133713906">16672</key><summary>Clarify text about date format range</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">teuneboon</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-02-15T13:32:21Z</created><updated>2016-02-16T12:47:22Z</updated><resolved>2016-02-15T15:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-15T15:05:39Z" id="184246337">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="teuneboon" created="2016-02-15T15:11:05Z" id="184247640">It should be signed now, don't know if the check will automatically re-run though.
</comment><comment author="clintongormley" created="2016-02-15T15:15:43Z" id="184249860">Success! :)  Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API and versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16671</link><project id="" key="" /><description>From https://github.com/elastic/elasticsearch/issues/16438:

I think we can improve things here by making request validation stricter. IMO the following do not make sense and should be rejected:
- The `version` on an update request is a syntactic sugar for get of a specific version, doc merge and a version index. We should reject requests with both upsert and a version.
- If the upsert _index request_ is versioned, we should reject the op. As Clint said, it's not possible through the rest layer, but Java clients can do so.
</description><key id="133709909">16671</key><summary>Update API and versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-02-15T13:18:59Z</created><updated>2017-04-28T13:46:01Z</updated><resolved>2017-04-28T12:45:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lewischen1123" created="2017-03-12T22:49:50Z" id="285983778">i want to pick it up and have a try thx</comment><comment author="nik9000" created="2017-04-28T12:49:51Z" id="297989363">@kunal642 wrote the validation which really closed this issue. I just pushed some breaking changes documentation to mark the issues closed.

@clintongormley, I don't think it is worth pushing a "Deprecated version in upsert requests" change to 5.x. What do you think?</comment><comment author="clintongormley" created="2017-04-28T13:46:01Z" id="298001852">@nik9000 no i don't think that's required</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moved AggregatorBuilder implementations into their own class files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16670</link><project id="" key="" /><description>Also renamed histogram.AbstractBuilcer to AbstractHistogramBuilder, range.AbstractBuilder to AbstractRangeBuilder and org.elasticsearch.search.aggregations.pipeline.having to org.elasticsearch.search.aggregations.pipeline.bucketselector
</description><key id="133704387">16670</key><summary>Moved AggregatorBuilder implementations into their own class files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-15T12:50:39Z</created><updated>2016-02-15T15:14:42Z</updated><resolved>2016-02-15T14:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-15T13:47:46Z" id="184215068">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Mapper Attachment Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16669</link><project id="" key="" /><description>Now that we have the ingest-attachment plugin (https://github.com/elastic/elasticsearch/pull/16490)  we should deprecate the mapper-attachment plugin.

Closes #16650.
</description><key id="133690523">16669</key><summary>Deprecate Mapper Attachment Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label></labels><created>2016-02-15T11:28:53Z</created><updated>2016-02-15T15:41:43Z</updated><resolved>2016-02-15T15:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-15T11:30:19Z" id="184174739">@clintongormley I also added this to the breaking changes although it's not really a breaking change. I found may be useful to warn our users that they should consider moving to ingest from the next major release.
Let me know what you think.
</comment><comment author="clintongormley" created="2016-02-15T14:42:28Z" id="184234800">Added a couple of comments, otherwise good to merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES2.2 Indexing latency is slower than ES1.7.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16668</link><project id="" key="" /><description>Hi,

I have performed a quick test on indexing latency comparing between ES2.2 and ES1.7.4 on the same spec of hardware (16GB of JVM on a single node on both cluster with no replica)

Posting the data below in ES2.2 that always took &gt; 20ms and sometime jump to 600ms to index data but when switch to ES1.7.4 it took &lt; 5 ms

Are you able to replicate this in your lab? 

I might be missing something so any suggestions in the configuration below are welcome.

[elasticsearch.yml.zip](https://github.com/elastic/elasticsearch/files/130552/elasticsearch.yml.zip)

Regards,
Sombut

```
POST /_bulk
{"index":{"_index":"logstash-2015.10.17","_type":"roles","_id":"1"}}
{"role":"ADMIN","permission":[{"http_method":"*","template":"/**"}]}
```

ES2.2

```
{
  "took": 61,
  "errors": false,
  "items": [
    {
      "index": {
        "_index": "logstash-2015.10.17",
        "_type": "roles",
        "_id": "1",
        "_version": 104,
        "_shards": {
          "total": 2,
          "successful": 2,
          "failed": 0
        },
        "status": 200
      }
    }
  ]
}
```

ES1.7.4

```
{
  "took": 2,
  "errors": false,
  "items": [
    {
      "index": {
        "_index": "logstash-2015.10.17",
        "_type": "roles",
        "_id": "1",
        "_version": 14,
        "status": 200
      }
    }
  ]
}
```
</description><key id="133686886">16668</key><summary>ES2.2 Indexing latency is slower than ES1.7.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sombut</reporter><labels /><created>2016-02-15T11:09:37Z</created><updated>2016-02-16T01:46:33Z</updated><resolved>2016-02-15T11:34:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-15T11:34:29Z" id="184175803">There are some differences between 2.x and 1.x indeed. Doc values are enabled by default. The transaction log is fsync'ed after every request.
That's probably the reason you have differences.

Note that your test should have a lot more data to really measure what the real cost is.
Also, those differences play not a great role at search time and memory wise.

Please ask questions like this on discuss.elastic.co.
</comment><comment author="sombut" created="2016-02-16T01:46:33Z" id="184471441">Thanks @dadoonet. I'll  put my question on discussion page instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix elasticsearch-plugin script on cygwin environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16667</link><project id="" key="" /><description>More information, see: https://github.com/elastic/elasticsearch/issues/16666

I have applied same solution than _elasticsearch_ script, see: https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch#L121-L127 but it is neccesary to apply to ES_HOME and CONF_DIR variables.

It is also necessary to escape JAVA variable as we can use spaces at Java path:
https://github.com/jorgediaz-lr/elasticsearch/blob/16666_fix_plugin_script_on_cygwin/distribution/src/main/resources/bin/elasticsearch-plugin#L122

Closes #16666
</description><key id="133683680">16667</key><summary>Fix elasticsearch-plugin script on cygwin environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jorgediaz-lr</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label></labels><created>2016-02-15T10:52:01Z</created><updated>2016-02-29T20:05:39Z</updated><resolved>2016-02-29T19:59:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-26T08:50:25Z" id="189167760">Heya @jorgediaz-lr 

Cygwin isn't a supported environment so we don't test cygwin installations, which makes this PR difficult to merge. We provide support for running Elasticsearch as a service on windows.  Can I ask why you prefer cygwin to using our supported windows setup instead?
</comment><comment author="jorgediaz-lr" created="2016-02-26T10:20:41Z" id="189205960">Hi @clintongormley 

First of all, my windows+cygwin setup is only used for development and testing purposes as our product (Liferay) needs to connect to a elasticsearch instance. For production environments we use Linux setups.

This pull request only fixes **elasticsearch-plugin** script that is used for installing and removing plugins, this fix is not related with **elasticsearch** startup script.

I always use cygwin console for my daily work, instead windows cmd as it is more powerful, so when I have to install or remove a plugin I have to switch from my cygwin console to a windows console.

The **elasticsearch** script that you can use for starting elasticsearch it already supports cygwin see:
- https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch#L121-L127
  - My pull request is a copy of that code.

As current **elasticsearch-plugin** does not work for cygwin, Do you think it will be enough for you to test that I am not breaking anything at Linux side? (I have already done that testing)

Saludos desde Madrid!
</comment><comment author="clintongormley" created="2016-02-28T23:25:20Z" id="189968111">Sounds reasonable to me. @rjernst you had concerns?
</comment><comment author="rjernst" created="2016-02-29T19:05:46Z" id="190336961">My concern is about testing. I did not realize we have cygwin checks in `bin/elasticsearch`, but having such a check _implies_ support, even if we don't officially support it. When it breaks, the handful of users using it will complain, and the question will always be "why did it break?", and the answer will always be "because we didn't test it".

&gt; First of all, my windows+cygwin setup is only used for development and testing purposes as our product (Liferay) needs to connect to a elasticsearch instance. For production environments we use Linux setups.

You really should be using a linux setup yourself then. There are subtle differences in setup between windows and linux. Have you considered using a linux VM on your windows box?
</comment><comment author="jasontedor" created="2016-02-29T19:12:49Z" id="190339258">&gt;  I did not realize we have cygwin checks in bin/elasticsearch, but having such a check _implies_ support, even if we don't officially support it.

+1, either we support or we don't and if we don't then code for it does not belong in our codebase.
</comment><comment author="jorgediaz-lr" created="2016-02-29T19:21:27Z" id="190343353">@rjernst working with a Linux VM inside windows box is not comfortable for me as you are working in a virtual drive outside windows paths.

There are other elasticsearch users working with windows+cygwin:
- https://github.com/elastic/elasticsearch/issues/10945
- https://github.com/elastic/elasticsearch/pull/11675
- https://github.com/elastic/elasticsearch/search?q=cygwin&amp;type=Issues&amp;utf8=%E2%9C%93

In my opinion it is better to have some unsupported functionality that works that do not having that functionality.
Perhaps script could print a disclaimer telling that cygwin is not officially supported?
</comment><comment author="jasontedor" created="2016-02-29T19:25:30Z" id="190345256">&gt; In my opinion it is better to have some unsupported functionality that works that do not having that functionality.

No, because of the interaction of that functionality with other functionality, and the burden for reviewing and maintaining such functionality.
</comment><comment author="jorgediaz-lr" created="2016-02-29T19:41:40Z" id="190350831">Cygwin minimal script adaptation only changes two environment variables in order to be able to call  windows java process, from that it is the same than calling java directly from windows scripts.

In my opinion you should add cygwin code to "elasticsearch-plugin" or remove it from "elasticsearch" but now it is not consistent.

In any case, I will manually patch my elasticsearch scripts after every new elasticsearch update, in order to be able to use it from cygwin.

Regards
</comment><comment author="jasontedor" created="2016-02-29T19:51:12Z" id="190354007">&gt;  or remove it from "elasticsearch"

I've opened #16871.
</comment><comment author="jorgediaz-lr" created="2016-02-29T19:59:06Z" id="190356415">Ok, I am closing this pull request
</comment><comment author="jasontedor" created="2016-02-29T20:05:39Z" id="190359789">&gt; Ok, I am closing this pull request.

I know that this is not the outcome that you wanted, but thank you for your understanding @jorgediaz-lr.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch-plugin script does not work on cygwin environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16666</link><project id="" key="" /><description>elasticsearch-plugin script does not work on cygwin environment, following error is thrown because there are spaces at JAVA_HOME:

```
Jorge@JorgeDiaz /mnt/d/ElasticSearch/elasticsearch-2.1.1/bin
$ ./plugin install lang-javascript
./plugin: line 122: C:Program: command not found
```

Without spaces, another error is also thrown, because Java doesn't recognize cygwin paths:

```
Jorge@JorgeDiaz /mnt/d/ElasticSearch/elasticsearch-2.1.1/bin
$ ./plugin install lang-javascript
Error: Could not find or load main class org.elasticsearch.plugins.PluginManagerCliParser
```

_Solution:_ Apply same path validations than _elasticsearch_ script
</description><key id="133676738">16666</key><summary>elasticsearch-plugin script does not work on cygwin environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jorgediaz-lr</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>enhancement</label></labels><created>2016-02-15T10:24:30Z</created><updated>2016-02-29T20:06:03Z</updated><resolved>2016-02-29T20:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jorgediaz-lr" created="2016-02-29T20:02:44Z" id="190358817">@jasontedor is removing cygwin code at elasticsearch script at https://github.com/elastic/elasticsearch/pull/16871 so I am closing this issue.

If anybody needs to patch scripts for cygwin, see following changes:
- elasticsearch-plugin: https://github.com/jorgediaz-lr/elasticsearch/commit/6d381c51644cc12f52fe73bca57c3b3a06aaa39c
- elasticsearch: https://github.com/elastic/elasticsearch/commit/58bcd7f8e71a282d534f570a39f0c05270568935
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16665</link><project id="" key="" /><description>I signed the CLA! Not sure how to retrigger....
</description><key id="133669824">16665</key><summary>Typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joynes</reporter><labels><label>docs</label></labels><created>2016-02-15T09:57:27Z</created><updated>2016-02-15T12:40:15Z</updated><resolved>2016-02-15T12:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-15T12:40:11Z" id="184190243">thanks @joynes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top-level inner_hits syntax confusing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16664</link><project id="" key="" /><description>I was very confused by the top-level `inner_hits` syntax. I spent an entire day tracking down why some search results would return inner hits and others didn't even though they should have. What was my confusion?

Here was the original request I used which would return inner hits some of the time.

``` js
{
    query: {
        has_child: {
            type: 'child_docs',
            score_mode: 'max',
            query: {
                bool: {
                    should: [
                        {
                            query_string: { query: 'google' }
                        },
                        {
                            has_child: {
                                type: 'grand_child_docs',
                                score_mode: 'max',
                                query: { query_string: { query: 'google' } }
                            }
                        }
                    ]
                }
            }
        }
    },
    inner_hits: {
        child_documents: {
            type: {
                child_docs: {
                    query: { 
                        query_string: { query: 'google' }
                    },
                    inner_hits: {
                        grand_child_documents: {
                            type: {
                                grand_child_docs: {
                                    query: {
                                        query_string: { query: 'google' }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

Here is the request with the correct syntax where inner hits were always returned.

``` js
{
    query: {
        has_child: {
            type: 'child_docs',
            score_mode: 'max',
            query: {
                bool: {
                    should: [
                        {
                            query_string: { query: 'google' }
                        },
                        {
                            has_child: {
                                type: 'grand_child_docs',
                                score_mode: 'max',
                                query: {
                                    query_string: { query: 'google' }
                                }
                            }
                        }
                    ]
                }
            }
        }
    },
    inner_hits: {
        child_documents: {
            type: {
                child_docs: {
                    query: {
                        bool: {
                            should: [
                                {
                                    query_string: { query: 'google' }
                                },
                                {
                                    has_child: {
                                        type: 'grand_child_docs',
                                        score_mode: 'max',
                                        query: {
                                            query_string: { query: 'google' }
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    inner_hits: {
                        grand_child_documents: {
                            type: {
                                grand_child_docs: {
                                    query: {
                                        query_string: { query: 'google' }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

If you notice, the difference is the child query in the `inner_hits` section. By repeating the original query in the `inner_hits` section for the child, everything works correctly. However I did not do that initially. Here was my thought process: 
1. Since the server already knows the search results, I don't have to repeat the query. Hmm that didn't give me the correct results.
2. I see `query` mentioned in the  [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html) and it says it must be specified otherwise all inner docs are returned.
3. I assume I need to define the child's query and grandchild's query separately similar to how nested `inner_hits` works. That is what the first example does. It never returns inner hits that are in grandchildren only.
4. Start trying random things because I don't know what I am missing. Eventually try copying the original query to the child's `inner_hits` section and that finally works.

I think the root of the confusion is that `inner_hits` is not consistent with other parts of the ES syntax. It doesn't work like the nested `inner_hits` nor does it work like other things like highlighting. By default, the highlighters assume you want to highlight based on the original query. They only highlight using a different query if you override it.

I think two things would have helped me:
1. Make it clear you need to repeat the original query in the `inner_hits` section.
2. Make the default to use the original query rather than returning all inner hit docs. Then have some sort of special field/value to say you want all the inner hit docs. This would also make more sense given the name "inner_hits" which I would assume to mean "inner search hits".
</description><key id="133599651">16664</key><summary>top-level inner_hits syntax confusing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rpedela</reporter><labels><label>:Inner Hits</label><label>breaking</label><label>discuss</label></labels><created>2016-02-15T01:10:46Z</created><updated>2016-02-15T16:39:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Adding a Index &amp; Filter not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16663</link><project id="" key="" /><description>Hi guys, im trying this

```
curl -XPOST localhost:9200/_aliases -d '{
    "actions": [{
        "add": {
            "index": "common",
            "alias": "alias_test",
            "filter": {"term": {"customer_id": 1}},
            "routing": "alias_test"
        }
    }]
}'
```

And getting this result 

```
$ curl -XGET 'localhost:9200/common/_alias/*?pretty'

{
  "common" : {
    "aliases" : {
      "alias_test" : {
        "index_routing" : "alias_test",
        "search_routing" : "alias_test"
      },
      "dolores98141" : {
        "filter" : {
          "term" : {
            "orgId" : "dolores98141"
          }
        },
        "index_routing" : "dolores98141",
        "search_routing" : "dolores98141"
      }
    }
  }
}
```

Where `dolores98141` was a previously created index (with a filter).

The issue here: I'm not seeing any filter associated with the new `alias_test` index. Is this a bug or am I using the API in the wrong way? 

Apparently the `filter` is only added at index creation and not when we are editing an alias

Thans in advance

Im using `elasticsearch:2.2.0`
</description><key id="133599091">16663</key><summary>Adding a Index &amp; Filter not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matiasdecarli</reporter><labels /><created>2016-02-15T01:00:01Z</created><updated>2016-02-15T16:28:43Z</updated><resolved>2016-02-15T01:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-15T01:30:21Z" id="184019896">Duplicates #16547, closed by #16553
</comment><comment author="matiasdecarli" created="2016-02-15T01:34:45Z" id="184021001">Thanks @jasontedor :clap: 
</comment><comment author="matiasdecarli" created="2016-02-15T15:30:42Z" id="184254962">As a workaround, and for further reference: Im using the [putAlias](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference-2-2.html#api-indices-putalias-2-2) method
</comment><comment author="jasontedor" created="2016-02-15T16:28:43Z" id="184283209">&gt; As a workaround, and for further reference: Im using the [putAlias](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference-2-2.html#api-indices-putalias-2-2) method

@matiasdecarli That will work; thanks for posting the workaround for others here until the next patch release includes the fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog recovery failure eating CPU</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16662</link><project id="" key="" /><description>Found one of the nodes in my idle (other than marvel), local **2.2.0** cluster eating a chunk of CPU today.

![image](https://cloud.githubusercontent.com/assets/6202/13036529/42007536-d32f-11e5-82bb-8a867af923e4.png)

```
% curl -s localhost:9200/_cat/nodes\?h=ip,master,name
127.0.0.1 * Graviton     
127.0.0.1 m Blood Spider 
% curl -s localhost:9200/_cat/shards | fgrep 02.13
.marvel-es-2016.02.13                     0 p INITIALIZING                127.0.0.1 Graviton     
.marvel-es-2016.02.13                     0 r UNASSIGNED
%
```

A bunch of checkpoint logs happen:

```
[2016-02-14 00:29:10,769][DEBUG][index.translog           ] [Graviton] [.marvel-es-2016.02.13][0] recovered local translog from checkpoint Checkpoint{offset=2556515, numOps=4273, translogFileGe
neration= 3691}
```

Both nodes load their data dir from the same `nodes/` path, and this volume is **87% full**.  So allocation of this shard keeps occurring:

```
[2016-02-14 00:29:28,147][INFO ][cluster.routing.allocation.decider] [Graviton] low disk watermark [85%] exceeded on [DG6Jls2UTrSSsrG2huoYMQ][Graviton][/home/aar/Downloads/elasticsearch-2.2.0/d
ata/elasticsearch/nodes/1] free: 66.2gb[14.8%], replicas will not be assigned to this node
[2016-02-14 00:29:28,147][INFO ][cluster.routing.allocation.decider] [Graviton] low disk watermark [85%] exceeded on [w8hx8OmwRg6ETfhbAA6XYg][Blood Spider][/home/aar/Downloads/elasticsearch-2.2
.0/data/elasticsearch/nodes/0] free: 66.2gb[14.8%], replicas will not be assigned to this node
```

Then the allocation is attempted:

```
[2016-02-14 00:29:29,120][DEBUG][index.translog           ] [Graviton] [.marvel-es-2016.02.13][0] translog closed
[2016-02-14 00:29:29,120][DEBUG][index                    ] [Graviton] [.marvel-es-2016.02.13] [0] closing... (reason: [failed recovery])
[2016-02-14 00:29:29,120][DEBUG][index.shard              ] [Graviton] [.marvel-es-2016.02.13][0] state: [RECOVERING]-&gt;[CLOSED], reason [failed recovery]
[2016-02-14 00:29:29,120][DEBUG][index.shard              ] [Graviton] [.marvel-es-2016.02.13][0] operations counter reached 0, will not accept any further writes
[2016-02-14 00:29:29,120][DEBUG][index.store              ] [Graviton] [.marvel-es-2016.02.13][0] store reference count on close: 0
[2016-02-14 00:29:29,120][DEBUG][index                    ] [Graviton] [.marvel-es-2016.02.13] [0] closed (reason: [failed recovery])
[2016-02-14 00:29:29,120][WARN ][indices.cluster          ] [Graviton] [[.marvel-es-2016.02.13][0]] marking and sending shard failed due to [failed recovery]
[.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:178)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        ... 5 more
Caused by: [.marvel-es-2016.02.13][[.marvel-es-2016.02.13][0]] EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
        ... 11 more
Caused by: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]];
        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:102)
        at org.elasticsearch.index.translog.TranslogReader.access$000(TranslogReader.java:46)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:297)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
        at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
        ... 12 more
Caused by: java.io.EOFException: read past EOF. pos [2556515] length: [4] end: [2556515]
        at org.elasticsearch.common.io.Channels.readFromFileChannelWithEofException(Channels.java:102)
        at org.elasticsearch.index.translog.ImmutableTranslogReader.readBytes(ImmutableTranslogReader.java:84)
        at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:91)
        ... 17 more
[2016-02-14 00:29:29,121][WARN ][cluster.action.shard     ] [Graviton] [.marvel-es-2016.02.13][0] received shard failed for [.marvel-es-2016.02.13][0], node[DG6Jls2UTrSSsrG2huoYMQ], [P], v[7374], s[INITIALIZING], a[id=0EodiK9gS8KIGvHSHNi1ew], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-14T06:29:10.518Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/0/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2343547] length: [4] end: [2343547]]; ]], indexUUID [Wl7rh0TuT1ms79hg-H8hzA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /home/aar/Downloads/elasticsearch-2.2.0/data/elasticsearch/nodes/1/indices/.marvel-es-2016.02.13/0/translog/translog-6.tlog]; nested: EOFException[read past EOF. pos [2556515] length: [4] end: [2556515]]; ]
```

This process repeats ad infinitum.  `DEBUG` log file is each over 1g the past couple of days.

`hot_threads` has this repeated section:

```
   98.7% (493.3ms out of 500ms) cpu usage by thread 'elasticsearch[Graviton][generic][T#1]'
     10/10 snapshots sharing following 24 elements
       sun.nio.fs.UnixNativeDispatcher.readdir(Native Method)
       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.readNextEntry(UnixDirectoryStream.java:168)
       sun.nio.fs.UnixDirectoryStream$UnixDirectoryIterator.hasNext(UnixDirectoryStream.java:201)
       org.elasticsearch.index.translog.Translog$OnCloseRunnable.handle(Translog.java:726)
       org.elasticsearch.index.translog.Translog$OnCloseRunnable.handle(Translog.java:713)
       org.elasticsearch.index.translog.ChannelReference.closeInternal(ChannelReference.java:67)
       org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64)
       org.elasticsearch.index.translog.TranslogReader.close(TranslogReader.java:143)
       org.apache.lucene.util.IOUtils.close(IOUtils.java:97)
       org.elasticsearch.index.translog.Translog.close(Translog.java:425)
       org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:129)
       org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:118)
       org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:183)
       org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
       org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)
       org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)
       org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)
       org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)
       org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
       org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
       org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```
</description><key id="133584003">16662</key><summary>Translog recovery failure eating CPU</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:Translog</label><label>enhancement</label></labels><created>2016-02-14T21:44:47Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-15T10:29:22Z" id="184154064">@drewr I understand  this is  a fresh 2.2.0 and not an upgrade from an older version? Do you have anything in the logs indicating why the primary would be unassigned to begin with? Was the cluster taken down? Also, can you save the index folder and put it somewhere we can take a look?

The CPU is explained by the master continuously trying to open the single shard it can find (and running into this issue). IMO we should treat translog corruption as proper corruption and make the shard as bad. @s1monw thoughts?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API for listing index file sizes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16661</link><project id="" key="" /><description>I'd like to propose this API to query index file sizes, as part of SegmentsStats.

_Copypasting a comment previously posted on #16131:_

Some comments/questions follow.

I'm consolidating file sizes using the same criteria of `SegmentsStats` to expose disk usage of Lucene index files: `Terms`, `TermVectors`, `StoredFields`, `Norms`, `DocValues` (class `IndexResources` in `SegmentsStats`).

`SegmentsStats` currently exposes memory consumption as bytes of each of the aforementioned so it's possible to add these as part of a class (`IndexResources`) and expose it as two fields of `SegmentsStats`, one for memory resources (existing stats) and other for disk resources. This is what I implemented in my branch, but I'm not sure if it would be better to expose this as another "sister" class of `SegmentsStats`, member of `CommonStats`, and leave `SegmentsStats` as it is. I'm however aware that my `toXContent` serialization could break clients expecting certain fields that now are placed elsewhere.

Some other couple questions I've got:
1. Is it necessary to consolidate sizes by the same criteria of `SegmentsStats` or may be better doing it using the file extensions? This last option has the advantage that would include additional information about postings, i.e., separate size info for positions, payloads, etc.
2. The meat of the matter: Is the way I open the Directory from the `SegmentInfo` and the compound reader a sensible approach? I'm not sure if this could introduce issues with the `Store`.

Closes #16131
</description><key id="133579874">16661</key><summary>API for listing index file sizes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">camilojd</reporter><labels><label>:Stats</label><label>feature</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-14T21:03:46Z</created><updated>2016-03-03T16:14:54Z</updated><resolved>2016-03-02T18:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-15T12:31:40Z" id="184187094">@mikemccand would you mind having a look at this PR please?
</comment><comment author="mikemccand" created="2016-02-15T14:00:52Z" id="184219154">&gt; Is the way I open the Directory from the SegmentInfo and the compound reader a sensible approach?

Yes, that's correct!

&gt; Is it necessary to consolidate sizes by the same criteria of SegmentsStats or may be better doing it using the file extensions? This last option has the advantage that would include additional information about postings, i.e., separate size info for positions, payloads, etc.

I think I'd rather see by file extension?  E.g. knowing whether your terms dict is huge, or your docs or positions or payloads are huge, is important information.

We could do extensions plus an "interpretation" e.g. ".tim (terms)", ".tip (terms index)", etc.?  Doing it this way also has the advantage that an unknown extension (which can easily happen when lucene changes its file format, e.g. the upcoming dimensional points is a change in 6.0) can just be added into the stats.
</comment><comment author="camilojd" created="2016-02-15T23:44:32Z" id="184442534">&gt; We could do extensions plus an "interpretation" e.g. ".tim (terms)", ".tip (terms index)", etc.? Doing it this way also has the advantage that an unknown extension (which can easily happen when lucene changes its file format, e.g. the upcoming dimensional points is a change in 6.0) can just be added into the stats.

I like this compromise. I'll simplify some things and proceed to fix the mentioned issues. :+1: 
</comment><comment author="camilojd" created="2016-02-19T22:44:24Z" id="186438518">@mikemccand Hi Mike,

I fixed all of the above, and some more. Details:
- Simplified field: just a `ImmutableOpenMap&lt;String, Long&gt;` to store per extension sizes.
- Changed exception handling to limit try/catch blocks to the minimum scope, logging with warn when necessary
- _Transport_ serialization doesn't serialize the map field until ES Version 3.0
- XContent serialization takes care of handling known/unknown extension descriptions by querying a map.
- Querying segment disk usage is optional: I added a boolean flag to `CommonStatsFlags`, triggered by setting a `detailed_segments` parameter in the REST endpoint to query segments disk stats. I don't know if there is a nicer way to do this.

So far it works and passes all the tests.

It seems to me that a better place for this would be under `StoreStats`, because `Store` already queries the effective size of all physical files and caches the result... but had no idea/luck on how to grab a `SegmentInfo` for uncommitted segments (only does show some disk usage after forcing a `_flush`). See https://github.com/camilojd/elasticsearch/commit/df0370ff071d94ba90d6b7eae2b7acf004ea921c
</comment><comment author="mikemccand" created="2016-02-21T10:49:38Z" id="186797317">Thanks @camilojd, this is looking very nice.  I just left some minor comments.

&gt; Querying segment disk usage is optional: I added a boolean flag to CommonStatsFlags, triggered by setting a detailed_segments parameter in the REST endpoint to query segments disk stats. I don't know if there is a nicer way to do this.

I think `CommonStatsFlags` is fine for now: I don't know of another place to put it (maybe someone else has a suggestion?).

Do we also need to punch the new boolean option through to e.g. the Java client API (and eventually the other language clients)?

&gt; It seems to me that a better place for this would be under StoreStats, because Store already queries the effective size of all physical files and caches the result... but had no idea/luck on how to grab a SegmentInfo for uncommitted segments (only does show some disk usage after forcing a _flush).

I do agree it would be more natural to have the stats there, e.g. we could just fix store stats to break out the file sizes by extension, and not cause any additional IO load to the filesystem.

Maybe it would be an OK limitation that it'd only list flushed segments?  We could explain that in the docs?
</comment><comment author="camilojd" created="2016-02-23T03:51:09Z" id="187514465">@mikemccand just pushed changes that address all your comments. I really like how the code became more self-contained. :-)

&gt; Do we also need to punch the new boolean option through to e.g. the Java client API (and eventually the other language clients)?

The option is in the Client API, accessible through `IndicesStatsRequest.detailedSegmentsStats(boolean)`, so also through `NodesStatsRequest.indices`. 

Currently it can be requested through HTTP in `RestIndicesStatsAction` and `RestNodesStatsAction`, but it's kinda ugly, just checks if the key `detailed_segments` is in the query string. Maybe leave the option only in the Client API by now?

&gt; Maybe it would be an OK limitation that it'd only list flushed segments? We could explain that in the docs?

It's possible to do that, but it's kind of weird. Currently `StoreStats` already informs the _real_ disk usage of the directory, but numbers in my implementation will almost always be very far from the total, because of unflushed segments. `SegmentsStats` is more accurate (not exactly 100% accurate because of cfs, etc).
</comment><comment author="mikemccand" created="2016-02-23T16:42:48Z" id="187783169">Thanks @camilojd, I'll review your latest changes.

&gt; The option is in the Client API, accessible through IndicesStatsRequest.detailedSegmentsStats(boolean), so also through NodesStatsRequest.indices.

Oh good, sorry I missed this.

&gt; Currently it can be requested through HTTP in RestIndicesStatsAction and RestNodesStatsAction, but it's kinda ugly, just checks if the key detailed_segments is in the query string. Maybe leave the option only in the Client API by now?

I think it's good to expose the option in the REST API too.

&gt; SegmentsStats is more accurate (not exactly 100% accurate because of cfs, etc).

OK let's leave it where it is now, but maybe add a TODO about whether this could/should move to `StoreStats` instead, explaining the tradeoffs we discussed here?
</comment><comment author="mikemccand" created="2016-02-23T16:59:06Z" id="187790863">This change looks great!

I just left a minor (naming, the hardest part!) comment, and let's add a TODO about maybe moving this to `StoreStats` in the future?

Can you add some tests here to confirm the feature is working and catch us if anyone breaks it in the future, and also update the docs explaining this new cool parameter?  Thanks @camilojd!
</comment><comment author="camilojd" created="2016-02-26T20:32:34Z" id="189472866">@mikemccand all comments were addressed.

Do you think there is anything left to improve?
Thanks for the amazing help!
</comment><comment author="mikemccand" created="2016-02-27T01:22:54Z" id="189546534">I think we just need to document the new parameter and then we are done!  Thanks @camilojd.
</comment><comment author="camilojd" created="2016-02-29T22:46:44Z" id="190434002">@mikemccand now it's done! :+1: Thank you sir!
</comment><comment author="mikemccand" created="2016-03-01T14:57:03Z" id="190756106">Thanks @camilojd I left one more comment!
</comment><comment author="camilojd" created="2016-03-02T01:23:13Z" id="191007323">@mikemccand I updated the code according to the comments :-) thanks!

Edit: fixed the exception handling to log the correct messages and limit the `try {}` scope to one line at most.
</comment><comment author="mikemccand" created="2016-03-02T18:57:01Z" id="191371543">Thanks @camilojd, this looks great, I'll push to 5.0 now!
</comment><comment author="camilojd" created="2016-03-03T04:25:07Z" id="191576310">Awesome! thank you very much @mikemccand!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Class permission for Groovy references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16660</link><project id="" key="" /><description>This commit adds a class permission for groovy.lang.Reference so they
can be used in scripts.

Closes #16657
</description><key id="133542587">16660</key><summary>Class permission for Groovy references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-14T14:16:18Z</created><updated>2016-02-15T12:23:14Z</updated><resolved>2016-02-14T15:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-14T14:16:24Z" id="183896021">@ywelsch Do you mind reviewing?
</comment><comment author="ywelsch" created="2016-02-14T14:18:29Z" id="183896447">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add permission to access groovy.lang.Reference from Groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16659</link><project id="" key="" /><description>Closes #16657
</description><key id="133542515">16659</key><summary>Add permission to access groovy.lang.Reference from Groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Scripting</label><label>review</label></labels><created>2016-02-14T14:15:13Z</created><updated>2016-02-14T15:00:49Z</updated><resolved>2016-02-14T14:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-14T14:18:51Z" id="183896461">Closed by #16660
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updates to resiliency documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16658</link><project id="" key="" /><description>Made the following updates:
- Moved "Use two phase commit for Cluster State publishing" to new "Completed but unreleased yet" section
- Moved "Make index creation more user friendly" a bit down and commented on linked issue #9126 (awaiting response there)
- A few additions to "OOM resiliency" and "Loss of documents during network partition"
- Added a new paragraph "Safe shard topology changes"
</description><key id="133541913">16658</key><summary>Updates to resiliency documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>docs</label></labels><created>2016-02-14T14:03:42Z</created><updated>2016-02-19T17:50:40Z</updated><resolved>2016-02-19T17:50:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-15T13:46:57Z" id="184214633">Thanks @ywelsch SO much for picking this up :) . I left some comments.
</comment><comment author="bleskes" created="2016-02-15T20:32:43Z" id="184378150">@ywelsch we should probably add something about delayed allocation on node leave and synced flush (resilient to network hickups/long GC) 
</comment><comment author="ywelsch" created="2016-02-18T23:47:43Z" id="185982789">@bleskes updated the docs with your suggestions (apart from your last comment on delayed allocation on node leave and synced flush)
</comment><comment author="bleskes" created="2016-02-19T15:21:01Z" id="186257687">LGTM. Thanks Yannick,
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script throws java.lang.NoClassDefFoundError: groovy/lang/Reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16657</link><project id="" key="" /><description>Elasticsearch 2.2.0.

Combination of `def` keyword and closure gives the exception.

This gives java.lang.NoClassDefFoundError: groovy/lang/Reference:

```
GET index/_search
{
  "script_fields": {
    "test_script": {
      "script": {
        "inline": "def val = \"\"; p.each{ val += it }; val",
        "params": {
          "p":["aaa", "bbb", "ccc"]
        }
      }
    }
  }
}
```

This is fine:

```
GET index/_search
{
  "script_fields": {
    "test_script": {
      "script": {
        "inline": "val = \"\"; p.each{ val += it }; val",
        "params": {
          "p":["aaa", "bbb", "ccc"]
        }
      }
    }
  }
}
```

Adding `permission org.elasticsearch.script.ClassPermission "groovy.lang.Reference"` in `modules/lang-groovy/plugin-security.policy` seems to fix the issue.
</description><key id="133524855">16657</key><summary>Groovy script throws java.lang.NoClassDefFoundError: groovy/lang/Reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Scripting</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-02-14T09:28:21Z</created><updated>2016-02-14T16:24:51Z</updated><resolved>2016-02-14T15:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-14T15:15:19Z" id="183904356">Closed by #16660
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove host from cat nodes API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16656</link><project id="" key="" /><description>As the host and ip fields are always equal by design, the host field in
the cat nodes API is redundant and should be removed.

Relates #12959, closes #16575 
</description><key id="133503809">16656</key><summary>Remove host from cat nodes API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-14T04:09:33Z</created><updated>2016-02-14T14:29:40Z</updated><resolved>2016-02-14T14:24:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-14T04:43:08Z" id="183820365">It looks good but you need to change also this page I think: https://www.elastic.co/guide/en/elasticsearch/reference/master/cat-nodes.html

(See option list)
</comment><comment author="jasontedor" created="2016-02-14T04:55:06Z" id="183821524">@dadoonet Thanks for reviewing! I pushed f12fc5e6626d2ddaed79ff6fd43147be8d79c305. I noticed that those docs are _horrifically_ out of date, but I think that getting them up to date should be a follow up item.
</comment><comment author="dadoonet" created="2016-02-14T08:07:00Z" id="183846120">Agreed. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Remove legacy testing and releasing scripts.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16655</link><project id="" key="" /><description>These scripts are no longer needed:
- build_randomization.rb - it used by CI a long time ago, but no longer
- client_tests_urls.prop - not sure when this was used, but it refers to ancient branches
- download-s3.py - replaced by s3cmd in release scripts
- upload-s3.py - replaced by s3cmd in release scripts
- upgrade-tests.py - these were the old upgrade tests, before the static index bwc tests
</description><key id="133489528">16655</key><summary>Build: Remove legacy testing and releasing scripts.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-14T01:14:31Z</created><updated>2016-02-23T00:32:51Z</updated><resolved>2016-02-22T21:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-02-22T21:23:16Z" id="187388829">LGTM
</comment><comment author="s1monw" created="2016-02-23T00:32:51Z" id="187452522">900+ deletions I am jealous!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2.0 delete by query plugin fails for data with external versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16654</link><project id="" key="" /><description>Using 2.2.0, I am unable to delete by query for data that has been indexed using `external_gte` [version type](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types).  Here's the error that I'm receiving:

```
[ec2-user@es1-dev ~]$ curl -XDELETE 'http://es1:9200/testindex/_query?q=repo:testing'
{"error":{"root_cause":[{"type":"action_request_validation_exception","reason":"Validation Failed: 1: illegal version value [0] for version type [INTERNAL];2: illegal version value [0] for version type [INTERNAL];3: illegal version value [0] for version type [INTERNAL];4: illegal version value [0] for version type [INTERNAL];5: illegal version value [0] for version type [INTERNAL];6: illegal version value [0] for version type [INTERNAL];7: illegal version value [0] for version type [INTERNAL];8: illegal version value [0] for version type [INTERNAL];9: illegal version value [0] for version type [INTERNAL];10: illegal version value [0] for version type [INTERNAL];"}],"type":"action_request_validation_exception","reason":"Validation Failed: 1: illegal version value [0] for version type [INTERNAL];2: illegal version value [0] for version type [INTERNAL];3: illegal version value [0] for version type [INTERNAL];4: illegal version value [0] for version type [INTERNAL];5: illegal version value [0] for version type [INTERNAL];6: illegal version value [0] for version type [INTERNAL];7: illegal version value [0] for version type [INTERNAL];8: illegal version value [0] for version type [INTERNAL];9: illegal version value [0] for version type [INTERNAL];10: illegal version value [0] for version type [INTERNAL];"},"status":400}
```

The delete by query succeeds for an index that doesn't use `external_gte`.

thanks!
</description><key id="133474219">16654</key><summary>ES 2.2.0 delete by query plugin fails for data with external versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">natelapp</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2016-02-13T21:19:06Z</created><updated>2016-10-03T11:35:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T00:13:23Z" id="183778740">@bleskes what do you think?
</comment><comment author="bleskes" created="2016-02-15T14:17:36Z" id="184225525">This indeed an unfortunate case where internal and external versioning do not mix well. Internal version mean that ES is the source of truth for changes - it is incremented with every change in ES and starts with 1. External versioning assumes that some other system tracks document changes (including deletes). Originally 0 was an invalid value for external versioning but it wasn't enforced in code. When we fixed the latter people complained and we have changed semantics to allow 0 as a valid external value (see https://github.com/elastic/elasticsearch/issues/5662). Now you can insert a value that's valid as an external version but is illegal for internal.

The delete by query plugin uses internal versioning to make sure the documents it deletes didn't change during it's operations. However, since the documents were indexed using the external versioning, their version is 0 which is illegal.  

Can you tell us a bit more about your setup? Why are you using the delete by query plugin where you have some external source of truth? I would presume you would delete documents there first and have those propagated to ES as deletes with an external version?
</comment><comment author="natelapp" created="2016-02-15T15:27:01Z" id="184253330">We receive our data from a third-party that supplies versions, starting with 0.  For one of our indexes, we only care about the most recent version of a given resource, but need to be able to support reloading old data (mapping changes, etc).  In order to ensure we're only keeping the latest (regardless of order received) we've gone with indexing using `external_gte`.  Our process simply ignores the VersionConflictException that gets returned when attempting to add an older version.  It has worked rather well for us.

Periodically, we'll need to delete data, for a variety of reasons.  These are one-off deletes, usually related to expiring license agreements and such, and are separate from any versioning scheme.  Historically we've just manually done a delete by query to handle these cases, which has served us well until recently.
</comment><comment author="niemyjski" created="2016-09-29T19:37:17Z" id="250569009">I'm using internal indexing and hitting this on index...

 illegal version value [0] for version type [INTERNAL];
</comment><comment author="bleskes" created="2016-10-03T11:35:55Z" id="251085159">@niemyjski as we discussed in another issue, your issue is different than this one.

@natelapp thanks for the update. The problem is that currently doesn't align with the main use case for external versioning, where some external source owns all changes to the documents, including deletes. I haven't come up with a clean way of allowing you to do what you need plus making other use cases work without surprises. As a workaround for now, I think the easiest for you is to always +1 the version you get from your data source (to allow a delete by query operation).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner hits _source shouldn't use relative paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16653</link><project id="" key="" /><description>In 2.0 we switched to requiring full paths whenever referring to a field.  However, the `_source` parameter for inner hits requires the relative path (even though the query requires a full path).

This is inconsistent and should be changed.

Example:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "bar": {
              "type": "object",
              "properties": {
                "baz": {
                  "type": "string"
                }
              }
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": {
    "bar": {
      "baz": "xxx"
    }
  }
}

GET t/t/_search
{
  "_source": false,
  "query": {
    "nested": {
      "path": "foo",
      "inner_hits": {
        "_source": "bar.*"
      },
      "query": {
        "match": {
          "foo.bar.baz": "xxx"
        }
      }
    }
  }
}
```
</description><key id="133428724">16653</key><summary>Inner hits _source shouldn't use relative paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Inner Hits</label><label>adoptme</label><label>breaking</label><label>v5.0.0-alpha4</label></labels><created>2016-02-13T12:36:29Z</created><updated>2016-05-27T11:57:22Z</updated><resolved>2016-05-27T11:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T12:38:09Z" id="183658729">Related to #16515
</comment><comment author="martijnvg" created="2016-02-15T21:52:49Z" id="184410214">The reason that the _source is relative here, is that each inner hit only has part of the source (the json object) it is referring to. The fetch phase splits the source up in smaller source instances (json objects) before executing the fetch operations for the inner hits. 
</comment><comment author="clintongormley" created="2016-02-17T15:37:07Z" id="185259007">@martijnvg OK, I understand the why now, but its inconsistency with all other uses of path is annoying. Anything we can do to fake it?
</comment><comment author="martijnvg" created="2016-02-17T16:26:13Z" id="185282434">@clintongormley Yes, I think we can fake it, by taking the provided `path` into account. So each time we do something with the nested source we remove the nested path prefix from the source field names.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2 Highlight is not working anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16652</link><project id="" key="" /><description>We worked on ES 1.4 for about 2 years, more or less. Deployed the product on the customer side and everyone was happy.

Now, we decided to upgrade to 2.2, recode all APIs changes and the highlight is not working anymore. It is not even returned.

Example of a query:

http://localhost:9200/index/Persons/_search

```
{
    "query": {
    "match": {
      "_all": "pippo"
    }
  },
    "highlight" : {
        "fields" : {
            "*" : {}
        }
    }
}
```

Results are returned but the Highlight node is gone, I tried all possible options and it seems that ES 2.2 doesn't return highlights at all but I look at the backlog and you didn't mention drastic changes under the highlight feature. 

```
{
  "took": 6,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 0.83263576,
    "hits": [
      {
        "_index": "myindex",
        "_type": "Persons",
        "_id": "8701653d-261c-4c67-b59c-9b7aac2df3b6",
        "_score": 0.83263576,
        "_source": {
          "Full_name": "Disney, Pippo",
          "Email": "pippomproof@gmail.com"
        }
      }
    ]
  }
}
```

Am I doing something wrong or simply ES 2.2 removed highlights?
</description><key id="133425915">16652</key><summary>ES 2.2 Highlight is not working anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raffaeu</reporter><labels /><created>2016-02-13T11:55:09Z</created><updated>2016-02-13T12:28:13Z</updated><resolved>2016-02-13T12:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raffaeu" created="2016-02-13T12:02:09Z" id="183654864">And of course highlight is mentioned in my mapping

```
{
  "myindex": {
    "mappings": {
      "Persons": {
        "properties": {
          "Email": {
            "type": "string"
          },
          "Full_name": {
            "type": "string"
          },
          "highlight": {
            "properties": {
              "fields": {
                "properties": {
                  "content": {
                    "type": "object"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-02-13T12:25:08Z" id="183657311">Hi @raffaeu 

It's in the breaking changes docs: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_search_changes.html#_only_highlight_queried_fields
</comment><comment author="raffaeu" created="2016-02-13T12:28:12Z" id="183657516">Thank you for pointing it out
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed scripts should be stored in cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16651</link><project id="" key="" /><description>Indexed scripts are now stored in a dedicated index. Moving indexed scripts to the cluster state will make the indexed script logic much simpler. We already have dedicated indexed script APIs in place so this shouldn't be too difficult.

When developing this we should think about bwc too, we can the first time we start ES after a full cluster restart decide to copy the scripts from the dedicated index into the cluster state, if we see there is a script index, but no scripts are in the cluster state.

Also we should then maybe rename indexed scripts, to stored scripts or something like that.
</description><key id="133420351">16651</key><summary>Indexed scripts should be stored in cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Indexed Scripts/Templates</label><label>adoptme</label><label>breaking</label></labels><created>2016-02-13T10:22:40Z</created><updated>2016-04-22T11:44:47Z</updated><resolved>2016-04-22T11:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-13T19:18:32Z" id="183728953">+1
</comment><comment author="uboness" created="2016-02-13T21:21:20Z" id="183755976">+1 as well...

this will also be a great opportunity to differentiate between scripts &amp; template (today we treat templates as scripts, while they are two different beasts... specially with template needing a context of escaping)

&gt; When developing this we should think about bwc too, we can the first time we start ES after a full cluster restart decide to copy the scripts from the dedicated index into the cluster state, if we see there is a script index, but no scripts are in the cluster state.

would be great if we do this for 3.0 and instead of having this migration done automatically, I think it's better to provide a tool that will do that as part of the 2.x-&gt;3.0 migration path.

&gt; Also we should then maybe rename indexed scripts, to stored scripts or something like that.

+1
</comment><comment author="clintongormley" created="2016-02-14T17:30:28Z" id="183933699">Related to #14837 
Related to #13729
</comment><comment author="martijnvg" created="2016-02-17T16:44:10Z" id="185291760">&gt; I think it's better to provide a tool that will do that as part of the 2.x-&gt;3.0 migration path.

+1 this is much simpler to develop and maintain.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Mapper Attachment Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16650</link><project id="" key="" /><description>Now that we have the ingest-attachment plugin (https://github.com/elastic/elasticsearch/pull/16490)  we should deprecate the mapper-attachment plugin.

The mapper-attachment plugin needs to remain deprecated for the life of 3.0, and can only be removed in 4.0.
</description><key id="133418900">16650</key><summary>Deprecate Mapper Attachment Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Mapper Attachment</label><label>deprecation</label><label>v2.3.0</label></labels><created>2016-02-13T10:02:18Z</created><updated>2016-02-15T15:41:20Z</updated><resolved>2016-02-15T15:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-13T11:11:41Z" id="183644083">+1. I can take it. It's basically just a question of documentation, right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos in exception/assert/log messages in core module.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16649</link><project id="" key="" /><description /><key id="133418643">16649</key><summary>Fix typos in exception/assert/log messages in core module.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-13T09:58:49Z</created><updated>2016-02-26T18:53:27Z</updated><resolved>2016-02-26T10:33:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dongjoon-hyun" created="2016-02-15T23:34:07Z" id="184441092">Hi, @clintongormley . Thank you for review. I rebased this PR to resolve the conflict.
</comment><comment author="danielmitterdorfer" created="2016-02-26T10:33:05Z" id="189209984">Thanks @dongjoon-hyun for your contribution. It looks good to me. I've merged the PR to master.
</comment><comment author="dongjoon-hyun" created="2016-02-26T16:50:44Z" id="189367314">Thank you, @danielmitterdorfer !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search nodes dont form a cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16648</link><project id="" key="" /><description>Hi All, 

I am trying to setup a ES cluster over windows machine using uni cast.  I think I have made all required  configuration changes, but still my ES nodes do not form cluster.

Here is my elasticseach.yml configurations 
=======Noed 8=======
cluster.name: elasticsearch
node.name: NODE8
node.data: true
network.host: "10.249.167.8"
network.publish_host: "10.249.167.8"
network.bind: "10.249.167.8"
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["10.249.167.9", "10.249.167.10", "10.249.167.8"]
# transport.tcp.port: 9300

=======Node9 Config========
cluster.name: elasticsearch
node.name: NODE9
node.data: true
network.host: "10.249.167.9"
network.publish_host: "10.249.167.9"
network.bind: "10.249.167.9"
discovery.zen.ping.multicast.enabled: false

discovery.zen.ping.unicast.hosts: ["10.249.167.9", "10.249.167.10", "10.249.167.8"]
# transport.tcp.port: 9300

I can query both ES node individually, but they dont form cluster 

Node 8 Get : http://10.249.167.8:9200/_cat/nodes?h=ip,port,heapPercent,name
10.249.167.8 9300 2 Cecilia Reyes 

Node 9 Get : http://10.249.167.9:9200/_cat/nodes?h=ip,port,heapPercent,name
10.249.167.9 9300 9 Victorius 

Following are the startup logs, any help would be appreciated a ton, I am stuck on this for a while now:(
[2016-02-13 01:08:06,395][WARN ][bootstrap                ] unable to install syscall filter: syscall filtering not supported for OS: 'Windows Server 2012 R2'
[2016-02-13 01:08:06,645][INFO ][node                     ] [NODE8] version[2.1.1], pid[7628], build[40e2c53/2015-12-15T13:05:55Z]
[2016-02-13 01:08:06,645][INFO ][node                     ] [NODE8] initializing ...
[2016-02-13 01:08:07,020][INFO ][plugins                  ] [NODE8] loaded [cloud-azure], sites []
[2016-02-13 01:08:07,051][INFO ][env                      ] [NODE8] using [1] data paths, mounts [[(C:)]], net usable_space [94.6gb], net total_space [126.6gb], spins? [unknown], types [NTFS]
[2016-02-13 01:08:09,170][INFO ][node                     ] [NODE8] initialized
[2016-02-13 01:08:09,170][INFO ][node                     ] [NODE8] starting ...
[2016-02-13 01:08:09,357][INFO ][transport                ] [NODE8] publish_address {10.249.167.8:9300}, bound_addresses {10.249.167.8:9300}
[2016-02-13 01:08:09,373][INFO ][discovery                ] [NODE8] elasticsearch/i42Qv-qNSJaSoLRCt2e5tg
[2016-02-13 01:08:13,936][INFO ][cluster.service          ] [NODE8] new_master {NODE8}{i42Qv-qNSJaSoLRCt2e5tg}{10.249.167.8}{10.249.167.8:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-02-13 01:08:13,983][INFO ][http                     ] [NODE8] publish_address {10.249.167.8:9200}, bound_addresses {10.249.167.8:9200}
[2016-02-13 01:08:13,983][INFO ][node                     ] [NODE8] started
[2016-02-13 01:08:16,715][INFO ][gateway                  ] [NODE8] recovered [1] indices into cluster_state

Node 9 Log========================================================================

[2016-02-13 01:08:44,988][WARN ][bootstrap                ] unable to install syscall filter: syscall filtering not supported for OS: 'Windows Server 2012 R2'
[2016-02-13 01:08:45,237][INFO ][node                     ] [NODE9] version[2.1.1], pid[6468], build[40e2c53/2015-12-15T13:05:55Z]
[2016-02-13 01:08:45,237][INFO ][node                     ] [NODE9] initializing ...
[2016-02-13 01:08:45,601][INFO ][plugins                  ] [NODE9] loaded [cloud-azure], sites []
[2016-02-13 01:08:45,625][INFO ][env                      ] [NODE9] using [1] data paths, mounts [[(C:)]], net usable_space [113.6gb], net total_space [126.6gb], spins? [unknown], types [NTFS]
[2016-02-13 01:08:47,554][INFO ][node                     ] [NODE9] initialized
[2016-02-13 01:08:47,554][INFO ][node                     ] [NODE9] starting ...
[2016-02-13 01:08:47,753][INFO ][transport                ] [NODE9] publish_address {10.249.167.9:9300}, bound_addresses {10.249.167.9:9300}
[2016-02-13 01:08:47,763][INFO ][discovery                ] [NODE9] elasticsearch/ys7WjfT3QR2DqwLFr-m6Ew
[2016-02-13 01:08:52,292][INFO ][cluster.service          ] [NODE9] new_master {NODE9}{ys7WjfT3QR2DqwLFr-m6Ew}{10.249.167.9}{10.249.167.9:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-02-13 01:08:52,342][INFO ][http                     ] [NODE9] publish_address {10.249.167.9:9200}, bound_addresses {10.249.167.9:9200}
[2016-02-13 01:08:52,342][INFO ][node                     ] [NODE9] started
[2016-02-13 01:08:53,649][INFO ][gateway                  ] [NODE9] recovered [0] indices into cluster_state
</description><key id="133382689">16648</key><summary>Elastic Search nodes dont form a cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smayank17</reporter><labels /><created>2016-02-13T01:21:41Z</created><updated>2016-02-13T06:43:05Z</updated><resolved>2016-02-13T06:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-13T06:43:05Z" id="183611689">May be a firewall issue? Your configuration looks correct but could be simplified.

No need for:
- cluster.name: elasticsearch
- node.data: true
- network.publish_host: "10.249.167.8"
- network.bind: "10.249.167.8"
- discovery.zen.ping.multicast.enabled: false

But please ask questions on discuss.elastic.co. We use github issues only for confirmed bugs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change topic order in ingest doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16647</link><project id="" key="" /><description>Changes:
- reordered processor topics to list them alphabetically (makes it easier to find a topic in the TOC)
- moved section about processors to the end of the guide (because it's reference info)
- moved the section about the ingest APIs up earlier in the book 

I built the doc and uploaded it to the following location, where you can see the reorganized topics more easily:

https://elasticsearchreference.firebaseapp.com/
</description><key id="133366558">16647</key><summary>Change topic order in ingest doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dedemorton</reporter><labels><label>:Ingest</label><label>docs</label><label>review</label></labels><created>2016-02-12T23:15:15Z</created><updated>2016-02-13T01:11:28Z</updated><resolved>2016-02-13T01:11:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-02-13T01:01:49Z" id="183549079">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Profile pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16646</link><project id="" key="" /><description>As a feature feature we may want to add the ability to profile a pipeline.
This then would tell for each processor how much time was spent relatively to the whole pipeline execution.
I think this should be part of the simulate api. Now that verbose simulate is being implemented as a processor decorator (#16562), this should be relatively easy to add.
</description><key id="133331276">16646</key><summary>Profile pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2016-02-12T20:06:07Z</created><updated>2016-02-12T20:06:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>SimpleRoutingIT.testRequiredRoutingMapping fails because a doc remains undeleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16645</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_metal/12355/

```
elastic/elasticsearch
master

SUMMARY

Please direct your attention to the attached stacktraces associated with some or all of these tests:

org.elasticsearch.routing.SimpleRoutingIT testRequiredRoutingMapping
FAILED: 1
ERROR: 0
SKIPPED: 80
TOTAL: 5644

BUILD INFO

Build   20160212182431-BE4C2D89
Log https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/406/console
Duration    12m 33s (753008ms)
Started 2016-02-12T18:24:32.000Z
Ended   2016-02-12T18:37:05.008Z
Exit Code   1
Host    slave-3740eaed (up 43 days)
OS  Ubuntu 15.04, Linux 3.19.0-42-generic
Specs   4 CPUs, 15.67GB RAM
java.version    1.8.0_45-internal
java.vm.name    OpenJDK 64-Bit Server VM
java.vm.version 25.45-b02
java.runtime.version    1.8.0_45-internal-b14
java.home   /usr/lib/jvm/java-8-openjdk-amd64
```

Failure replicates for me.

Due to https://github.com/elastic/elasticsearch/pull/10136, when a _routing is defined in the mapping, "Broadcast Deletes" are no longer allowed.  The `testRequiredRoutingMapping` test was updated in the single-delete case so that `isExists(), equalTo(true)` each time through the loop, since the document should not be deleted.

However, it looks like the Bulk Delete case wasn't changed, so the test is still trying to verify that the bulk delete "broadcasted" despite no _routing being specified:

``` java
for (int i = 0; i &lt; 5; i++) {
    try {
        client().prepareGet(indexOrAlias(), "type1", "1").execute().actionGet().isExists();
        fail();
    } catch (RoutingMissingException e) {
        assertThat(e.status(), equalTo(RestStatus.BAD_REQUEST));
        assertThat(e.getMessage(), equalTo("routing is required for [test]/[type1]/[1]"));
    }
    assertThat(
      client().prepareGet(indexOrAlias(), "type1", "1")
        .setRouting("0")
        .execute()
        .actionGet()
        .isExists(),
      equalTo(false));  // &lt;-- Here
}
```

I assumed this was just a test problem, so I updated it to match the single-delete case (`isExists(), equalTo(true))`.  But that begins to fail under other seeds:

```
gradle :core:integTest -Dtests.seed=42425D480F1F72E8 -Dtests.class=org.elasticsearch.routing.SimpleRoutingIT -Dtests.method="testRequiredRoutingMapping" -Dtests.locale=es-CL -Dtests.timezone=AET
```

So either:
1. The test was correct, Bulk broadcast deletes without _routing are allowed, and the document isn't being deleted for some reason
2. Or the test is incorrect, bulk broadcast deletes _shouldn't_ be allowed, but they are working anyway under certain conditions.

/cc @javanna any thoughts?
</description><key id="133329170">16645</key><summary>SimpleRoutingIT.testRequiredRoutingMapping fails because a doc remains undeleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Bulk</label><label>bug</label></labels><created>2016-02-12T19:55:58Z</created><updated>2016-02-27T18:03:33Z</updated><resolved>2016-02-27T18:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T11:56:56Z" id="183653713">&gt; Or the test is incorrect, bulk broadcast deletes shouldn't be allowed, but they are working anyway under certain conditions.

I'd say the test is incorrect.  Could they be working when the custom routing just happens to map to the same shard as default routing would?
</comment><comment author="javanna" created="2016-02-15T09:43:45Z" id="184135390">Wow, the change was made almost a year ago, yet I see there is something missing in bulk. Seems like bulk has always worked differently around routing required hence why the test was left unchanged. This looks like an actual bug, digging and coming up with a PR.
</comment><comment author="javanna" created="2016-02-15T15:22:36Z" id="184251436">I did some digging, here are my findings. 

As part of #10136 I should have removed broadcast delete from bulk as well, but I didn't realize it had a completely different code path for bulk, so it was left behind, which is why the test was still testing the broadcast delete.

The leftover broadcast delete for bulk is broken in many ways though:
- one delete item becomes multiple delete requests, one per shard. but the response item is only one, so if one fails, you'll see the failure in the bulk response only if it was the last one returned from the shards.
- the same delete request object was reused throughout the different shards. As a result, the version in the request was updated with the result of the last delete executed, which affects the following delete operations (if the shards are on the same node) by setting a version that is not `-3` (match_any) but rather `1`. This may cause stuff like `VersionConflictEngineException[[type1][1]: version conflict, current version [2] is higher or equal to the one provided [1]]` depending on the execution order and the number of shards.

I have no idea why this test started failing only recently, but the reason why it failed was that in some cases the broadcast delete on the shard that contained the document to delete caused a version conflict, which may get returned or not as part of the bulk response (anyways the response wasn't checked in the test). I think the follow-up is simply to remove any leftover of broadcast delete and return a bulk item failure in case routing is required but it is not specified.
</comment><comment author="itcuihao" created="2016-02-22T07:28:29Z" id="187051789">This code is what role ?Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Does index.codec best_compression still need to be experimental?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16644</link><project id="" key="" /><description>It's supported by core Lucene, so can we really call it an experimental feature?  If it is experimental and we were to remove it in the future, how would users access data stored using the best_compression codec?  This seems to be something we have to commit to one way or the other.
</description><key id="133324084">16644</key><summary>[docs] Does index.codec best_compression still need to be experimental?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>docs</label></labels><created>2016-02-12T19:31:57Z</created><updated>2016-02-15T12:34:22Z</updated><resolved>2016-02-15T12:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T11:54:17Z" id="183652193">@jpountz what do you think?
</comment><comment author="jpountz" created="2016-02-13T12:44:34Z" id="183659450">I wish we kept it experimental so that we could remove it some day if it becomes irrelevant. But maybe we should be more explicit about the fact that it will be supported in terms of bw compat even if it ever gets removed?
</comment><comment author="clintongormley" created="2016-02-13T13:54:32Z" id="183669528">I think that, if it becomes irrelevant one day, then removing it would be an enhancement, no?  eg replacing facets with aggregations, or replacing binary doc values with numeric.  We're allowed to remove things that aren't experimental. I think your statement about bwc indicates that this is a feature we will continue to support as long as it makes sense.
</comment><comment author="jasontedor" created="2016-02-13T14:09:38Z" id="183675331">Related Discourse post ["Use &#8220;best_compression&#8221; index setting in 2.1.1?"](https://discuss.elastic.co/t/use-best-compression-index-setting-in-2-1-1/40169).
</comment><comment author="jpountz" created="2016-02-14T22:17:05Z" id="183991327">@clintongormley fine with me
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use MappedFieldType.termQuery to generate simple_query_string queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16643</link><project id="" key="" /><description>For Numeric types, if the query's text is passed to create a boolean
query, the 'Long' analyzer can return an exception similar to:

```
IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647
```

This change looks up the `MappedFieldType` for the specified fields (if
available) and uses the `.termQuery` function to create the query from
the string, instead of analyzing it by creating a new boolean query by
default.

Resolves #16577
</description><key id="133289060">16643</key><summary>Use MappedFieldType.termQuery to generate simple_query_string queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-02-12T16:49:53Z</created><updated>2016-02-17T15:50:54Z</updated><resolved>2016-02-16T02:53:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T16:57:55Z" id="183412064">LGTM. I had to do a similar thing to cross_field multi_match a month or so ago.
</comment><comment author="javanna" created="2016-02-12T16:59:54Z" id="183413024">LGTM
</comment><comment author="rjernst" created="2016-02-12T17:10:12Z" id="183416381">Hrm, this looks weird that we have to special case numerics. That's not what I meant in the related issue. I assume the single analyzer passed in is a per field analyzer wrapper. Why doesn't that handle string to field conversion (ie somewhere internally calling value on the field type, which is what the termQuery call added here does)?
</comment><comment author="dakrone" created="2016-02-12T17:44:52Z" id="183426505">@rjernst it _looks_ like NumericLongTokenizer does do this:

``` java
public class NumericLongTokenizer extends NumericTokenizer {

    public NumericLongTokenizer(int precisionStep, char[] buffer) throws IOException {
        super(new NumericTokenStream(precisionStep), buffer, null);
    }

    @Override
    protected void setValue(NumericTokenStream tokenStream, String value) {
        tokenStream.setLongValue(Long.parseLong(value)); // &lt;-- here
    }
}
```

I don't understand why .setValue is not used (actually there's a lot about this analysis chain that I don't understand). For instance, it is being wrapped by `CachingTokenFilter`, so it's opaque to me as to where the actual `NumericLongTokenizer` is actually fitting in to the calls:

```
Caused by: java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647
    at org.apache.lucene.util.NumericUtils.longToPrefixCodedBytes(NumericUtils.java:147)
    at org.apache.lucene.util.NumericUtils.longToPrefixCoded(NumericUtils.java:121)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.getBytesRef(NumericTokenStream.java:163)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:217)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:148)
    at org.apache.lucene.util.AttributeSource$State.clone(AttributeSource.java:55)
    at org.apache.lucene.util.AttributeSource.captureState(AttributeSource.java:288)
    at org.apache.lucene.analysis.CachingTokenFilter.fillCache(CachingTokenFilter.java:96)
    at org.apache.lucene.analysis.CachingTokenFilter.incrementToken(CachingTokenFilter.java:70)
    at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:223)
    at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
    at org.elasticsearch.index.query.SimpleQueryParser.newDefaultQuery(SimpleQueryParser.java:69)
    at org.apache.lucene.queryparser.simple.SimpleQueryParser.consumeToken(SimpleQueryParser.java:410)
    at org.apache.lucene.queryparser.simple.SimpleQueryParser.parseSubQuery(SimpleQueryParser.java:211)
    at org.apache.lucene.queryparser.simple.SimpleQueryParser.parse(SimpleQueryParser.java:151)
    at org.elasticsearch.index.query.SimpleQueryStringBuilder.doToQuery(SimpleQueryStringBuilder.java:287)
    at org.elasticsearch.index.query.AbstractQueryBuilder.toQuery(AbstractQueryBuilder.java:78)
    at org.elasticsearch.index.query.QueryShardContext.toQuery(QueryShardContext.java:429)
    at org.elasticsearch.index.query.QueryShardContext.toQuery(QueryShardContext.java:417)
    ... 14 more
```
</comment><comment author="dakrone" created="2016-02-12T18:33:38Z" id="183442957">I stepped through this in a debugger, it looks like the value "123" is correctly being passed to `.setValue` and parsed as 123, but the precision step is incorrectly parsed somewhere. :-/
</comment><comment author="dakrone" created="2016-02-12T22:20:49Z" id="183509868">Related Lucene bug: https://issues.apache.org/jira/browse/LUCENE-7027
</comment><comment author="uschindler" created="2016-02-13T17:19:28Z" id="183706040">Hi,
the bug in NumericTermAttribute (not NumericTokenStream) was fixed and will be part of Lucene 5.5.
I'd suggest to add the usual "assert" statement to the code, so once Lucene is updated, this patch can be reverted.
</comment><comment author="dakrone" created="2016-02-15T18:08:05Z" id="184329073">@rjernst okay, since this has been merged to the Lucene 5.5 branch, I propose just merging this PR to the 2.2 branch so it fixes the problem for 2.2.1. Sound good to you?
</comment><comment author="rjernst" created="2016-02-15T18:10:34Z" id="184330228">+1
</comment><comment author="dakrone" created="2016-02-16T02:53:06Z" id="184489347">the files were in slightly different places so I opened https://github.com/elastic/elasticsearch/pull/16686 for the 2.2 port, closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limitation of nested aggregations - Need ability to access owner fields from nested aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16642</link><project id="" key="" /><description>Currently it is impossible to access owner's fields from a nested aggregation. So it is not possible to do following:
- Bucket aggregations on them (a very common use case) as a sub aggregation of  the nested one. For example If multiple lawyers are assigned to cases of different types and each assignment captures his/her hours spent and report needs to be total hours by lawyer by case type. It is impossible to do so. Using reverse_nested and back will not produce desired results 
- Filter on owner's fields within aggregations. 
- Metrics aggregations on owner's fields are a little less useful as there will be double-counting but often are necessary as well
</description><key id="133284815">16642</key><summary>Limitation of nested aggregations - Need ability to access owner fields from nested aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Aggregations</label><label>discuss</label><label>feedback_needed</label></labels><created>2016-02-12T16:33:15Z</created><updated>2016-02-15T16:18:04Z</updated><resolved>2016-02-15T12:26:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T11:50:06Z" id="183650799">@roytmana could you please provide examples of what you are trying to achieve (and what can't be achieved by simply including the required parent field in the nested doc itself)?

I'm sure that you are aware that nested docs are separate docs from the parent, so accessing the parent from the nested doc would be prohibitively expensive.
</comment><comment author="roytmana" created="2016-02-14T16:08:45Z" id="183909196">Hello @clintongormley 

The example below is just a small example to demonstrate the issue. In real life I have a number of nested (sometimes twice) of fields in the main object and the main object has hundreds of properties. I cannot embed all the parent object's properties into nested child objects because it is an entirely dynamic system and users can craft any kind of query or aggregation on any combination of properties. It would be impractical to repeat all parent (owner) properties in the nested child not to mention that the JSON is not for searching and aggregation only it is heavily used by other systems and other reports.

Of course I understand that nested is a separate document and price of accessing parent may not be trivial but it is lot less expensive than in parent/child and in context of aggregation it could be optimized nicely if parent aggregation is on the owner object so it is already loaded and available within child aggregations. Also if trying to work around it with reverse nested aggregation incurs cost of fetching owner objects anyway (and does not work as you can see in example below).

I think allowing access to parent fields using something like ../parentProperty would be terrific feature in supporting cases that otherwise required messy, inefficient and often very much unwanted denormalization 

All the issues could be in some form worked around if you are hand-crafting a specific query and can modify your json on a dime (by bringing in needed parent properties into a nested field) just to satisfy your query. But in dynamic systems where queries are not known upfront and where JSON structure needs to be kept stable or versioned carefully it is just not possible 

Imagine having Teams that handle Requests of certain Priority. Each request may have multiple teams assigned to it and each team request assignment has number of hours allocated for this team on a request:

``` javascript
request:{
  priority:1,
  teams:[
    {name:'team1', hours:10},
    {name:'team2', hours:20},
  ]
}
```

Let's try to produce a report that gives number of hours by team by priority. Doing nested aggregation on **teams.name** and then reverse nested to group on **priority** and then nested to sum **teams.hours** double-counts hours because second nesting on teams knows nothing about  upstream nesting as it is executed in context of request and as result it will lump hours for each team on request under the top level team aggregation 

If i could access "../priority" from teams nested document context I would not have to use reverse nested and lose my aggregation context and everything would have worked like a charm

Similar case with filter aggregation - it is not possible to add any filtering logic on parent fields to it. While it is possible to put that logic into parent agg on the owner document it will only work with simple single purpose queries. Complex queries with multiple filters as sub aggs often have conflicting criteria in the filters (ex. each filter agg segments out a portion of docs based on parent's properties) forcing the aggregation to be split onto multiple queries and then re-aggregated  on the client. It could be non trivial if it is happening within some multi-bucket aggs

Thank you,
Alex
</comment><comment author="clintongormley" created="2016-02-15T12:26:16Z" id="184186158">This issue is pretty much a duplicate of #16642, so I'll close this one in favour of the other
</comment><comment author="roytmana" created="2016-02-15T16:16:03Z" id="184277648">@clintongormley you are sating "This issue is pretty much a duplicate of #16642, so I'll close this one in favour of the other" but this is the #16642 I think you have already closed the other one
</comment><comment author="clintongormley" created="2016-02-15T16:18:04Z" id="184278087">Sorry, I meant #16380
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Took time for index requests ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16641</link><project id="" key="" /><description>As of now, and AFAIK, the only way to know if a query exceeded a certain time threshold to process is enabling slow logs. Why does IndexResponse simply not include a took time ? 

We are sending indexing bulk requests to our cluster for real-time analytics and we would just want to know, if a bulk was slow to process, which one of the queries it included took a long time to execute immediately in our application, as it slows down our entire bulk request.
</description><key id="133276482">16641</key><summary>Took time for index requests ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rdigiorgio</reporter><labels><label>:CRUD</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-02-12T16:00:04Z</created><updated>2016-03-03T21:34:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T16:13:47Z" id="183391423">&gt; Why does IndexResponse simply not include a took time ?

I think it'd be safe to just add it. I'm marking this as something that should be easy for a new contributor because my gut says it should be.
</comment><comment author="rdigiorgio" created="2016-02-12T16:25:00Z" id="183396538">By the way, it also involves updating BulkItemResponse, which should include the took time too.
</comment><comment author="jasontedor" created="2016-03-03T21:34:35Z" id="191973523">&gt; I'm marking this as something that should be easy for a new contributor because my gut says it should be.

@nik9000 Something to keep in mind is that the hard part here is going to be writing tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove copy constructors from request classes and TransportMessage type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16640</link><project id="" key="" /><description>As a followup of #15776 we can remove all copy constructors that are left in our request classes, that were used to copy headers and context. Also, `TransportMessage` doesn't need a generic type anymore.
</description><key id="133273245">16640</key><summary>Remove copy constructors from request classes and TransportMessage type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T15:46:59Z</created><updated>2016-02-12T21:37:40Z</updated><resolved>2016-02-12T16:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T15:57:52Z" id="183385218">&gt; Also, TransportMessage doesn't need a generic type anymore.

Hurray!
</comment><comment author="nik9000" created="2016-02-12T16:01:30Z" id="183386203">Does removing the generic-ness just fix 123412421 raw type errors? I'm a bit surprised everything still compiles but ok.
</comment><comment author="nik9000" created="2016-02-12T16:02:24Z" id="183386456">LGTM I think.
</comment><comment author="s1monw" created="2016-02-12T21:37:40Z" id="183497173">cool thx @javanna 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>updates to Java API docs with the changes due to aggregator refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16639</link><project id="" key="" /><description /><key id="133269876">16639</key><summary>updates to Java API docs with the changes due to aggregator refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-12T15:32:27Z</created><updated>2016-02-15T10:35:16Z</updated><resolved>2016-02-15T10:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-12T21:15:52Z" id="183490494">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string errors as Caused by: java.lang.NumberFormatException: For input string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16638</link><project id="" key="" /><description>Hi Team,

I have got the following error when using simple_query_string with multiple fields containing numeric field.
Version: 2.2.0
OS: Windows

**Sample data:**
`curl -XPUT http://localhost:9200/blog/post/1 -d '{"id":123,"name":"name_123","relatedDocs":[{"id":12301,"name":"related1"},{"id":12302,"name":"related2"}]}'`

`curl -XPUT http://localhost:9200/blog/post/2 -d '{"id":124,"name":"name_124","relatedDocs":[{"id":12401,"name":"related2"},{"id":12402,"name":"related3"}]}'`

**Query String query:**
`curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query":{"query_string":{"default_operator":"AND","query":"12301 OR related2","fields":["relatedDocs.*","name"]}}}'`

**Error**
`{
    "error": {
        "root_cause": [
            {
                "type": "number_format_exception",
                "reason": "For input string: \"related2\""
            }
        ],
        "type": "search_phase_execution_exception",
        "reason": "all shards failed",
        "phase": "query",
        "grouped": true,
        "failed_shards": [
            {
                "shard": 0,
                "index": "blog",
                "node": "Ys1vmRrzT5SdSEYmfrEfYQ",
                "reason": {
                    "type": "number_format_exception",
                    "reason": "For input string: \"related2\""
                }
            }
        ]
    },
    "status": 400
}`
</description><key id="133263962">16638</key><summary>query_string errors as Caused by: java.lang.NumberFormatException: For input string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harishkannarao</reporter><labels /><created>2016-02-12T15:08:57Z</created><updated>2016-02-15T08:54:49Z</updated><resolved>2016-02-15T08:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T11:36:35Z" id="183648004">Hi @harishkannarao 

If you look at the [docs for the query string query](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/query-dsl-query-string-query.html), you'll find the `lenient` option.  Please ask questions like these on the forum instead: https://discuss.elastic.co/
</comment><comment author="harishkannarao" created="2016-02-15T08:54:49Z" id="184119225">Hi @clintongormley , Thanks for your inputs, setting lenient parameter has solved the issue. In future I will post my question as suggested before raising an issue. Cheers.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a text field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16637</link><project id="" key="" /><description>This new field is intended to replace analyzed string fields.
</description><key id="133254160">16637</key><summary>Add a text field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T14:23:40Z</created><updated>2016-02-15T09:45:56Z</updated><resolved>2016-02-15T09:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-12T19:44:57Z" id="183463343">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alternate to aggregationContext.ensureScoreDocsInOrder()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16636</link><project id="" key="" /><description> I am writing a custom Aggregation. For my requirements, I need the doc Ids to be received in ascending order in the overridden collect() method. To ensure this we were using we were using `aggregationContext.ensureScoreDocsInOrder()` which seems to have been was removed in ES 2.X. How can I ensure the same behavior in 2.x
</description><key id="133241266">16636</key><summary>Alternate to aggregationContext.ensureScoreDocsInOrder()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkalhans</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2016-02-12T13:26:00Z</created><updated>2016-02-14T22:28:32Z</updated><resolved>2016-02-14T22:28:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T11:32:40Z" id="183647389">@jpountz @colings86 any ideas?
</comment><comment author="jpountz" created="2016-02-14T22:28:32Z" id="183993191">Out of order collection has been removed in Lucene 5.0 / Elasticsearch 2.0. So you are now guaranteed to always collect documents in doc id order. See https://issues.apache.org/jira/browse/LUCENE-6179 for more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem __ToString() when populate, can help please? Is important</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16635</link><project id="" key="" /><description>php app/console fos:elastica:populate  --index=en --type=article --batch-size=100....

Populating en/article, 8.1% (377000/4626642), 507 objects/s (RAM : current=205Mo peak=206Mo)
Populating en/article, 8.2% (377100/4626642), 511 objects/s (RAM : current=205Mo peak=206Mo)
PHP Fatal error:  Method MongoDBODMProxies__CG__\Vavel\SearchBundle\Document\Article::__toString() must not throw an exception in /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Transformer/ModelToElasticaAutoTransformer.php on line 0
PHP Stack trace:
PHP   1. {main}() /usr/share/nginx/html/search/app/console:0
PHP   2. Symfony\Component\Console\Application-&gt;run() /usr/share/nginx/html/search/app/console:27
PHP   3. Symfony\Bundle\FrameworkBundle\Console\Application-&gt;doRun() /usr/share/nginx/html/search/vendor/symfony/symfony/src/Symfony/Component/Console/Application.php:124
PHP   4. Symfony\Component\Console\Application-&gt;doRun() /usr/share/nginx/html/search/vendor/symfony/symfony/src/Symfony/Bundle/FrameworkBundle/Console/Application.php:96
PHP   5. Symfony\Component\Console\Application-&gt;doRunCommand() /usr/share/nginx/html/search/vendor/symfony/symfony/src/Symfony/Component/Console/Application.php:193
PHP   6. Symfony\Component\Console\Command\Command-&gt;run() /usr/share/nginx/html/search/vendor/symfony/symfony/src/Symfony/Component/Console/Application.php:894
PHP   7. FOS\ElasticaBundle\Command\PopulateCommand-&gt;execute() /usr/share/nginx/html/search/vendor/symfony/symfony/src/Symfony/Component/Console/Command/Command.php:252
PHP   8. FOS\ElasticaBundle\Command\PopulateCommand-&gt;populateIndexType() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Command/PopulateCommand.php:89
PHP   9. FOS\ElasticaBundle\Doctrine\AbstractProvider-&gt;populate() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Command/PopulateCommand.php:154
PHP  10. Vavel\SearchBundle\Elastica\ObjectPersister-&gt;insertMany() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Doctrine/AbstractProvider.php:79
PHP  11. FOS\ElasticaBundle\Persister\ObjectPersister-&gt;transformToElasticaDocument() /usr/share/nginx/html/search/src/Vavel/SearchBundle/Elastica/ObjectPersister.php:23
PHP  12. FOS\ElasticaBundle\Transformer\ModelToElasticaAutoTransformer-&gt;transform() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Persister/ObjectPersister.php:191
PHP  13. FOS\ElasticaBundle\Transformer\ModelToElasticaAutoTransformer-&gt;normalizeValue() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Transformer/ModelToElasticaAutoTransformer.php:92
PHP  14. FOS\ElasticaBundle\Transformer\ModelToElasticaAutoTransformer-&gt;FOS\ElasticaBundle\Transformer{closure}() /usr/share/nginx/html/search/vendor/friendsofsymfony/elastica-bundle/Transformer/ModelToElasticaAutoTransformer.php:147
</description><key id="133233311">16635</key><summary>Problem __ToString() when populate, can help please? Is important</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avito22</reporter><labels /><created>2016-02-12T12:51:40Z</created><updated>2016-02-12T12:58:37Z</updated><resolved>2016-02-12T12:58:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-12T12:58:01Z" id="183315354">Please ask on discuss.elastic.co.

Sounds like a PHP error here. May be you should report your problem at https://github.com/FriendsOfSymfony/FOSElasticaBundle ?
</comment><comment author="avito22" created="2016-02-12T12:58:37Z" id="183315453">Done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DuelFieldDataTests.testDuelGeoPoints sometimes fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16634</link><project id="" key="" /><description>This failure reproduces consistently on master with the seed: `-Dtests.seed=AD46AA1C13DCCBCC`

The stack trace is:

```
java.lang.NumberFormatException: Invalid shift value (-32) in prefixCoded bytes (is encoded value really an INT?)
    at __randomizedtesting.SeedInfo.seed([AD46AA1C13DCCBCC:7920A55097BF4789]:0)
    at org.apache.lucene.util.NumericUtils.getPrefixCodedLongShift(NumericUtils.java:200)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$1.accept(OrdinalsBuilder.java:435)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:478)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexGeoPointFieldData$GeoPointTermsEnum.next(AbstractIndexGeoPointFieldData.java:55)
    at org.elasticsearch.index.fielddata.plain.GeoPointArrayIndexFieldData.loadFieldData22(GeoPointArrayIndexFieldData.java:100)
    at org.elasticsearch.index.fielddata.plain.GeoPointArrayIndexFieldData.loadDirect(GeoPointArrayIndexFieldData.java:82)
    at org.elasticsearch.index.fielddata.plain.GeoPointArrayIndexFieldData.loadDirect(GeoPointArrayIndexFieldData.java:1)
    at org.elasticsearch.index.fielddata.DuelFieldDataTests.duelFieldDataGeoPoint(DuelFieldDataTests.java:564)
    at org.elasticsearch.index.fielddata.DuelFieldDataTests.testDuelGeoPoints(DuelFieldDataTests.java:449)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)


```

@mikemccand @nknize Maybe this is something to do with the new GeoPoint implementation?
</description><key id="133227389">16634</key><summary>DuelFieldDataTests.testDuelGeoPoints sometimes fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Fielddata</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T12:18:43Z</created><updated>2016-03-18T13:24:56Z</updated><resolved>2016-02-12T16:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-02-12T14:24:47Z" id="183350902">Hmm this looks quite bad: it looks likely we indexed with PREFIX encoding but tried to search using the legacy NUMERIC?  Something is wrong w/ the back compat layer @nknize?
</comment><comment author="s1monw" created="2016-02-12T14:25:53Z" id="183351187">@colings86 can we mute the test?
</comment><comment author="colings86" created="2016-02-12T14:32:59Z" id="183353496">Sure. Will do

On Friday, 12 February 2016, Simon Willnauer notifications@github.com
wrote:

&gt; @colings86 https://github.com/colings86 can we mute the test?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16634#issuecomment-183351187
&gt; .
</comment><comment author="colings86" created="2016-02-12T15:42:56Z" id="183378733">test has been muted in https://github.com/elastic/elasticsearch/commit/83885423a0577c83e4ca7da922547d212b0b5fd7
</comment><comment author="nknize" created="2016-02-12T16:14:44Z" id="183391856">Pushing fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support http_proxy environment variable for the plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16633</link><project id="" key="" /><description>As previously discussed in #11001, I think we should support `http_proxy` system variable which seems pretty standard in Linux world.

Should not be hard to implement in the plugin manager.

Unsure if we need that elsewhere though.
</description><key id="133223653">16633</key><summary>Support http_proxy environment variable for the plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-12T12:01:48Z</created><updated>2016-05-15T15:47:33Z</updated><resolved>2016-02-20T22:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-12T21:09:39Z" id="183489149">I don't think the http_proxy environment variable is typical for java applications. Instead, you would add `-Dhttp.proxyHost` and `-Dhttp.proxyPort` to the java command (which can be done by setting JAVA_OPTS env var). Unfortunately this does not support authenticated proxies out of the box, but it looks fairly easy for us to add this by setting the default authenticator in eg the plugin cli main when we see `-Dhttp.proxyUser` and `-Dhttp.proxyPassword`.
</comment><comment author="clintongormley" created="2016-02-13T12:00:19Z" id="183654763">&gt; I don't think the http_proxy environment variable is typical for java applications. 

But it is typical for most linux applications.  Any downside to supporting it?
</comment><comment author="rjernst" created="2016-02-13T19:10:07Z" id="183728117">Just the complexity. We would need to parse the env var in our bash scripts, and split it into those sysprops. Having users specify proxies with the sysprops seems a much simpler and less error prone path. 
</comment><comment author="hulu1522" created="2016-02-13T19:47:34Z" id="183737580">@rjernst Will this help?

``` bash
#Automaticly import system proxy settings
if [ -n "$http_proxy" ] ; then
    echo $http_proxy | grep "@"
    if [ $? -eq 0 ]; then # If variable has username and password, its parse method different
        PROXY_HOST=$(echo $http_proxy | sed 's/http:\/\/.*@\(.*\):.*/\1/')
        PROXY_PORT=$(echo $http_proxy | sed 's/http:\/\/.*@.*:\(.*\)/\1/' | tr -d "/")
        USERNAME=$(echo $http_proxy | sed 's/http:\/\/\(.*\)@.*/\1/'|awk -F: '{print $1}')
        PASSWORD=$(echo $http_proxy | sed 's/http:\/\/\(.*\)@.*/\1/'|awk -F: '{print $2}')
    else # If it doesn't have username and password, its parse method this
        PROXY_HOST=$(echo $http_proxy | sed 's/http:\/\/\(.*\):.*/\1/')
        PROXY_PORT=$(echo $http_proxy | sed 's/http:\/\/.*:\(.*\)/\1/' | tr -d "/")
    fi
fi
```
</comment><comment author="hulu1522" created="2016-02-13T19:53:36Z" id="183741149">Also... is there a change of supporting a 'quiet' (`-y, --yes`) flag which will allow us to answer yes to the plugin installation?  With Elasticsearch 2.2.0 and the Java security updates, plugins are asking me to agree before finishing installation.  This does not play nice with provisioning applications like Chef or Puppet and causes them to hang and timeout.
</comment><comment author="jasontedor" created="2016-02-13T19:54:01Z" id="183741518">&gt; Having users specify proxies with the sysprops seems a much simpler and less error prone path.

I'm with @rjernst here, we do not need to be managing this in our startup script when `http_proxy` and related environment variables are _not_ standard in the Java world.
</comment><comment author="rjernst" created="2016-02-13T19:57:57Z" id="183743880">&gt; Also... is there a change of supporting a 'quiet' (-y, --yes) flag which will allow us to answer yes to the plugin installation?

There is a "batch" mode which skips confirmation that is enabled by default if there is no console (ie should work automatically from automation tools). If you want to force it, you can use the `-b` flag.
</comment><comment author="jasontedor" created="2016-02-13T20:06:49Z" id="183746162">I think that the regexes [provided](https://github.com/elastic/elasticsearch/issues/16633#issuecomment-183737580) prove the point. But that's not even the beginning of it as there are other environment variables at play here such as `HTTPS_PROXY`, `ALL_PROXY`, and `NO_PROXY`. At the end of this we will just have added a lot of complexity to our startup script that is _much_ easier for the end-user to manage.
</comment><comment author="clintongormley" created="2016-02-13T20:39:50Z" id="183751496">Point taken :)
</comment><comment author="jasontedor" created="2016-02-20T22:09:19Z" id="186691117">I closed this as I think we have consensus that we should not do this; the end-user can manage using the Java-specific properties. Please feel free to leave a comment if you have a good argument for why we should support this.
</comment><comment author="rjernst" created="2016-02-20T22:27:37Z" id="186692874">We still need an issue to track adding support for basic auth, so we could reopen this for that purpose, or have another describing specifically what we need there in plugin cli. 
</comment><comment author="muelli" created="2016-02-22T07:59:04Z" id="187061124">FWIW: It may help to set `java.net.useSystemProxies` to true. At least that's what both [Oracle](https://docs.oracle.com/javase/7/docs/api/java/net/doc-files/net-properties.html) and [OpenJDK](https://github.com/srisatish/openjdk/blob/6ffc2d1/jdk/src/share/lib/net.properties#L10-L18) advertise.
</comment><comment author="hulu1522" created="2016-02-22T19:01:05Z" id="187321278">@muelli +1

That is a lot easier to implement and very little maintenance of the code.  Essentially:

``` bash
if [ -n "$http_proxy" ] ; then
    $JAVA_OPTS = $JAVA_OPTS + " -Djava.net.useSystemProxies=true"
fi
```
</comment><comment author="jasontedor" created="2016-02-22T20:19:25Z" id="187366982">@muelli: That property only applies on Windows and Gnome desktop systems. The same answer given before applies here too: the user can manage these proxy settings as they need for their specific system and proxy setup.
</comment><comment author="clintongormley" created="2016-03-10T11:16:15Z" id="194795877">&gt; We still need an issue to track adding support for basic auth, so we could reopen this for that purpose, or have another describing specifically what we need there in plugin cli.

@rjernst I've opened https://github.com/elastic/elasticsearch/issues/17046
</comment><comment author="c33s" created="2016-05-15T15:47:33Z" id="219293375">it should be implemented simply because of the DX/UX (developer/user experience), simply because it then works out of the box and i don't have to get it running.

it is a better choice to solve this one time in the app by a few people, than having every proxy user ran into this problem and made him google and solve this problem for only himself.

even a lot of my windows app (even java apps) respect my `http_proxy` env variable.

please add support for this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>removes a lot of warnings from the aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16632</link><project id="" key="" /><description /><key id="133214701">16632</key><summary>removes a lot of warnings from the aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label></labels><created>2016-02-12T11:17:57Z</created><updated>2016-02-15T10:34:00Z</updated><resolved>2016-02-15T10:33:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T13:41:54Z" id="183334022">I like it though I don't know enough about the refactoring to really review it fully! Thanks though!
</comment><comment author="jpountz" created="2016-02-12T21:13:54Z" id="183490084">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Evaluate the replacement of the scroll API with search_after</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16631</link><project id="" key="" /><description>The scroll API is widely used for a lot of use cases (Map/Reduce, reindex, delete by query, ...). 
The performance is great but there are some limitations that IMO are difficult to handle: 
- The scroll API is using a `snapshot` of the index, if the scroll takes a long time a lot of resources are consumed (and kept) just for the scroll.
- The scroll is always bounded to one replica per shard, this means that there is no way to spread the load among the available replicas for one shard. 

Now that we added the support for `search_after` (https://github.com/elastic/elasticsearch/issues/8192) we could evaluate the cost of using this feature instead of the scroll. 
There is no context attached to a `search_after` request, this means that we can use the newest searcher on each sub-query but also that each sub-query could run on any available replicas. 
The downside of this approach is the need to use a tiebreaker for the sort (the scroll API is using the internal Lucene docid for this). The best tiebreaker we could think of is the _uid because we are sure that it has a unique value per document. Though the cost of sorting on _uid is big (see https://github.com/elastic/elasticsearch/issues/11887).
IMO I don't think that we should use docvalues but  rather rely on the fielddata because it can be used lazily (we pay the cost of having the _uid in fieldata but only when there is a scroll request running and we can easily remove it when we are done).   
</description><key id="133203140">16631</key><summary>Evaluate the replacement of the scroll API with search_after</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Scroll</label><label>discuss</label></labels><created>2016-02-12T10:23:46Z</created><updated>2016-02-12T15:17:03Z</updated><resolved>2016-02-12T15:17:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T13:22:37Z" id="183326218">&gt; The scroll API is using a snapshot of the index, if the scroll takes a long time a lot of resources are consumed (and kept) just for the scroll.

Losing the "snapshot" behavior in reindex would certainly be a change. It'd be really nice not to have to worry about the snapshot disappearing out from under you though.
</comment><comment author="bleskes" created="2016-02-12T13:44:14Z" id="183335097">There are some more things to consider here:
- currently we guarantee a point in time snapshot, which also means we never return the same document twice. For arbitrary sort orders, you can even drop a document. That's something we can stop supporting, but at I don't see how we can do it without using lucene's snapshots. 
- the biggest user of the scroll API is the scan search type - there we by design have no order but the one on disk. If we change our merge policies to honor some ordering we could change that, but for now I would expect a big penalty hit to the scan search if we sort as well.
</comment><comment author="javanna" created="2016-02-12T14:03:55Z" id="183343964">Another aspect is in terms of usability, the snapshot makes sure that no changes will be returned in case indexing is happening while scrolling. Even sorting by `_uid` depending on the id of documents and where are with the scroll, new documents might be returned or not. Maybe that turns out not to be a big deal, but seems like a big change at first glance.
</comment><comment author="jimczi" created="2016-02-12T15:17:01Z" id="183369868">&gt; the biggest user of the scroll API is the scan search type - there we by design have no order but the one on disk. If we change our merge policies to honor some ordering we could change that, but for now I would expect a big penalty hit to the scan search if we sort as well.

Yes and it's amplified now that the scroll is using a MinDocQuery and terminateAfter when sorting on Lucene docids only:
https://github.com/elastic/elasticsearch/pull/12983
So yeah speed matters and `search_after` is not ready for this kind of speed. I ran some tests and the difference can be huge. 
I completely missed the MinDocQuery thing which is brilliant and hard to beat.
I'll close it until we find a way to achieve the same speed without snapshot ;).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can not run Tika StandAlone runner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16630</link><project id="" key="" /><description>In mapper-attachments plugin, I created originally a [Stand Alone command line tool](https://github.com/elastic/elasticsearch/blob/master/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java) which helps me to extract data from binary files.
It helps to understand what Tika actually extracts before sending the text to elasticsearch. Basically it helps to debug when users report issues.

In master branch, when I run the StandAlone tool (which is a `main()` - not a test) from IntelliJ, I'm getting this error:

```
Exception in thread "main" java.lang.IllegalStateException: running tests but failed to invoke RandomizedContext#getRandom
    at org.elasticsearch.common.Randomness.get(Randomness.java:105)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.randomNodeName(InternalSettingsPreparer.java:194)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.finalizeSettings(InternalSettingsPreparer.java:167)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:106)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:100)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:91)
    at org.elasticsearch.mapper.attachments.StandaloneRunner.&lt;init&gt;(StandaloneRunner.java:171)
    at org.elasticsearch.mapper.attachments.StandaloneRunner.main(StandaloneRunner.java:176)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.elasticsearch.common.Randomness.get(Randomness.java:101)
    ... 12 more
Caused by: java.lang.IllegalStateException: No context information for thread: Thread[id=1, name=main, state=RUNNABLE, group=main]. Is this thread running under a class com.carrotsearch.randomizedtesting.RandomizedRunner runner context? Add @RunWith(class com.carrotsearch.randomizedtesting.RandomizedRunner.class) to your test class. Make sure your code accesses random contexts within @BeforeClass and @AfterClass boundary (for example, static test class initializers are not permitted to access random contexts).
    at com.carrotsearch.randomizedtesting.RandomizedContext.context(RandomizedContext.java:244)
    at com.carrotsearch.randomizedtesting.RandomizedContext.current(RandomizedContext.java:151)
    ... 17 more
```

I'm not expecting anything from the test framework here so I'm a bit surprise about the result.

I know that a `main` should not be in our `src/test` dir but I don't really want to commit this class in `src/main`.

Should I simply remove that class because we can't use it anymore?
Knowing that this plugin will probably become deprecated by the node-ingest one...
</description><key id="133202588">16630</key><summary>Can not run Tika StandAlone runner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>test</label></labels><created>2016-02-12T10:19:56Z</created><updated>2016-02-13T14:20:29Z</updated><resolved>2016-02-13T14:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-12T10:20:07Z" id="183262372">@rjernst WDYT?
</comment><comment author="clintongormley" created="2016-02-13T13:46:52Z" id="183667751">Given that we're deprecating the mapper-attachment plugin in https://github.com/elastic/elasticsearch/issues/16650 I'd say this can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add filtering support within Setting class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16629</link><project id="" key="" /><description>Now we have a nice Setting infra, we can define in Setting class if a setting should be filtered or not.
So when we register a setting, setting filtering would be automatically done.

Instead of writing:

``` java
Setting&lt;String&gt; KEY_SETTING = Setting.simpleString("cloud.aws.access_key", false, Setting.Scope.CLUSTER);
settingsModule.registerSetting(AwsEc2Service.KEY_SETTING, false);
settingsModule.registerSettingsFilterIfMissing(AwsEc2Service.KEY_SETTING.getKey());
```

We could simply write:

``` java
Setting&lt;String&gt; KEY_SETTING = Setting.simpleString("cloud.aws.access_key", false, Setting.Scope.CLUSTER, true);
settingsModule.registerSettingsFilterIfMissing(AwsEc2Service.KEY_SETTING.getKey());
```

It also removes `settingsModule.registerSettingsFilterIfMissing` method.

The plan would be to remove as well `settingsModule.registerSettingsFilter` method but it still used with wildcards. For example in Azure Repository plugin:

``` java
module.registerSettingsFilter(AzureStorageService.Storage.PREFIX + "*.account");
module.registerSettingsFilter(AzureStorageService.Storage.PREFIX + "*.key");
```

Also this PR changes Setting signature and uses now for future needs a varargs of `Property`. 
Properties can be:
- Filtered
- Dynamic
- NodeScope,
- IndexScope
- Deprecated

Closes #16598.
</description><key id="133195532">16629</key><summary>Add filtering support within Setting class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T09:37:24Z</created><updated>2016-03-13T18:13:13Z</updated><resolved>2016-03-13T18:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-12T09:37:59Z" id="183252732">@s1monw Could you look at this?
</comment><comment author="dadoonet" created="2016-02-22T20:34:53Z" id="187371850">@s1monw Do you think you could review it?
</comment><comment author="s1monw" created="2016-02-24T22:07:51Z" id="188473350">initially I was confused about your example but I think I like where this is going. I wonder since I will have to add yet another boolean here in the near future if we should make it a little more generic. For instance we can add an enum like this:

``` Java
public enum SettingsProperty {
  Filtered,
  Dynamic,
  ClusterScope,
  NodeScope,
  IndexScope,
  HereGoesYours;
}
```

and then the siguature can just have a varArgs like this `, SettingsProperty... properties` on all methods and existing code stays like it is? Then we can internally hold an enum set and reuse it on several levels. This would also allow us to remove the Scope entirely. As a default we can just use `EnumSet.of(NodeScope)`
</comment><comment author="dadoonet" created="2016-02-27T23:44:43Z" id="189747980">@s1monw Thanks for your comment. I started doing this locally and it's really a nice idea! I think I'll finish that on monday and will push my changes on my branch. Stay tuned!
</comment><comment author="dadoonet" created="2016-02-28T10:26:56Z" id="189836777">@s1monw I added new commits:
- Add SettingsProperty enumSet and use it for Scopes and Filtered settings
- Replace boolean `dynamic` with `SettingsProperty.Dynamic` property
- Add a check for Scopes: we don't want more than one Scope on a given property

Note: for now, I think we are not using the `NodeScope`. I guess I should at least use it for discovery/repository plugins for cloud client settings which are defined within a node.
Also, I think that as we are not using `NodeScope`, may be a better `default` could be `ClusterScope`?
It would help to simplify a lot of settings like:

``` java
Setting&lt;String&gt; setting = Setting.simpleString("foo.bar", SettingsProperty.ClusterScope);
```

 with 

``` java
Setting&lt;String&gt; setting = Setting.simpleString("foo.bar");
```

Let me know what you think.

Of course, I'll need to rebase on master to take into account new settings or changed which has been made in the meantime.
</comment><comment author="dadoonet" created="2016-03-01T10:30:25Z" id="190653222">@s1monw Do you prefer I rebase first on master before you review it?
</comment><comment author="dadoonet" created="2016-03-02T09:14:29Z" id="191145144">@s1monw I merged with master and added some new commits. Let me know.

(I think I should rename the PR as it's more than adding settings filtering :) )
</comment><comment author="s1monw" created="2016-03-03T10:02:14Z" id="191690255">@dadoonet this looks very good thanks! I left some suggestions
</comment><comment author="dadoonet" created="2016-03-04T16:10:47Z" id="192337722">@s1monw I pushed some new commits. LMK what you think.
</comment><comment author="s1monw" created="2016-03-04T16:49:05Z" id="192353423">I replied to your comments
</comment><comment author="dadoonet" created="2016-03-04T20:44:47Z" id="192460872">I added a new commit.
</comment><comment author="s1monw" created="2016-03-13T13:22:35Z" id="195956703">@dadoonet left two minors, no further review is needed please bring it up-to-date and push/merge once they are fixed ie. LGTM otherwise
</comment><comment author="dadoonet" created="2016-03-13T18:05:05Z" id="196013679">Fantastic! Thanks for the review.

I ran the full test suite with `gradle check` and finally everything seems to be good:

```
BUILD SUCCESSFUL

Total time: 51 mins 9.109 secs
```

Will merge soonish.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing failure when using bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16628</link><project id="" key="" /><description>Experienced an error while indexing content from drupal to elasticsearch installed on same VM. Noticed two things from elasticsearch logs below:
- failed to parse, document is empty.
- 'defaiya_articles_alias' alias which i think it was created automatically on first bulk operation occured.

Am using Elasticsearch 1.7.4 along drupal module elasticsearch_connector-7.x-1.0-alpha9

This happens when posting with Bulk endpoint
localhost:9200/defaiya_articles/defaiya_articles/_bulk?refresh=1

I have no clue about whats happening but i know am missing some configuration on elasticsearch. That because drupal module was working fine with a trial service from found.elasic.co.
### Elasticsearch log

`[2016-02-12 09:05:37,968][DEBUG][action.index             ] [Lucifer] [defaiya_articles][0], node[jDATrKcOQc2i0x7FmkSRNA], [P], s[STARTED]: Failed to execute [index {[defaiya_articles_alias][defaiya_articles][AVLUTKGwN3desrFRGnhC], source[_na_]}]
org.elasticsearch.index.mapper.MapperParsingException: failed to parse, document is empty
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:562)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:465)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:201)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:574)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
`
### Index information

`{
  "defaiya_articles" : {
    "aliases" : {
      "defaiya_articles_alias" : { }
    },
    "mappings" : {
      "defaiya_articles" : {
        "properties" : {
          "articledate" : {
            "type" : "date",
            "format" : "yyyy-MM-dd HH:mm:ss"
          },
          "body:value" : {
            "type" : "string"
          },
          "id" : {
            "type" : "integer",
            "include_in_all" : false
          },
          "search_api_language" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "title" : {
            "type" : "string"
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1454879612879",
        "uuid" : "b5ljdnseQc2ZFDGLliYTLA",
        "number_of_replicas" : "1",
        "number_of_shards" : "1",
        "version" : {
          "created" : "1070499"
        }
      }
    },
    "warmers" : { }
  }
}
`
</description><key id="133193991">16628</key><summary>Indexing failure when using bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fadi-assaad</reporter><labels /><created>2016-02-12T09:30:02Z</created><updated>2016-02-13T13:45:07Z</updated><resolved>2016-02-13T13:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T13:45:05Z" id="183667498">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log suppressed stack traces under DEBUG</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16627</link><project id="" key="" /><description>To make API's output more easy to read we are suppressing stack traces (#12991) unless explicitly requested by setting `error_trace=true` on the request. To compensate we are logging the stacktrace into the logs so people can look it up even the error_trace wasn't enabled. Currently we do so using the `INFO` level which can be verbose if an api is called repeatedly by some automation.  For example, if someone tries to read from an index that doesn't exist we will respond with a 404 exception and log under info every time. We should reduce the level to `DEBUG` as we do with other API driven errors (internal issues that are not related to the API should be logged by other components already, based on their severity)

Closes #15329
Closes #16622
</description><key id="133191663">16627</key><summary>Log suppressed stack traces under DEBUG</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.2.2</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T09:19:07Z</created><updated>2016-03-13T20:56:36Z</updated><resolved>2016-03-13T20:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-12T19:56:24Z" id="183467021">I wonder if it makes more sense to have a severity on ESExceptions and use that. The 404 exception can be debug or trace or whatever and unexpected ones can stay info or warn even. I dunno. I'm not picky though.
</comment><comment author="nik9000" created="2016-02-12T19:59:53Z" id="183468245">Actually, we already have a status to return. How about we log all 4xx statuses as debug and all 5xx statuses as info?
</comment><comment author="s1monw" created="2016-02-12T20:47:26Z" id="183483412">&gt; Actually, we already have a status to return. How about we log all 4xx statuses as debug and all 5xx statuses as info?

+1
</comment><comment author="bleskes" created="2016-02-13T01:00:52Z" id="183548977">&gt; Actually, we already have a status to return. How about we log all 4xx statuses as debug and all 5xx statuses as info?

I was thinking along the same line but didn't want to open the discussion (and I also see some downsides). It seems that using rest status occurred to more people. I'll do that.
</comment><comment author="nik9000" created="2016-02-13T01:03:08Z" id="183549212">&gt; I also see some downsides

Please post them when you can. I can't think of any but I certainly don't know all there is to know.
</comment><comment author="bleskes" created="2016-02-13T01:05:39Z" id="183549467">@nik9000 @s1monw I pushed an update
</comment><comment author="nik9000" created="2016-02-13T03:42:41Z" id="183580682">Fine by me.
</comment><comment author="bleskes" created="2016-02-15T14:41:55Z" id="184234661">I updated the PR log with `ERROR` for 5xx codes.

&gt; &gt; I also see some downsides
&gt; 
&gt; Please post them when you can. I can't think of any but I certainly don't know all there is to know.

There are high traffic APIs that can respond with 503  - hitting a node that can't join it's master. Indexing to a shard whose primary is having problems etc. This will spam the logs. I wanted to have an easy win so I think logging an error on 5xx (instead of the current info) and debug on other errors gives an improvement to what we doing now (info all the time). 
</comment><comment author="nik9000" created="2016-02-15T15:15:21Z" id="184249788">&gt; This will spam the logs. I wanted to have an easy win so I think logging an error on 5xx (instead of the current info) and debug on other errors gives an improvement to what we doing now (info all the time).

That makes sense. I think we should fix that but I don't know how really. On the other hand this change is obviously an improvement and we should get it. LGTM.
</comment><comment author="s1monw" created="2016-03-13T13:38:21Z" id="195959033">@bleskes wanna merge this? LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add additional fallback to http.publish_port and restrict fallback to transport.publish_port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16626</link><project id="" key="" /><description>PR #14535 fixed an issue where the port of a publish address would possibly not match the port of the corresponding bound address. This was implemented by matching publish to bound addresses and selecting the port of the bound address that matched. In case of http, we would fail if we did not find a match. For tcp transport, we would (similar to old behavior) not fail but just chose the port of the first bound address.

This behavior makes it difficult to properly set up network settings in case where publish address does not match any of the bound addresses, for example in case of Docker containers (see https://discuss.elastic.co/t/upgrading-to-es-2-2-0-doesnt-allow-node-to-have-different-bind-and-publish-addresses/41290 ).

I suggest the following change in fallback behavior:

In case where the publish address cannot be matched to any of the bound addresses and an explicit publish_port setting is not provided, the publish_port is now selected as the unique port of bound addresses. An exception is thrown in case of ambiguities. This applies to both tcp and http. The fallback now ensures that when the user has specified a concrete value for http.port (not a range), http.bind_port also falls back to that one. In case of transport.publish_port it is a bit more restrictive: Instead of chosing the port of the first bound address, we require that the port of bound addresses is unique.
</description><key id="133190891">16626</key><summary>Add additional fallback to http.publish_port and restrict fallback to transport.publish_port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T09:13:40Z</created><updated>2016-03-01T16:02:50Z</updated><resolved>2016-03-01T16:02:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-12T09:13:58Z" id="183244955">@rmuir can you have a look at this one?
</comment><comment author="rmuir" created="2016-03-01T12:13:55Z" id="190691118">I honestly think this is getting too complex. We should really open an issue to remove this publish host alltogether. 

If node B is talking to node A, instead of allowing node A to "lie about his phone number", node B should just look at the caller ID and not trust him at all. We are talking about TCP here, this is infallible.

This means just use https://docs.oracle.com/javase/7/docs/api/java/net/Socket.html#getRemoteSocketAddress%28%29

That's how every other networking app works and why we see so much confusion around this publish host.
</comment><comment author="bleskes" created="2016-03-01T12:14:18Z" id="190691287">I think the change makes sense. Left some comments. I would also like to have this clearly documented some where...
</comment><comment author="bleskes" created="2016-03-01T12:15:37Z" id="190691994">I share @rmuir sentiment that this is getting out of hand and agree we should re-evaluate long term. I don't think it should stop this fix though... 
</comment><comment author="ywelsch" created="2016-03-01T13:10:39Z" id="190717534">Rebased on current master and added two commits to address the comments. Can you have another look @bleskes ?
</comment><comment author="bleskes" created="2016-03-01T13:14:41Z" id="190719782">The change LGTM. Do we have a test for the NettyTransport case? (i.e. not http)
</comment><comment author="ywelsch" created="2016-03-01T13:29:47Z" id="190724803">@bleskes I've added one just now. Also randomized the port in the test.
</comment><comment author="bleskes" created="2016-03-01T13:34:34Z" id="190726119">Thanks @ywelsch. I do think we need to beef up testing. We need at least:

1) Multiple bound addresses
2) Sometimes they should all have different ports (and we resolve publish address to one of them and succeed or check that we fail o.w.)
3) Sometimes they should all have the same port and we should succeed both in the case we specify a publish port or not.
4) Make sure that when we do specific a publish port it is always used.

IMO these test don't even need to start the transport, I hope we can make the relevant methods static and pass what they need to them.
</comment><comment author="bleskes" created="2016-03-01T15:24:10Z" id="190769463">LGTM. Thanks for the tests!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write shard state metadata as soon as shard is created / initializing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16625</link><project id="" key="" /><description>As we now rely on active allocation ids persisted in the cluster state to select
the primary shard copy, we can write shard state metadata on the allocated node
as soon as the node knows about receiving this shard. This also ensures that
in case of primary relocation, when the relocation target is marked as started
by the master node, the shard state metadata with the correct allocation id has
already been written on the relocation target. Before this change, shard state
metadata was only written once the node knows it is marked as started. In case
of failures between master marking the node as started and the node
receiving and processing this event, the relation between the shard copy on disk
and the cluster state could get lost. This means that manual allocation of
the shard using the reroute command allocate_stale_primary was necessary.

Relates to #14739
</description><key id="133183768">16625</key><summary>Write shard state metadata as soon as shard is created / initializing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T08:30:13Z</created><updated>2016-02-29T13:33:08Z</updated><resolved>2016-02-29T13:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-22T18:19:35Z" id="187303541">@bleskes ping
</comment><comment author="bleskes" created="2016-02-29T09:59:13Z" id="190133909">change looks good to me. Left some suggestions and questions re testing..
</comment><comment author="ywelsch" created="2016-02-29T11:40:03Z" id="190171019">Pushed another commit addressing review comments. Also found a copy-paste bug in a test.
</comment><comment author="bleskes" created="2016-02-29T12:19:10Z" id="190189829">LGTM. Thanks @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Null pointer exception in Aggregation Code Path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16624</link><project id="" key="" /><description>In ValuesSourceAggregatorFactory.java note the following code 

```
  @Override
    public void doValidate() {
        if (config == null || !config.valid()) {
            resolveValuesSourceConfigFromAncestors(name, parent, config.valueSourceType());
        }
    }
```

When this method is called with config = null this throws null pointer exception. I am hitting this issue while writing a plugin where I am extending the Aggregator. 
</description><key id="133169317">16624</key><summary>Null pointer exception in Aggregation Code Path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkalhans</reporter><labels><label>:Aggregations</label><label>bug</label><label>discuss</label></labels><created>2016-02-12T06:43:42Z</created><updated>2016-03-25T14:52:46Z</updated><resolved>2016-03-25T14:52:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-22T15:42:41Z" id="199872677">@rkalhans The functionality for resolving the config of an aggregation from its ancestors has been removed in 5.0 so this should not be an issue from 5.0 onwards. The reason it has been removed is that it is error prone and trappy since it can be hard to tell which ancestor will provide the config for an aggregation.

As far as fixing this in 2.x I'm not sure there is a lot we could do short of throwing a better exception if `config` is `null`. The problem is that we need to know what type of value source to look for in an ancestor and we can only know that if the config is not `null`. All of the built in aggregations will not have a `null` config, in the case that a field is not mapped on a shard, the config object is still built but `config.valid()` returns `false`. Without knowing what you are trying to do it is hard to provide a workaround for you but `ValuesSourceAggregatorFactory.resolveConfig()` should ensure that the `config` object is created, this is usually called in the `init()` method.
</comment><comment author="clintongormley" created="2016-03-25T14:52:46Z" id="201320847">thanks @colings86 - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest plugins to Elasticsearch plugin docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16623</link><project id="" key="" /><description>Summary of changes:
- Added new container topic called ingest.asciidoc to group the ingest plugins together
- Added ingest.asciidoc to index.asciidoc
- Changed heading levels to get the TOC structure to work
- Removed the callout tag for the table because it was causing syntax errors. I'll need to find another way to show this because I don't think callouts are supported in asciidoc tables.

@debadair Please verify that I've added the content in the correct place. Just getting the content to build; I will edit the content later.
</description><key id="133139412">16623</key><summary>Add ingest plugins to Elasticsearch plugin docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dedemorton</reporter><labels><label>:Ingest</label><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-12T02:01:44Z</created><updated>2016-02-12T15:42:40Z</updated><resolved>2016-02-12T15:42:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-12T07:13:19Z" id="183210354">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Excessive stack trace logging in 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16622</link><project id="" key="" /><description>Every 404 on an unknown index in Elasticsearch 2.2 generates a 63 line stack trace in the main log by default. It's not very useful.

Other similarly minor log lines also print stack traces, but it usually seems to only be a few lines which isn't as big of a deal.

It seems rare an end user of Elasticsearch would ever want stack traces on anything but ERROR by default.
</description><key id="133127775">16622</key><summary>Excessive stack trace logging in 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schmichael</reporter><labels /><created>2016-02-12T00:38:02Z</created><updated>2016-02-12T20:31:51Z</updated><resolved>2016-02-12T20:31:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-12T01:48:59Z" id="183146076">Can you give an example of the 63 line stack trace you see?
</comment><comment author="schmichael" created="2016-02-12T02:01:04Z" id="183147936">```
2016/02/11 15:31:42.984000 [INFO] rest.suppressed           /testindex Params: {index=testindex}
[testindex] IndexNotFoundException[no such index]
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:586)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:133)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:77)
        at org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.checkBlock(TransportDeleteIndexAction.java:74)
        at org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.checkBlock(TransportDeleteIndexAction.java:41)
        at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.doStart(TransportMasterNodeAction.java:129)
        at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.start(TransportMasterNodeAction.java:121)
        at org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:93)
        at org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.doExecute(TransportDeleteIndexAction.java:69)
        at org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.doExecute(TransportDeleteIndexAction.java:41)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
        at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1187)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.delete(AbstractClient.java:1327)
        at org.elasticsearch.rest.action.admin.indices.delete.RestDeleteIndexAction.handleRequest(RestDeleteIndexAction.java:50)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:363)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="schmichael" created="2016-02-12T02:01:35Z" id="183148127">From:

```
curl -s localhost:9200/testindex # when testindex doesn't exist
```
</comment><comment author="s1monw" created="2016-02-12T08:23:19Z" id="183225544">@schmichael I kinda agree with you but it's a thin line to draw. Let say there is really something wrong how can I help anybody without a stacktrace? We had this soo often in the past that users reported something that was not reproducible and no trace? I wonder if we can make it smarter afterall but without a trace a failure is useless?
</comment><comment author="schmichael" created="2016-02-12T18:51:46Z" id="183448385">_tl;dr_ I understand the inclination that more data &gt; less data for debugging, so feel free to close this if you feel it's the right amount of context. Obviously you deal with far more support issues than me. :)

However, if you are willing to indulge some bike shedding: I can't imagine ever caring about the stack trace for an index, mapping, template, etc 404. Even in the most deep dive debugging I can't imagine more than like the top 10 frames of the stack being useful. This stack is 50% netty internals!

Is there a way to ellide stack traces on &lt;=INFO log lines in my log4j conf? That'd work for me at least.
</comment><comment author="nik9000" created="2016-02-12T19:15:14Z" id="183454889">&gt; Is there a way to elide stack traces on &lt;=INFO log lines in my log4j conf? That'd work for me at least.

You can silence the whole log line without restarting with the [update settings API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html). [This](https://www.elastic.co/blog/elasticsearch-logging-secrets) blog post explains it. You'd set the `rest.suppressed` logger to `WARN`.

I don't know of an easy way to elide the exception though.
</comment><comment author="bleskes" created="2016-02-12T19:50:49Z" id="183465180">@schmichael after our [mini conversation](https://twitter.com/schmichael/status/697940896499695617) on twitter I opened this https://github.com/elastic/elasticsearch/pull/16627  ... imho this should be debug. 
</comment><comment author="schmichael" created="2016-02-12T20:31:51Z" id="183478259">@bleskes lgtm; closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel 2.2.0 URL is not working </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16621</link><project id="" key="" /><description>I am new to ElasticSearch, I just installed ES 2.2.0 and it is working fine.
I am trying to get Marvel to work but without any luck.
I installed it like this:
bin/plugin install license
bin/plugin install marvel-agent 

but when I try to browse: http://localhost:5601/app/marvel
I get an error message : Unable to connect
as if there is nothing is running on this url

this happened in both Windows and Ubuntu , using v 2.2.0 for both ES and Marvel.

please advise.

Thanks
</description><key id="133125235">16621</key><summary>Marvel 2.2.0 URL is not working </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devheroo</reporter><labels /><created>2016-02-12T00:22:13Z</created><updated>2016-02-12T01:47:50Z</updated><resolved>2016-02-12T01:47:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="allenmchan" created="2016-02-12T00:33:21Z" id="183126208">Marvel is a kibana plugin now. You will need to install kibana and the marvel plugin in order to see the marvel stats. 
https://www.elastic.co/guide/en/marvel/current/installing-marvel.html
</comment><comment author="devheroo" created="2016-02-12T01:21:17Z" id="183141254">Thanks so much for your help.
I will look into kibana installation.
</comment><comment author="allenmchan" created="2016-02-12T01:44:47Z" id="183145298">Also please visit https://discuss.elastic.co/ for general help. There is a huge community there.
Github is for bugs and feature requests. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest docs to the build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16620</link><project id="" key="" /><description>Summary of changes:
- fixed a couple of build errors
- renamed ingest.asciidoc to ingest-node.asciidoc
- created new file called ingest.asciidoc to hold part intro (named to be consistent with other parts of the doc)
- added ingest.asciidoc to the main index.asciidoc
- changed heading levels in ingest-node.asciidoc to create proper structure for the TOC
</description><key id="133103817">16620</key><summary>Add ingest docs to the build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dedemorton</reporter><labels><label>:Ingest</label><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T22:26:03Z</created><updated>2016-02-12T15:33:07Z</updated><resolved>2016-02-12T15:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-12T07:10:38Z" id="183208504">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move reindex from a plugin to a module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16619</link><project id="" key="" /><description /><key id="133091785">16619</key><summary>Move reindex from a plugin to a module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label></labels><created>2016-02-11T21:27:54Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-11T22:40:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T21:28:04Z" id="183065719">@dakrone probably the last one for the day.
</comment><comment author="dakrone" created="2016-02-11T21:59:47Z" id="183076899">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex final docs review</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16618</link><project id="" key="" /><description>After the first PR for reindex we've been playing it a bit fast a loose with regards to documentation. This issue will force us to revisit them another time.
</description><key id="133083205">16618</key><summary>Reindex final docs review</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>docs</label></labels><created>2016-02-11T20:43:45Z</created><updated>2016-05-03T14:01:00Z</updated><resolved>2016-05-03T14:01:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T20:43:59Z" id="183053653">Another time when we are closer to releasing them, that is.
</comment><comment author="clintongormley" created="2016-05-03T14:01:00Z" id="216536775">Reindex docs are done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score Query: make parsing stricter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16617</link><project id="" key="" /><description>Function Score Query now checks the type of token that we are parsing, which makes parsing stricter and allows to throw useful errors in case the json is malformed. It also makes code more readable as in what gets parsed when.

Closes #16583
</description><key id="133037263">16617</key><summary>Function Score Query: make parsing stricter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T17:37:32Z</created><updated>2016-02-13T11:50:38Z</updated><resolved>2016-02-12T16:45:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-12T15:42:33Z" id="183378621">@javanna I left a couple comments, otherwise LGTM!
</comment><comment author="javanna" created="2016-02-12T16:20:34Z" id="183393890">@abeyad I pushed new commits ;)
</comment><comment author="abeyad" created="2016-02-12T16:25:17Z" id="183396747">@javanna LGTM! :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch with marvel-agent frequently crashed!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16616</link><project id="" key="" /><description>&amp;emsp;&amp;emsp;We have a experimental elasticsearch cluster. However, the master node of them frequently crashed! There is the log.

``` txt
[2016-02-11 06:40:09,188][INFO ][monitor.jvm              ] [node-1] [gc][young][53015][3954] duration [708ms], collections [1]/[1.4s], total [708ms]/[46.7s], memory [171mb]-&gt;[173.7mb]/[1015.6mb], all_pools {[young] [24.1mb]-&gt;[31.7mb]/[66.5mb]}{[survivor] [2.7mb]-&gt;[3.6mb]/[8.3mb]}{[old] [144.1mb]-&gt;[144.1mb]/[940.8mb]}
[2016-02-11 06:52:55,906][WARN ][monitor.jvm              ] [node-1] [gc][young][53118][3966] duration [7.8s], collections [2]/[1m], total [7.8s]/[56.8s], memory [197.1mb]-&gt;[197mb]/[1015.6mb], all_pools {[young] [48.9mb]-&gt;[46.5mb]/[66.5mb]}{[survivor] [5mb]-&gt;[993.4kb]/[8.3mb]}{[old] [145mb]-&gt;[149.8mb]/[940.8mb]}
[2016-02-11 07:54:37,060][ERROR][marvel.agent.collector.indices] [node-1] collector [{}] timed out when collecting data
[2016-02-11 08:23:44,857][ERROR][marvel.agent.collector.cluster] [node-1] collector [{}] timed out when collecting data
[2016-02-11 09:14:50,675][WARN ][monitor.jvm              ] [node-1] [gc][young][53121][3967] duration [3.2s], collections [1]/[2.3h], total [3.2s]/[1m], memory [203.8mb]-&gt;[187.9mb]/[1015.6mb], all_pools {[young] [53.6mb]-&gt;[36.6mb]/[66.5mb]}{[survivor] [993.4kb]-&gt;[1.7mb]/[8.3mb]}{[old] [149.8mb]-&gt;[149.8mb]/[940.8mb]}
```

&amp;emsp;&amp;emsp;There is no more useful message. Could anyone tell me what happened to my cluster?
</description><key id="133032697">16616</key><summary>Elasticsearch with marvel-agent frequently crashed!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peakkk</reporter><labels /><created>2016-02-11T17:15:23Z</created><updated>2016-02-13T13:20:13Z</updated><resolved>2016-02-13T13:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T13:20:13Z" id="183662921">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene 5.5.0-snapshot-850c6c2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16615</link><project id="" key="" /><description>This PR upgrades ES to latest lucene 5.5.0 RC snapshot. Major change includes refactoring `geo_point` type for 2.x to use newly available `PREFIX` encoding optimizations. 
</description><key id="133032342">16615</key><summary>Upgrade to lucene 5.5.0-snapshot-850c6c2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T17:14:04Z</created><updated>2016-02-11T20:32:00Z</updated><resolved>2016-02-11T20:32:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-02-11T18:37:39Z" id="183001327">Thanks @nknize ... this gives us a nice jump on the upcoming 5.5.0 release.

It looks like much of this was from renaming noise, but there are important back compat changes here (NUMERIC vs PREFIX encoding of the geo terms for ES 2.2.x vs ES 2.3.x indices) that are hard for me to check (out of my area).  Do we have back compat tests that have ES 2.2.x geo indices, confirming they still work correctly in ES 2.3.x?

Also, with this upgrade, since geopoint moved from sandbox to spatial module, can we remove ES's dependency on Lucene's sandbox?  Or does ES use sandbox APIs elsewhere?
</comment><comment author="nknize" created="2016-02-11T19:04:02Z" id="183011784">&gt; Do we have back compat tests that have ES 2.2.x geo indices, confirming they still work correctly in ES 2.3.x?

There aren't any explicit backcompat tests. This [commit](https://github.com/nknize/elasticsearch/commit/dc778157441fa3528609321d0820030dbaff2033), merged as part of PR #14667, added version randomization to the Geo unit tests to ensure backwards compatibility.

&gt; Since geopoint moved from sandbox to spatial module, can we remove ES's dependency on Lucene's sandbox?

Good question. For spatial we can. I'm not sure if anything else uses sandbox but that's easy enough to check. Should we track that in a separate issue?
</comment><comment author="nknize" created="2016-02-11T19:36:02Z" id="183027746">&gt;  can we remove ES's dependency on Lucene's sandbox?

Looks like we still require sandbox for `DocValuesTermsQuery`
</comment><comment author="mikemccand" created="2016-02-11T20:27:56Z" id="183049302">OK thanks @nknize, LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rework retries in reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16614</link><project id="" key="" /><description>We want 500ms for the first retry and about a minute of total retries.

Closes #16608
</description><key id="133031737">16614</key><summary>Rework retries in reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label></labels><created>2016-02-11T17:11:16Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-11T19:49:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T17:11:37Z" id="182962164">@bleskes I think this is what we talked about.
</comment><comment author="bleskes" created="2016-02-11T19:47:46Z" id="183033766">LGTM. Thanks @nik9000 
</comment><comment author="nik9000" created="2016-02-11T19:49:59Z" id="183034627">Merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Teach reindex to stop when cancelled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16613</link><project id="" key="" /><description>All we do is check the cancelled flag and stop the request at a few key
points.

Adds the cancellation cause to the status so any request that is cancelled
but doesn't die can be seen in the task list.
</description><key id="133025657">16613</key><summary>Teach reindex to stop when cancelled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label></labels><created>2016-02-11T16:47:17Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-11T20:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T16:47:36Z" id="182952959">@dakrone and @imotov this is for you to review I think.
</comment><comment author="nik9000" created="2016-02-11T19:44:12Z" id="183032085">@dakrone I did all the things!
</comment><comment author="dakrone" created="2016-02-11T20:13:53Z" id="183045424">LGTM, thanks Nik!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use 256m for bwc nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16612</link><project id="" key="" /><description>The default allows up to 1g which is a ton for jenkins machines.
</description><key id="133022084">16612</key><summary>Use 256m for bwc nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-11T16:34:50Z</created><updated>2016-02-11T17:41:16Z</updated><resolved>2016-02-11T17:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T16:35:21Z" id="182946176">I checked this on linux but I suspect it'll just work on Windows because it uses the OS agnostic part of the jvm.
</comment><comment author="martijnvg" created="2016-02-11T17:33:52Z" id="182968687">LGTM
</comment><comment author="nik9000" created="2016-02-11T17:41:15Z" id="182971363">Merged to 2.x branch and cherry-picked to 2.1, 2.2, and 2.3 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Has child query forces default similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16611</link><project id="" key="" /><description>@martijnvg, should already have a CLA signed from #4977.  Closes #16550.
</description><key id="133013892">16611</key><summary>Has child query forces default similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">gpstathis</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T16:01:42Z</created><updated>2016-03-09T10:39:28Z</updated><resolved>2016-02-15T21:20:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-11T20:09:57Z" id="183044369">@gpstathis Thanks! The change looks good. Can you squash the commits into a single commit? 
</comment><comment author="gpstathis" created="2016-02-12T04:04:57Z" id="183171957">@martijnvg done!
</comment><comment author="martijnvg" created="2016-02-15T21:19:57Z" id="184400953">@gpstathis I pushed this to master: https://github.com/elastic/elasticsearch/commit/b17a92c9110259fa0f3049374908f9d6b9150f1e

and I replaced the integration test with a unit test instead: https://github.com/elastic/elasticsearch/commit/3290cfbd31438aca27529514b320edd80687fdb1

When back porting to 2.x, your integration tests will guard that that the similarity is properly passed down the IndexSearcher.
</comment><comment author="gpstathis" created="2016-02-16T15:43:52Z" id="184738156">Thanks @martijnvg. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor IndicesRequestCache to make it testable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16610</link><project id="" key="" /><description>This commit moves IndicesRequestCache into o.e.indics and makes all API in this
class package private. All references to SearchReqeust, SearchContext etc. have been factored
out and relevant glue code has been added to IndicesService. The IndicesRequestCache is not a
simple class without any hard dependencies on ThreadPool nor SearchService or IndexShard. This now
allows to add unittests.
This commit also removes two settings `indices.requests.cache.clean_interval` and `indices.fielddata.cache.clean_interval`
in favor of `indices.cache.clean_interval` which cleans both caches.
</description><key id="133010923">16610</key><summary>Refactor IndicesRequestCache to make it testable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cache</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T15:52:54Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-02-12T08:27:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-11T15:53:41Z" id="182929231">@jpountz  can you review that please
</comment><comment author="jpountz" created="2016-02-11T16:31:29Z" id="182945036">The change looks good.

I'm not too sure about the merge of the clean interval setting. I would rather remove this setting entirely given that it makes little sense since those caches cannot become stale.
</comment><comment author="s1monw" created="2016-02-11T16:32:46Z" id="182945387">&gt; I'm not too sure about the merge of the clean interval setting. I would rather remove this setting entirely given that it makes little sense since those caches cannot become stale.

you mean just have it fixed to 1 min?
</comment><comment author="jpountz" created="2016-02-11T16:36:02Z" id="182946468">Oh sorry, this setting does not do what I thought it did. I thought it would remove all entries from the cache.

LGTM then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the MapperBuilders utility class.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16609</link><project id="" key="" /><description>We can just call constructors directly.
</description><key id="133003265">16609</key><summary>Remove the MapperBuilders utility class.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T15:25:40Z</created><updated>2016-02-11T16:36:25Z</updated><resolved>2016-02-11T16:36:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T15:32:22Z" id="182917971">LGTM

I didn't know this class was a thing! You are right that it doesn't seem useful.
</comment><comment author="rjernst" created="2016-02-11T15:45:34Z" id="182923701">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change reindex default retry time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16608</link><project id="" key="" /><description>Right now reindex's smallest retry time when it receives task rejections is 50ms and it does 10 retries for a total (exponential) backoff of about 25 seconds. @bleskes suggested changing the defaults to 500ms with the goal of about a minute total backoff. The reasoning is that 50ms is super duper fast and unlikely to be long enough for the node to become less congested. 500ms is better while still being pretty soon.
</description><key id="132998242">16608</key><summary>Change reindex default retry time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label></labels><created>2016-02-11T15:11:06Z</created><updated>2016-02-13T13:29:45Z</updated><resolved>2016-02-13T13:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T13:29:35Z" id="183665280">Closed by #16614
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with Dis Max Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16607</link><project id="" key="" /><description>..
</description><key id="132995429">16607</key><summary>Problem with Dis Max Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">WhiteTrashLord</reporter><labels /><created>2016-02-11T15:01:26Z</created><updated>2016-02-12T01:41:05Z</updated><resolved>2016-02-11T15:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-11T15:44:41Z" id="182923375">This looks like a question better asked in our forums at https://discuss.elastic.co. We'll help you better there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Similarity of field cannot be changed if index contains more than one type with same field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16606</link><project id="" key="" /><description>The [guide](https://www.elastic.co/guide/en/elasticsearch/guide/current/changing-similarities.html) states that if you want to change the similarity of a field you can close the index, update the mapping and then open it again. This is useful for example if someone wants to update parameters of BM25.

It does not seem to work currently if an index has more than one type which contains the field. The error message is `"Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different [similarity]]"`. Setting the `update_all_types` parameter does not help either.

Full reproduction below.

A workaround is to define a custom similarity for each field when the index is created and then later update the settings when the index is closed instead of the mapping. But I still think this is a bug?

```
PUT test
{
  "mappings": {
    "type1": {
      "properties": {
        "text": {
          "type": "string",
          "similarity": "BM25"
        }
      }
    },
    "type2": {
      "properties": {
        "text": {
          "type": "string",
          "similarity": "BM25"
        }
      }
    }
  }
} 

POST test/_close

POST test/_mapping/type1?update_all_types
{
  "properties": {
    "text": {
      "type": "string",
      "similarity": "default"
    }
  }
}

```
</description><key id="132973932">16606</key><summary>Similarity of field cannot be changed if index contains more than one type with same field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2016-02-11T13:31:45Z</created><updated>2016-02-11T18:55:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-02-11T18:55:38Z" id="183009320">forgot to mention that this is on 2.2 release @jpountz says this works on master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple recovery source/target logic and transport piping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16605</link><project id="" key="" /><description>The current logic for doing recovery from a source to a target shard is tightly coupled with the underlying network pipes. This changes decouple the two, making it easier to add unit tests for shard recovery that don't involve the node and network environment.

On top that, RecoveryTarget is renamed to RecoveryTargetService leaving space to renaming RecoveryStatus to RecoveryTarget (and thus avoid the confusion we have today with RecoveryState).

Correspondingly RecoverySource is renamed to RecoverySourceService.
</description><key id="132955084">16605</key><summary>Decouple recovery source/target logic and transport piping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>non-issue</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T12:05:45Z</created><updated>2016-02-11T20:46:56Z</updated><resolved>2016-02-11T20:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-11T12:05:57Z" id="182828411">@dakrone can you take a look? /cc @s1monw 
</comment><comment author="s1monw" created="2016-02-11T13:10:15Z" id="182857470">FUCK YEAH! this is awesome! thanks so much! I didn't review every detail since it's just structural so LGTM 
</comment><comment author="bleskes" created="2016-02-11T13:17:11Z" id="182858962">@mikemccand can you sanity check the move to a global (i.e. per recovery not per file) [byte counting](https://github.com/elastic/elasticsearch/pull/16605/files#diff-fba6bfac4311a6801e0df6116d86bdecR48) before going into the ratelimiter? 
</comment><comment author="dakrone" created="2016-02-11T16:06:32Z" id="182935882">Left a few really minor comments but other than that this looks great!

I think with the re-org we can remove a few files from the checkstyle suppression list also
</comment><comment author="mikemccand" created="2016-02-11T18:21:23Z" id="182989449">&gt; @mikemccand can you sanity check the move to a global (i.e. per recovery not per file) byte counting before going into the ratelimiter?

That part looks great to me!

It makes the rate limiter more accurate for recovery, because small files (below the "check every N bytes" limit) are now aggregated and counted, vs slipping under-the-radar today.
</comment><comment author="bleskes" created="2016-02-11T19:41:06Z" id="183030643">@dakrone I pushed another commit addressing your  feedback. thanks.

@s1monw can you look at the CancellabelThreads change as well?

&gt; That part looks great to me!

Thx @mikemccand 
</comment><comment author="s1monw" created="2016-02-11T20:35:07Z" id="183051122">LGTM
</comment><comment author="dakrone" created="2016-02-11T20:35:46Z" id="183051291">LGTM too, great cleanup Boaz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>document (pdf or doc) containing quotes are not well parsed or queried</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16604</link><project id="" key="" /><description>Hello, I've got a little problem. I'm indexing documents (PDF or Word) in french using the excellent plugin "mapper-attachment". We are using elastic-search version 2.7 and have installed the plugin corresponding to the right version (at least I hope)
The plugin has been installed like that: sudo /usr/share/elasticsearch/bin/plugin install elasticsearch/elasticsearch-mapper-attachments/2.7.0
Everything works fine except for words with quotes. In french we need to separate words using quotes for exemple " the attention " is translated as " l'attention ". When I index an attachment having the words " l'attention " and when I search for "attention", it doesn't match.
When I index a regular String using the same analyzer it works fine. " l'attention " matches with "attention". The french elision filter works fine on regular Strings but not on attachment.
I hope someone will be able to help me.
If needed I can provide you a testcase that shows exactly the problem.
</description><key id="132945545">16604</key><summary>document (pdf or doc) containing quotes are not well parsed or queried</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rodrigueLeopold</reporter><labels /><created>2016-02-11T11:16:02Z</created><updated>2016-02-11T12:08:17Z</updated><resolved>2016-02-11T12:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-11T12:08:17Z" id="182829509">Hi @rodrigueLeopold please join us on [discuss.elastic.co](https://discuss.elastic.co/) so we can discuss your problem and help you. We use github issues for actual bugs or feature requests only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move IndicesQueryCache and IndicesRequestCache into IndicesService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16603</link><project id="" key="" /><description>this is a minor cleanup that detaches `IndicesRequestCache` and `IndicesQueryCache`
from guice and moves it into `IndicesService`. It also decouples the `IndexShard` and `IndexService`
from these caches which are unnecessary dependencies.
</description><key id="132944164">16603</key><summary>Move IndicesQueryCache and IndicesRequestCache into IndicesService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T11:06:50Z</created><updated>2016-02-11T13:21:36Z</updated><resolved>2016-02-11T13:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-11T13:19:14Z" id="182859437">Looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate AWS settings to new settings infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16602</link><project id="" key="" /><description>Reintroducing commit fb7723c but now deals with setting names conflicts
Also adds java documentation for each setting

Closes #16293.
Related to https://github.com/elastic/elasticsearch/pull/16477#discussion_r52469084
</description><key id="132943698">16602</key><summary>Migrate AWS settings to new settings infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T11:04:19Z</created><updated>2016-04-11T13:18:19Z</updated><resolved>2016-02-11T18:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-11T11:04:55Z" id="182810988">@danielmitterdorfer I'm assigning it to you as you already reviewed the first PR. Sorry! :p 
</comment><comment author="danielmitterdorfer" created="2016-02-11T12:20:49Z" id="182834172">@dadoonet: I assumed in my review that the only thing that has changed between here and #16477 is the handling of duplicate settings. I've left two minor comments, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to prevent base64 conversions by using bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16601</link><project id="" key="" /><description>CBOR is natively supported in Elasticsearch and allows for byte arrays.
This means, that by using CBOR the user can prevent base64 conversions
for the data being sent back and forth.

This PR adds support to extract data from a byte array in addition to
a string.
</description><key id="132939167">16601</key><summary>Allow to prevent base64 conversions by using bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-02-11T10:42:54Z</created><updated>2016-04-11T16:40:52Z</updated><resolved>2016-04-11T12:14:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-11T11:12:18Z" id="182812384">left a minor comment LGTM otherwise
</comment><comment author="martijnvg" created="2016-02-11T13:11:07Z" id="182857746">LGTM
</comment><comment author="kimchy" created="2016-02-11T15:22:57Z" id="182913101">maybe we can have special support for `IngestDocument#getFieldValue(String path, byte[].class)`? then we can have that logic encapsulated in IngestDocument and used in other processors down the road?
</comment><comment author="kimchy" created="2016-02-13T20:10:43Z" id="183746418">btw, if we do that, we can only test `IngestDocument` in a unit test that it handles the `byte[]` case correctly, and thats it. No need I think to add an integration test (we shouldn't test CBOR support in ES, we already do that elsewhere) for example.
</comment><comment author="spinscale" created="2016-02-15T13:58:33Z" id="184218372">that makes sense. I added a `IngestDocument.getFieldValueAsBytes()` method, which is doing the base64 conversion, instead of doing this in the `AttachmentProcessor`.

Also removed the integration test
</comment><comment author="spinscale" created="2016-02-15T17:36:51Z" id="184319588">I just found out that using byte arrays and copying them breaks the equality check in the tests... the bytesreference workaround I had in mind is broken. Will revert this and come up with something more useful.
</comment><comment author="martijnvg" created="2016-02-22T18:47:22Z" id="187314495">LGTM
</comment><comment author="javanna" created="2016-02-22T18:53:10Z" id="187316831">LGTM as well
</comment><comment author="clintongormley" created="2016-03-10T18:33:21Z" id="194992921">@spinscale want to get this in?
</comment><comment author="spinscale" created="2016-03-14T09:13:40Z" id="196214896">this one still has breaking tests that need to be fixed first
</comment><comment author="spinscale" created="2016-04-08T12:28:41Z" id="207411948">@martijnvg I finally resurrected this one and fixed the outstanding tests. The main issue was parts of the code rely on equality, which requires a custom `equals` implementation for byte arrays. The same applies for the `ValueSource` interface

What I did here, was adding changing `equals` in `IngestDocument` so that byte arrays are converted to an `ByteArray` object that uses `Arrays.equals()` in its `equals` method. The other possibility would be to do this conversion in the ctor when the `IngestDocument` is instantiated, but I felt it makes more sense to defer only when `equals` is called. Maybe you have a better idea that I overlooked, happy to get any input.
</comment><comment author="martijnvg" created="2016-04-11T11:36:16Z" id="208300178">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ping_timeout is documented in ec2-discovery but does not exist in code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16600</link><project id="" key="" /><description>`ping_timeout` is documented at https://www.elastic.co/guide/en/elasticsearch/plugins/master/discovery-ec2-discovery.html#discovery-ec2-discovery
 but I can't find it in code.

It's either:
- a documentation issue and we forgot to remove it
- something we never supported
- some magic vodoo code which actually uses it
- something we did support but removed by mistake

I'm opening this issue to track this and don't forget about it.
</description><key id="132935377">16600</key><summary>ping_timeout is documented in ec2-discovery but does not exist in code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>docs</label></labels><created>2016-02-11T10:26:33Z</created><updated>2016-07-21T13:06:02Z</updated><resolved>2016-07-21T13:04:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add infrastructure to rewrite query builders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16599</link><project id="" key="" /><description>QueryBuilders today do all their heavy lifting in toQuery() which
can be too late for several operations. For instance if we want to fetch geo shapes
on the coordinating node we need to do all this before we create the actual lucene query
which happens on the shard itself. Also optimizations for request caching need to be done
to the query builder rather than the query which then in-turn needs to be serialized again.
This commit adds the basic infrastructure for query rewriting and moves the heavy lifting into
the rewrite method for the following queries:
- `WrapperQueryBuilder`
- `GeoShapeQueryBuilder`
- `TermsQueryBuilder`
- `TemplateQueryBuilder`

Other queries like `MoreLikeThisQueryBuilder` still need to be fixed / converted. The nice
sideeffect of this is that queries like template queries will now also match the request cache
if their non-template equivalent has been cached befoore. In the future this will allow to
add optimizataion like rewriting time-based queries into primitives like `match_all_docs` or `match_no_docs`
based on the currents shards bounds. This is especially appealing for indices that are read-only ie. never change.
</description><key id="132930226">16599</key><summary>Add infrastructure to rewrite query builders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T09:56:41Z</created><updated>2016-02-12T21:35:43Z</updated><resolved>2016-02-12T21:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-11T10:01:27Z" id="182788801">@colings86 @MaineC @cbuescher @javanna this would have been a damn disaster without all the tests you folks added for queries. Thanks so much for this, the reward for all the pain is huge since you enabled others to make massive progress in no time! Worth all the pain I hope!
</comment><comment author="colings86" created="2016-02-11T10:41:34Z" id="182801083">@s1monw I left a couple of comments but I really like this change :)
</comment><comment author="javanna" created="2016-02-11T12:01:53Z" id="182827300">I left a bunch of comments, mainly minors, but this looks very good, I like the change. One question: we are not moving yet to calling rewrite on the coordinating node, thus fetching shapes and terms is still on the data nodes, but is that the plan on the long run? Cause when we discussed that in the past, we weren't sure if it was a good idea to move the fetch to the coord node and then serialize fetched terms and shapes from coord node to the data nodes. Curious if anything has changed there.
</comment><comment author="s1monw" created="2016-02-12T11:54:42Z" id="183294164">@colings86 @javanna I pushed new commits addressing all comments.

&gt; I left a bunch of comments, mainly minors, but this looks very good, I like the change. One question: we are not moving yet to calling rewrite on the coordinating node, thus fetching shapes and terms is still on the data nodes, but is that the plan on the long run? Cause when we discussed that in the past, we weren't sure if it was a good idea to move the fetch to the coord node and then serialize fetched terms and shapes from coord node to the data nodes. Curious if anything has changed there.

I don't know it was just an example of how this can be used. I am more focusing on rewriting before stuff is added to the percolator etc. in the future.
</comment><comment author="javanna" created="2016-02-12T13:17:45Z" id="183325190">left 2 minors, LGTM otherwise!
</comment><comment author="s1monw" created="2016-02-12T14:17:48Z" id="183348652">@javanna pushed new commit
</comment><comment author="javanna" created="2016-02-12T14:27:18Z" id="183351767">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add filtering support within Setting class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16598</link><project id="" key="" /><description>Now we have a nice Setting infra, we can define in Setting class if a setting should be filtered or not.
So when we register a setting, setting filtering would be automatically done.

Instead of writing:

``` java
Setting&lt;String&gt; KEY_SETTING = Setting.simpleString("cloud.aws.access_key", false, Setting.Scope.CLUSTER);
settingsModule.registerSetting(AwsEc2Service.KEY_SETTING, false);
settingsModule.registerSettingsFilterIfMissing(AwsEc2Service.KEY_SETTING.getKey());
```

We could simply write:

``` java
Setting&lt;String&gt; KEY_SETTING = Setting.simpleString("cloud.aws.access_key", false, Setting.Scope.CLUSTER, true);
settingsModule.registerSettingsFilterIfMissing(AwsEc2Service.KEY_SETTING.getKey());
```

And then remove `settingsModule.registerSettingsFilterIfMissing` and `settingsModule.registerSettingsFilter` methods.
</description><key id="132926462">16598</key><summary>Add filtering support within Setting class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T09:35:01Z</created><updated>2016-03-13T18:06:30Z</updated><resolved>2016-03-13T18:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix typos in error messages and comments of dev_tools module.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16597</link><project id="" key="" /><description /><key id="132925343">16597</key><summary>Fix typos in error messages and comments of dev_tools module.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T09:28:15Z</created><updated>2016-02-26T18:53:27Z</updated><resolved>2016-02-11T14:28:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-11T14:28:44Z" id="182887943">Thanks again @dongjoon-hyun ! These are safe and helpful!
</comment><comment author="dongjoon-hyun" created="2016-02-11T18:03:07Z" id="182981353">Thank you again @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Potential NullPointerException in class org.elasticsearch.action.index.IndexRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16596</link><project id="" key="" /><description>The [line 649](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L649) is:

```
timestamp = MappingMetaData.Timestamp.parseStringTimestamp(defaultTimestamp, mappingMd.timestamp().dateTimeFormatter(), getVersion(metaData, concreteIndex));
```

Here the variable mappingMd can be `null`, so `mappingMd.timestamp()` can raise a NPE.
I don't know in which context this case can occur, but still there's a potential bug here.

Regards
</description><key id="132919821">16596</key><summary>Potential NullPointerException in class org.elasticsearch.action.index.IndexRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonbrandhof</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-02-11T08:52:18Z</created><updated>2016-02-15T00:28:46Z</updated><resolved>2016-02-14T19:42:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-14T19:41:13Z" id="183963276">First, I think that you meant to refer to [line 629](https://github.com/elastic/elasticsearch/blob/cca611171c53254b74f2da7239e05177fec92b1d/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L629).

More importantly, there's no issue here but it's not straightforward why. Here's the relevant block of code.

``` java
617  String defaultTimestamp = TimestampFieldMapper.Defaults.DEFAULT_TIMESTAMP;
618  if (mappingMd != null &amp;&amp; mappingMd.timestamp() != null) {
619    // If we explicitly ask to reject null timestamp
620    if (mappingMd.timestamp().ignoreMissing() != null &amp;&amp; mappingMd.timestamp().ignoreMissing() == false) {
621      throw new TimestampParsingException("timestamp is required by mapping");
622    }
623    defaultTimestamp = mappingMd.timestamp().defaultTimestamp();
624  }
625
626  if (defaultTimestamp.equals(TimestampFieldMapper.Defaults.DEFAULT_TIMESTAMP)) {
627    timestamp = Long.toString(System.currentTimeMillis());
628  } else {
629    timestamp = MappingMetaData.Timestamp.parseStringTimestamp(defaultTimestamp, mappingMd.timestamp().dateTimeFormatter(), getVersion(metaData, concreteIndex));
630  }
```

The `else` block can only be hit if `defaultTimestamp` is not equal to `TimestampFieldMapper.Default.DEFAULT_TIMESTAMP`. But on [line 617](https://github.com/elastic/elasticsearch/blob/cca611171c53254b74f2da7239e05177fec92b1d/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L617) it is initialized to have that value. That value is possibly changed from the default on [line 623](https://github.com/elastic/elasticsearch/blob/cca611171c53254b74f2da7239e05177fec92b1d/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L623), but that only occurs if `mappingMd != null` (from the condition on [line 618](https://github.com/elastic/elasticsearch/blob/cca611171c53254b74f2da7239e05177fec92b1d/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L618)). Thus, if the `else` block in question is hit, it is guaranteed that `mappingMd` is not null; there can not be an NPE here. However, it's possible that `mappingMd.timestamp().defaultTimestamp()` is equal to `TimestampFieldMapper.Defaults.DEFAULT_TIMESTAMP` and that's why the code is written this way.

I'll push a commit that has an assert that `mappingMd` is not null here, and that should keep your IDE or static analysis tool from complaining. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non developer readme contribution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16595</link><project id="" key="" /><description>I have written a clear, easy-to-comprehend introduction for elasticsearch. I believe this introduction, free of complex technical terms, will swiftly and thoroughly inform anyone about the use-cases, merit, and advantages, including the celebrated features, of elasticsearch. Seasoned developers, new programmers, and even nontechnical visitors will benefit from the clear language, exposing elasticsearch to a broader range of users.
</description><key id="132896512">16595</key><summary>Non developer readme contribution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">carpefukendiem</reporter><labels><label>docs</label><label>review</label></labels><created>2016-02-11T05:52:39Z</created><updated>2016-03-12T22:05:43Z</updated><resolved>2016-03-12T22:05:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-29T22:42:13Z" id="190432644">Since Elasticsearch is primarily a product for developers, GitHub is a developer-oriented site, and the README.textile is oriented towards developers, can you help us understand what this paragraph is adding to what is already there?
</comment><comment author="jasontedor" created="2016-03-12T22:05:43Z" id="195818014">No feedback, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regression: cannot change default similarity for all fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16594</link><project id="" key="" /><description>One should be able to change the default similarity for all fields by putting the following setting into `elasticsearch.yml` (see [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html#default-base)):

```
// Should it be index.similarity.classic.type now?
index.similarity.default.type: BM25
```

While debugging #16550 I realized this is currently broken in `master`. See following `curl` statements for reproducing:

```
echo "Delete previous tests"
curl -XDELETE 'http://localhost:9200/test/?pretty=true'
sleep 1

echo " "
echo "Create settings and mappings"
curl -XPUT 'http://localhost:9200/test/?pretty=true' -d '{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0,
      "similarity.classic.type": "BM25",
      "similarity.custom.type": "classic"
    }
  },
  "mappings": {
    "type1": {
      "properties": {
        "field1": { "type": "string" },
        "field2": { "type": "string", "similarity": "custom" }
      }
    }
  }
}'
sleep 1

echo " "
echo "Add sample data"
curl -XPUT 'http://localhost:9200/test/type1/1?pretty=true' -d '{
  "field1": "the quick brown fox jumped over the lazy dog",
  "field2": "the quick brown fox jumped over the lazy dog"
}'
sleep 1

echo " "
echo "Search against Classic similarity field"
curl -s 'http://localhost:9200/test/type1/_search?pretty=1' -d '{
  "query" : {
    "match" : {
      "field2" : "quick brown fox"
    }
  }
}' | grep '"_score"'

echo " "
echo "NOTE!!! Classic similarity field yields a score of 0.16608897"

echo " "
echo "Search against BM25 similarity field"
curl -s 'http://localhost:9200/test/type1/_search?pretty=1' -d '{
  "query" : {
    "match" : {
      "field1" : "quick brown fox"
    }
  }
}' | grep '"_score"'

echo " "
echo "NOTE!!! BM25 field should yield a score of 0.8169974. Instead it yields 0.16608897 which is the same score as the Classic similarity field above"
```

I think the issue is with the `SimilarityService.addSimilarities()` method overwriting any `providers` configured via index settings. Working on a PR.
</description><key id="132883408">16594</key><summary>Regression: cannot change default similarity for all fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">gpstathis</reporter><labels><label>:Similarities</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-11T04:15:13Z</created><updated>2016-03-08T14:30:44Z</updated><resolved>2016-03-08T14:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gpstathis" created="2016-02-11T05:10:20Z" id="182711205">@s1monw see commit af0e07a above in my repo. Happy to open a PR if it looks ok to you.
</comment><comment author="s1monw" created="2016-02-11T08:32:43Z" id="182762820">@gpstathis I think there is more wrong here than just that. Af far as I can tell you should not be able to overwrite this similarity that's why it's applied afterwards. To change the default you can define `similarity.base` but this seems broken too. Since default is used instead of base in lots of places. I think we need to fix all of this? Do you wanna take a look at it? 

I think in `PerFieldSimilarity` we should only use `defaultSimilarity` and the `baseSimilarity` member should go away and be assigned to `defaultSimilarity` if it's present. I also think we need to throw an exception if a build-in similarity is changed? This can be a different change. Regarding your commit I think this should all be done in a unittest which I can provide you with and example if you wanna take this on you?
</comment><comment author="gpstathis" created="2016-02-11T15:16:33Z" id="182908981">@s1monw yes, a unit test for what you would like the end behavior to be would be helpful. Also, I'm a bit unclear on a few things now:
- Are we saying that one will no longer be able to define an alternative similarity that applies to all fields without going into the schema field-by-field and adding it? I.e. is this no longer desirable: 

```
index.similarity.default.type: BM25
```
- Regarding `similarity.base`, I thought it only applied to `coord()` and `queryNorm()` and was there for expert use per the [comment in the code](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java#L82). Would consolidating with `defaultSimilarity` introduce regressions elsewhere?

I can take a stab at a fix but only after #16550 which is a blocker for us at the moment and already present since 2.0. At least this bug is only on `master` and there is a workaround which is to overwrite every field in our schema with our custom similarity, field-by-field.
</comment><comment author="s1monw" created="2016-02-11T15:48:36Z" id="182925651">regarding `similarity.base` I agree I misread that part. 

What I am saying is that I think nobody should be able to change the `index.similarity.classic` similarity nor any similarity that is built-in. I think we should add a dedicated `index.similarity.default.type` that can be defined by the user? 
</comment><comment author="gpstathis" created="2016-02-11T17:30:55Z" id="182967953">@s1monw got it now. Thanks for clarifying.
</comment><comment author="gpstathis" created="2016-02-15T20:36:30Z" id="184381386">@s1monw see PR. Not sure if this is what you had in mind. The commit adds the ability to define a `index.similarity.default.type` and prevents users from redefining any built-in similarity types. The only thing I am unclear on though is the use of the `SimilarityService.DEFAULT_SIMILARITY` constant across the code and the effects this may have. `FieldMapper`, `TypeParsers` and `AllFieldMapper` reference that constant directly. I left `similarity.base` unchanged per our discussion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Performing derivatives on arbitrary aggregations. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16593</link><project id="" key="" /><description>I was looking into implementing the derivative pipeline into our aggregations. 
For example, we may return a sum aggregation of numeric type (post likes) that has been segmented previously by a string type (channel, fb/twitter/etc). Also specifically in our case they were bucketed under a date_range aggregation.

I noticed that derivatives could only be provided if parent aggregation was a histogram or a date histogram. Is there any plan or desire to support derivatives (and other pipeline metrics for the general case of numeric aggregations? I'm guessing there would also be some work around the gap policy.

I also noticed an issue with parsing the bucket path when i have an aggregation with a name containing a "." (like I have below), I was able to fix that by just renaming to "_"

sample query where i'd like to have derivatives for:
`"sum":{  "field":"post.likes_count" }`
the request:

``` json
{  
   "aggregations":{  
      "date_range_buckets":{  
         "date_range":{  
            "field":"post.creation_date",
            "ranges":[  
               {  
                  "from":"2015-12-15T00:00:00.000Z",
                  "to":"2015-12-21T00:00:00.000Z"
               },
               {  
                  "from":"2015-12-21T00:00:00.000Z",
                  "to":"2016-01-01T00:00:00.000Z"
               }
            ]
         },
         "aggregations":{  
              "channel":{  
                   "terms":{  
                      "field":"internal.channel"
                   },
                   "aggregations":{  
                      "post_likes_count":{  
                         "sum":{  
                            "field":"post.likes_count"
                         }
                      }
                   }
               }
         }
      }
   },
   "size": 0
}
```

Thanks, let me know if i'm just missing something obvious. :)
</description><key id="132821571">16593</key><summary>Performing derivatives on arbitrary aggregations. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kennycason</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2016-02-10T21:24:51Z</created><updated>2016-02-13T12:19:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-02-12T20:30:00Z" id="183477409">Hmm, do you just want the "total" derivative of the summed `like_count` between the two different ranges?  Or do you want to see the derivative between each term in the ranges?

The first is doable by using a `sum_bucket` agg to accumulate all the `like_count` for a single range, then a derivative over that sum:

``` json
{
   "aggregations": {
      "date_range_buckets": {
         "date_histogram": {
            "field": "creation_date",
            "interval": "day"
         },
         "aggregations": {
            "channel": {
               "terms": {
                  "field": "channel"
               },
               "aggregations": {
                  "post_likes_count": {
                     "sum": {
                        "field": "likes_count"
                     }
                  }
               }
            },
            "sum_of_likes": {
               "sum_bucket": {
                  "buckets_path": "channel&gt;post_likes_count.value"
               }
            },
            "deriv": {
               "derivative": {
                  "buckets_path": "sum_of_likes"
               }
            }
         }
      }
   },
   "size": 0
}
```

The second option is not currently possible...the pipeline aggs don't really play nicely with terms aggs at the moment.  We dont have any syntax to "path" into a specific term, and the aggs don't know how to "link up" matching terms between different ranges. 

We've definitely talked about this type of functionality before.  Unsure where we left off, it might be pending some refactoring.  @colings86 thoughts?
</comment><comment author="kennycason" created="2016-02-13T00:59:10Z" id="183548813">I admittedly probably made the example more complicated by adding in multiple date ranges. :) Thanks for the response!

When you said "the pipeline aggs don't really play nicely with terms aggs at the moment." - That was the primary point of interest for me, the ability to do pipeline metrics on any aggregation (or at least terms aggs) and not just on the histogram aggregations. 

As it stands now, I am building a generic metrics pipeline that functions on an Elasticsearch response for our api. i.e. given an Elasticsearch response, and a list of fields &amp; desired metrics (derivative/avg/etc), apply it to the response before returning to user. Fortunately the calculations aren't super complex but I found my code getting a bit more complex when I started trying to implement my own "gap policy" before applying my own pipeline.
</comment><comment author="yaauie" created="2016-02-13T01:10:40Z" id="183550144">`date_range` aggregation shares a property with `date_histogram` aggs, which may make them suitable for derivative aggregations, specifically that the ordering of the buckets is meaningful (so long as one doesn't completely encompass another), therefore the pipeline has enough information to provide the derivative. Given a `date_range` agg, it would be very useful to apply derivative in order to support period-over-period comparisons for arbitrary periods (as opposed to the set of pre-defined periods that `date_range` aggs make possible).
</comment><comment author="kennycason" created="2016-02-13T04:12:27Z" id="183584626">@yaauie that's a good point on the ordering and something that makes our case a bit different is that we have nested term aggregations, then for each of the buckets we perform sub aggregations like sum/average, or just use count. those are all numerical values that we then use to sort our buckets THEN we manually apply the derivative pipeline.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos in comments of JAVA code.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16592</link><project id="" key="" /><description /><key id="132821275">16592</key><summary>Fix typos in comments of JAVA code.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T21:23:12Z</created><updated>2016-02-26T18:53:26Z</updated><resolved>2016-02-11T02:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-10T22:36:30Z" id="182611642">LGTM. I'll integrate later.
</comment><comment author="dongjoon-hyun" created="2016-02-10T22:49:41Z" id="182618558">Thank you, @jasontedor .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compress entire data portion of snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16591</link><project id="" key="" /><description>The idea is straight forward:

When creating a snapshot, gzip (or otherwise compress) the entire snapshot, then upload it to the repository.

Upside:
- Storage may be reduced for upload, download, and storage itself.

Downsides:
- Processing time, memory, and _local_ temp storage to compress segments.
- Processing time, memory, and _local_ temp storage to decompress the snapshot.
- Incremental snapshots, which is what snapshots are, would require downloading entire previous compressed snaphots (or forgoing incremental snapshots entirely), thereby increasing the workload necessary to _restore_ from snapshots.
- For compressed containers to work with incremental snapshots, an additional file that describes its contents (the segments) would need to be created, thus creating another potential source for issues.
</description><key id="132809748">16591</key><summary>Compress entire data portion of snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Snapshot/Restore</label></labels><created>2016-02-10T20:29:05Z</created><updated>2016-02-10T20:34:10Z</updated><resolved>2016-02-10T20:30:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-02-10T20:30:41Z" id="182571242">I wanted to create this issue so that it was re-openable incase we ever decide to change our minds on it. However, currently we see this as an undesirable problem rather than a benefit.

As a workaround, if you do want this effect, you _can_ force merge your index down to a single segment and snapshot afterward. In doing so, then you get the same effect and you only have to download a single file that does not need to be decompressed at the end.
</comment><comment author="s1monw" created="2016-02-10T20:34:10Z" id="182572536">&gt; Storage may be reduced for upload, download, and storage itself.

this might not be as much as you wish it would be... The lucene index is compressed pretty well I don't think it would buy us a lot.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos in comments in elasticsearch/common.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16590</link><project id="" key="" /><description /><key id="132781115">16590</key><summary>Fix typos in comments in elasticsearch/common.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T18:32:03Z</created><updated>2016-02-26T18:53:24Z</updated><resolved>2016-02-10T18:35:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-10T18:36:02Z" id="182520527">Thanks for this @dongjoon-hyun ! I've reviewed and it all looked great so I merged.

Thanks again!
</comment><comment author="dongjoon-hyun" created="2016-02-10T18:37:56Z" id="182521039">Thank you for fast merging, @nik9000 ! :)
</comment><comment author="nik9000" created="2016-02-10T18:40:15Z" id="182521964">This time because it was all in comments I didn't have to run the tests so I could do the whole thing on the internet which was convenient!

Thanks for fixing these. They might be small but it is nice to have them right and no one else was doing it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a new `keyword` field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16589</link><project id="" key="" /><description>The `keyword` field is intended to replace `not_analyzed` string fields. It is
indexed and has doc values by default, and doesn't support enabling term
vectors.

Although it doesn't support setting an analyzer for now, there are plans for
it to support basic normalization in the future such as case folding.

Relates #14113
</description><key id="132768460">16589</key><summary>Add a new `keyword` field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T17:39:35Z</created><updated>2016-06-28T09:31:14Z</updated><resolved>2016-02-11T17:23:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T18:17:12Z" id="182512108">Looks good. Left a couple suggestions.
</comment><comment author="jpountz" created="2016-02-11T17:08:44Z" id="182960973">@rjernst I pushed another commit.
</comment><comment author="rjernst" created="2016-02-11T17:17:31Z" id="182964256">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds validation to the Aggregator Builder implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16588</link><project id="" key="" /><description /><key id="132756617">16588</key><summary>Adds validation to the Aggregator Builder implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-10T16:53:19Z</created><updated>2016-02-12T09:16:31Z</updated><resolved>2016-02-12T09:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-11T15:14:08Z" id="182906788">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing default payloads settings for completion type on ES 2.2.0-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16587</link><project id="" key="" /><description>Before ES 2.2.0, you could omit 'payloads: false' and it would default automatically to false in your mapping.

```
curl -X PUT localhost:9200/music
curl -X PUT localhost:9200/music/song/_mapping -d '{
  "song" : {
        "properties" : {
            "name" : { "type" : "string" },
            "suggest" : { "type" : "completion",
                          "analyzer" : "simple",
                          "search_analyzer" : "simple"
            }
        }
    }
}'
```

Executing this on ES &lt; 2.2.0 gives:

```
{
  "song": {
    "properties": {
      "name": {
        "type": "string"
      },
      "suggest": {
        "max_input_length": 50,
        "payloads": false,
        "analyzer": "simple",
        "preserve_position_increments": true,
        "type": "completion",
        "preserve_separators": true
      }
    }
  }
}
```

But now gives this on 2.2.0:

```
{
  "song": {
    "properties": {
      "name": {
        "type": "string"
      },
      "suggest": {
        "max_input_length": 50,
        "analyzer": "simple",
        "preserve_position_increments": true,
        "type": "completion",
        "preserve_separators": true
      }
    }
  }
}
```

And for some reason, bulk indexing on the index without the `"payloads"` param is failing silently. 
</description><key id="132747685">16587</key><summary>Missing default payloads settings for completion type on ES 2.2.0-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raphi</reporter><labels /><created>2016-02-10T16:24:38Z</created><updated>2016-02-13T18:59:46Z</updated><resolved>2016-02-13T18:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T18:59:45Z" id="183724740">Hi @raphi 

In 2.2.0-SNAPSHOT, we had the new document-oriented completion suggester, which no longer uses payloads.  Before the final release we decided to back out this change because it had bwc issues, and move it to 3.0 only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds wait for task registration to testCanFetchIndexStatus</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16586</link><project id="" key="" /><description>In the testCanFetchIndexStatus the task check can occur before the indexing process is started making the test to fail. This commit adds an additional lock to make sure we check tasks only after at least one of the tasks is registered.
</description><key id="132734310">16586</key><summary>Adds wait for task registration to testCanFetchIndexStatus</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>review</label><label>test</label></labels><created>2016-02-10T15:41:39Z</created><updated>2016-02-11T01:06:01Z</updated><resolved>2016-02-11T01:06:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-10T15:55:36Z" id="182443219">thanks @imotov LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Does Elasticsearch need to include wrappers for java.util.logging and slf4j?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16585</link><project id="" key="" /><description>Right now Elasticsearch has three logger implementations - one for log4j, one for java.util.logging, and one for slf4j. What does that buy us? The elasticsearch zip, tar, deb, and rpm distributions bundle log4j and logging.yaml doesn't work without it. When Elaticsearch is used as a client library master no longer tries to configure the logger because that is crazy behavior for a library. Normal Java libraries just pick a logging dependencies and use it. They rely on the user to include whatever bridges are required. Can we get away with being normal?

Proposal: Remove support for java.util.logging and slf4j logging support. We fold the log4j support into its parent class/package. We put a breaking change note for users of the TransportClient that they'll need to include the appropriate shim to point log4j12 at whatever logging library they use. They probably already have it on their classpath. We get to nuke three whole packages, lots of weird try/catch statements, and 2 optional dependencies.

If anyone asks why we still use log4j12 when log4j2 and logback are all around we tell them that it works and we have bigger fish to fry....
</description><key id="132727240">16585</key><summary>Does Elasticsearch need to include wrappers for java.util.logging and slf4j?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T15:18:04Z</created><updated>2016-02-26T21:42:03Z</updated><resolved>2016-02-26T21:42:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T16:47:23Z" id="182474548">Thanks for starting this discussion Nik, I agree our logging setup is crazy. :)

I have an alternative suggestion though: let's use java.util.logging for everything. This removes a dependency, and I don't think we really _use_ anything complicated in log4j?  Until we have a separate client, any users can configure bridges to j.u.l, just like they would log4j.
</comment><comment author="nik9000" created="2016-02-10T17:14:59Z" id="182488001">&gt; I don't think we really use anything complicated in log4j

The LogConfigurator is the complex bit. Not because it is complex but because the logging features it exposes come from log4j. I get wanting to dump the dependency though. I haven't investigated how difficult it would be to get the same features we have out of j.u.l.

Can we get it in steps? Collapse onto log4j in step 1 and switch to j.u.l is step 2 if we still think it is worth it?

The question of "do we even need our own logging wrapper" is a fine one and will probably be easier to answer when we only have a single logging wrapper rather than three.
</comment><comment author="rjernst" created="2016-02-10T17:17:23Z" id="182488691">&gt; Can we get it in steps? Collapse onto log4j in step 1 and switch to j.u.l is step 2 if we still think it is worth it?

Absolutely. I think that is a fine approach, and getting rid of slf4j as a start solves a ton of problems we have in gradle and our distributions (how we include this right now is a huge hack and very annoying).
</comment><comment author="rjernst" created="2016-02-10T17:37:06Z" id="182496053">Also, another thing we should look at is `logging.yml`. This should really just be a prefix in `elasticsearch.yml`, there is no reason to have a separate config file.
</comment><comment author="clintongormley" created="2016-02-13T19:03:24Z" id="183726468">&gt; Also, another thing we should look at is logging.yml. This should really just be a prefix in elasticsearch.yml, there is no reason to have a separate config file.

++
</comment><comment author="clintongormley" created="2016-02-14T17:40:50Z" id="183936097">The one thing we'll probably be asked for is different log management schemes, eg https://github.com/elastic/elasticsearch/issues/14956

Not sure how much we can support things like this?
</comment><comment author="rjernst" created="2016-02-14T19:09:07Z" id="183955681">We should get the configration/code we have cleaned up, and then consider additional features for how we expose logging, like that. But it should be through our own configuration settings, not through exposing the api of our underlying impl. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Crash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16584</link><project id="" key="" /><description>Since upgrading from 1.6 to 2.1 we've had elastic crash on us twice, once in production and once in staging.  The crash log can be found at https://gist.github.com/dstarh/007ee8d544581f6c0234. The first crash in production, nothing was happening other than normal user searches and the second crash, in staging I was re-building an index.  There's nothing out of the ordinary in the elastic logs themselves, just the crash log in /tmp

Versions: 
elasticsarch:  2.1.1
java: version "1.7.0_79"
OpenJDK Runtime Environment (IcedTea 2.5.6) (7u79-2.5.6-0ubuntu1.14.04.1)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)
ubuntu 14.04.2
</description><key id="132726827">16584</key><summary>Elasticsearch Crash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dstarh</reporter><labels><label>jvm bug</label></labels><created>2016-02-10T15:16:36Z</created><updated>2016-02-14T18:49:03Z</updated><resolved>2016-02-14T18:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-10T15:40:48Z" id="182432833">Hi @dstarh 

It sounds like a bug between Groovy and the JVM... Could you try upgrading your JVM to the latests Java 8 and see if that fixes it?
</comment><comment author="jasontedor" created="2016-02-10T15:41:34Z" id="182433498">This is a hard crash of the JVM, which is in general a hard thing to do and almost always indicative of a JVM bug. I searched for the specific issue you're seeing and there isn't anything obvious out there. It looks like you're using the default startup flags, so there is nothing egregious there. I did observe some Groovy scripting engine events in your log, which might be surfacing a JVM bug? Maybe try upgrading the JVM to at least 7u95 if you have to stay in the 2.x line of IcedTea, otherwise try going to the latest 8u release if that's an option for you. If this reliably reproduces, I think you'll want to file a bug with OpenJDK.
</comment><comment author="dstarh" created="2016-02-10T15:44:58Z" id="182436276">@clintongormley @jasontedor 8.x open jdk or oracle or does it matter?
</comment><comment author="jasontedor" created="2016-02-10T15:46:54Z" id="182437701">&gt; 8.x open jdk or oracle or does it matter?

@dstarh: Oracle if you have a choice.
</comment><comment author="jasontedor" created="2016-02-14T18:48:45Z" id="183949093">@dstarh: Since we think that the problem you were experiencing was due to a JVM bug, I'm closing this issue. Please feel free to open if you gather evidence otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>'size' parameter ignored when executing a function_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16583</link><project id="" key="" /><description>It seems that the order of the keys in the search body matter when executing a function_score query.  Consider these 2 cURL requests:

```
    curl -d '{
        "query":{
            "function_score":{
                "random_score":{},
                "query":{
                    "bool":{
                        "must":{"match":{"location":"London"}}},
                        "filter":{"bool":{
                            "must":[
                                {"match":{"location":"London"}},
                                {"term":{"state":"approved"}},
                                {"range":{"num_reviews":{"gte":3}}}
                            ]
                        }
                    }
                }
            }
        },
        "size": 1
        }' "http://localhost:9200/agent_profiles_development/_search?pretty"

In this case, size is ignored and the default of 10 is used.  If we move the `size` key as so:
```

```
curl -d '{
    "size": 1,
    "query":{
        "function_score":{
            "random_score":{},
            "query":{
                "bool":{
                    "must":{"match":{"location":"London"}}},
                    "filter":{"bool":{
                        "must":[
                            {"match":{"location":"London"}},
                            {"term":{"state":"approved"}},
                            {"range":{"num_reviews":{"gte":3}}}
                        ]
                    }
                }
            }
        }
    }
    }' "http://localhost:9200/agent_profiles_development/_search?pretty"
```

Then it works as expected.
</description><key id="132725681">16583</key><summary>'size' parameter ignored when executing a function_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ryanmcilmoyl</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-02-10T15:12:19Z</created><updated>2016-02-12T16:45:10Z</updated><resolved>2016-02-12T16:45:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T17:45:43Z" id="182498625">This works fine for me (with a simple match all query). Can you give a recreation (putting documents and running the two queries)? Do you see the same problem when using a simple query like `match_all`?
</comment><comment author="ryanmcilmoyl" created="2016-02-10T17:58:40Z" id="182504255">Just tested with match_all, works as expected.  Also works when just doing the `bool` query, without the `function_score`.  Seems to be the combination of the 2.

I'll build out a test case and post it.
</comment><comment author="javanna" created="2016-02-11T14:04:34Z" id="182877423">I was able to reproduce this. The query is malformed, the first must clause ends up closing the whole bool query (one too many curly brackets). Our parsers should throw an error though instead of going ahead. As a result, the second bool query replaces the previous query as part of the function score, and size gets ignored. I will work on making parsing stricter so that an error is thrown instead.
</comment><comment author="ryanmcilmoyl" created="2016-02-11T15:29:06Z" id="182916932">Ha, so I was doing something wrong in there :).  

Thanks for checking that out!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner_hits empty if null objects exist in path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16582</link><project id="" key="" /><description>I use top level inner hits to retrieve data from multiple nested documents. 
Elasticsearch returns empty inner hits arrays for some of the hits, which is probably due to the existence of null objects inside the nested documents. 
Please see a detailed description of the problem in this forum entry:
[https://discuss.elastic.co/t/nested-query-missing-inner-hits/41038](url)
I have looked at these documents with jq like this:
`jq .a.b.c.d.first[].e.f.second[].message&lt;json&gt;`
and the output is e.g. like this:

```
"Some message"
"Some other message"
jq: error: Cannot iterate over null
```

So I believe Elasticsearch iterates over the nested documents in a similar way and returns null if any of the nested objects is null (?). 
I guess the ES code needs to be fixed then? 
</description><key id="132718940">16582</key><summary>inner_hits empty if null objects exist in path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abulhol</reporter><labels /><created>2016-02-10T14:51:16Z</created><updated>2016-02-12T16:56:00Z</updated><resolved>2016-02-12T16:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abulhol" created="2016-02-12T16:56:00Z" id="183411547">I found out that this is not really the reason for my problem, see the update in the forum issue I wrote. It has to do with the complex syntax for inner hits that I did not fully understand. 
See also: 
https://github.com/elastic/elasticsearch/issues/13064
and this one:
https://github.com/elastic/elasticsearch/pull/16143#issuecomment-177973852
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices Recovery Failure after ES upgrade (from 2.1 to 2.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16581</link><project id="" key="" /><description>I get the error below after upgrading from ES 2.1 to 2.2. 
When i close the related indices (3 among 12) the error disappears but the cluster stucks in red state.

Any help please ?

Logs from Master Node : 

[2016-02-10 14:34:16,281][INFO ][gateway                  ] [Node0] recovered [28] indices into cluster_state
[2016-02-10 14:34:16,924][WARN ][cluster.action.shard     ] [Node0] [patentsample][2] received shard failed for [patentsample][2], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=5Q7BsdFMQ7uw1NLUUtmC4g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [NVocWIBeT_OSfX5BzWCbug], message [failed recovery], failure [IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]]; ]
[patentsample][[patentsample][2]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:224)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Hit unexpected exception while reading segment infos (resource=commit(null))
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:170)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:199)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:481)
        at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:361)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:493)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:490)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:731)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:683)
        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:490)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:163)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.Codec.forName(Codec.java:113)
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:469)
        ... 15 more
[2016-02-10 14:34:16,926][WARN ][cluster.action.shard     ] [Node0] [publicationsample][4] received shard failed for [publicationsample][4], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=YFaIx3-lTcek_bvzrBBgnw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.485Z]], indexUUID [slQCqXWKS5W1nwfux3WD6w], message [failed recovery], failure [IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]]; ]
[publicationsample][[publicationsample][4]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:224)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Hit unexpected exception while reading segment infos (resource=commit(null))
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:170)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:199)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:481)
        at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:361)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:493)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:490)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:731)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:683)
        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:490)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:163)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.Codec.forName(Codec.java:113)
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:469)
        ... 15 more
[2016-02-10 14:34:17,491][WARN ][cluster.action.shard     ] [Node0] [publicationsample][4] received shard failed for [publicationsample][4], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=YFaIx3-lTcek_bvzrBBgnw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.485Z]], indexUUID [slQCqXWKS5W1nwfux3WD6w], message [master {Node0}{JA-FfaaNSu-YHEmTRWmdKw}{10.150.232.141}{10.150.232.141:9301} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-10 14:34:17,533][WARN ][cluster.action.shard     ] [Node0] [patentsample][2] received shard failed for [patentsample][2], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=5Q7BsdFMQ7uw1NLUUtmC4g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [NVocWIBeT_OSfX5BzWCbug], message [master {Node0}{JA-FfaaNSu-YHEmTRWmdKw}{10.150.232.141}{10.150.232.141:9301} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-10 14:34:17,585][WARN ][cluster.action.shard     ] [Node0] [publicationsample][1] received shard failed for [publicationsample][1], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=eRGZ06jJRQukGunuw-KR5g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.485Z]], indexUUID [slQCqXWKS5W1nwfux3WD6w], message [failed recovery], failure [IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]]; ]
[publicationsample][[publicationsample][1]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:224)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Hit unexpected exception while reading segment infos (resource=commit(null))
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:170)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:199)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:481)
        at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:361)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:493)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:490)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:731)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:683)
        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:490)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:163)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.Codec.forName(Codec.java:113)
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:469)
        ... 15 more
[2016-02-10 14:34:17,757][WARN ][cluster.action.shard     ] [Node0] [familysample][0] received shard failed for [familysample][0], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=lQICArHATBSnSm-PU_B4Cg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [aeC20SE7TkSeE3iMu3i_Fw], message [failed recovery], failure [IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]]; ]
[familysample][[familysample][0]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:224)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Hit unexpected exception while reading segment infos (resource=commit(null))
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:170)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:199)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:481)
        at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:361)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:493)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:490)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:731)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:683)
        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:490)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:163)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.Codec.forName(Codec.java:113)
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:469)
        ... 15 more
[2016-02-10 14:34:17,758][WARN ][cluster.action.shard     ] [Node0] [familysample][3] received shard failed for [familysample][3], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=PQq5FgyvR3Kd9_9TLgta0w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [aeC20SE7TkSeE3iMu3i_Fw], message [failed recovery], failure [IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]]; ]
[familysample][[familysample][3]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: CorruptIndexException[Hit unexpected exception while reading segment infos (resource=commit(null))]; nested: IllegalArgumentException[Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?]; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:224)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.index.CorruptIndexException: Hit unexpected exception while reading segment infos (resource=commit(null))
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:170)
        at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:199)
        ... 5 more
Caused by: java.lang.IllegalArgumentException: Could not load codec 'Lucene54'.  Did you forget to add lucene-backward-codecs.jar?
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:481)
        at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:361)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:493)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:490)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:731)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:683)
        at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:490)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:163)
        ... 7 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.Codec with name 'Lucene54' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [Lucene40, Lucene41, Lucene42, Lucene45, Lucene46, Lucene49, Lucene410, Lucene50, Lucene53]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.Codec.forName(Codec.java:113)
        at org.apache.lucene.index.SegmentInfos.readCodec(SegmentInfos.java:469)
        ... 15 more
[2016-02-10 14:34:17,780][WARN ][cluster.action.shard     ] [Node0] [familysample][3] received shard failed for [familysample][3], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=PQq5FgyvR3Kd9_9TLgta0w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [aeC20SE7TkSeE3iMu3i_Fw], message [master {Node0}{JA-FfaaNSu-YHEmTRWmdKw}{10.150.232.141}{10.150.232.141:9301} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-10 14:34:17,780][WARN ][cluster.action.shard     ] [Node0] [familysample][0] received shard failed for [familysample][0], node[MJvzZG3GRYWsWQMkwW_iTQ], [P], v[7], s[INITIALIZING], a[id=lQICArHATBSnSm-PU_B4Cg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-10T14:34:15.484Z]], indexUUID [aeC20SE7TkSeE3iMu3i_Fw], message [master {Node0}{JA-FfaaNSu-YHEmTRWmdKw}{10.150.232.141}{10.150.232.141:9301} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-10 14:34:19,159][INFO ][cluster.service          ] [Node0] added {{Node2}{XS3GAcc1QJ2PiqINJ0vmWw}{10.150.232.143}{10.150.232.143:9300},}, reason: zen-disco-join(join from node[{Node2}{XS3GAcc1QJ2PiqINJ0vmWw}{10.150.232.143}{10.150.232.143:9300}])
</description><key id="132717503">16581</key><summary>Indices Recovery Failure after ES upgrade (from 2.1 to 2.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iliasse-hassala</reporter><labels /><created>2016-02-10T14:45:38Z</created><updated>2016-02-11T09:37:37Z</updated><resolved>2016-02-11T09:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T17:22:41Z" id="182490495">How did you upgrade? It looks like you have lucene jars from ES 2.1 on your classpath.
</comment><comment author="iliasse-hassala" created="2016-02-11T08:50:30Z" id="182770237">Hi rjernst, thank you for your answer.

Actually I'm using a Dockerfile (below) i just changed the image i'm pulling (2.1 to 2.2).
I have a 3 nodes setup : Node{0,1,2}

```
FROM elasticsearch:2.2

        EXPOSE 9200 9300

        RUN /usr/share/elasticsearch/bin/plugin install license
        RUN /usr/share/elasticsearch/bin/plugin install marvel-agent
        RUN /usr/share/elasticsearch/bin/plugin install delete-by-query
        RUN export ES_HEAP_SIZE=30g
        COPY ESconf/* /usr/share/elasticsearch/config/

CMD elasticsearch
```

Basically what i noticed is that the translogs of 3 indices among 12 were not empty. And when i close these 3 indices the errors disappears but the cluster is stuck in red state.
I run "/_cat/shards" in the master node (Node3) and i see that there are some UNASSIGNED shards belonging to Node0 I presume (primary ones, since i had no replicas in order to speed up the bulk injection).

The Master node sees 3 nodes :  "_health" gives this

```
{
  "cluster_name" : "NOSQL_elastic_cluster",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 52,
  "active_shards" : 69,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 25,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 73.40425531914893
}
```

But when i run the same command on Node0 I get nothing (it times out) so may be something else is going on with Node0. What is weird is that the logs of Node0 says everything is ok (it detects the master and joins the cluster) :

```
[2016-02-10 16:54:27,638][INFO ][node                     ] [Node0] version[2.2.0], pid[6], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-10 16:54:27,638][INFO ][node                     ] [Node0] initializing ...
[2016-02-10 16:54:28,409][INFO ][plugins                  ] [Node0] modules [lang-expression, lang-groovy], plugins [license, marvel-agent, delete-by-query], sites []
[2016-02-10 16:54:28,464][INFO ][env                      ] [Node0] using [1] data paths, mounts [[/usr/share/elasticsearch/data (h097b:/vol/K1NQDATASQL1/ESdata/Node0)]], net usable_space [1.3tb], net total_space [2.9tb], spins? [possibly], types [nfs]
[2016-02-10 16:54:28,464][INFO ][env                      ] [Node0] heap size [30.6gb], compressed ordinary object pointers [true]
[2016-02-10 16:54:31,097][INFO ][node                     ] [Node0] initialized
[2016-02-10 16:54:31,097][INFO ][node                     ] [Node0] starting ...
[2016-02-10 16:54:31,505][INFO ][transport                ] [Node0] publish_address {10.150.232.141:9301}, bound_addresses {[::]:9301}
[2016-02-10 16:54:31,520][INFO ][discovery                ] [Node0] NOSQL_elastic_cluster/JFUJziK_QViw9kPut7Aopw
[2016-02-10 16:54:34,672][INFO ][cluster.service          ] [Node0] detected_master {Node2}{ZyxfRlELTECjeiSk63Wutw}{10.150.232.143}{10.150.232.143:9300}, added {{Node1}{kv-JGvl0RaCzdzCL-WWQ0A}{10.150.232.142}{10.150.232.142:9300},{Node2}{ZyxfRlELTECjeiSk63Wutw}{10.150.232.143}{10.150.232.143:9300},}, reason: zen-disco-receive(from master [{Node2}{ZyxfRlELTECjeiSk63Wutw}{10.150.232.143}{10.150.232.143:9300}])
[2016-02-10 16:54:34,820][INFO ][license.plugin.core      ] [Node0] license [4eedac74-a1d1-4023-8006-82db439db4f1] - valid
[2016-02-10 16:54:35,201][INFO ][http                     ] [Node0] publish_address {10.150.232.141:9201}, bound_addresses {[::]:9201}
[2016-02-10 16:54:35,202][INFO ][node                     ] [Node0] started
```

Any idea ? thanks for your help.
</comment><comment author="iliasse-hassala" created="2016-02-11T09:37:35Z" id="182782344">I had to reroute manually the UNASSIGNED shards. I guess that i should stop the cluster more properly (i was doing some bulk injection) before doing the update.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dead support for Java Service Wrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16580</link><project id="" key="" /><description>This commit removes bootstrap support for [Java Service Wrapper](http://wrapper.tanukisoftware.com/doc/english/download.jsp). The
implementation of this has been moved to its own [repository](https://github.com/elastic/elasticsearch-servicewrapper) where it was
deprecated, does not work with Elasticsearch 2.x, and is untested and
therefore unmaintained.

Relates #154, #243
</description><key id="132711211">16580</key><summary>Remove dead support for Java Service Wrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T14:19:58Z</created><updated>2016-02-13T19:01:15Z</updated><resolved>2016-02-10T17:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-10T14:20:41Z" id="182393711">I discovered this while starting on #16579.
</comment><comment author="nik9000" created="2016-02-10T17:06:03Z" id="182483451">LGTM. I don't remember have much experience with the java service wrapper code though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch does not start as a daemon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16579</link><project id="" key="" /><description>Due to the settings refactoring and the magical action-at-a-distance way that some settings like `es.foreground` are set via system properties, Elasticsearch does not currently start as a daemon (and thus as a service) when built from master:

```
$ ~/elasticsearch/elasticsearch-3.0.0-SNAPSHOT/bin/elasticsearch -d
$ Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [foreground]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:227)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:216)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:60)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:224)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:156)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:294)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:37)
Refer to the log for complete error details.
```

Closes #11564
</description><key id="132710986">16579</key><summary>Elasticsearch does not start as a daemon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T14:19:00Z</created><updated>2016-03-15T23:25:35Z</updated><resolved>2016-03-15T23:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Data loss due to unassigned shard status on restarting application.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16578</link><project id="" key="" /><description>Hi...

I am using play-java framework and also implemented elastic in my code. Its working fine till the application is in running mode. But when i restart application some shards get unassigned due to which data loss occurs. 

Way to get client&gt;&gt;&gt;&gt;&gt;&gt;

public class ElasticConfig {
private static  Node node =null;
 static private Node getNode(){
        return nodeBuilder().settings(Settings.builder()
                                .put("path.home", "/home/user"+"/elasticData")
                                .put("node.name", "elastic")
                                .put("cluster.name", "elasticCluster")
                                .put("number_of_replicas", 0)
                                //.put("http.port",9200)
                                //.put("index.number_of_shards", "1")
                                .put("index.routing.allocation.include.size", "big"))
                        .node();
}
public static Client getElasticNodeClient(){
    if(node == null){
        node = getNode();
    }
            return node.client();
    }}

&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

**How should i prevent shard unassigning on restarting application.**

I searched a lot, not get proper and final solution.
</description><key id="132705208">16578</key><summary>Data loss due to unassigned shard status on restarting application.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hemraj-sharma</reporter><labels /><created>2016-02-10T13:50:58Z</created><updated>2016-02-13T04:27:18Z</updated><resolved>2016-02-11T07:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-10T14:50:20Z" id="182404856">An unassigned shard means the master can not find any shard copies after the restart. Can you check your data folder and see what's in it? Do you see any errors in the ES logs? 

PS. you run with no replicas which means that if something goes wrong with the single copy you have  - there is no backup replica to replcae it
</comment><comment author="hemraj-sharma" created="2016-02-11T06:12:05Z" id="182733209">as inside the data folder, under the folder of the name with index name, usually there are folders named 0,1,...,4 corresponding to the shards. But when I get unassigned shard status. I find folders corresponding to the unassigned shards missing. And I didnot get any thing in the ES logs. But I find cluster health red in that case. 

   I changed replica to "0". as I found a suggestion on stackoverflow for the same. Moreover I added the setting parameter 
----&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  *\* put("index.routing.allocation.include.size", "big")**
&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;-----
As the solution was given somewhere  on the stack overflow
</comment><comment author="bleskes" created="2016-02-11T07:18:11Z" id="182744217">&gt;  find folders corresponding to the unassigned shards missing.

I think this is the source of your problem. If the data folder is missing, shard can not be assigned - which means ES does the right thing here.. I suggest your research what can cause this in your application or environment. At the moment it doesn't like ES was doing it, so I'm going to close this issue - if you do find any evidence that ES is deleting the shards please do reopen the ticket.
</comment><comment author="hemraj-sharma" created="2016-02-11T09:16:55Z" id="182777036">Okey thanks.
</comment><comment author="hemraj-sharma" created="2016-02-12T10:04:28Z" id="183259005">Sir I am new to the elastic search. I need  help from your side  to identify the problem that why it is happening. 

when I execute my application with the above configuration, 

I find the nodes directory as follows:
nodes:
           0:
           1:
           2:
and increasing, along with this the the folders(0,1,2,3,4) under the index name get deleted.
"http://localhost:9200/_nodes" 
gives the complete detail of the nodes. But I need to keep only single node in the cluster.
I think there is issue with my configuration but unable to figure out, Can you help me.
Thanks a lot.
</comment><comment author="bleskes" created="2016-02-12T15:14:32Z" id="183369204">@hemraj-sharma we will need more details to know exactly. However, we reserve github to track issues with the software. Can you take this discussion to http://discuss.elastic.co ? we can then continue there.. 
</comment><comment author="hemraj-sharma" created="2016-02-13T04:27:18Z" id="183587658">sure sir, Thanks for your support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple_query_string gives java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16577</link><project id="" key="" /><description>Hi Team,

I have got the following error when using simple_query_string with multiple fields containing numeric field.
Version: 2.2.0
OS: Windows

**Sample data:**
`curl -XPUT http://localhost:9200/blog/post/1 -d '{"foo":123, "bar":"xyzzy"}'`
`curl -XPUT http://localhost:9200/blog/post/2 -d '{"foo":456, "bar":"xyzzy"}'`

**Use simple_query_string with multiple fields**
`curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query":{"simple_query_string":{"query":"123","fields":["foo","bar"]}}}'`

**Error**
`{
    "error": {
        "root_cause": [
            {
                "type": "illegal_argument_exception",
                "reason": "Illegal shift value, must be 0..63; got shift=2147483647"
            }
        ],
        "type": "search_phase_execution_exception",
        "reason": "all shards failed",
        "phase": "query",
        "grouped": true,
        "failed_shards": [
            {
                "shard": 0,
                "index": "blog",
                "node": "6jVhZCw3Tau3cP5VtEg8Tw",
                "reason": {
                    "type": "illegal_argument_exception",
                    "reason": "Illegal shift value, must be 0..63; got shift=2147483647"
                }
            }
        ]
    },
    "status": 400
}`

**multi_match query works with the same fields**
`curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query":{"bool":{"must":[{"multi_match":{"query":"123","type":"cross_fields","fields":["foo","bar"],"operator":"and"}}]}}}'` 

Issue similar to #15860
</description><key id="132693077">16577</key><summary>simple_query_string gives java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">harishkannarao</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-02-10T12:58:00Z</created><updated>2016-02-18T23:27:53Z</updated><resolved>2016-02-18T23:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T18:00:08Z" id="182504877">I think what happens is the string value is handled by the hidden analyzer for long fields, which expects a Long.  We should be using the field type for each field to call the `value` function, which in the case of `long` field type will call `Long.parseLong`. @dakrone thoughts?
</comment><comment author="dakrone" created="2016-02-11T20:43:15Z" id="183053461">@rjernst yeah, I think that is a good idea. I will look into fixing this.
</comment><comment author="dakrone" created="2016-02-18T23:27:43Z" id="185978828">Pushed a fix for this to the 2.2 branch that will be in 2.2.1, and it has been fixed in Lucene 5.5, which will be in the 2.3+ releases of ES
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>field mapping conflicts are critical errors?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16576</link><project id="" key="" /><description>Hi,

I've upgraded our ES cluster from 1.7.4 to 2.2 yesterday. Before updating, i've run the migration plugin to check for errors. It showed two BLUE alerts - one about boolean fields returning 0/1 rather than T/F, and the other one about mapping field conflicts. 
The mapping field conflict warning was because i had a bug in my mapping with _source specified as a field. The migration plugin noted this as a BLUE warning and said it's ok to upgrade. From the plugin documentation https://github.com/elastic/elasticsearch-migration:

&gt; Blue
&gt; An advisory note that something has changed. No action needed.

I've fixed the schema, and all new indices were created with proper schema (this is a logstash cluster with daily/weekly indices). 

After upgrading the cluster to 2.2, elasticsearch woud start, but then exit soon. The logs contained:

```
2016-02-09 08:14:34,146][INFO ][node                     ] [es1.logstash.example.com] version[2.2.0], pid[7960], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-09 08:14:34,147][INFO ][node                     ] [es1.logstash.example.com] initializing ...
[2016-02-09 08:14:34,651][INFO ][plugins                  ] [es1.logstash.example.com] modules [lang-expression, lang-groovy], plugins [], sites []
[2016-02-09 08:14:34,673][INFO ][env                      ] [es1.logstash.example.com] using [1] data paths, mounts [[/ (/dev/sda3)]], net usable_space [1.1tb], net total_space [1.4tb], spins? [possibl
y], types [ext4]
[2016-02-09 08:14:34,673][INFO ][env                      ] [es1.logstash.example.com] heap size [27.8gb], compressed ordinary object pointers [true]
[2016-02-09 08:14:36,994][ERROR][gateway                  ] [es1.logstash.example.com] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [flows-2016.01.06], reason: [Field [_source] is defined both as an object and a field in [netflow]]
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:339)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
        at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
        at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Field [_source] is defined both as an object and a field in [netflow]
        at org.elasticsearch.index.mapper.MapperService.checkFieldUniqueness(MapperService.java:375)
        at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:385)
        at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:411)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:314)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
        ... 48 more
[2016-02-09 08:14:37,590][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [flows-2016.01.06], reason: [Field [_source] is defined both as an object and a field in [netflow]]
Likely root cause: java.lang.IllegalArgumentException: Field [_source] is defined both as an object and a field in [netflow]
        at org.elasticsearch.index.mapper.MapperService.checkFieldUniqueness(MapperService.java:375)
        at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:385)
        at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:411)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:314)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
        at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
        at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Unfortunately, it kept crashing like that all the time until i removed all the affected indices ( 1.5 months of netflow logs). I understand the conflicting mapping might not be supported, but the migration plugin should at least mark that as RED / Critical error. If it did, i could have waited a month before upgrading so that new indices use the proper, not broken schema.
</description><key id="132686762">16576</key><summary>field mapping conflicts are critical errors?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sysmonk</reporter><labels /><created>2016-02-10T12:28:12Z</created><updated>2016-02-13T19:17:32Z</updated><resolved>2016-02-13T19:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T18:24:00Z" id="182515379">&gt; Unfortunately, it kept crashing like that all the time until i removed all the affected indices

The indexes should have been intact. The error you see is part of the "upgrade" process when starting a node. Moving them to another location should have allowed you to still open them with a 1.7 cluster. Also, this is why it is highly recommended to backup your data with a snapshot before upgrading.

&gt; the migration plugin should at least mark that as RED / Critical error.

@clintongormley That seems like an easy fix? This blue warning you got is not actually the error here. This is because of a more recent change which does not allow a field to be an "object" as well as a regular field at the same time. For example, if in your mappings you had a field underneath `_source`, then _source would become an object field.
</comment><comment author="clintongormley" created="2016-02-13T19:17:32Z" id="183728878">@rjernst fixed in https://github.com/elastic/elasticsearch-migration/commit/0d6dce8dcd58b15201fbcd3206ca33eba1558a12

Closing this issue as i don't think there is any more to do here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>host field in _cat/nodes is always an IP address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16575</link><project id="" key="" /><description>I've upgraded my elasticsearch cluster from 1.7.4 to 2.2.0 yesterday, and i noticed that _cat/nodes?v now always returns an IP address in the "host" field.

ElasticSearch 2.2 Example:

```
host          ip            heap.percent ram.percent load node.role master name
192.168.0.75   192.168.0.75            68          99 1.07 d         m      es1.logstash.example.com     
192.168.0.55   192.168.0.55            64         100 0.44 d         m      es2.logstash.example.com      
192.168.0.69   192.168.0.69            29          98 1.09 d         m      es4.logstash.example.com     
192.168.0.79   192.168.0.79            30         100 1.14 d         m      es6.logstash.example.com     
192.168.0.246  192.168.0.246           68         100 1.19 d         m      es5.logstash.example.com      
192.168.0.49   192.168.0.49            73          97 0.62 d         *      es8.logstash.example.com      

```

Previously, in ElasticSearch 1.7.4 it had the 'host' field populated with the hostname (same as the 'name' field). 
I've tried setting network.bind_host network.publish_host network.host to the hostnames, even tried adding PTR record in case ES does PTR lookups, but the host field is always an IP address.

Other, kinda related issues, is that the _cat/nodes list is sorted randomly at every refresh. In 1.7.4 it was also "random" - except that the randomness didn't change with every refresh, but only when nodes leave/join cluster.
</description><key id="132681397">16575</key><summary>host field in _cat/nodes is always an IP address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sysmonk</reporter><labels><label>:CAT API</label></labels><created>2016-02-10T12:08:37Z</created><updated>2016-02-14T14:30:02Z</updated><resolved>2016-02-14T14:30:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-10T15:37:21Z" id="182431063">I was able to reproduce this and see yeah, the hosts are ip addresses instead of hostnames.

&gt; Other, kinda related issues, is that the _cat/nodes list is sorted randomly at every refresh.

Probably makes sense to sort the nodes so they appear in a consistent order for the output
</comment><comment author="jasontedor" created="2016-02-14T04:06:51Z" id="183818190">It was an [intentional choice](https://github.com/elastic/elasticsearch/pull/12959) to remove hostname resolving for the underlying `TransportAddress`es. This leads to the `host` and `ip` fields always being equal here. As such, the `host` field here should probably just be removed from the cat nodes API; if you need a name one appears as the `name` field in the cat nodes API (already shown in your output); this is something that you can completely control via node configuration.

As for the sorting, the design of cat API is meant to be friendly with standard command line tools. In this case, if you need to sort on `name` you can just do:

```
$ curl -XGET locahost:9200/_cat/nodes | sort -k8
```

(until the default output changes to have the `host` field removed).
</comment><comment author="jasontedor" created="2016-02-14T14:30:02Z" id="183900323">Closed by #16656
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor attachment processor improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16574</link><project id="" key="" /><description>Fixed a test that failed a few times: http://build-us-00.elastic.co/job/es_g1gc_master_metal/30241/testReport/junit/org.elasticsearch.ingest.attachment/AttachmentProcessorTests/testEnglishTextDocumentWithRandomFields/

Removed unused NAME field and set the additional fields only once as a map of fields rather than using setFieldValue for each of them.
</description><key id="132675746">16574</key><summary>Minor attachment processor improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugin Ingest Attachment</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T11:41:22Z</created><updated>2016-02-13T10:01:04Z</updated><resolved>2016-02-10T12:20:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-02-10T11:53:51Z" id="182331966">LGTM, thanks for fixing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated parameter from field sort builder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16573</link><project id="" key="" /><description>This removes the deprecated ignore_unmapped parameter from field
sort builder.

This is in preparation of #16127
</description><key id="132668841">16573</key><summary>Remove deprecated parameter from field sort builder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T11:03:41Z</created><updated>2016-03-08T12:22:08Z</updated><resolved>2016-03-08T12:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-03-08T09:22:24Z" id="193685062">@jpountz or @cbuescher Have time for a review for this?
</comment><comment author="jpountz" created="2016-03-08T09:27:14Z" id="193686462">LGTM.

As a follow-up, maybe we should fix this parameter to be considered deprecated on the 2.x branch similarly to what is being done in #16910 cc @clintongormley.
</comment><comment author="clintongormley" created="2016-03-08T09:32:30Z" id="193687628">++ to adding deprecation logging to this in 2.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make GeoDistanceSortBuilder serializable, 2nd try</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16572</link><project id="" key="" /><description>Adds to GeoDistanceSortBuilder:
- equals
- hashcode
- writeto/readfrom
- moves xcontent parsing logic over
- adds roundtrip tests
- fixes roundtrip test for xcontent by keeping points just as geopoints not geohashes internally
- fixes xcontent parsing of ignore_malformed if coerce is set/unset
- adds exception to sortMode setter to avoid setting invalid sort modes

Relates to #15178

Apart from test fixes this is identical to #16151 which got a LGTM, was merged but then reverted on my request due to subtle test issues. Re-introducing the changes in a fresh PR.
</description><key id="132659204">16572</key><summary>Make GeoDistanceSortBuilder serializable, 2nd try</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T10:24:32Z</created><updated>2016-02-10T10:25:51Z</updated><resolved>2016-02-10T10:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Inner hits _source definition and multiple nesting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16571</link><project id="" key="" /><description>I added information on how to define _source fields for inner_hits. In addition, the current information on the retrieval of inner_hits for multiple nested documents is incorrect (see also https://github.com/elastic/elasticsearch/issues/11118).
</description><key id="132640230">16571</key><summary>Inner hits _source definition and multiple nesting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abulhol</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2016-02-10T08:59:20Z</created><updated>2016-02-28T21:42:19Z</updated><resolved>2016-02-28T21:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T18:23:33Z" id="183717715">Hi @abulhol 

Thanks for the PR.  The fact that we have to use relative paths here is a bug, and I've opened https://github.com/elastic/elasticsearch/issues/16653 to get this fixed in 3.0.

Would you mind changing your PR to be shorter: just a "NOTE: A bug in Elasticsearch 2.x means that..."  to flag the fact that relative paths must be used in the `_source` case.

thanks
</comment><comment author="abulhol" created="2016-02-17T15:55:57Z" id="185268734">Hi @clintongormley,
I am happy that I can contribute a bit to the project. I have changed the text in the PR as you requested.
</comment><comment author="clintongormley" created="2016-02-28T21:42:15Z" id="189950587">thanks @abulhol 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with dynamic multifields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16570</link><project id="" key="" /><description># All the fields that are being added to the index does not have a corresponding analyzed mapping.

I am using a template mapping for my indexes as 
`{
  "template": "log",
  "settings": {
    "number_of_shards": 20,
    "number_of_replicas": 1,
    "refresh_interval": "30s"
  },
  "mappings": {
    "error": {
      "_all": {
        "enabled": false
      },
      "dynamic_templates": [{
        "strings": {
          "match_mapping_type": "string",
          "path_match":"*",
          "mapping": {
            "type": "multi_field",
            "fields": {
              "analyzed": {
                "type": "string"
              }
            }
          }
        }
      }],
      "_ttl": {
        "enabled": true,
        "default": "7d"
      },
      "properties": {
        "browserName": {
          "type": "string",
          "index": "not_analyzed"
        },
        "browserVersion": {
          "type": "string",
          "index": "not_analyzed"
        },
        "category": {
          "type": "string",
          "index": "not_analyzed"
        },
        "event": {
          "type": "string",
          "index": "not_analyzed"
        },
        "ip": {
          "type": "string","index": "not_analyzed"
        },
        "osName": {
          "type": "string",
          "index": "not_analyzed"
        },
        "referer": {
          "type": "string","index": "not_analyzed"
        },
        "userAgent": {
          "type": "string","index": "not_analyzed"
        }

```
  }
}
```

  }
}
`
The expectation was to keep all the string fields as analyzed and also as raw in order to support both terms and search query.
On inserting documents into the indexes this resulted in the following mapping in the indexe -&gt; 

`{
  "log" : {
    "mappings" : {
      "error" : {
        "_all" : {
          "enabled" : false
        },
        "_ttl" : {
          "enabled" : true,
          "default" : 604800000
        },
        "dynamic_templates" : [ {
          "strings" : {
            "mapping" : {
              "type" : "multi_field",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                }
              }
            },
            "match_mapping_type" : "string",
            "path_match" : "*"
          }
        } ],
        "properties" : {
          "browserName" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "browserVersion" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "category" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "data" : {
            "properties" : {
              "text" : {
                "type" : "string",
                "index" : "no",
                "fields" : {
                  "analyzed" : {
                    "type" : "string"
                  }
                }
              },
              "version" : {
                "type" : "string",
                "index" : "no",
                "fields" : {
                  "analyzed" : {
                    "type" : "string"
                  }
                }
              }
            }
          },
          "event" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "ip" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "osName" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "osVersion" : {
            "type" : "string",
            "index" : "no",
            "fields" : {
              "analyzed" : {
                "type" : "string"
              }
            }
          },
          "referer" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "type" : {
            "type" : "string",
            "index" : "no",
            "fields" : {
              "analyzed" : {
                "type" : "string"
              }
            }
          },
          "userAgent" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      }
    }
  }
}`
</description><key id="132624336">16570</key><summary>Problem with dynamic multifields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raunakb94</reporter><labels /><created>2016-02-10T07:10:38Z</created><updated>2016-02-10T22:40:35Z</updated><resolved>2016-02-10T22:40:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raunakb94" created="2016-02-10T07:12:16Z" id="182233687">One of the payloads was 

{
    "userAgent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.103 Safari/537.36",
    "osVersion": "8.1",
    "osName": "Windows",
    "timestamp": "2016-02-08T14:17:43Z",
    "event": "Storage frame sent failed status [un-false] [ls-false] [v4-true] M0",
    "referer": "https://www.****.com/",
    "ip": "45.126.204.12",
    "type": "exception",
    "browserName": "Chrome",
    "category": "*******",
    "browserVersion": "48.0.2564.103",
    "data": {
      "version": "4.0",
      "text": "[we.co] load 0.204s\n[v4] load 0.028s"
    }
  }'
</comment><comment author="rjernst" created="2016-02-10T22:40:35Z" id="182614731">Dymamic templates are used for dynamic mappings, but it looks like you are explicitly creating mappings. You will need to set up the multi field yourself, for example, using setting up your `browserField`:

```
"browserVersion": {
    "type": "string",
    "index": "not_analyzed",
    "fields": {
        "analyzed": { "type": "string" }
    }
},
```

Note that the format for how multi fields are specified was change very long ago:
https://www.elastic.co/guide/en/elasticsearch/reference/current/_multi_fields.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposed additions to readme file for more information and clarity on&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16569</link><project id="" key="" /><description>&#8230; what elasticsearch can do and be used for

To make Elasticsearch more approachable to a larger number of users&#8212;from informed developers to curious passersby and new and interested visitors&#8212;I have added a new section to the Elasticsearch README. The section is entitled What to Use Elasticsearch for and When to Use It. As the name suggests, the new section, a brief paragraph, explains in plain and straightforward language what Elasticsearch is used for and when one should use it.
</description><key id="132597698">16569</key><summary>Proposed additions to readme file for more information and clarity on&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zsadler</reporter><labels><label>docs</label><label>v5.0.0-alpha2</label></labels><created>2016-02-10T03:24:03Z</created><updated>2016-04-26T11:32:19Z</updated><resolved>2016-04-26T11:29:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:54:52Z" id="206462989">Left a comment about this, if you want to address that I'll be happy to merge it.
</comment><comment author="zsadler" created="2016-04-21T00:42:32Z" id="212671923">Made the suggested change and pushed it up. Thanks for the merge!
</comment><comment author="jasontedor" created="2016-04-21T16:25:23Z" id="212994953">I have the same thoughts I had on #16595. I'm not sure I see the point, I don't like how it displaces more valuable content down the page, and many of the word-choices are non-specific.
</comment><comment author="zsadler" created="2016-04-21T23:22:52Z" id="213157056">Hi Jason,

I see your point but as a developer myself I'd heard of ElasticSearch from a backend dev who was adding it to a blog site we built at work. Wanting to understand what it was I came here to check it out. After I read the readme I found myself saying ok but what are other use cases besides what we were using it for on the blogs site? Now I'm totally new to this type of thing, being for the a frontend dev, and I had to do more research to figure out how this could actually be utilized. Call me a novice if you must but if there's one there must be more. By the other users post it seems he's trying to add clarity as well. Maybe there might be a need for this for people who don't have your level of understanding. I hope this clears up why I felt there was a need to add more.
</comment><comment author="jasontedor" created="2016-04-25T15:23:31Z" id="214391357">Sure, but this is just displacing a link to the [product page](https://www.elastic.co/products/elasticsearch) that already includes such detail and even includes a link to [use cases](https://www.elastic.co/use-cases)? Keep in mind that the GitHub repository is oriented towards developers.
</comment><comment author="clintongormley" created="2016-04-26T11:07:15Z" id="214704411">I'm tending to agree with @jasontedor here
</comment><comment author="jasontedor" created="2016-04-26T11:32:19Z" id="214712055">Thanks for the pull request @zsadler, but I think we prefer the README the way that it is currently written, leaving the product pages and use cases pages on the Elastic website for the content that you were looking to contribute here. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade instructions for Disabling allocation are wrong for 0.90.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16568</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html#_step_1_disable_shard_allocation_2

https://github.com/elastic/elasticsearch/edit/2.2/docs/reference/setup/cluster_restart.asciidoc

For 0.90.x to 1.x says to do  "cluster.routing.allocation.disable_allocation": false to disable allocation. I believe this is wrong. To disable allocation 'disable_allocation' should be set to true. And to enable allocation, 'disable_allocation' should be set to false.

I only mention this because its on the 2.2 and other upgrade instructions.
</description><key id="132585199">16568</key><summary>Upgrade instructions for Disabling allocation are wrong for 0.90.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msimos</reporter><labels><label>docs</label></labels><created>2016-02-10T01:29:33Z</created><updated>2016-02-13T18:03:54Z</updated><resolved>2016-02-13T18:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fail build if we define equals but not hashCode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16567</link><project id="" key="" /><description>That is like some kind of cardinal sin or something, right?

We had two violations though they weren't super likely to be keys in a hashmap
any time soon.
</description><key id="132584550">16567</key><summary>Fail build if we define equals but not hashCode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T01:23:34Z</created><updated>2016-02-10T03:00:51Z</updated><resolved>2016-02-10T03:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-10T02:24:51Z" id="182171077">I left some comments, one thing to think about. I trust your judgement, LGTM when you're ready.
</comment><comment author="nik9000" created="2016-02-10T02:27:57Z" id="182171494">&gt; I trust your judgement

Dangerous.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename three static class variables.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16566</link><project id="" key="" /><description>This PR renames the followings to fix typos.
- DEFAULT_NUMBER_COEERCE_POLICY -&gt; DEFAULT_NUMBER_COERCE_POLICY
- GLOABL_COMPATIBILITY_VERSION        -&gt; GLOBAL_COMPATIBILITY_VERSION
- JVM_BASE_PORT_OFFEST                       -&gt; JVM_BASE_PORT_OFFSET
</description><key id="132578868">16566</key><summary>Rename three static class variables.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-10T00:43:22Z</created><updated>2016-02-26T18:53:23Z</updated><resolved>2016-02-10T01:37:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-10T01:11:39Z" id="182155798">Looks good to me. I'll leave this in my queue so I can run the tests myself before merging like a good paranoid engineer but if another elastic member gets to it first feel free to assign it to yourself.
</comment><comment author="nik9000" created="2016-02-10T01:12:44Z" id="182155982">I've added the non-issue tag to this so it doesn't end up for consideration to be included in the release notes. Its a helpful fix, just not important for someone following along at the level of release notes.
</comment><comment author="jasontedor" created="2016-02-10T01:37:33Z" id="182162044">I verified that the build passes with this change set. Thanks for the contribution @dongjoon-hyun!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node not marked as client unless node.client: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16565</link><project id="" key="" /><description>From a cluster running ES 2.1.1 (or ES 1.x) with 8 nodes, two of which were client nodes:

``` http
GET _cluster/stats
```

produces:

``` json
"nodes": {
  "count": {
    "total": 8,
    "master_only": 2,
    "data_only": 4,
    "master_data": 0,
    "client": 0
  }
  ...
}
```

These nodes were set to being client nodes implicitly by using

``` yaml
node.master: false
node.data: false
```

It appears that the node.client count is only tripped when using `node.client: true`, but it should be `node.client || (node.master || node.data || node.ingest) == false`. This should be done at the source where ever we do that check initially. The annoying thing is that it kind of prevents other types of nodes from being dynamically added without "lying" if we change that check.
</description><key id="132570729">16565</key><summary>Node not marked as client unless node.client: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Cluster</label><label>enhancement</label></labels><created>2016-02-09T23:51:27Z</created><updated>2016-03-29T19:55:57Z</updated><resolved>2016-03-29T19:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-10T15:06:05Z" id="182412560">interesting point. It raises additional questions though, especially with the node ingest addition.

At the moment, when `node.client` is set to true a node cannot become master nor can hold data. Yet we would need to double check what the precedence is if one specifies also `node.data` or `node.master`... When it comes to `node.ingest`, the assumption is that client nodes can do ingestion, so `node.client` set to `false` doesn't turn ingestion off at the moment, you need to set `node.ingest` to `false` too. I tend to find the `node.client` setting confusing and hard to handle internally at this point, maybe we could keep the setting but make it a shortcut to what it needs to be internally?
</comment><comment author="pickypg" created="2016-02-10T15:11:44Z" id="182417330">&gt; I tend to find the `node.client` setting confusing and hard to handle internally at this point, maybe we could keep the setting but make it a shortcut to what it needs to be internally?

I completely agree and I love the idea.
</comment><comment author="javanna" created="2016-02-12T11:01:17Z" id="183275331">We discussed this during FixItFriday. We reached consensus that the `node.client` setting adds confusion and should be removed in favour of being explicit and say `node.master: false` and `node.data: false`. Being explicit would also make things less confusing around the `node.ingest` setting that was recently added (whether a client node does ingestion or not).

We don't really need to keep `node.client` around as a shortcut as we now validate settings and we can fail at startup from the next major version on if anybody uses the `node.client` setting. We need to be careful with node attributes as they don't have their own namespace as part of settings, so we should be careful that if somebody specifies `node.client` we fail and we don't treat it as a node attribute (or take the chance to give node attributes their namespace).

Last bit is we need to update documentation and code to make sure that client nodes are not mentioned anywhere. This will have an impact on the rest output of some apis like cluster stats, as `client` should not be used there anymore.
</comment><comment author="jtharpla" created="2016-02-12T18:56:30Z" id="183450131">Wouldn't node.master: false and node.data: false still result in an implicit client node? Seems like that that should still be included in the documentation, along with possible use cases where one might want such client nodes.  Or is there an intention to move away from the concept of client nodes altogether? (in other words is this more than a config change, but actually a shift in what architectures are recommended/supported?)
</comment><comment author="pickypg" created="2016-02-13T00:30:46Z" id="183541557">@jtharpla 

&gt; Wouldn't node.master: false and node.data: false still result in an implicit client node?

Yes. This is currently true and it will be true afterward.

&gt; Seems like that that should still be included in the documentation

I agree, but thankfully we recently [rewrote the node module in our documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/modules-node.html). This actually avoids using `node.client` for pretty much the reason of this ticket -- it's confusing.

&gt; Or is there an intention to move away from the concept of client nodes altogether?

No, we're keeping the idea around. Node clients serve a good and reasonable purpose for large clusters (at least for now).
</comment><comment author="javanna" created="2016-02-22T06:18:17Z" id="187030840">I am working on this, and made good progress, but we need to figure out how to update the cluster stats api output. We currently have `master_only`, `data_only`, `master_data` and `client`. Ingest nodes should be taken into account too, but then the combinations become more than just current 4 that we have. There can potentially be `master_ingest`, `data_ingest`, `master_data_ingest` &amp; `ingest_only` nodes in addition to the ones listed above. We could expose all of the different combinations, or do we only want to report the most common ones (e.g. master_ingest and data_ingest shouldn't get that popular)?

I would also rename `client` to `coordinating_only` to try and clarify what those nodes do.
</comment><comment author="pickypg" created="2016-02-22T06:23:35Z" id="187031491">I don't like not reporting them. Perhaps we can use an `EnumSet`, though I don't necessarily want to pass that over the wire? However, doing that (or equivalent!) should be much simpler than trying to pass around all of the various checks.
</comment><comment author="javanna" created="2016-02-22T06:29:28Z" id="187033563">Let's try and decide what we want to return from the REST layer first. @pickypg I take it that you would be ok with having 8 different node type combinations returned rather than the current 4?
</comment><comment author="pickypg" created="2016-02-22T06:31:01Z" id="187034213">@javanna I would. I think it makes it a lot easier to debug, if necessary. It sucks though because suddenly we're left with `2^n` combinations (where `n` is fortunately just `3` for now), but I'd rather than that semi-arbitrarily hiding some.

I expect some small scale instances to use `data_ingest` for example.
</comment><comment author="pickypg" created="2016-02-22T06:32:40Z" id="187034888">Maybe we can flip the REST layer around, similar to the Java layer. If the response is just an array of the node types that it _is_, then all of the combinations can be determined by the client. Then we can avoid the combinatorial nightmare (and empty implies `coordinating_only`).
</comment><comment author="javanna" created="2016-02-22T17:17:01Z" id="187275134">ok let's see what @clintongormley thinks about the proposed changes api-wise.
</comment><comment author="clintongormley" created="2016-02-29T13:54:27Z" id="190221563">The `_cluster/stats` API provides a summary (the details can be checked in the nodes-info API), so I think the right thing to do here is to report how many nodes there are in each role, eg:

```
"total": 8,
"master": 3,
"data": 5,
"ingest": 2,
....
```

I don't think we should return combinations like `master_data` as those will just explode as we add more roles.  The only weird one is "how many nodes are there that act only as clients" (because all nodes can act as clients).  Perhaps this can be represented with `client_only`

&gt; I tend to find the node.client setting confusing and hard to handle internally at this point, maybe we could keep the setting but make it a shortcut to what it needs to be internally?

I agree - easier than setting 3, 4, ... N other default roles to false.

&gt; We need to be careful with node attributes as they don't have their own namespace as part of settings, so we should be careful that if somebody specifies node.client we fail and we don't treat it as a node attribute (or take the chance to give node attributes their namespace).

Node attributes should really be moved into their own namespace so that we can be strict about settings, otherwise we're just guessing.
</comment><comment author="javanna" created="2016-02-29T15:42:50Z" id="190261974">On keeping `node.client` as a shortcut, I thought more about it and it may make things even more confusing, cause then people will have a setting in their configuration that is not in their final settings. And still people will keep on wondering "does client:true mean ingest:false or not?", note that at the moment a node client is an ingest node unless `node.ingest` is set to `false`, which is right or wrong depending on how you ask :) Being explicit seems the best way to remove ambiguity. I think the best way to go is to reject `node.client` with an error at startup. Would you agree @clintongormley ?

On the combinations, I follow your reasoning, but I am afraid that `master_only`, `ingest_only`, `data_only` and `coordinating_only` (I would get rid of the client terminology) may come in handy to have a quick overview, otherwise the summary may become less useful, especially with many nodes and multiple roles. Shall we maybe have these 4 plus the totals as you mentioned? Just throwing it out there, not sure it's a great idea though.
</comment><comment author="rjernst" created="2016-02-29T18:46:42Z" id="190328865">&gt; Node attributes should really be moved into their own namespace so that we can be strict about settings, otherwise we're just guessing.

I agree that node attributes should have their own namespace, but I also think that continuing to have confusing settings/semantics for node roles, instead of renaming/changing semantics, only hurts users. the argument I have heard supporting a need for keeping the old names is always "but there are so many users/articles out there that know/use this name". My counter argument is as users continue to grow, new users should not be forced into confusion by antiquated naming. We should always strive to rename things when appropriate to help the new users, and at the same time find ways to help old users migrate (eg forcing functions like failing on startup with old names, with helpful error messages).

I remember two discussions before about node "roles" that I will summarize here.  The first is having a single `node.roles` setting, which is an array of the allowed roles on a node.  For example, having `node.roles: ["master_eligible", "data", "coordinating"]`. This would make it very clear, in a single setting, all the roles the node can play in the cluster. The other idea was about being explicit with the combinations that are allowed. For example, `node.type: "master_data_coordinating"`. The problem with this was the explosion of combinations as new node types are added, but I included it here for completeness.
</comment><comment author="javanna" created="2016-02-29T18:56:11Z" id="190331981">No need to discuss the node attributes problem here, everybody agrees, we will address it in a followup PR, my comment was to remember that we have to take those into account if we want to remove node.client or client will become a node attribute which is bad.

I think we should make baby steps and get rid of `node.client` that is confusing, especially since we have ingest nodes too. I would love to move forward solving the problem reported here, as I said I have an upcoming PR and the main issue was what to do with cluster stats api, which @clintongormley replied to. We can reason about renaming etc. later I think. PR coming tomorrow :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore should check disk availability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16564</link><project id="" key="" /><description>Currently, restoring from a snapshot can lead to running out of disk space because it does not check the amount of available space on nodes before executing.

We should check for available disk space before attempting to restore.
</description><key id="132568672">16564</key><summary>Restore should check disk availability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2016-02-09T23:36:31Z</created><updated>2017-03-14T00:35:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Conflicting doc values in mappings causes shard initialization failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16563</link><project id="" key="" /><description>I found a way to create conflicting mappings using doc values in Elasticsearch 2.2.0.

Steps to reproduce:

```
# Create index with doc values enabled
curl -XPUT localhost:9200/index -d '{
    "mappings": {
        "type1": {
            "properties": {
                "field_name": {
                    "type": "string",
                    "index": "not_analyzed",
                    "doc_values": true
                }
            }
        }
    }
}'

# Add type with doc values disabled
# These must be done as separate requests, otherwise ES recognizes that they are conflicting
curl -XPUT localhost:9200/index/_mapping/type2 -d '{
    "properties": {
        "field_name": {
            "type": "string",
            "index": "not_analyzed",
            "doc_values": false
        }
    }
}'
```

Getting the mappings returns conflicting doc values for this index: one field has doc values disabled, the other has doc values enabled.

```
{
   "index": {
      "mappings": {
         "type2": {
            "properties": {
               "field_name": {
                  "type": "string",
                  "index": "not_analyzed",
                  "doc_values": false
               }
            }
         },
         "type1": {
            "properties": {
               "field_name": {
                  "type": "string",
                  "index": "not_analyzed"
               }
            }
         }
      }
   }
}
```

This does not cause problems for the cluster until the nodes are restarted. When the nodes try to initialize the index with conflicting mappings it _can_ cause nodes to enter an endless loop of trying (and failing) to allocate shards:

```
[2016-02-09 16:04:56,313][WARN ][indices.cluster          ] [Fantasia] [index] failed to add mapping [type1], source [{"type1":{"properties":{"field_name":{"type":"string","index":"not_analyzed"}}}}]
java.lang.IllegalArgumentException: Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:170)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:402)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:411)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:314)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:388)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:348)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:164)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:600)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-09 16:04:56,740][WARN ][indices.cluster          ] [Fantasia] [[index][4]] marking and sending shard failed due to [failed to update mappings]
java.lang.IllegalArgumentException: Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:170)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:402)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:411)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:314)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:388)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:348)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:164)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:600)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-09 16:04:56,743][WARN ][cluster.action.shard     ] [Fantasia] [index][4] received shard failed for [index][4], node[tf4WmuoCQHKoHctABxTNwg], [P], v[89], s[INITIALIZING], a[id=zgBVat8RTKaWToeKvEQ9tg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-09T22:04:54.620Z], details[failed to update mappings, failure IllegalArgumentException[Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]]]], indexUUID [Nn0u7aW-RNiSyba5s1YrsQ], message [failed to update mappings], failure [IllegalArgumentException[Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]]]
java.lang.IllegalArgumentException: Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:170)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:402)
    at org.elasticsearch.index.mapper.MapperService.checkMappersCompatibility(MapperService.java:411)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:314)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:388)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:348)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:164)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:600)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-09 16:04:56,745][WARN ][indices.cluster          ] [Fantasia] [[index][3]] marking and sending shard failed due to [master [{Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed]
[2016-02-09 16:04:56,746][WARN ][cluster.action.shard     ] [Fantasia] [index][3] received shard failed for [index][3], node[tf4WmuoCQHKoHctABxTNwg], [P], v[140], s[STARTED], a[id=LW-dtt99Sq6fDsfAMy7qBw], indexUUID [Nn0u7aW-RNiSyba5s1YrsQ], message [master [{Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed], failure [Unknown]
[2016-02-09 16:04:56,746][WARN ][indices.cluster          ] [Fantasia] [[index][1]] marking and sending shard failed due to [master [{Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed]
[2016-02-09 16:04:56,746][WARN ][cluster.action.shard     ] [Fantasia] [index][1] received shard failed for [index][1], node[tf4WmuoCQHKoHctABxTNwg], [P], v[134], s[STARTED], a[id=xuQ5AlXZR9SoR-5zeb161Q], indexUUID [Nn0u7aW-RNiSyba5s1YrsQ], message [master [{Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed], failure [Unknown]
[2016-02-09 16:04:56,747][WARN ][cluster.action.shard     ] [Fantasia] [index][4] received shard failed for [index][4], node[tf4WmuoCQHKoHctABxTNwg], [P], v[89], s[INITIALIZING], a[id=zgBVat8RTKaWToeKvEQ9tg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-09T22:04:54.620Z], details[failed to update mappings, failure IllegalArgumentException[Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]]]], indexUUID [Nn0u7aW-RNiSyba5s1YrsQ], message [master {Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-09 16:04:56,747][WARN ][cluster.action.shard     ] [Fantasia] [index][0] received shard failed for [index][0], node[tf4WmuoCQHKoHctABxTNwg], [P], v[117], s[INITIALIZING], a[id=L2V8OQcFSZmi8X55QereqQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-09T22:04:54.620Z], details[failed to update mappings, failure IllegalArgumentException[Mapper for [field_name] conflicts with existing mapping in other types:
[mapper [field_name] has different [doc_values] values, cannot change from disabled to enabled]]]], indexUUID [Nn0u7aW-RNiSyba5s1YrsQ], message [master {Fantasia}{tf4WmuoCQHKoHctABxTNwg}{127.0.0.1}{127.0.0.1:9300} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
```

The above failure to allocate shards is not guaranteed, but I have always been able to get it to happen by restarting the node several times.
</description><key id="132558363">16563</key><summary>Conflicting doc values in mappings causes shard initialization failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KeithTr</reporter><labels /><created>2016-02-09T22:45:48Z</created><updated>2016-02-10T22:43:13Z</updated><resolved>2016-02-10T22:43:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T22:43:13Z" id="182616396">This was just fixed in 2.x: #16264. Currently that will be 2.3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add on_failure exception metadata to ingest document for verbose simulate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16562</link><project id="" key="" /><description>The Simulate with verbose feature was not kept up to date with changes in master around on_failure metadata. This meant simulations did not behave as they would in reality.

thanks to @inqueue for reporting this find!
</description><key id="132557170">16562</key><summary>add on_failure exception metadata to ingest document for verbose simulate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T22:40:33Z</created><updated>2016-03-24T23:22:24Z</updated><resolved>2016-03-24T23:22:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-10T16:57:06Z" id="182478434">I wish there was a way to not make this mistake again, for instance being able to reuse the code instead of copying for simulate. That doesn't seem trivial though... but making methods public only so simulate works doesn't feel great either, it's error prone. @martijnvg WDYT? maybe it is time to try and refactor simulate a bit? Or we can always get this fix in and then refactor things.
</comment><comment author="talevy" created="2016-02-10T17:03:35Z" id="182481942">@javanna I feel exactly the same way

&gt; Or we can always get this fix in and then refactor things.

that is how I felt about it yesterday, that is why I pushed out this PR. I do agree there 
could be a way to reuse more code so that this doesn't happen. The current strategy does not 
really lend itself nicely to that
</comment><comment author="martijnvg" created="2016-02-10T21:07:13Z" id="182582301">@talevy @javanna yes, this isn't nice and shows that we should fix the code duplication. 

Just an idea, what if instead of using the pipeline kept by the `PipelineStore` the simulate creates its own pipeline instance. This gives us the oppurtunity to decorate the actual processors with a processor that makes a copy of the ingest document after each processor it wraps has executed. This `IngestDocumentTrackerProcessor` implementation allows us remove logic from the `SimulateExecutionService` and then this change isn't needed. 

``` java
    class IngestDocumentTrackerProcessor implements Processor {

        private final Processor actualProcessor;
        private final List&lt;SimulateProcessorResult&gt; processorResultList;

        IngestDocumentTrackerProcessor(Processor actualProcessor, List&lt;SimulateProcessorResult&gt; processorResultList) {
            this.actualProcessor = actualProcessor;
            this.processorResultList = processorResultList;
        }

        @Override
        public void execute(IngestDocument ingestDocument) throws Exception {
            try {
                actualProcessor.execute(ingestDocument);
                processorResultList.add(new SimulateProcessorResult(actualProcessor.getTag(), new IngestDocument(ingestDocument)));
            } catch (Exception e) {
                processorResultList.add(new SimulateProcessorResult(actualProcessor.getTag(), e));
                throw e;
            }
        }

        @Override
        public String getType() {
            return actualProcessor.getType();
        }

        @Override
        public String getTag() {
            return actualProcessor.getTag();
        }
    }
```
</comment><comment author="talevy" created="2016-02-10T21:28:51Z" id="182589255">This is an interesting idea, I will try out it out and see how it fits
</comment><comment author="talevy" created="2016-02-10T22:53:30Z" id="182619409">@martijnvg, your intuition to leverage the Processor interface was pretty spot on. I quickly tried to get it to work, and I think it is mostly there. let me know what you think! I just had to make a recursive modification to your proposal for wrapping processors within compound-processor types.

code is updated with it
</comment><comment author="talevy" created="2016-03-21T23:47:16Z" id="199539800">@martijnvg I've rebased, and added more tests to this PR. let me know what you think!
</comment><comment author="martijnvg" created="2016-03-22T09:24:50Z" id="199713749">@talevy Looks good. Left a note about the compound processor decorating logic.
</comment><comment author="martijnvg" created="2016-03-24T22:07:51Z" id="201045309">:+1: The decorating logic is now much easier! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No documentation on what stopwords are in each set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16561</link><project id="" key="" /><description>For instance, there is no documentation of what words are in the `_english_` stop words set.
</description><key id="132549716">16561</key><summary>No documentation on what stopwords are in each set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">soaxelbrooke</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-02-09T22:05:40Z</created><updated>2016-02-17T15:18:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T17:57:53Z" id="183714799">Yeah, I've tried to put together a list before, but they're quite difficult to round up in Lucene.
</comment><comment author="hjc" created="2016-02-15T19:03:05Z" id="184348626">I can certainly confirm that these are quite hard to round up. We had a client demand the list for all stop words we were using for every language configured. [Here](https://gist.github.com/hjc1710/6f0aa01bb436249ab3c6) is a quick description on what it took for me to find, and prove, that the list of stop words that ES is using come from `org.apache.lucene.analysis.snowball` and where to find the files. As you can see, it wasn't fun.

Maybe this is something I can take if I have some free time? Seems like a great beginner ticket and I have experience doing this already!
</comment><comment author="clintongormley" created="2016-02-17T15:18:06Z" id="185248792">@hjc1710 please do - would be great to have this info in the docs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Like This Query does not work with aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16560</link><project id="" key="" /><description>More Like This Query with docs as input does not with if you use an alias as the index for the doc
</description><key id="132547803">16560</key><summary>More Like This Query does not work with aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reevesch</reporter><labels /><created>2016-02-09T21:57:11Z</created><updated>2016-02-09T22:29:30Z</updated><resolved>2016-02-09T22:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-09T22:29:30Z" id="182107897">This looks somewhat similar to the question raised in this issue: #13348.
Please take a look and feel free to reopen this issue if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove license header check files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16559</link><project id="" key="" /><description>These look like they were used with an older license header check. I suspect
these are obsolete.
</description><key id="132533497">16559</key><summary>Remove license header check files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T21:01:56Z</created><updated>2016-02-09T21:25:39Z</updated><resolved>2016-02-09T21:25:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-09T21:21:11Z" id="182072239">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to create repository Unknown [repository] type [azure]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16558</link><project id="" key="" /><description>I just installed the azure plugin and I'm getting this weird error when I try to create the repository:

``` sh
curl -XPUT 'http://localhost:9200/_snapshot/azure_backup' -d '{"type": "azure", "settings": { "container": "backups"}}'
```

returns

```
{"error":{"root_cause":[{"type":"repository_exception","reason":"[azure_backup] failed to create repository"}],"type":"repository_exception","reason":"[azure_backup] failed to create repository","caused_by":{"type":"illegal_argument_exception","reason":"Unknown [repository] type [azure]"}},"status":500}
```

I don't think its about my the configuration as I was getting the same error for an S3 type repository, installed the S3 plugin and started getting  "No bucket defined for s3 gateway" which makes sense to me.

I'm just using the repo for EC snapshots and my config is pretty straightforward:

```
cloud:
   azure:
      management:
          subscription.id: xxxxx
      storage:
          accountfilipe:
              account: elasticsearch
              key: xxxxx
```

Please help &#128553;&#128553;&#128553;
</description><key id="132533074">16558</key><summary>Failed to create repository Unknown [repository] type [azure]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jorgemrsantos</reporter><labels /><created>2016-02-09T21:00:07Z</created><updated>2016-02-10T14:41:58Z</updated><resolved>2016-02-10T14:03:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-09T21:57:33Z" id="182095506">Can you gives your logs please? What gives `GET _cat/plugins?v` please?
</comment><comment author="jorgemrsantos" created="2016-02-10T13:25:00Z" id="182371629">Here goes

```
[2016-02-09 20:54:17,883][WARN ][repositories             ] [Caregiver] failed to create repository [azure_backup]
RepositoryException[[azure_backup] failed to create repository]; nested: IllegalArgumentException[Unknown [repository] type [azure]];
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:411)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unknown [repository] type [azure]
    at org.elasticsearch.common.util.ExtensionPoint$SelectedType.bindType(ExtensionPoint.java:146)
    at org.elasticsearch.repositories.RepositoryTypesRegistry.bindType(RepositoryTypesRegistry.java:49)
    at org.elasticsearch.repositories.RepositoryModule.configure(RepositoryModule.java:58)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:61)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:105)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:143)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    ... 9 more
[2016-02-09 20:54:17,887][INFO ][rest.suppressed          ] /_snapshot/azure_backup Params: {repository=azure_backup}
RepositoryException[[azure_backup] failed to create repository]; nested: IllegalArgumentException[Unknown [repository] type [azure]];
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:411)
    at org.elasticsearch.repositories.RepositoriesService.registerRepository(RepositoriesService.java:368)
    at org.elasticsearch.repositories.RepositoriesService.access$100(RepositoriesService.java:55)
    at org.elasticsearch.repositories.RepositoriesService$1.execute(RepositoriesService.java:110)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unknown [repository] type [azure]
    at org.elasticsearch.common.util.ExtensionPoint$SelectedType.bindType(ExtensionPoint.java:146)
    at org.elasticsearch.repositories.RepositoryTypesRegistry.bindType(RepositoryTypesRegistry.java:49)
    at org.elasticsearch.repositories.RepositoryModule.configure(RepositoryModule.java:58)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:61)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:105)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:143)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
    at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
    ... 9 more
```

`GET _cat/plugins?v` returns this:

```
name      component   version type url 
Caregiver cloud-aws   2.1.1   j        
Caregiver cloud-azure 2.1.1   j   
```
</comment><comment author="dadoonet" created="2016-02-10T14:03:04Z" id="182388286">It fails because your azure settings are incorrect. It should be:

``` yml
cloud.azure.storage.account: elasticsearch
cloud.azure.storage.key: xxxxx
```

Read here: https://www.elastic.co/guide/en/elasticsearch/plugins/2.1/cloud-azure-repository.html

Note that if you upgrade to 2.2, you can use then what you defined. See: https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-azure-repository.html
</comment><comment author="jorgemrsantos" created="2016-02-10T14:41:58Z" id="182401704">That error message is awfully misleading!

Thank you @dadoonet, changed my settings to what you suggested and it started working.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hide null-valued metadata fields from WriteableIngestDocument#toXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16557</link><project id="" key="" /><description>Currently, null-valued metadata fields show up when rendering IngestDocuments in responses.

e.g.

``` json
{
   "docs": [
      {
         "processor_results": [
            {
               "tag": "processor_2",
               "doc": {
                  "_parent": null,
                  "_timestamp": null,
                  "_routing": null,
                  "_id": "_id",
                  "_index": "_index",
                  "_ttl": null,
                  "_type": "_type",
                  "_source": {
                     "flags": "new|hot|super|fun|interesting"
                  },
                  "_ingest": {
                     "timestamp": "2016-02-09T20:05:27.575+0000"
                  }
               }
            },
...
```

This PR removes such values from the response, rendering the above as follows:

``` json
{
   "docs": [
      {
         "processor_results": [
            {
               "tag": "processor_2",
               "doc": {
                  "_index": "_index",
                  "_id": "_id",
                  "_type": "_type",
                  "_source": {
                     "flags": "new|hot|super|fun|interesting"
                  },
                  "_ingest": {
                     "timestamp": "2016-02-09T20:51:21.137+0000"
                  }
               }
            },
...
```
</description><key id="132530700">16557</key><summary>hide null-valued metadata fields from WriteableIngestDocument#toXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T20:52:23Z</created><updated>2016-02-10T16:29:18Z</updated><resolved>2016-02-10T16:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-10T06:34:08Z" id="182225091">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Teach reindex to retry on rejection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16556</link><project id="" key="" /><description>And count it in the status too!

Closes #16093
</description><key id="132520879">16556</key><summary>Teach reindex to retry on rejection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T20:10:15Z</created><updated>2016-03-16T19:46:52Z</updated><resolved>2016-02-10T19:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-09T20:10:58Z" id="182042041">@dakrone next reindex thing!

@danielmitterdorfer this uses your retry!
</comment><comment author="danielmitterdorfer" created="2016-02-10T09:03:34Z" id="182264048">&gt; @danielmitterdorfer this uses your retry!

Yay, I like it. :)
</comment><comment author="nik9000" created="2016-02-10T18:12:30Z" id="182509111">@dakrone this is ready for round two I think.
</comment><comment author="dakrone" created="2016-02-10T18:26:47Z" id="182516693">LGTM, thanks Nik!
</comment><comment author="nik9000" created="2016-02-10T18:29:01Z" id="182517356">&gt; LGTM, thanks Nik!

Thank you so much for reviewing it! Its so nice to have this moving quickly again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Varrying numbers of results from scan and scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16555</link><project id="" key="" /><description>Right off the bat, here's a little info:

```
$ uname -a
Linux jj-big-box 3.19.0-49-generic #55~14.04.1-Ubuntu SMP Fri Jan 22 11:24:31 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

$ curl -XGET 'localhost:9200'
{
  "status" : 200,
  "name" : "Bast",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.7.4",
    "build_hash" : "0d3159b9fc8bc8e367c5c40c09c2a57c0032b32e",
    "build_timestamp" : "2015-12-15T11:25:18Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

I've seen some similar posts, but I've had trouble squaring their results with mine. I've noticed that I have do not receive consistent numbers of documents when running scan and scroll in elastic search. Here is python code exhibiting the behavior (hopefully the use of sockets is not too confusing...at first I was trying to make sure the problem had nothing to do with elasticsearch-py and that's why I went the route of raw code):

``` python
import socket
import httplib
import json
import re

HOST = 'localhost'
PORT = 9200

CRLF = "\r\n\r\n"

init_msg = """
GET /index/document/_search?search_type=scan&amp;scroll=15m&amp;timeout=30&amp;size=10 HTTP/1.1
Host: localhost:9200
Accept-Encoding: identity
Content-Length: 94
connection: keep-alive

{"query": {"regexp": {"date_publ": "2001.*"}}, "_source": ["doc_id", "date_publ", "abstract"]}
"""

scroll_msg = """
GET /_search/scroll?scroll=15m HTTP/1.1
Host: localhost:9200
Accept-Encoding: identity
Content-Length: {sid_length}
connection: keep-alive

{sid}
"""

def get_stream(host, port, verbose=True):
    # Set up the socket.
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.connect((HOST, PORT))
    s.send(init_msg)

    # Fetch scroll_id and total number of hits.
    data = s.recv(4096)
    payload = json.loads(data.split(CRLF)[-1])
    sid = payload['_scroll_id']
    total_hits = payload['hits']['total']

    if verbose:
        print "Total hits: {}".format(total_hits)

    # Iterate through results.
    while True:
        # Send data request.
        msg = scroll_msg.format(sid=sid, sid_length=len(sid))
        s.send(msg)

        # Fetch the response body.
        data = s.recv(1024)
        header, body = data.split(CRLF)
        content_length = int(re.findall('Content-Length: (\d*)', header)[0])
        while len(body) &lt; content_length:
            body += s.recv(1024)

        # Extract results from response body.
        payload = json.loads(body)
        sid = payload['_scroll_id']
        hits = payload['hits']['hits']

        #print payload['_shards']

        if not hits:
            break

        for hit in hits:
            yield hit


for count, _ in enumerate(get_stream(HOST, PORT), 1): pass

print count
```

When I run that a few times, I get the following:

```
$ python new_test.py 
Total hits: 56366
11650
$ python new_test.py 
Total hits: 56366
24550
$ python new_test.py 
Total hits: 56366
8550
```

Now if I un-comment the line `#print payload['_shards']`, the ended up being the following during one run:

```
Total hits: 56366
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}

...

{u'successful': 4, u'failed': 0, u'total': 4}
{u'successful': 4, u'failed': 0, u'total': 4}
{u'successful': 4, u'failed': 0, u'total': 4}
{u'successful': 4, u'failed': 0, u'total': 4}
{u'successful': 4, u'failed': 0, u'total': 4}
{u'successful': 4, u'failed': 0, u'total': 4}
28110
```

and ended up as the following the next run:

```
Total hits: 56366
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}

...

{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 5, u'failed': 0, u'total': 5}
{u'successful': 3, u'failed': 0, u'total': 3}
{u'successful': 1, u'failed': 0, u'total': 1}
{u'successful': 0, u'failed': 0, u'total': 0}
56366
```

_Note_: The last run apparently returned _all_ documents. This is the first time I've seen this during this experimentation.

Does anyone have any idea what's going on here? As far as I can tell, I never run into these issues when not doing the regular expression as part of the search, but other than that I'm at a loss.

Thanks for any help!
</description><key id="132520031">16555</key><summary>Varrying numbers of results from scan and scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ApproximateIdentity</reporter><labels><label>:Scroll</label><label>discuss</label></labels><created>2016-02-09T20:07:45Z</created><updated>2017-04-26T11:13:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T14:32:37Z" id="183676958">Regexp can be a heavy query, and you have a timeout of 30 milliseconds...I get the same results when trying this locally.  Setting the timeout higher solves the problem.

Wondering if we should change the behaviour if timeouts occur?
</comment><comment author="colings86" created="2017-04-26T10:58:01Z" id="297356370">@clintongormley is this issue still relevant?</comment><comment author="clintongormley" created="2017-04-26T11:13:05Z" id="297362554">Yes, I think we should change the behaviour of scroll - it seems wrong that we drop results here.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect default value documented for cluster.routing.allocation.allow_rebalance in 2.1 documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16554</link><project id="" key="" /><description>The documentation located here: [Cluster Level Shard Allocation](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/shards-allocation.html)

States that the default setting for `cluster.routing.allocation.allow_rebalance`is "`always"`
It actually appears to be `"indices_all_active"`
</description><key id="132517085">16554</key><summary>Incorrect default value documented for cluster.routing.allocation.allow_rebalance in 2.1 documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctf2009</reporter><labels /><created>2016-02-09T19:55:28Z</created><updated>2016-02-13T14:17:14Z</updated><resolved>2016-02-13T14:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix _aliases filter and null parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16553</link><project id="" key="" /><description>Fixes 2 issues with the REST `_aliases` endpoint:

`POST /_aliases` ignores the `filter` when filtered aliases are created: Closes #16549
`POST /_aliases` throws NullPointerException instead of validation error with proper error message when `alias` is specified as `null`: Closes #16547
</description><key id="132515633">16553</key><summary>Fix _aliases filter and null parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Aliases</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T19:50:58Z</created><updated>2016-02-09T20:46:21Z</updated><resolved>2016-02-09T20:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-09T20:27:21Z" id="182046962">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows service: Use JAVA_HOME environment variable in registry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16552</link><project id="" key="" /><description>This allows for updating Java without having to re-install the service.

Closes #13521

@Mpdreamz @russcam mind reviewing?
</description><key id="132512621">16552</key><summary>Windows service: Use JAVA_HOME environment variable in registry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T19:39:40Z</created><updated>2016-02-13T13:25:35Z</updated><resolved>2016-02-11T17:36:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="russcam" created="2016-02-09T23:02:13Z" id="182125083">LGTM :+1: 
</comment><comment author="Mpdreamz" created="2016-02-10T06:06:44Z" id="182214427">Lgtm but we should document that updating across JVM types is still unsupported right?

If this resolves to a JRE and the updated JAVA_HOME is SE the dll ref might still be broken. 
</comment><comment author="gmarz" created="2016-02-10T19:40:29Z" id="182544681">@Mpdreamz that's a great point, and yea we should definitely document it.  

Linking https://github.com/elastic/elasticsearch/issues/16455 as a reminder to mention this when we update the docs.
</comment><comment author="gmarz" created="2016-02-11T22:45:15Z" id="183089693">Merged to master and back ported to 2.0-2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent/Child and Nested field support for query_string queries </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16551</link><project id="" key="" /><description>I am requesting that query_string queries be able to specify parent fields as part of the query_string query. This should not cause any scaling challenges as parent-child mapping ensures that both documents will be located on the same shard, the syntax is well supported for nested data, and this is an intuitive use case for parent mappings.

Currently there are two terrible ways to return Children based on search criteria for Parents:
1) use the has_parent query construct - fine if you are able to build a single query use case, but virtually unusable if you are providing functionality to a user-facing search UI and leveraging query_string
2) nest the parent metadata into the each child - defeats the whole purpose of parent-child mapping, and requires every child to be updated anytime a parent record changes

If there is some technical reason this cannot be accomplished, please advise. If this is a feature others could use, please leave a +1 to get this issue some attention.

Thanks,
D
</description><key id="132495868">16551</key><summary>Parent/Child and Nested field support for query_string queries </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drush</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-02-09T18:34:20Z</created><updated>2017-07-21T18:10:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewvc" created="2016-02-09T19:44:29Z" id="182027048">+1 . 

I'm wondering about the complexity here. AFAIK the query string query is a Lucene feature exposed by ES, and parent/child is ES only. I'm not sure what the technical implications of doing this would be, since I think it would require moving the query string syntax code into the ES core, which IIRC is in Lucene core ATM.
</comment><comment author="Raydius" created="2016-02-09T20:50:18Z" id="182059471">+1 definitely helpful
</comment><comment author="mabeller" created="2016-02-09T21:48:42Z" id="182092537">+1
</comment><comment author="dotnetallday" created="2016-02-12T01:55:33Z" id="183147001">+1
</comment><comment author="clintongormley" created="2016-02-13T13:39:49Z" id="183665793">I've updated the title to include access to nested fields (originally requested in https://github.com/elastic/elasticsearch/issues/11322).

What I'd be interested in seeing is what the query string syntax would look like for accessing parent/child/nested docs.
</comment><comment author="drush" created="2016-02-19T22:06:11Z" id="186429479">@clintongormley I think the syntax would be exactly as it is for [Inner Objects](https://www.elastic.co/guide/en/elasticsearch/guide/current/complex-core-fields.html#inner-objects) which use simple dot-notation to reference deeper elements of the indexed JSON source.  The root field should be the name of the parent type, so let's say you have Company and Employee types, where Company is parent to the Employee, I should be able to query:

`/index/Employee/_search`
`... { query_string: "name:bob* AND Company.location:Boston AND Company.employees.women:&gt;=15" }`

In this contrived example, location is a field of the parent Company type, and employees.women is a inner object of the Company type.  Name is a field of the Employee type.

query_string queries already support inner field/nested object query specifiers via dot-notation.

I have reviewed #11322 and Nested Documents are a significantly different use case than 'inner fields' which is the reference I intended to make in my intro.  The largest difference is that the parent will always be a has_one type relationship to the child - which means that there isn't some type of document reduction necessary to calculate score, etc - as appears may be the case in the nested document scenario.

So, to keep the complexity and effort for this case low, I'd suggest not to co-mingle it with the potentially more challenging Nested Documents scenario.
</comment><comment author="Alex1sz" created="2016-02-25T19:48:23Z" id="188949357">+1
</comment><comment author="bipin-nag" created="2016-03-24T04:11:53Z" id="200654635">+1
</comment><comment author="rmm5t" created="2016-09-19T19:22:59Z" id="248095564">&gt; What I'd be interested in seeing is what the query string syntax would look like for accessing parent/child/nested docs.

Chiming in here to reiterate my comment from https://github.com/elastic/elasticsearch/issues/11322#issuecomment-248093863

&gt; Overall, I'd really just like to see an ability to narrow a query string search to one particular embedded object. I'd like to see a syntax that looked like this:
&gt; 
&gt; `children:(gender:male AND age:&gt;=18 AND age:&lt;=25)`
&gt; 
&gt; Otherwise, there's no way to use the query string syntax and (in this particular US-centric example) find parents who have children who should be signed up for the US Selective Service System.
</comment><comment author="jefyjiang" created="2016-11-04T00:52:39Z" id="258317299">+1
</comment><comment author="mihir83in" created="2016-11-21T09:48:58Z" id="261891515">+1</comment><comment author="kchudinov" created="2017-01-04T16:07:06Z" id="270409291">+1</comment><comment author="Jackalx" created="2017-01-04T16:07:13Z" id="270409331">+1</comment><comment author="anosulchik" created="2017-01-04T16:09:06Z" id="270409857">+1</comment><comment author="OleksandrObidniak" created="2017-01-04T16:09:51Z" id="270410066">+1</comment><comment author="dnetrebenko-smartling" created="2017-01-04T16:10:01Z" id="270410111">+1</comment><comment author="nibum2001" created="2017-01-05T12:34:18Z" id="270634279">+1</comment><comment author="mrkamel" created="2017-02-10T15:48:27Z" id="278979960">+1</comment><comment author="macks22" created="2017-03-03T20:12:37Z" id="284058373">+1</comment><comment author="prasadkhandagale" created="2017-04-11T10:15:11Z" id="293214144">+1
</comment><comment author="tmpace" created="2017-04-20T16:25:07Z" id="295800721">+1</comment><comment author="mattcaldwell" created="2017-05-09T01:39:46Z" id="300037339">+1</comment><comment author="daniel-keller" created="2017-06-20T20:45:11Z" id="309886079">+1</comment><comment author="nirajpatel" created="2017-07-21T18:10:14Z" id="317073282">+1</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Has child query forces default similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16550</link><project id="" key="" /><description>D&#233;j&#224; vu of #4977 on 2.2.0. The `has_child` query seems to only use the default similarity when scoring child documents. Even the built-in non-default similarities don't seem to be picked up. See the curl statements below for reproducing. I reached out to @imotov who confirmed this issue on `master`.

No promises on a PR this time but we'll start looking into it and see how far we can get.

`curl` statements for reproducing:

```
# delete index
curl -XDELETE 'http://localhost:9200/has_child_similarity_test/?pretty=true'

# create index with proper parent/child mappings
curl -XPUT 'http://localhost:9200/has_child_similarity_test/?pretty=true' -d '{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    }
  },
  "mappings": {
    "author": {
      "properties": {
        "name": { "type": "string" }
      }
    },
    "post": {
      "_parent": { "type": "author" },
      "properties": {
        "content": { "type": "string" }
      }
    }
  }
}'

# add data
curl -XPUT 'http://localhost:9200/has_child_similarity_test/author/1?pretty=true' -d '{
   "name": "George P. Stathis"
}'
curl -XPUT 'http://localhost:9200/has_child_similarity_test/post/1?parent=1&amp;pretty=true' -d '{
  "content": "Lorem ipsum dolor sit amet."
}'
curl -XPUT 'http://localhost:9200/has_child_similarity_test/post/2?parent=1&amp;pretty=true' -d '{
  "content": "Lorem ipsum dolor sit amet again!"
}'

echo " "
echo "Sleep for two secs to allow for indexing"
sleep 2

# Search posts directly
echo " "
echo "Run query against child docs"
curl 'http://localhost:9200/has_child_similarity_test/post/_search?pretty=1' -d '{
  "query" : {
    "query_string" : {
      "default_field": "content",
      "query" : "Lorem ipsum"
    }
  }
}' | grep '_score'

echo " "
echo "Two docs should have matched with scores 0.61871845 and 0.53033006"

# Search with has_child
echo " "
echo "Run same query as has_child query in 'sum' mode"
curl 'http://localhost:9200/has_child_similarity_test/_search?pretty=1' -d '{
  "query": {
    "has_child": {
      "type": "post",
      "query": {
        "query_string": {
          "query": "Lorem ipsum"
        }
      },
      "score_mode": "total"
    }
  }
}' | grep '_score'

echo " "
echo "One parent doc should have matched with score 1.1490486 (i.e. 0.61871845 + 0.53033006)"
sleep 5

# delete index
echo " "
echo "Start over and re-index posts using BM25 similarity"
curl -XDELETE 'http://localhost:9200/has_child_similarity_test/?pretty=true'

# create index with proper parent/child mappings
curl -XPUT 'http://localhost:9200/has_child_similarity_test/?pretty=true' -d '{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    }
  },
  "mappings": {
    "author": {
      "properties": {
        "name": { "type": "string" }
      }
    },
    "post": {
      "_parent": { "type": "author" },
      "properties": {
        "content": { "type": "string", "similarity" : "BM25" }
      }
    }
  }
}'

# add data
curl -XPUT 'http://localhost:9200/has_child_similarity_test/author/1?pretty=true' -d '{
   "name": "George P. Stathis"
}'
curl -XPUT 'http://localhost:9200/has_child_similarity_test/post/1?parent=1&amp;pretty=true' -d '{
  "content": "Lorem ipsum dolor sit amet."
}'
curl -XPUT 'http://localhost:9200/has_child_similarity_test/post/2?parent=1&amp;pretty=true' -d '{
  "content": "Lorem ipsum dolor sit amet again!"
}'

echo " "
echo "Sleep for another two secs to allow for indexing"
sleep 2

# Search posts directly
echo " "
echo "Run query against child docs"
curl 'http://localhost:9200/has_child_similarity_test/post/_search?pretty=1' -d '{
  "query" : {
    "query_string" : {
      "default_field": "content",
      "query" : "Lorem ipsum"
    }
  }
}' | grep '_score'

echo " "
echo "NOTE!!! Two docs should now have matched with scores 0.80081946 and 0.67905 because we are using BM25"

# Search with has_child
echo " "
echo "Run same query as has_child query in 'sum' mode"
curl 'http://localhost:9200/has_child_similarity_test/_search?pretty=1' -d '{
  "query": {
    "has_child": {
      "type": "post",
      "query": {
        "query_string": {
          "query": "Lorem ipsum"
        }
      },
      "score_mode": "total"
    }
  }
}' | grep '_score'

echo " "
echo "NOTE!!! One parent doc matched but with with score 1.1490486 which is the sum of the child doc scores as computed by the default similarity (i.e. 0.61871845 + 0.53033006) not BM25. With BM25, the expected parent doc score should have been 0.80081946 + 0.67905 = 1.47986946"

```
</description><key id="132490983">16550</key><summary>Has child query forces default similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gpstathis</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2016-02-09T18:11:59Z</created><updated>2016-02-15T21:17:36Z</updated><resolved>2016-02-15T21:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-11T09:26:35Z" id="182779383">This regression must have sneaked in during the parent/child refactoring that was part of ES 2.0...
</comment><comment author="martijnvg" created="2016-02-11T10:06:39Z" id="182789757">@gpstathis I see you already worked on tests, but I can't a pr or the actual fix. The actual fix should be easy, from the `HasChildQueryBuilder#doToQuery(...)` and `HasParentQueryBuilder#doToQuery(...)` we should pass the `Similarity` from `context#getSearchSimilarity()` to `LateParsingQuery` as an constructor argument, in `LateParsingQuery` the similarity should become a field too. Then in `LateParsingQuery#rewrite()` just pass this similarity to the `IndexSearcher` being created there.
</comment><comment author="gpstathis" created="2016-02-11T15:18:02Z" id="182909868">@martijnvg got caught up with #16594 which I stumbled across as I was creating the tests for this one. Let me take a look and see how far I can get today.
</comment><comment author="gpstathis" created="2016-02-11T15:57:31Z" id="182931952">@martijnvg that was easy indeed. `SimilarityIT.testHasChildWithNonDefaultFieldSimilarity()` is passing now. `SimilarityIT.testHasChildWithNonDefaultGlobalSimilarity()` is passing but wrongly so because of #16594. Once that is fixed that unit test should still pass. Will open a PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when ALIAS is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16549</link><project id="" key="" /><description>The issue is: https://github.com/ywelsch/elasticsearch/blob/bef0bedba9e4d3417205eee7a0601eaf9763a831/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java#L289

Which should be `aliasAction.aliases == null || aliasAction.aliases.length == 0`. 

Now for the following request:

```
POST /_aliases
{
  "actions": [
    {
      "add": {
        "index": "test_index",
        "alias": null,
        "fdasdilter": {
          "termssse": {
            "tagss": "customerconsent"
          }
        }
      }
    }
  ]
}
```

We get the following errors:

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "null_pointer_exception",
      "reason": null
   },
   "status": 500
}
```

```
[2016-02-09 15:56:02,492][INFO ][rest.suppressed          ] /_aliases Params: {}
java.lang.NullPointerException
    at org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest.validate(IndicesAliasesRequest.java:289)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:62)
    at org.elasticsearch.client.node.
```
</description><key id="132489050">16549</key><summary>NullPointerException when ALIAS is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>bug</label></labels><created>2016-02-09T18:03:01Z</created><updated>2016-02-09T20:46:20Z</updated><resolved>2016-02-09T20:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2016-02-09T18:06:24Z" id="181984746">This was introduced in #15305
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Renames PipelineAggregatorFactory to PipelineAggregatorBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16548</link><project id="" key="" /><description>Also renames all the implementations appropriately
</description><key id="132481306">16548</key><summary>Renames PipelineAggregatorFactory to PipelineAggregatorBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-09T17:29:40Z</created><updated>2016-02-10T11:49:36Z</updated><resolved>2016-02-10T11:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-10T08:42:46Z" id="182253878">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered Aliases don't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16547</link><project id="" key="" /><description>This started to occur in ES v2.2.0. Easy steps to repro:
1. Crete and index
2. Put an alias
3. Display the alias
4. Check that the filter is not presented

```

PUT test_index
{
  "mappings": {
    "document_type": {
      "properties": {
        "tags": {
          "type": "string",
          "index": "not_analyzed",
          "fields": {
            "analyzed": {
              "type": "string"
            }
          },
          "ignore_above": 256
        }
      }
    }
  }
}

POST /_aliases
{
  "actions": [
    {
      "add": {
        "index": "test_in*", &lt;----- with wildcards
        "alias": "test_index_filter",
        "filter": {
          "term": {
            "tags": "customerconsent"
          }
        }
      }
    }
  ]
}

OR

POST /_aliases
{
  "actions": [
    {
      "add": {
        "index": "test_index",     &lt;------ without wildcards 
        "alias": "test_index_filter",
        "filter": {
          "term": {
            "tags": "customerconsent"
          }
        }
      }
    }
  ]
}

GET _aliases
```

Result

```
{
  "test_index": {
    "aliases": {
      "test_index_filter": {}
    }
  }
}
```
</description><key id="132480360">16547</key><summary>Filtered Aliases don't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T17:26:31Z</created><updated>2016-02-09T23:04:21Z</updated><resolved>2016-02-09T20:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2016-02-09T17:48:30Z" id="181978415">Creating an index and an alias at the same time work though:

```
PUT test_index
{
  "mappings": {
    "document_type": {
      "properties": {
        "tags": {
          "type": "string",
          "index": "not_analyzed",
          "fields": {
            "analyzed": {
              "type": "string"
            }
          },
          "ignore_above": 256
        }
      }
    }
  },
  "aliases": {
    "test_index_filter": {
      "filter": {
        "term": {
          "tags": "customerconsent"
        }
      }
    }
  }
}
```

```
  "test_index": {
    "aliases": {
      "test_index_filter": {
        "filter": {
          "term": {
            "tags": "customerconsent"
          }
        }
      }
    }
  }
```
</comment><comment author="gmoskovicz" created="2016-02-09T18:15:30Z" id="181988865">The bug is here https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java#L136

Now the code looks like:

```
AliasActions aliasActions = new AliasActions(type, indices, aliases);
```

Previously it was:

```
AliasAction aliasAction = newAddAliasAction(index, alias).filter(filter);
```

The `.filter(filter)` is missing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing other bucket in FiltersAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16546</link><project id="" key="" /><description>Currently, `FiltersAggregator::buildEmptyAggregation` does not check if `other_bucket_key` was set on the request. This results in a missing `other` bucket in the response when no documents were matched. Is this intended? 

Here's a snippet from a date histogram agg with a filters agg.

```
{
    "key_as_string": "2015-10-01 00:00:00.000",
    "key": 1443657600000,
    "doc_count": 0,
    "ex": {
        "buckets": {
            "a: {
                "doc_count": 0
            },
            "b": {
                "doc_count": 0
            },
            "c": {
                "doc_count": 0
            }
        }
    }
},
{
    "key_as_string": "2015-11-01 00:00:00.000",
    "key": 1446336000000,
    "doc_count": 5,
    "ex": {
        "buckets": {
            "a": {
                "doc_count": 0
            },
            "b": {
                "doc_count": 3
            },
            "c": {
                "doc_count": 2
            },
            "other": {
                "doc_count": 0
            }
        }
    }
}



```
</description><key id="132477794">16546</key><summary>Missing other bucket in FiltersAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pjo256</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label></labels><created>2016-02-09T17:17:00Z</created><updated>2016-03-23T15:59:42Z</updated><resolved>2016-03-23T09:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-22T15:45:11Z" id="199873556">This is a bug and should be an easy fix. Thanks for raising this @pjo256 
</comment><comment author="colings86" created="2016-03-23T09:17:59Z" id="200263362">Sorry @pjo256 I didn't see your PR before raising my own. Thanks for contributing, your PR looks good so I'll merge that one :smile: 
</comment><comment author="pjo256" created="2016-03-23T15:59:42Z" id="200411163">@colings86 Thanks :smiley: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made AggregatorFactory fields final and removed AggregationContext from createInternal() parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16545</link><project id="" key="" /><description /><key id="132477162">16545</key><summary>Made AggregatorFactory fields final and removed AggregationContext from createInternal() parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-09T17:14:42Z</created><updated>2016-02-10T11:01:30Z</updated><resolved>2016-02-10T11:01:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-10T08:43:43Z" id="182254051">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting Shard failures for a quoted string search query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16544</link><project id="" key="" /><description>I am using Elasticsearch 2.2.0.

I have the following schema for the index:

```
{
  "settings" : {
    "index.number_of_shards" : 1,
    "index.similarity.default.type": "BM25"
  }
}
```

I index 5 documents and I even test it in the code as follows:

```
Client testClient = client();
.
. // index the documents here
.
SearchResponse countResponse = testClient.prepareSearch(indexName)
            .setTypes("post").setSize(0).execute().actionGet();
assertHitCount(countResponse, 5L); // this one passes which means all 5 documents were indexed

String singleWord = "Lorem";
String twoWords = "Lorem ipsum";
String quotedPhrase = "\"Lorem ipsum\"";

QueryStringQueryBuilder query = queryStringQuery(quotedPhrase);
query.useDisMax(true);
query.defaultOperator(QueryStringQueryBuilder.Operator.AND);
query.autoGeneratePhraseQueries(true);
query.field("content", 1f);

SearchRequestBuilder request = testClient.prepareSearch(indexName).setTypes("student");
request.setQuery(query);
request.addField("*");
request.setExplain(true);
SearchResponse response = request.execute().actionGet();
assertHitCount(response, 1L);
```

I notice that the query for `quotedPhrase` results in the following errors arbitrarily:
- Shard Failures

```
Failed to execute phase [query_fetch], all shards failed
; shardFailures {[ppHLclgASbiK4nZBBjIYjg][default_index][0]: RemoteTransportException[[node_t1][local[2]][indices:data/read/search[phase/query+fetch]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AbstractMethodError[org.apache.lucene.search.TwoPhaseIterator.matchCost()F]; }
```
- No hits

```
java.lang.AssertionError: Hit count is 0 but 1 was expected.  Total shards: 9 Successful shards: 8 &amp; 1 shard failures:
```
- Sometimes that test passes.

**UPDATE**: This works in ES 2.1.1 but not in ES 2.2.0

Other details:
1. I am using `ESIntegTestCase`
2. On the top of the class name I have `@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST)`
3. This test runs for `singleWord` and `twoWords`

What could be the issue?

I am attacking the stack trace as well:

```
org.elasticsearch.transport.RemoteTransportException: [node_t0][local[1]][indices:data/read/search[phase/query+fetch]]
org.elasticsearch.search.query.QueryPhaseExecutionException: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:468) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-2.2.0.jar:2.2.0]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_66]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_66]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66]
Caused by: java.lang.AbstractMethodError: org.apache.lucene.search.TwoPhaseIterator.matchCost()F
    at org.apache.lucene.search.ConjunctionDISI$TwoPhaseConjunctionDISI.&lt;init&gt;(ConjunctionDISI.java:186) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhaseConjunctionDISI.&lt;init&gt;(ConjunctionDISI.java:164) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhase.&lt;init&gt;(ConjunctionDISI.java:227) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhase.&lt;init&gt;(ConjunctionDISI.java:221) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI.intersect(ConjunctionDISI.java:50) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionScorer.&lt;init&gt;(ConjunctionScorer.java:41) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.req(BooleanWeight.java:403) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:326) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:262) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:69) ~[lucene-test-framework-5.3.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:69) ~[lucene-test-framework-5.3.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:818) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384) ~[elasticsearch-2.2.0.jar:2.2.0]
    ... 10 common frames omitted
13:57:52.458 [elasticsearch[node_t0][search][T#3]] DEBUG action.search.type - [node_t0] All shards failed for phase: [query_fetch]
org.elasticsearch.transport.RemoteTransportException: [node_t0][local[1]][indices:data/read/search[phase/query+fetch]]
org.elasticsearch.search.query.QueryPhaseExecutionException: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:468) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-2.2.0.jar:2.2.0]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_66]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_66]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_66]
Caused by: java.lang.AbstractMethodError: org.apache.lucene.search.TwoPhaseIterator.matchCost()F
    at org.apache.lucene.search.ConjunctionDISI$TwoPhaseConjunctionDISI.&lt;init&gt;(ConjunctionDISI.java:186) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhaseConjunctionDISI.&lt;init&gt;(ConjunctionDISI.java:164) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhase.&lt;init&gt;(ConjunctionDISI.java:227) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI$TwoPhase.&lt;init&gt;(ConjunctionDISI.java:221) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionDISI.intersect(ConjunctionDISI.java:50) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.ConjunctionScorer.&lt;init&gt;(ConjunctionScorer.java:41) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.req(BooleanWeight.java:403) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:326) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:262) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:69) ~[lucene-test-framework-5.3.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:69) ~[lucene-test-framework-5.3.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:818) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535) ~[lucene-core-5.4.1.jar:5.4.1 1725212 - jpountz - 2016-01-18 11:44:59]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384) ~[elasticsearch-2.2.0.jar:2.2.0]
    ... 10 common frames omitted
```
</description><key id="132469462">16544</key><summary>Getting Shard failures for a quoted string search query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-02-09T16:44:37Z</created><updated>2016-02-29T08:43:50Z</updated><resolved>2016-02-29T08:43:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T14:03:18Z" id="183672666">@jpountz any ideas what is happening here?
</comment><comment author="clintongormley" created="2016-02-28T18:44:49Z" id="189921279">@jpountz pinging again
</comment><comment author="apanimesh061" created="2016-02-28T19:05:56Z" id="189923597">@clintongormley I guess I found the issue. It is due to the lucene version. ES 2.1.1 uses Lucene 5.3.1 and ES 2.2.0 uses Lucene 5.4.1. The `TwoPhaseIterator.matchCost()` was introduced in in Lucene 5.4. if I am not wrong.

I would suggest you mention somewhere, that the Lucene core version should be updated to 5.4, when updating to ES 2.2.0.
</comment><comment author="clintongormley" created="2016-02-29T08:43:50Z" id="190104831">@apanimesh061 you won't be able to run the wrong lucene version as of 2.3.0 (see https://github.com/elastic/elasticsearch/pull/16305)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid use of java.security.MessageDigest#clone()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16543</link><project id="" key="" /><description>This commit forbids the usage of java.security.MessageDigest#clone() in
favor of getting a thread local instance using
org.elasticsearch.common.hash.MessageDigests. This is to prevent running
into java.lang.CloneNotSupportedExceptions for message digest providers
that do not support clone.

Relates #16479 
</description><key id="132464431">16543</key><summary>Forbid use of java.security.MessageDigest#clone()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T16:26:34Z</created><updated>2016-02-13T13:59:08Z</updated><resolved>2016-02-09T16:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-02-09T16:29:31Z" id="181943851">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused pmd file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16542</link><project id="" key="" /><description>This pmd file is unused by gradle.
</description><key id="132463653">16542</key><summary>Remove unused pmd file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label></labels><created>2016-02-09T16:24:24Z</created><updated>2016-03-08T17:28:22Z</updated><resolved>2016-03-08T13:11:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-09T16:26:40Z" id="181942293">@s1monw this file came from 05db5dc2c8e3b1d41081b9c7fb677b5850e7b3db which was yours but I get the sense that it came from somewhere else before that. It looks like we had an option to build a pmd report. Do you know if that is still important?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove old Eclipse run config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16541</link><project id="" key="" /><description>The config doesn't work. The current right way to debug Elasticsearch in an
IDE is to `gradle run --debug-jvm` and then do remove debugging.
</description><key id="132461058">16541</key><summary>Remove old Eclipse run config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T16:13:56Z</created><updated>2016-02-09T16:43:30Z</updated><resolved>2016-02-09T16:43:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-09T16:15:46Z" id="181936491">In master we have `buildSrc` which is kind of like gradle's dev-tools. As such there isn't much much left in there so I'm doing a quick inventory. This isn't needed and doesn't even work.
</comment><comment author="rjernst" created="2016-02-09T16:40:31Z" id="181948995">LGTM. I've been slowly removing stuff from dev tools. IIRC, scripts for bwc index building, and releasing, are left. 
</comment><comment author="nik9000" created="2016-02-09T16:43:27Z" id="181950122">&gt; IIRC, scripts for bwc index building, and releasing, are left.

I think those are the things that haven't been migrated. Everything that isn't one of those things is suspect.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add permission to access sun.reflect.MethodAccessorImpl from Groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16540</link><project id="" key="" /><description>Groovy uses reflection to invoke closures. These reflective calls are optimized by the JVM after `sun.reflect.inflationThreshold` number of invocations.
After inflation, access to `sun.reflect.MethodAccessorImpl` is required from the security manager.

Closes #16536
</description><key id="132457755">16540</key><summary>Add permission to access sun.reflect.MethodAccessorImpl from Groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Scripting</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T16:03:07Z</created><updated>2016-02-20T03:04:05Z</updated><resolved>2016-02-09T16:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-09T16:08:52Z" id="181932185">&gt; After inflation, access to sun.reflect.MethodAccessorImpl is required from the security manager.

Well, to be clear, its only "needed" because groovy is messy and uses internal JDK apis... and if you look at the description of what this class does, its clear that no part of elasticsearch - scripting or otherwise - has any business messing with it.

thanks for adding the tests, change looks good.
</comment><comment author="jasontedor" created="2016-02-09T16:09:53Z" id="181932795">I think this needs to go to [v2.3.0](https://github.com/elastic/elasticsearch/labels/v2.3.0) too?

LGTM.
</comment><comment author="ywelsch" created="2016-02-09T16:39:26Z" id="181948514">@rmuir Groovy "just" uses `java.lang.reflect.Method` (which is public API). I see the issue more in that the JVM does an optimisation that has an unexpected effect on a security policy that was perfectly fine before the optimisation.

Btw. the Groovy classloader has an "interesting" way of handling this situation:
https://github.com/groovy/groovy-core/blob/4993b10737881b2491c2daa01526fb15dd889ac5/src/main/groovy/lang/GroovyClassLoader.java#L699

@jasontedor thanks for noticing, I've added the missing label.
</comment><comment author="rmuir" created="2016-02-09T16:49:38Z" id="181952598">&gt; @rmuir Groovy "just" uses java.lang.reflect.Method (which is public API).

I know you honestly don't believe that.

```
// remove this if you do not believe me!!!!!!
permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
```
</comment><comment author="ywelsch" created="2016-02-09T20:05:33Z" id="182037814">@rmuir the following experiment validates my point:
- Add `-Dsun.reflect.inflationThreshold=1000000` as JVM parameter to tests.
- Remove 

```
permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
permission org.elasticsearch.script.ClassPermission "sun.reflect.ConstructorAccessorImpl";
permission org.elasticsearch.script.ClassPermission "sun.reflect.MethodAccessorImpl";
```

from `plugin-security.policy`.
- Run all the tests successfully.
</comment><comment author="rmuir" created="2016-02-12T09:04:57Z" id="183243171">That proves nothing. Search groovy codebase for sun.reflect and you will see. Thats why only groovy needs these bogus permissions to directly access internal jdk classes. Just groovy. Because its sloppy, messy and insecure. The sad thing is, not only do we support it... But we support it out of box. 
</comment><comment author="ywelsch" created="2016-02-12T11:15:39Z" id="183281936">@rmuir: This has nothing to do with Groovy. I'll give it one final attempt to show the issue here. Look at the following example:

```
import java.lang.reflect.Method;
import java.security.Permission;

public class Main {
    static int ITERATIONS = 16;

    public static void main(String[] args) throws Throwable {
        System.setSecurityManager(new SecurityManager() {
            @Override
            public void checkPermission(Permission perm) {
                System.out.println(perm);
            }
        });

        Method method = Main.class.getMethod("someMethod");
        for (int i = 0; i &lt; ITERATIONS; i++) {
            method.invoke(new Main());
        }
    }

    public static void someMethod() {}
}
```

Running this prints:

```
("java.lang.RuntimePermission" "createClassLoader")
("java.lang.RuntimePermission" "accessClassInPackage.sun.reflect")
("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
```

If you don't let inflation kick in (e.g. setting `ITERATIONS` to less than 16 or setting `sun.reflect.inflationThreshold` to a value higher than `ITERATIONS`) then no permissions are required.
</comment><comment author="rmuir" created="2016-02-20T00:34:32Z" id="186466733">I'm telling you it does. Here is the thing, inflation is unrelated: its not the problem, its just the trigger in this one case. The problem is, groovy sometimes operates on the wrong class, and it causes many issues, such as complete incompatibility with java 9 jigsaw (https://issues.apache.org/jira/browse/GROOVY-7587).

A simple example from there:

```
def test = Paths.get('test.txt')
print test.toString()
```

fails because it illegally tries to operate on sun.nio.fs.WindowsPath. Its the same thing here.

In this case, the object being generated gains an additional interface due to inflation: which are the classes in question. Groovy should not be messing with those interfaces!

This is why it does not happen with any other scripting language. You pretend to "prove" otherwise but you have a logical fallacy in your argument: you are only trying to prove whether or not its related to inflation, which is not the thing in question.

I argue that it is groovy's fault because it behaves wrongly here when operating on the wrong class and failing when it can't because some error is returned.  That's why its not a problem for any other scripting language.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest: use bulk thread pool for bulk request processing (was index before)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16539</link><project id="" key="" /><description>Use bulk thread pool for bulk request processing (was index before)

Closes #16503
</description><key id="132457327">16539</key><summary>Ingest: use bulk thread pool for bulk request processing (was index before)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T16:01:40Z</created><updated>2016-02-10T10:49:22Z</updated><resolved>2016-02-10T10:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-09T16:02:23Z" id="181928687">@martijnvg can you have a look please?
</comment><comment author="martijnvg" created="2016-02-09T16:19:19Z" id="181937807">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Javadoc for stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16538</link><project id="" key="" /><description>Adds some javadoc to Streamable, Writeable, and friends.
</description><key id="132456796">16538</key><summary>Javadoc for stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T16:00:14Z</created><updated>2016-02-29T08:04:20Z</updated><resolved>2016-02-26T21:34:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-09T16:01:21Z" id="181928119">I sneak in one tiny code change to make StreamOutput look like StreamInput.
</comment><comment author="nik9000" created="2016-02-11T16:04:31Z" id="182935287">Boo - a bunch of weird file snuck into this. Will drop.
</comment><comment author="nik9000" created="2016-02-11T16:52:27Z" id="182955683">&gt; Will drop.

Cleaned up!
</comment><comment author="danielmitterdorfer" created="2016-02-26T11:11:46Z" id="189224928">Left a few comments, otherwise LGTM.
</comment><comment author="nik9000" created="2016-02-26T18:52:53Z" id="189421880">@danielmitterdorfer I pushed another commit fixing things you commented on. Also added a package-info.java on the off chance anyone looks at generated javadocs. Running precommit to make sure I didn't break anything.
</comment><comment author="nik9000" created="2016-02-26T21:35:41Z" id="189490722">@danielmitterdorfer given your earlier LGTM I just merged it with the fixes you proposed and the package-info.java. Thanks for review!
</comment><comment author="danielmitterdorfer" created="2016-02-29T08:04:20Z" id="190085816">@nik9000 I think merging was fine. You're welcome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top_hits and sub-aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16537</link><project id="" key="" /><description>Hi!

Why top_hits doesn't allow to have sub-aggregations?

I.e. I want to calculate a median of last N hits in each category
</description><key id="132442080">16537</key><summary>top_hits and sub-aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsideup</reporter><labels /><created>2016-02-09T15:01:22Z</created><updated>2016-02-13T14:10:50Z</updated><resolved>2016-02-13T14:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-09T17:19:29Z" id="181965367">because the `top_hits` agg is a metric aggregations and metric aggs can't
have sub aggs.
How would you determine what last N hit would be the median? On what field
are you going to sort? Maybe instead use the `percentile` agg?

On 9 February 2016 at 16:01, Sergei Egorov notifications@github.com wrote:

&gt; Hi!
&gt; 
&gt; Why top_hits doesn't allow to have sub-aggregations?
&gt; 
&gt; I.e. I want to calculate a median of last N hits in each category
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16537.

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="bsideup" created="2016-02-09T17:54:18Z" id="181980864">@martijnvg I use "sort" from top_hits for such decisions. 

Also, I need to calculate median of exactly last N hits in each category, I can't calculate median (aka 50th percentile) of all hits :(
</comment><comment author="clintongormley" created="2016-02-13T14:10:50Z" id="183675645">This sounds like you will need to postprocess your results on the client.  As @martijnvg says, metric aggs can't support sub-aggs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy: Reflection causes exceptions after several runs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16536</link><project id="" key="" /><description>A user in the [watcher forum](https://discuss.elastic.co/t/failed-to-execute-watch-transform-noclassdeffounderror/41257/5) hit a `ClassNotFoundError`, after a watch ran successful several times.

Turns out this is a groovy issue and can be reproduced using this test:

``` java
package org.elasticsearch.script.groovy;

import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.script.CompiledScript;
import org.elasticsearch.script.ExecutableScript;
import org.elasticsearch.script.ScriptService;
import org.elasticsearch.test.ESTestCase;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class GroovyCompilationTests extends ESTestCase {

    private GroovyScriptEngineService se;

    @Override
    public void setUp() throws Exception {
        super.setUp();
        se = new GroovyScriptEngineService(Settings.EMPTY);
        // otherwise will exit your VM and other bad stuff
        assumeTrue("test requires security manager to be enabled", System.getSecurityManager() != null);
    }

    @Override
    public void tearDown() throws Exception {
        se.close();
        super.tearDown();
    }

    public void testThatPotentialBugWithSeveralExecutions() throws Exception {
        String script = "return hits.collect({\"${it.message}\"})";

        Map&lt;String, Object&gt; data = new HashMap&lt;&gt;();
        data.put("message", "The big lebowski");

        List&lt;Object&gt; hitsArray = new ArrayList&lt;&gt;();
        hitsArray.add(data);

        Map&lt;String, Object&gt; hits = new HashMap&lt;&gt;();
        hits.put("hits", hitsArray);

        Object compiledObject = se.compile(script, Collections.emptyMap());
        CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "test", "groovy", compiledObject);
        ExecutableScript executable = se.executable(compiledScript, hits);

        // 100 runs will fail, 10 runs work..
        for (int i = 0; i &lt; 100; i++) {
            executable.run();
        }
    }

}
```

@ywelsch already found out, that one can trigger this immediately by setting the `-Dsun.reflect.noInflation=true` property.

For more info about that, check out https://blogs.oracle.com/buck/entry/inflation_system_properties
</description><key id="132430181">16536</key><summary>Groovy: Reflection causes exceptions after several runs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Scripting</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T14:16:49Z</created><updated>2016-02-20T00:29:03Z</updated><resolved>2016-02-09T16:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-09T14:22:00Z" id="181884186">The reason it fails is that after 15 iterations inflation kicks in. An explanation of inflation is given here:
https://blogs.oracle.com/buck/entry/inflation_system_properties
The following stackoverflow post also has some information:
http://stackoverflow.com/questions/20527776/why-does-my-custom-securitymanager-cause-exceptions-the-16th-time-i-create-an-ob

Adding the following permission to plugin-security.policy solves it:

```
permission org.elasticsearch.script.ClassPermission "sun.reflect.MethodAccessorImpl";
```
</comment><comment author="ywelsch" created="2016-02-09T14:26:59Z" id="181886983">@rmuir would it make sense to add `sun.reflect.ConstructorAccessorImpl` and `sun.reflect.MethodAccessorImpl` to `STANDARD_CLASSES` in `ClassPermission`?
I'm not sure whether other script plugins might as well be affected by this issue.
</comment><comment author="rmuir" created="2016-02-09T14:41:34Z" id="181895740">The issue may be groovy specific. Groovy is the only one that tries to access sun.reflect directly, hence the only one with this:

```
  permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
```
</comment><comment author="rmuir" created="2016-02-09T14:46:20Z" id="181897508">Also note that groovy runtime already has ConstructorAccessor, because only a constructor was reflected in a loop:

```
  permission org.elasticsearch.script.ClassPermission "sun.reflect.ConstructorAccessorImpl";
```

So i would add the other inflation-related class right there beside it, just for groovy. We should add a loop test for the case. If we can add the same loop test to Javascript/Python to show it works fine without this, that would be nice.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_match query different scoring results 1.7 - 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16535</link><project id="" key="" /><description>We're in the progress of upgrading to ES 2.1 and have noticed that some queries now have different results.
I'm trying to figure out the root cause. So far my guess is that it is a analyzer change within Lucene.

Here is the mapping I'm using:

```
{
    "settings": {
        "number_of_shards": 2
    },
    "mappings": {
        "default": {
            "dynamic": "true",
            "_all": {
                "enabled": false
            },
            "properties": {
                "description": {
                    "type": "string",
                    "index": "not_analyzed",
                    "doc_values": true,
                    "copy_to": [
                        "name_description_ft"
                    ]
                },
                "id": {
                    "type": "string",
                    "index": "not_analyzed",
                    "doc_values": true
                },
                "kind": {
                    "type": "string",
                    "index": "not_analyzed",
                    "doc_values": true
                },
                "name": {
                    "type": "string",
                    "index": "not_analyzed",
                    "doc_values": true,
                    "copy_to": [
                        "name_description_ft"
                    ]
                },
                "name_description_ft": {
                    "type": "string",
                    "analyzer": "english"
                }
            }
        }
    }
}
```

Here are the records:

```
{"index": {"_id": "1"}}
{"id":"1","name":"North West Ripple","kind":"Galaxy","description":"Relative to life on NowWhat, living on an affluent world in the North West ripple of the Galaxy is said to be easier by a factor of about seventeen million."}
{"index": {"_id": "2"}}
{"id":"2","name":"Outer Eastern Rim","kind":"Galaxy","description":"The Outer Eastern Rim of the Galaxy where the Guide has supplanted the Encyclopedia Galactica among its more relaxed civilisations."}
{"index": {"_id": "3"}}
{"id":"3","name":"Galactic Sector QQ7 Active J Gamma","kind":"Galaxy","description":"Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains."}
{"index": {"_id": "4"}}
{"id":"4","name":"Aldebaran","kind":"Star System","description":"Max Quordlepleen claims that the only thing left after the end of the Universe will be the sweets trolley and a fine selection of Aldebaran liqueurs."}
{"index": {"_id": "5"}}
{"id":"5","name":"Algol","kind":"Star System","description":"Algol is the home of the Algolian Suntiger, the tooth of which is one of the ingredients of the Pan Galactic Gargle Blaster."}
{"index": {"_id": "6"}}
{"id":"6","name":"Alpha Centauri","kind":"Star System","description":"4.1 light-years northwest of earth"}
{"index": {"_id": "7"}}
{"id":"7","name":"Altair","kind":"Star System","description":"The Altairian dollar is one of three freely convertible currencies in the galaxy, though by the time of the novels it had apparently recently collapsed."}
{"index": {"_id": "8"}}
{"id":"8","name":"Allosimanius Syneca","kind":"Planet","description":"Allosimanius Syneca is a planet noted for ice, snow, mind-hurtling beauty and stunning cold."}
{"index": {"_id": "9"}}
{"id":"9","name":"Argabuthon","kind":"Planet","description":"It is also the home of Prak, a man placed into solitary confinement after an overdose of truth drug caused him to tell the Truth in its absolute and final form, causing anyone to hear it to go insane."}
{"index": {"_id": "10"}}
{"id":"10","name":"Arkintoofle Minor","kind":"Planet","description":"Motivated by the fact that the only thing in the Universe that travels faster than light is bad news, the Hingefreel people native to Arkintoofle Minor constructed a starship driven by bad news."}
{"index": {"_id": "11"}}
{"id":"11","name":"Bartledan","kind":"Planet","description":"An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically."}
{"index": {"_id": "12"}}
{"id":"12","name":"","kind":"Planet","description":"This Planet doesn't really exist"}
{"index": {"_id": "13"}}
{"id":"13","name":null,"kind":"Galaxy","description":"The end of the Galaxy.%"}
```

And this is the query:

```
{
    "explain": true,
    "query": {
        "multi_match": {
            "fields": ["kind^0.8", "name_description_ft^0.6"],
            "query": "planet earth"
        }
    }
}
```

In 1.7 the top 2 hits are

```
            {
                "_id": "6",
                "_score": 0.22184466,
                "_source": {
                    "id": "6",
                    "name": "Alpha Centauri",
                    "kind": "Star System",
                    "description": "4.1 light-years northwest of earth"
                }
            },
            {
                "_id": "12",
                "_score": 0.21719791,
                "_source": {
                    "id": "12",
                    "name": "",
                    "kind": "Planet",
                    "description": "This Planet doesn't really exist"
                }
            },
```

and in 2.1 they are:

```
             {
                "_id": "6",
                "_score": 0.2600391,
                "_source": {
                    "id": "6",
                    "name": "Alpha Centauri",
                    "kind": "Star System",
                    "description": "4.1 light-years northwest of earth"
                }
            },
            {
                "_id": "11",
                "_score": 0.15168947,
                "_source": {
                    "id": "11",
                    "name": "Bartledan",
                    "kind": "Planet",
                    "description": "An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically."
                }
            },
```

I've also tried to do a snapshot on 1.7 and then restore the snapshot in 2.1. This results in 2.1 producing the same result as in 1.7 which is why I assume that something at indexing time is now handled differently.

I've also tried to see if the `_analyze` API returns a different result somewhere. But all descriptions are tokenized the same in 1.7 and 2.1 - the only difference is that in one version position is starting from 0 and in the other version it's starting from 1.

Maybe I'm missing something obvious here, 
</description><key id="132422604">16535</key><summary>multi_match query different scoring results 1.7 - 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels /><created>2016-02-09T13:44:19Z</created><updated>2016-02-09T14:57:20Z</updated><resolved>2016-02-09T14:57:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-09T14:01:25Z" id="181875026">&gt; "settings": { "number_of_shards": 2 },

This is enough to do it. If you want consistent scoring, you need to enable distributed term statistics, otherwise the IDF values used are based on local information, not global. 

See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-search-type.html
</comment><comment author="mfussenegger" created="2016-02-09T14:40:38Z" id="181895307">But shouldn't the local IDF values be deterministic if _id isn't randomly generated? The shard allocation should be deterministic which should result in the shards/lucene-indices having the same documents. 

You're right in that if I change number_of_shards to 1 I get the same results in both 1.7 and 2.1 - could it be that the routing allocation algorithm changed?
</comment><comment author="mfussenegger" created="2016-02-09T14:57:20Z" id="181902538">Okay, thanks for the pointer in the right direction. Murmur is now used - it's even documented in the mapping changes.
Due to that the distribution is different from before and due to that the scoring is different.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not greedy behaviour of "must_not" for array field of terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16534</link><project id="" key="" /><description>For example, i have two records with 'name' field:
{_id: 1, name: ['aaa', 'bbb']}
{_id: 2,  name: ['aaa']}
When i execute search query like this
`{must_not: [{term: { name: 'bbb' }}]}`
i get only 2. How can i get [1, 2](not exclude 1 because it contain not only) as result without addition efforts? Do ES have some flag to change greedy of must_not?
</description><key id="132410901">16534</key><summary>Not greedy behaviour of "must_not" for array field of terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">a0s</reporter><labels /><created>2016-02-09T12:46:35Z</created><updated>2016-02-09T18:37:55Z</updated><resolved>2016-02-09T18:37:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-09T18:37:55Z" id="181996629">Please use http://discuss.elastic.co for questions like this. Github is used for bug reports and feature requests.

I don't know why you think this has anything to do with "greedy". In doc 1, `aaa` and `bbb` are separate terms. The must not query you use finds the `bbb` term and excludes that document. It sounds like you are looking for something to say "must not have only this term". There is no direct support for that type of query that I know of. But you should also think about how useful this query is, since must nots do not contribute to scoring.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse Aggregations on Coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16533</link><project id="" key="" /><description /><key id="132406235">16533</key><summary>Parse Aggregations on Coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-09T12:17:58Z</created><updated>2016-02-10T11:01:25Z</updated><resolved>2016-02-10T10:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-10T08:57:04Z" id="182259374">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up of some NORELEASE comments for Agg refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16532</link><project id="" key="" /><description /><key id="132399264">16532</key><summary>Clean up of some NORELEASE comments for Agg refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-09T11:45:51Z</created><updated>2016-02-10T09:59:03Z</updated><resolved>2016-02-10T09:58:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-10T08:50:40Z" id="182256786">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move remaining settings in NettyHttpServerTransport to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16531</link><project id="" key="" /><description>Some bw incompatible setting changes (to simplify and conform to other settings):
- http.netty.http.blocking_server -&gt; http.tcp.blocking_server
- http.netty.host (removed, we just have http.host)
- http.netty.bind_host (removed, we just have http.bind_host)
- http.netty.publish_host (removed, we just have http.publish_host)
- http.netty.tcp_no_delay -&gt; http.tcp.no_delay
- http.netty.tcp_keep_alive -&gt; http.tcp.keep_alive
- http.netty.reuse_address -&gt; http.txp.reuse_address
- http.netty.tcp_send_buffer_size -&gt; http.tcp.send_buffer_size
- http.netty.tcp_receive_buffer_size -&gt; http.tcp.receive_buffer_size
</description><key id="132399239">16531</key><summary>Move remaining settings in NettyHttpServerTransport to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T11:45:39Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-02-11T13:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-09T11:45:48Z" id="181830521">@colings86 care to take a look?
</comment><comment author="colings86" created="2016-02-09T11:52:42Z" id="181832651">@bleskes I left a comment about where the constants should live but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse existing allocation id for primary shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16530</link><project id="" key="" /><description>This solves an issue where shard was activated by master but the cluster crashes before the target node persisted the shard state.

Relates to #14739
</description><key id="132395226">16530</key><summary>Reuse existing allocation id for primary shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T11:25:25Z</created><updated>2016-02-13T13:08:30Z</updated><resolved>2016-02-11T13:02:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-11T10:00:54Z" id="182788563">Left some very minor comments...
</comment><comment author="ywelsch" created="2016-02-11T10:51:24Z" id="182807924">@bleskes Thanks for the review! I've updated the PR according to your suggestions and made one additional change: I removed the overloads of the `initialize` method that I earlier introduced in `ShardRouting`, `RoutingNodes` and `UnassignedShards` as there are not that many call sites for these methods. With each of these methods now being properly documented, I believe that adding `null` on these very few method calls to be cleaner (similar to `-1` that is present for `expectedShardSize` on most call sites).
</comment><comment author="bleskes" created="2016-02-11T12:45:05Z" id="182846321">LGTM. Thanks @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos in docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16529</link><project id="" key="" /><description /><key id="132378412">16529</key><summary>Fix typos in docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>docs</label></labels><created>2016-02-09T10:08:59Z</created><updated>2016-02-26T18:53:21Z</updated><resolved>2016-02-09T19:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dongjoon-hyun" created="2016-02-09T19:05:09Z" id="182008549">Thank you!
</comment><comment author="nik9000" created="2016-02-09T19:07:56Z" id="182009842">Merged! Thanks @dongjoon-hyun!

Backported to 2.x with ae02ed549f5459e86de38f9ae1cd41edec79adb3 and da63bddf0011210854d346e897f8ebd878a7ece3.

Backproted to 2.2 bc051c1ba62346ee507211819fb87152cf68c536 and bb3a0c2463ddb9ccc48f5fed1fb2f70fddfe3bc4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to run Elasticsearch (2.2.0) windows service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16528</link><project id="" key="" /><description>Hi,

Installed the service from zip (windows 8.1 professional - developer machine) with all default options.
JVM is present and all settings seems to be alright.

**\- service install -&gt; installs perfectly**
**\- service manager (then start - argument: start) -&gt; service starts and then stops (logging mode set to Debug)**

**elasticsearch-service-x64-stderr.2016-02-09.log** =&gt;
2016-02-09 10:00:32 Commons Daemon procrun stderr initialized
Java HotSpot(TM) 64-Bit Server VM warning: Using the ParNew young collector with the Serial old collector is deprecated and will likely be removed in a future release
Exception in thread "main" tstrap.Bootstrap.init(Bootstrap.java:248)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details

**elasticsearch-service-x64.2016-02-09.log** =&gt;
[2016-02-09 10:00:32] [debug](prunsrv.c:1679) [ 8288] Commons Daemon procrun log initialized
[2016-02-09 10:00:32] [info](prunsrv.c:1683) [ 8288] Commons Daemon procrun (1.0.15.0 64-bit) started
[2016-02-09 10:00:32] [info](prunsrv.c:1596) [ 8288] Running 'elasticsearch-service-x64' Service...
[2016-02-09 10:00:32] [debug](prunsrv.c:1374) [ 5200] Inside ServiceMain...
[2016-02-09 10:00:32] [debug](prunsrv.c:844) [ 5200] reportServiceStatusE: 2, 0, 3000, 0
[2016-02-09 10:00:32] [info](prunsrv.c:1127) [ 5200] Starting service...
[2016-02-09 10:00:32] [debug](javajni.c:233) [ 5200] loading jvm 'C:\Program Files\Java\jdk1.8.0_11\jre\bin\server\jvm.dll'
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[0] -XX:+UseParNewGC
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[1] -Xms256m
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[2] -Xmx1g
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[3] -Djava.awt.headless=true
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[4] -XX:+UseCMSInitiatingOccupancyOnly
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[5] -XX:+HeapDumpOnOutOfMemoryError
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[6] -XX:+DisableExplicitGC
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[7] -Dfile.encoding=UTF-8
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[8] -Djna.nosys=true
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[9] -Delasticsearch
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[10] -Des.path.home=C:\LogAggregator\elasticsearch
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[11] -Des.default.path.home=C:\LogAggregator\elasticsearch
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[12] -Des.default.path.logs=C:\LogAggregator\elasticsearch\logs
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[13] -Des.default.path.data=C:\LogAggregator\elasticsearch\data
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[14] -Des.default.path.conf=C:\LogAggregator\elasticsearch\config
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[15] -Djava.class.path=C:\LogAggregator\elasticsearch/lib/elasticsearch-2.2.0.jar;C:\LogAggregator\elasticsearch/lib/antlr-runtime-3.5.jar;C:\LogAggregator\elasticsearch/lib/apache-log4j-extras-1.2.17.jar;C:\LogAggregator\elasticsearch/lib/asm-4.1.jar;C:\LogAggregator\elasticsearch/lib/asm-commons-4.1.jar;C:\LogAggregator\elasticsearch/lib/commons-cli-1.3.1.jar;C:\LogAggregator\elasticsearch/lib/compiler-0.8.13.jar;C:\LogAggregator\elasticsearch/lib/compress-lzf-1.0.2.jar;C:\LogAggregator\elasticsearch/lib/elasticsearch-2.1.1.jar;C:\LogAggregator\elasticsearch/lib/elasticsearch-2.2.0.jar;C:\LogAggregator\elasticsearch/lib/groovy-all-2.4.4-indy.jar;C:\LogAggregator\elasticsearch/lib/guava-18.0.jar;C:\LogAggregator\elasticsearch/lib/HdrHistogram-2.1.6.jar;C:\LogAggregator\elasticsearch/lib/hppc-0.7.1.jar;C:\LogAggregator\elasticsearch/lib/jackson-core-2.6.2.jar;C:\LogAggregator\elasticsearch/lib/jackson-dataformat-cbor-2.6.2.jar;C:\LogAggregator\elasticsearch/lib/jackson-dataformat-smile-2.6.2.jar;C:\
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[16] exit
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[17] -Xms256m
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[18] -Xmx1024m
[2016-02-09 10:00:32] [debug](javajni.c:704) [ 8444] Jvm Option[19] -Xss256k
[2016-02-09 10:00:32] [debug](javajni.c:888) [ 8444] argv[0] = start
[2016-02-09 10:00:32] [debug](javajni.c:941) [ 8444] Java Worker thread started org/elasticsearch/bootstrap/Elasticsearch:main
[2016-02-09 10:00:32] [debug](javajni.c:952) [ 8444] Exception has been thrown
[2016-02-09 10:00:32] [debug](javajni.c:964) [ 8444] Java Worker thread finished org/elasticsearch/bootstrap/Elasticsearch:main with status=6
[2016-02-09 10:00:33] [debug](prunsrv.c:1186) [ 5200] Java started org/elasticsearch/bootstrap/Elasticsearch
[2016-02-09 10:00:33] [info](prunsrv.c:1284) [ 5200] Service started in 1095 ms.
[2016-02-09 10:00:33] [debug](prunsrv.c:844) [ 5200] reportServiceStatusE: 4, 0, 0, 0
[2016-02-09 10:00:33] [debug](prunsrv.c:1528) [ 5200] Waiting for worker to finish...
[2016-02-09 10:00:33] [debug](prunsrv.c:1533) [ 5200] Worker finished.
[2016-02-09 10:00:33] [debug](prunsrv.c:1559) [ 5200] Waiting for all threads to exit
[2016-02-09 10:00:33] [debug](prunsrv.c:844) [ 5200] reportServiceStatusE: 3, 0, 0, 0
[2016-02-09 10:00:33] [debug](prunsrv.c:1563) [ 5200] JVM destroyed.
[2016-02-09 10:00:33] [debug](prunsrv.c:844) [ 5200] reportServiceStatusE: 1, 1066, 0, 1
[2016-02-09 10:00:33] [info](prunsrv.c:1598) [ 8288] Run service finished.
[2016-02-09 10:00:33] [info](prunsrv.c:1764) [ 8288] Commons Daemon procrun finished

What is missing?
Just to enable ELS service to run on a windows machine (single default node)
</description><key id="132370895">16528</key><summary>Unable to run Elasticsearch (2.2.0) windows service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">habibcs</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2016-02-09T09:34:12Z</created><updated>2016-04-18T10:14:52Z</updated><resolved>2016-04-18T10:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2016-02-09T22:16:05Z" id="182103523">@habibcs can you also share your `elasticsearch.log` file?
</comment><comment author="habibcs" created="2016-02-10T08:16:58Z" id="182248933">[logs.zip](https://github.com/elastic/elasticsearch/files/124367/logs.zip)
Hi in the log folder (subfolder inside elasticsearch folder) - these are the only log files - same as what I pasted above.
</comment><comment author="fenneh" created="2016-02-12T17:38:54Z" id="183424912">Having the exact same issue. Server 2012 R2 machine using

```
C:\Elasticsearch\bin&gt;java -version
java version "1.8.0_66"
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
```
</comment><comment author="robedhbw" created="2016-04-14T08:25:23Z" id="209825685">Hello,
I ve got the same error trying to install the jdbc river. Also on the Server 2012 R2..

Do someone have a workaround for this?
</comment><comment author="fenneh" created="2016-04-14T15:24:17Z" id="210001751">I'm actually using ES 2.3.1 now with no issues on the same config as previously posted. 
</comment><comment author="clintongormley" created="2016-04-14T17:13:42Z" id="210055334">@habibcs could you try 2.3.1 as well please?
</comment><comment author="gmarz" created="2016-04-14T18:56:05Z" id="210099072">@habibcs I noticed you have two elasticsearch jars in your class path:

[2016-02-09 10:00:32] debug [ 8444] Jvm Option[15] -Djava.class.path=C:\LogAggregator\elasticsearch/lib/elasticsearch-2.2.0.jar;C:\LogAggregator\elasticsearch/lib/antlr-runtime-3.5.jar;C:\LogAggregator\elasticsearch/lib/apache-log4j-extras-1.2.17.jar;C:\LogAggregator\elasticsearch/lib/asm-4.1.jar;C:\LogAggregator\elasticsearch/lib/asm-commons-4.1.jar;C:\LogAggregator\elasticsearch/lib/commons-cli-1.3.1.jar;C:\LogAggregator\elasticsearch/lib/compiler-0.8.13.jar;C:\LogAggregator\elasticsearch/lib/compress-lzf-1.0.2.jar;**C:\LogAggregator\elasticsearch/lib/elasticsearch-2.1.1.jar;C:\LogAggregator\elasticsearch/lib/elasticsearch-2.2.0.jar;**C:\LogAggregator\elasticsearch/lib/groovy-all-2.4.4-indy.jar;C:\LogAggregator\elasticsearch/lib/guava-18.0.jar;C:\LogAggregator\elasticsearch/lib/HdrHistogram-2.1.6.jar;C:\LogAggregator\elasticsearch/lib/hppc-0.7.1.jar;C:\LogAggregator\elasticsearch/lib/jackson-core-2.6.2.jar;C:\LogAggregator\elasticsearch/lib/jackson-dataformat-cbor-2.6.2.jar;C:\LogAggregator\elasticsearch/lib/jackson-dataformat-smile-2.6.2.jar;C:\

I am willing to bet that's your problem.  Was there already an existing elasticsearch installation in `C:\LogAggregator`?  If so, you should completely delete the existing `bin` and `lib` folders before extracting the new version.
</comment><comment author="habibcs" created="2016-04-15T07:21:13Z" id="210326098">Thanks for replying, I cannot test anymore (at least not now).
Got reply above (suggesting new version or now it would work) almost after two months; it wasn't a hobby project but was evaluating for client; and ultimately client decided other products.
Other ppl having the same issue, please recheck with the newer version and confirm if it is working, perhaps that can be useful specially @robedhbw 
</comment><comment author="clintongormley" created="2016-04-15T08:16:55Z" id="210352942">&gt; I am willing to bet that's your problem. Was there already an existing elasticsearch installation in C:\LogAggregator? If so, you should completely delete the existing bin and lib folders before extracting the new version.

@gmarz weird we didn't get a jarhell exception here?
</comment><comment author="gmarz" created="2016-04-15T14:03:46Z" id="210475324">@clintongormley I actually do get a jarhell exception

```
Exception in thread "main" anifest(JarHell.java:224)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:177)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:164)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
The data area passed to a system call is too small.

Failed to start service
```

It's indeed strange that the logs @habibcs posted do not show the same exception.  But the double jar files in the class path indicate that something is probably funky with the installation- so it could be something else, but I'm not able to reproduce this otherwise.

@robedhbw I don't think you're running into the same issue. [Rivers have been removed in 2.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_removed_features.html#_rivers_have_been_removed), so if you're trying to install the JDBC river with ES 2.2, it won't work :).
</comment><comment author="robedhbw" created="2016-04-15T15:05:44Z" id="210497422">@gmarz Ou thanks. Is there any other useful plugin to import Ms Sql databases`? Thanks.
</comment><comment author="clintongormley" created="2016-04-18T10:14:52Z" id="211312554">i think we can close this issue then
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove suppressAccessCheck permission for Groovy script plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16527</link><project id="" key="" /><description>As part of #16196, the (undesirable) `supressAccessChecks` permission was granted to the ES Groovy plugin to not break existing scripts with Groovy closures running under the security manager. At the same time, a patch was contributed to Groovy (https://github.com/apache/groovy/pull/248 and https://issues.apache.org/jira/browse/GROOVY-7735) that makes it possible to use Groovy closures under a security manager without granting the `supressAccessChecks` permission. This issue serves as a reminder that once Groovy 2.4.6 is released (current is 2.4.5), we can go ahead and remove the undesirable permission. 
</description><key id="132362795">16527</key><summary>Remove suppressAccessCheck permission for Groovy script plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2016-02-09T08:53:51Z</created><updated>2016-02-27T18:04:38Z</updated><resolved>2016-02-27T16:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlighting for top hits aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16526</link><project id="" key="" /><description>According to the documentation for top hits aggregations (ES 2.1), there is support for highlighting of the hits returned. Is there an example of how to implement this? 

I've tried the following (bucket agg combined with top hits agg) and it doesn't seem to return any highlighted fields:

```
{'aggs': {
    'dup_groups': {
        'aggs': {
            'first_doc': {
                'top_hits': { 
                    'highlight': {'fields': {'text.stemmed': {}}},
                    'size': 1,
                    'sort': 'date'}}},
    'terms': {'field': 'dup_parent', 'size': 0}}},
 'query': {'query_string': {'query': 'world'}}}
```
</description><key id="132359905">16526</key><summary>Highlighting for top hits aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkphlam</reporter><labels /><created>2016-02-09T08:38:38Z</created><updated>2016-02-13T18:20:25Z</updated><resolved>2016-02-13T18:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-09T09:05:59Z" id="181769885">How is `text.stemmed` field mapped in your mapping?
</comment><comment author="pkphlam" created="2016-02-09T09:10:34Z" id="181771409">```
{'mappings': {'documents': {'text.stemmed': {'full_name': 'text.stemmed',
     'mapping': {'stemmed': {'analyzer': 'doc_analyzer_stemmed',
       'term_vector': 'with_positions_offsets',
       'type': 'string'}}}}}}}
```

I should note that highlighting on `text.stemmed` works perfectly fine in a regular search (not in an aggregation).
</comment><comment author="pkphlam" created="2016-02-10T04:49:35Z" id="182196956">Hmm okay so for future reference, the answer to this is that highlight must be defined BOTH outside and inside the aggregation for it to work, not just one or the other.

```
{'aggs': {'dup_groups': {'aggs': {'first_doc': {'top_hits': {'highlight': {'fields': {'text.stemmed': {}}},
      'size': 1,
      'sort': 'date'}}},
   'terms': {'field': 'dup_parent', 'size': 100}}},
 'highlight': {'fields': {'text.stemmed': {'number_of_fragments': 0}}},
 'query': {'query_string': {'default_field': 'text.stemmed',
   'query': 'world'}},
 'sort': [{'_score': {'order': 'desc'}}]}
```
</comment><comment author="martijnvg" created="2016-02-10T07:36:27Z" id="182238301">This is unexpected. Highlighting specified inside a `top_hits` agg should work irregardless if normal highlighting is specified. 
</comment><comment author="clintongormley" created="2016-02-13T18:20:25Z" id="183717539">This appears to work just fine. Tested on 2.1.0. and 2.2.0:

```
PUT t
{
  "mappings": {
    "documents": {
      "properties": {
        "text": {
          "type": "string",
          "fields": {
            "stemmed": {
              "term_vector": "with_positions_offsets",
              "type": "string",
              "analyzer": "english"
            }
          }
        }
      }
    }
  }
}

PUT t/documents/1
{
  "dup_parent": "foo",
  "text": "hello world",
  "date": "2001-01-01"
}

GET t/_search
{
  "size": 0,
  "aggs": {
    "dup_groups": {
      "aggs": {
        "first_doc": {
          "top_hits": {
            "highlight": {
              "fields": {
                "text.stemmed": {}
              }
            },
            "size": 1,
            "sort": "date"
          }
        }
      },
      "terms": {
        "field": "dup_parent",
        "size": 100
      }
    }
  },
  "query": {
    "query_string": {
      "default_field": "text.stemmed",
      "query": "world"
    }
  },
  "sort": [
    {
      "_score": {
        "order": "desc"
      }
    }
  ]
}
```

Please reopen if you can provide a curl recreation like the above which demonstrates the problem.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException for has_child query with avg score_mode and explain turned on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16525</link><project id="" key="" /><description>Elasticsearch version 2.2.0
java version 1.8.0_25

Steps to reproduce:

```
# clean start
curl -XDELETE localhost:9200/blogs

# prepare parent child relationship
curl -XPUT localhost:9200/blogs -d '{
  "mappings": {
    "blog_tag": {
      "_parent" : {
         "type" : "doc"
      }
    }
  }
}'

# index parent document containing the word 'title'
curl -XPUT 'http://localhost:9200/blogs/doc/1' -d '{
    "message" : "my title"
}'

# index child document NOT containing the word 'title'
curl -XPUT localhost:9200/blogs/blog_tag/1?parent=1 -d '{
    "tag" : "my tag"
}'

# search for any parent or child matching 'title' with explain turned on and
# 'avg' score_mode for child scoring
curl -XGET http://localhost:9200/blogs/_search?pretty -d '{
    "explain": true,
    "query": {
        "bool": {
            "should": [{
                "query_string": {"query": "title"}
            }, {
                "has_child": {
                    "query": {
                        "query_string": {"query": "title"}
                    },
                    "score_mode": "avg",
                    "type": "blog_tag"
                }
            }]
        }
    }
}'
```

The query raises a NullPointerException and returns:

```
[2016-02-09 09:07:13,740][DEBUG][action.search.type       ] [Manta] [blogs][0], node[oYZDW2pHT6SYvh-uTmIKIQ], [P], v[2], s[STARTED], a[id=dnJfwYnXTcahkyT8g019mQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@611b4d10]
RemoteTransportException[[Manta][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerException;
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Occurrences.getOccurrence(GlobalOrdinalsWithScoreCollector.java:375)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Avg.score(GlobalOrdinalsWithScoreCollector.java:229)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreQuery$W.explain(GlobalOrdinalsWithScoreQuery.java:131)
    at org.apache.lucene.search.BooleanWeight.explain(BooleanWeight.java:148)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:875)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:852)
    at org.elasticsearch.search.internal.ContextIndexSearcher.explain(ContextIndexSearcher.java:128)
    at org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase.hitExecute(ExplainFetchSubPhase.java:61)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:187)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:478)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-09 09:07:13,741][DEBUG][action.search.type       ] [Manta] All shards failed for phase: [query_fetch]
RemoteTransportException[[Manta][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerException;
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Occurrences.getOccurrence(GlobalOrdinalsWithScoreCollector.java:375)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Avg.score(GlobalOrdinalsWithScoreCollector.java:229)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreQuery$W.explain(GlobalOrdinalsWithScoreQuery.java:131)
    at org.apache.lucene.search.BooleanWeight.explain(BooleanWeight.java:148)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:875)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:852)
    at org.elasticsearch.search.internal.ContextIndexSearcher.explain(ContextIndexSearcher.java:128)
    at org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase.hitExecute(ExplainFetchSubPhase.java:61)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:187)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:478)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-02-09 09:07:13,742][INFO ][rest.suppressed          ] /blogs/_search Params: {pretty=, index=blogs}
Failed to execute phase [query_fetch], all shards failed; shardFailures {[oYZDW2pHT6SYvh-uTmIKIQ][blogs][0]: RemoteTransportException[[Manta][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: NullPointerException; }
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:228)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ; nested: NullPointerException;
    at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:386)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
    at java.lang.Throwable.printStackTrace(Throwable.java:665)
    at java.lang.Throwable.printStackTrace(Throwable.java:721)
    at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
    at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
    at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
    at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
    at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
    at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
    at org.apache.log4j.Category.callAppenders(Category.java:206)
    at org.apache.log4j.Category.forcedLog(Category.java:391)
    at org.apache.log4j.Category.log(Category.java:856)
    at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo(Log4jESLogger.java:125)
    at org.elasticsearch.common.logging.support.AbstractESLogger.info(AbstractESLogger.java:90)
    at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:131)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:96)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:87)
    at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAction.java:316)
    ... 10 more
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Occurrences.getOccurrence(GlobalOrdinalsWithScoreCollector.java:375)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreCollector$Avg.score(GlobalOrdinalsWithScoreCollector.java:229)
    at org.apache.lucene.search.join.GlobalOrdinalsWithScoreQuery$W.explain(GlobalOrdinalsWithScoreQuery.java:131)
    at org.apache.lucene.search.BooleanWeight.explain(BooleanWeight.java:148)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:875)
    at org.apache.lucene.search.IndexSearcher.explain(IndexSearcher.java:852)
    at org.elasticsearch.search.internal.ContextIndexSearcher.explain(ContextIndexSearcher.java:128)
    at org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase.hitExecute(ExplainFetchSubPhase.java:61)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:187)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:478)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    ... 3 more
{
  "error" : {
    "root_cause" : [ {
      "type" : "null_pointer_exception",
      "reason" : null
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query_fetch",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "blogs",
      "node" : "oYZDW2pHT6SYvh-uTmIKIQ",
      "reason" : {
        "type" : "null_pointer_exception",
        "reason" : null
      }
    } ]
  },
  "status" : 500
}
```

'avg' is the only score_mode that raises this exception, other modes work correctly. Also if I turn explain off, the exception is not raised.
In version 1.3.7 of Elasticsearch the above sequence works without raising an exception. I have not tested any version in-between.
</description><key id="132357166">16525</key><summary>NullPointerException for has_child query with avg score_mode and explain turned on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fxh</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2016-02-09T08:19:18Z</created><updated>2016-03-04T19:06:53Z</updated><resolved>2016-03-04T19:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-09T08:23:40Z" id="181756229">@fxh Thanks for reporting, this is clearly a bug.
</comment><comment author="martijnvg" created="2016-03-03T22:47:21Z" id="192003021">This error is caused by a bug in the joining code in Lucene. The joining code always tries to compute a score explanation even in the case that a document doesn't match. With `score_mode` set to `avg` this causes a NPE and with other score modes it returns a score explanation that indicates that the document matched while it didn't.

I opened an issue in Lucene to fix the problem: https://issues.apache.org/jira/browse/LUCENE-7065
</comment><comment author="clintongormley" created="2016-03-04T19:06:53Z" id="192416576">[LUCENE-7065](https://issues.apache.org/jira/browse/LUCENE-7065) has been fixed in Lucene 6.0, and so will be fixed in Elasticsearch 5.0.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Builds term suggestion execution context from a term suggestion builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16524</link><project id="" key="" /><description>@cbuescher I wanted to create this PR against your phrase-suggest-build branch, but didn't have access, so just created this PR for your review. 
</description><key id="132335000">16524</key><summary>Builds term suggestion execution context from a term suggestion builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-02-09T05:27:45Z</created><updated>2016-02-12T16:19:49Z</updated><resolved>2016-02-12T16:19:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-10T22:45:20Z" id="182617429">@cbuescher @areek @colings86 This needs review.  Tests passed locally.
</comment><comment author="cbuescher" created="2016-02-11T11:25:39Z" id="182815410">@abeyad great change, I took a look and left a few comments. Most of them minor, but one thing I'd like to look into more closely is the way we currently set the analyzer if not specified in the suggestion element itself. The order of precedens isn't completely clear to me, I'd need a second opinion from @areek here possibly.
</comment><comment author="abeyad" created="2016-02-11T23:45:21Z" id="183112438">@cbuescher @areek I implemented the changes and also added Areek's suggestion for handling analyzers along with tests.  All tests passed locally.
</comment><comment author="areek" created="2016-02-12T15:30:42Z" id="183374607">@abeyad this looks great :), LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loading templates via templates/ directory is not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16523</link><project id="" key="" /><description>Loading templates via templates/ directory is not working with elastic 2.x , so is this feature removed or its a bug?

That is I placed my templates in $HOME/config/templates/abc.json

But it was not taken while index creation in 2.x, where as its working with 1.7.1
</description><key id="132317836">16523</key><summary>Loading templates via templates/ directory is not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prashanttct07</reporter><labels /><created>2016-02-09T02:55:53Z</created><updated>2016-02-09T06:52:01Z</updated><resolved>2016-02-09T06:52:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-09T06:52:01Z" id="181736122">Removed. See https://github.com/elastic/elasticsearch/pull/11052
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename variables.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16522</link><project id="" key="" /><description>This PR renames the following three variables to fix a typo `settting` into `setting`.
- Rename a static class member:
  INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTTING -&gt; INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING
- Rename a parameter: aSettting --&gt; aSetting
- Rename a local variable: indexSetttings -&gt; indexSettings
</description><key id="132310047">16522</key><summary>Rename variables.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>:Settings</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T01:54:58Z</created><updated>2016-02-26T18:53:20Z</updated><resolved>2016-02-09T09:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-09T09:35:00Z" id="181781426">(T)thanks @dongjoon-hyun . I'll merge it in... 
</comment><comment author="dongjoon-hyun" created="2016-02-09T09:54:05Z" id="181788446">Thank you for merging, @bleskes ! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Created introductory paragraph named "What is Elasticsearch" explaini&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16521</link><project id="" key="" /><description>&#8230;ng what Elasticsearch is in layman's terms.
</description><key id="132305062">16521</key><summary>Created introductory paragraph named "What is Elasticsearch" explaini&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zsadler</reporter><labels /><created>2016-02-09T01:20:44Z</created><updated>2016-02-09T01:21:20Z</updated><resolved>2016-02-09T01:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add node version check to shard allocation during restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16520</link><project id="" key="" /><description>Verifies that the version of a node is compatible with the version of a shard that's being restored on this node.

Fixes #16519
</description><key id="132302008">16520</key><summary>Add node version check to shard allocation during restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-09T00:58:16Z</created><updated>2016-02-23T05:00:47Z</updated><resolved>2016-02-23T05:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-09T02:38:41Z" id="181675465">LGTM
</comment><comment author="bleskes" created="2016-02-09T09:31:47Z" id="181779503">Thx @imotov . In the bug description you refer to "In a mixed cluster with a newer master node". Why is that newer master node relevant?
</comment><comment author="imotov" created="2016-02-09T12:31:56Z" id="181845713">@bleskes Because a master that has a version older than the snapshot will refuse to restore the snapshot right away. So, we will not even get to shard allocation and therefore this problem will not occur. The problem occurs when we have a master that is version-compatible with the snapshot and some data nodes that are not. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Newer snapshot shards may be allocated on older nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16519</link><project id="" key="" /><description>In a mixed cluster with a newer master node and a mixture of older and newer data nodes, shards from a snapshot compatible with newer data nodes might be randomly assigned to older data nodes where they cannot be restored.

The problem is reproducible using SnapshotBackwardsCompatibilityIT.testBasicWorkflow with the following command line in 2.x branch

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:2.1 -Dtests.seed=A8C99BB8980117DF -Dtests.class=org.elasticsearch.bwcompat.SnapshotBackwardsCompatibilityIT -Dtests.method="testBasicWorkflow" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Des.node.mode=local -Dtests.locale=fr-BE -Dtests.timezone=Asia/Thimphu
```

This test fails because it tries to allocated shards from the snapshot created on 2.x to nodes that are running 2.1.
</description><key id="132278678">16519</key><summary>Newer snapshot shards may be allocated on older nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2016-02-08T22:46:10Z</created><updated>2016-02-23T05:00:46Z</updated><resolved>2016-02-23T05:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T20:21:38Z" id="183747252">Might a better solution be to disallow restores in a mixed cluster of master/data nodes?
</comment><comment author="imotov" created="2016-02-16T17:25:54Z" id="184786026">@clintongormley I don't think we can guarantee that an old nodes will not join the cluster later on - after we performed an initial check and allowed the restore to start. So, checking the state at the beginning of the restore will not be a complete solution.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to skip signing rpm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16518</link><project id="" key="" /><description>Fix contributing.md note about fedora.
</description><key id="132258058">16518</key><summary>Add option to skip signing rpm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>build</label><label>v2.3.0</label></labels><created>2016-02-08T21:12:57Z</created><updated>2016-02-08T21:37:03Z</updated><resolved>2016-02-08T21:37:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-08T21:24:54Z" id="181573214">It looks like it bypassed signing the RPM, but it needs to bypass signing the DEB too:

```
[INFO] --- jdeb:1.4:jdeb (default) @ elasticsearch ---
[INFO] Passphrase was not encrypted
[INFO] Creating debian package: /home/hinmanm/es/elasticsearch/distribution/deb/target/releases/elasticsearch-2.3.0-SNAPSHOT.deb
[WARNING] Signing requested, but no key supplied
[ERROR] Failed to create debian package /home/hinmanm/es/elasticsearch/distribution/deb/target/releases/elasticsearch-2.3.0-SNAPSHOT.deb
org.vafer.jdeb.PackagingException: Failed to create debian package /home/hinmanm/es/elasticsearch/distribution/deb/target/releases/elasticsearch-2.3.0-SNAPSHOT.deb
    at org.vafer.jdeb.DebMaker.makeDeb(DebMaker.java:296)
    at org.vafer.jdeb.maven.DebMojo.execute(DebMojo.java:552)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: java.lang.NullPointerException
    at org.vafer.jdeb.signing.PGPSigner.getSecretKey(PGPSigner.java:146)
    at org.vafer.jdeb.signing.PGPSigner.&lt;init&gt;(PGPSigner.java:54)
    at org.vafer.jdeb.DebMaker.makeDeb(DebMaker.java:280)
    ... 23 more
```
</comment><comment author="nik9000" created="2016-02-08T21:25:31Z" id="181573359">&gt; It looks like it bypassed signing the RPM, but it needs to bypass signing the DEB too:

Bah. I'm getting sloppy. Will fix.
</comment><comment author="nik9000" created="2016-02-08T21:30:17Z" id="181575180">@dakrone that should help
</comment><comment author="dakrone" created="2016-02-08T21:36:33Z" id="181577988">@nik9000 tested and this works now, LGTM, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logstash -b is unrecognized option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16517</link><project id="" key="" /><description>Has this been deprecated without warning? I no longer see this option in logstash which is weird because I see it here https://github.com/elastic/logstash/blob/master/logstash-core/lib/logstash/pipeline.rb and in the documentation.

&gt; -b, --pipeline-batch-size SIZE
&gt;  This parameter defines the maximum number of events an individual worker thread will collect
&gt;  before attempting to execute its filters and outputs. Default is 125 events.
&gt;  Larger batch sizes are generally more efficient, but come at the cost of increased memory
&gt;  overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
&gt;  variable to effectively use the option.

`./logstash -b
Clamp::UsageError: Unrecognised option '-b'
  signal_usage_error at /opt/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:103
         find_option at /opt/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/option/parsing.rb:62
       parse_options at /opt/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/option/parsing.rb:28
               parse at /opt/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:52
                 run at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.1.2-java/lib/logstash/runner.rb:79
                call at org/jruby/RubyProc.java:281
                 run at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-2.1.2-java/lib/logstash/runner.rb:95
                call at org/jruby/RubyProc.java:281
          initialize at /opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/task.rb:24
`
</description><key id="132234950">16517</key><summary>Logstash -b is unrecognized option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2016-02-08T19:25:55Z</created><updated>2016-02-08T20:05:34Z</updated><resolved>2016-02-08T19:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2016-02-08T19:32:33Z" id="181532463">The following is the only valid configuration flags that I am allowed to pass:

`Options:
    -f, --config CONFIG_PATH      Load the logstash config from a specific file
                                  or directory.  If a directory is given, all
                                  files in that directory will be concatenated
                                  in lexicographical order and then parsed as a
                                  single config file. You can also specify
                                  wildcards (globs) and any matched files will
                                  be loaded in the order described above.
    -e CONFIG_STRING              Use the given string as the configuration
                                  data. Same syntax as the config file. If no
                                  input is specified, then the following is
                                  used as the default input:
                                  "input { stdin { type =&gt; stdin } }"
                                  and if no output is specified, then the
                                  following is used as the default output:
                                  "output { stdout { codec =&gt; rubydebug } }"
                                  If you wish to use both defaults, please use
                                  the empty string for the '-e' flag.
                                   (default: "")
    -w, --filterworkers COUNT     Sets the number of filter workers to run.
                                   (default: 0)
    -l, --log FILE                Write logstash internal logs to the given
                                  file. Without this flag, logstash will emit
                                  logs to standard output.
    -v                            Increase verbosity of logstash internal logs.
                                  Specifying once will show 'informational'
                                  logs. Specifying twice will show 'debug'
                                  logs. This flag is deprecated. You should use
                                  --verbose or --debug instead.
    --quiet                       Quieter logstash logging. This causes only 
                                  errors to be emitted.
    --verbose                     More verbose logging. This causes 'info' 
                                  level logs to be emitted.
    --debug                       Most verbose logging. This causes 'debug'
                                  level logs to be emitted.
    -V, --version                 Emit the version of logstash and its friends,
                                  then exit.
    -p, --pluginpath PATH         A path of where to find plugins. This flag
                                  can be given multiple times to include
                                  multiple paths. Plugins are expected to be
                                  in a specific directory hierarchy:
                                  'PATH/logstash/TYPE/NAME.rb' where TYPE is
                                  'inputs' 'filters', 'outputs' or 'codecs'
                                  and NAME is the name of the plugin.
    -t, --configtest              Check configuration for valid syntax and then exit.
    --[no-]allow-unsafe-shutdown  Force logstash to exit during shutdown even
                                  if there are still inflight events in memory.
                                  By default, logstash will refuse to quit until all
                                  received events have been pushed to the outputs.
                                   (default: false)
    -h, --help                    print help
`
</comment><comment author="jasontedor" created="2016-02-08T19:40:58Z" id="181535224">There is a dedicated [repository](https://github.com/elastic/logstash) for Logstash. 
</comment><comment author="ghost" created="2016-02-08T20:05:33Z" id="181544277">Apologies, my bad.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set meta data for pipeline aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16516</link><project id="" key="" /><description>Closes #16484
</description><key id="132234885">16516</key><summary>Set meta data for pipeline aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T19:25:33Z</created><updated>2016-02-13T13:21:28Z</updated><resolved>2016-02-11T17:27:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-09T17:19:20Z" id="181965244">LGTM we should probably back port this to 2.x and 2.2 as well
</comment><comment author="colings86" created="2016-02-09T17:20:02Z" id="181965694">Actually, one minor thing, could you add a test to MetaDataIT to test metadata with pipeline aggs?
</comment><comment author="gmarz" created="2016-02-09T17:31:24Z" id="181971856">@colings86 thanks for the review.  Definitely, I'll add a test shortly.
</comment><comment author="gmarz" created="2016-02-10T22:58:12Z" id="182622400">@colings86 mind giving this another review?

I've changed MetaDataIT completely so that we test meta data on all three aggregation types.  While doing this though, I discovered that there wasn't a way to set meta data via MetricsAggregationBuilder, so fixed that as well.
</comment><comment author="colings86" created="2016-02-11T09:16:42Z" id="182776929">@gmarz LGTM, thanks for fixing this. The test looks good too and is going to be important as the aggregation refactoring I am doing at the moment will change the builders quite a bit so it's good that I have something to catch it if I break this in that refactoring :)
</comment><comment author="gmarz" created="2016-02-11T22:45:43Z" id="183089799">Merged to master and back ported to 2.0-2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return IP type for inner hits in IP format (dot decimal)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16515</link><project id="" key="" /><description>Data of type IP is returned in decimal equivalent of 32bit for inner hits (as fielddata_fields). 
I have not found a way to return them in dot decimal format (x.x.x.x). 

I have put this question up for discussion in the forum a while ago:
[https://discuss.elastic.co/t/return-ip-field-from-inner-hit-in-dot-decimal-format/40452/3](url)
but nobody seems to have an idea how to solve it.
</description><key id="132228505">16515</key><summary>Return IP type for inner hits in IP format (dot decimal)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abulhol</reporter><labels /><created>2016-02-08T18:57:42Z</created><updated>2016-02-13T12:38:53Z</updated><resolved>2016-02-08T19:13:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-08T19:13:31Z" id="181525605">Duplicates #16290 
</comment><comment author="abulhol" created="2016-02-08T21:23:16Z" id="181572801">Hi Jason,
I know that I reported this before, but I do not see any real solution to this issue.
It may look trivial, but once you need to look at IP addresses, it is crucial.
Any suggestions welcome.
</comment><comment author="abulhol" created="2016-02-10T14:52:34Z" id="182405919">Hi again, 
I have found the solution to this problem, see here: 
https://discuss.elastic.co/t/return-ip-field-from-inner-hit-in-dot-decimal-format/40452
</comment><comment author="clintongormley" created="2016-02-13T12:38:53Z" id="183658771">@abulhol thanks for posting the solution - i think requiring the relative path is inconsistent, and i've opened https://github.com/elastic/elasticsearch/issues/16653 to get this changed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch Allows Decimal For Type BYTE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16514</link><project id="" key="" /><description>ElasticSearch documentation states:

`The number types have the same ranges as corresponding Java types`

[Java types](http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html) is linked and states the following:

`The byte data type is an 8-bit signed two's complement integer.`

Yet when I have a field mapped as type "byte" I can store a decimal when indexing it as a number. When I index the number as a string it gives me the correct error.

**Mapping**

```
POST /my_index
{
    "mappings": {
        "my_type": {
            "properties": {
                "my_byte": {
                    "type": "byte"
                }
            }
        }
    }
}
```

**Index as number**

```
POST /my_index/my_type
{
   "my_byte":  98.6
}
```

```
{
   "_index": "my_index",
   "_type": "my_type",
   "_id": "AVLCEWrzgBH0py33OQn-",
   "_version": 1,
   "_shards": {
      "total": 2,
      "successful": 1,
      "failed": 0
   },
   "created": true
}
```

**GET request and response**

```
GET /my_index/my_type/AVLCEWrzgBH0py33OQn-
```

```
{
   "_index": "my_index",
   "_type": "my_type",
   "_id": "AVLCEWrzgBH0py33OQn-",
   "_version": 1,
   "found": true,
   "_source": {
      "my_byte": 98.6
   }
}
```

**Unsuccessful INDEX as string (correct behavior)**

```
POST /my_index/my_type
{
    "my_byte":"98.6"
}
```

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse [my_byte]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "failed to parse [my_byte]",
      "caused_by": {
         "type": "number_format_exception",
         "reason": "For input string: \"98.6\""
      }
   },
   "status": 400
}
```
</description><key id="132221906">16514</key><summary>ElasticSearch Allows Decimal For Type BYTE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">threejeez</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-02-08T18:26:48Z</created><updated>2016-10-24T14:48:33Z</updated><resolved>2016-10-24T14:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-10T23:38:04Z" id="182636246">The behavior you see is actually expected, at least when passing in the decimal. The `coerce` setting of numeric fields defaults to true, and part of the definition of coerce is `Floating points will be truncated for integer values.` . See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/coerce.html.

I believe the bug here is actually in what you are seeing for string behavior. The current code does a "coerce check" in the string case, but that only guards before it tries to call Integer.parseInt. However, in the coerce case, Double.parseDouble should be called, and then longValue(). @clintongormley thoughts?
</comment><comment author="threejeez" created="2016-02-11T15:46:48Z" id="182924306">@rjernst Thanks for your response. Based on the link you provided - and considering the mapping above - coerce should be set to the default value of true. But it doesn't seem to be coercing anything, evidenced by the result I'm getting when retrieving the document:

```
   "_source": {
      "my_byte": 98.6 
   }
```

The value I'm getting back **is a double**. If it's being coerced at all (while indexed or retrieved) I would expect to get back the truncated value of `98`.
</comment><comment author="rjernst" created="2016-02-11T15:53:57Z" id="182929375">The value is coerced before indexing, but source is never changed (imagine you want to reindex as a float in the future).  If you want source to be truncated, you will need to do that yourself when ingesting the docs. 
</comment><comment author="threejeez" created="2016-02-11T16:34:32Z" id="182945933">Your point about _source makes perfect sense. Thank you for clarifying. Can you please point out how to get the coerced (int) value? I assumed it would be by specifying the `fields` parameter, but I'm seeing a difference between fields returned when doing a GET vs a SEARCH:

This: `curl search.keenhome.io:9200/my_index/my_type/_search?fields=my_byte`
Yeilds a double: `"fields":{"my_byte":[98.6]}`

Whereas this: `curl search.keenhome.io:9200/my_index/my_type/AVLCEWrzgBH0py33OQn-?fields=my_byte`
Yields an int: `"fields":{"my_byte":[98]}`

Is this expected behavior?
</comment><comment author="clintongormley" created="2016-02-13T13:18:15Z" id="183662842">@rjernst makes sense to me.  Perhaps this should be folded into a bigger cleanup as in https://github.com/elastic/elasticsearch/issues/11513
</comment><comment author="jpountz" created="2016-10-24T14:48:33Z" id="255762204">This is fixed in 5.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Register bootstrap settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16513</link><project id="" key="" /><description>This commit registers bootstrap settings used on startup. Without
registration, setting any of these settings causes node startup to
fail. By registering these settings (rather than clearing) after use, we
enable them to be visible in any APIs that show all settings.
</description><key id="132219530">16513</key><summary>Register bootstrap settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T18:15:12Z</created><updated>2016-02-08T21:36:35Z</updated><resolved>2016-02-08T19:18:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-08T18:16:15Z" id="181507203">Current behavior in master without this is

```
$ ./bin/elasticsearch -Dbootstrap.mlockall=false
[2016-02-08 12:35:17,937][INFO ][node                     ] [Clive] version[3.0.0-SNAPSHOT], pid[99582], build[cbeae22/2016-02-08T16:15:33.123Z]
[2016-02-08 12:35:17,938][INFO ][node                     ] [Clive] initializing ...
[2016-02-08 12:35:18,283][INFO ][plugins                  ] [Clive] modules [lang-mustache, ingest-grok, lang-expression, lang-groovy], plugins []
[2016-02-08 12:35:18,304][INFO ][env                      ] [Clive] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [124.6gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-02-08 12:35:18,304][INFO ][env                      ] [Clive] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-02-08 12:35:18,305][WARN ][env                      ] [Clive] max file descriptors [10240] for elasticsearch process likely too low, consider increasing to [65536]
Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [boostrap.mlockall]
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:227)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:216)
        at org.elasticsearch.common.settings.SettingsModule.configure(SettingsModule.java:60)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:224)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:156)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:186)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:303)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:37)
Refer to the log for complete error details.
```
</comment><comment author="s1monw" created="2016-02-08T19:08:20Z" id="181523943">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search_analyzer without analyzer, and/or docvalues with analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16512</link><project id="" key="" /><description>When a field is not analyzed on index time, it is not possible to specify a search_analyzer.
I get a "analyzer on field [xxx] must be set when search_analyzer is set" exception.

To me this does not make sense. I can set a keyword analyzer on the field to circumvent this exception, which does exactly the same thing. However, while doing that, I cannot enable doc_values.

I find myself duplicating fields many times because of this limitations.
Those limitations are:
- docvalues don't work on analyzed fields
- search_analyzer cannot be specified on non-analyzed fields.

I presume ES is trying to protect me, but I _know_ what I'm doing.
Is it possible to make this behavior more lenient?
</description><key id="132212951">16512</key><summary>search_analyzer without analyzer, and/or docvalues with analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pweerd</reporter><labels /><created>2016-02-08T17:50:10Z</created><updated>2016-02-13T12:52:52Z</updated><resolved>2016-02-13T12:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-08T20:05:08Z" id="181543949">&gt;  I know what I'm doing.

I believe you :) . Can you give a concrete example using documents and queries? I think it will help understanding the need for the request / perhaps offer an alternative?
</comment><comment author="pweerd" created="2016-02-10T08:11:16Z" id="182247694">Trying to index TV shows, also using aggregations.

### First attempt, using fielddata:

``` bash
curl -XPUT localhost:9200/tv_shows -d'
{
   settings: {  
      "analysis" : {
         "analyzer" : {
             "lc_text" : {
                "tokenizer" : "keyword",
                "filter": ["lowercase", "asciifolding"]
             }
          }
      }   
   },
   "mappings": {
      "doc": {
         "properties": { 
            "tv_show": {"type": "string", "analyzer" : "lc_text"}
         }
      }
   }
}'
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 Tu&#269;&#328;&#225;ci" }'
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 Tucnaci" }'
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 tucnaci" }'

curl -XPUT localhost:9200/_search -d'
{
   "aggs": {
      "tv_show": {
         "terms": {
            "field": "tv_show"
         }
      }
   }
}'
```

Everything works fine, but fielddata is used, and common sense is that we should move to doc_values.
OK, trying that:

### Second attempt, using docvalues :

``` bash
curl -XPUT localhost:9200/tv_shows -d'
{
   "mappings": {
      "doc": {
         "properties": { 
            "tv_show": {"type": "string", "index" : "not_analyzed"}
         }
      }
   }
}'
```

But now the previous query obviously returns 3 different shows in the aggregation.
So, I have to do the normalization in my code. I'll do that. Though not beautiful, because it is a-symmetric)

Using the same mapping, I do:

``` bash
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 tucnaci" }'
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 tucnaci" }'
curl -XPOST localhost:9200/tv_shows/doc -d '{ "tv_show":"3-2-1 tucnaci" }'
```

And we won: the query returns only 1 aggregation.
However, when users are copy-pasting the real TV show nam,e into the query box, they find nothing (of course).
So, I tried to use a search analyzer:

### Third attempt, using search_analyzer:

``` bash
curl -XPUT localhost:9200/tv_shows -d'
{
   settings: {  
      "analysis" : {
         "analyzer" : {
             "lc_text" : {
                "tokenizer" : "standard",
                "filter": ["lowercase", "asciifolding"]
             }
          }
      }   
   },
   "mappings": {
      "doc": {
         "properties": { 
            "tv_show": {"type": "string", "index" : "not_analyzed", "search_analyzer": "lc_text"}
         }
      }
   }
}'
```

However, this is not accepted by ES, it states: "analyzer on field [tv_show] must be set when search_analyzer is set"
That is somewhat strange, because putting a keyword analyzer there (which is semantically the same as having no analyzer), would fix this exception.

### Fouth attempt, using 2 fields:

``` bash
curl -XPUT localhost:9200/tv_shows -d'
{
   settings: {  
      "analysis" : {
         "analyzer" : {
             "lc_text" : {
                "tokenizer" : "keyword",
                "filter": ["lowercase", "asciifolding"]
             }
          }
      }   
   },
   "mappings": {
      "doc": {
         "properties": { 
            "tv_show": {"type": "string", "index" : "no", "copy_to": ["tv_show_a", "tv_show_s"]}
            "tv_show_a": {"type": "string", "index" : "not_analyzed"}
            "tv_show_s": {"type": "string", "analyzer": "lc_text"}
         }
      }
   }
}'
```

This will work, but now I have to educate people to use tv_show_a for aggregations and tv_show_s for searching.
I know, doing this makes it even possible to use the standard tokenizer and let the user search for words in the show.
However, that is currently not the use case.

### Whishes:
- It would be great to have the ability to specify an analyzer specific for doc_values. Something like "docvalues_analyzer": "lc_keyword".
- Furthermore, is it possible to make a search_analyzer work on a not-analyzed field? 
</comment><comment author="rjernst" created="2016-02-10T20:01:29Z" id="182554581">&gt; It would be great to have the ability to specify an analyzer specific for doc_values. Something like "docvalues_analyzer": "lc_keyword".

Doc values doesn't have anything directly to do with analysis, but anything that is analyzed (ie broken up into multiple terms) cannot be stored in doc values. However, we would like to allow simple normalization like you want to do here on keyword fields, see this issue that is in progress: #12394.

&gt; Furthermore, is it possible to make a search_analyzer work on a not-analyzed field?

You can set the search analyzer on any string field, but it is required to specify the analyzer. This is to make the user understand that the index time analysis still happens with a different analyzer. In your case, I would allow searching with your normalized field, but aggregate with a non analyzed multi field:

```
"mappings": {
    "doc": {
        "properties": { 
            "tv_show": {
                "type": "string",
                "analyzer": "lc_text",
                "fields": {
                    "original" : {
                        "type": "string",
                        "index": "not_analyzed"
                    }
                }
            }
         }
      }
   }
}
```

Now with this you would still search using your `lc_text` analyzer, but you should specify `tv_show.original` for doing aggregations.
</comment><comment author="pweerd" created="2016-02-10T21:34:59Z" id="182591686">&gt; Doc values doesn't have anything directly to do with analysis, but anything that is analyzed (ie broken up into multiple terms) cannot be stored in doc values. However, we would like to allow simple normalization like you want to do here on keyword fields, see this issue that is in progress: #12394

I partially agree. It can be dangerous to let the user put multiple terms in the index. But, suppose that we have a field with a few comma-separated values... Then it makes sense. 
Analyzing was of course tied to the Lucene indexed fields. But why not broadening that, and make it available for doc-values as well?

&gt; You can set the search analyzer on any string field, but it is required to specify the analyzer. This is to make the user understand that the index time analysis still happens with a different analyzer.

I did understand what was going on here. But for the sake of 'understanding' we make things impossible?  :-(
Maybe it is time for advanced mode vs novice mode. Like is done with the 

``` bash
PUT _cluster/settings
{
    "transient": {
        "action.destructive_requires_name": true
    }
}
```

ES lets you choose here between protecting yourself or not.

&gt; Now with this you would still search using your lc_text analyzer, but you should specify tv_show.original for doing aggregations.

That solution is equivalent to my last attempt. But like I said: it is far from perfect...
</comment><comment author="clintongormley" created="2016-02-13T12:52:52Z" id="183660846">As @rjernst said, the new `keyword` field is being introduced specifically for this case where you want to use doc values for aggregations (possibly with case normalization) and have the field available for search as well with a `text` field.

Also note that you can set the `not_analyzed` version to `{"index": false, "doc_values": true}` so that it ONLY has doc values, and doesn't take up space in the index.

The third option that you have is to specify an analyzer at query time.

Closing in favour of https://github.com/elastic/elasticsearch/issues/12394
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor/make agg factory immutable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16511</link><project id="" key="" /><description>Relies on https://github.com/elastic/elasticsearch/pull/16509 being merged first
</description><key id="132209756">16511</key><summary>Refactor/make agg factory immutable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-08T17:36:45Z</created><updated>2016-02-09T09:29:55Z</updated><resolved>2016-02-09T09:29:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T17:46:54Z" id="181494241">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest builder refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16510</link><project id="" key="" /><description /><key id="132209707">16510</key><summary>Suggest builder refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels /><created>2016-02-08T17:36:32Z</created><updated>2016-02-09T13:33:08Z</updated><resolved>2016-02-09T13:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-09T13:33:08Z" id="181866567">@abeyad sorry, I had problems merging this in to my own branch, so I merely copied the lines over. Closing this since I added the change to #16505.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouples the AggregatorBuilder from the AggregatorFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16509</link><project id="" key="" /><description /><key id="132209247">16509</key><summary>Decouples the AggregatorBuilder from the AggregatorFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-08T17:33:59Z</created><updated>2016-02-09T09:29:58Z</updated><resolved>2016-02-09T09:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T17:41:46Z" id="181491991">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add field names to several mapping errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16508</link><project id="" key="" /><description>Closes #16378
</description><key id="132194776">16508</key><summary>Add field names to several mapping errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T16:47:37Z</created><updated>2016-02-09T15:17:35Z</updated><resolved>2016-02-09T15:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-08T16:48:00Z" id="181461107">@jpountz can you look?
</comment><comment author="rjernst" created="2016-02-08T21:00:16Z" id="181565630">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add build method to create SuggestionContext to PhraseSuggestionBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16507</link><project id="" key="" /><description>This adds a build() method to PhraseSuggestionBuilder which is going to be used to output the PhraseSuggestionContext. Also adding tests that check that the old parsing method via SuggestParseElement outputs the same PhraseSuggestionContext than the builder#build() method does.
</description><key id="132187665">16507</key><summary>Add build method to create SuggestionContext to PhraseSuggestionBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-02-08T16:24:30Z</created><updated>2016-02-10T18:05:13Z</updated><resolved>2016-02-10T18:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-02-09T23:47:09Z" id="182136103">Thanks @cbuescher, overall this LGTM, I added some minor comments 
</comment><comment author="cbuescher" created="2016-02-10T09:54:07Z" id="182284245">@areek thanks for the input, I added a commit that addresses your comments. Let me know if I should merge or you want to have a final look.
</comment><comment author="areek" created="2016-02-10T17:54:19Z" id="182502848">@cbuescher this looks great! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log warning if max file descriptors too low</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16506</link><project id="" key="" /><description>This commit adds logging for a warning message if the max file
descriptor count is too low.
</description><key id="132187553">16506</key><summary>Log warning if max file descriptors too low</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T16:24:02Z</created><updated>2016-02-13T19:58:50Z</updated><resolved>2016-02-08T16:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-08T16:32:25Z" id="181450421">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add fromXContent method to SuggestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16505</link><project id="" key="" /><description>This adds the ability to parse from xContent to the SuggestBuilder. Borrowing some test infrastructure code (WritableTestCase) from  #16398 and extending it to be also able to use NamedWritable streamsl.
</description><key id="132186326">16505</key><summary>Add fromXContent method to SuggestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-02-08T16:19:14Z</created><updated>2016-02-09T16:57:20Z</updated><resolved>2016-02-09T16:57:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-08T17:37:47Z" id="181490905">@cbuescher LGTM, I made one change, making the text field a ParseField instead of just a string literal.  I was not able to create a PR to your branch (I don't think I have permissions to your repo), so I just created this PR here: https://github.com/elastic/elasticsearch/pull/16510.  I can squash the commits into one if you think its OK.
</comment><comment author="cbuescher" created="2016-02-09T13:31:49Z" id="181866002">@abeyad thanks, good catch. I added the ParseField you mentioned. Will wait with the merge just to ask if modifying the WritableTestCase here creates merge problems for @areek, then I can merge this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multifield aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16504</link><project id="" key="" /><description>Is there a way to find aggerated values based on more than one fields for example average of difference of two integer fields of a document?
</description><key id="132184879">16504</key><summary>Multifield aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rohitkumarbhagat</reporter><labels /><created>2016-02-08T16:13:34Z</created><updated>2016-02-13T19:57:43Z</updated><resolved>2016-02-13T19:57:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T19:57:43Z" id="183743790">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest preprocessing for bulk requests should use bulk TP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16503</link><project id="" key="" /><description>Currently ingest node only executes ingest preprocessing on the index thread pool. For both index requests and bulk requests. This should be changed, if pipelines are specified on bulk requests the bulk TP should be used for ingest preprocessing.
</description><key id="132169872">16503</key><summary>Ingest preprocessing for bulk requests should use bulk TP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2016-02-08T15:18:32Z</created><updated>2016-02-10T10:49:21Z</updated><resolved>2016-02-10T10:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Teach reindex to retry on rejection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16502</link><project id="" key="" /><description>And count it in the status too!
</description><key id="132166830">16502</key><summary>Teach reindex to retry on rejection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label></labels><created>2016-02-08T15:06:01Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-09T20:08:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-08T15:06:33Z" id="181416644">I based this on the same code used in #16461 because it was easier that way. Thus this is blocked on that PR first.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detach QueryShardContext from IndexShard and remove obsolete threadlocals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16501</link><project id="" key="" /><description>IndexShard currently holds an arbitraritly used `getQueryShardContext` that comes
out of a ThreadLocal. It's usage is undefined and arbitraty since there is also
such a method with different semantics on `IndexService` This commit removes the threadLocal on
IndexShard as well as on the context itself. It's types are now a member and the QueryShardContext
lifecycle is managed byt SearchContext which passes the types on from the SearchRequest.

@colings86 can you take a look at this?
</description><key id="132153896">16501</key><summary>Detach QueryShardContext from IndexShard and remove obsolete threadlocals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label></labels><created>2016-02-08T14:18:38Z</created><updated>2016-02-13T20:15:50Z</updated><resolved>2016-02-08T20:01:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-08T14:51:22Z" id="181408048">@s1monw I left a few comments but I think its pretty good already
</comment><comment author="s1monw" created="2016-02-08T16:24:24Z" id="181448187">@colings86 pushed a new commit
</comment><comment author="colings86" created="2016-02-08T16:33:20Z" id="181450677">Left a small comment but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relocation source should be marked as relocating before starting recovery to primary relocation target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16500</link><project id="" key="" /><description>In case of primary relocation, recovery on the target shard can complete before the source index shard even knows that it is relocating. The reason is that in RecoverySource.recover, we decide whether we can start recovery based on current cluster state by accessing clusterService.state(). Important to note is that the state in cluster service is updated before cluster state listeners are executed. RecoverySource can thus verify the relocation target and complete the recovery before the listeners for this new cluster state are finished (in particular, before IndexShard.updateRoutingEntry() is done).
This means that RecoverySource can move IndexShard.state to RELOCATED before the IndexShard.shardRouting is updated with the information that a relocation is taking place.

Fixes build failure: http://build-us-00.elastic.co/job/es_core_master_oracle_6/4615/
</description><key id="132151242">16500</key><summary>Relocation source should be marked as relocating before starting recovery to primary relocation target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T14:09:23Z</created><updated>2016-02-13T19:58:14Z</updated><resolved>2016-02-08T16:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-08T14:15:49Z" id="181389809">LGTM - good fix thanks!
</comment><comment author="bleskes" created="2016-02-08T15:16:44Z" id="181419411">LGTM2. Left one question.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove _source 'disable' from the create-index documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16499</link><project id="" key="" /><description>As we recommend users not to disable _source and were even thinking about removing this feature, I think we should remove this from the docs.
</description><key id="132146856">16499</key><summary>Remove _source 'disable' from the create-index documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmusa</reporter><labels><label>docs</label></labels><created>2016-02-08T13:51:56Z</created><updated>2016-02-13T19:55:06Z</updated><resolved>2016-02-13T19:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T14:27:49Z" id="181395079">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move sorting tests w/o scripting back to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16498</link><project id="" key="" /><description>In preparation of #16127 this moves sorting tests back to core
that don't actually need scripting or groovy to work.
</description><key id="132145990">16498</key><summary>Move sorting tests w/o scripting back to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-08T13:48:33Z</created><updated>2016-02-08T14:29:21Z</updated><resolved>2016-02-08T14:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T14:27:19Z" id="181394823">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Test]: Allow REST tests to specify custom indices and templates to be deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16497</link><project id="" key="" /><description>It could be nice to be able to specify custom indices and/or template name to be deleted between REST tests.
</description><key id="132145427">16497</key><summary>[Test]: Allow REST tests to specify custom indices and templates to be deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Internal</label><label>test</label></labels><created>2016-02-08T13:45:57Z</created><updated>2016-02-15T14:34:40Z</updated><resolved>2016-02-15T09:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-09T12:11:45Z" id="181840666">I had a look at this, I wonder if we should instead expose the ability to provide exclusions, just to clarify what this is for, it is not meant to disable deletions completely, just to provide exclusions on eventual indices or templates that are special and should stay around between tests.
</comment><comment author="tlrx" created="2016-02-09T12:12:33Z" id="181841129">It makes sense. I'll update the code asap.
</comment><comment author="tlrx" created="2016-02-09T14:16:02Z" id="181881807">@javanna I just updated the code according to your comments. Can you please have another look? Thanks
</comment><comment author="javanna" created="2016-02-09T15:02:31Z" id="181905006">thanks @tlrx I left a few small comments.
</comment><comment author="tlrx" created="2016-02-09T15:12:22Z" id="181909286">thanks @javanna ; I updated the code
</comment><comment author="rjernst" created="2016-02-09T16:44:01Z" id="181950307">Can you explain why thia feature would be "nice"? It seems to me it would add repeatability problems to rest tests. 
</comment><comment author="javanna" created="2016-02-09T16:53:24Z" id="181954167">&gt; It seems to me it would add repeatability problems to rest tests.

Would not accepting expressions but only explicit names make things better in terms of repeatability? BTW we already have `excludeTemplates` in `ESIntegTestCase`.
</comment><comment author="rjernst" created="2016-02-09T17:22:10Z" id="181966577">&gt; Would not accepting expressions but only explicit names make things better in terms of repeatability? 

How would it? Tests would still not always have a pristine state to start with.

&gt; BTW we already have excludeTemplates in ESIntegTestCase.

ESIntegTestCase is the poster child of non-reproducibility. I don't think we should use it as an example of "how to model tests". `excludeTemplates` seems like something left over from when we had the global shared cluster. And we got rid of the global shared cluster because it was not reproducible!

We simply must maintain that tests cannot opt out of isolation.
</comment><comment author="javanna" created="2016-02-09T17:28:41Z" id="181970472">&gt; How would it? Tests would still not always have a pristine state to start with.

I think we are talking about not deleting the same templates/indices all the time, so I am not sure I get the repeatability problem. I do see how it is not ideal that tests start with a certain state that is not 100% clean. This change is needed for QA testing of some plugins (same was for excludeTemplate in ESIntegTestCase), Tanguy can explain better why this is needed. 
</comment><comment author="clintongormley" created="2016-02-13T14:08:56Z" id="183675070">Some clients run these rest tests in random order - I don't think we can depend on any preexisting state.  If a template should exist at the beginning of a rest test then it should be added during the setup phase, no?
</comment><comment author="javanna" created="2016-02-15T08:57:06Z" id="184120165">@clintongormley I think the problem is testing some plugins that create their own templates/indices and rely on them. We wanted to make sure that they don't get deleted all the time, those templates/indices should be created once (not as part of setup) and be left alone. I think Tanguy is looking into alternatives though to make everybody happy, so hopefully this PR will be superseded by some other one.
</comment><comment author="s1monw" created="2016-02-15T09:12:11Z" id="184124548">&gt; @clintongormley I think the problem is testing some plugins that create their own templates/indices and rely on them. We wanted to make sure that they don't get deleted all the time, those templates/indices should be created once (not as part of setup) and be left alone. I think Tanguy is looking into alternatives though to make everybody happy, so hopefully this PR will be superseded by some other one.

@javanna is there any reason except of performance for this change?
</comment><comment author="javanna" created="2016-02-15T09:14:29Z" id="184126808">@s1monw as far as I understood the problem is all but performance :) the change was needed otherwise some plugins test simply fail. But since I failed to explain it clearly up until now, maybe @tlrx can chip in and help me describe what the problem is.
</comment><comment author="tlrx" created="2016-02-15T09:57:06Z" id="184139756">I'm sorry this few lines change takes so much time to so many people. I should have better explained my need. 

I'd like to write REST tests for a plugin that creates templates and indices asynchronously, ie _N_ seconds after the node started. The REST tests should not start until the template and indices are installed (so I added a wait condition in a `@Before` method in the REST Java class) and ideally should not be cleaned up between tests. Since this is something we allow in `ESIntegTestCase` I thought it was OK to propose it for REST tests too. I now better understand how this can add repeatability problems to tests and I'm currently trying to change my REST test so that this PR would not be required anymore.

I'm closing this for now and I'll reopen it if I don't find a better way to do what I need. Sorry for the noise.
</comment><comment author="javanna" created="2016-02-15T10:30:36Z" id="184154858">no worries @tlrx thanks for the explanation, it's not noise it's an actual problem that we have to solve ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>test framework 2.2.0 -&gt; 2.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16496</link><project id="" key="" /><description>We should upgrade to 2.3.2 (which is what lucene is using) just for consistency. We have been using it in master for a while.

I don't wish to rework the maven build here to "clean this up" better even though its messy how we have includes/excludes. 3.0 is clean and lucene is now clean (https://issues.apache.org/jira/browse/LUCENE-6953) so lets just do this the simple way.
</description><key id="132144227">16496</key><summary>test framework 2.2.0 -&gt; 2.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-08T13:40:50Z</created><updated>2016-02-08T15:12:55Z</updated><resolved>2016-02-08T14:29:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-08T14:05:53Z" id="181384687">LGTM
</comment><comment author="s1monw" created="2016-02-08T14:06:08Z" id="181384844">I think we should also backport this to 2.2? 
</comment><comment author="rmuir" created="2016-02-08T14:23:21Z" id="181393894">We can do that, though it should be 2.3.1 there i think. https://github.com/apache/lucene-solr/blob/releases/lucene-solr/5.4.1/lucene/ivy-versions.properties#L10
</comment><comment author="s1monw" created="2016-02-08T14:43:07Z" id="181401646">&gt; We can do that, though it should be 2.3.1 there i think.

++
</comment><comment author="rmuir" created="2016-02-08T15:12:55Z" id="181418437">2.2 backport: https://github.com/elastic/elasticsearch/commit/6fe88cee78c72fde021898eb79883c12edf07c3d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Broken translog on most indexes like NoSuchFileException elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16495</link><project id="" key="" /><description>### Act 1 - Preface

How I run into that issue
- I had elastic 2.0.0 + Kibana 4 + Logstash used as ELK stack.
- Everything was ok until 05 Feb when I logged into server through SSH to check something related to my project task.
- I've spotted that elastic utilize 3-4 cores of my server up to 100% each
- I've shut down elastic, and reviewed it logs
- There was said that

```
[2016-02-05 17:56:05,596][WARN ][indices.cluster          ] [dev-node] [[logstash-2015.11.20][1]] marking and sending shard failed due to [failed recovery]
[logstash-2015.11.20][[logstash-2015.11.20][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2015.11.20/1/translog/translog-42.ckp: Too many open files];
```
- I've asked admin to raise available opened-files limit. And then after elastic was started again I begin to see another errors in elastic logs

```
[2016-02-08 12:00:30,905][WARN ][cluster.action.shard     ] [dev-node] [logstash-2016.01.04][2] received shard failed for [logstash-2016.01.04][2], node[Qg_JBfpNRfa3iApw8BDVsA], [P], v[15], s[INITIALIZING], a[id=mCviKgbnTomx3HaqKYFZ5w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-08T12:00:29.932Z]], indexUUID [50id1z4pS8SsVmFe7kdsvA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp]; ]
```
- I've updated elastic to latest version 2.2.0, nothing has changed
- I assume that I've terminated elastic with SIGKILL that time

It is functioning, Kibana works **but** one (or more) of my shards can't be initialized and elastic tries to start them in infinite loop. Because of that I have negative impact to server:
- Some threads of elastic instance consumes from 300 to 450% of CPU (I'm on 8 core server)
- elastic writes a lot of messages in logs with very high frequency. That leads to increasing of /var/log/elasticsearch/dev-cluster.log for 500 MB each hour or so.

So I may suggest that it does not work as it should .
### Act 2 - Pathetic Attempts to fix that thing by my own

I've read related bug reports and articles, like
https://github.com/elastic/elasticsearch/issues/14989, https://github.com/elastic/elasticsearch/issues/15021, https://github.com/elastic/elasticsearch/issues/9699

have tried several things:
- Removed all ckp files from data folder with
  `find . -type f -name '*.ckp' -delete`
  didn't help
- Removed all .tlog files from data folder with
  `find . -type f -name '*.tlog' -delete`
  didn't help
- changed config in elastic to force it don't create replicas or many shards with updating config/elasticsearch.yml

```
index.number_of_shards: 1
index.number_of_replicas: 0
```

didn't help

Also read articles about `_cluster/reroute` like
https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html
didn't help

http://www.jillesvangurp.com/2015/02/18/elasticsearch-failed-shard-recovery/
I didn't try to use "org.apache.lucene.index.CheckIndex" approach because there might be another workaround

Example of logs I'm getting

**Normal start**

```
[2016-02-08 11:46:28,635][WARN ][bootstrap                ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna--1666338091/jna407580579145636854.tmp: /tmp/jna--1666338091/jna407580579145636854.tmp: failed to map segment from shared object: Operation not permitted
    at java.lang.ClassLoader$NativeLibrary.load(Native Method)
    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1938)
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1821)
    at java.lang.Runtime.load0(Runtime.java:809)
    at java.lang.System.load(System.java:1086)
    at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:761)
    at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:736)
    at com.sun.jna.Native.&lt;clinit&gt;(Native.java:131)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45)
    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:89)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2016-02-08 11:46:28,637][WARN ][bootstrap                ] cannot check if running as root because JNA is not available
[2016-02-08 11:46:28,637][WARN ][bootstrap                ] cannot install syscall filters because JNA is not available
[2016-02-08 11:46:28,637][WARN ][bootstrap                ] cannot register console handler because JNA is not available
[2016-02-08 11:46:28,768][INFO ][node                     ] [dev-node] version[2.2.0], pid[22950], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-08 11:46:28,768][INFO ][node                     ] [dev-node] initializing ...
[2016-02-08 11:46:29,165][INFO ][plugins                  ] [dev-node] modules [lang-expression, lang-groovy], plugins [], sites []
[2016-02-08 11:46:29,182][INFO ][env                      ] [dev-node] using [1] data paths, mounts [[/usr (/dev/md122)]], net usable_space [167.4gb], net total_space [196.6gb], spins? [possibly], types [ext4]
[2016-02-08 11:46:29,182][INFO ][env                      ] [dev-node] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-02-08 11:46:30,905][INFO ][node                     ] [dev-node] initialized
[2016-02-08 11:46:30,905][INFO ][node                     ] [dev-node] starting ...
[2016-02-08 11:46:30,975][INFO ][transport                ] [dev-node] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2016-02-08 11:46:30,983][INFO ][discovery                ] [dev-node] dev-cluster/ARHJZZXRQnOl36kgJQxyUw
[2016-02-08 11:46:34,010][INFO ][cluster.service          ] [dev-node] new_master {dev-node}{ARHJZZXRQnOl36kgJQxyUw}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-02-08 11:46:34,020][INFO ][http                     ] [dev-node] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
[2016-02-08 11:46:34,021][INFO ][node                     ] [dev-node] started
[2016-02-08 11:46:37,463][INFO ][cluster.routing.allocation] [dev-node] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[.kibana][0]] ...]).
[2016-02-08 11:47:31,731][INFO ][cluster.metadata         ] [dev-node] [logstash-2016.02.08] update_mapping [logs]
[2016-02-08 11:47:35,048][INFO ][cluster.metadata         ] [dev-node] [logstash-2016.02.08] update_mapping [logs]
```

**Failed start**

```
[2016-02-08 12:00:24,717][WARN ][bootstrap                ] unable to load JNA native support library, native methods will be disabled.
java.lang.UnsatisfiedLinkError: /tmp/jna--1666338091/jna7690950639376952187.tmp: /tmp/jna--1666338091/jna7690950639376952187.tmp: failed to map segment from shared object: Operation not permitted
    at java.lang.ClassLoader$NativeLibrary.load(Native Method)
    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1938)
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1821)
    at java.lang.Runtime.load0(Runtime.java:809)
    at java.lang.System.load(System.java:1086)
    at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:761)
    at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:736)
    at com.sun.jna.Native.&lt;clinit&gt;(Native.java:131)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45)
    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:89)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2016-02-08 12:00:24,719][WARN ][bootstrap                ] cannot check if running as root because JNA is not available
[2016-02-08 12:00:24,719][WARN ][bootstrap                ] cannot install syscall filters because JNA is not available
[2016-02-08 12:00:24,720][WARN ][bootstrap                ] cannot register console handler because JNA is not available
[2016-02-08 12:00:24,853][INFO ][node                     ] [dev-node] version[2.2.0], pid[24746], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-08 12:00:24,853][INFO ][node                     ] [dev-node] initializing ...
[2016-02-08 12:00:25,266][INFO ][plugins                  ] [dev-node] modules [lang-expression, lang-groovy], plugins [], sites []
[2016-02-08 12:00:25,283][INFO ][env                      ] [dev-node] using [1] data paths, mounts [[/usr (/dev/md122)]], net usable_space [167.4gb], net total_space [196.6gb], spins? [possibly], types [ext4]
[2016-02-08 12:00:25,283][INFO ][env                      ] [dev-node] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-02-08 12:00:26,787][INFO ][node                     ] [dev-node] initialized
[2016-02-08 12:00:26,787][INFO ][node                     ] [dev-node] starting ...
[2016-02-08 12:00:26,866][INFO ][transport                ] [dev-node] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2016-02-08 12:00:26,875][INFO ][discovery                ] [dev-node] dev-cluster/Qg_JBfpNRfa3iApw8BDVsA
[2016-02-08 12:00:29,905][INFO ][cluster.service          ] [dev-node] new_master {dev-node}{Qg_JBfpNRfa3iApw8BDVsA}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-02-08 12:00:29,918][INFO ][http                     ] [dev-node] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
[2016-02-08 12:00:29,918][INFO ][node                     ] [dev-node] started
[2016-02-08 12:00:30,903][WARN ][indices.cluster          ] [dev-node] [[logstash-2016.01.04][2]] marking and sending shard failed due to [failed recovery]
[logstash-2016.01.04][[logstash-2016.01.04][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [logstash-2016.01.04][[logstash-2016.01.04][2]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:156)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1450)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1434)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:925)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    ... 5 more
Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.newByteChannel(Files.java:407)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
    at java.nio.file.Files.newInputStream(Files.java:152)
    at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:82)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:330)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:179)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    ... 11 more
```

```
[2016-02-08 12:00:30,905][WARN ][cluster.action.shard     ] [dev-node] [logstash-2016.01.04][2] received shard failed for [logstash-2016.01.04][2], node[Qg_JBfpNRfa3iApw8BDVsA], [P], v[15], s[INITIALIZING], a[id=mCviKgbnTomx3HaqKYFZ5w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-02-08T12:00:29.932Z]], indexUUID [50id1z4pS8SsVmFe7kdsvA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp]; ]
[logstash-2016.01.04][[logstash-2016.01.04][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];

...

[2016-02-08 12:00:31,608][WARN ][indices.cluster          ] [dev-node] [[logstash-2016.01.04][2]] marking and sending shard failed due to [failed recovery]
[logstash-2016.01.04][[logstash-2016.01.04][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];
...

[2016-02-08 12:00:31,609][WARN ][cluster.action.shard     ] [dev-node] [logstash-2016.01.04][2] received shard failed for [logstash-2016.01.04][2], node[Qg_JBfpNRfa3iApw8BDVsA], [P], v[15], s[INITIALIZING], a[id=X3x681vYTHK_ue0wNCimGg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-08T12:00:30.906Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp]; ]], indexUUID [50id1z4pS8SsVmFe7kdsvA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp]; ]
[logstash-2016.01.04][[logstash-2016.01.04][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];
...

[2016-02-08 12:00:31,613][WARN ][cluster.action.shard     ] [dev-node] [logstash-2016.01.04][2] received shard failed for [logstash-2016.01.04][2], node[Qg_JBfpNRfa3iApw8BDVsA], [P], v[15], s[INITIALIZING], a[id=X3x681vYTHK_ue0wNCimGg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-08T12:00:30.906Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp]; ]], indexUUID [50id1z4pS8SsVmFe7kdsvA], message [master {dev-node}{Qg_JBfpNRfa3iApw8BDVsA}{127.0.0.1}{127.0.0.1:9300} marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-02-08 12:00:32,284][WARN ][indices.cluster          ] [dev-node] [[logstash-2016.01.04][2]] marking and sending shard failed due to [failed recovery]
[logstash-2016.01.04][[logstash-2016.01.04][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp];
```

`GET /_cat/shards`
outputs

```
logstash-2016.01.04 3 p STARTED    11634  7.9mb 127.0.0.1 dev-node 
logstash-2016.01.04 1 p STARTED    11894    8mb 127.0.0.1 dev-node 
logstash-2016.01.04 2 p UNASSIGNED                                 
logstash-2016.01.04 4 p STARTED    11859  7.9mb 127.0.0.1 dev-node 
logstash-2016.01.04 0 p STARTED    11779  7.9mb 127.0.0.1 dev-node 
logstash-2016.01.02 3 p STARTED    10617  6.1mb 127.0.0.1 dev-node 
logstash-2016.01.02 1 p STARTED    10593  6.1mb 127.0.0.1 dev-node 
logstash-2016.01.02 2 p STARTED    10797  6.2mb 127.0.0.1 dev-node 
logstash-2016.01.02 4 p STARTED    10509    6mb 127.0.0.1 dev-node 
logstash-2016.01.02 0 p STARTED    10716  6.2mb 127.0.0.1 dev-node 
logstash-2016.01.03 3 p STARTED    11204  6.4mb 127.0.0.1 dev-node 
logstash-2016.01.03 2 p STARTED    11237  6.4mb 127.0.0.1 dev-node 
logstash-2016.01.03 1 p STARTED    11417  6.6mb 127.0.0.1 dev-node 
logstash-2016.01.03 4 p STARTED    11260  6.6mb 127.0.0.1 dev-node 
logstash-2016.01.03 0 p STARTED    11218  6.4mb 127.0.0.1 dev-node 
.kibana             0 p STARTED        6 86.8kb 127.0.0.1 dev-node 
logstash-2016.02.08 0 p STARTED     2769  1.8mb 127.0.0.1 dev-node 
logstash-2016.01.01 3 p STARTED    10450  6.4mb 127.0.0.1 dev-node 
logstash-2016.01.01 2 p STARTED    10201  6.3mb 127.0.0.1 dev-node 
logstash-2016.01.01 1 p STARTED    10474  6.5mb 127.0.0.1 dev-node 
logstash-2016.01.01 4 p STARTED    10436  6.5mb 127.0.0.1 dev-node 
logstash-2016.01.01 0 p STARTED    10585  6.5mb 127.0.0.1 dev-node 
```

Other failed indexes were temporately moved out of `data` folder to backup folder
### Act 3: What's next?

This issue is reproduced all across logstash indexes I have. Some have that errors, some no. So its not about single index issue.

How I suggest to solve that issue
I need some API like

```
curl -XPOST localhost:9200/stop_infinite_recovery -d '{
 "stop_that_creepy_recovery_and_ignore_all_errors": true
}
```

that gonna stop elastic infinite reports of broken/missed translog files, ignore all previous translog errors and run without errors even if it gonna require to loose/drop/delete **some** data but not **all** my ELK indexes.
Otherwise - I can't use my logs for last 1.5 months because getting many errors about almost every logstash index.
I've already deleted all logs older than 1.5 months, when tried to solve that issue. But that didn't help either.
### Worst case scenario

I've already tried to run elastic with clean `data` directory, and then ELK stack runs as usual, no CPU overhead, no tonns of logs, everything clean and smooth.
I can drop my existed ELK logs for this time. BUT! I won't be able to do so each time I getting translog errors or smth like that.

So guys - any advice on how to force elasticsearch to ignore that damn translog errors? :)
</description><key id="132134173">16495</key><summary>Broken translog on most indexes like NoSuchFileException elasticsearch/data/dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.ckp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">k-vladyslav</reporter><labels /><created>2016-02-08T12:53:19Z</created><updated>2017-01-14T18:50:24Z</updated><resolved>2016-02-08T13:18:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="k-vladyslav" created="2016-02-08T13:04:47Z" id="181357244">@s1monw @jasontedor plz take a look at related question if have any time.
</comment><comment author="s1monw" created="2016-02-08T13:11:38Z" id="181361003">There are tons of bugs fixed along those lines. I don't think upgrading will heal what the bug did to you. The only possibility to help you would have been to get directory listings from you translog files. But apparently you deleted them all?

&gt; I can drop my existed ELK logs for this time. BUT! I won't be able to do so each time I getting translog errors or smth like that.

there are not translog issues known in the later 2.x versions so I am pretty confident you will be ok. 

can you tell me why you did this:

&gt; Removed all ckp files from data folder with find . -type f -name '_.ckp' -delete didn't help
&gt; Removed all .tlog files from data folder with find . -type f -name '_.tlog' -delete didn't help

and also why didn't you open an issue before you delete stuff on your filesystem? :)
</comment><comment author="s1monw" created="2016-02-08T13:18:17Z" id="181363793">btw the solution to your problem would have been simple if you had your tlog files still. You ran into the same issue reported here: https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336/6 and fixed here https://github.com/elastic/elasticsearch/pull/15788
</comment><comment author="k-vladyslav" created="2016-02-08T14:15:44Z" id="181389774">@s1monw thank You for response, no worries, I have backups :) Now I answer Ur questions

I didn't delete original problematic indexes except ELK indexes older than 1.5 months from now (I don't need them). Other indexes are backed up in separate folder, and copied back to `./data` folder so I could experiment as much as I need.
I've only deleted old /var/log/elasticsearch/dev-cluster.log file because it was like 2GB size, so I can't reconstruct what was the reason of that mess

 Here is list of ckp files, for sample problematic index

```
$ find . -type f -name '*.ckp' | grep 2016.01.04
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-225.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-228.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-222.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-224.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-223.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-227.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-229.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-226.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-225.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-228.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-222.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-224.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-223.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-227.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-226.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-221.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-230.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-225.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-228.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-224.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-231.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-227.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-229.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-226.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-225.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-222.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-224.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-223.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-220.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-225.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-222.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-224.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-223.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-227.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-226.ckp
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-221.ckp
./dev-cluster/nodes/1/indices/logstash-2016.01.04/0/translog/translog.ckp
./dev-cluster/nodes/1/indices/logstash-2016.01.04/1/translog/translog.ckp
./dev-cluster/nodes/1/indices/logstash-2016.01.04/4/translog/translog.ckp
./dev-cluster/nodes/1/indices/logstash-2016.01.04/2/translog/translog.ckp
./dev-cluster/nodes/1/indices/logstash-2016.01.04/3/translog/translog.ckp
```

here is list of tlog files

```
$ find . -type f -name '*.tlog' | grep 2016.01.04
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-230.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-227.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-229.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/0/translog/translog-228.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-227.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-229.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-226.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/1/translog/translog-228.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-230.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-231.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-229.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/4/translog/translog-232.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-227.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog-226.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-225.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-227.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-226.tlog
./dev-cluster/nodes/0/indices/logstash-2016.01.04/3/translog/translog-228.tlog
./dev-cluster/nodes/1/indices/logstash-2016.01.04/0/translog/translog-1.tlog
./dev-cluster/nodes/1/indices/logstash-2016.01.04/1/translog/translog-1.tlog
./dev-cluster/nodes/1/indices/logstash-2016.01.04/4/translog/translog-1.tlog
./dev-cluster/nodes/1/indices/logstash-2016.01.04/2/translog/translog-1.tlog
./dev-cluster/nodes/1/indices/logstash-2016.01.04/3/translog/translog-1.tlog
```

&gt; can you tell me why you did this

that was possible solutions for older versions I've found while researching.

&gt; You ran into the same issue reported here: https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336/6 and fixed here #15788

So I need to copy-paste each .ckp file by hand in each day index (38 in my case) on each shard that may fail (5 in my case)? Won't there any automatic function exists for that in elastic?

Also https://github.com/elastic/elasticsearch/pull/15788 was merged on Jan 6 and I've setup ES 2.0.0 several months before that, so on 2.0.0 probably that bug could be occured, yep? Can't 2.2.x detect that problems left from previous versions and resolve them automatically?
</comment><comment author="k-vladyslav" created="2016-02-08T14:30:01Z" id="181395973">@s1monw also solution suggested here https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336/6 won't work bcz I have almost 40 indexes (might that be 400 indexes in case when I keep all ELK data) and translog-226.ckp is not the only file missed even in scope of one index.
So only automatic solution gonna help here.
</comment><comment author="s1monw" created="2016-02-08T14:56:50Z" id="181412723">there is no automatic solution for this. but I can maybe come up with a tool that can help you?
</comment><comment author="k-vladyslav" created="2016-02-08T15:08:33Z" id="181417275">@s1monw okay :( I just thought I have missed something in elastic API / CLI docs that could solve such problems automatically.
Anyway. I can neglect old logs for now, all critical events gathered from ELK were moved into tasks several days before that incident, so it's ok.
If that will be reproduced in 2.2.x and higher - I'll report again with more details, if U don't mind.
Thank You for Ur time.
</comment><comment author="s1monw" created="2016-02-08T19:52:35Z" id="181539156">@k-vladyslav just to double checking can I get a copy of `./dev-cluster/nodes/0/indices/logstash-2016.01.04/2/translog/translog.ckp`
</comment><comment author="k-vladyslav" created="2016-02-09T08:57:52Z" id="181767358">@s1monw it was sent to Your Github profile email.
</comment><comment author="PasiKoistinen" created="2016-10-15T07:48:22Z" id="253969181">Hi s1monw,
I'm running a ES 2.2.2 on ubuntu 14 stable. We've hit this or a related problem a couple of times now. I think this might have something to do with low disk space situations but not sure. I've seen this happening now twice so I can say this is indeed a class A nuisance. Actually this makes me wonder if ES is stable data storage option at all.

Can you please suggest actions that would help solve this issue? 

This is what we're seeing in logs: 
Caused by: java.nio.file.NoSuchFileException: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/lindex/0/translog/translog-148.ckp

That directory at /var/lib/elasticsearch/elasticsearch/nodes/0/indices/lindex/0/translog/ 
contains a couple of translog files: 
translog-146.ckp  translog-147.ckp  translog-148.tlog  translog-149.tlog  translog.ckp

As you can see, the translog-148.ckp is indeed missing. Hence, ES is unable to finish initializing a shard because of this missing transaction file. I find this a critical bug as it can cause denial of service to an otherwise totally working index. 

I'm asking for two kinds of help here:
1. ) can you please advice how to bypass this error in this acute situation? I'd hate to do backup restore every time this happens. I'm looking for a deterministic way to correct this now and next time. 
2.) ES should have a startup option, or a tool/ feature that allows you to start it so that it cleanly wipes out all translogs. After this it would return to a last known working good state. Maybe something like --purge-translogs. 

Thanks in advance, 
P
</comment><comment author="s1monw" created="2016-10-15T12:15:41Z" id="253980794">&gt;  can you please advice how to bypass this error in this acute situation? I'd hate to do backup restore every time this happens. I'm looking for a deterministic way to correct this now and next time.

upgrade to the latest version so this shouldn't happen anymore. We fixed a bunch of bugs related to this. in 5.0 which will go GA soon we have a [commandline tool](https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules-translog.html#corrupt-translog-truncation)

For now have to restore your index. Again, please upgrade to the latest 2.4 version asap. 
</comment><comment author="igormasternoy" created="2016-11-01T11:22:12Z" id="257543273">Currently, I have 2.4.0 version and still ES can enter into endless loop. I store ES data in separately mounted SSD. Some times SSD's can be dropped and after I'll mount them again it causes ES to enter recovery loop. It tries to load transact log file which is empty at the moment, as the result I'm getting EOFException.
</comment><comment author="rpedela" created="2017-01-14T18:50:24Z" id="272644509">I have a very similar set up to @igormasternoy on my dev machine and get a similar error. In my case, my laptop battery died and the computer shutdown automatically. When I booted, the translog was corrupted. I am using ES 2.3.3. I will be updating to the latest 2.4. Unfortunately I am not able to upgrade to 5.x yet because one of the 3rd-party plugins I use hasn't updated yet. Here an excerpt from the log:

```
[2017-01-14 11:15:28,824][WARN ][indices.cluster          ] [Stryfe] [[filing_docs_1477420655582][2]] marking and sending shard failed due to [failed recovery]
[filing_docs_1477420655582][[filing_docs_1477420655582][2]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: EOFException;
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [filing_docs_1477420655582][[filing_docs_1477420655582][2]] EngineCreationFailureException[failed to create engine]; nested: EOFException;
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
        ... 5 more
Caused by: java.io.EOFException
        at org.elasticsearch.common.io.stream.InputStreamStreamInput.readByte(InputStreamStreamInput.java:43)
        at org.elasticsearch.index.translog.TranslogReader.open(TranslogReader.java:199)
        at org.elasticsearch.index.translog.Translog.openReader(Translog.java:377)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:334)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:179)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
        ... 11 more
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JarHell check prevents having Derby on the classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16494</link><project id="" key="" /><description>We are trying to use Elasticsearch in an application which also happens to have Derby on the classpath.

Setting aside the huge bugs with running under a security manager which the Elasticsearch developers appear to be unwilling to fix but which can be worked around for now by granting additional security privileges to the wildcard code source, JarHell then proceeds to crash, complaining about other issues. Some of its complaints are legit, others not so much.

For instance, take Apache Derby. The Derby build contains three jars (ignoring tools), all of which have some overlapping classes with the others but none of which are a strict superset of any other. (Like a Venn diagram, I guess.) We use two of these in our desktop application and three of them in the server application.

The JarHell check chokes on these, making it impossible to use both Derby and Elasticsearch in the same application.

I found a ticket over at Derby about extracting a "core" jar with common code, but even though that ticket was created a decade ago, it doesn't look like any work has begun. For completeness, I will file it here as well, as I notice that there are exceptions for some other libraries (e.g. log4j), so I guess adding exceptions for individual libraries does not appear to be out of the question.
</description><key id="132056559">16494</key><summary>JarHell check prevents having Derby on the classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">trejkaz</reporter><labels><label>feedback_needed</label></labels><created>2016-02-08T05:34:32Z</created><updated>2016-02-09T15:49:45Z</updated><resolved>2016-02-09T15:49:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-08T08:50:51Z" id="181258725">Putting aside your passive aggressive voice on this ticket, I think you are totally missing to communicate your usecase. Are you embedding Elasticsearch, if so there is no jarhell checks. Are you using Elasticsaerch as a client application then there is not jarhell check. Are you building a plugin then you have to provide a clean classpath that's just what we agreed on which prevents bugs rather than being one. If you are using our test-framework and running into jarhell then you can disable this now with `2.2`. 

A general advice or a personal note, if you feel like you need help on an open source project no matter if backed by a company or a foundation it's wise to be friendly and to explain your problem on a technical level. Statements like this:

&gt; Setting aside the huge bugs with running under a security manager which the Elasticsearch developers appear to be unwilling to fix but which can be worked around for now by granting additional security privileges to the wildcard code source, JarHell then proceeds to crash, complaining about other issues. Some of its complaints are legit, others not so much.

are not helpful and don't make anybody want to help you. Even further is a reference of your attitude on the internet which I think needs to be put out there with care. I don't expect you to respond to this but I felt like I am putting it out there. 
</comment><comment author="trejkaz" created="2016-02-08T20:26:43Z" id="181554067">We're embedding it, but the JarHell checks are still being done.
</comment><comment author="s1monw" created="2016-02-08T20:58:24Z" id="181565167">are you installing plugins?
</comment><comment author="trejkaz" created="2016-02-08T22:10:06Z" id="181591014">Yeah, we have the analysis-icu and analysis-kuromoji plugins present.
</comment><comment author="trejkaz" created="2016-02-08T22:17:46Z" id="181594617">Sample test:

```
import java.io.File;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

import org.elasticsearch.client.Client;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.node.Node;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;

import static org.elasticsearch.node.NodeBuilder.*;

public class TestEmbeddedElasticSearch
{
    @Rule
    public final TemporaryFolder temporaryFolder = new TemporaryFolder();

    @Test
    public void test() throws Exception
    {
        try (Node node = createNode();
             Client client = node.client())
        {

        }
    }

    private Node createNode() throws Exception
    {
        File nodeHome = temporaryFolder.newFolder();
        Path pluginsPath = Paths.get("storage-core/build/elasticsearch-plugins");
        if (!Files.exists(pluginsPath))
        {
            throw new IllegalStateException("Could not find ES plugins path. Tests will fail. Plugin path: " + pluginsPath);
        }

        return nodeBuilder()
            .settings(Settings.settingsBuilder()
                              .put("path.home", nodeHome.getAbsolutePath())
                              .put("path.plugins", pluginsPath.toString())
                              .put("index.number_of_shards", 1)
                              .put("index.number_of_replicas", 0)
                              .put("node.http.enabled", "false"))
            .local(true)
            .node();
    }
}
```

Fails with:

```
java.lang.IllegalStateException: failed to load bundle [] due to jar hell

    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:421)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:129)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
    at TestEmbeddedElasticSearch.createNode(TestEmbeddedElasticSearch.java:49)
    at TestEmbeddedElasticSearch.test(TestEmbeddedElasticSearch.java:25)
    ...junit launcher stuff...
Caused by: java.lang.IllegalStateException: jar hell!
class: org.apache.commons.logging.impl.NoOpLog
jar1: /Users/daniel/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.3/f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f/commons-logging-1.1.3.jar
jar2: /Users/daniel/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.7.12/adef7a9e1263298255fdb5cb107ff171d07c82f3/jcl-over-slf4j-1.7.12.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:280)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:186)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:419)
    ... 30 more
```
</comment><comment author="rjernst" created="2016-02-08T23:00:18Z" id="181611583">@trejkaz I see your test is trying to run an elasticsearch node in the same process as your tests, along with having plugins already installed in a "special directory". This won't work because the plugin service is meant to be used by elasticsearch running as a server, which is what starting a node in the way you are does.  In the master branch of ES, we have removed NodeBuilder and replaced it with simply constructing a Node, which takes "classpath plugins". These are what tests use (like ESIntegTestCase), as they expect the plugins and ES to be in the same test process. I would recommend using that test case as a base, or creating a subclass of `Node` to use the constructor that is now exposed in master.

However, a better solution to test the real integration here would be to start ES in a separate process. This is what our integration tests do in 2.x and master, and it has proven to be very useful for testing how things interact with ES in a realistic way.  If developing a plugin, and using the shared POM in 2.x (or the gradle plugin in master), you can utilize the existing build infrastructure to start the ES node and install plugins before integ tests start.
</comment><comment author="trejkaz" created="2016-02-09T02:49:23Z" id="181677849">This workaround works, using

```
    Environment environment = InternalSettingsPreparer.prepareEnvironment(settings, null);
    Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins = ImmutableList.of(AnalysisICUPlugin.class, AnalysisKuromojiPlugin.class);
    return new Node(environment, Version.CURRENT, plugins){}.start();
```

instead of the builder. The settings are identical. And in many ways, having the plugins loaded from the classpath is much nicer anyway.
</comment><comment author="s1monw" created="2016-02-09T15:49:44Z" id="181922340">oh great you found a solution.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix recovery translog stats totals when recovering from store</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16493</link><project id="" key="" /><description>Recovery from store fails to correctly set the translog recovery stats. This fixes it and tightens up the logic bringing it all to IndexShard (previously it was set by the recovery logic).

Closes #15974
</description><key id="132018321">16493</key><summary>Fix recovery translog stats totals when recovering from store</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-07T21:55:16Z</created><updated>2016-02-08T12:48:53Z</updated><resolved>2016-02-08T11:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-07T21:55:27Z" id="181129944">@s1monw can you take a look?
</comment><comment author="s1monw" created="2016-02-08T08:43:16Z" id="181257571">LGTM left a cosmetic comment
</comment><comment author="s1monw" created="2016-02-08T10:29:49Z" id="181296764">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extract non-transport logic from TransportReplicationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16492</link><project id="" key="" /><description>Recent refactoring in TransportReplicationAction has moved all logic to clear units. We can take an extra step and extract those classes so they can be tested (and added to other tests) without requiring a (fake) transport layer and support.

This is a POC pending feedback. The usage of the new classes is shown in TransportReplicationAction2 - which is now nothing but network glue. Also, TransportIndexAction2 is an example implementation. Note how that class contains it's own IndexShardReference which is in charge of doing the actual work. This allows us in the future to extract that class and use it to setup a unit test with multiple shards connected by the replication logic, but without the transport and other unneeded dependencies.

@s1monw @jasontedor @ywelsch @areek I would like to get your feedback on this.
</description><key id="132018095">16492</key><summary>Extract non-transport logic from TransportReplicationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-02-07T21:53:21Z</created><updated>2016-04-25T14:05:36Z</updated><resolved>2016-04-19T13:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-08T08:57:24Z" id="181259896">Pushed some more commits ..
</comment><comment author="ywelsch" created="2016-02-08T11:08:59Z" id="181310327">I like the idea of decoupling transport from the actual action logic. By not having everything in a single file anymore, we need to be extra careful however to document what each class does, especially w.r.t. failure modes.

Looking at the code, I would like to suggest more changes to make the classes even more testable / self-contained. The idea is to get rid of abstract classes and use listeners and functional interfaces where possible:
- `ReplicaOperation`: I would just add an `ActionListener&lt;Void&gt;` parameter to the class. We can then remove `finishedAsSuccess` and `finishedAsFailed` and make it a concrete class (no need for tests to do some subclassing).
- `PrimaryReroute`: Add an `ActionListener&lt;Response&gt;` parameter, replacing `finishedAsSuccess` and `finishedAsFailed`. This leaves us with two abstract methods `performPrimaryOperation` and `finishedAsReroute` in `PrimaryReroute`. We could replace them by adding two parameters to the class, a performPrimaryAction and a rerouteAction, both implementing the same functional interface:

```
interface NodeAction&lt;Request, Response&gt; {
  void execute(DiscoveryNode, Request, ActionListener&lt;Response&gt;)
}
```
- `PrimaryOperation`: This class has an inner class `ReplicationPhase`. I would split them into two top-level classes.
- For `PrimaryOperation`, add again ActionListener&lt;Response&gt; parameter. Also add parameter `NodeAction&lt;Request, Response&gt; routeToRelocatedPrimaryAction` and finally a parameter `Consumer&lt;Tuple&lt;Response, ReplicaRequest&gt;&gt; replicationAction` that executes the replication action.
- For `ReplicationPhase`, the following two parameters need to be added:
  `ActionListener&lt;Response&gt; listener` (called after replication phase is finished) and
  `NodeAction&lt;ReplicaRequest, Void&gt; performOnReplicaAction` (the transport action to actually execute the request on replica)

How does this look like?

https://github.com/ywelsch/elasticsearch/commit/6c4a101d0ddf81a597cbcc0c93b104303312152d
</comment><comment author="bleskes" created="2016-02-08T12:18:25Z" id="181341760">&gt; we need to be extra careful however to document what each class does

Agreed. Didn't do that yet.

&gt; The idea is to get rid of abstract classes and use listeners and functional interfaces where possible.

I thought about that too (and tried it some as well). My biggest reason not go down that route is that I find the code easier to read when you implement methods with name, compared to a long list of callables as parameters to the constructors. 

&gt; PrimaryOperation: This class has an inner class ReplicationPhase. I would split them into two top-level classes.

I debated this one for a long time as well. I ended falling on the side of the fence where having a replication phase on it's own is a step too far. My idea was that this way we have a clear separation of "things that happen on route to the primary"(i.e., `PrimaryReroute`) , "things that happen on/done by the primary" (i.e., PrimaryOpereation) and "things that happen on the replica" (i.e., ReplicaOperation)

All of which is a matter of taste. Let's see what others think.
</comment><comment author="bleskes" created="2016-03-17T11:26:54Z" id="197832614">I pushed another iteration, this time only dealing with things that are done in the current PrimaryPhase &amp; Replication. This is to reduce the scope of the change. We can then decide whether to do something similar for reroute phase or not.

The new itration is under the [poc2](https://github.com/bleskes/elasticsearch/tree/replication_the_great_recouple/core/src/main/java/org/elasticsearch/action/support/replication/poc2) package. I chose not to do it inline as I think the diff will only be confusing and it's easier to use the old implementaion as refrece. Concreting we have:
1) [ReplicatedOperation](https://github.com/bleskes/elasticsearch/blob/replication_the_great_recouple/core/src/main/java/org/elasticsearch/action/support/replication/poc2/ReplicatedOperation.java) - this is the replacement of the PrimaryPhase and ReplicationPhase. I chose to merge the two because I think it makes the code simpler. 
2) [TransportReplicationAction3](https://github.com/bleskes/elasticsearch/blob/replication_the_great_recouple/core/src/main/java/org/elasticsearch/action/support/replication/poc2/TransportReplicationAction3.java) - This class takes care of all the glue of hooking up ReplicatedOperation with the rest of the infrastructure. 
3) [TransportIndexAction3](https://github.com/bleskes/elasticsearch/blob/replication_the_great_recouple/core/src/main/java/org/elasticsearch/action/support/replication/poc2/TransportIndexAction3.java) - a port of TransportIndexAction to the new base class. The change is minimal (minor method change) , which is great.

On the testing side everyting is in the [poc2](https://github.com/bleskes/elasticsearch/tree/replication_the_great_recouple/core/src/test/java/org/elasticsearch/action/support/replication/poc2) package as well. There are two classes - one to test the replication logic in ReplicatedOperation (the complicated part) and one simple class to test all the bridges (TransportReplicationAction3Tests).  For bravity I didn't copy the tests for the reroute phase.

@s1monw @ywelsch @jasontedor - I would appreciate your feedback. If we like this, I would rewrite the PR to actually change the code (and address some nocommits). I would also like to only do this once #17038 is in, so I can take care of the conflicts (and not vice versa).
</comment><comment author="bleskes" created="2016-03-17T17:02:33Z" id="197975071">and of course, @areek as well.. sorry for not adding you ^^
</comment><comment author="bleskes" created="2016-04-12T13:14:42Z" id="208899274">@ywelsch @jasontedor as request I rebased code and folded it into the "standard" TRA. I want to make another sweep myself with a fresh mind (it has been a while since I made this and it's still rough and has nocommits) but tests pass and I think you can start the initial review.
</comment><comment author="ywelsch" created="2016-04-14T07:40:10Z" id="209808686">Going through this PR, I have two main criticisms.

My first criticism is about the split of TRA and ReplicatedOperation. Although ReplicatedOperation looks simpler, the control flow is still complex between the two. Some logic is hiding in TRA and other in ReplicatedOperation. I'm not sure if the boundary of cutting up TransportReplicationAction and ReplicatedOperation is the right one is some cases (I wrote a comment to mark some of them). As is, TransportReplicationAction still contains lots of logic and retry code that is not directly related to transport. With this PR I want us to better understand the complex flow of failures and retry mechanisms, independent of the transport layer.

My second criticism is about naming. I find the method/interface names quite confusing and I think we need to come up with a clean vocabulary first. As an example for this let's look at the ReplicatedOperation class. The method performOnPrimary calls primary.performOperation and then also performOnReplica which is just the method that actually delegates to the action on the replica which then again has a similar name performOn (on the ReplicasProxy). Too many methods in ReplicatedOperation and TRA are called performXYZ but have vastly different semantics. Here are a few thoughts about naming:
- performOnReplica could mean "perform action on replica shard". We know however that this can also be a primary shard. I would thus prefer to just have a method "replicateRequest(ReplicationRequest, ShardRouting)" which replicates a request to a shard (be it another primary or a replica shard). In our vocabulary we could use the words replication source and replication target.
- I'm not sure whether we need the separation into Primary and ReplicasProxy interfaces. The methods failShard and performOn in ReplicasProxy don't share much commonality except that they both use transport. I think we could just fold Primary and ReplicasProxy into one interface (and call failShard from Primary failReplicationSourceShard and the other one failReplicationTargetShard).
</comment><comment author="bleskes" created="2016-04-14T10:27:11Z" id="209868636">&gt; My first criticism is about the split of TRA and ReplicatedOperation. Although ReplicatedOperation looks simpler, the control flow is still complex between the two. Some logic is hiding in TRA and other in ReplicatedOperation. 

I think I failed to explain the purpose of this PR correctly (and it changed during the iteration here). The main goal of here is to create an as much as possible isolated class that takes a set of "shards" (one primary and the rest replicas) and performs and operation on them. I think of it as a replication black box - you give it a request and a set of "shards" (i'm quoting it as it can be anything as far as the class is concerned) and it will return the result of the operation on the primary and replicate it to all replicas. The reason for this is to allow to run integration tests on the "shard level" without the need for transport, masters and other node/cluster level services.

Re the naming, how about the following (taking some of your suggestions):
1) rename performOnPrimary to just run()
2) rename ReplicasProxy to just Replicas (or ReplicasCollection)

&gt; I'm not sure whether we need the separation into Primary and ReplicasProxy interfaces. The methods failShard and performOn in ReplicasProxy don't share much commonality except that they both use transport. I think we could just fold Primary and ReplicasProxy into one interface (and call failShard from Primary failReplicationSourceShard and the other one failReplicationTargetShard).

I'm not sure I follow you here. I think it's good to make a clear distinction between the primary and replicas as their role is very much different. Does calling the current ReplicasProxy as ReplicasCollection make methods like `performOn` and `failShard` (or just `fail` for consistency's sake) more intuitive for you?
</comment><comment author="bleskes" created="2016-04-18T11:34:21Z" id="211340801">@ywelsch @jasontedor Thanks for all the feedback. I pushed more commits addressing what we discussed. Would appreciate another look.
</comment><comment author="ywelsch" created="2016-04-18T14:34:17Z" id="211405906">left one nit, otherwise looks ok. Thanks @bleskes.
</comment><comment author="jasontedor" created="2016-04-19T01:32:13Z" id="211675335">@bleskes I left more nits, otherwise LGTM.
</comment><comment author="bleskes" created="2016-04-19T13:03:04Z" id="211916288">@ywelsch @jasontedor thank you for the reviews. It's a tricky one :) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating upgrade.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16491</link><project id="" key="" /><description>Better notes about how to upgrade plugins
</description><key id="131977226">16491</key><summary>Updating upgrade.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tebriel</reporter><labels><label>docs</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-07T15:32:43Z</created><updated>2016-02-09T16:55:27Z</updated><resolved>2016-02-09T16:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-09T16:55:22Z" id="181955146">LGTM, will merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest: Add attachment processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16490</link><project id="" key="" /><description>This is a simple port of the mapper attachment plugin to the ingest
functionality, no new features. The only option is to limit
the number of chars to prevent indexing of huge documents.

Fields can be selected in the processor as well.

Close #16303

This code allows for full deletion of the mapper attachment plugin, whenever we wish, but it hasnt been done as part of this PR. Most of the code is basically copying the licenses. Security Permissions have been copied as well.
</description><key id="131902975">16490</key><summary>Ingest: Add attachment processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugin Ingest Attachment</label><label>feature</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-06T21:48:22Z</created><updated>2016-02-13T13:13:01Z</updated><resolved>2016-02-09T16:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-07T19:52:47Z" id="181094004">@spinscale This looks great! About the name of the plugin, personally I like the `ingest-extract` name suggested in #16303. 
</comment><comment author="spinscale" created="2016-02-07T22:16:05Z" id="181133528">I dont have any particular opinion about the name. `ingest-extract` might be ambigouos in its meaning... like extracting fields from other fields/arrays.

guess we just need some consensus here... :)
</comment><comment author="dadoonet" created="2016-02-08T03:43:30Z" id="181183718">I like:

ingest-tika because it's short and easy to write

ingest-attachments because we were using that until now and because it is hiding the implementation which means that we could change from Tika to anything else or add other features which are not in Tika.
</comment><comment author="dadoonet" created="2016-02-08T06:27:40Z" id="181224176">I &lt;3 it! Left some comments but it looks very good to me. Thanks Alex!
</comment><comment author="talevy" created="2016-02-08T19:58:57Z" id="181541359">awesome!
</comment><comment author="martijnvg" created="2016-02-09T10:29:55Z" id="181804201">Left a last small comment. LGTM
</comment><comment author="spinscale" created="2016-02-09T12:34:48Z" id="181846184">I think I answered on all review comments, last part is still the naming... 

Summoning @clintongormley for naming skills :)
</comment><comment author="clintongormley" created="2016-02-09T12:41:41Z" id="181847928">/me appears in a cloud of smoke!

I don't like `ingest-tika` because it requires prior knowledge.  Not crazy about `ingest-extract` because it is too vague.  `ingest-attachment` I think is probably the best option (and I agree it should be singular) given that it echoes the current naming.  The only other idea I had was `ingest-document`, but I'm wary of that because of how we use the word "document" already in Elasticsearch.

My vote: `ingest-attachment`
</comment><comment author="javanna" created="2016-02-09T13:17:41Z" id="181859338">ingest-attachment ++
</comment><comment author="dadoonet" created="2016-02-09T13:19:53Z" id="181860147">Cool! LGTM 
</comment><comment author="martijnvg" created="2016-02-09T14:34:00Z" id="181891194">+1 for `ingest-attachment` too
</comment><comment author="Analect" created="2016-02-09T16:49:22Z" id="181952513">@spinscale 
This might not be the right place for this, but just happened to notice this merge and since it's a question that has popped up on various forums with an inconclusive answer, I wanted to get clarification. 
Does this **tika** functionality residing behind `ingest-attachment` include [**tesseract**](https://github.com/tesseract-ocr) functionality, for OCR?  A good test document is this one linked below, disseminated from the ECB. With tika and no tesseract, it usually only captures the contents of page 9, where as with tesseract, it should capture all the contents.

https://assets.documentcloud.org/documents/1061096/1703-2012-vik-ck-s2013-185014-conf.pdf
</comment><comment author="kimchy" created="2016-02-09T20:16:28Z" id="182044321">@spinscale great work. Quick note (sorry for the late one), I see that we treat the field as base64, we also support other formats (CBOR specifically) that do have native support for binary data, so the data might actually come be bytes already with no need to do base64 in JSON.
</comment><comment author="spinscale" created="2016-02-09T21:07:44Z" id="182068275">@Analect this is a pure 1:1 port of the existing functionality. So the tesseract integration of tika is not used here. One could add this, but I suppose it would be fairly complex to implement with the security manager in place.
</comment><comment author="Analect" created="2016-02-09T23:19:54Z" id="182129861">@spinscale Thanks for the info ... my understanding is that Tika comes with OCR support as long as Tesseract is installed (https://issues.apache.org/jira/browse/TIKA-93). I'm not familiar with the _security manager_, but is there scope to install Tesseract on a suggested dedicated ingest-attachment node?

For OCR on PDFs, as per my example doc linked above, it seems you need to set usage for an auto-detect parser (rather than a PDF-specific one) for it to work, as per this ticket: https://issues.apache.org/jira/browse/TIKA-1729

It would be nice to try to get this to work. I'm happy to try to test and document if you can give me some pointers as to how one would get tesseract installed on a given node.
</comment><comment author="spinscale" created="2016-02-10T09:42:24Z" id="182280156">@kimchy I like the idea, as much as I hate that base64 conversion.

Right now the `IngestDocument` is not aware of its content type to pass it on to each processor, but that can be added easily in the pipeline execution service. In combination with a configuration option (dont like diverging handling dependent of the content type by default), this looks relatively easy to add.
</comment><comment author="kimchy" created="2016-02-10T22:35:55Z" id="182611253">@spinscale another option is to get the field and check if it is a String (so base64) or byte[]?
</comment><comment author="s1monw" created="2016-02-11T08:14:55Z" id="182758662">&gt; @spinscale great work. Quick note (sorry for the late one), I see that we treat the field as base64, we also support other formats (CBOR specifically) that do have native support for binary data, so the data might actually come be bytes already with no need to do base64 in JSON.

I wonder if it make sense to make the parser agnostic to this. We can just pull bytes and if it's JSON the method does the base64 conversion?
</comment><comment author="kimchy" created="2016-02-11T14:33:50Z" id="182889847">&gt; I wonder if it make sense to make the parser agnostic to this. We can just pull bytes and if it's JSON the method does the base64 conversion?

We have support for it in `XContentParser` (through Jackson), but when we move it to "map of maps", we loose that, and it will either be String (for JSON, base64) or byte[](for CBOR).
</comment><comment author="spinscale" created="2016-02-11T14:42:58Z" id="182892741">@kimchy opened #16601 to support this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trim char filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16489</link><project id="" key="" /><description>It'd be nice to have a trim char filter - you can totally do it with pattern_replace right now but reaching for regexes seems like overkill when you just what to strip trailing or leading characters.
</description><key id="131899993">16489</key><summary>Trim char filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2016-02-06T21:10:20Z</created><updated>2016-05-02T13:19:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-02-06T21:16:54Z" id="180865257">Love it. Makes sense when you have common things like DNS names that technically trail with `.` even though no one expects them too.
</comment><comment author="jpountz" created="2016-02-12T10:50:20Z" id="183270955">@nik9000 What use-cases do you have in mind? We were discussing this issue in fixit friday and @jimferenczi made the note that we already have a trim token filter, so we were wondering what use-cases would be covered by a trim char filter that would not be covered with a trim token filter? The trim token filter only supports removing whitespaces but if this is the only issue then maybe it would be easier to just add the ability to trim arbitrary chars to the trim token filter?
</comment><comment author="nik9000" created="2016-02-12T13:30:44Z" id="183329765">&gt; already have a trim token filter

That'd probably be enough.

&gt; only supports removing whitespaces

That'd be simple to fix, yeah.

I believe the thing mostly comes up in the path_hierarchy tokenizer. Say you want to use it for a directory like in the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html) but you want users to be a bit more fluid. You want searching for "foo/bar" or "/foo/bar/" to find
- "/foo/bar"
- "/foo/bar/baz"

I haven't tried it myself, but was talking with @pickypg who was trying it - the leading / was getting in the way. I reached for a char_filter because I figured it'd be simpler to think about.

In our case we were talking about dns which is the same but with the reverse flag. DNS is worse because some systems like to spit out the trailing dot very few people are used to it.
</comment><comment author="pickypg" created="2016-02-12T16:13:59Z" id="183391505">As @nik9000 said, it was due to DNS. In DNS, you really want the reversed path hierarchy and actual DNS has a goofy `.` trailing at the end:

```
elastic.co.
google.com.
apple.com.
```

So it would be ideal to trim that off because otherwise the first token is:

```
co.
com.
com.
```

I certainly wouldn't expect to search for that with a term filter that wants to only find the `.com` TLD. I'd either try `.com` or `com`, but never `com.`.
</comment><comment author="clintongormley" created="2016-02-13T11:45:10Z" id="183649113">I'm not convinced that we need to expand the `trim` character filter.  The examples provided are actually quite complex... perhaps you want to trim front and end, or just front or just end...  The char filter works on the whole field value, not on individual terms, which makes it less flexible.

The flexibility required is easily supported by regexes - the `pattern_replace` token filter looks like the perfect solution here.
</comment><comment author="cbuescher" created="2016-05-02T13:19:16Z" id="216232199">Just to add a datapoint, similar use case came up in the forums last week: https://discuss.elastic.co/t/solved-question-about-custom-analyzer/48680
This user was trying to search on ip adress prefixes (like **192.168**.1.1), so I suggested using the path_hierarchy tokenizer, but for similar reasons as @nik9000 stated above it was necessary to remove trailing dots from the search input. `pattern_replace` was doing the trick but I was also suprised that `trim` is limited to whitespace.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing Suite and Origin in ElasticSearch Release file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16488</link><project id="" key="" /><description>I've been using Ubuntu automatic updates on my servers and it has been working fine for a few years. I'd like to be able to do the same with ElasticSearch. I found a [post on askubuntu.com explaining](https://askubuntu.com/questions/87849/how-to-enable-silent-automatic-updates-for-any-repository) how to do it.

It tells to look into `/var/lib/apt/lists/` directory and at the files ending with 'Release'. I'm checking elasticsearch (2.x) repo file and unfortunately, it doesn't contain `Origin` and `Suite` is empty which is a problem.

Logstash doesn't have this problem, it has both `Origin` and `Suite` and they both contain values.
</description><key id="131786873">16488</key><summary>Missing Suite and Origin in ElasticSearch Release file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThomasdOtreppe</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2016-02-05T23:19:09Z</created><updated>2016-03-17T13:25:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-08T17:27:30Z" id="181486780">&gt; I've been using Ubuntu automatic updates on my servers and it has been working fine for a few years. I'd like to be able to do the same with ElasticSearch.

I'm interpreting this to mean that you want automatic and unattended _upgrades_ to the Elasticsearch installation. I advise _very_ strongly against this, or any other course of action that does not involve testing that an upgrade is not to going to break your dependencies of Elasticsearch. An unattended upgrade for a server process is _very_ risky and such an upgrade should be performed deliberately. There is such an incredible variety of things that can cause issues including plugins not working, performance regressions, deprecated APIs being removed, etc. Even for a patch upgrade with the rigorous testing processes that we have in place, we can not account for every setup and use case. Computers are hard.

&gt; I'm checking elasticsearch (2.x) repo file and unfortunately, it doesn't contain Origin and Suite is empty which is a problem.

The above disclaimer out of the way, I do not see a reason for this to not be fixed. Can you take a look @drewr?
</comment><comment author="ThomasdOtreppe" created="2016-02-08T17:49:07Z" id="181495360">Could you elaborate why you don't see a reason to fix the file?
</comment><comment author="jasontedor" created="2016-02-08T18:06:22Z" id="181503217">&gt; Could you elaborate why you don't see a reason to fix the file?

I'm truly sorry that the double negative was confusing. To be perfectly clear, I think that it _should_ be fixed and I am unable to come up with any reasons why it should not be fixed (but there might be one that I don't see).

Also, I still advise strongly against automatic and unattended upgrades to the Elasticsearch installation. :smile: 
</comment><comment author="ThomasdOtreppe" created="2016-02-08T19:47:09Z" id="181537165">You have a good point, that's a bad idea. It's not much of a problem for Logstash (it has been pretty stable and there only has been 2 breaking changes since the first time I used it and now and those were easily fixed)
</comment><comment author="clintongormley" created="2016-02-13T20:15:02Z" id="183746894">@rjernst does the new gradle build add these values?
</comment><comment author="jasontedor" created="2016-02-13T20:17:11Z" id="183747013">@clintongormley I think these are properties on the actual repository, not on the packages themselves.
</comment><comment author="clintongormley" created="2016-02-13T20:42:47Z" id="183752615">@jasontedor ok - however, these repositories are created by our own release scripts, not something @drewr is responsible for.
</comment><comment author="ThomasdOtreppe" created="2016-03-16T21:03:39Z" id="197548634">Any update on this?
</comment><comment author="jasontedor" created="2016-03-17T04:11:31Z" id="197684921">&gt; Any update on this?

At this time this is not a priority.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script in aggregation not working in 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16487</link><project id="" key="" /><description>Reported problem in 2.0 (#14273), but this time around there are different errors:

Here's the aggregation expressed in elasticsearch-dsl-py (which works fine in 1.x):

```
s.aggs \
  .bucket('hour', 'terms', script= \
          'use(groovy.time.TimeCategory) { \
            new Date(doc["timestamp"].value).format("HH") \
          }', size=0, order={'_term': 'asc'}) \
  .metric('bandwidth', 'sum', field='size') \
    .bucket('action', 'terms', field='action', size=0) \
    .metric('bandwidth', 'sum', field='size')
```

In 2.2 we get this error:

```
es_1          | [2016-02-05 23:09:19,452][DEBUG][action.search.type       ] [Dmitri Bukharin] [356019-20160128][4], node[vJdfzAOUSzySzoEHaePQfg], [P], v[2], s[STARTED], a[id=ynZWRaZdQ6uhnwbsyMD1DA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@496d4b1a] lastShard [true]
es_1          | RemoteTransportException[[Dmitri Bukharin][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ScriptException[failed to run inline script [use(groovy.time.TimeCategory) {                 new Date(doc["timestamp"].value).format("HH")               }] using lang [groovy]]; nested: MissingPropertyException[No such property: groovy for class: 9bb55a3e15db1c99c3f7f34eb032629ba6109bc4];
es_1          | Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ScriptException[failed to run inline script [use(groovy.time.TimeCategory) {                 new Date(doc["timestamp"].value).format("HH")               }] using lang [groovy]]; nested: MissingPropertyException[No such property: groovy for class: 9bb55a3e15db1c99c3f7f34eb032629ba6109bc4];
es_1          |     at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
es_1          |     at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
es_1          |     at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364)
es_1          |     at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:376)
es_1          |     at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
es_1          |     at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
es_1          |     at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
es_1          |     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
es_1          |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
es_1          |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
es_1          |     at java.lang.Thread.run(Thread.java:745)
es_1          | Caused by: ScriptException[failed to run inline script [use(groovy.time.TimeCategory) {                 new Date(doc["timestamp"].value).format("HH")               }] using lang [groovy]]; nested: MissingPropertyException[No such property: groovy for class: 9bb55a3e15db1c99c3f7f34eb032629ba6109bc4];
es_1          |     at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:318)
es_1          |     at org.elasticsearch.search.aggregations.support.values.ScriptBytesValues.setDocument(ScriptBytesValues.java:53)
es_1          |     at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$1.collect(StringTermsAggregator.java:80)
es_1          |     at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
es_1          |     at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:173)
es_1          |     at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:218)
es_1          |     at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:169)
es_1          |     at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
es_1          |     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
es_1          |     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
es_1          |     at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
es_1          |     ... 10 more
es_1          | Caused by: groovy.lang.MissingPropertyException: No such property: groovy for class: 9bb55a3e15db1c99c3f7f34eb032629ba6109bc4
es_1          |     at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:53)
es_1          |     at org.codehaus.groovy.vmplugin.v7.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:177)
es_1          |     at 9bb55a3e15db1c99c3f7f34eb032629ba6109bc4.run(9bb55a3e15db1c99c3f7f34eb032629ba6109bc4:1)
es_1          |     at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:311)
es_1          |     at java.security.AccessController.doPrivileged(Native Method)
es_1          |     at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:308)
```
</description><key id="131786114">16487</key><summary>Groovy script in aggregation not working in 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reflection</reporter><labels /><created>2016-02-05T23:14:27Z</created><updated>2016-06-09T17:16:06Z</updated><resolved>2016-02-06T00:39:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reflection" created="2016-02-06T00:39:20Z" id="180636372">Nevermind, in 2.2 we either need to disable the new Java Security Manager or whitelist any classes we need.
</comment><comment author="rvalenciano" created="2016-06-09T17:15:48Z" id="224963819">@reflection or anyone, do you have specific steps for whitelist any classes we need.? I'm having issues using groovy.json.JsonOutput.toJson in a script, using elastic 2.3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to parse time with CET as the Time Zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16486</link><project id="" key="" /><description>I am using 2.1.1 with Java 8

This unit test fails and it fails in Elasticsearch also.  What am I missing

```
    String parseThis = "Thursday January 28, 2016 00:16:37 CET";
    DateTimeFormatter fmt = DateTimeFormat.forPattern("EEE MMM d, yyyy HH:mm:ss zzz");
    DateTime dateTime = fmt.parseDateTime(parseThis);

    This one passes

    String parseThis = "Thursday January 28, 2016 05:19:43 UTC";
    DateTimeFormatter fmt = DateTimeFormat.forPattern("EEE MMM d, yyyy HH:mm:ss zzz");
    DateTime dateTime = fmt.parseDateTime(parseThis);
```
</description><key id="131775249">16486</key><summary>Unable to parse time with CET as the Time Zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digitalrinaldo</reporter><labels /><created>2016-02-05T22:27:43Z</created><updated>2016-02-06T14:44:27Z</updated><resolved>2016-02-06T14:37:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-06T14:37:08Z" id="180776618">[Time zone abbreviations](https://en.wikipedia.org/wiki/List_of_time_zone_abbreviations) are ambiguous. For example `CST` is the abbreviation for five different time zones: 

```
CST Central Standard Time (North America) UTC&#8722;06
CST China Standard Time                   UTC+08
CST Central Standard Time (Australia)     UTC+09:30
CST Central Summer Time (Australia)       UTC+10:30
CST Cuba Standard Time                    UTC&#8722;05
```

This is why the [`DateTimeFormat` docs](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html) says:

&gt; Zone names: Time zone names ('z') cannot be parsed.

`UTC` is treated specially, however. From the Javadocs for [`DateTimeZone#forID`](http://www.joda.org/joda-time/apidocs/org/joda/time/DateTimeZone.html#forID-java.lang.String-):

&gt; Short ids, as accepted by [`TimeZone`](http://docs.oracle.com/javase/8/docs/api/java/util/TimeZone.html), are not accepted. All IDs must be specified in the long format. The exception is UTC, which is an acceptable id.

You will need to translate your time zone IDs to something that can be parsed, either an unambiguous time zone name, or an offset.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin does not work with ES 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16485</link><project id="" key="" /><description>Get complaints about java permissions but adding them doesn't seem to fix...

[2016-02-05 11:39:42,346][WARN ][discovery.gce ] [homer] Exception caught during discovery: access denied ("java.lang.RuntimePermission" "setFactory") java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "setFactory") at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372) at java.security.AccessController.checkPermission(AccessController.java:559) at java.lang.SecurityManager.checkPermission(SecurityManager.java:549) at java.lang.SecurityManager.checkSetFactory(SecurityManager.java:1625) at javax.net.ssl.HttpsURLConnection.setSSLSocketFactory(HttpsURLConnection.java:362) at com.google.api.client.http.javanet.NetHttpTransport.buildRequest(NetHttpTransport.java:145) at com.google.api.client.http.javanet.NetHttpTransport.buildRequest(NetHttpTransport.java:62) at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:863) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) at org.elasticsearch.cloud.gce.GceComputeServiceImpl$1$1.run(GceComputeServiceImpl.java:94) at org.elasticsearch.cloud.gce.GceComputeServiceImpl$1$1.run(GceComputeServiceImpl.java:90) at java.security.AccessController.doPrivileged(Native Method) 

Glad to provide any information needed to help troubleshoot.

I answered 'y' (yes) to the prompt to add security options to java, also attempted to add them manually. Still no luck. Uninstalled and reinstalled the plugin...

I rolled back to 2.1.1 and it works again...
</description><key id="131732601">16485</key><summary>Plugin does not work with ES 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">phutchins</reporter><labels><label>:Plugin Cloud GCE</label><label>bug</label></labels><created>2016-02-05T19:29:20Z</created><updated>2016-03-01T13:26:35Z</updated><resolved>2016-03-01T07:55:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinliteon" created="2016-02-08T11:34:46Z" id="181327800">Same for me :+1: 
</comment><comment author="nivethav" created="2016-02-13T00:33:28Z" id="183542554">The cloud-gce plugin works with elasticsearch 2.2 as long as no other plugins are installed. Then this is causing the following error : 
*\* Exception caught during discovery: access denied ("java.lang.RuntimePermission" "setFactory")**
</comment><comment author="clintongormley" created="2016-02-13T12:20:29Z" id="183656458">@jasontedor could you take a look at this please?
</comment><comment author="jasontedor" created="2016-02-13T12:24:16Z" id="183657110">&gt; Get complaints about java permissions but adding them doesn't seem to fix...

@phutchins Where did you add the `setFactory` permission?
</comment><comment author="phutchins" created="2016-02-13T15:41:20Z" id="183687978">@jasontedor I tried the server.policy file and I believe one other place that I can't recall at the moment...
</comment><comment author="jasontedor" created="2016-02-13T17:33:02Z" id="183708898">&gt; I tried the server.policy file and I believe one other place that I can't recall at the moment...

@phutchins Would you be willing to try adding it to `${path.plugins}/cloud-gce/plugin-security.policy` as I think that that is where you'll need to add it (depends on where your plugins are installed)? Can you add

```
permission java.lang.RuntimePermission "setFactory";
```

to the `grant` entries there? If you bump into any other permissions that are needed, add them there too (reading through the [code](https://github.com/google/google-http-java-client/blob/1.20.0/google-http-client/src/main/java/com/google/api/client/http/javanet/NetHttpTransport.java) and the [docs](https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/HttpsURLConnection.html), another one that _might_ come up is `permission javax.net.ssl.SSLPermission "setHostnameVerifier";`). Please note that adding any additional security permissions is at your own risk. You can read about the [`setFactory` runtime permission](http://download.java.net/jdk7/archive/b123/docs/api/java/lang/RuntimePermission.html) and the _possible_ [`setHostNameVerifier` SSL permission](https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/SSLPermission.html) but if you report them back here we will do a thorough investigation on them in advance of the next patch release.
</comment><comment author="jasontedor" created="2016-02-25T15:02:53Z" id="188824678">@phutchins I'd like to make sure this can be addressed for a maintenance release. Have you had the opportunity to try the above settings?
</comment><comment author="phutchins" created="2016-02-26T18:09:55Z" id="189402369">Hey @jasontedor, I've not yet but will as soon as I get a chance. Hope to get back to you soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline aggregations do not return metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16484</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/2.2/agg-metadata.html

This works fine for bucket and metrics aggregations, but for pipeline aggregations the supplied metadata isn't returned with the result.

This is a blocker for the .NET client as we're trying to leverage aggregation metadata for deserialization.
</description><key id="131730602">16484</key><summary>Pipeline aggregations do not return metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T19:19:22Z</created><updated>2016-02-11T17:27:47Z</updated><resolved>2016-02-11T17:27:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-09T17:17:57Z" id="181964234">@gmarz which pipeline aggregation are you seeing this with? (or is it all of them?) Do you have a recreation script I can run to show the bug?
</comment><comment author="colings86" created="2016-02-09T17:18:30Z" id="181964653">Actually never mind, didn't see you already have a fix
</comment><comment author="gmarz" created="2016-02-09T17:30:06Z" id="181971263">Yea, sorry I wasn't clear enough originally, it's with all pipeline aggregations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implements creating a term suggester from x-content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16483</link><project id="" key="" /><description /><key id="131713942">16483</key><summary>Implements creating a term suggester from x-content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-02-05T18:11:34Z</created><updated>2016-02-09T20:42:55Z</updated><resolved>2016-02-09T20:42:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-09T11:56:43Z" id="181834268">@abeyad I took a first look and only left a few minor things as comments. Can you also activate the testFromXContent() tests in TermSuggestionBuilderTests? When I did that they were still failing for me. Maybe you can take a look, I guess there might be still some things missing from the parser or some parameters not surviving the roundtrip correctly.
</comment><comment author="abeyad" created="2016-02-09T19:54:11Z" id="182033827">@cbuescher I implemented the changes and all tests pass now.
</comment><comment author="abeyad" created="2016-02-09T20:11:50Z" id="182042465">@cbuescher I changed all `Locale.US` to `Locale.ROOT`
</comment><comment author="abeyad" created="2016-02-09T20:38:46Z" id="182051874">@cbuescher made the changes
</comment><comment author="cbuescher" created="2016-02-09T20:39:38Z" id="182052513">Thanks, LGTM now. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade GeoPointField to use Lucene 5.5 PrefixEncoding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16482</link><project id="" key="" /><description>Lucene 5.5 includes a new `PREFIX_ENCODING` for `GeoPointField` that reduces the number of terms from 8 to 4, and `sizeOf(terms)` from 8 bytes to 5. This gives ~68% savings on index size. The encoding also includes an improved TermEnumerator that provides a slight boost in query performance. The field includes bwc support to guarantee compatibility with `NUMERIC_ENCODING` fields in 2.2. This improvement will upgrade GeoPointField to use the new `PREFIX_ENCODING` for 2.x
</description><key id="131710531">16482</key><summary>Upgrade GeoPointField to use Lucene 5.5 PrefixEncoding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T17:57:38Z</created><updated>2016-02-15T19:46:43Z</updated><resolved>2016-02-15T19:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-02-15T19:46:43Z" id="184360646">Closed by #16615 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_distance query, performance regression (from 1.7 to 2.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16481</link><project id="" key="" /><description>I'm currently migrating from Elasticsearch 1.7 to 2.2 and I have a performance issue with the geo_distance query.

My model is simple:
- 20k documents in a type
- each document contains a field with type "array of object" (nested mapping)
- these nested documents contains a "location" field (lat/lon)

I want to filter my root documents by distance.

My search queries:

Elasticsearch 1.7:

``` json
{
 "filter": {
   "geo_distance": {
     "distance": "100000m",
     "stores.location": {
       "lat": 48.86,
       "lon": 2.35
     }
   }
 }
}
```

Elasticsearch 2.2:

``` json
{
  "query": {
    "bool": {
      "filter": {
        "geo_distance": {
          "distance": "100000m",
          "stores.location": {
            "lat": 48.86,
            "lon": 2.35
          }
        }
      }
    }
  }
}
```

With Elasticsearch 1.7, the response time (took) is around 35ms.
With Elasticsearch 2.2, the response time is around 3700ms.

If I test with random locations, the response time of Elasticsearch 1.7 stay stable. (there is no cached result)

Is there a way to improve performance?
</description><key id="131692313">16481</key><summary>geo_distance query, performance regression (from 1.7 to 2.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">pierrre</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2016-02-05T16:49:09Z</created><updated>2016-03-30T18:13:35Z</updated><resolved>2016-02-13T13:33:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-02-05T17:45:39Z" id="180464920">Can you post your mapping? Showing nested type definitions and all.
</comment><comment author="pierrre" created="2016-02-05T18:17:26Z" id="180479677">Of course (I removed non relevant fields):

``` json
{  
  "title":{  
    "type":"string",
    "analyzer":"french"
  },
  "stores":{  
    "type":"nested",
    "include_in_parent":true,
    "properties":{  
      "id":{  
        "type":"string",
        "index":"not_analyzed"
      },
      "location":{  
        "type":"geo_point"
      }
    }
  }
}
```

In my documents `stores` is an array of objects.
There are usually 50-200 nested documents per root document.

I didn't use the "nested" query in my previous message, because it doesn't change anything to the performance issue.
But I still need to use "nested" in my case (I have other fields).
</comment><comment author="pierrre" created="2016-02-08T10:11:08Z" id="181288953">Even weirder:

I reimplemented the "geo_distance query" with a "script query":

``` json
{
  "query": {
    "bool": {
      "filter": {
        "script": {
          "script": "import org.elasticsearch.common.unit.DistanceUnit;\nimport org.elasticsearch.common.geo.GeoDistance;\nfor (storeLocation in doc[\"stores.location\"]) {\n    distance = GeoDistance.ARC.calculate(location.lat, location.lon, storeLocation.lat, storeLocation.lon, DistanceUnit.METERS);\n    if (distance &lt;= storeDistanceMax) {\n        return true;\n    }\n}\nreturn false;",
          "params": {
            "location": {
              "lat": 49.85,
              "lon": 2.35
            },
            "storeDistanceMax": 100000
          }
        }
      }
    }
  },
  "fields": []
}
```

My search took 200ms! (instead of 3700ms for the "geo_distance query")
</comment><comment author="pierrre" created="2016-02-08T10:24:31Z" id="181295594">It's getting worse:

I added a "term query" before the "geo_distance query", to restrict the search to 1 hit:

``` json
{
  "query": {
    "bool": {
      "filter": {
        "query": {
          "bool": {
            "must": [
              {
                "term": {
                  "_id": "51a89a7c4eb8dd9e41000025"
                }
              },
              {
                "geo_distance": {
                  "distance": "100000m",
                  "stores.location": {
                    "lat": 48.71,
                    "lon": 2.35
                  }
                }
              }
            ]
          }
        }
      }
    }
  }
}
```

There is only 1 document with this id.
It takes 500ms.
(there are 53 sub documents in the "store" field)
</comment><comment author="clintongormley" created="2016-02-08T10:51:07Z" id="181303642">@pierrre Could you set the `profile` parameter to `true` and upload the response please?
</comment><comment author="pierrre" created="2016-02-08T11:25:41Z" id="181323461">Search:

``` json
{
  "query": {
    "bool": {
      "filter": {
        "geo_distance": {
          "distance": "100000m",
          "stores.location": {
            "lat": 49.83,
            "lon": 2.34
          }
        }
      }
    }
  },
  "fields": [],
  "profile": true
}
```

Profile (I removed the hits):

``` json
{  
  "took":1880,
  "timed_out":false,
  "_shards":{  
    "total":5,
    "successful":5,
    "failed":0
  },
  "hits":{  
    "total":4701,
    "max_score":0
  },
  "profile":{  
    "shards":[  
      {  
        "id":"[ST0bPIZTSbOP768od67YHA][test-v5][1]",
        "searches":[  
          {  
            "query":[  
              {  
                "query_type":"BooleanQuery",
                "lucene":"+ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0 #ConstantScore(_type:offer)",
                "time":"4961.247043ms",
                "breakdown":{  
                  "score":74250,
                  "create_weight":285299,
                  "next_doc":461141,
                  "match":0,
                  "build_scorer":1653267115,
                  "advance":0
                },
                "children":[  
                  {  
                    "query_type":"BoostQuery",
                    "lucene":"ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0",
                    "time":"3305.913181ms",
                    "breakdown":{  
                      "score":19386,
                      "create_weight":12349,
                      "next_doc":0,
                      "match":0,
                      "build_scorer":1652853718,
                      "advance":123164
                    },
                    "children":[  
                      {  
                        "query_type":"GeoPointTermQueryConstantScoreWrapper",
                        "lucene":"GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952]",
                        "time":"1652.904564ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":3376,
                          "next_doc":0,
                          "match":0,
                          "build_scorer":1652844406,
                          "advance":56782
                        }
                      }
                    ]
                  },
                  {  
                    "query_type":"ConstantScoreQuery",
                    "lucene":"ConstantScore(_type:offer)",
                    "time":"1.246057000ms",
                    "breakdown":{  
                      "score":0,
                      "create_weight":253447,
                      "next_doc":106307,
                      "match":0,
                      "build_scorer":234222,
                      "advance":99908
                    },
                    "children":[  
                      {  
                        "query_type":"TermQuery",
                        "lucene":"_type:offer",
                        "time":"0.5521730000ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":244762,
                          "next_doc":62877,
                          "match":0,
                          "build_scorer":166216,
                          "advance":78318
                        }
                      }
                    ]
                  }
                ]
              }
            ],
            "rewrite_time":48665,
            "collector":[  
              {  
                "name":"SimpleTopScoreDocCollector",
                "reason":"search_top_hits",
                "time":"0.1369960000ms"
              }
            ]
          }
        ]
      },
      {  
        "id":"[ST0bPIZTSbOP768od67YHA][test-v5][2]",
        "searches":[  
          {  
            "query":[  
              {  
                "query_type":"BooleanQuery",
                "lucene":"+ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0 #ConstantScore(_type:offer)",
                "time":"4704.517483ms",
                "breakdown":{  
                  "score":103082,
                  "create_weight":1672361,
                  "next_doc":599497,
                  "match":0,
                  "build_scorer":1566220273,
                  "advance":0
                },
                "children":[  
                  {  
                    "query_type":"BoostQuery",
                    "lucene":"ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0",
                    "time":"3131.965622ms",
                    "breakdown":{  
                      "score":27974,
                      "create_weight":21414,
                      "next_doc":0,
                      "match":0,
                      "build_scorer":1565840240,
                      "advance":164784
                    },
                    "children":[  
                      {  
                        "query_type":"GeoPointTermQueryConstantScoreWrapper",
                        "lucene":"GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952]",
                        "time":"1565.911210ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":8763,
                          "next_doc":0,
                          "match":0,
                          "build_scorer":1565832122,
                          "advance":70325
                        }
                      }
                    ]
                  },
                  {  
                    "query_type":"ConstantScoreQuery",
                    "lucene":"ConstantScore(_type:offer)",
                    "time":"3.956648000ms",
                    "breakdown":{  
                      "score":0,
                      "create_weight":1627981,
                      "next_doc":131485,
                      "match":0,
                      "build_scorer":183439,
                      "advance":124196
                    },
                    "children":[  
                      {  
                        "query_type":"TermQuery",
                        "lucene":"_type:offer",
                        "time":"1.889547000ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":1614342,
                          "next_doc":70688,
                          "match":0,
                          "build_scorer":110984,
                          "advance":93533
                        }
                      }
                    ]
                  }
                ]
              }
            ],
            "rewrite_time":64977,
            "collector":[  
              {  
                "name":"SimpleTopScoreDocCollector",
                "reason":"search_top_hits",
                "time":"0.1800850000ms"
              }
            ]
          }
        ]
      },
      {  
        "id":"[ST0bPIZTSbOP768od67YHA][test-v5][0]",
        "searches":[  
          {  
            "query":[  
              {  
                "query_type":"BooleanQuery",
                "lucene":"+ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0 #ConstantScore(_type:offer)",
                "time":"5089.745722ms",
                "breakdown":{  
                  "score":90465,
                  "create_weight":358410,
                  "next_doc":621934,
                  "match":0,
                  "build_scorer":1695828128,
                  "advance":0
                },
                "children":[  
                  {  
                    "query_type":"BoostQuery",
                    "lucene":"ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0",
                    "time":"3391.571303ms",
                    "breakdown":{  
                      "score":24217,
                      "create_weight":17164,
                      "next_doc":0,
                      "match":0,
                      "build_scorer":1695648631,
                      "advance":159863
                    },
                    "children":[  
                      {  
                        "query_type":"GeoPointTermQueryConstantScoreWrapper",
                        "lucene":"GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952]",
                        "time":"1695.721428ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":4688,
                          "next_doc":0,
                          "match":0,
                          "build_scorer":1695642225,
                          "advance":74515
                        }
                      }
                    ]
                  },
                  {  
                    "query_type":"ConstantScoreQuery",
                    "lucene":"ConstantScore(_type:offer)",
                    "time":"1.275482000ms",
                    "breakdown":{  
                      "score":0,
                      "create_weight":313628,
                      "next_doc":105851,
                      "match":0,
                      "build_scorer":93796,
                      "advance":195599
                    },
                    "children":[  
                      {  
                        "query_type":"TermQuery",
                        "lucene":"_type:offer",
                        "time":"0.5666080000ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":304105,
                          "next_doc":52834,
                          "match":0,
                          "build_scorer":42238,
                          "advance":167431
                        }
                      }
                    ]
                  }
                ]
              }
            ],
            "rewrite_time":66164,
            "collector":[  
              {  
                "name":"SimpleTopScoreDocCollector",
                "reason":"search_top_hits",
                "time":"0.1565940000ms"
              }
            ]
          }
        ]
      },
      {  
        "id":"[ST0bPIZTSbOP768od67YHA][test-v5][3]",
        "searches":[  
          {  
            "query":[  
              {  
                "query_type":"BooleanQuery",
                "lucene":"+ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0 #ConstantScore(_type:offer)",
                "time":"4168.444183ms",
                "breakdown":{  
                  "score":112480,
                  "create_weight":1390519,
                  "next_doc":651504,
                  "match":0,
                  "build_scorer":1387739181,
                  "advance":0
                },
                "children":[  
                  {  
                    "query_type":"BoostQuery",
                    "lucene":"ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0",
                    "time":"2775.107749ms",
                    "breakdown":{  
                      "score":30077,
                      "create_weight":21344,
                      "next_doc":0,
                      "match":0,
                      "build_scorer":1387401154,
                      "advance":178740
                    },
                    "children":[  
                      {  
                        "query_type":"GeoPointTermQueryConstantScoreWrapper",
                        "lucene":"GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952]",
                        "time":"1387.476434ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":8741,
                          "next_doc":0,
                          "match":0,
                          "build_scorer":1387392110,
                          "advance":75583
                        }
                      }
                    ]
                  },
                  {  
                    "query_type":"ConstantScoreQuery",
                    "lucene":"ConstantScore(_type:offer)",
                    "time":"3.442750000ms",
                    "breakdown":{  
                      "score":0,
                      "create_weight":1345801,
                      "next_doc":142323,
                      "match":0,
                      "build_scorer":180087,
                      "advance":139750
                    },
                    "children":[  
                      {  
                        "query_type":"TermQuery",
                        "lucene":"_type:offer",
                        "time":"1.634789000ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":1336059,
                          "next_doc":76175,
                          "match":0,
                          "build_scorer":114883,
                          "advance":107672
                        }
                      }
                    ]
                  }
                ]
              }
            ],
            "rewrite_time":43438,
            "collector":[  
              {  
                "name":"SimpleTopScoreDocCollector",
                "reason":"search_top_hits",
                "time":"0.1943090000ms"
              }
            ]
          }
        ]
      },
      {  
        "id":"[ST0bPIZTSbOP768od67YHA][test-v5][4]",
        "searches":[  
          {  
            "query":[  
              {  
                "query_type":"BooleanQuery",
                "lucene":"+ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0 #ConstantScore(_type:offer)",
                "time":"5621.826671ms",
                "breakdown":{  
                  "score":105805,
                  "create_weight":230198,
                  "next_doc":612438,
                  "match":0,
                  "build_scorer":1873433238,
                  "advance":0
                },
                "children":[  
                  {  
                    "query_type":"BoostQuery",
                    "lucene":"ConstantScore(GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952])^0.0",
                    "time":"3746.271286ms",
                    "breakdown":{  
                      "score":28564,
                      "create_weight":11564,
                      "next_doc":0,
                      "match":0,
                      "build_scorer":1873000894,
                      "advance":170871
                    },
                    "children":[  
                      {  
                        "query_type":"GeoPointTermQueryConstantScoreWrapper",
                        "lucene":"GeoPointDistanceQueryImpl: field=stores.location: Lower Left: [0.9473075707611631,48.93168471588048] Upper Right: [3.7326924292388366,50.72831528411952]",
                        "time":"1873.059393ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":3611,
                          "next_doc":0,
                          "match":0,
                          "build_scorer":1872983462,
                          "advance":72320
                        }
                      }
                    ]
                  },
                  {  
                    "query_type":"ConstantScoreQuery",
                    "lucene":"ConstantScore(_type:offer)",
                    "time":"1.173706000ms",
                    "breakdown":{  
                      "score":0,
                      "create_weight":199370,
                      "next_doc":137624,
                      "match":0,
                      "build_scorer":234299,
                      "advance":124913
                    },
                    "children":[  
                      {  
                        "query_type":"TermQuery",
                        "lucene":"_type:offer",
                        "time":"0.4775000000ms",
                        "breakdown":{  
                          "score":0,
                          "create_weight":188244,
                          "next_doc":75295,
                          "match":0,
                          "build_scorer":120317,
                          "advance":93644
                        }
                      }
                    ]
                  }
                ]
              }
            ],
            "rewrite_time":44649,
            "collector":[  
              {  
                "name":"SimpleTopScoreDocCollector",
                "reason":"search_top_hits",
                "time":"0.1850390000ms"
              }
            ]
          }
        ]
      }
    ]
  }
}
```

I can't use "profile" with Elasticsearch 1.7, but my query is:

``` json
{
  "filter": {
    "geo_distance": {
      "distance": "100000m",
      "stores.location": {
        "lat": 49.83,
        "lon": 2.34
      }
    }
  },
  "fields": []
}
```
</comment><comment author="clintongormley" created="2016-02-08T11:30:49Z" id="181326656">@pierrre thanks. btw - did you reindex your geo-points in 2.2, or reuse geo-points from an earlier index?
</comment><comment author="pierrre" created="2016-02-08T12:40:45Z" id="181352140">@clintongormley yes, I created a new index (it's a new Elasticsearch instance), and reindexed all my data.

I also observe the same regression with the "geo_bounding_box" and "geo_distance_range" queries.
</comment><comment author="jimczi" created="2016-02-08T13:21:31Z" id="181364747">&gt; My model is simple: 20k documents in a type

@pierre is it the total number of documents with a field named `stores.location` ? Do you have other types with the same field name in your index ? What is the total number of documents in this index (20k ?) ?
</comment><comment author="pierrre" created="2016-02-08T13:30:40Z" id="181367609">&gt; is it the total number of documents with a field named stores.location ?

@jimferenczi yes, there are 20k root documents with the field `stores.location`.
There are between 50-200 documents in each `stores`fields. (sometimes more)

&gt; Do you have other types with the same field name in your index ?

No, there is only 1 type that uses `stores` &amp; `stores.location`.
I checked with GET `myindex/_mapping`.

&gt; What is the total number of documents in this index (20k ?) ?

GET `myindex/_count` returns 207322.
</comment><comment author="clintongormley" created="2016-02-08T14:39:35Z" id="181399978">I've tried this out locally, build an index with 1 million random lat/lon points written in 2.1.1 and in a new index in 2.2.0.  Also added in 50 nested docs in each doc, and using the following settings/mappings:

```
PUT x
{
  "settings": {
    "index.number_of_shards": 1,
    "index.queries.cache.type": "none"
  },
  "mappings": {
    "t": {
      "properties": {
        "loc": {
          "type": "geo_point"
        },
        "nested": {
          "type": "nested",
          "properties": {
            "foo": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```

(note I've disabled caching so that that doesn't interfere with the measurements)

Then I query it like this:

```
GET x/t/_search?size=1
{
  "query": {
    "bool": {
      "must": {
        "geo_distance": {
          "distance": "10000km",
          "loc": {
            "lat": 0,
            "lon": 0
          }
        }
      }
    }
  }
}
```

Performance for v1 geo-points is pretty much the same whatever the distance (&gt; distance == more matching docs).  V2 varies greatly depending on how many docs match, eg for a distance of "10000km" it is about the same as v1, eg 1000ms, for eg "10km" this drops to eg 16ms.

So things seem to be working as expected, with v2 much faster than v1, even with the nested docs.

@pierrre are you running this on eg your laptop?  v1 distance calculations are memory based, so all geopoints get loaded onto your heap.  v2 calculations use the index so it is important to have enough file system cache (and use fast disks) to get best performance.  I wonder if you're running this in an unrealistic environment, eg laptop with little free memory for file system caching?
</comment><comment author="pierrre" created="2016-02-08T14:53:22Z" id="181409931">@clintongormley in my mapping, the location field is in nested documents: https://github.com/elastic/elasticsearch/issues/16481#issuecomment-180479677

I'll try to write a script to create the index/add data.

&gt; are you running this on eg your laptop? v1 distance calculations are memory based, so all geopoints get loaded onto your heap. v2 calculations use the index so it is important to have enough file system cache (and use fast disks) to get best performance. I wonder if you're running this in an unrealistic environment, eg laptop with little free memory for file system caching?

Yes I run my test on a laptop:
- Intel(R) Core(TM) i7-3610QM CPU @ 2.30GHz (max 3.3Ghz), 4 cores, 2 threads per core
- 8GB RAM
- SSD Samsung 840

I also use the default Elasticsearch config: -Xms256m -Xmx1g
Should I increase the memory usage?
</comment><comment author="clintongormley" created="2016-02-08T14:57:05Z" id="181412946">&gt; in my mapping, the location field is in a nested documents

But you're querying it at the root level, because I don't see a nested query anywhere?

&gt; Should I increase the memory usage?

No, that's the point.  V2 is in your file system cache, NOT the ES heap, so if you don't have much free space for caching, then it'll need to hit disk all the time.
</comment><comment author="clintongormley" created="2016-02-08T14:58:27Z" id="181414469">&gt; But you're querying it at the root level, because I don't see a nested query anywhere?

OK so the difference here is multiple geopoints per field at the root level, I'll try that.
</comment><comment author="pierrre" created="2016-02-08T15:00:06Z" id="181415232">&gt; &gt; in my mapping, the location field is in a nested documents
&gt; 
&gt; But you're querying it at the root level, because I don't see a nested query anywhere?

In my queries, I need a nested query (there are other fields in my nesteds documents).
But I've got a performance issue even if I don't use a nested query.
</comment><comment author="clintongormley" created="2016-02-08T15:39:21Z" id="181427533">OK, with 100,000 docs, each with 50 random geopoints, v1 takes about 2 seconds for me, regardless of how many docs match.  v2 varies dramatically with the number of docs that match, eg if i set the distance to 10km (5 docs) it takes 13ms, for 100km (751 docs) it takes 80ms, for 1000km (54,747 docs) it takes 2,200ms, and for 10000km (88,551 docs) it took 100 seconds!

The times for v2 with a geo-bounding-box covering most of the earth was better, but still 10-15x slower than v1.  @nknize over to you!
</comment><comment author="pierrre" created="2016-02-08T16:49:09Z" id="181461774">Here is a script (Go) to reproduce the issue: https://gist.github.com/pierrre/1b95f47d25d30e03316c

Edit: with Go 1.7, you need to run the `search()` function 5-10 times, then the execution time is low.
</comment><comment author="nknize" created="2016-02-08T18:26:41Z" id="181510363">Thanks for the reproduction @pierrre. There's an issue with multiple geo-fields in Lucene 5.4.1. I have a fix that will be available in the ES 2.2.1 release.
</comment><comment author="pierrre" created="2016-02-08T20:30:45Z" id="181555968">Lucene issue: https://issues.apache.org/jira/browse/LUCENE-7018
</comment><comment author="nknize" created="2016-02-09T04:35:26Z" id="181706285">update: feature branch cut. Bug fix back ported. 5.5 will be cut on Wed. There are a few other relational fixes I'm going to back port to 5.4.2. Will keep this issue updated when 5.4.2 is released. 
</comment><comment author="nknize" created="2016-02-12T18:26:41Z" id="183440741">@pierrre with the release of 5.5 and 2.3 right around the corner we have decided to forgo fixing this in 2.2.x. This bugfix, along with other encoding optimizations, will be available in the 2.3 release coming real soon.
</comment><comment author="pierrre" created="2016-02-12T18:42:59Z" id="183445407">OK, thank you.
</comment><comment author="pierrre" created="2016-02-12T21:18:46Z" id="183491120">Any idea of ETA for Elasticsearch 2.3.0? 1 week? 1 month? When it's done? :D
</comment><comment author="nknize" created="2016-02-12T21:22:44Z" id="183492485">Should be more information coming out at Elasticon. I defer to @clintongormley 
</comment><comment author="clintongormley" created="2016-02-13T12:02:43Z" id="183654880">it'll definitely be out before 2.3.1 :)
</comment><comment author="clintongormley" created="2016-02-13T13:33:43Z" id="183665448">Closed by https://github.com/elastic/elasticsearch/pull/16615
</comment><comment author="jweber" created="2016-03-12T21:29:36Z" id="195811043">My apologies if this isn't the right place to comment on this, but I wanted to mention that we are seeing similar poor performance with `geo_distance` queries in 2.2.0 but on documents without nested collections of geopoints. I [commented on a post](https://discuss.elastic.co/t/slower-geo-distance-queries-in-2-2/41655/4) in the Elasticsearch forums with this data but I also wanted to add it here to increase visibility.

I also tried [the script based approach](https://github.com/elastic/elasticsearch/issues/16481#issuecomment-181288953) that @pierrre performed and found it to return results faster than the equivalent `geo_distance` but under load started encountering shard errors using it.

Here's the comment I posted on the forums:

---

I've definitely encountered this issue as well and are able to recreate it with test data. The following comparisons were performed against v2.1.2, v2.2.0 and 5.0.0 (built from source).

The test data is 6 million documents with random locations on the following index (single shard, query cache disabled). I'm running Elasticsearch with the stock configs on my development machine.

```
{
  "test": {
    "mappings": {
      "document": {
        "properties": {
          "id": {
            "type": "integer"
          },
          "location": {
            "type": "geo_point",
            "lat_lon": true
          }
        }
      }
    }
  }
}
```

The `geo_distance` queries look like:

```
{
  "query": {
    "bool": {
      "filter": {
        "geo_distance": {
          "distance": "100mi",
          "location": "[lat], [lon]"
        }
      }
    }
  }
}
```

Here are the average times when performing the same 100 `geo_distance` searches against the different Elasticsearch versions: 

```
Radius      2.1.2       2.2.0       5.0.0
1mi         256ms       1ms         2ms
10mi        206ms       10ms        2ms
30mi        236ms       36ms        3ms
50mi        252ms       277ms       40ms
100mi       258ms       461ms       74ms
250mi       241ms       2,088ms     315ms
500mi       227ms       3,534ms     643ms
1000mi      223ms       943ms       339ms
2000mi      280ms       754ms       683ms
```

I was also curious about bounding box performance, so I ran the same queries using a bounding box that circumbscribes the diameter of the `geo_distance` query:

```
            2.1.2       2.2.0       5.0.0
1mi         211ms       23ms        9ms
10mi        217ms       72ms        13ms
30mi        221ms       60ms        18ms
50mi        221ms       91ms        23ms
100mi       220ms       89ms        32ms
250mi       223ms       130ms       54ms
500mi       231ms       168ms       81ms
1000mi      231ms       156ms       96ms
2000mi      226ms       143ms       100ms
```

The slower queries for 2.2.0 and 5.0.0 always have the largest matching count of documents, so queries over areas with lower matching documents perform quickly.

I'm not too sure what to think about the v2.2.0 performance at 250mi and 500mi, but it's definitely not an option for us to run those searches. The next version looks to be an improvement over 2.2.0 but still lags behind 2.1.2 with regards to larger radius searches.
</comment><comment author="cdonnellytx" created="2016-03-22T00:32:09Z" id="199558295">We are also seeing poor performance with documents indexed on 2.2.1 (as opposed to a preexisting index created on 1.7.3 prior to upgrade).

It looks like this was closed because 2.3.0 was slated to be released sometime in February, but it's been over a month and 2.3.0 hasn't been released, nor can I find any documentation on when it will be released.

Is there a workaround on 2.2 where you can tell Elasticsearch to index using the old format? Or are the only workarounds the script / not upgrading at this time?
</comment><comment author="jweber" created="2016-03-24T16:54:37Z" id="200922590">We've not been able to find an adequate workaround for this issue on 2.2.0. The script option, while performing better than using geo_distance queries, still does not give acceptable performance. I ran the same performance tests against the current 2.3.0 branch and it looks like that will in fact resolve this issue. It would be nice to have an updated estimate on when 2.3.0 will be released as 2.2.x does not perform well enough in these scenarios for us to use.
</comment><comment author="tmatei" created="2016-03-29T23:34:38Z" id="203158565">I've encountered the same issue and I'm running 2.2.1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sense output truncated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16480</link><project id="" key="" /><description>When using Sense (2.0.0-beta2), the output window is truncated (i.e. the displayed output is as high as the query on the left):

![image](https://cloud.githubusercontent.com/assets/1345886/12851600/1332a28e-cc2b-11e5-9b72-8cb230e2164e.png)
</description><key id="131682569">16480</key><summary>Sense output truncated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wsw70</reporter><labels /><created>2016-02-05T16:09:06Z</created><updated>2016-02-05T19:14:08Z</updated><resolved>2016-02-05T17:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-05T17:59:45Z" id="180471133">There is a [repository](https://github.com/elastic/sense) for Sense that would be a better place to report this issue.
</comment><comment author="wsw70" created="2016-02-05T19:14:08Z" id="180506756">Thanks. [Done](https://github.com/elastic/sense/issues/109).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid cloning MessageDigest instances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16479</link><project id="" key="" /><description>This commit modifies the MessageDigests message digest provider to
return a thread local instance of MessageDigest instances instead of
using clone since some providers do not support clone.
</description><key id="131680665">16479</key><summary>Avoid cloning MessageDigest instances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T16:03:54Z</created><updated>2016-02-09T13:34:23Z</updated><resolved>2016-02-06T14:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-05T20:33:09Z" id="180545836">Performance benchmarks show that switching from clone to creating a new instance on every request for an instance is devastating to performance. However, using a thread local instance has a positive impact on the performance:

```
Benchmark                                        Mode  Cnt         Score         Error  Units
MessageDigestDigestBenchmark.cloneAndDigest     thrpt   15  23384576.673 &#177;  205035.770  ops/s
MessageDigestDigestBenchmark.createAndDigest    thrpt   15   5002641.979 &#177;  160748.144  ops/s
MessageDigestDigestBenchmark.threadLocalDigest  thrpt   15  26248552.071 &#177; 1447414.585  ops/s
```

Additional details on [gist](https://gist.github.com/jasontedor/be374573194044dd845c).
</comment><comment author="jaymode" created="2016-02-05T21:11:56Z" id="180555419">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Ingest info to api and validate processors exist across cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16478</link><project id="" key="" /><description>- Adds IngestInfo to _nodes api
- Adds /_ingest/processors rest endpoint to fetch processors across
  cluster
- Adds getProcessors to client
- Validates that all processors defined in a stored or simulated
  pipeline exist on all ingest nodes across the cluster
</description><key id="131669335">16478</key><summary>Add Ingest info to api and validate processors exist across cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>discuss</label><label>enhancement</label><label>WIP</label></labels><created>2016-02-05T15:18:49Z</created><updated>2016-02-29T16:46:13Z</updated><resolved>2016-02-29T16:46:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-29T16:46:12Z" id="190283426">Closed in favour of #16865
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate AWS settings to new settings infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16477</link><project id="" key="" /><description>Closes #16293.
</description><key id="131668347">16477</key><summary>Migrate AWS settings to new settings infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Settings</label></labels><created>2016-02-05T15:14:30Z</created><updated>2016-02-13T12:43:39Z</updated><resolved>2016-02-10T13:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-05T15:14:52Z" id="180395682">@s1monw @danielmitterdorfer Wanna look at this?
</comment><comment author="danielmitterdorfer" created="2016-02-08T10:57:30Z" id="181306631">@dadoonet I went through the changes and made a few comments here and there.
</comment><comment author="dadoonet" created="2016-02-10T12:11:55Z" id="182338632">@danielmitterdorfer I pushed another commit based on your comments. Thanks!
</comment><comment author="danielmitterdorfer" created="2016-02-10T12:20:16Z" id="182343493">Left one comment related to `AwsSignersTests` but apart from that LGTM.
</comment><comment author="dadoonet" created="2016-02-10T12:45:25Z" id="182356462">Thanks @danielmitterdorfer. I Just rebased, squashed. Running all tests ATM. I will push it later today.

Thanks again for the review!
</comment><comment author="dadoonet" created="2016-02-10T13:08:34Z" id="182365042">Ha! Checkstyle now complains once I rebased on master due to our new policy. Will fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add foreground setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16476</link><project id="" key="" /><description>I toyed with the idea of renaming it to bootstrap.foreground but we already
have a Setting for pidfile so I figured that is what we meant to do. Without
this the RPM and DEB packages don't start at all.
</description><key id="131668125">16476</key><summary>Add foreground setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Settings</label><label>bug</label></labels><created>2016-02-05T15:13:23Z</created><updated>2016-02-13T19:32:13Z</updated><resolved>2016-02-05T15:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-05T15:15:27Z" id="180395821">We already have one or two sysprops for this. Can the old shit be removed? I'd really rather us not have a third.
</comment><comment author="nik9000" created="2016-02-05T15:17:48Z" id="180396738">I'm done with this. I'll go edit something else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Client: Reference to org.apache.lucene.util.Version in org.elasticsearch.Version drags in lots of Lucene classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16475</link><project id="" key="" /><description>We tried to exclude some of the large list of transitive dependencies when using the Java Client of Elasticsearch to avoid having the complete Elasticsearch server part of the client application.

Some of the transitive dependencies use other licenses than Apache-2.0, so it is currently hard to verify license status of a product when it tries to use the Elasticsearch Java Client.

The thinking is that it should be possible to not include most of lucene as it is not really used on the client and this way avoid some more dependencies further down, like spatial4j and others.

One of the first stumbling blocks is that there is a constant in org.elasticsearch.Version which drags in a lot of Lucene classes simply by referencing org.apache.lucene.util.Version. 

Hopefully removing this reference would allow to exclude some more transitive dependencies here.

Any interest in PRs or a list of references that can be excluded on the client-side?
</description><key id="131643704">16475</key><summary>Java Client: Reference to org.apache.lucene.util.Version in org.elasticsearch.Version drags in lots of Lucene classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-02-05T13:25:26Z</created><updated>2017-05-05T15:14:39Z</updated><resolved>2017-05-05T15:14:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-05T13:54:22Z" id="180368455">The trouble is that the Java client is the client Elasticsearch uses for node to node communication. So anything you do on the Java client is has to make sense in the node to node context.

Lots of folks have talked about making a Java client that isn't the internode client. Like one with minimal dependencies. I think this is the right thing to do.

&gt; spatial4j

This is being worked on from the other end - getting spatial4j as not a dependency at all. It is a long process. More than one major version off.
</comment><comment author="centic9" created="2016-02-09T13:51:09Z" id="181871854">FYI, the following things can be excluded in Gradle builds when only the client-side of Elasticsearch is used:

``` groovy
configurations {
    // exclude some unneded Elasticsearch parts 
    all*.exclude group: 'org.apache.lucene', module: 'lucene-backward-codecs'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-expressions'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-highlighter'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-join'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-memory'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-queries'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-queryparser'
    all*.exclude group: 'org.apache.lucene', module: 'lucene-spatial'

    all*.exclude group: 'org.hdrhistogram'
    // needed by TransportClient: all*.exclude group: 'com.twitter'
    // needed by BulkProcessor: all*.exclude group: 'com.carrotsearch'
    // needed by TransportClient: all*.exclude group: 'com.tdunning'
    all*.exclude group: 'commons-cli'
}
```
</comment><comment author="nik9000" created="2016-02-09T14:35:51Z" id="181892586">Thanks for that.

I've _wanted_ to remove dependencies from the Client but it just feels like a better idea to build a java client. On the other hand I'm not actively doing that right now either. So I don't really know what to say.
</comment><comment author="javanna" created="2017-05-05T15:14:02Z" id="299492239">This was brought in other issues as well, and recently discussed in https://github.com/elastic/elasticsearch/issues/23331#issuecomment-286830030 . We have released a low level REST client that allows to communicate with Elasticsearch via http and requires minimal dependencies. We are working on a high level REST client that helps with building requests and reading responses, but that one will still depend on Elasticsearch, at least in the beginning.
See https://www.elastic.co/blog/state-of-the-official-elasticsearch-java-clients .

We would like to minimize the dependencies needed by the high level REST client as well, but we are now focussing on getting out the high level REST client and have its requests and responses compatible with the java API that many users have used up until now. Later, we will work on further improvements, but it will take time to have the high level client completely independent and with a minimal set of dependencies. We certainly won't to this for the `TransportClient` which is going to be replaced by the high level REST client.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decimal histograms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16474</link><project id="" key="" /><description>I am not finding any good reference about how to set up a range histogram using decimal bins.
I could find a plugin that implements this (until version 1.XX) but I am worried with the sustainability of this fix.
Any new plans?
</description><key id="131639690">16474</key><summary>Decimal histograms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">njss</reporter><labels /><created>2016-02-05T13:07:14Z</created><updated>2016-02-08T11:19:44Z</updated><resolved>2016-02-08T09:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T09:11:10Z" id="181265471">Duplicate of #4847.

Nothing new unfortunately. Aggregations are currently undergoing a significant refactoring that aims at making them parsed on the coordinating node instead of only on the shards. Once it is finished, I think we should look into this feature again.
</comment><comment author="njss" created="2016-02-08T11:19:44Z" id="181318784">Ok, thank you for the answer.
This is considered a very important feature, right!?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split AggregatorFactory into AggregatorBuilder and AggregatorFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16473</link><project id="" key="" /><description /><key id="131627347">16473</key><summary>Split AggregatorFactory into AggregatorBuilder and AggregatorFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-05T12:10:41Z</created><updated>2016-02-08T14:53:04Z</updated><resolved>2016-02-08T14:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T14:21:04Z" id="181393376">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>S3/Azure IT should use ESBlobStoreRepositoryIntegTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16472</link><project id="" key="" /><description>As we now have #14050, we can try use it for Azure and S3 Integration tests as well.

See for example: https://github.com/elastic/elasticsearch/blob/master/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java
- [x] Azure. See https://github.com/elastic/elasticsearch/issues/18451
- [ ] S3
</description><key id="131627295">16472</key><summary>S3/Azure IT should use ESBlobStoreRepositoryIntegTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>:Plugin Repository S3</label><label>adoptme</label><label>test</label></labels><created>2016-02-05T12:10:28Z</created><updated>2016-05-27T06:41:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Apply system properties after all arguments are parsed in BootstrapCLIParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16471</link><project id="" key="" /><description>One of our tests leaked a system property here since we failed after appling some
system properties in BootstrapCLIParser. This is not a huge deal in production since
we exit the JVM if we fail on that. Yet for correctnes we should only apply them if
we manage to parse them all.
This also caused a test failure lately on CI but on an unrelated test:
  https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/314/console
</description><key id="131606058">16471</key><summary>Apply system properties after all arguments are parsed in BootstrapCLIParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T10:23:52Z</created><updated>2016-03-14T13:46:07Z</updated><resolved>2016-02-05T13:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-05T10:23:59Z" id="180285516">@ywelsch can you look 
</comment><comment author="ywelsch" created="2016-02-05T10:32:44Z" id="180287524">Left minor comment and one question (which we can address in a later PR). LGTM otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make IndicesWarmer a private class of IndexService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16470</link><project id="" key="" /><description>There is no need for IndicesWarmer to be a global accessible class. All it needs
access to is inside IndexService. It also doesn't need to be mutable once it's not a per node
instance. This commit move IndicesWarmer to IndexWarmer and makes the default impls like field data and
norms warming an impl detail. Also the IndexShard doesn't depend on this class anymore, instead it accepts
an Engine.Warmer as a ctor argument which delegates to the actual warmer from the index.
</description><key id="131599484">16470</key><summary>Make IndicesWarmer a private class of IndexService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T09:47:44Z</created><updated>2016-02-13T19:30:56Z</updated><resolved>2016-02-05T14:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-05T12:07:00Z" id="180321785">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify IndicesFieldDataCache and detach from guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16469</link><project id="" key="" /><description>Indices level field data cacheing belongs into IndicesService and doesn't need to be
wired by guice. This commit also moves the async cache refresh out of the class into
IndicesService such that threadpool dependencies are removed and testing / creation becomes
simpler.
</description><key id="131587702">16469</key><summary>Simplify IndicesFieldDataCache and detach from guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T08:49:48Z</created><updated>2016-02-05T13:24:24Z</updated><resolved>2016-02-05T13:24:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-05T13:14:46Z" id="180352362">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch picks wrong data type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16468</link><project id="" key="" /><description>I have added new boolean column into one of my tables. Now, I didnt reindexed the whole table/records because I dont use this column to search records, but we chose to reindex for future use maybe.

But elasticsearch index picks up this column with datatype as 'long'. Will you please shed some light why elasticsearch picks this wrong datatype ?

Some docs/pointers will be helpful. Very helpful. This happened to me twice.
</description><key id="131584797">16468</key><summary>Elasticsearch picks wrong data type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kavitakanojiya</reporter><labels /><created>2016-02-05T08:32:31Z</created><updated>2016-02-05T08:48:08Z</updated><resolved>2016-02-05T08:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-05T08:48:08Z" id="180254578">Please join us on discuss.elastic.co. We will provide better help there.

You probably did not define your mapping and your inject process sent to elasticsearch a number like `"foo": 1` instead of `"foo": true`.
As you seem to be importing from a database, you should give details about all what you are doing on discuss so we will have a better picture of what is happening.

Closing as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make IDEs use separate build dirs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16467</link><project id="" key="" /><description>We currently put all builds for eclipse and intellij under gradle's normal build dir. However, this means running something like `gradle clean` from the command line wipes the IDEs build out from under it, which can cause huge issues (eg eclipse dying a horrible death of compilation failures that it cannot recover from).

This change makes `build-idea` and `build-eclipse` directories for the respective IDEs. The `generated-resources` for plugins is also moved under the relevant dir, so that dir no longer lives at the root of each project.
</description><key id="131511903">16467</key><summary>Make IDEs use separate build dirs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-05T00:04:19Z</created><updated>2016-02-05T02:10:11Z</updated><resolved>2016-02-05T02:09:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-05T01:28:10Z" id="180141268">This is wonderful, worked very well for me on my machine, and addresses a pain point. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not publish modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16466</link><project id="" key="" /><description>Modules are an internal implementation detail of our build, and there is
no need to publish them with gradle. This change disables publishing of
all modules.
</description><key id="131474613">16466</key><summary>Do not publish modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T21:10:26Z</created><updated>2016-02-04T21:34:59Z</updated><resolved>2016-02-04T21:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-04T21:15:22Z" id="180054784">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for minimum_should_match, one term and multiple fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16465</link><project id="" key="" /><description>This adds a test case similar to the issue in #13884 which was fixed in #16155.
</description><key id="131473790">16465</key><summary>Add test for minimum_should_match, one term and multiple fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T21:06:43Z</created><updated>2016-02-05T12:16:51Z</updated><resolved>2016-02-05T12:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-05T11:42:34Z" id="180313501">@jimferenczi I added a unit test, could you take a look if this is good?
</comment><comment author="jimczi" created="2016-02-05T12:10:20Z" id="180322531">@cbuescher thanks, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>try / catch failing in groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16464</link><project id="" key="" /><description>I assume this is because of the new 'whitelist' in ES 2.2.0 but why would a try / catch not be allowed?  Here is the code we are using:

``` groovy
str = doc[field].value.toString();

try {
  if (element == "before") {
    sub = str.substring(0, str.indexOf(character)).trim()
  } else {
    sub = str.substring(str.indexOf(character) +1).trim()
  }
} catch (e) {
  sub = null;
}; 

return sub;
```

Here is the ES response:

``` json
{
  "took": 19,
  "timed_out": false,
  "_shards": {
    "total": 12,
    "successful": 5,
    "failed": 7,
    "failures": [
      {
        "shard": 0,
        "index": "&lt;index&gt;",
        "node": "WxYFZTjlT52MpvBRQPm_1Q",
        "reason": {
          "type": "no_class_def_found_error",
          "reason": "java/lang/Throwable",
          "caused_by": {
            "type": "class_not_found_exception",
            "reason": "java.lang.Throwable"
          }
        }
      }
    ]
  },
  "hits": {
    "total": 0,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "panel_filter": {
      "doc_count": 0,
      "group_list": {
        "doc_count_error_upper_bound": 0,
        "sum_other_doc_count": 0,
        "buckets": []
      }
    }
  }
}
```
</description><key id="131473479">16464</key><summary>try / catch failing in groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hulu1522</reporter><labels /><created>2016-02-04T21:05:14Z</created><updated>2016-02-06T22:16:38Z</updated><resolved>2016-02-04T21:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T21:27:39Z" id="180060144">This is covered in the [breaking changes for 2.2.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-2.2.html#_scripting_and_security); in particular, see [Scripting and the Java Security Manager](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/modules-scripting-security.html). You need to whitelist `java.lang.Throwable` and probably continue down the Throwable hierarchy until you reach whatever concrete class is being thrown here.
</comment><comment author="hulu1522" created="2016-02-04T23:07:09Z" id="180095701">I followed that and it doesn't work.  I put this file in the elasticsearch users home directory and root's just to be sure.  I have not yet put it in the JAVA_HOME location yet but this way should work.

Locations:
`/home/elastic/.java.policy`
`/root/.java.policy`

``` java
//
//
//     This File is Managed by Chef!
//
//  Any modifications will be overwritten.
//
//


grant {
  permission org.elasticsearch.script.ClassPermission "java.lang.Throwable";
};
```

Neither of them work.
</comment><comment author="clintongormley" created="2016-02-05T05:34:51Z" id="180207058">Your policy file needs to contain the following:

```
grant {
  permission org.elasticsearch.script.ClassPermission "java.lang.Throwable";
  permission org.elasticsearch.script.ClassPermission "java.lang.Exception";
  permission org.elasticsearch.script.ClassPermission "groovy.lang.GroovyException";
};
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How do master elections work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16463</link><project id="" key="" /><description>We should document this, cause why not!
At the moment you need to read [the code](https://github.com/elastic/elasticsearch/tree/master/core/src/main/java/org/elasticsearch/discovery/zen) to understand it.

Via [this thread](https://discuss.elastic.co/t/which-nodes-should-participate-in-election/40788/5) on discuss.
</description><key id="131467258">16463</key><summary>How do master elections work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-02-04T20:37:41Z</created><updated>2017-01-13T22:25:37Z</updated><resolved>2016-02-14T18:42:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T21:34:38Z" id="180061946">At a high-level, this is covered in the [Zen Discovery documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election) and at a low-level the source code is the best documentation. Do you see it differently @markwalkom?
</comment><comment author="markwalkom" created="2016-02-04T21:39:05Z" id="180063459">That the process of how nodes talk about the election, but we don't explain
how the "election" itself is done, ie which node they figure out to elect.

On 5 February 2016 at 08:35, Jason Tedor notifications@github.com wrote:

&gt; At a high-level, this is covered in the Zen Discovery documentation
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election
&gt; and at a low-level the source code is the best documentation. Do you see it
&gt; differently @markwalkom https://github.com/markwalkom?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16463#issuecomment-180061946
&gt; .
</comment><comment author="rjernst" created="2016-02-04T21:49:47Z" id="180066278">&gt; but we don't explain how the "election" itself is done, ie which node they figure out to elect.

That is an implementation detail? No user should be depending on that?
</comment><comment author="markwalkom" created="2016-02-05T00:08:38Z" id="180114315">Sure it doesn't impact userspace, but I don't see the harm in explaining how it works.
</comment><comment author="clintongormley" created="2016-02-14T00:04:00Z" id="183776181">@markwalkom PRs welcome ;)
</comment><comment author="jasontedor" created="2016-02-14T18:42:31Z" id="183948315">&gt; Sure it doesn't impact userspace, but I don't see the harm in explaining how it works.

If there is no benefit to userspace (as we all seem to agree?), but there is a real cost to writing and maintaining such docs, I'm not sure if I see the point.
</comment><comment author="patrickconant" created="2017-01-13T21:40:24Z" id="272555143">Thanks for the pointer to the code that performs leader election. It was useful to me in answering this question: Does leader election prefer master-only nodes to master-and-data nodes? 

I was wondering whether a cluster with 1 master-only node and several master-eligible data nodes would reliably elect the master-only node as their master. The answer from the code appears to be 'no'. 

It follows that having one or two master-only nodes is unwise. If you really want to push cluster-master responsibilities off of your data nodes, you ought to have three master-only nodes and make your data nodes ineligible to be master. </comment><comment author="jasontedor" created="2017-01-13T21:51:14Z" id="272557562">&gt; I was wondering whether a cluster with 1 master-only node and several master-eligible data nodes would reliably elect the master-only node as their master. The answer from the code appears to be 'no'.</comment><comment author="jasontedor" created="2017-01-13T21:52:02Z" id="272557744">&gt; I was wondering whether a cluster with 1 master-only node and several master-eligible data nodes would reliably elect the master-only node as their master. The answer from the code appears to be 'no'.

Correct.

&gt; It follows that having one or two master-only nodes is unwise. If you really want to push cluster-master responsibilities off of your data nodes, you ought to have three master-only nodes and make your data nodes ineligible to be master.

Yes, you got it, with one caveat: set `discovery.zen.minimum_master_nodes` to two.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce JDK 7 signatures only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16462</link><project id="" key="" /><description>This commit adds usage of the animal-sniffer-maven-plugin to ensure that
compiled artifacts are only using signatures from JDK 7. This addresses
an issue where many developers are using JDK 8 locally.
</description><key id="131461607">16462</key><summary>Enforce JDK 7 signatures only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-04T20:15:10Z</created><updated>2016-02-04T22:34:15Z</updated><resolved>2016-02-04T22:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T20:16:17Z" id="180029784">This produces messages like

```
[INFO] Checking unresolved references to org.codehaus.mojo.signature:java17:1.0
[ERROR] /Users/jason/src/elastic/elasticsearch-2.x/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java:493: Undefined reference: String String.join(CharSequence, CharSequence[])
[WARNING] Rule 0: org.codehaus.mojo.animal_sniffer.enforcer.CheckSignatureRule failed with message:
Signature errors found. Verify them and ignore them with the proper annotation if needed.
.
.
.
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (check-signatures) on project elasticsearch: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1]
```

and fails the build. I verified this in core, and lang-groovy just to ensure that it works across the codebase.
</comment><comment author="abeyad" created="2016-02-04T20:21:00Z" id="180032132">LGTM
</comment><comment author="rjernst" created="2016-02-04T20:23:22Z" id="180032946">Very cool. +1
</comment><comment author="rmuir" created="2016-02-04T20:24:32Z" id="180033366">+1, looks very simple. thanks for investigating this.
</comment><comment author="drewr" created="2016-02-04T20:24:34Z" id="180033380">:sweat_smile: thanks @jasontedor!
</comment><comment author="uschindler" created="2016-02-04T20:33:40Z" id="180037852">+1 in 2.x
</comment><comment author="jasontedor" created="2016-02-04T22:02:29Z" id="180071277">Integrated to 2.x via 48ea8bb9cb558d68c49ba0e45c481fa5df2ff11c and 2.2 via 802b5a35f38441ccb0c3050f9d9a9bb719262a67.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add reindex progress indicator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16461</link><project id="" key="" /><description>Adds a progress indicator for reindex and update_by_query requests that you
can fetch like so:

```
curl 'localhost:9200/_tasks/*/*byquery*?pretty&amp;detailed'
```

```
{
  "nodes" : {
    "r1A2WoRbTwKZ516z6NEs5A" : {
      "name" : "Tyrannus",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "attributes" : {
        "testattr" : "test",
        "portsfile" : "true"
      },
      "tasks" : [ {
        "node" : "r1A2WoRbTwKZ516z6NEs5A",
        "id" : 36619,
        "type" : "transport",
        "action" : "indices:data/write/update/byquery",
        "status" : {       &lt;---------------------------- Status is this
          "total" : 6154,
          "updated" : 3500,
          "created" : 0,
          "deleted" : 0,
          "batches" : 36,
          "version_conflicts" : 0,
          "noops" : 0,
          "failures" : [ ]
        },
        "description" : "update-by-query [test][test]"
      } ]
    }
  }
}
```

The progress is just (updated + created + deleted) / total
</description><key id="131445104">16461</key><summary>Add reindex progress indicator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>review</label></labels><created>2016-02-04T19:04:49Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-09T18:24:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-04T19:05:21Z" id="180004240">@bleskes and @imotov here is the proposal for reindex progress!
</comment><comment author="nik9000" created="2016-02-04T19:05:49Z" id="180004357">This supersedes https://github.com/elastic/elasticsearch/pull/16029
</comment><comment author="bleskes" created="2016-02-08T12:32:03Z" id="181348753">the output looks good to me (thanks @nik9000 !). The failure list did made me think about how we accumulate those - do we stop after a certain amount? reindex can potentially cause thousands of failures if we just go on. If we hold to all of them will go OOM.
</comment><comment author="nik9000" created="2016-02-08T13:08:19Z" id="181358711">&gt; The failure list did made me think about how we accumulate those - do we stop after a certain amount? reindex can potentially cause thousands of failures if we just go on. If we hold to all of them will go OOM.

Its just like the with the response - if you get a single the process will abort at the end of the current batch. It'll clear the scroll and just stop. Same deal here. Its pretty unlikely you'll get a failure in the status because you'd have to catch it while it was processing the bulk response which is pretty fast but the status shares logic/code/data with the response and the response needs it. I can certainly skip sending it for the status but I didn't think it was worth it.
</comment><comment author="nik9000" created="2016-02-08T14:41:06Z" id="181400638">Added some docs!
</comment><comment author="dakrone" created="2016-02-08T22:22:12Z" id="181595923">Left lots of little comments
</comment><comment author="nik9000" created="2016-02-08T22:24:09Z" id="181596918">&gt; Left lots of little comments

Thanks!
</comment><comment author="nik9000" created="2016-02-08T22:57:32Z" id="181610248">@dakrone converted took to a TimeValue.
</comment><comment author="nik9000" created="2016-02-08T23:23:06Z" id="181618599">@dakrone maybe all better now.
</comment><comment author="dakrone" created="2016-02-09T16:16:41Z" id="181936945">@nik9000 now I'm curious about the behavior of the `BulkByScrollTask`, it _looks_ like we only set the failures/searchfailures once, after the response, so do we need the AtomicReference at all?
</comment><comment author="dakrone" created="2016-02-09T16:19:32Z" id="181937874">Left some pretty minor comments, but other than that, LGTM
</comment><comment author="nik9000" created="2016-02-09T16:35:50Z" id="181946271">&gt; like we only set the failures/searchfailures once

Right. We could almost certainly get away with a volatile reference and _probably_ get away with a non-volatile reference. I think as soon as we get any errors we fail on the same thread so they'd always be visible.

I wanted to keep the implementation simple so I always reached for the java.util.concurrent things rather than leave long comments about why weaker visibility guarantees are ok.

I imagine I could remove the failures from the Task and Status altogether and have them be a Response only thing. If so the whole questions evaporates.
</comment><comment author="nik9000" created="2016-02-09T17:30:05Z" id="181971246">&gt; I imagine I could remove the failures from the Task and Status altogether and have them be a Response only thing. If so the whole questions evaporates.

Done.

One thing this brings up: when we have a search or bulk failure we still honor the `refresh` parameter. I don't see why we shouldn't honor it but its a funky thing.
</comment><comment author="dakrone" created="2016-02-09T17:49:37Z" id="181978726">LGTM
</comment><comment author="nik9000" created="2016-02-09T18:47:35Z" id="182000036">@dakrone do you think it'd be ok for me to squash these and force push to feature/reindex? I don't think anyone has that checked out....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>register signer of class S3Signer with S3SignerType name for ceph integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16460</link><project id="" key="" /><description>Register signer of class S3Signer with S3SignerType name for ceph integration.

It solves https://github.com/elastic/elasticsearch-cloud-aws/issues/255.

I have created this PR for master branch. I understand it should be cherry-picked also on 2.1.x branch and 2.2.x.
</description><key id="131440831">16460</key><summary>register signer of class S3Signer with S3SignerType name for ceph integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels /><created>2016-02-04T18:48:35Z</created><updated>2016-02-05T19:52:10Z</updated><resolved>2016-02-05T19:52:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-04T19:58:42Z" id="180023719">I don't think we should be hacking with internal sdk classes. From what I understand, ceph only supports v2 auth. The issue is with the default signer, which basically always tries to use v4 auth (this is the "AWSS3Signer").

There was an issue, aws/aws-sdk-java#372, about this last year, which is supposed to be fixed with 1.10+. The last comment in the issue suggests doing this hack should not be necessary. As you said, it seems to be timing out, not failing (which is what I would expect trying to use a v4 sig on a v2 endpoint would do).  So I think something else is going on? 
</comment><comment author="xuzha" created="2016-02-04T21:04:16Z" id="180048819">See here: https://github.com/aws/aws-sdk-java/blob/1.10.33/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L285, S3SignerType  it has been statically registered in 1.10.33). 
</comment><comment author="awislowski" created="2016-02-05T19:52:10Z" id="180529440">@xuzha your right. I close this PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running integration test with ES 2.2 throw ControlAccessException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16459</link><project id="" key="" /><description>It seems the security controls kick in on integration tests using ESIntegTestCase.

Relevant stack overflow thread: http://stackoverflow.com/questions/35205150/running-unit-tests-for-elasticsearch-2-2-0-in-intellij-fails-with-accesscontrole/35208827#35208827

Repro: 

```
@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
public class AndrejTest extends ESIntegTestCase {
    @Test
    public void shouldPrintHello() throws Exception {
        System.out.println("hello");
    }
}
```

```
18:34:04 WARN  bootstrap:48 - JNA not found. native methods will be disabled.
java.lang.ClassNotFoundException: com.sun.jna.Native
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.elasticsearch.bootstrap.Natives.&lt;clinit&gt;(Natives.java:45)
    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:89)
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:85)
    at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:99)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$1.run(RandomizedRunner.java:573)
18:34:04 WARN  bootstrap:65 - cannot check if running as root because JNA is not available
18:34:04 WARN  bootstrap:96 - cannot install syscall filters because JNA is not available
18:34:04 WARN  bootstrap:57 - cannot mlockall because JNA is not available
18:34:04 WARN  bootstrap:81 - cannot register console handler because JNA is not available
java.security.AccessControlException: access denied ("org.elasticsearch.ThreadPermission" "modifyArbitraryThreadGroup")

    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at org.elasticsearch.SecureSM.checkThreadGroupAccess(SecureSM.java:166)
    at org.elasticsearch.SecureSM.checkAccess(SecureSM.java:113)
    at java.lang.ThreadGroup.checkAccess(ThreadGroup.java:315)
    at java.lang.ThreadGroup.getParent(ThreadGroup.java:167)
    at com.carrotsearch.randomizedtesting.Threads.getTopThreadGroup(Threads.java:113)
    at com.carrotsearch.randomizedtesting.Threads.getAllThreads(Threads.java:97)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.&lt;init&gt;(ThreadLeakControl.java:346)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:654)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.access$200(RandomizedRunner.java:138)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$1.run(RandomizedRunner.java:579)

REPRODUCE WITH: mvn test -Pdev -Dtests.seed=15E6042541843D23 -Dtests.class=io.searchbox.client.AndrejTest -Dtests.locale=en_GB -Dtests.timezone=Europe/London
NOTE: test params are: codec=null, sim=null, locale=null, timezone=(null)
NOTE: Linux 4.2.0-27-generic amd64/Oracle Corporation 1.8.0_72 (64-bit)/cpus=4,threads=1,free=92285192,total=122683392
NOTE: All tests run in this JVM: [AndrejTest]
java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "setDefaultUncaughtExceptionHandler")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.Thread.setDefaultUncaughtExceptionHandler(Thread.java:1893)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:602)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.run(RandomizedRunner.java:444)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)

Process finished with exit code 254
```
</description><key id="131436765">16459</key><summary>Running integration test with ES 2.2 throw ControlAccessException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>build</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-04T18:35:44Z</created><updated>2016-03-09T10:38:26Z</updated><resolved>2016-02-08T19:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="centic9" created="2016-02-05T07:01:53Z" id="180228665">Workaround from the so-discussion: Set VM options "-Dtests.security.manager=false"
</comment><comment author="wallecnik" created="2016-02-08T09:25:29Z" id="181270137">I am not sure if I'm doing anything wrong, but running this empty test fails with the same exception:

```
public class MyTestES extends ESIntegTestCase { 
    @Test 
    public void test() {} 
}
```

I would appreciate the testing framework to be simple as extending the `ESIntegTestCase` class.
</comment><comment author="s1monw" created="2016-02-08T10:28:53Z" id="181296582">hmm this seems like a bug but I wonder how you integrated it into you project? Which version of randomized runner is referenced by your projects since we are granting this permission to `grant codeBase "${codebase.randomizedtesting-runner-2.2.0.jar}" {
`
</comment><comment author="s1monw" created="2016-02-08T10:29:08Z" id="181296622">also does it work if you run a build outside of your IDE?
</comment><comment author="centic9" created="2016-02-08T11:03:25Z" id="181308841">For me the only relevant dependencies are 

&gt; dependencies {
&gt;     testCompile 'org.elasticsearch:elasticsearch:2.2.0'
&gt;     testCompile 'org.elasticsearch:elasticsearch:2.2.0:tests'
&gt;     testCompile 'org.apache.lucene:lucene-test-framework:5.4.1'
&gt; }

randomizedtesting-runner is not specified explicitely, the transitive dependencies seem to pull in a difference version via the lucene-test-framework:

&gt; --- org.apache.lucene:lucene-test-framework:5.4.1
&gt;      +--- org.apache.lucene:lucene-codecs:5.4.1
&gt;      +--- org.apache.lucene:lucene-core:5.4.1
&gt;      +--- com.carrotsearch.randomizedtesting:junit4-ant:2.3.1
&gt;      +--- com.carrotsearch.randomizedtesting:randomizedtesting-runner:2.3.1

Version 5.4.1 of lucene-test-framework is listed at http://mvnrepository.com/artifact/org.elasticsearch/elasticsearch/2.2.0
</comment><comment author="centic9" created="2016-02-08T11:04:47Z" id="181309072">And for me it also happens outside the IDE when using Gradle.
</comment><comment author="wallecnik" created="2016-02-08T11:12:41Z" id="181312620">Same here, using Maven and Intellij. Dependencies are:

```
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
    &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;
    &lt;version&gt;5.4.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;2.2.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
    &lt;artifactId&gt;lucene-test-framework&lt;/artifactId&gt;
    &lt;version&gt;5.4.1&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;2.2.0&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
    &lt;type&gt;test-jar&lt;/type&gt;
&lt;/dependency&gt;
```

EDIT: Running it from IDE fails as well as running `mvn clean test`
I've created blank project with only these dependencies and it fails too with the same test class:

```
public class MyTestES extends ESIntegTestCase {
    @Test
    public void test() {}
}
```
</comment><comment author="rendel" created="2016-02-08T12:07:28Z" id="181338193">We are facing a similar issue. We managed to solve the `modifyArbitraryThreadGroup` and `setDefaultUncaughtExceptionHandler` by setting the `com.carrotsearch.randomizedtesting` dependency to version 2.2.0. However, we stumbled upon another issue about `java.io.FilePermission`. 
We are using ESIntegTestCase for our unit tests, and it looks like it does not have the proper file permissions for our test directories. The only way we found so far to be able to run unit tests is to specify our own policy at runtime using -Djava.security.policy=my_policy.

You can find more information in my post on discuss: https://discuss.elastic.co/t/permission-denied-error-in-plugin-for-es-2-2-0/40915/4?u=renaud1
</comment><comment author="s1monw" created="2016-02-08T12:50:36Z" id="181354175">ok so there are several problems which are unrelated. Adding the right randoimized runner version should fix the problem for @wallecnik - can you check if it does? It's a bug we will have to fix in 2.2.1
</comment><comment author="s1monw" created="2016-02-08T12:52:28Z" id="181354491">@rendel @wallecnik are you using our parent pom file? 
</comment><comment author="rendel" created="2016-02-08T13:16:38Z" id="181363437">@s1monw, after adding the elasticsearch parent pom:

```
  &lt;parent&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;parent&lt;/artifactId&gt;
    &lt;version&gt;2.2.0&lt;/version&gt;
  &lt;/parent&gt;
```

to our pom, we still have the same file permission error.
</comment><comment author="rmuir" created="2016-02-08T13:25:28Z" id="181365534">something isn't right with the maven build here i think. @dweiss gave me a simple repro.
</comment><comment author="dweiss" created="2016-02-08T13:56:00Z" id="181380515">Unfortunately I don't think this was related -- my problem was that the plugin used a custom assembly descriptor (Maven) which after upgrading to ES 2.2.0 didn't include the plugin's security policy, causing errors. A fix to include the security policy worked.
</comment><comment author="rendel" created="2016-02-08T14:35:02Z" id="181397817">I tried to reproduce the issue with a simple maven project, and I found that the cause was due to our custom `path.data` node setting (see below). Removing this custom setting fixed the `FilePermission` error.

```
  @Override
  protected Settings nodeSettings(int nodeOrdinal) {
    return settingsBuilder()
      .put("path.data", "./target/elasticsearch-test/data/")
      .put(super.nodeSettings(nodeOrdinal)).build();
  }
```
</comment><comment author="s1monw" created="2016-02-08T14:41:14Z" id="181400686">@rendel thanks for clarifying!!
</comment><comment author="wallecnik" created="2016-02-08T15:06:07Z" id="181416542">Adding the `randomizedtesting-runner:2.2.0` fixes the empty test as @s1monw asked. Thanks for that!
</comment><comment author="andrejserafim" created="2016-02-08T18:00:45Z" id="181500010">Adding the `randomizedtesting-runner:2.2.0` does indeed fix the basic test. Thank you for that. 

However I'm still getting some proxy related security exceptions. Could you give me a hand with these?

A repro:

```
@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
public class AndrejTest extends ESIntegTestCase {
    @Test
    public void shouldPrintHello() throws Exception {
        System.out.println("default proxy settings" + ProxySelector.getDefault());
    }
}
```

Exception: 

```
java.security.AccessControlException: access denied ("java.net.NetPermission" "getProxySelector")

    at __randomizedtesting.SeedInfo.seed([FB4E16B7BA293FA:4AD43B0D357967E9]:0)
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.net.ProxySelector.getDefault(ProxySelector.java:94)
    at AndrejTest.shouldPrintHello(AndrejTest.java:10)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="s1monw" created="2016-02-08T19:10:16Z" id="181524376">fixed by https://github.com/elastic/elasticsearch/commit/6fe88cee78c72fde021898eb79883c12edf07c3d
</comment><comment author="s1monw" created="2016-02-08T19:11:28Z" id="181524883">@andrejserafim you can't just use arbitrary things in the ES test framework. If you are testing a plugin you have to give it the right permissions first.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Manual script reload from disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16458</link><project id="" key="" /><description>Currently, scripts are scanned periodically for changes via a reload feature (https://www.elastic.co/guide/en/elasticsearch/reference/2.2/modules-scripting.html#_automatic_script_reloading).  For users who are concerned about security and turn off script reloading, it can be helpful to provide a way to trigger a manual reload of the script directory.  So that they can push a change and issue a manual reload themselves.
</description><key id="131436640">16458</key><summary>Manual script reload from disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Scripting</label><label>discuss</label><label>enhancement</label></labels><created>2016-02-04T18:35:21Z</created><updated>2016-03-02T20:30:38Z</updated><resolved>2016-03-02T20:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T13:48:57Z" id="190730061">Given that reload would have to be a protected API, how would this differ from using indexed scripts and making those APIs protected instead?
</comment><comment author="ppf2" created="2016-03-02T20:30:38Z" id="191415805">True, indexed scripts can be the alternate option here :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor Clean up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16457</link><project id="" key="" /><description>- Minor clean up of Writer constants.
- Fixed Writer imports.
- Removed synthetic attribute from the generated constructor and method.
- Added a safeguard for maximum script length.
</description><key id="131428273">16457</key><summary>Minor Clean up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T18:08:56Z</created><updated>2016-04-05T11:07:15Z</updated><resolved>2016-02-05T20:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-05T19:38:55Z" id="180517555">LGTM
</comment><comment author="jdconrad" created="2016-02-05T20:01:21Z" id="180535153">Thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add support for STS authentication using security_token</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16456</link><project id="" key="" /><description>The following change addresses https://github.com/elastic/elasticsearch/issues/16428 by adding support for using AWS STS authentication when registering a repository using the `PUT /_snapshot/REPOSITORY` endpoint. 

Because STS credentials expire, I did not see a need to also add support for having it be provided via `elasticsearch.yaml` configuration file. 

For more information about this form of authentication, see [explicit-credentials](http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html#credentials-explicit) and [BasicSessionCredentials](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/?com/amazonaws/auth/BasicSessionCredentials.html).

An additional [PR](https://github.com/elastic/elasticsearch/pull/16434) has already been submitted which was based on master but I was hoping to get this change available for 2.x as well.
</description><key id="131427640">16456</key><summary>add support for STS authentication using security_token</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jipperinbham</reporter><labels><label>:Plugin Cloud AWS</label><label>discuss</label><label>feedback_needed</label></labels><created>2016-02-04T18:06:18Z</created><updated>2016-06-24T09:30:54Z</updated><resolved>2016-06-24T09:30:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T17:14:15Z" id="206469944">@jipperinbham the PR on master was closed, I'm wondering what we should do? we certainly don't want a feature only on 2.x and not in master, do you know why you closed the one on master?
</comment><comment author="bleskes" created="2016-06-24T09:30:54Z" id="228301222">closing, no feedback in a long time.. Feel free to reopen if relevant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running Elasticsearch on Windows: expand service configuration doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16455</link><project id="" key="" /><description>Please add more detail around configuring necessary service settings on Windows. [Running as a Service on Windows](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service-win.html) is a great foundation, though it should be expanded to include:
- _specific_ steps for configuring minimum and maximum heap size
- any/all other service options documented in [Running as a Service on Linux](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html#setup-service) relevant to Windows. Mostly, how and where to configure environment variables.
- other Windows specific considerations

For most of these, it just means launching `bin/service manager` and adding configurations to the Java tab. 

The Linux doc is a good example of how the Windows version should be. Whether ES uses init.d or systemd, it is clear where and how to configure the service. 
</description><key id="131392085">16455</key><summary>Running Elasticsearch on Windows: expand service configuration doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">inqueue</reporter><labels><label>docs</label></labels><created>2016-02-04T16:00:23Z</created><updated>2016-04-07T22:01:42Z</updated><resolved>2016-04-07T22:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2016-02-04T16:00:53Z" id="179916367">+1 on this! 
</comment><comment author="gmarz" created="2016-02-10T19:39:42Z" id="182544424">+1 we'll jump on this asap

/cc @elastic/microsoft 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename bin/plugin in bin/elasticsearch-plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16454</link><project id="" key="" /><description /><key id="131382147">16454</key><summary>Rename bin/plugin in bin/elasticsearch-plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugins</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T15:28:29Z</created><updated>2016-02-08T09:55:29Z</updated><resolved>2016-02-05T10:25:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-04T17:16:15Z" id="179952879">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the inner structure of the plugins zip</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16453</link><project id="" key="" /><description>Pack all the plugin files into a single folder named `elasticsearch` at the root of the plugin zip.
If you use the gradle build, this structure is automatically generated:

&gt; &lt;plugin_name&gt;-&lt;version&gt;.zip
&gt; |____elasticsearch
&gt; | |____&lt;plugin_files&gt;, plugin-descriptor.properties
</description><key id="131382119">16453</key><summary>Change the inner structure of the plugins zip</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugins</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T15:28:20Z</created><updated>2016-02-13T18:25:02Z</updated><resolved>2016-02-10T09:14:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-04T15:32:58Z" id="179902145">I think you may need also to modify https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-authors.html ?
</comment><comment author="jimczi" created="2016-02-04T15:57:59Z" id="179915038">@dadoonet thanks, I've added a note.
</comment><comment author="jimczi" created="2016-02-09T11:39:22Z" id="181828119">@rjernst I've removed the inner dir in the plugin and the install command unzips only the content of the `elasticsearch` directory as suggested, can you check ?
</comment><comment author="rjernst" created="2016-02-09T18:28:46Z" id="181993129">Thanks @jimferenczi, I left some new comments.
</comment><comment author="jimczi" created="2016-02-09T20:11:52Z" id="182042475">Thanks for the review @rjernst, I've made the suggested changes 
</comment><comment author="rjernst" created="2016-02-09T20:18:57Z" id="182044907">I would use the term "named" instead of "titled" in the docs. Other than that, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DFS support from TermVector API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16452</link><project id="" key="" /><description>Retrieving distributed DF for TermVectors is beside it's esotheric justification
a very slow process and can cause serious load on the cluster. We also don't have nearly
enough testing for this stuff and given the complexity we should remove it rather than carrying it
around.
</description><key id="131380837">16452</key><summary>Remove DFS support from TermVector API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Term Vectors</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T15:22:43Z</created><updated>2016-02-04T15:47:55Z</updated><resolved>2016-02-04T15:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-04T15:29:49Z" id="179899669">+1, i dont think it makes sense to do this with the raw stats. Seems like it should only be done higher up, like the dfs option for scoring. That one is only once per query, this one seems possibly per-document, depending on how its being used: i think its too open ended.
</comment><comment author="s1monw" created="2016-02-04T15:31:09Z" id="179900738">&gt; That one is only once per query, this one seems possibly per-document, depending on how its being used: i think its too open ended.

we have that option on a search request! I agree
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining analyzer on index doesn't complain about unknown parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16451</link><project id="" key="" /><description>The following snippet does not produce an error:

```
post analyzertest1
  {
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "standard",
                    "abracadabra": "foo"
                }
            }
        }
    }
  }
```
</description><key id="131365326">16451</key><summary>Defining analyzer on index doesn't complain about unknown parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fsoikin</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2016-02-04T14:32:10Z</created><updated>2016-02-12T11:21:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fsoikin" created="2016-02-04T15:21:30Z" id="179896518">/cc: @markharwood 
</comment><comment author="javanna" created="2016-02-11T17:24:44Z" id="182966200">We have recently introduced settings validation in master, meaning that unknown settings are now rejected. It doesn't help this case though as validation doesn't go that deep. For the analysis section we define settings groups, like `index.analysis.filter`, `index.analysis.tokenizer` etc. and only known groups can be used. But within each group, we don't have validation. That would need to be implemented by each analyzer, tokenizer, token filter etc. and we have a lot of them. Maybe an alternative would be, given that all the settings are loaded in memory, that while reading settings each component removes what was read and if anything was left at the end those are unsupported settings and we should throw error. Not sure how feasible this is though. Marking for discussion.
</comment><comment author="javanna" created="2016-02-12T11:19:02Z" id="183283077">We discussed this as part of FixItFriday. Everybody agrees that we should fix this. The check should be made on the node that receives the request though (or on the master node), rather than failing on each shard while trying to create the index. That makes it a high hanging fruit, along with having to find a smart way to fail without requiring explicit validation on each single tokenizer, token filter etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent naming of "filter" vs. "filters" when defining analyzer on an index vs. specifying its parameters inline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16450</link><project id="" key="" /><description>In the snippet below, index `analyzertest1` has an analyzer defined with `filters`, and `analyzertest2` - with `filter`. The latter works, the former doesn't (see also #16451).

When I specify filters inline, the situation is reversed: `filters` works, `filter` doesn't.

```
post analyzertest1
  {
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "standard",
                    "filters": ["lowercase"]
                }
            }
        }
    }
  }

post analyzertest1/_analyze 
{
   "analyzer": "my_analyzer", "text": "ABCD"
}
// Result: "ABCD"

post analyzertest2
  {
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "standard",
                    "filter": ["lowercase", "stop"]
                }
            }
        }
    }
  }

post analyzertest2/_analyze 
{
   "analyzer": "my_analyzer", "text": "ABCD"
}
// Result: "abcd"

post _analyze 
{
   "tokenizer": "standard",
   "filter": ["lowercase"],
   "text": "ABCD" 
}
// Result: error "unknown parameter [filter]"

post _analyze 
{
   "tokenizer": "standard",
   "filters": ["lowercase"],
   "text": "ABCD" 
}
// Result: "abcd"
```
</description><key id="131364896">16450</key><summary>Inconsistent naming of "filter" vs. "filters" when defining analyzer on an index vs. specifying its parameters inline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fsoikin</reporter><labels /><created>2016-02-04T14:30:42Z</created><updated>2016-02-09T11:36:37Z</updated><resolved>2016-02-09T11:36:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fsoikin" created="2016-02-04T15:21:24Z" id="179896488">/cc: @markharwood 
</comment><comment author="javanna" created="2016-02-09T11:36:37Z" id="181827283">I think there are two problems here:

1) we don't validate (yet) analyzer settings, so non valid ones are accepted and stored in the cluster state. Same as #16451 .

2) analyze api and create index api are inconsistent, same as #15189.

I will close this issue as duplicate then as we already have specific issues for each of the corresponding problems. Thanks for opening this though ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when calling _analyze with nonexistent tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16449</link><project id="" key="" /><description>Request (in Sense format): 

```
POST _analyze
{ 
    "tokenizer": "abracadabra", 
    "text": "Whatevs" 
}
```

Elastic node never sends an HTTP response and prints this in console:

```
[2016-02-04 14:16:15,724][ERROR][transport                ] [Margaret Power] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@74015702]
java.lang.NullPointerException
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
        at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="131360844">16449</key><summary>NullPointerException when calling _analyze with nonexistent tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fsoikin</reporter><labels /><created>2016-02-04T14:18:08Z</created><updated>2016-02-04T14:23:17Z</updated><resolved>2016-02-04T14:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T14:23:17Z" id="179866828">Duplicates #15148, closed by #15447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove plugability of the gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16448</link><project id="" key="" /><description>In the spirit of simplification, remove the ability to have a custom gateway. We haven't seen any valid use for it so far.
</description><key id="131343666">16448</key><summary>Remove plugability of the gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T13:18:27Z</created><updated>2016-02-04T14:47:44Z</updated><resolved>2016-02-04T14:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-04T14:23:58Z" id="179867353">w00t LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dependency from IndexShard on TermVectorService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16447</link><project id="" key="" /><description>This dependency is just syntactic sugar and complicates IndexShard creation.
This commit move it out where it's actually used and needed.
</description><key id="131324670">16447</key><summary>Remove dependency from IndexShard on TermVectorService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T11:58:13Z</created><updated>2016-02-04T16:42:21Z</updated><resolved>2016-02-04T16:42:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-04T12:06:49Z" id="179792107">+1 to removing this dependency from IndexShard and just let TVService use a provided IndexShard.
</comment><comment author="bleskes" created="2016-02-04T12:24:32Z" id="179800564">Lgtm. Thanks Simon ..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `gateway.initial_meta` and always rely on min master nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16446</link><project id="" key="" /><description>During initial cluster forming, when  a master is elected, it reaches out to all other masters nodes and ask the last cluster state they persisted. To make sure we select the right state, we must successfully read from a `min_master_nodes` nodes. The gateway currently have specific settings to override this behavior, but I don't think they are ever used. We can drop them and reach out to the discovery layer, the single source of truth for the min master nodes settings.
</description><key id="131313510">16446</key><summary>Remove `gateway.initial_meta` and always rely on min master nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T11:03:11Z</created><updated>2016-02-13T23:39:44Z</updated><resolved>2016-02-04T12:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-04T11:03:22Z" id="179772358">@s1monw @beiske can you take a look?
</comment><comment author="s1monw" created="2016-02-04T11:37:06Z" id="179783414">LGTM thanks this is so much simpler
</comment><comment author="beiske" created="2016-02-04T11:47:18Z" id="179786211">LGTM
</comment><comment author="s1monw" created="2016-02-04T12:06:09Z" id="179791772">@bleskes does this mean we can drop the pluggability of Gateway entirely and make it final etc?
</comment><comment author="bleskes" created="2016-02-04T12:47:12Z" id="179813982">@s1monw I think I can fold into GatewayService. I'll give it a shot in another PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add settings filtering to node info requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16445</link><project id="" key="" /><description>Today we don't filter settings in NodeInfo so transport clients get unfiltered settings.
</description><key id="131293848">16445</key><summary>Add settings filtering to node info requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T09:49:20Z</created><updated>2016-02-04T10:26:04Z</updated><resolved>2016-02-04T10:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-04T10:05:17Z" id="179744500">changes looks good to me , but I think we need a test as well?
</comment><comment author="s1monw" created="2016-02-04T10:15:48Z" id="179749782">@bleskes pushed a test
</comment><comment author="bleskes" created="2016-02-04T10:17:36Z" id="179750170">++ . thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java heap space Error on document indexing with spatial field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16444</link><project id="" key="" /><description>Hi Elasticsearch team,

I have recently migrated from ES 1.7.x to ES 2.x. I came across a bug that seem to be related to the storage of spatial mappings. I have seen this behaviour on  ES 2.0.x and the latest 2.2.0. In 1.7.x this worked without any issues. I am running Ubuntu 15.10 64bit with ES 2.2.0 installed from the elastic PPA. This also occured on a Ubuntu 14.04 64bit installation.

I have attached the json files to recreate the issue. Here are the curl requests using the json files as the payload content.

`curl -XDELETE localhost:9200/test`

`curl -vX PUT localhost:9200/test -d @create-index_broken.json --header "Content-Type: application/json"`

`curl -vX PUT localhost:9200/test/entry/124 -d @put-record_working.json --header "Content-Type: application/json"`
(this takes about 6 seconds on my machine)

`curl -vX PUT localhost:9200/test/entry/123 -d @put-record_broken.json --header "Content-Type: application/json"`

The spatial extent is bigger than in the previous request. This takes some time, and eventually I get the following error:

``` json
{
    "error": {
        "root_cause": [
            {
                "type": "index_failed_engine_exception",
                "reason": "Index failed for [entry#123]",
                "shard": "0",
                "index": "test"
            }
        ],
        "type": "index_failed_engine_exception",
        "reason": "Index failed for [entry#123]",
        "shard": "0",
        "index": "test",
        "caused_by": {
            "type": "out_of_memory_error",
            "reason": "Java heap space"
        }
    },
    "status": 500
}
```

When I create the index with a larger precision (1000m), both records are inserted in acceptable time:

`curl -vX PUT localhost:9200/test -d @create-index_working.json --header "Content-Type: application/json"`

The json files are attached as a zip:
[spatial-index-bug.zip](https://github.com/elastic/elasticsearch/files/117142/spatial-index-bug.zip)
</description><key id="131288457">16444</key><summary>Java heap space Error on document indexing with spatial field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">matthesrieke</reporter><labels><label>:Geo</label><label>enhancement</label></labels><created>2016-02-04T09:25:45Z</created><updated>2016-02-13T20:24:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="matthesrieke" created="2016-02-04T09:29:44Z" id="179729987">`/var/log/elasticsearch/elasticsearch.log` does not provide too much information, still here are the entries starting with index creation:

[spatial-index-bug_log.txt](https://github.com/elastic/elasticsearch/files/117148/spatial-index-bug_log.txt)
</comment><comment author="dakrone" created="2016-02-04T22:41:36Z" id="180085943">@nknize maybe you could take a look at this?
</comment><comment author="nknize" created="2016-02-05T18:03:48Z" id="180472697">Hi @matthesrieke 

I'll try to reproduce the issue myself in a bit. In the meantime, if you have a chance try explicitly setting `distance_error_pct : 0.025`. 
</comment><comment author="matthesrieke" created="2016-02-09T08:19:41Z" id="181755372">hi @nknize 
thanks for investigating. indeed, setting `distance_error_pct` (even to a very small double, e.g. 0.0001) seems to resolve the memory error and improve performance!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CliTool: Cleanup and document Terminal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16443</link><project id="" key="" /><description>This change documents the Terminal abstraction that cli tools use, as
well as simplifies the api to be a minimal set of methods to interact
with a terminal.
</description><key id="131257117">16443</key><summary>CliTool: Cleanup and document Terminal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T06:24:47Z</created><updated>2016-02-08T09:51:48Z</updated><resolved>2016-02-04T10:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2016-02-04T08:05:11Z" id="179702012">left a small comment, OTT LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename index folder to index_uuid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16442</link><project id="" key="" /><description>Following up https://github.com/elastic/elasticsearch/pull/16217 , This PR uses `${data.paths}/nodes/{node.id}/indices/{index.uuid}` 
instead of `${data.paths}/nodes/{node.id}/indices/{index.name}` pattern to store index 
folder on disk.
This way we avoid collision between indices that are named the same (deleted and recreated).

Closes #13265
Closes #13264
Closes #14932
Closes #15853
Closes #14512
</description><key id="131246748">16442</key><summary>Rename index folder to index_uuid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Store</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T05:24:17Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-02-04T05:56:47Z" id="179660593">This is still a work in progress but would be good to have it reviewed.
IMO, we should rename pre-3.0 index folder when loading the global state for the first time in `GatewayMetaState`. This means once the indices are renamed they can't be read by 2.x. WDYT @bleskes @s1monw?
</comment><comment author="bleskes" created="2016-02-04T07:45:22Z" id="179694429">@areek  you work in amazing speeds :) 

I scanned through this and I think this is the right direction. My only feedback is that we want to have the index folder be "INDEXNAME-UUID", for example if the index name is `test` and the uuid is `ax5fd` the folder will be `test-ax5fd` . The goal here is to make something both human readable and unique.
</comment><comment author="bleskes" created="2016-02-04T08:07:25Z" id="179702523">&gt; we should rename pre-3.0 index folder when loading the global state for the first time in GatewayMetaState. 

Agreed - see 2.x's MultiDataPathUpgrader.upgradeMultiDataPath for an example (which I think you alread saw :) ). 

&gt; This means once the indices are renamed they can't be read by 2.x. 

Correct, but Simon pointed out that this is easy to reverse by renaming the folders back. I **do** think we should double check how the dangling index logic in 2.x will behave with these renamed folders (to 2.x, it's weird that a folder name is different than the what the index state file in it says).
</comment><comment author="s1monw" created="2016-02-04T15:30:14Z" id="179899976">`I scanned through this and I think this is the right direction. My only feedback is that we want to have the index folder be "INDEXNAME-UUID", for example if the index name is test and the uuid is ax5fd the folder will be test-ax5fd . The goal here is to make something both human readable and unique.`

maybe bikeshedding but should it be `test_ax5fd`?
</comment><comment author="dakrone" created="2016-02-04T15:30:46Z" id="179900407">&gt; My only feedback is that we want to have the index folder be "INDEXNAME-UUID", for example if the index name is test and the uuid is ax5fd the folder will be test-ax5fd . The goal here is to make something both human readable and unique.

Yes please, I was going to leave the same feedback, `${index-name}-${uuid}` would be great. Or with a `_` like Simon said.
</comment><comment author="bleskes" created="2016-02-04T15:31:47Z" id="179901203">&gt; maybe bikeshedding but should it be test_ax5fd?

sure :) 
</comment><comment author="areek" created="2016-02-05T03:39:02Z" id="180178340">Thanks for the feedback @bleskes, @s1monw and @dakrone! I have changed the index folder name to `${index-name}_${uuid}` as suggested.

Would like to get some thoughts on the state of `DanglingIndicesState` see https://github.com/elastic/elasticsearch/pull/16442#discussion_r51973523. 

&gt; Agreed - see 2.x's MultiDataPathUpgrader.upgradeMultiDataPath for an example (which I think you alread saw :) ).

Thanks for the pointer @bleskes. I will start on the bwc after we are happy with the rename.
</comment><comment author="clintongormley" created="2016-02-05T05:10:14Z" id="180201019">I thought we were going to deal with some of the issues around using null bytes, control chars, and non-ascii chars in index names with this PR as well? (eg the pathIdentifier could replace any chars in the name that don't match `/\w|\-/`

Happy to leave that for a separate PR though.
</comment><comment author="bleskes" created="2016-02-05T07:49:42Z" id="180240880">&gt; (eg the pathIdentifier could replace any chars in the name that don't match /\w|-/

seems like an easy one to add here... I don't see why not. @areek any objections? 
</comment><comment author="areek" created="2016-02-05T21:52:12Z" id="180579569">&gt;  I thought we were going to deal with some of the issues around using null bytes, control chars, and non-ascii chars in index names with this PR as well? 

Do you mean validating the index name upon index creation or replacing illegal characters on upgrade? We should do both, IMO tighten index name validation and rename pre-3x indices if they contain anything other then alphanumeric or hyphen characters in this pr. one problem is we might have collisions in index names for old indices, if we replace illegal characters on upgrade, index metadata is still keyed by index name rather than the index object. Thoughts?
</comment><comment author="bleskes" created="2016-02-06T10:06:50Z" id="180728756">&gt; Do you mean validating the index name upon index creation or replacing illegal characters on upgrade? 

I think the idea is to make sure that index names are not bound by the limitation of the file system. This will make us free to decided what we want to enforce from ES on the higher levels, without having to take this OS/FS specific limitations. Since we always add a unique identifier to the folder name, it's OK to just clean up the index name part and replace potential troublesome chars. I think a simple replace in the getPathIdentifier() utility method. Nothing more.
</comment><comment author="clintongormley" created="2016-02-29T13:01:44Z" id="190201024">Just a note: users will no longer be able to import dangling indices under a different name, just by renaming the index folder.  Not sure there is anything we can do about this short of providing a command line tool to rename the index.
</comment><comment author="bleskes" created="2016-02-29T13:12:28Z" id="190204999">&gt; users will no longer be able to import dangling indices under a different name, just by renaming the index folder

Correct. I think we're good with that. I believe that the current plan is to rename this folder to the right name on node start, which is how we plan to deal with BWC. Correct @areek ?

Another dangling indices limitation is that we may not be able to import a (Correctly named) folder because the cluster already has another index of the same name.  We should decide how to deal with it. Options are:
1) Ignore and leave the folder as is (log warnings though).
2) Wait a bit (2h I believe was an option before), log warnings and eventually delete.
3) Delete immediately as it is clear the index was replaced.

I'm inclined to go with (1) . Ideas?
</comment><comment author="clintongormley" created="2016-03-02T08:50:13Z" id="191132733">I'm good with option 1
</comment><comment author="areek" created="2016-03-03T03:40:32Z" id="191565807">Now we upgrade old indices on startup (rename index folder from `index.name` to `{name}_{index.uuid}`). 

&gt; Just a note: users will no longer be able to import dangling indices under a different name, just by renaming the index folder.

Right, but users can still import dangling indices with valid index folder names. Dangling indices are imported on the next cluster state update, ex: execute cluster reroute command. 

We ignore dangling indices that are:
- invalid (no index state)
- valid but have the same name as an existing index in the cluster
- valid but with an invalid folder name (we read the index state to log the right name)

Still a work in progress (mainly for tests) but reviews are appreciated :), core tests are passing, including bwc tests.
</comment><comment author="bleskes" created="2016-03-08T13:26:49Z" id="193783928">@areek I left some initial feedback. Thx.
</comment><comment author="uboness" created="2016-03-08T14:46:17Z" id="193810484">While we're at it, I'd vote for completely removing the index names from the FS dir structure. In many cases, having these names as part of the dir names is a security concern. Instead, just use UUIDs. We can still provide a cli tool that helps map dirs to indices based on metadata files (for "offline" data dir "browsing")
</comment><comment author="areek" created="2016-03-09T14:56:35Z" id="194332426">Updated PR: 
- Simplified bwc logic (rename top level index folder)
- use index.uuid as the index folder name

Thanks @bleskes for the feedback, I addressed all the comments
</comment><comment author="bleskes" created="2016-03-09T16:29:26Z" id="194379339">I think we can take an even simpler approach. Instead of retrieving all indices and then rename all folders of each index, we should rather work on the state file level. That means list all index state files and the go one by one and make sure their parent directory is what we want it to be. This approach will be more resilient to node crashes half way the rename. Currently if some of the folders are renamed and some are not the code will be confused.

Also - we should double check that shard replicas are well behaved (and add tests, including half way crashes and concurrent modifications :)).
</comment><comment author="areek" created="2016-03-11T05:35:33Z" id="195201447">Thanks @bleskes for the feedback! Updated the PR with the suggested approach (much simpler now). I added tests to ensure we behave as expected:
- when we crash while upgrading
- when multiple nodes try to rename folders concurrently in a shared FS (shadow replicas)

IMO, this is getting close, would appreciate a review :)

@dakrone would be awesome if you could take a look at the [test mocking upgrade on shared FS](https://github.com/areek/elasticsearch/commit/ad7df211cc595cc51a7e83234191744fc6e8e80b)
</comment><comment author="s1monw" created="2016-03-13T13:37:12Z" id="195958976">I love this change really nice! left some minor comments
</comment><comment author="bleskes" created="2016-03-14T10:48:15Z" id="196251895">Thx @areek . I made another round and it looks really good. Left some comments, most importantly around the the custom path case and validating state versions in advance.
</comment><comment author="areek" created="2016-03-14T21:16:09Z" id="196524747">Thanks @bleskes and @s1monw for the feedback, I addressed all the comments!
</comment><comment author="bleskes" created="2016-03-14T21:28:50Z" id="196528477">Thx @areek . Left two questions about hasCustomDataPath strictness and the dangling indices logic. O.w. looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backwards incompatibility: _cache is disallowed for Or Filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16441</link><project id="" key="" /><description>I'm using ElasticSearch v2.1.

The 2.0 [docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking_20_query_dsl_changes.html#_filter_auto_caching) state that the `_cache` field is deprecated and will be ignored, but not rejected, if present in any Filters. This holds true for some filters like And Filters and Terms Filters, but for some reason ElasticSearch rejects a search request if an Or Filter contains the `_cache` field.
</description><key id="131246564">16441</key><summary>Backwards incompatibility: _cache is disallowed for Or Filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MHova</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-02-04T05:22:20Z</created><updated>2017-04-26T11:23:44Z</updated><resolved>2017-04-26T11:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Tarotato" created="2017-03-13T21:14:33Z" id="286245888">Hi, I'm new to this project and wanted to contribute, is this still available to fix?</comment><comment author="colings86" created="2017-04-26T11:23:44Z" id="297366000">Or filters have been removed in 5.0 so I think this issue can be closed</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify the result interfaces from get and search in Java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16440</link><project id="" key="" /><description>We are using the Java client, and we have some custom code to inflate Java objects from the returned doucments. The get and search APIs seem to return completely different interfaces and object trees, even though they both contain the same underlying information. This is a problem for us because our inflation code is then duplicated which introduces possible bugs. Can the get and search APIs on the Java client be refactored so they use the same data structures for each hit returned?
</description><key id="131245338">16440</key><summary>Unify the result interfaces from get and search in Java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnagappan</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2016-02-04T05:08:07Z</created><updated>2017-06-29T23:24:03Z</updated><resolved>2017-06-29T09:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T23:33:55Z" id="183771331">could you give us some examples?
</comment><comment author="rnagappan" created="2016-02-14T00:14:19Z" id="183778880">This first example is from a Get query:

```
    GetResponse getResponse = new GetRequestBuilder(...).get();
    GetField getField = getResponse.getField("some field");
    Map&lt;String, GetField&gt; getFields = getResponse.getFields();
    Object value = getField.getValue();// not an interface method
    MyClass myGetObject = new MyClass(getFields) // custom deserialization code on GetField objects
```

And this second example is from a search query for the same document

```
    SearchResponse searchResponse = new SearchRequestBuilder(...).get();
    SearchHits hits = searchResponse.getHits();
    SearchHit searchHit = hits.getAt(0);
    SearchHitField searchHitField = searchHit.field("some field");
    Map&lt;String, SearchHitField&gt; searchHitFields = searchHit.getFields();
    Object value = searchHitField.getValue(); // not an interface method
    MyClass mySearchObject = new MyClass(searchHitFields) // custom deserialization code on SearchHitField objects
```

So both code snippets are finding the same document in ES and both are deserializing to the same java Object in the end. However, they have to go through completely different code paths to get from the same original data to the same end result. 

```
    // constructor 1
    for (Map.Entry&lt;String, GetField&gt; getField : getFields)
    {
        GetField field = getField.getValue();
        setSomeValue(field.getValue());
    }

   // constructor 2
    for (Map.Entry&lt;String, SearchHitField&gt; searchHitField : searchHitFields)
    {
        SearchHitField field = searchHitField.getValue();
        setSomeValue(field.getValue());
    }
```

This is really inconvenient, and prone to bugs, because I have to write almost identical two copies of that custom deserialization code - one to deal with SearchHit and SearchHitField, and the other to deal with GetResponse and GetField - and always remember to keep them in sync with each other. It would be far better if both of these ES class structures implemented a common interface structure so I can write one common deserization method that does not care whether the document given to it came from a Get request or Search request. My deserialization code should not have to be tied to the type of request that generated the data, or even aware of it.
</comment><comment author="javanna" created="2017-05-05T15:07:11Z" id="299490415">I agree that this is odd. `SearchHitField` and `GetField` hold the same members and could be unified.</comment><comment author="rnagappan" created="2017-06-29T23:24:03Z" id="312134242">Thanks for fixing this :)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Conditional update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16439</link><project id="" key="" /><description>Is there support for updating a document only if some condition is true? I don't mean versioning. For example if the document in ES is missing some field and I have a possibly newer version of that document with the field present can I tell ES only to update it if the one in the store is missing that field? 

Due to stringent operational speed requirements I really don't want to query for the existing document to compare it to.
</description><key id="131244533">16439</key><summary>Conditional update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnagappan</reporter><labels /><created>2016-02-04T04:59:45Z</created><updated>2016-02-14T00:48:50Z</updated><resolved>2016-02-04T05:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T05:08:38Z" id="179643300">&gt; Is there support for updating a document only if some condition is true?

Please see [partial updates to documents](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.htm) and the potential use of  scripting on updates.

&gt; Due to stringent operational speed requirements I really don't want to query for the existing document to compare it to.

Maybe I'm misunderstanding your requirements, but you can just get the document by ID rather than querying. That will be fast.

For future reference, questions like this are best asked on the [Elastic Discourse forums](https://discuss.elastic.co). GitHub is reserved for bug reports and feature requests.
</comment><comment author="rnagappan" created="2016-02-04T05:29:46Z" id="179648370">Thanks for the reply. I thought I was making a feature request!
</comment><comment author="clintongormley" created="2016-02-13T23:36:07Z" id="183771541">@rnagappan that's what scripted updates can be used for
</comment><comment author="rnagappan" created="2016-02-13T23:54:23Z" id="183773260">What I want to do is something like this

http://stackoverflow.com/questions/31599068/using-a-script-to-conditionally-update-a-document-in-elasticsearch

But checking for a field value rather than version. So my code would be something like:

`if (myField != null) do update else cancel operation`

I can see a similar example on the page provided above for delete, but is an equivalent test for update?
</comment><comment author="clintongormley" created="2016-02-14T00:18:36Z" id="183779326">set `ctx.op = "none"`
</comment><comment author="rnagappan" created="2016-02-14T00:48:50Z" id="183781430">Thanks. So I'm guessing that from this page

https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html

The query would be written as

```
POST /website/blog/1/_update
{
   "script" : "ctx.op = ctx._source.views == count ? 'update' : 'none'",
    "params" : {
        "count": 1
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert needs better version collision between new and existing docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16438</link><project id="" key="" /><description>We have the situation where different users can write to the same ES instance from different client applications so there is no central application layer where we can enforce write consistency - we're relying on ES as a distributed store to do that for us.

So consider two different users on two different clients that try to write the same document to ES using the same deterministic ID (e.g. the document's name). User A sets their document to version 1 because ES requires a minimum of version 1 to store, and then the upsert request happily indexes this as a new document. But then user B on a completely different client also creates what they think is a new document with the same ID and sets it's version to 1 also, and they submit an upsert request for it. ES happily accepts it as an update instead and overwrites the document that user A just submitted under the same ID. What should have happened in this case is a version collision instead. Upsert has no way to differentiate between a new document and the first update to an existing document, because they both have version 1.

It would be better for us if upsert could accept version 0 for a new document. One workaround is that on our client we can test for it, but then we are using create and update, not upsert.
</description><key id="131243166">16438</key><summary>Upsert needs better version collision between new and existing docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnagappan</reporter><labels /><created>2016-02-04T04:51:44Z</created><updated>2016-02-15T22:03:06Z</updated><resolved>2016-02-04T05:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T05:27:45Z" id="179646532">Maybe I'm misunderstanding, but it sounds like you want the semantics of create not upsert, and you want the second creation attempt to fail? That is, you want the loser of the race to fail. Elasticsearch provides this is with the [index API if you use `_create` on the request](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#operation-type).

For future reference, questions like this are best asked on the [Elastic Discourse forums](https://discuss.elastic.co). GitHub is reserved for bug reports and feature requests.
</comment><comment author="rnagappan" created="2016-02-04T05:35:24Z" id="179651735">Ok thanks. But upsert still has that weakness that when two different clients both submit version 1, the first one will eventually get their data overwritten by the second one. This seems like a failure of the version semantics to me.
</comment><comment author="jasontedor" created="2016-02-04T05:39:40Z" id="179652243">But you shouldn't use upsert for create semantics. And _after_ the document is created, you can use [optimistic concurrency control](https://www.elastic.co/guide/en/elasticsearch/guide/current/optimistic-concurrency-control.html) to ensure that the scenario that you describe can't happen. That is, if a user thinks they are creating a new document, use the index API with `_create` and the request will fail if the document already exists. And if the user is updating a document, use the index API with the expected version on the index request which will fail if that version is wrong (i.e., two users grab version 1 of the document, they both submit index requests with expected version 1, only one of the requests will win and the second will fail).
</comment><comment author="rnagappan" created="2016-02-04T05:53:04Z" id="179657518">Thanks, I am familiar with that page. I guess what I'm saying is that insert/create is safe for concurrent usage, and update is also safe for concurrent usage. But upsert is not always safe for concurrent usage because the "update" and "insert" parts of it can conflict with each other. I was hoping to raise this as a bug so that two clients could safely upsert at the same time, but you seem to saying that they can't - that they can only safely use the update part of the upsert at the same time. Maybe I am missing something here. Anyway, no matter, thanks for your help.
</comment><comment author="jasontedor" created="2016-02-04T06:18:30Z" id="179666272">&gt; But upsert is not always safe for concurrent usage because the "update" and "insert" parts of it can conflict with each other.

Can you you provide a minimal reproduction exhibiting what you think is a bug with a complete explanation of what you observe happening and what you think should be happening? What I _think_ you're saying should not be possible because of an internal versioning mechanism that's in place; a reproduction should help disambiguate the discussion. 
</comment><comment author="rnagappan" created="2016-02-04T21:35:13Z" id="180062098">If two users on two different clients both try to create a new doc with the same ID and version 1, one will fail, this is expected. If two users on two different clients both try to update an existing doc with the same ID and version 1, one will fail, this is expected. 

But if two users on two different clients both try to upsert a _new_ doc with the same ID and version 1, _both_ will succeed, but the second user's upsert will just overwrite the first user's upsert. Compared to the fail fast concurrency semantics of create and update above, this behaviour seems wrong to me. 

The problem is that version 1 is doing double duty as both the initial insert version and the first update version. In my opinion if you are going to offer a combined insert/update operation then version 1 should not have this dual conflicting duty.

I can get around this in my client code by writing "if new doc then call create API else call update API" but I thought that is what the upsert API is supposed to be!
</comment><comment author="jasontedor" created="2016-02-04T21:57:02Z" id="180069204">But can you provide a minimal reproduction exhibiting what you think is a bug? It will really help disambiguate this discussion. I _think_ that in master (after #13955), the situation that you're describing will produce a `VersionConflictException` but code will help clarify the discussion.
</comment><comment author="clintongormley" created="2016-02-14T00:02:06Z" id="183775129">@rnagappan Have you actually tried this?

If I set `?version=1` for a non-existent doc, I get a conflict failure:

```
POST t/t/1/_update?version=1
{
  "doc": {
    "foo": 2
  },
  "upsert": {
    "foo": 1
  }
}
```

Returns:

```
{
  "error": {
    "root_cause": [
      {
        "type": "version_conflict_engine_exception",
        "reason": "[t][1]: version conflict, current [-1], provided [1]",
        "shard": "3",
        "index": "t"
      }
    ],
    "type": "version_conflict_engine_exception",
    "reason": "[t][1]: version conflict, current [-1], provided [1]",
    "shard": "3",
    "index": "t"
  },
  "status": 409
}
```
</comment><comment author="rnagappan" created="2016-02-14T03:29:09Z" id="183813990">Apologies for my absence, I was snowed under with other work.

Yes, if I set version 1 for the insert part of an upsert query then I get a version conflict error.

Here is a test that I just wrote to illustrate the problem:

```
@org.junit.Test
public void testSomething() throws Exception
{
    Settings settings = Settings.builder().put("cluster.name", "minions").build();
    TransportClient client = TransportClient.builder()
                                            .settings(settings)
                                            .build()
                                            .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));
    try
    {
        // Part 1: the first client thinks they have a new document and they try to upsert it
        XContentBuilder source1 = XContentFactory.jsonBuilder()
                                                 .startObject()
                                                 .field("minionese", "banana!")
                                                 .endObject();
        IndexRequest indexRequest1 = new IndexRequest("minion-language", "sayings", "10")
            .version(-3)
            .source(source1);
        UpdateRequest updateRequest1 = new UpdateRequest("minion-language", "sayings", "10")
            .version(1)
            .doc(source1)
            .upsert(indexRequest1);
        client.update(updateRequest1).get();

        // Part 2: meanwhile, the second client also thinks they have a new document and they try to upsert it too
        XContentBuilder source2 = XContentFactory.jsonBuilder()
                                                 .startObject()
                                                 .field("minionese", "bee do bee do")
                                                 .endObject();
        IndexRequest indexRequest2 = new IndexRequest("minion-language", "sayings", "10")
            .version(-3)
            .source(source2);
        UpdateRequest updateRequest2 = new UpdateRequest("minion-language", "sayings", "10")
            .version(1)
            .doc(source2)
            .upsert(indexRequest2);
        client.update(updateRequest2).get();

        client.admin().indices().prepareRefresh("minion-language").get();
    }
    finally
    {
        client.close();
    }
}
```

When I run this test as-is both parts 1 and 2 succeed. So the resulting data in ES is:

```
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "minion-language",
        "_type": "sayings",
        "_id": "10",
        "_version": 2,
        "_score": 1,
        "_source": {
          "minionese": "bee do bee do"
        }
      }
    ]
  }
}
```

What I want is a pattern where part 1 succeeds but part 2 fails - because both clients think they are upserting a totally new document. But I want to be able to do this using the upsert API rather than have to switch between create and update manually. As it currently stands, when client 2 calls upsert on their data they are unwittingly overwriting client 1's data. This seems potentially dangerous to me.
</comment><comment author="clintongormley" created="2016-02-14T12:19:37Z" id="183880963">&gt; But if two users on two different clients both try to upsert a new doc with the same ID and version 1, both will succeed, but the second user's upsert will just overwrite the first user's upsert. Compared to the fail fast concurrency semantics of create and update above, this behaviour seems wrong to me.

I don't think this is what is happening at all.  The first request's upsert succeeds, then the second request's partial doc update succeeds, resulting in version 2 of the document.  

Note: setting the version on the `upsert` portion of the request is not possible via the REST api, so the `version` of the update request (ie `1`) is applied instead, which means that an upsert cannot be combined with a specified version.

Using versions for concurrency control only makes sense if you first GET the current document, then update it passing in the same version as was retrieved.  What you are doing is not the same.  As @jasontedor said you are looking for create semantics instead.
</comment><comment author="rnagappan" created="2016-02-15T01:19:41Z" id="184018665">&gt; Using versions for concurrency control only makes sense if you first GET the current document, then update it passing in the same version as was retrieved. What you are doing is not the same.

I can sort of see what you are saying, but if I were to depend on calling Get first, that makes it an update operation, not an upsert.
</comment><comment author="bleskes" created="2016-02-15T10:03:01Z" id="184141018">I think we can improve things here by making request validation stricter. IMO the following do not make sense and should be rejected:
- The version on an update request is a syntactic sugar for get of a specific version, doc merge and a version index. We should reject requests with both upsert and a version.
- If the upsert index request is versioned, we should reject the op. As Clint said, it's not possible through the rest layer, but Java clients can do so.
</comment><comment author="clintongormley" created="2016-02-15T13:19:19Z" id="184203088">I've opened https://github.com/elastic/elasticsearch/issues/16671 to get the above changed
</comment><comment author="rnagappan" created="2016-02-15T22:03:06Z" id="184413744">Excellent. More clarity is always better :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin cli can leak temp files in extreme cases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16437</link><project id="" key="" /><description>The plugin cli has handling for wiping the unzipped plugin directory on plugin installation. However, if there is an issue when downloading, checking the checksum, or unzipping, some temp files can be left over.
</description><key id="131237216">16437</key><summary>Plugin cli can leak temp files in extreme cases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2016-02-04T04:20:06Z</created><updated>2017-03-15T20:52:34Z</updated><resolved>2017-03-15T20:52:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nikoncode" created="2016-05-13T13:26:14Z" id="219042032">Possible solution is make list with urls of temporary files and remove all files when error thrown. 
</comment><comment author="rjernst" created="2017-03-15T20:52:34Z" id="286875370">This was inadvertently fixed as part of #22126.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More robust handling of CORS HTTP Access Control</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16436</link><project id="" key="" /><description>Uses a refactored version of Netty's CORS implementation to provide more
robust cross-origin resource request functionality. The CORS specific
Elasticsearch parameters remain the same, just the underlying
implementation has changed.

It has also been refactored in a way that allows dropping in Netty's
CORS handler as a replacement once Elasticsearch is upgraded to Netty 4.
</description><key id="131234565">16436</key><summary>More robust handling of CORS HTTP Access Control</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-02-04T04:05:59Z</created><updated>2016-04-19T18:06:43Z</updated><resolved>2016-02-04T15:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-02-04T04:07:54Z" id="179612447">@spinscale I had to make a few changes to use Java 7 only features for 2.x, so if you can review before merging.  All tests pass locally.
</comment><comment author="spinscale" created="2016-02-04T15:02:37Z" id="179885745">LGTM
</comment><comment author="clintongormley" created="2016-02-13T23:14:41Z" id="183770413">@abeyad this should go into master as well, no?
</comment><comment author="abeyad" created="2016-02-13T23:28:30Z" id="183770995">@clintongormley Its here: https://github.com/elastic/elasticsearch/pull/16092
</comment><comment author="clintongormley" created="2016-02-13T23:44:23Z" id="183772344">@abeyad ah thanks - i missed that
</comment><comment author="aleybovich" created="2016-04-04T18:27:19Z" id="205436196">Looks like it broke CORS support - after upgrading 2.2.2 -&gt; 2.3.1 I am getting this error:

XMLHttpRequest cannot load http://xxx.xxx.xxx.xxx:9200/_search. Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response.

I have the same http settings in elasticsearch.yml as 2.2.2:

```
http.cors.enabled: true
http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE
http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type,Content-Length
http.cors.allow-origin: "*"
```

Any changes in settings required for 2.3?
</comment><comment author="abeyad" created="2016-04-04T20:02:32Z" id="205470781">@aleybovich Just to be clear, you sent an `OPTIONS` http request with the `Origin` and `Access-Control-Request-Method` set in the http headers?  

And what was the error response / message exactly?
</comment><comment author="aleybovich" created="2016-04-04T23:13:42Z" id="205536795">I use SearchKit UI running locally; ES is also running locally. When I run ES2.3 and searchkit does a search, I see the following in dev console:
The request type is OPTOINS (instead of POST when ES 2.2 is running);

Request URL:http://localhost:9200/_search
Request Method:OPTIONS
Status Code:200 OK
Remote Address:127.0.0.1:9200
Response Headers
view source
Access-Control-Allow-Methods:
Access-Control-Allow-Origin:*
Access-Control-Max-Age:1728000
content-length:0
date:Mon, 04 Apr 2016 23:04:50 GMT
Request Headers
view source
Accept:_/_
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8
Access-Control-Request-Headers:accept, content-type
Access-Control-Request-Method:POST
Cache-Control:max-age=0
Connection:keep-alive
Host:localhost:9200
Origin:http://localhost:8080
Referer:http://localhost:8080/
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.110 Safari/537.36

 Request return 200 but in JS console I see:

_XMLHttpRequest cannot load http://xxx.xxx.xxx.xxx:9200/_search. Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response_
</comment><comment author="abeyad" created="2016-04-05T03:13:27Z" id="205621577">@aleybovich Thank you for finding this issue!  It is indeed a legitimate issue and I created a fix for it at #17524 (also created a fix for master).
</comment><comment author="aleybovich" created="2016-04-05T09:26:50Z" id="205728676">Thank you for addressing it! Do you know when a build with this fix would
be released?

On Tuesday, April 5, 2016, Ali Beyad notifications@github.com wrote:

&gt; @aleybovich https://github.com/aleybovich Thank you for finding this
&gt; issue! It is indeed a legitimate issue and I created a fix for it at
&gt; #17524 https://github.com/elastic/elasticsearch/pull/17524 (also
&gt; created a fix for master).
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16436#issuecomment-205621577
</comment><comment author="marcoplaut" created="2016-04-11T11:13:47Z" id="208291265">are there any work around to avoid the problem currently ?
</comment><comment author="aleybovich" created="2016-04-11T12:32:02Z" id="208317535">We have to roll back to 2.2.2 until the CORS issue is fixed.
On Apr 11, 2016 7:15 AM, "marcoplaut" notifications@github.com wrote:

&gt; are there any work around to avoid the problem currently ?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16436#issuecomment-208291265
</comment><comment author="marcoplaut" created="2016-04-11T12:40:49Z" id="208320677">if that could help someone, having nginx as reverse proxy in front of my elasticsearch, i solve the problem following this post: http://enable-cors.org/server_nginx.html
</comment><comment author="abeyad" created="2016-04-11T15:07:36Z" id="208391844">If you depend on CORS pre-flight requests working correctly, @aleybovich 's suggestion is correct to roll back until 2.3.2 is released.
</comment><comment author="hackel" created="2016-04-19T17:42:16Z" id="212036853">I just got hit by this as well, and wasted quite a bit of time debugging before I landed here.  This is a really critical bug.  Would be great if you added a big, fat warning about this issue at least to the release notes, if not to the homepage of elastic.io.  This basically disables remote elasticsearch functionality entirely.
</comment><comment author="clintongormley" created="2016-04-19T17:47:48Z" id="212038496">&gt; Would be great if you added a big, fat warning about this issue at least to the release notes, 

@hackel, we did: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-2.3.html
</comment><comment author="hackel" created="2016-04-19T17:52:58Z" id="212039983">Aha, thanks I missed that.  I was looking at https://www.elastic.co/guide/en/elasticsearch/reference/current/release-notes-2.3.0.html  I didn't realise there was a separate page for (more?) breaking changes.  My bad, I guess.
</comment><comment author="clintongormley" created="2016-04-19T18:06:43Z" id="212048377">@hackel i've added a link to the relevant breaking changes section from each page of release notes: https://github.com/elastic/elasticsearch/commit/f9a64eaa945201950d888a91c14f4961b2ad658c

Should help :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce builds are against JDK 7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16435</link><project id="" key="" /><description>This commit addresses an issue where builds against JDK 8 will succeed
but the artifacts produced will fail when run against JDK 7 (the minimum
required for the 2.x line). While the language level was set to Java 7,
the required Java version was only bounded below by 1.7 (but not bounded
above) and thus methods and classes from JDK 8 could slip in but go
undetected.
</description><key id="131218963">16435</key><summary>Enforce builds are against JDK 7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-04T02:25:01Z</created><updated>2016-02-13T23:26:22Z</updated><resolved>2016-02-04T02:48:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-04T02:26:41Z" id="179580517">LGTM
</comment><comment author="jasontedor" created="2016-02-04T02:48:12Z" id="179588812">Closed by fffc4ba71a0d60b7f467ec015cd414f5c0d3d5db and backported to 2.2 in 2ecfda5f9505732c37f74bcfde4007a6e6f1c6d7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add support for STS authentication using security_token</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16434</link><project id="" key="" /><description>The following change addresses https://github.com/elastic/elasticsearch/issues/16428 by adding support for using AWS STS authentication when registering a repository using the `PUT /_snapshot/REPOSITORY` endpoint. 

Because STS credentials expire, I did not see a need to also add support for having it be provided via `elasticsearch.yaml` configuration file. 

For more information about this form of authentication, see [explicit-credentials](http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html#credentials-explicit) and [BasicSessionCredentials](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/?com/amazonaws/auth/BasicSessionCredentials.html).
</description><key id="131213796">16434</key><summary>add support for STS authentication using security_token</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jipperinbham</reporter><labels /><created>2016-02-04T01:52:24Z</created><updated>2016-02-04T18:20:46Z</updated><resolved>2016-02-04T18:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add local cardinality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16433</link><project id="" key="" /><description>if the distinct value is already the routing one, just merge the HLL's results instead of the objects.
</description><key id="131212357">16433</key><summary>add local cardinality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yynil</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>stalled</label></labels><created>2016-02-04T01:43:13Z</created><updated>2017-04-07T23:11:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yynil" created="2016-02-04T01:59:57Z" id="179572853">For a distinct count metric aggregation, if the distinct value is already the routing one, user_id for an example, the merge node doesn't need to merge all HLL objects. The merge node just needs to merge the result(just a long value) of all distributed HLL objects in all shard nodes. It saves the network and cpu in both the shard nodes and the merge node.
Though it's rare in a lot of distinct aggregation that the distinct value equals the routing value, it's useful for those who already do the tricks. It's especially useful in calculating the unique users in web analytics. 
</comment><comment author="jpountz" created="2016-02-08T14:54:00Z" id="181410527">Things are indeed much easier if the field values are unique per shard. We would probably not even need HLL and could just collect seen ordinals in a bit set and count the number of set bits.

I will put this PR on hold as there is a large ongoing refactoring of aggregations (#14136) but tend to agree it would be nice to do something.
</comment><comment author="dakrone" created="2016-04-06T17:12:28Z" id="206469471">@jpountz now that the agg refactoring is merged is this something we should revisit?
</comment><comment author="jpountz" created="2016-04-08T08:45:23Z" id="207325795">Yes.
</comment><comment author="yynil" created="2016-04-12T03:09:13Z" id="208684225">In case this implementation, I find there is a bug in it. I will submit another code change to this repository to fix this bug.  
</comment><comment author="yynil" created="2016-04-18T08:43:10Z" id="211273543">Hi,@jpountz  I just checked the fixes I've made recently. The fixes include : 1. fix the status missing after reducing. 2. make it compilable in the upstream/master.
However I still have a serious situation that's test. The previous test cases I wrote can't be compiled and the new environment is quite a myth for me. I hope I could understand the system more quickly, but without any instructions, the improvements are very little. Would you please show me some full examples that I can write the cardinality test case? Thanks a lot!
</comment><comment author="yynil" created="2016-04-18T08:44:22Z" id="211274363">@jpountz Also what I wanna do is the exact cardinality using local bitmap based on the local cardinality function. But I need to finish the test case first, then I can continue to develop more codes.
</comment><comment author="jpountz" created="2016-04-19T16:29:41Z" id="212005815">&gt; However I still have a serious situation that's test. The previous test cases I wrote can't be compiled and the new environment is quite a myth for me.

What exact problem are you having? Maybe you can look at how other aggregations work?

&gt; Also what I wanna do is the exact cardinality using local bitmap based on the local cardinality function.

+1. I think we could do it automatically when there is a single queried shard (SearchContext.numberOfShards() == 1) and the memory usage of the bitmap would be lower than the HyperLogLog structure.
</comment><comment author="yynil" created="2016-04-20T08:11:25Z" id="212317344">Actually the problem is that I'm not very familiar with JUnit. So I want to find some already implemented aggregation test cases and copy&amp;modify them. Unfortunately my lack of JUnit knowledge didn't find out which aggregation test case in the mainstream repository is complete. Never mind I will solve the problem anyway.
For the bitmap function, I wanna to create a new optional parameter in the cardinality  aggregation since we have various situations to use Elastic Search. In some cases memory does matter and in other cases memory doesn't. I think it will be better to leave this decision to the actual users.  
</comment><comment author="jpountz" created="2016-04-20T08:56:18Z" id="212337238">I would suggest that we deal with the bitmap optimization in a different PR and focus on transporting the shard cardinality rather than the HyperLogLog structure in this one. Does it work for you?

&gt; For the bitmap function, I wanna to create a new optional parameter in the cardinality aggregation since we have various situations to use Elastic Search.

I do not want to add too many parameters to this aggregation. In my opinion, there should be a single parameter that tells elasticsearch that it can assume that shards have disjoint values. This will mean that it can transport the cardinality using a long rather than the HyperLogLog structure. And also that it can use a bitmap locally if the memory usage of the bitmap would be less than the hyperloglog for the configured `precision_threshold`. If someone wants to use a bitmap all the time, (s)he would just have to pass a high value for `precision_threshold.

By the way, maybe we should rename `isSumDirectly` to `disjoint_shard_values`. I think this is more descriptive of what this optimization is about?
</comment><comment author="yynil" created="2016-04-22T06:14:34Z" id="213277637">@jpountz you suggestions are good to me, I'll change the configuration and make only one parameter exist, then the precision can help to decide if bitmap is used. 
For the JUnit test, I'm a little bit confused about the tests in the aggregation packages. For an example, the average test, there are two files AvgIT and AvgTests. Neither one is testable. From my understanding, AvgTests should do the real job, am I right?
I also wanna know the schedule of 5.0.0. Since I'm kinda busy for now, I'm afraid I can't catch up your guys pace. 
</comment><comment author="jpountz" created="2016-04-22T07:43:47Z" id="213307394">&gt; AvgIT and AvgTests

AvgTests tests things like serialization and parameter validation. AvgIT starts an elasticsearch cluster, runs avg aggregations on it and checks the response (`IT` stands for Integration Test). What do you mean by "neither one is testable"?

&gt; I also wanna know the schedule of 5.0.0. Since I'm kinda busy for now, I'm afraid I can't catch up your guys pace. 

We are working towards releasing 5.0, but there is no date set in stone.  The only thing I can tell for sure that it will still take at least a couple more weeks since we only released an alpha version so far. For the record, there would be no issue getting this feature in 5.1 or 5.2.
</comment><comment author="yynil" created="2016-04-22T09:44:16Z" id="213359793">&gt; "neither one is testable"?

I use "gradle test --tests" parameter, when I input _Avg_, no tests run. But when I run _JsonVsCborTests_, it tests and run successfully. I'll do deeper dig to understand it better.
</comment><comment author="rjernst" created="2016-04-22T21:38:32Z" id="213596242">&gt; I use "gradle test --tests" parameter, when I input Avg, no tests run

The `--tests` parameter doesn't work because we dont use the gradle test runner. See `TESTING.asciidoc` at the root of the project for help on how to run individual tests.
</comment><comment author="dakrone" created="2016-09-12T21:25:51Z" id="246499189">@jpountz can this be marked as un-stalled now that the aggregation refactor is done, or is there still interest in this PR?
</comment><comment author="dakrone" created="2017-04-07T23:11:57Z" id="292672999">@jpountz ping again about this</comment><comment author="elasticmachine" created="2017-04-07T23:11:59Z" id="292673001">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add foreach processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16432</link><project id="" key="" /><description>This processor is useful when all elements of a json array need to be processed in the same way.

This avoids that a processor needs to be defined for each element in an array. Also it is very likely that it is unknown how many elements are inside an json array, which makes it impossible to even try that.
</description><key id="131194796">16432</key><summary>Add foreach processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-04T00:00:22Z</created><updated>2016-02-04T22:45:08Z</updated><resolved>2016-02-04T22:45:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-02-04T17:14:07Z" id="179952146">Looks sweet!

should we add one test (either in `80_foreach.yaml` or `ForeachProcessorTests`) for when one of the processors to be applied throws an exception?
</comment><comment author="talevy" created="2016-02-04T17:17:57Z" id="179953858">OTT LGTM
</comment><comment author="martijnvg" created="2016-02-04T21:32:48Z" id="180061459">@talevy I've updated the PR. The `foreach` processor in case of failure will now leave the array field unmodified.
</comment><comment author="talevy" created="2016-02-04T22:16:39Z" id="180075787">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Scala based elasticsearch-client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16431</link><project id="" key="" /><description>Adds a link to our new Elasticsearch scala client that uses a DSL against the REST endpoint
</description><key id="131192429">16431</key><summary>Add Scala based elasticsearch-client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rcoh</reporter><labels><label>docs</label></labels><created>2016-02-03T23:50:11Z</created><updated>2016-02-14T00:05:51Z</updated><resolved>2016-02-14T00:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-05T01:43:20Z" id="180147919">Making a comment causes the CLA-checker to recheck.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More robust handling of CORS HTTP Access Control</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16430</link><project id="" key="" /><description>Uses a refactored version of Netty's CORS implementation to provide more
robust cross-origin resource request functionality.  The CORS specific
Elasticsearch parameters remain the same, just the underlying
implementation has changed.

It has also been refactored in a way that allows dropping in Netty's
CORS handler as a replacement once Elasticsearch is upgraded to Netty 4.
</description><key id="131187410">16430</key><summary>More robust handling of CORS HTTP Access Control</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>enhancement</label></labels><created>2016-02-03T23:22:37Z</created><updated>2016-02-04T15:08:22Z</updated><resolved>2016-02-03T23:22:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Kibana : How to create interactive searchbar in dashboard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16429</link><project id="" key="" /><description>I want to create an interactive search bar in Kibana. Lets say i have a field that has 5 values

```
Field_A
a
b
c
d
e
```

is it possible to have a searchbar in a Kibana dashboard that allows a user to type in a query like

```
Field_A="a"
```

and then all of the records for the query results show up

It would be even better if i can have a dropdown list that gives me all of the values in the Field_A, possibly sorted, and then the user can select a particular value and see all the records that have that value.

I know this is possible in splunk.
</description><key id="131185603">16429</key><summary>Kibana : How to create interactive searchbar in dashboard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2016-02-03T23:13:48Z</created><updated>2016-02-03T23:25:11Z</updated><resolved>2016-02-03T23:24:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T23:24:16Z" id="179526959">Sorry if I'm missing something here, but this sounds like it would get better attention on the [Kibana repository](https://github.com/elastic/kibana) and maybe even better attention on [Elastic Discourse forums](https://disucss.elastic.co) since it's more of a question than a bug report or a feature request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for AWS STS credentials for snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16428</link><project id="" key="" /><description>The [current setup](https://github.com/elastic/elasticsearch/blob/b24dde88de8ea4fd7d62e4518e36c99a84abb2e7/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L155-L168) does not support the ability to provide an `AWS_SECURITY_TOKEN` to be used with AWS STS authentication.

I believe the changes necessary would be to add an additional field to `Settings` for the cloud-aws plugin `CLOUD_S3.TOKEN` and if set, then return an instance of `BasicSessionCredentials(accessKey, secretKey, sessionToken);` instead of `new BasicAWSCredentials(account, key)`.
</description><key id="131175469">16428</key><summary>Add support for AWS STS credentials for snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jipperinbham</reporter><labels><label>:Plugin Repository S3</label><label>feedback_needed</label></labels><created>2016-02-03T22:33:16Z</created><updated>2016-09-13T21:26:53Z</updated><resolved>2016-07-21T12:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-06-24T09:36:07Z" id="228302348">@dadoonet gracefully agreed to research and see what the implications of this are
</comment><comment author="dadoonet" created="2016-06-24T09:46:18Z" id="228304240">Useful links to understand STS:
- http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html
- http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html

If I'm not mistaken it means that we want to give a temporary credential to a S3 repository, right?

If so, I think it makes sense to support it only at a repository level but not in cluster/node settings. So having a temporary value for a setting like `cloud.aws.token` does not make sense.

If we want to implement it, I'd support it only when we create a repo. Like:

```
PUT _snapshot/tmp_repo
{
  "type": "s3",
  "settings": {
    "token": "temp-token-here"
  }
}
```

@jipperinbham WDYT?
</comment><comment author="dadoonet" created="2016-07-21T12:05:13Z" id="234234259">No news on this. So I'm closing for now.
Feel free to comment and reopen.
</comment><comment author="elliott-davis" created="2016-09-13T21:26:53Z" id="246830188">incase anyone else runs across this - It looks like it was resolved in #19556
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin permission errors in 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16427</link><project id="" key="" /><description>I am trying to write a plugin for ES 2.2.0. When I start elasticsearch with plugin installed, this is what I get -

Likely root cause: java.security.AccessControlException: access denied ("javax.management.MBeanServerPermission" "createMBeanServer")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
at java.security.AccessController.checkPermission(AccessController.java:884)
at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
at java.lang.management.ManagementFactory.getPlatformMBeanServer(ManagementFactory.java:465)
at com.yammer.metrics.reporting.JmxReporter.(JmxReporter.java:388)
at com.yammer.metrics.reporting.JmxReporter.startDefault(JmxReporter.java:340)
at com.yammer.metrics.Metrics.(Metrics.java:20)

This is my plugin-security.policy file -

grant {
permission javax.management.MBeanServerPermission "createMBeanServer";
};

This supposed to work, right? 
</description><key id="131174965">16427</key><summary>plugin permission errors in 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkelkarbv</reporter><labels /><created>2016-02-03T22:30:22Z</created><updated>2016-03-16T11:49:26Z</updated><resolved>2016-02-03T22:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-03T22:54:16Z" id="179516365">There is more to do than simply adding the permission. You must wrap the code trying to use the extra permissions with a `AccessController.doPrivileged` block. See https://www.elastic.co/guide/en/elasticsearch/plugins/2.2/plugin-authors.html#_java_security_permissions.
</comment><comment author="rendel" created="2016-02-05T11:55:37Z" id="180319244">Hi @rjernst 

We are facing a similar issue, but with the elasticsearch test framework. We are using `ESIntegTestCase` for our unit tests, and it looks like it does not have the proper file permissions for our test directories. The only way we found so far to be able to run unit tests is to specify our own policy at runtime using `-Djava.security.policy=my_policy`.
You can find more information in my post on discuss: https://discuss.elastic.co/t/permission-denied-error-in-plugin-for-es-2-2-0/40915/4?u=renaud1
</comment><comment author="mrgambal" created="2016-03-16T11:49:26Z" id="197280392">@rjernst this issue is a duplicate for #16459 and it has been already fixed in v2.2.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update network.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16426</link><project id="" key="" /><description>Added a note explaining that `network.host` can accept an array of values
</description><key id="131162094">16426</key><summary>Update network.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuespetre</reporter><labels><label>docs</label></labels><created>2016-02-03T21:36:55Z</created><updated>2016-02-13T23:04:59Z</updated><resolved>2016-02-13T23:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T23:04:59Z" id="183769974">thanks @tuespetre 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rewrite SettingsFilter to be immutable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16425</link><project id="" key="" /><description>This change rewrites the entire settings filtering mechanism to be immutable.
All filters must be registered up-front in the SettingsModule. Filters that are comma-sparated are
not allowed anymore and check on registration.
This commit also adds settings filtering to the default settings recently added to ensure we don't render
filtered settings.
</description><key id="131124900">16425</key><summary>Rewrite SettingsFilter to be immutable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T19:14:51Z</created><updated>2016-02-04T09:15:49Z</updated><resolved>2016-02-04T09:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-03T19:43:23Z" id="179425414">This is great! One less reason for a plugin to need to create a Module. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Spelling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16424</link><project id="" key="" /><description>Corrected spelling, nothing to special.
</description><key id="131113527">16424</key><summary>Spelling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">evanfreed</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T18:32:16Z</created><updated>2016-02-26T18:41:38Z</updated><resolved>2016-02-26T18:38:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T22:58:55Z" id="183769552">Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="evanfreed" created="2016-02-19T17:51:10Z" id="186329271">Done.
</comment><comment author="nik9000" created="2016-02-26T18:39:01Z" id="189417925">OK! Merged to 2.2 with: 727f77d6bd39cbe01423d556a2e338348b5f897d

I'm going to cherry pick it to a few useful branches.
</comment><comment author="nik9000" created="2016-02-26T18:40:51Z" id="189418538">master: 7ed30a9c00cc1d302efc75fa98ba76ebe8260d97
2.x: 64e49b85901e63a8d92ed2ab8f1a6228aa3e234f
2.1: 16ea47b109ab3405eba2e63b1c97217b7d158ddb
2.0: ae2f0787e51f01de6adec382680fb12851396705
</comment><comment author="nik9000" created="2016-02-26T18:41:18Z" id="189418649">All done! Thanks @evanfreed !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the es.foreground property after it is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16423</link><project id="" key="" /><description>The strict Settings check was failing if Elasticsearch was being backgrounded
because the backgrounding mechanism is setting a system property that the
Settings infrastructure picks up.
</description><key id="131105532">16423</key><summary>Remove the es.foreground property after it is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Settings</label><label>bug</label><label>review</label></labels><created>2016-02-03T18:02:45Z</created><updated>2016-02-08T10:04:04Z</updated><resolved>2016-02-05T14:59:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-03T18:03:44Z" id="179378854">@s1monw is this a good way to do it? I don't think the Settings stuff is used during bootstrap so it just made sense to drop the property so it doesn't get mixed up with the Settings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow snapshot deletion concurrent to creation in another repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16422</link><project id="" key="" /><description>Allow snapshot deletion to proceed if the in-progress snapshot is being
created in a different repository.
</description><key id="131101472">16422</key><summary>Allow snapshot deletion concurrent to creation in another repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jloomis</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label></labels><created>2016-02-03T17:47:37Z</created><updated>2016-03-24T16:45:23Z</updated><resolved>2016-03-24T16:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T19:41:27Z" id="183734054">@ywelsch could you take a look at this please?
</comment><comment author="jloomis" created="2016-03-16T23:23:41Z" id="197601563">ping
</comment><comment author="ywelsch" created="2016-03-24T16:45:23Z" id="200918598">@jloomis Sorry for the delay. I'm hesitant on going forward with this PR. The snapshot infrastructure does not currently support any kind of concurrent operations. This PR would create an exception to this rule for this one specific case, possibly creating confusion for users. Introducing more concurrency on the snapshot level is something we have been discussing for some time now and has also been in the back of our minds (as you've probably noticed, the cluster state can hold multiple snapshot entries). We are actively working on improving our test infrastructure for snapshots in preparation for changes like this one. One such example is the work on task management which helps to test snapshot functionality in isolation from other parts of ES. Another one is the mocking of infrastucture so that we can better test our AWS / EC2 / ... plugins. Once we have the proper infrastructure in place, we will feel much safer adding functionality like the one suggested here. I&#8217;m going to close this PR for now but we will definitely revisit concurrent snapshot capabilities in the future. Thanks for sending the PR and sorry again for the delay.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>'Failed to send ping' and firewall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16421</link><project id="" key="" /><description>Got myself firewalled between datacenters (without knowing it) and when adding a node it seemed to just not being able to find anything:

```
[2016-02-03 16:48:46,094][INFO ][node                     ] [ottsc-tiger] stopped
[2016-02-03 16:48:46,094][INFO ][node                     ] [ottsc-tiger] closing ...
[2016-02-03 16:48:46,145][INFO ][node                     ] [ottsc-tiger] closed
[2016-02-03 16:48:47,493][INFO ][node                     ] [ottsc-tiger] version[2.2.0], pid[21119], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-02-03 16:48:47,495][INFO ][node                     ] [ottsc-tiger] initializing ...
[2016-02-03 16:48:48,531][INFO ][plugins                  ] [ottsc-tiger] modules [lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-02-03 16:48:50,545][INFO ][node                     ] [ottsc-tiger] initialized
[2016-02-03 16:48:50,545][INFO ][node                     ] [ottsc-tiger] starting ...
[2016-02-03 16:48:50,680][INFO ][transport                ] [ottsc-tiger] publish_address {10.72.58.246:9300}, bound_addresses {10.72.58.246:9300}
[2016-02-03 16:48:50,692][INFO ][discovery                ] [ottsc-tiger] ottsc-elastic-large/lk3l2COET7y296byYYT3ZA
[2016-02-03 16:49:00,551][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:49:10,552][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:49:20,553][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:49:20,692][WARN ][discovery                ] [ottsc-tiger] waited for 30s and no initial state was set by the discovery
[2016-02-03 16:49:20,716][INFO ][http                     ] [ottsc-tiger] publish_address {10.72.58.246:9200}, bound_addresses {10.72.58.246:9200}
[2016-02-03 16:49:20,717][INFO ][node                     ] [ottsc-tiger] started
[2016-02-03 16:49:30,553][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:49:40,554][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:49:50,555][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:00,556][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:10,560][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:20,560][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:30,561][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:40,562][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:50:50,562][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:00,563][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:10,564][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:20,564][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:30,566][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:40,567][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:51:50,567][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:00,568][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:10,569][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:20,569][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:30,570][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:40,571][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:52:50,572][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:00,572][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:10,573][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:20,574][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:30,574][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:40,575][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:53:50,576][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:00,576][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:10,577][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:20,578][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:30,579][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:40,580][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:54:50,580][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:00,581][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:10,582][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:20,583][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:30,583][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:40,584][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:55:50,585][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:56:00,585][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:56:10,586][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
[2016-02-03 16:56:20,587][INFO ][marvel.agent.exporter    ] [ottsc-tiger] skipping exporter [default_local] as it isn't ready yet
```

Once punched ports 9300-9400 it suddenly start talking

```
[2016-02-03 16:56:21,573][WARN ][discovery.zen.ping.unicast] [ottsc-tiger] failed to send ping to [{#zen_unicast_2#}{10.88.58.13}{10.88.58.13:9300}]
SendRequestTransportException[[][10.88.58.13:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][10.88.58.13:9300] Node not connected];
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$1000(UnicastZenPing.java:83)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:403)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][10.88.58.13:9300] Node not connected]
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1115)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:802)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
        ... 6 more
[2016-02-03 16:56:21,620][WARN ][discovery.zen.ping.unicast] [ottsc-tiger] failed to send ping to [{#zen_unicast_1#}{10.88.58.12}{10.88.58.12:9300}]
SendRequestTransportException[[][10.88.58.12:9300][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[][10.88.58.12:9300] Node not connected];
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$1000(UnicastZenPing.java:83)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:403)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[][10.88.58.12:9300] Node not connected]
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1115)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:802)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
        ... 6 more
[2016-02-03 16:56:24,357][INFO ][cluster.service          ] [ottsc-tiger] detected_master {ottsc-elastic-01}{gXG7_pfnSCe6X4sOCJwL2w}{10.88.58.12}{10.88.58.12:9300}, added {{ottsc-elastic-02}{Ll4MtYTjSIuU2z-3Rjf48Q}{10.88.58.13}{10.88.58.13:9300},{ottsc-elastic-01}{gXG7_pfnSCe6X4sOCJwL2w}{10.88.58.12}{10.88.58.12:9300},{kibana-large}{h5cB_G0vTx6z8Ixmgc7ZPw}{10.88.58.10}{10.88.58.10:9300}{data=false, master=false},}, reason: zen-disco-receive(from master [{ottsc-elastic-01}{gXG7_pfnSCe6X4sOCJwL2w}{10.88.58.12}{10.88.58.12:9300}])
[2016-02-03 16:56:24,843][INFO ][license.plugin.core      ] [ottsc-tiger] license [55847aa4-e3bc-4935-84c7-83e20bdead0b] - valid
```

I would expect it will say earlier about ping failure? 

Ta
Marcin
</description><key id="131088959">16421</key><summary>'Failed to send ping' and firewall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcinkubica</reporter><labels><label>:Discovery</label><label>:Logging</label><label>discuss</label></labels><created>2016-02-03T17:04:28Z</created><updated>2017-03-17T23:34:51Z</updated><resolved>2017-03-17T23:34:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T22:54:24Z" id="183768100">It says:

```
[2016-02-03 16:49:20,692][WARN ][discovery                ] [ottsc-tiger] waited for 30s and no initial state was set by the discovery
```

Setting it to DEBUG would provide more info, but I don't think we should be warning about every missed ping, no?

What would you change?
</comment><comment author="marcinkubica" created="2016-02-14T00:30:48Z" id="183780121">Hi I would change this message

`[2016-02-03 16:56:21,573][WARN ][discovery.zen.ping.unicast] [ottsc-tiger] failed to send ping to [{#zen_unicast_2#}{10.88.58.13}{10.88.58.13:9300}]`
to be in the log _before_ the actual recovery.

Instead it popped in after.

Cheers
Marcin
</comment><comment author="marcinkubica" created="2016-02-16T21:58:52Z" id="184891645">I think current logic is ping failures do pop in log if a node is connected to the cluster, but has issues connecting to some nodes. 

However if it can't join the cluster on start, then it seems like it's not respecting the unicast list and is not reporting connection failures.
</comment><comment author="jasontedor" created="2017-03-17T23:34:51Z" id="287496333">We log failed pings at the trace level, I do not think there is anything that needs to be done here.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update .dir-locals.el for enforced line length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16420</link><project id="" key="" /><description>Also add compilation command

Relates to #16413
</description><key id="131068035">16420</key><summary>Update .dir-locals.el for enforced line length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T15:59:42Z</created><updated>2016-02-29T16:38:24Z</updated><resolved>2016-02-04T21:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-04T21:54:04Z" id="180068117">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices cat API doesn't list closed indices when filtered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16419</link><project id="" key="" /><description>Hi,

With the `indices` cat API, I can list my indices, either _open_ or _close_.

```
&#8594; curl http://127.0.0.1:9200/_cat/indices
       close partner_results-20150930
yellow open  partner_requests-20150929     1 1     17     0 230.3kb 230.3kb
yellow open  partner_results-20151102      5 1   4311     0   1.5mb   1.5mb
yellow open  search_requests-20150902      5 1     10     0  51.3kb  51.3kb
```

If I filter my results, I no longer see _close_ indices : 

```
&#8594; curl http://127.0.0.1:9200/_cat/indices/\*_results-\*
yellow open  partner_results-20151102      5 1   4311     0   1.5mb   1.5mb
```

I didn't find the documentation about that so I don't know if it a bug or an undocumented behavior.

I thought there might be an argument for `/_cat/indices` to show `all/open/close` indices, but I haven't found anything.
</description><key id="131059182">16419</key><summary>Indices cat API doesn't list closed indices when filtered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels><label>:CAT API</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-02-03T15:32:29Z</created><updated>2016-05-25T08:02:15Z</updated><resolved>2016-05-25T08:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="debarshiraha" created="2016-02-19T09:48:51Z" id="186142506">I would like to start contributing by fixing this one, if this can be assigned to me!
</comment><comment author="nik9000" created="2016-02-19T15:12:02Z" id="186253932">&gt; I would like to start contributing by fixing this one, if this can be assigned to me!

Grumble grumble it looks like you can't assign non-"elastic members" to issues. I was wrong! Anyway, consider it claimed.
</comment><comment author="dadoonet" created="2016-02-19T15:26:07Z" id="186259290">@nik9000 In such a case, I think we can remove the `adoptme` label. I just updated the issue.
</comment><comment author="nik9000" created="2016-02-19T15:28:49Z" id="186260070">&gt; @nik9000 In such a case, I think we can remove the adoptme label. I just updated the issue.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail early on JDK with compiler bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16418</link><project id="" key="" /><description>Early versions of JDK 8 have a compiler bug that prevents assignment to
a definitely unassigned final variable inside the body of a lambda. This
commit adds an early-out to the build process which also gives a useful
error message.

Relates #16097, relates #16362 
</description><key id="131057354">16418</key><summary>Fail early on JDK with compiler bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>jvm bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T15:25:33Z</created><updated>2016-03-10T18:36:07Z</updated><resolved>2016-02-03T18:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T15:29:20Z" id="179293271">This gives:

``` bash
10:27:09 1d [jason:~/src/elastic/elasticsearch] java-8-compiler-bug &#177; \
&gt; JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/ \
&gt; gradle :core:compileJava
.
.
.
FAILURE: Build failed with an exception.

* Where:
Build file '/Users/jason/src/elastic/elasticsearch/core/build.gradle' line: 24

* What went wrong:
A problem occurred evaluating project ':core'.
&gt; Failed to apply plugin [id 'elasticsearch.build']
   &gt; JDK Oracle Corporation 1.8.0_31 has compiler bug JDK-8052388, update your JDK to at least 8u40
```
</comment><comment author="jasontedor" created="2016-02-03T16:17:08Z" id="179319429">@ywelsch Care to review?
</comment><comment author="ywelsch" created="2016-02-03T18:21:14Z" id="179388302">Left one minor comment about potentially confusing warning message. As I'm not sure how often that warning might appear, I prefer to err on the cautious side. I don't have a strong opinion on it, so feel free to push.

LGTM! Thanks @jasontedor, this will make it easier for new contributors to get started!
</comment><comment author="jasontedor" created="2016-02-03T18:28:55Z" id="179391362">Thanks @ywelsch for the helpful review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Agg Factories for All the aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16417</link><project id="" key="" /><description /><key id="131057352">16417</key><summary>Create Agg Factories for All the aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-03T15:25:32Z</created><updated>2016-02-08T15:42:09Z</updated><resolved>2016-02-05T12:11:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-05T12:11:03Z" id="180322801">superseded by https://github.com/elastic/elasticsearch/pull/16473
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.2 : Cant install marvel plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16416</link><project id="" key="" /><description>I am trying to install the marvel sense plugin for ES and Kibana on windows. I have ES version 2.2.0 and Kibana version 4.4.0

Here is what i tried to do

For ES

```
plugin install license
```

i get

```
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugi
n/license/2.2.0/license-2.2.0.zip ...
ERROR: failed to download out of all possible locations..., use --verbose to get
 detailed information
```

i also tried

```
plugin install marvel-agent
```

but i get

```
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugi
n/marvel-agent/2.2.0/marvel-agent-2.2.0.zip ...
ERROR: failed to download out of all possible locations..., use --verbose to get
 detailed information
```

For Kibana, i knew it wasn't going to work but still

i tried

```
plugin --install elasticsearch/marvel/latest
```

and i get

```
Installing marvel
Attempting to transfer from https://download.elastic.co/elasticsearch/marvel/mar
vel-latest.tar.gz
Error: Client request error: connect ETIMEDOUT
Plugin installation was unsuccessful due to error "Client request error: connect
 ETIMEDOUT"
```

has anyone faced similar issues? Any help would be appreciated. I did not face these problems for ES 2.1 and Kibana 4.3

I downloaded everything from the ES website.
</description><key id="131047581">16416</key><summary>Elasticsearch 2.2 : Cant install marvel plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2016-02-03T14:50:33Z</created><updated>2016-02-03T15:27:34Z</updated><resolved>2016-02-03T14:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-03T14:54:06Z" id="179275460">https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.2.0/marvel-agent-2.2.0.zip actually exists. May be you are running behind a proxy?

Note that if you want SENSE, you don't need to install Marvel at all.

Feel free to join us on discuss.elastic.co where we can provide a better support for questions like this one.
</comment><comment author="abtpst" created="2016-02-03T15:24:55Z" id="179291558">thanks, i am behind a proxy but i was able to perform the same operations for ES 2.1 and Kibana 4.3. moreover, i have the proxy env variables defined. By the way, how do i use SENSE without installing sense? when i go to

http://localhost:5601/app/sense

i get

{"statusCode":404,"error":"Not Found","message":"Unknown app sense"}
</comment><comment author="dadoonet" created="2016-02-03T15:27:34Z" id="179292591">I said that you don't need to install MARVEL.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail demoted primary shards and retry request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16415</link><project id="" key="" /><description>This commit handles the scenario where a replication action fails on a
replica shard, the primary shard attempts to fail the replica shard
but the primary shard is notified of demotion by the master. In this
scenario, the demoted primary shard must be failed, and then the
request rerouted again to the new primary shard.

Relates #14252 
</description><key id="131034120">16415</key><summary>Fail demoted primary shards and retry request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T13:59:44Z</created><updated>2016-02-24T10:24:57Z</updated><resolved>2016-02-10T16:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-04T07:57:16Z" id="179698858">Thanks Jason. left some comments with suggestions
</comment><comment author="jasontedor" created="2016-02-09T12:31:30Z" id="181845589">@bleskes I've pushed more commits.
</comment><comment author="bleskes" created="2016-02-10T15:51:02Z" id="182440451">I think this turned out well. I left some comments. I'm thinking we should get this in and then open a follow up issue to deal with the annoying request-reset issue.
</comment><comment author="bleskes" created="2016-02-10T16:18:03Z" id="182453990">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Seal all processor implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16414</link><project id="" key="" /><description /><key id="131031535">16414</key><summary>Seal all processor implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T13:48:49Z</created><updated>2016-02-08T09:35:43Z</updated><resolved>2016-02-03T13:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-03T13:51:36Z" id="179245029">LGTM +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail build for lines longer than 140 characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16413</link><project id="" key="" /><description>140 characters is our official line length from CONTRIBUTING.md. It is a
little longer than fits well in github but you can use a userstyle to fix
that. It is perfect for Eclipse and unified diffs but too wide for side by
side diffs. Regardless, its the official limit we've had for years. We just
haven't enforced it automatically and we haven't enforced it using github
because line limits are hard to notice there.

This only hits about 2/3 of the java files - those that didn't have failures
already. If they did have a failure it suppresses them. We should pick files
off that list as time goes on.

For posterity I generated the suppressions by running checkstyle with
ignoreFailures = true, piping the output to a file, and then running it
through this:
perl -ne 'print if s{.*[ant:checkstyle] /.+/elasticsearch/}{  &lt;suppress files="} &amp;&amp; s{\.java.+}{\.java" checks="LineLength" /&gt;}' | uniq
</description><key id="131027611">16413</key><summary>Fail build for lines longer than 140 characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T13:35:37Z</created><updated>2016-02-04T21:45:54Z</updated><resolved>2016-02-04T21:45:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-03T13:39:50Z" id="179237816">I think if we ever want to enforce our line length we should start with something like this. I'm open to the argument that we really don't care about the line length in which case I'll drop this.

I think it isn't worth trying to enforce line length limits in code review: its just noise. But if we enforce them with a tool then the person writing the code can just deal with error quickly and move on.
</comment><comment author="dakrone" created="2016-02-03T15:04:11Z" id="179281095">This is super out-there territory, but for:

``` xml
&lt;!DOCTYPE suppressions PUBLIC
          "-//Puppy Crawl//DTD Suppressions 1.1//EN"
          "http://www.puppycrawl.com/dtds/suppressions_1_1.dtd"&gt;
```

Should we host this DTD somewhere ourselves so in the event a 3rd party goes down we don't have a bad link?

Other than that, LGTM. I definitely think it's worth enforcing this. (I do wish it were 100 instead of 140 but that's another story)
</comment><comment author="nik9000" created="2016-02-03T15:10:03Z" id="179285375">&gt; Other than that, LGTM. I definitely think it's worth enforcing this. (I do wish it were 100 instead of 140 but that's another story)

I'm happy with 140 because its the limit we have. I'd probably have chosen 100 or so if I'd chosen but we have it and it work fine. Its not as good for side by side diffs but I don't use them anyway. I probably don't use them because of the 140 column limit but, yeah.

&gt; Should we host this DTD somewhere ourselves so in the event a 3rd party goes down we don't have a bad link?

I dunno how often its actually checked. We could certainly cram it in a checkstyle directory but I if checkstyle's DTD's dropped out lots of folks would have trouble.

I'm going to let this sit for another day or two so it can accumulate any contrarian views.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't cache resolved hostnames forever</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16412</link><project id="" key="" /><description>Today we use InetAddress to represent IP addresses.  InetAddress handles the work of resolving hostnames from DNS and from the local `hosts` file.  

With the security manager enabled, successful hostname lookups are cached forever to prevent spoofing attacks. I don't know if this behaviour was different before the security manager was enabled, but it seems unlikely given issues such as https://github.com/elastic/elasticsearch/issues/10337 and https://github.com/elastic/elasticsearch/issues/14441.

It would be a useful improvement to be able to specify unicast hosts as hostnames which are looked up from DNS or hosts, then if the IP addresses change and the node need to reconnect to the cluster, it can just do a fresh lookup to gather the current IPs.  Similar logic would help the clients.

If we make this change, it should be configurable (otherwise we're introducing the chance for spoofing) and we should consider the impact on hostname verification of ssl certs.

Testing this change would be hard...
</description><key id="131021777">16412</key><summary>Don't cache resolved hostnames forever</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>enhancement</label></labels><created>2016-02-03T13:15:31Z</created><updated>2017-03-15T19:53:10Z</updated><resolved>2016-11-22T19:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-12T10:41:17Z" id="183266703">The DNS cache of a Java process is handled by  the property `networkaddress.cache.ttl`. Quoting [Oracle Docs](http://docs.oracle.com/javase/8/docs/technotes/guides/net/properties.html):

&gt; Specified in java.security to indicate the caching policy for successful name lookups from the name service.. The value is specified as integer to indicate the number of seconds to cache the successful lookup.
&gt; 
&gt; A value of -1 indicates "cache forever". The default behavior is to cache forever when a security manager is installed, and to cache for an implementation specific period of time, when a security manager is not installed.
</comment><comment author="danielmitterdorfer" created="2016-02-12T12:52:53Z" id="183312337">As per the [Oracle documentation on policy files](http://docs.oracle.com/javase/8/docs/technotes/guides/security/PolicyFiles.html) the above-mentioned property (and maybe also the related property `networkaddress.cache.negative.ttl` (for failed DNS lookups)) have to be specified in a file called `java.security`. This file is located in `$JRE_HOME/lib/security` and the settings there are system-wide.

It is possible to provide an application-specific `java.security` file that overrides the system-wide defaults by adding the system property `-Djava.security.properties=$CUSTOM_SECURITY_FILE` and specifying application-specific overrides there _but_ this is only possible if the property `security.overridePropertiesFile` is set to `true` in `$JRE_HOME/lib/security/java.security` (the default is `true`). So in case an administrator disables this, we won't be able to override the system-wide setting.

I wonder whether we should just point users to the Oracle documentation on how to change the DNS cache lifetime system-wide because I am not sure whether they want to configure different (Java) DNS cache lifetimes for different applications on the same machine anyway. Wdyt @clintongormley?
</comment><comment author="clintongormley" created="2016-02-13T11:31:55Z" id="183647335">@danielmitterdorfer I'd be happy with just adding this documentation.  Is there anything we need to change code-wise as well? Or perhaps just adding tests to ensure that the documented solution works?
</comment><comment author="clintongormley" created="2016-02-13T13:49:44Z" id="183668315">Also related to https://github.com/elastic/elasticsearch/issues/10337 and https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/131 and https://github.com/elastic/elasticsearch/pull/11256
</comment><comment author="miah" created="2016-02-13T16:54:46Z" id="183702579">We run all our jvms with this configuration. We also only pass hostnames to
ES via command arguments at startup. When we roll out new instances on aws
ES never refreshes the hosts IPs via DNS lookups.

Would appreciate if somebody else could verify that it works as expected
because it doesn't seem to in our environment.
On Feb 13, 2016 3:33 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; @danielmitterdorfer https://github.com/danielmitterdorfer I'd be happy
&gt; with just adding this documentation. Is there anything we need to change
&gt; code-wise as well? Or perhaps just adding tests to ensure that the
&gt; documented solution works?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16412#issuecomment-183647335
&gt; .
</comment><comment author="danielmitterdorfer" created="2016-02-15T13:09:07Z" id="184199164">@clintongormley I would just point users to the Oracle documentation and not add any tests for two reasons:
1. This is a configuration that is purely related to the JVM and nothing of this is Elasticsearch specific
2. It is a global change so the JDK configuration has to change in _every_ environment where the tests are run.
</comment><comment author="danielmitterdorfer" created="2016-02-15T13:30:54Z" id="184209640">@miah I have created a small demo program to verify that everything works as expected ([source code as gist](https://gist.github.com/danielmitterdorfer/4897b330a2cf21ebcf04)). In addition, I've set these values in my `java.security` in the JRE/lib directory:

```
networkaddress.cache.ttl=10
networkaddress.cache.negative.ttl=-1
```

When you look at the demo source code, you'll see that we query for one existing host ("www.google.com") and for one non-existing one ("www.this-does-not-exist.io"). My expectation is that we see DNS requests every 10 seconds for the existing host and only one for the non-existing one when we periodically clear the OS DNS cache.

When you invoke the demo program as described in the gist and open tcpdump (I did `sudo tcpdump -vvv -s 0 -l -n port 53`) and periodically clear the DNS cache (depending on your OS) you'll see the expected behavior (my environment is Mac OS X 10.11):

Output from the application:

```
14:04:10
www.google.com =&gt; 173.194.39.18
14:04:15
www.google.com =&gt; 173.194.39.18
14:04:20
www.google.com =&gt; 173.194.39.18
14:04:25
www.google.com =&gt; 173.194.39.18
14:04:30
www.google.com =&gt; 173.194.39.18
14:04:35
www.google.com =&gt; 173.194.39.18
```

Output from tcpdump:

```
tcpdump: data link type PKTAP
tcpdump: listening on pktap, link-type PKTAP (Packet Tap), capture size 262144 bytes
14:04:10.592232 IP (tos 0x0, ttl 255, id 64876, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.60113 &gt; 192.168.1.1.53: [udp sum ok] 60929+ A? www.google.com. (32)
14:04:10.592301 IP (tos 0x0, ttl 255, id 6814, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.58959 &gt; 192.168.1.1.53: [udp sum ok] 36807+ AAAA? www.google.com. (32)
14:04:10.624765 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 140)
    192.168.1.1.53 &gt; 192.168.1.103.60113: [udp sum ok] 60929 q: A? www.google.com. 5/0/0 www.google.com. [4m48s] A 173.194.39.20, www.google.com. [4m48s] A 173.194.39.18, www.google.com. [4m48s] A 173.194.39.17, www.google.com. [4m48s] A 173.194.39.16, www.google.com. [4m48s] A 173.194.39.19 (112)
14:04:10.625961 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 88)
    192.168.1.1.53 &gt; 192.168.1.103.58959: [udp sum ok] 36807 q: AAAA? www.google.com. 1/0/0 www.google.com. [4m54s] AAAA 2a00:1450:4005:800::1010 (60)
14:04:10.695845 IP (tos 0x0, ttl 255, id 17766, offset 0, flags [none], proto UDP (17), length 72)
    192.168.1.103.49957 &gt; 192.168.1.1.53: [udp sum ok] 27956+ A? www.this-does-not-exist.io. (44)
14:04:10.695942 IP (tos 0x0, ttl 255, id 63692, offset 0, flags [none], proto UDP (17), length 72)
    192.168.1.103.51379 &gt; 192.168.1.1.53: [udp sum ok] 47751+ AAAA? www.this-does-not-exist.io. (44)
14:04:10.727572 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 141)
    192.168.1.1.53 &gt; 192.168.1.103.49957: [udp sum ok] 27956 NXDomain q: A? www.this-does-not-exist.io. 0/1/0 ns: io. [19m14s] SOA ns1.communitydns.net. nicadmin.nic.io. 1455538915 3600 1800 3600000 3600 (113)
14:04:10.728133 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 141)
    192.168.1.1.53 &gt; 192.168.1.103.51379: [udp sum ok] 47751 NXDomain q: AAAA? www.this-does-not-exist.io. 0/1/0 ns: io. [19m14s] SOA ns1.communitydns.net. nicadmin.nic.io. 1455538915 3600 1800 3600000 3600 (113)
14:04:20.733510 IP (tos 0x0, ttl 255, id 51420, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.58825 &gt; 192.168.1.1.53: [udp sum ok] 11803+ A? www.google.com. (32)
14:04:20.733566 IP (tos 0x0, ttl 255, id 13172, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.60441 &gt; 192.168.1.1.53: [udp sum ok] 14292+ AAAA? www.google.com. (32)
14:04:20.737039 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 140)
    192.168.1.1.53 &gt; 192.168.1.103.58825: [udp sum ok] 11803 q: A? www.google.com. 5/0/0 www.google.com. [4m38s] A 173.194.39.19, www.google.com. [4m38s] A 173.194.39.16, www.google.com. [4m38s] A 173.194.39.17, www.google.com. [4m38s] A 173.194.39.18, www.google.com. [4m38s] A 173.194.39.20 (112)
14:04:20.737435 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 88)
    192.168.1.1.53 &gt; 192.168.1.103.60441: [udp sum ok] 14292 q: AAAA? www.google.com. 1/0/0 www.google.com. [4m44s] AAAA 2a00:1450:4005:800::1010 (60)
14:04:30.748453 IP (tos 0x0, ttl 255, id 17598, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.55087 &gt; 192.168.1.1.53: [udp sum ok] 1167+ A? www.google.com. (32)
14:04:30.748806 IP (tos 0x0, ttl 255, id 29531, offset 0, flags [none], proto UDP (17), length 60)
    192.168.1.103.60394 &gt; 192.168.1.1.53: [udp sum ok] 60974+ AAAA? www.google.com. (32)
14:04:30.754529 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 140)
    192.168.1.1.53 &gt; 192.168.1.103.55087: [udp sum ok] 1167 q: A? www.google.com. 5/0/0 www.google.com. [4m28s] A 173.194.39.20, www.google.com. [4m28s] A 173.194.39.19, www.google.com. [4m28s] A 173.194.39.16, www.google.com. [4m28s] A 173.194.39.17, www.google.com. [4m28s] A 173.194.39.18 (112)
14:04:30.754533 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 88)
    192.168.1.1.53 &gt; 192.168.1.103.60394: [udp sum ok] 60974 q: AAAA? www.google.com. 1/0/0 www.google.com. [4m34s] AAAA 2a00:1450:4005:800::1010 (60)
```

You see that we query only once for "www.this-does-not-exist.io" but every 10 seconds for "www.google.com". Similarly, you should also see only one DNS request for "google.com" when you set `networkaddress.cache.ttl=-1` in java.security.
</comment><comment author="danielmitterdorfer" created="2016-02-15T13:36:15Z" id="184211154">@miah One more thing: Note that the program runs with the security manager enabled. The JDK implementation behaves differently whether or not a security manager is enabled (see also my comment above with the link to the Oracle docs). As Elasticsearch 3.0 will make security mandatory (#16176) it is a sensible assumption that we assume here too that a security manager is enabled.
</comment><comment author="alexbrasetvik" created="2016-02-25T07:48:22Z" id="188657674">It would also be nice if the transport client could do the lookups when it connects, instead of just when the client object is created, as IPs can change e.g. when going through a load balancer.
</comment><comment author="beiske" created="2016-02-25T13:13:37Z" id="188780650">Despite keeping the hostname an InetAdress will never attempt to resolve the ip after it has been created. I think this is a source of confusion for many using the transport client. It is particularly important when connecting to the cloud service. Due to the load balancers even a single node cluster has multiple ip adresses and they may change at any point in time.

@clintongormley Is this issue also relevant for the transport client or should we make a separate issue?
</comment><comment author="danielmitterdorfer" created="2016-02-26T09:50:48Z" id="189196583">@alexbrasetvik, @beiske: I don't know what @clintongormley thinks about your idea but I think it would be better to create a new ticket for the transport client topic.
</comment><comment author="lifeofguenter" created="2016-04-11T13:17:34Z" id="208336380">What's the status on this? This causes problems with AWS ElasticSearch service.
</comment><comment author="danielmitterdorfer" created="2016-04-11T14:07:57Z" id="208364401">@lifeofguenter The topic discussed in this ticket has nothing to do with Elasticsearch per se. It is a pure JVM level setting so (in the scope of this ticket) we will not change any code but maybe just add documentation on how to change this setting in Elasticsearch.

Just to be sure: by "AWS ElasticSearch service" you refer to Amazon's service and _not_ our Elasticsearch cloud offering (called [Elastic Cloud](http://cloud.elastic.co/)), right? We have no additional insight on Amazon's offering and I am afraid you will also not be able to change a JVM level setting there. I fear this has to be addressed by Amazon (as it is a JVM level setting that we cannot change from within the application).
</comment><comment author="lifeofguenter" created="2016-04-11T14:18:29Z" id="208369219">I did the following changes (ubuntu 14.04) in `/usr/lib/jvm/jdk-8-oracle-x64/jre/lib/security/java.security`:

```
networkaddress.cache.ttl=60
networkaddress.cache.negative.ttl=10
```

But that somehow did not do the trick?

Yes, I am referring to: https://aws.amazon.com/elasticsearch-service/ - however we are running logstash as per: http://www.lifeofguenter.de/2016/01/elk-aws-elasticbeanstalk-laravel.html - which is hosted on our "own" EC2 instance, thus we are able to do changes, and thats also the link that currently complaints if the dns record to ElasticSearch changes.

**UPDATE: sorry my problems are most probably unrelated!**
</comment><comment author="miah" created="2016-04-25T19:07:04Z" id="214482130">Still have this issue...

If I replace masters, I need to reboot every node in the cluster otherwise it never detects the IP changes.

I have a DNS TTL of 6 minutes. I replaced my master servers, and 20 minutes later elasticsearch is still trying to connect to the old IP's. I have the java.security changes in place. Elasticsearch is configured to connect to a round-robin dns entry for the master nodes.

```
services 16207 17.0 63.3 35426032 9753368 ?    Sl   Apr05 4911:31 /usr/lib/jvm/java-8-oracle/bin/java --http.port 9200 
--transport.tcp.port 9300 
--cluster.name=logsearch-dev 
--cluster.routing.allocation.allow_rebalance=always --cluster.routing.allocation.cluster_concurrent_rebalance=2 
--cluster.routing.allocation.node_concurrent_recoveries=2 --cluster.routing.allocation.node_initial_primaries_recoveries=12 
--cluster.routing.allocation.enable=all 
--node.name=logsearch-data-5.dev.bs.com
--node.master=false 
--node.data=true 
--node.auto_attributes=true  
--discovery.zen.minimum_master_nodes=2 
--discovery.zen.ping.multicast.enabled=false 
--discovery.zen.ping.unicast.hosts=app.logsearch-master.dev.bs.com 
--discovery.zen.ping_timeout=10s 
--discovery.zen.fd.ping_interval=1s 
--discovery.zen.fd.ping_timeout=60s 
--discovery.zen.fd.ping_retries=3
```

```
grep cache /usr/lib/jvm/java-8-oracle/jre/lib/security/java.security
# The Java-level namelookup cache policy for successful lookups:
# any positive value: the number of seconds to cache an address for
# zero: do not cache
# is to cache for 30 seconds.
networkaddress.cache.ttl=0
# The Java-level namelookup cache policy for failed lookups:
# any negative value: cache forever
# any positive value: the number of seconds to cache negative lookup results
# zero: do not cache
networkaddress.cache.negative.ttl=0
```
</comment><comment author="jasontedor" created="2016-04-25T19:12:28Z" id="214484986">@clintongormley @danielmitterdorfer I don't think it's correct that today setting the DNS cache properties at the JVM level is going to resolve the problems being reported here. The underlying reason is that [we do hostname lookup during initialization of unicast zen ping and never do lookups again](https://github.com/elastic/elasticsearch/blob/2f6405ee277ec64269dd13fdfbd8445f96b4516d/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java#L163-L175). This is currently a _deliberate_ choice.
</comment><comment author="miah" created="2016-04-25T19:17:18Z" id="214487240">@jasontedor That definitely seems to be the case.
</comment><comment author="danielmitterdorfer" created="2016-05-13T14:23:29Z" id="219057758">@jasontedor Agreed. In that case the DNS cache settings will not help. As you explicitly mention that this is a deliberate choice, does it make sense to close this ticket then (and maybe document the decision or at least its consequences)?
</comment><comment author="bleskes" created="2016-05-14T10:27:17Z" id="219212917">&gt; This is currently a deliberate choice.

I think it's OK to re-resolve the configured unicast host list when pinging. We don't do it often (only on master loss/initialization ) and we also ping all ips of last known nodes on top of it. 

My only concern is that DNS resolution timeout/failure should not block the pinging or delay it (remember we do it on master loss and we block writes until pinging is done). This means implementing this can be tricky code wise (that code is already hairy)
</comment><comment author="jasontedor" created="2016-05-14T10:57:54Z" id="219214113">&gt; I think it's OK to re-resolve the configured unicast host list when pinging.

I do too, I'm only explaining why the DNS cache settings here did and do nothing. 
</comment><comment author="danielmitterdorfer" created="2016-05-18T15:19:29Z" id="220060601">So this sounds to me we should remove the "Discuss" label and add "AdoptMe" instead.
</comment><comment author="thxmasj" created="2016-10-07T12:45:36Z" id="252242856">Any good workarounds for this? I have a similar problem running on Docker with swarm mode, where the master/gossip nodes are runnig as a service and the data nodes point to the service name. As Docker uses DNS for the discovery this is a problem there as well.
</comment><comment author="Nils-Magnus" created="2016-10-11T13:05:14Z" id="252910702">@thxmasj I provide the full list (as reported by Docker) explicitly. That mitigates, but does not resolve the problem.
</comment><comment author="jasontedor" created="2016-11-23T01:53:25Z" id="262418058">This is now addressed in the forthcoming 5.1.0 (no date, sorry). If you are in an environment where DNS changes are a thing, you will have to adjust the JVM DNS cache settings in your system security policy. Please consult the Zen discovery docs for details. </comment><comment author="dustinschultz" created="2016-12-22T18:19:48Z" id="268859006">Has anyone tried enabling `client.transport.sniff=true` as a workaround? Curious if this would work around the issue.</comment><comment author="devulapalli8" created="2017-03-15T00:23:25Z" id="286603600">is there any update on this issue ? whats the fix</comment><comment author="jasontedor" created="2017-03-15T00:25:34Z" id="286603966">&gt; is there any update on this issue ? whats the fix

Yes, it's addressed starting in Elasticsearch 5.1.1. You can read about this in the [zen discovery docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/modules-discovery-zen.html#unicast).</comment><comment author="devulapalli8" created="2017-03-15T00:30:55Z" id="286604837">Thanks for quick reply. I'm using ES 2.1 version , when one of ES instances is rebuilt then its throwing NoRouteToHostException. to resolve the issue its forcing me rebuild dependent applications which connects to ES as client.  client.transport.sniff=true will enable to discover the new nodes added to the ES cluster. Is there workaround for ES2.1 version 
</comment><comment author="devulapalli8" created="2017-03-15T00:33:49Z" id="286605267">As mentioned in the document , I still set networkaddress.cache.ttl=0 in java.security but it didnt resolve the issue since when i ping the new ES instance its resolved with new IP address as expected so I dont think its DNS caching issue.</comment><comment author="jasontedor" created="2017-03-15T03:25:25Z" id="286630482">It's not addressed in the 2.x series, there is nothing you can do there. It's only resolved since 5.1.1.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move non-groovy geo tests from messy tests to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16411</link><project id="" key="" /><description>This splits the SimpleSortTests and GeoDistanceTests suites and moves those tests related to geo distance (sorting) that do not require groovy to run back into the core tests suite.
</description><key id="131014836">16411</key><summary>Move non-groovy geo tests from messy tests to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Geo</label><label>:Search Refactoring</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T12:49:07Z</created><updated>2016-02-03T13:52:01Z</updated><resolved>2016-02-03T13:52:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-03T13:05:28Z" id="179219018">LGTM thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query TimeAllowed in Elastic Server Side </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16410</link><project id="" key="" /><description>Hi ,

I'm started new project in Elastic Search . 

My previous one was on SOLR where I used the parameter timeAllowed in the server side and overwrite in the client side when needed to terminate long running queries and return partial results to  avoid SOLR crashes . 

It seems that this crusiel feature is missing in Elastic search . 
I saw the parameter TerminateAfter . But as far as I understand it can apply only in the client side .
Also it seems that it will not work when running heavy aggregation query which return only small number of rows .

Also saw some workarounds like  Circuit Breakers which return an error to the client .

The TimeAllowed in SOLR seems to be based on  Lucene or maybe I'm wrong .

https://lucene.apache.org/core/4_4_0/core/org/apache/lucene/search/TimeLimitingCollector.html

It looks to me as a great feature to add which will help elasatic search stability and avoid from users to run bad queries on production .

Thanks

Alon
</description><key id="131010215">16410</key><summary>Query TimeAllowed in Elastic Server Side </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aloneldi</reporter><labels /><created>2016-02-03T12:31:54Z</created><updated>2016-02-15T16:23:36Z</updated><resolved>2016-02-13T22:42:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T22:42:13Z" id="183766941">Hi @aloneldi 

We already use TimeLimitingCollector, see https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java#L374

I think you're looking for the `timeout` parameter.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="aloneldi" created="2016-02-14T13:33:10Z" id="183889526">Hi Clinton ,

No it's not timeout parameter .
Timeout parameter only timeout the client request in the server side the request is keep running .
I'm looking for equivalent SOLR time-allowed parameter which can be configured in server side and return partial result incase the query exceeded its limit .

Alon
&#1504;&#1513;&#1500;&#1495; &#1502;&#1492;-iPhone &#1513;&#1500;&#1497;

&#8235;&#1489;-14 &#1489;&#1508;&#1489;&#1512;&#1523; 2016, &#1489;&#1513;&#1506;&#1492; 00:43, &#8207;&#8207;Clinton Gormley &#8207;notifications@github.com &#1499;&#1514;&#1489;/&#1492;:&#8236;

&gt; Hi @aloneldi
&gt; 
&gt; We already use TimeLimitingCollector, see https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java#L374
&gt; 
&gt; I think you're looking for the timeout parameter.
&gt; 
&gt; Please ask questions like these in the forum instead: https://discuss.elastic.co/
&gt; 
&gt; The github issues list is reserved for bug reports and feature requests only.
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2016-02-14T14:59:54Z" id="183903262">&gt; No it's not timeout parameter .

Yes it is. See the docs: https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search-request-body.html

&gt; `timeout`
&gt; A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Defaults to no timeout. 
&gt; 
&gt; Timeout parameter only timeout the client request in the server side the request is keep running .

No, depends on the client but they usually implement this timeout as the `request_timeout`.
</comment><comment author="aloneldi" created="2016-02-15T11:35:52Z" id="184176268">Hi Clinton .

I mean the timeAllowed Paremeter in SOLR is not timeout parameter in 
Elastic .

in SOLR there is timeout which terminate the session in the client side 
but keep running the request in the server side .

TimeAllowed in SOLR in the other hand can be configured in the server 
side to protect the server incase user run a wild query and also in the 
client side incase you want to extend it for specific .

TimeAllowed in SOLR also terminate the search operation in the server side .

I'm looking for the same protection type in Elastic . In the Server side 
and that the query will be terminated in case it exceed a specific 
threshold

https://wiki.apache.org/solr/CommonQueryParameters#timeAllowed

Thanks

Alon

On 14/02/2016 5:00 PM, Clinton Gormley wrote:

&gt; ```
&gt; No it's not timeout parameter .
&gt; ```
&gt; 
&gt; Yes it is. See the docs: 
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search-request-body.html
&gt; 
&gt; ```
&gt; |timeout|
&gt; A search timeout, bounding the search request to be executed
&gt; within the specified time value and bail with the hits accumulated
&gt; up to that point when expired. Defaults to no timeout.
&gt; 
&gt; Timeout parameter only timeout the client request in the server
&gt; side the request is keep running .
&gt; ```
&gt; 
&gt; No, depends on the client but they usually implement this timeout as 
&gt; the |request_timeout|.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elastic/elasticsearch/issues/16410#issuecomment-183903262.
</comment><comment author="clintongormley" created="2016-02-15T12:07:48Z" id="184182186">&gt; in SOLR there is timeout which terminate the session in the client side
&gt; but keep running the request in the server side .

This depends on the client and is usually called `request_timeout`.

&gt; TimeAllowed in SOLR in the other hand can be configured in the server
&gt; side to protect the server incase user run a wild query and also in the
&gt; client side incase you want to extend it for specific .
&gt; 
&gt; TimeAllowed in SOLR also terminate the search operation in the server side .

This can be set per request with the `timeout` parameter or globally on the server with the `search.default_search_timeout` setting.

See https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search.html#global-search-timeout
</comment><comment author="aloneldi" created="2016-02-15T12:56:11Z" id="184195513">Thanks Clinton .

Does it mean that using this parameter with value 30 (seconds) will 
terminate the query in the server side in case it exceed 30 seconds as 
well ?

Because its not clear in the docs nor in Forums .

https://www.elastic.co/guide/en/elasticsearch/guide/current/empty-search.html

"

It should be noted that this|timeout|does nothalt the execution of the 
query; it merely tells the coordinating node to return the results 
collected/so far/and to close the connection. In the background, other 
shards may still be processing the query even though results have been sent.

Use the time-out because it is important to your SLA, not because you 
want to abort the execution of long-running queries."

On 15/02/2016 2:08 PM, Clinton Gormley wrote:

&gt; ```
&gt; in SOLR there is timeout which terminate the session in the client
&gt; side
&gt; but keep running the request in the server side .
&gt; ```
&gt; 
&gt; This depends on the client and is usually called |request_timeout|.
&gt; 
&gt; ```
&gt; TimeAllowed in SOLR in the other hand can be configured in the server
&gt; side to protect the server incase user run a wild query and also
&gt; in the
&gt; client side incase you want to extend it for specific .
&gt; 
&gt; TimeAllowed in SOLR also terminate the search operation in the
&gt; server side .
&gt; ```
&gt; 
&gt; This can be set per request with the |timeout| parameter or globally 
&gt; on the server with the |search.default_search_timeout| setting.
&gt; 
&gt; See 
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search.html#global-search-timeout
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elastic/elasticsearch/issues/16410#issuecomment-184182186.
</comment><comment author="clintongormley" created="2016-02-15T15:04:35Z" id="184245769">This works in the same way.  The collector expires after the timeout and so it returns partial results.  This doesn't necessarily stop parts of the query running on the node, eg a heavy rewrite on a wildcard query. The only way to interrupt things like this is to add checks, which would also make them all slow.

See https://github.com/elastic/elasticsearch/issues/9156 and https://github.com/elastic/elasticsearch/pull/4586 for background
</comment><comment author="aloneldi" created="2016-02-15T16:08:00Z" id="184274257">So what you are saying is that in SOLR when using _timeAllowed_ there 
are cases in which the query continue to run ?

As SOLR have Timeout parameter which can be set in the request and 
TimeAllowed .

in All Forums I see that there no issues with TimeAllowed .

Where For Elastic There are many .

My Main issue is when query return many rows and I need to display to 
the fire 100-1000 rows .

When using also sort by time stamp the query would run to get all data 
and then return the the first 100 rows .

When my Database has billion rows and the user put filter which return 
90% of the data this would take a lot of time and many times carshed the 
JVM
with out of memory error .

Using TimeAllowed in solr to 30 Seconds we could be sure that the user 
will get data and that the query will terminate in 30 seconds.

So your suggestion to use *search.default_search_timeout *would solve 
this issue in Elastic ?

Thanks

Alon

On 15/02/2016 5:05 PM, Clinton Gormley wrote:

&gt; This works in the same way. The collector expires after the timeout 
&gt; and so it returns partial results. This doesn't necessarily stop parts 
&gt; of the query running on the node, eg a heavy rewrite on a wildcard 
&gt; query. The only way to interrupt things like this is to add checks, 
&gt; which would also make them all slow.
&gt; 
&gt; See #9156 https://github.com/elastic/elasticsearch/issues/9156 and 
&gt; #4586 https://github.com/elastic/elasticsearch/pull/4586 for background
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elastic/elasticsearch/issues/16410#issuecomment-184245769.
</comment><comment author="clintongormley" created="2016-02-15T16:15:51Z" id="184277606">&gt; So what you are saying is that in SOLR when using _timeAllowed_ there are cases in which the query continue to run ?

I don't know enough about SOLR to be certain.  A colleague believes that SOLR 5 also added the ability to interrupt operations on the terms dictionary, but that there were performance issues with this approach.  i don't know if it is true or if the situation has changed.  Either way, Elasticsearch doesn't do this.

&gt; My Main issue is when query return many rows and I need to display to the fire 100-1000 rows .

Then set the `size` limit to 1000.

&gt; When my Database has billion rows and the user put filter which return 90% of the data this would take a lot of time and many times carshed the JVM with out of memory error .

This won't happen (unless you set `size` to something stupid like MAXINT, and we now prevent you from doing this, see https://www.elastic.co/guide/en/elasticsearch/reference/2.2/breaking_21_search_changes.html#_from_size_limits)

&gt; Using TimeAllowed in solr to 30 Seconds we could be sure that the user will get data and that the query will terminate in 30 seconds.

This doesn't ensure you get the right results, just some results. 

This conversation is definitely one for the forums, and not the issues list. 
</comment><comment author="aloneldi" created="2016-02-15T16:23:36Z" id="184279879">Thanks Clinton .

On 15/02/2016 6:17 PM, Clinton Gormley wrote:

&gt; ```
&gt; So what you are saying is that in SOLR when using /timeAllowed/
&gt; there are cases in which the query continue to run ?
&gt; ```
&gt; 
&gt; I don't know enough about SOLR to be certain. A colleague believes 
&gt; that SOLR 5 also added the ability to interrupt operations on the 
&gt; terms dictionary, but that there were performance issues with this 
&gt; approach. i don't know if it is true or if the situation has changed. 
&gt; Either way, Elasticsearch doesn't do this.
&gt; 
&gt; ```
&gt; My Main issue is when query return many rows and I need to display
&gt; to the fire 100-1000 rows .
&gt; ```
&gt; 
&gt; Then set the |size| limit to 1000.
&gt; 
&gt; ```
&gt; When my Database has billion rows and the user put filter which
&gt; return 90% of the data this would take a lot of time and many
&gt; times carshed the JVM with out of memory error .
&gt; ```
&gt; 
&gt; This won't happen (unless you set |size| to something stupid like 
&gt; MAXINT, and we now prevent you from doing this, see 
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/2.2/breaking_21_search_changes.html#_from_size_limits)
&gt; 
&gt; ```
&gt; Using TimeAllowed in solr to 30 Seconds we could be sure that the
&gt; user will get data and that the query will terminate in 30 seconds.
&gt; ```
&gt; 
&gt; This doesn't ensure you get the right results, just some results.
&gt; 
&gt; This conversation is definitely one for the forums, and not the issues 
&gt; list.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elastic/elasticsearch/issues/16410#issuecomment-184277606.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout cloud_azure/15_index_creation/Test the smb_mmap_fs directory wrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16409</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_2x_windows-2012-r2/536/

Failure:

```
java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{"cluster_name":"prepare_release","status":"yellow","timed_out":true,"number_of_nodes":1,"number_of_data_nodes":1,"active_primary_shards":1,"active_shards":1,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":1,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":50.0}]
    at __randomizedtesting.SeedInfo.seed([17258535D2F76202:9F71BAEF7C0B0FFA]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:103)
    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:375)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
```

Sadly it doesn't reproduce.

@dakrone I'm assigning this to you as you have recently added the test.
</description><key id="130988522">16409</key><summary>Timeout cloud_azure/15_index_creation/Test the smb_mmap_fs directory wrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">bleskes</reporter><labels /><created>2016-02-03T10:48:11Z</created><updated>2016-02-03T14:43:46Z</updated><resolved>2016-02-03T14:43:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-03T14:43:46Z" id="179271518">I pushed a fix for this here: https://github.com/elastic/elasticsearch/commit/59a44b0a012e5046165a553a50cfbc23eeeeb995
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"fields" is missing in hit if script field returns a null value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16408</link><project id="" key="" /><description>I use Elasticsearch 2.2 &amp; 1.7.

With this query:

``` json
{
  "script_fields": {
    "test": {
      "script": "return null"
    }
  }
}
```

Elasticsearch 1.7 returns:

``` json
{  
  "_index":"xxx",
  "_type":"yyy",
  "_id":"51a79adcb5d07a18fc522160",
  "_score":1,
  "fields":{  
    "test":[  
      null
    ]
  }
}
```

Elasticsearch 2.2 returns:

``` json

{  
  "_index":"xxx",
  "_type":"yyy",
  "_id":"51a79adcb5d07a18fc522160",
  "_score":1
}
```

Is it a bug or a feature?
</description><key id="130987832">16408</key><summary>"fields" is missing in hit if script field returns a null value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierrre</reporter><labels><label>:Scripting</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-02-03T10:44:06Z</created><updated>2016-06-17T20:41:25Z</updated><resolved>2016-06-17T20:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-11T00:31:31Z" id="182656247">This is the result of #8592. If we want to support `null` as a valid return value from scripts, we would need to remove the first `if` condition added there. @jpountz thoughts?
</comment><comment author="jpountz" created="2016-02-11T13:37:33Z" id="182867043">@rjernst Sounds good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unneeded quoting when adding tests.jvm.argline to mvn reproduction line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16407</link><project id="" key="" /><description>&#8230;
Current it results in double quotes that gives you the wrong thing:

```
mvn verify -Pdev -Dskip.unit.tests...  -Dtests.jvm.argline=""-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops -Djava.net.preferIPv4Stack=true"" ....
```
</description><key id="130987226">16407</key><summary>Remove unneeded quoting when adding tests.jvm.argline to mvn reproduction line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-03T10:40:36Z</created><updated>2016-02-03T12:39:26Z</updated><resolved>2016-02-03T12:39:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-03T10:40:47Z" id="179158669">@dadoonet can you take a look?
</comment><comment author="jasontedor" created="2016-02-03T10:45:11Z" id="179159490">Relates #15048
</comment><comment author="jasontedor" created="2016-02-03T10:46:28Z" id="179159736">LGTM.
</comment><comment author="bleskes" created="2016-02-03T10:46:49Z" id="179159808">@jasontedor you have the memory of an elephant :tongue:  
</comment><comment author="dadoonet" created="2016-02-03T12:32:31Z" id="179198136">+1 to merge
</comment><comment author="bleskes" created="2016-02-03T12:39:26Z" id="179202208">merged...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document network changes in 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16406</link><project id="" key="" /><description>Hi,

I have a 2.1 cluster, with default `network.host` configuration and a custom `network.publish_host`.

After upgrading to `2.2.0` the cluster didn't start.

I've had to change from : 

```
network.publish_host: 123.456.789.123
```

to

```
network.host: ["127.0.0.1", "123.456.789.123"]
network.publish_host: ["123.456.789.123"]
```

It'd be great it this were documented : 
- in the changelog, with instruction for upgrading
- in the documentation

That said, congratulations for this release. Elasticsearch is awesome. :clap: 
</description><key id="130985143">16406</key><summary>Document network changes in 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels><label>:Network</label><label>feedback_needed</label></labels><created>2016-02-03T10:32:15Z</created><updated>2016-03-01T13:46:21Z</updated><resolved>2016-03-01T13:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schmorgs" created="2016-02-10T09:40:06Z" id="182279458">Hi, I have the exact same problem.  Settings worked in 2.1.0 but broke in 2.2.0

I tried your suggestion but in my case, I only want ES to listen on loopback as we have it behind a reverse proxy.  When I add both interfaces to network.host, it means the port is open externally as well.

If I then override network.bind_host to 127.0.0.1 only, it breaks the cluster again.

This breaks : 

```
network.host: ["127.0.0.1","1.2.3.4"]
network.publish_host: ["1.2.3.4"]
transport.bind_host: 1.2.3.4
network.bind_host: 127.0.0.1
http.port: 9200
```

This works but means ES listens on external interface which is less secure (in our case) : 

```
network.host: ["127.0.0.1","1.2.3.4"]
network.publish_host: ["1.2.3.4"]
transport.bind_host: 1.2.3.4

http.port: 9200
```

Looks like this change is the reason : https://github.com/elastic/elasticsearch/pull/13954 
</comment><comment author="clintongormley" created="2016-02-13T18:31:35Z" id="183718817">Hi @jlecour 

The breaking change is documented in the breaking changes list: https://www.elastic.co/guide/en/elasticsearch/reference/2.2/breaking_20_network_changes.html and the network setup docs have been greatly improved here: https://www.elastic.co/guide/en/elasticsearch/reference/2.2/modules-network.html#advanced-network-settings

What do you think is still missing?
</comment><comment author="clintongormley" created="2016-02-13T18:32:19Z" id="183718889">@schmorgs you seem to have misunderstood these settings (if i understand you correctly).  If Elasticsearch binds only to localhost, then it is unable to communicate with nodes on other servers.
</comment><comment author="schmorgs" created="2016-02-13T20:15:55Z" id="183746939">Maybe I have, but the intention is that ES will only listen on loopback for incoming http requests. I then then sit ES:9200 behind a reverse proxy to control incoming access.
In 2.1.0 it worked as desired but in 2.2.0 won't start up. I've been given a potential solution elsewhere which is to set http.publish_host and I haven't tried that yet but will update if it works

Trevor

On 13 Feb 2016, 18:34 +0000, Clinton Gormleynotifications@github.com, wrote:

&gt; @schmorgs(https://github.com/schmorgs)you seem to have misunderstood these settings (if i understand you correctly). If Elasticsearch binds only to localhost, then it is unable to communicate with nodes on other servers.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/16406#issuecomment-183718889).
</comment><comment author="clintongormley" created="2016-02-13T20:41:54Z" id="183752561">@schmorgs in which case you want to set `http.host`: https://www.elastic.co/guide/en/elasticsearch/reference/2.2/modules-http.html
</comment><comment author="clintongormley" created="2016-03-01T13:46:21Z" id="190729384">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BalancedShardsAllocator mutates keys in HashMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16405</link><project id="" key="" /><description>The shard balancer currently relies on model nodes and model indices to simulate moving shards around in the cluster. Each model node contains a set of model indices. A model index represents a set of shards of the same index that are allocated to the node that owns the model index. Currently model indices store their shards in a hash map.

First observation: allocation decisions stored as map values are actually never used, and can be removed.
Second observation: ShardRouting instances in the shard balancer are mutable and are modified while being keys in the map of the model index. Mutating a key in a hashmap is dangerous as the element can then not be found anymore. Luckily, for now the map is not accessed by key after an instance in it is mutated. This is, however, all but clear from the current implementation. I looked at it in detail and it works, but only for the following reasons:
- in the method `tryRelocateShard`: After shard is added to the map, its state is changed from started to relocated but then it is not considered anymore for the containsShard() method and removeShard() method in subsequent calls of tryRelocateShard as we pass only started shards to these methods here.
- in the method `allocateUnassigned`: After shard is added to the map, its state is changed from unassigned to initializing. This shard is then not considered in this method anymore.
- in the `move` method: Safe as it removes shard routing from the map before changing it.

Although this works for now, it is very fragile and has high chances of future bugs.
</description><key id="130975897">16405</key><summary>BalancedShardsAllocator mutates keys in HashMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2016-02-03T10:01:28Z</created><updated>2016-05-10T19:28:55Z</updated><resolved>2016-05-10T19:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-05-10T19:28:55Z" id="218264596">Fixed in #17028 and #17821
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16404</link><project id="" key="" /><description /><key id="130975289">16404</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarunsuthar</reporter><labels /><created>2016-02-03T09:59:05Z</created><updated>2016-02-03T09:59:32Z</updated><resolved>2016-02-03T09:59:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>request body search better cover all parameters uri search support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16403</link><project id="" key="" /><description>currently, there are 3 (as far as I know) paramters that uri search support but request body search not:
    track_scores
    timeout
    terminate_after
better make them  all supported by request body search, which could make the API more consistent.
</description><key id="130967384">16403</key><summary>request body search better cover all parameters uri search support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-02-03T09:31:14Z</created><updated>2016-02-12T11:17:06Z</updated><resolved>2016-02-12T11:17:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-12T11:17:06Z" id="183282352">This is solved in master by https://github.com/elastic/elasticsearch/pull/13752
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>it can not start the shard without the translog file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16402</link><project id="" key="" /><description>I'm tring to start a shard without the translog files or with the damaged translog files using elasticsearch 2.0,but it's failed and print some exception,such as:

```
NoSuchFileException[/data/.../.../{index}/0/translog/translog.ckp
```

or

```
NotSerializableExceptionWrapper[/data/.../.../{index}/0/translog/translog.ckp
```

I think the translog from the shard is invasive too strong with the index's shard,so a shard has data,but it can't start just because of the lacking translog files.
</description><key id="130934269">16402</key><summary>it can not start the shard without the translog file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Macln</reporter><labels /><created>2016-02-03T07:04:56Z</created><updated>2016-02-04T03:53:42Z</updated><resolved>2016-02-03T07:43:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-03T07:43:07Z" id="179070544">Indeed we fail starting a shard if it misses the necessary translog files. The reason for this that opening shard will amount to loss of data. Note that we rely on another shard copy to take over, just like we do in any case of corruption. Can you elaborate on what made you delete the translog file? 

I'm closing this issue as things work as intended. We can continue the discussion and if it turns out something is wrong, we can surely reopen it.
</comment><comment author="Macln" created="2016-02-03T07:59:15Z" id="179076770">@bleskes this problem can't deal with it? I think not only because of missing the translog files or damaged translog files which be known as notSerializableExceptionWrapper files that the index's shard cant 't start,just those shards possess data.
why can't fix the translog file,if the translog files is missing or damaged when starting the shards.
</comment><comment author="bleskes" created="2016-02-03T08:03:21Z" id="179078224">I&#8217;m not sure  I follow you completely. The high level thinking is that the translog stores data just like lucene and any corruption there is treated the same way as corruption elsewhere. If you have another copy (i.e., replica) ES will recover on it&#8217;s own. Otherwise you sadly need to reindex.

&gt; On 03 Feb 2016, at 08:59, Macln notifications@github.com wrote:
&gt; 
&gt; @bleskes this problem can't deal with it? I think not only because of missing the translog files or damaged translog files which be known as notSerializableExceptionWrapper files that the index's shard cant 't start,just those shards possess data.
&gt; why can't fix the translog file,if the translog files is missing or damaged when starting the shards.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="bleskes" created="2016-02-03T14:02:03Z" id="179250246">People have reminded me that we do have an escape route for these cases - you can set `index.engine.force_new_translog` to true on the index , wait for it to recover and reset back to false. Note though that leaving it on true is dangerous - if the cluster shutsdown and restarts it will ignore the translog when recovering (and loose data).
</comment><comment author="Macln" created="2016-02-04T03:53:42Z" id="179608268">@bleskes OK,thanks,I see,I'm going to try.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"query" around "match" causes ConstantQuery and ignores boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16401</link><project id="" key="" /><description>I'm running on ES 2.1.1.

The following query

```
GET /.../.../_search?explain=true
{
"fields": [], 
"query": { 
    "bool": {
        "should": [
            {"match": {
                "primary_skills":{
                    "query": "communication skills",
                    "boost": 3
            }}}
        ]
    }}}
```

works as expected.

But the same query, with an extra (redundant) `query` around `match` simply causes `ConstantScore` query and doesn't take the boost into account.

```
GET /.../.../_search?explain=true
{
    "fields": [], 
    "query": { 
        "bool": {
            "should": [
                {"query": {"match": {
                    "primary_skills":{
                        "query": "communication skills",
                        "boost": 3
                }}}}
    ]
   }}}
```

shows 

```
"description": "ConstantScore((primary_skills:communication primary_skills:skills)^3.0), product of:",
                           "details": [
                              {
                                 "value": 1,
                                 "description": "boost",
                                 "details": []
                              },
```

in the `explain` output.
</description><key id="130921450">16401</key><summary>"query" around "match" causes ConstantQuery and ignores boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asldevi</reporter><labels /><created>2016-02-03T05:58:58Z</created><updated>2016-02-03T11:53:22Z</updated><resolved>2016-02-03T11:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-03T11:53:22Z" id="179184601">That's because the `query` element is a remnant from 1.x, where it was the query filter (see https://www.elastic.co/guide/en/elasticsearch/reference/1.7/query-dsl-query-filter.html)  (ie it allowed you to use a query in filter context).

Queries and filters are now the same thing so the `query` filter has no purpose - it has been left there for bwc reasons, and is removed in 3.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene 5.5.0-snapshot-4de5f1d</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16400</link><project id="" key="" /><description>I disabled one polygon test (https://github.com/elastic/elasticsearch/issues/16399) and re-enabled index backwards compatibility tests (https://github.com/elastic/elasticsearch/issues/16373).
</description><key id="130904962">16400</key><summary>Upgrade to lucene 5.5.0-snapshot-4de5f1d</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T04:08:40Z</created><updated>2016-02-08T09:27:40Z</updated><resolved>2016-02-03T11:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-03T07:48:44Z" id="179071599">LGTM. Thanks @rmuir 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoPolygonQueryBuilderTests makes illegal minLon value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16399</link><project id="" key="" /><description>When i upgrade lucene, things are pickier and the test fails. I'd rather @nknize or @mikemccand look into this as I know they have battle scars already.

FYI looks like this (I will just disable the test):

```
==&gt; Test Info: seed=8E1FA201BA08A5B8; jvms=4; suites=641
Suite: org.elasticsearch.index.query.GeoPolygonQueryBuilderTests
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=8E1FA201BA08A5B8 -Dtests.class=org.elasticsearch.index.query.GeoPolygonQueryBuilderTests -Dtests.method="testToQuery" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=be-BY -Dtests.timezone=America/Panama
ERROR   0.04s J0 | GeoPolygonQueryBuilderTests.testToQuery &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.IllegalArgumentException: invalid minLon -180.000001
   &gt;    at __randomizedtesting.SeedInfo.seed([8E1FA201BA08A5B8:79E4A03FCB8B6052]:0)
   &gt;    at org.apache.lucene.util.GeoRect.&lt;init&gt;(GeoRect.java:29)
   &gt;    at org.apache.lucene.util.GeoUtils.polyToBBox(GeoUtils.java:221)
   &gt;    at org.apache.lucene.search.GeoPointInPolygonQuery.&lt;init&gt;(GeoPointInPolygonQuery.java:60)
   &gt;    at org.elasticsearch.index.query.GeoPolygonQueryBuilder.doToQuery(GeoPolygonQueryBuilder.java:152)
   &gt;    at org.elasticsearch.index.query.AbstractQueryBuilder.toQuery(AbstractQueryBuilder.java:78)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.testToQuery(AbstractQueryTestCase.java:516)
   &gt;    at org.elasticsearch.index.query.GeoPolygonQueryBuilderTests.testToQuery(GeoPolygonQueryBuilderTests.java:115)
   &gt;    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="130902326">16399</key><summary>GeoPolygonQueryBuilderTests makes illegal minLon value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Geo</label><label>test</label></labels><created>2016-02-03T03:47:10Z</created><updated>2016-02-15T19:47:20Z</updated><resolved>2016-02-15T19:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-02-04T06:57:57Z" id="179677726">Thank @rmuir  I pushed a fix to https://github.com/apache/lucene-solr/commit/62dfc815b030ca051379a12061fb0a9aa98ca09c
</comment><comment author="clintongormley" created="2016-02-13T23:37:36Z" id="183771603">@nknize is this closed by https://github.com/elastic/elasticsearch/pull/16615 ?
</comment><comment author="nknize" created="2016-02-15T19:47:20Z" id="184360894">Closed by #16615 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initial refactoring for completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16398</link><project id="" key="" /><description>This PR implements serialization methods for CompletionSuggestionBuilder based on the suggest refactoring work started in https://github.com/elastic/elasticsearch/pull/16241 and improves test coverage. This simplifies CompletionSuggestionBuilder by making fuzzy, regex and context queries implement their own serialization methods.
</description><key id="130895908">16398</key><summary>Initial refactoring for completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-02-03T03:02:56Z</created><updated>2016-02-10T21:32:52Z</updated><resolved>2016-02-10T21:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-02-03T12:09:54Z" id="179190401">@areek looks great, I left a few small comments but other than that tests look good and also pulling out all those separate classes makes things much easier to understand I think.
</comment><comment author="areek" created="2016-02-10T05:15:30Z" id="182202352">@cbuescher Thanks for the review. I addressed all your comments. 
`CompletionSuggestionBuilder#innerFromXContent` has not been implemented yet. We should clean up the parser in `CompletionSuggestParser` to use CompletionSuggestionBuilder as the consumer and fix setters with multiple params [(prefix, fuzzy), (prefix, regex), etc.] before.
</comment><comment author="cbuescher" created="2016-02-10T10:31:23Z" id="182301191">@areek thanks, left one minor comment and I'm getting a few test failures (StringDistanceImplTests, TermSuggestionBuilderTests, CompletionSuggesterBuilderTests), some of which might already be fixed on the feature branch. 
Otherwise LGTM. Implementing parsing via innerFromXContent can be another PR, or did you want to add that here?
</comment><comment author="areek" created="2016-02-10T18:46:39Z" id="182526019">Thanks @cbuescher for looking, I think we should merge this in after investigating the test failures and implement parsing via innerFromXContent for CompletionSuggestionBuilder in a subsequent PR. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es-2.2 is unconfigurable owing to use of unsupported es.default.config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16397</link><project id="" key="" /><description>The manifest code below fails to  configure es-2.2 correctly.

```
        class { 'elasticsearch':
                datadir =&gt; '/var/lib/elasticsearch-data',
                java_install =&gt; true,
                package_url =&gt; 'https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.2.0/elasticsearch-2.2.0.deb',
                config =&gt; { 'cluster.name' =&gt; 'cluster_graphana' }
        }

        elasticsearch::instance { 
                'es-01': 
                 config =&gt; { 'node.name' =&gt; 'cluster_graphana' }
        }

```

Above produces following 

```
cat  /lib/systemd/system/elasticsearch-es-01.service 
[Unit]
Description=Starts and stops a single elasticsearch instance on this system
Documentation=http://www.elasticsearch.org

[Service]
Type=forking
EnvironmentFile=/etc/default/elasticsearch-es-01
User=elasticsearch
Group=elasticsearch
PIDFile=/var/run/elasticsearch/elasticsearch-es-01.pid
ExecStart=/usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch-es-01.pid -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR
# See MAX_OPEN_FILES in sysconfig
LimitNOFILE=65535
# See MAX_LOCKED_MEMORY in sysconfig, use "infinity" when MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true

LimitMEMLOCK=

# Shutdown delay in seconds, before process is tried to be killed with KILL (if configured)
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target

```

On running the ExecStart as userid elasticsearch below error comes.

```
/usr/share/elasticsearch/bin/elasticsearch -p /var/run/elasticsearch/elasticsearch-es-01.pid -Des.default.config=/etc/elasticsearch/es-01/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch/es-01 -Des.default.path.data= -Des.default.path.work= -Des.default.path.conf=/etc/elasticsearch/es-01

[2016-02-03 07:50:07,654][INFO ][bootstrap                ] es.default.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.

```
</description><key id="130891991">16397</key><summary>es-2.2 is unconfigurable owing to use of unsupported es.default.config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmallah</reporter><labels /><created>2016-02-03T02:40:12Z</created><updated>2016-02-03T03:36:10Z</updated><resolved>2016-02-03T03:36:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmallah" created="2016-02-03T03:36:10Z" id="178983275">issue opened in wrong place . I will open in https://github.com/elastic/puppet-elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard state action request logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16396</link><project id="" key="" /><description>This commit modifies the string representation of a shard state action
request. The issue being addressed is that the previous logging would
log `failure: [Unknown]` for shard started actions but this just leads
to confusion that there is a failure but its cause is unknown.
</description><key id="130891373">16396</key><summary>Shard state action request logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T02:35:32Z</created><updated>2016-02-08T09:30:47Z</updated><resolved>2016-02-03T11:53:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T02:38:04Z" id="178965116">See #16391 for an example where there might be some confusion that the shard started request is indicating a failure:

&gt; I'm also seeing this in the logs:
&gt; 
&gt; ```
&gt; [2016-02-02 16:05:12,166][DEBUG][cluster.action.shard     ] [George Stacy] [analytics_test_201602][4] sending shard started for [analytics_test_201602][4], node[OcKoG1smQ0a__zSHxix33A], [P], v[1], s[INITIALIZING], a[id=ZgJoAG_vRGO6pVamLpHOHg], unassigned_info[[reason=INDEX_CREATED], at[2016-02-02T23:05:12.087Z]], expected_shard_size[7404], indexUUID [_WBlFe96TRO0kbG7S3ZgDg], message [after recovery from store], failure [Unknown]
&gt; ```
</comment><comment author="bleskes" created="2016-02-03T07:46:21Z" id="179071268">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix calling ensureOpen() on the wrong directory (master forwardport)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16395</link><project id="" key="" /><description>Also removes ensureCanWrite since this already passes the
TRUNCATE_EXISTING flag when opening.

Adds a REST test that fails without this fix due to the classloader
isolation.

Additionally, move SmbDirectoryWrapper into an elasticsearch package instead of a lucene one, so this would be found at compile time instead of runtime.

Forward-port of #16383
</description><key id="130860170">16395</key><summary>Fix calling ensureOpen() on the wrong directory (master forwardport)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Plugin Store SMB</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-03T00:01:37Z</created><updated>2016-02-21T20:22:31Z</updated><resolved>2016-02-03T15:11:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-03T01:25:46Z" id="178943889">+1, thanks again for cleaning all this up @dakrone 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suppress checkstyle on generated files in painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16394</link><project id="" key="" /><description>Closes #16387
</description><key id="130858585">16394</key><summary>Suppress checkstyle on generated files in painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T23:51:24Z</created><updated>2016-02-03T03:04:22Z</updated><resolved>2016-02-03T03:03:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T00:13:45Z" id="178902067">LGTM. Thanks for doing this.
</comment><comment author="nik9000" created="2016-02-03T01:43:58Z" id="178950728">@jdconrad is this ok?
</comment><comment author="jdconrad" created="2016-02-03T02:54:14Z" id="178970580">@nik9000 Great!  Thanks for fixing this.
</comment><comment author="nik9000" created="2016-02-03T03:04:22Z" id="178975244">Thanks for the reviews @jasontedor and @jdconrad! I'm glad I could help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove long lines from ElasticsearchException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16393</link><project id="" key="" /><description>Mostly just wrapping the exception list.

Also:
- Reworks the docs on ElasticsearchExceptionHandle
- Removes long lines from ExceptionSerializationTests
  - Switches one method from arrow shaped to early returns
  - Adds line breaks
</description><key id="130850406">16393</key><summary>Remove long lines from ElasticsearchException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T23:18:05Z</created><updated>2016-02-03T14:33:50Z</updated><resolved>2016-02-03T14:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-03T08:18:22Z" id="179082556">LGTM. Thanks @nik9000 
</comment><comment author="nik9000" created="2016-02-03T14:33:48Z" id="179267297">Rebased relatively cleanly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tribe node fails to start with non default port settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16392</link><project id="" key="" /><description>I'm trying to run tribe node using elasticsearch 2.2.0. I have cluster of 2 machines with the following config:

&lt;pre&gt;
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/
cluster.name: logstash-data
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["10.16.1.32", "10.16.1.75"]
&lt;/pre&gt;

On another machine I run 2 elasticsearch instances with following configs:

&lt;pre&gt;
transport.tcp.port: 9310
http.port: 9210
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/
cluster.name: logstash-kibana
&lt;/pre&gt;

and

&lt;pre&gt;
#transport.tcp.port: 9301
#http.port: 9201
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/

tribe:
    data:
        cluster.name: logstash-data
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.16.1.32", "10.16.1.75"]
        network.host: 0.0.0.0
    kibana:
        cluster.name: logstash-kibana
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["127.0.0.1:9310"]
        network.host: 0.0.0.0
&lt;/pre&gt;

This config works ok. But if I'll uncomment custom port settings I'm getting following exception:

&lt;pre&gt;
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Exception in thread "main" BindTransportException[Failed to bind to [9300-9400]]; nested: ChannelException[Failed to bind to: /0.0.0.0:9400]; nested: AccessControlException[access denied ("java.net.SocketPermission" "localhost:9400" "listen,resolve")];
Likely root cause: java.security.AccessControlException: access denied ("java.net.SocketPermission" "localhost:9400" "listen,resolve")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkListen(SecurityManager.java:1131)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:221)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Refer to the log for complete error details.
&lt;/pre&gt;
</description><key id="130847921">16392</key><summary>tribe node fails to start with non default port settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kt97679</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2016-02-02T23:05:41Z</created><updated>2016-11-14T20:09:45Z</updated><resolved>2016-11-14T20:09:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-11T07:19:53Z" id="182744878">The problem here is the tribe nodes create an internal node for each cluster, and bind to the configured port for that client node. When you specify a custom ports for http and transport, security manager is configured with those ports, instead of the default ranges, which for transport are 9300-9400. But then the tribe client nodes continue to use the default ports, and fail when attempting to bind.

The only way I can see to fix this is to add knowledge about tribe settings to `Security.addBindPermissions`. /cc @rmuir 
</comment><comment author="CVTJNII" created="2016-03-28T23:29:46Z" id="202624540">So the problem isn't that the values differ from the standard, it's that the values are pinned at all correct?  To mitigate this I attempted to pin back to the default ports of 9200 for http and 9300 for transport but still received the exception.  If the value is pinned the security manager and clients all try and use the pinned port, which fail, whereas if the value is unset the security manager uses the lowest free port, which is the default, and the clients use the subsequent ports in the ranges.  Is my understanding correct?

EDIT: No.  The port value must be a range to accommodate the clients, and the tribe node will start with a range that starts with the default, but will fail to start if the range does not start from the default.

``` yaml
http.port: 9500-9502
transport.tcp.port: 9600-9602
```

```
Exception in thread "main" BindTransportException[Failed to bind to [9300-9400]]; nested: ChannelException[Failed to bind to: /172.17.0.17:9400]; nested: AccessControlException[access denied ("java.net.SocketPermission" "localhost:9400" "listen,resolve")];
Likely root cause: java.security.AccessControlException: access denied ("java.net.SocketPermission" "localhost:9400" "listen,resolve")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkListen(SecurityManager.java:1131)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:221)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Refer to the log for complete error details.
```
</comment><comment author="chinmoydas1" created="2016-05-12T09:22:36Z" id="218704495">Was there any solution to this issue? I am facing the same
</comment><comment author="rjernst" created="2016-05-15T03:05:16Z" id="219263499">@chinmoydas1 The only workaround at the moment is to use the default ports. I've marked this as adoptme.
</comment><comment author="nellicus" created="2016-08-10T09:25:20Z" id="238812989">+1
</comment><comment author="ajaytlabs" created="2016-09-14T06:55:04Z" id="246922708">I am facing the same problem
</comment><comment author="nellicus" created="2016-09-14T07:57:45Z" id="246935257">my tribe config

```
abonuccelli@w530 /opt/elk/TEST/tribe_test/tribeNode $ egrep '^[^#]+' elasticsearch-2.3.5/config/elasticsearch.yml 
cluster.name: tribe
network.host: 192.168.1.105
http.port: 9220
transport.tcp.port: 9320
tribe:
  A: 
    cluster.name: clusterA
    discovery.zen.ping.unicast.hosts: ["192.168.1.105:9300"]
    network.publish_host: 192.168.1.105
    transport.tcp.port: 9340
  B: 
    cluster.name: clusterB
    discovery.zen.ping.unicast.hosts: ["192.168.1.105:9301"]
    network.publish_host: 192.168.1.105
    transport.tcp.port: 9341
  M:
    cluster.name: monitoring
    discovery.zen.ping.unicast.hosts: ["192.168.1.105:9310"]
    network.publish_host: 192.168.1.105
    transport.tcp.port: 9342
marvel.enabled: false

```

was getting

```
./elasticsearch-2.3.5/bin/elasticsearch
[2016-09-14 09:51:06,229][INFO ][node                     ] [Venom] version[2.3.5], pid[5567], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:06,229][INFO ][node                     ] [Venom] initializing ...
[2016-09-14 09:51:06,740][INFO ][plugins                  ] [Venom] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:08,052][INFO ][node                     ] [Venom/A] version[2.3.5], pid[5567], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:08,052][INFO ][node                     ] [Venom/A] initializing ...
[2016-09-14 09:51:08,400][INFO ][plugins                  ] [Venom/A] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:08,679][INFO ][node                     ] [Venom/A] initialized
[2016-09-14 09:51:08,680][INFO ][node                     ] [Venom/B] version[2.3.5], pid[5567], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:08,680][INFO ][node                     ] [Venom/B] initializing ...
[2016-09-14 09:51:08,986][INFO ][plugins                  ] [Venom/B] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:09,189][INFO ][node                     ] [Venom/B] initialized
[2016-09-14 09:51:09,190][INFO ][node                     ] [Venom/M] version[2.3.5], pid[5567], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:09,190][INFO ][node                     ] [Venom/M] initializing ...
[2016-09-14 09:51:09,447][INFO ][plugins                  ] [Venom/M] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:09,593][INFO ][node                     ] [Venom/M] initialized
[2016-09-14 09:51:09,598][INFO ][node                     ] [Venom] initialized
[2016-09-14 09:51:09,599][INFO ][node                     ] [Venom] starting ...
[2016-09-14 09:51:09,670][INFO ][transport                ] [Venom] publish_address {192.168.1.105:9320}, bound_addresses {192.168.1.105:9320}
[2016-09-14 09:51:09,673][INFO ][discovery                ] [Venom] tribe/lINF7OL1S0yiuYt5nV154w
[2016-09-14 09:51:09,674][WARN ][discovery                ] [Venom] waited for 0s and no initial state was set by the discovery
[2016-09-14 09:51:09,691][INFO ][http                     ] [Venom] publish_address {192.168.1.105:9220}, bound_addresses {192.168.1.105:9220}
[2016-09-14 09:51:09,691][INFO ][node                     ] [Venom/A] starting ...
[2016-09-14 09:51:09,743][INFO ][node                     ] [Venom/A] stopping ...
[2016-09-14 09:51:09,745][INFO ][node                     ] [Venom/A] stopped
[2016-09-14 09:51:09,745][INFO ][node                     ] [Venom/A] closing ...
[2016-09-14 09:51:09,749][INFO ][node                     ] [Venom/A] closed
[2016-09-14 09:51:09,750][INFO ][node                     ] [Venom/B] closing ...
[2016-09-14 09:51:09,752][INFO ][node                     ] [Venom/B] closed
[2016-09-14 09:51:09,752][INFO ][node                     ] [Venom/M] closing ...
[2016-09-14 09:51:09,754][INFO ][node                     ] [Venom/M] closed
Exception in thread "main" BindTransportException[Failed to bind to [9340]]; nested: ChannelException[Failed to bind to: /192.168.1.105:9340]; nested: AccessControlException[access denied ("java.net.SocketPermission" "localhost:9340" "listen,resolve")];
Likely root cause: java.security.AccessControlException: access denied ("java.net.SocketPermission" "localhost:9340" "listen,resolve")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkListen(SecurityManager.java:1131)
    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:221)
    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
    at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Refer to the log for complete error details.
[2016-09-14 09:51:09,760][INFO ][node                     ] [Venom] stopping ...
[2016-09-14 09:51:09,772][INFO ][node                     ] [Venom] stopped
[2016-09-14 09:51:09,772][INFO ][node                     ] [Venom] closing ...
[2016-09-14 09:51:09,774][INFO ][node                     ] [Venom] closed
```

I was able to workaround adding a security exception (use this at your own risk)

```
abonuccelli@w530 /opt/elk/TEST/tribe_test $ cat tribeNode/elasticsearch-2.3.5/java.policy 
grant {
    permission java.net.SocketPermission "localhost:9340", "listen,resolve";
};
```

then

```
abonuccelli@w530 /opt/elk/TEST/tribe_test/tribeNode $ export ES_JAVA_OPTS=-Djava.security.policy=file:///opt/elk/TEST/tribe_test/tribeNode/elasticsearch-2.3.5/java.policy; ./elasticsearch-2.3.5/bin/elasticsearch
[2016-09-14 09:51:37,629][INFO ][node                     ] [Satannish] version[2.3.5], pid[5749], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:37,630][INFO ][node                     ] [Satannish] initializing ...
[2016-09-14 09:51:38,162][INFO ][plugins                  ] [Satannish] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:39,397][INFO ][node                     ] [Satannish/A] version[2.3.5], pid[5749], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:39,398][INFO ][node                     ] [Satannish/A] initializing ...
[2016-09-14 09:51:39,656][INFO ][plugins                  ] [Satannish/A] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:39,892][INFO ][node                     ] [Satannish/A] initialized
[2016-09-14 09:51:39,893][INFO ][node                     ] [Satannish/B] version[2.3.5], pid[5749], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:39,893][INFO ][node                     ] [Satannish/B] initializing ...
[2016-09-14 09:51:40,178][INFO ][plugins                  ] [Satannish/B] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:40,337][INFO ][node                     ] [Satannish/B] initialized
[2016-09-14 09:51:40,337][INFO ][node                     ] [Satannish/M] version[2.3.5], pid[5749], build[90f439f/2016-07-27T10:36:52Z]
[2016-09-14 09:51:40,337][INFO ][node                     ] [Satannish/M] initializing ...
[2016-09-14 09:51:40,599][INFO ][plugins                  ] [Satannish/M] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent], sites []
[2016-09-14 09:51:40,710][INFO ][node                     ] [Satannish/M] initialized
[2016-09-14 09:51:40,714][INFO ][node                     ] [Satannish] initialized
[2016-09-14 09:51:40,714][INFO ][node                     ] [Satannish] starting ...
[2016-09-14 09:51:40,778][INFO ][transport                ] [Satannish] publish_address {192.168.1.105:9320}, bound_addresses {192.168.1.105:9320}
[2016-09-14 09:51:40,780][INFO ][discovery                ] [Satannish] tribe/R-S-WQtmSvOLUInO-Cke7w
[2016-09-14 09:51:40,781][WARN ][discovery                ] [Satannish] waited for 0s and no initial state was set by the discovery
[2016-09-14 09:51:40,786][INFO ][http                     ] [Satannish] publish_address {192.168.1.105:9220}, bound_addresses {192.168.1.105:9220}
[2016-09-14 09:51:40,786][INFO ][node                     ] [Satannish/A] starting ...
[2016-09-14 09:51:40,794][INFO ][transport                ] [Satannish/A] publish_address {192.168.1.105:9340}, bound_addresses {192.168.1.105:9340}
[2016-09-14 09:51:40,797][INFO ][discovery                ] [Satannish/A] clusterA/RcPrSmbnRlmSh1jPShYQjA
^C[2016-09-14 09:51:45,560][INFO ][node                     ] [Satannish] stopping ...
[2016-09-14 09:51:45,584][INFO ][node                     ] [Satannish] stopped
[2016-09-14 09:51:45,584][INFO ][node                     ] [Satannish] closing ...
[2016-09-14 09:51:45,584][INFO ][node                     ] [Satannish/A] stopping ...
[2016-09-14 09:51:45,590][INFO ][node                     ] [Satannish/A] stopped
[2016-09-14 09:51:45,590][INFO ][node                     ] [Satannish/A] closing ...
[2016-09-14 09:51:45,595][INFO ][node                     ] [Satannish/A] closed
[2016-09-14 09:51:45,595][INFO ][node                     ] [Satannish/B] closing ...
[2016-09-14 09:51:45,598][INFO ][node                     ] [Satannish/B] closed
[2016-09-14 09:51:45,598][INFO ][node                     ] [Satannish/M] closing ...
[2016-09-14 09:51:45,601][INFO ][node                     ] [Satannish/M] closed
[2016-09-14 09:51:45,603][INFO ][node                     ] [Satannish] closed

```

cc @rjernst 
</comment><comment author="jasontedor" created="2016-11-13T20:13:51Z" id="260209318">I'm removing the adoptme label as I have a fix for this and will open a PR soon. 
</comment><comment author="jasontedor" created="2016-11-14T17:07:22Z" id="260396168">I opened #21546.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException with nested aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16391</link><project id="" key="" /><description>We are running a query that has nested aggregations.  We are experiencing sporadic failures with our test suite around this query.  

We are running 2.1.1.  Here is the version info:

```
{
  number: "2.1.1",
  build_hash: "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
  build_timestamp: "2015-12-15T13:05:55Z",
  build_snapshot: false,
  lucene_version: "5.3.1"
}
```

We are using a time-based index strategy (one index per month), controlled by a template.  The issue seems to be related to time.  If we mock out the current time in our tests so that all data is created in a single monthly index, the issue goes away.  

The query we are running looks like this:

```
{
  "size" : 0,
  "query" : {
    "filtered" : {
      "filter" : {
        "and" : [
          {
            "terms" : {
              "newsroom_id" : [
                "2165011d66f4115cca001815"
              ]
            }
          },
          {
            "range" : {
              "timestamp" : {
                "gte" : "2016-01-03T22:47:03.000Z",
                "lte" : "2016-02-02T22:47:03.000+00:00"
              }
            }
          }
        ]
      }
    }
  },
  "aggs" : {
    "post_id" : {
      "terms" : {
        "field" : "post_id",
        "order" : [
          {
            "opportunities_ae220c23&gt;value.sum" : "desc"
          },
          {
            "all_stats.sum" : "desc"
          }
        ],
        "size" : 0
      },
      "aggs" : {
        "opportunities_ae220c23" : {
          "filter" : {
            "term" : {
              "stage" : "ae220c23"
            }
          },
          "aggs" : {
            "value" : {
              "stats" : {
                "field" : "stats.opportunities"
              }
            }
          }
        },
        "all_stats" : {
          "stats" : {
            "field" : "stats.opportunities"
          }
        }
      }
    }
  }
}
```

In our test suite, we do the following:
- delete all the documents in the index
- create a bunch of new documents
- refresh the indexes
- run the query

This test seems to fail 5 out of 10 times, and we get different failures.  Sometimes we get some data back, but not all the data we are expecting.  Other times we get a `500` response back from ElasticSearch.

I turned up logging and here are a few samples of what I'm seeing:

```
[2016-02-02 15:47:04,003][DEBUG][action.search.type       ] [George Stacy] [analytics_test_201602][0], node[OcKoG1smQ0a__zSHxix33A], [P], v[2], s[STARTED], a[id=_-rGSdgvTtyeXZ1yynsypA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@793d7235]
RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException;
Caused by: java.lang.ArrayIndexOutOfBoundsException
[2016-02-02 15:47:04,003][DEBUG][action.search.type       ] [George Stacy] [analytics_test_201601][0], node[OcKoG1smQ0a__zSHxix33A], [P], v[2], s[STARTED], a[id=ynufly_CRLGMmPEfk0Q1pA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@793d7235] lastShard [true]
RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException;
Caused by: java.lang.ArrayIndexOutOfBoundsException
[2016-02-02 15:47:04,003][DEBUG][action.search.type       ] [George Stacy] All shards failed for phase: [query]
RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException;
Caused by: java.lang.ArrayIndexOutOfBoundsException
[2016-02-02 15:47:04,004][INFO ][rest.suppressed          ] /analytics_test_201601,analytics_test_201602/_search Params: {routing=2165011d66f4115cca001815, index=analytics_test_201601,analytics_test_201602}
Failed to execute phase [query], all shards failed; shardFailures {[OcKoG1smQ0a__zSHxix33A][analytics_test_201601][0]: RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException; }{[OcKoG1smQ0a__zSHxix33A][analytics_test_201602][0]: RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException; }
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:228)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ; nested: ArrayIndexOutOfBoundsException;
    at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:382)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
    at java.lang.Throwable.printStackTrace(Throwable.java:665)
    at java.lang.Throwable.printStackTrace(Throwable.java:721)
    at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
    at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
    at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
    at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
    at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
    at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
    at org.apache.log4j.Category.callAppenders(Category.java:206)
    at org.apache.log4j.Category.forcedLog(Category.java:391)
    at org.apache.log4j.Category.log(Category.java:856)
    at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo(Log4jESLogger.java:125)
    at org.elasticsearch.common.logging.support.AbstractESLogger.info(AbstractESLogger.java:90)
    at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:131)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:96)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:87)
    at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAction.java:316)
    ... 10 more
Caused by: java.lang.ArrayIndexOutOfBoundsException
```

```
RemoteTransportException[[George Stacy][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException[1];
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
    at org.elasticsearch.common.util.BigArrays$DoubleArrayWrapper.get(BigArrays.java:260)
    at org.elasticsearch.search.aggregations.metrics.stats.StatsAggegator.metric(StatsAggegator.java:135)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$Aggregation$2.compare(InternalOrder.java:200)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$Aggregation$2.compare(InternalOrder.java:197)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$CompoundOrder$CompoundOrderComparator.compare(InternalOrder.java:280)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$CompoundOrder$CompoundOrderComparator.compare(InternalOrder.java:266)
    at org.elasticsearch.search.aggregations.bucket.terms.support.BucketPriorityQueue.lessThan(BucketPriorityQueue.java:37)
    at org.elasticsearch.search.aggregations.bucket.terms.support.BucketPriorityQueue.lessThan(BucketPriorityQueue.java:26)
    at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:258)
    at org.apache.lucene.util.PriorityQueue.add(PriorityQueue.java:135)
    at org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:151)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:176)
    at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:142)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

I'm also seeing this in the logs:

```
[2016-02-02 16:05:12,166][DEBUG][cluster.action.shard     ] [George Stacy] [analytics_test_201602][4] sending shard started for [analytics_test_201602][4], node[OcKoG1smQ0a__zSHxix33A], [P], v[1], s[INITIALIZING], a[id=ZgJoAG_vRGO6pVamLpHOHg], unassigned_info[[reason=INDEX_CREATED], at[2016-02-02T23:05:12.087Z]], expected_shard_size[7404], indexUUID [_WBlFe96TRO0kbG7S3ZgDg], message [after recovery from store], failure [Unknown]
```

Is anybody experiencing similar issues?

For reference, the data we create for this test looks like this:

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 10,
    "successful": 10,
    "failed": 0
  },
  "hits": {
    "total": 5,
    "max_score": 1,
    "hits": [
      {
        "_index": "analytics_test_201601",
        "_type": "content_scoring_stats",
        "_id": "8e1db69dd42f8eece90b97f129537cdca7eabfd0e4110d9a5cfa9c3703bf0708",
        "_score": 1,
        "_routing": "2165011d66f4115cca001815",
        "_source": {
          "created_at": "2016-02-02T22:47:06.990Z",
          "updated_at": "2016-02-02T22:47:06.990Z",
          "job_id": "2bc0c0dd-5a38-42d5-8961-22510ae6fdeb",
          "request_id": null,
          "timestamp": "2016-01-31T22:47:06.990Z",
          "recorded_at": "2016-02-02T22:47:00.548+00:00",
          "newsroom_id": "2165011d66f4115cca001815",
          "post_id": "d0c672a0e5f0f66e906ccd90",
          "stage": "11494a78",
          "stats": {
            "opportunities": 5.5,
            "revenue": 100.5
          }
        }
      },
      {
        "_index": "analytics_test_201601",
        "_type": "content_scoring_stats",
        "_id": "0e6e49d68abe67150cee093c38fe2acdcec88a6491b862a3a80adcf348a4df1b",
        "_score": 1,
        "_routing": "2165011d66f4115cca001815",
        "_source": {
          "created_at": "2016-02-02T22:47:06.992Z",
          "updated_at": "2016-02-02T22:47:06.992Z",
          "job_id": "3a19c4e6-8446-43c4-a6b6-ed471411129f",
          "request_id": null,
          "timestamp": "2016-01-31T22:47:06.992Z",
          "recorded_at": "2016-02-02T22:47:00.548+00:00",
          "newsroom_id": "2165011d66f4115cca001815",
          "post_id": "bd2b0c5993413414f1f92f3d",
          "stage": "f89d57cb",
          "stats": {
            "opportunities": 10.2,
            "revenue": 24.3
          }
        }
      },
      {
        "_index": "analytics_test_201602",
        "_type": "content_scoring_stats",
        "_id": "2982db4c6f50ca5611f331f51474a882d32ea40b628db51eca1f8beed8484e35",
        "_score": 1,
        "_routing": "2165011d66f4115cca001815",
        "_source": {
          "created_at": "2016-02-02T22:47:06.995Z",
          "updated_at": "2016-02-02T22:47:06.995Z",
          "job_id": "406b92b1-2b05-4cb1-93af-7651021d25d5",
          "request_id": null,
          "timestamp": "2016-02-01T22:47:06.995Z",
          "recorded_at": "2016-02-02T22:47:00.548+00:00",
          "newsroom_id": "2165011d66f4115cca001815",
          "post_id": "d0c672a0e5f0f66e906ccd90",
          "stage": "11494a78",
          "stats": {
            "opportunities": 0.3,
            "revenue": 43.1
          }
        }
      },
      {
        "_index": "analytics_test_201602",
        "_type": "content_scoring_stats",
        "_id": "c7140c8a6ebb4422b5ab1c495c6afca517ccbd5ea284e3aa0d6755320cba0426",
        "_score": 1,
        "_routing": "2165011d66f4115cca001815",
        "_source": {
          "created_at": "2016-02-02T22:47:06.997Z",
          "updated_at": "2016-02-02T22:47:06.997Z",
          "job_id": "3ca37f8c-b143-46f7-916b-5fbd2f06c653",
          "request_id": null,
          "timestamp": "2016-02-01T22:47:06.997Z",
          "recorded_at": "2016-02-02T22:47:00.548+00:00",
          "newsroom_id": "2165011d66f4115cca001815",
          "post_id": "c766f88321f126c2309ce791",
          "stage": "f89d57cb",
          "stats": {
            "opportunities": 0.5,
            "revenue": 1.8
          }
        }
      },
      {
        "_index": "analytics_test_201602",
        "_type": "content_scoring_stats",
        "_id": "4b1cd5c7df3aa55879253021ef59ac73975736652763eef8f798ea50ac004ef5",
        "_score": 1,
        "_routing": "e2d34791522c86d9210673f2",
        "_source": {
          "created_at": "2016-02-02T22:47:06.999Z",
          "updated_at": "2016-02-02T22:47:06.999Z",
          "job_id": "56e1a19d-b8be-42ba-8912-aff1084bf347",
          "request_id": null,
          "timestamp": "2016-02-01T22:47:06.999Z",
          "recorded_at": "2016-02-02T22:47:00.548+00:00",
          "newsroom_id": "e2d34791522c86d9210673f2",
          "post_id": "c766f88321f126c2309ce791",
          "stage": "f89d57cb",
          "stats": {
            "opportunities": 10,
            "revenue": 30
          }
        }
      }
    ]
  }
}
```
</description><key id="130846404">16391</key><summary>ArrayIndexOutOfBoundsException with nested aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bwebster</reporter><labels><label>:Aggregations</label><label>:Nested Docs</label><label>bug</label><label>feedback_needed</label></labels><created>2016-02-02T22:56:48Z</created><updated>2016-05-12T11:54:20Z</updated><resolved>2016-05-12T11:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T02:42:14Z" id="178966149">Someone else will look at the core of the issue here but I just wanted to let you know that the message

&gt; I'm also seeing this in the logs:
&gt; 
&gt; ```
&gt; [2016-02-02 16:05:12,166][DEBUG][cluster.action.shard     ] [George Stacy] [analytics_test_201602][4] sending shard started for [analytics_test_201602][4], node[OcKoG1smQ0a__zSHxix33A], [P], v[1], s[INITIALIZING], a[id=ZgJoAG_vRGO6pVamLpHOHg], unassigned_info[[reason=INDEX_CREATED], at[2016-02-02T23:05:12.087Z]], expected_shard_size[7404], indexUUID [_WBlFe96TRO0kbG7S3ZgDg], message [after recovery from store], failure [Unknown]
&gt; ```

that you pointed out is fine, it's not a failure, it's just bad logging. In this case, a shard is being started after recovering. However, shard started and shard failure actions share the same request object and their shared string representation prints out `failure: [Unknown]` when there is no failure. It's bothered me for awhile, but seeing it here finally brought me to do something about it; I opened #16396. Sorry for the confusion!
</comment><comment author="bwebster" created="2016-02-03T05:07:21Z" id="179013457">Thanks for the explanation @jasontedor.  Good to know that the `failure: [Unknown]` isn't something to worry about.
</comment><comment author="bwebster" created="2016-02-03T23:50:31Z" id="179536306">Update: the test suite is now much more reliable.  The only difference is that the test data is no longer overlapping a month boundary.  All the test data is created in one index (for the month of February).

Seems to be related to the query needing to hit more than one index.
</comment><comment author="2e3s" created="2016-02-27T15:56:25Z" id="189673014">I have the same error, narrowed it if it helps. In short, the different parts are, query with the error:

``` json
  "aggregations": {
    "root": {
      "terms": {
        "field": "result",
        "order": [{"current&gt;amount": "desc"}]
      },
```

with no error

``` json
  "aggregations": {
    "root": {
      "terms": {
        "field": "result",
        "order": [{"current": "desc"}]
      },
```

it happens on a single index, a few records, quite a trivial config.
While trying it more I figured out that putting a record with this `curl -XPUT 'http://localhost:9200/sortbug/sortbug/c1t987654' -d '` with the first query makes the error and this `curl -XPUT 'http://localhost:9200/sortbug/sortbug/c1t98765' -d '` doesn't.
</comment><comment author="colings86" created="2016-03-29T14:06:20Z" id="202909397">This looks to be the same as https://github.com/elastic/elasticsearch/issues/17225 which has been fixed by https://github.com/elastic/elasticsearch/pull/17379 and should be available from version 2.3.1. Could you confirm whether you still see this on the latest 2.3 branch?
</comment><comment author="clintongormley" created="2016-05-12T11:54:20Z" id="218735451">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix ingest client put error test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16390</link><project id="" key="" /><description>The transport client wraps exceptions at different layers in the request lifetime, and there was a one-off in probing the exact `ElasticsearchParseException` that caused the chain to fail
</description><key id="130843199">16390</key><summary>fix ingest client put error test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T22:41:46Z</created><updated>2016-02-13T22:19:48Z</updated><resolved>2016-02-02T23:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-02-02T22:43:13Z" id="178870457">output from test

```
$ gradle :core:integTest -Dtests.seed=E616665F28EA444B -Dtests.class=org.elasticsearch.ingest.IngestClientIT -Dtests.method="testPutWithPipelineFactoryError" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvms=6 -Dtests.locale=he -Dtests.timezone=Africa/Djibouti
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:sourcesJar UP-TO-DATE
:buildSrc:signArchives SKIPPED
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.8
  OS Info               : Mac OS X 10.11.1 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_45 [Java HotSpot(TM) 64-Bit Server VM 25.45-b02]
:core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processResources
:core:classes
:core:jar
:test:framework:compileJava
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:test:framework:processResources
:test:framework:classes
:test:framework:jar
:core:compileTestJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processTestResources
:core:testClasses
:core:integTest
   [junit4] &lt;JUnit4&gt; says kaixo! Master seed: E616665F28EA444B
==&gt; Test Info: seed=E616665F28EA444B; jvm=1; suite=1
Suite: org.elasticsearch.ingest.IngestClientIT
  1&gt; [2016-02-02 14:41:25,347][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
  1&gt; [2016-02-02 14:41:25,347][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
  1&gt; [2016-02-02 14:41:30,031][WARN ][org.elasticsearch.cluster.service] [node_s2] failed to notify ClusterStateListener
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:420)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:596)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:761)
  1&gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$FilterRunnable.run(EsThreadPoolExecutor.java:227)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-02-02 14:41:30,213][WARN ][org.elasticsearch.cluster.service] [node_s2] failed to notify ClusterStateListener
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:420)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:596)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:761)
  1&gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$FilterRunnable.run(EsThreadPoolExecutor.java:227)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-02-02 14:41:30,316][WARN ][org.elasticsearch.cluster.service] [node_s2] failed to notify ClusterStateListener
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:420)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:596)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:761)
  1&gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$FilterRunnable.run(EsThreadPoolExecutor.java:227)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-02-02 14:41:30,405][WARN ][org.elasticsearch.cluster.service] [node_s2] failed to notify ClusterStateListener
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:420)
  1&gt;    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:596)
  1&gt;    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:761)
  1&gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor$FilterRunnable.run(EsThreadPoolExecutor.java:227)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
  1&gt;    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
Completed [1/1] in 5.33s, 1 test

==&gt; Test Summary: 1 suite, 1 test
   [junit4] JVM J0:     0.57 ..     6.35 =     5.78s
   [junit4] Execution time total: 6.37 sec.
   [junit4] Tests summary: 1 suite, 1 test

BUILD SUCCESSFUL

Total time: 1 mins 19.137 secs
```
</comment><comment author="talevy" created="2016-02-02T23:18:04Z" id="178880216">[latest commit](https://github.com/elastic/elasticsearch/commit/8b37827ac636b53e55f04c505d06e202e4535768) fixes the NPEs that were seen

rebased from master and now following test show as:

```
(fix_ingest_client_test)$ gradle :core:integTest -Dtests.seed=E616665F28EA444B -Dtests.class=org.elasticsearch.ingest.IngestClientIT
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:sourcesJar UP-TO-DATE
:buildSrc:signArchives SKIPPED
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.8
  OS Info               : Mac OS X 10.11.1 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_45 [Java HotSpot(TM) 64-Bit Server VM 25.45-b02]
:core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processResources UP-TO-DATE
:core:classes
:core:jar
:test:framework:compileJava
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:test:framework:processResources UP-TO-DATE
:test:framework:classes
:test:framework:jar
:core:compileTestJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processTestResources UP-TO-DATE
:core:testClasses
:core:integTest
   [junit4] &lt;JUnit4&gt; says kaixo! Master seed: E616665F28EA444B
==&gt; Test Info: seed=E616665F28EA444B; jvm=1; suite=1
Suite: org.elasticsearch.ingest.IngestClientIT
  1&gt; [2016-02-02 15:17:39,202][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
  1&gt; [2016-02-02 15:17:39,202][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
Completed [1/1] in 6.98s, 4 tests

==&gt; Test Summary: 1 suite, 4 tests
   [junit4] JVM J0:     0.59 ..     8.04 =     7.45s
   [junit4] Execution time total: 8.06 sec.
   [junit4] Tests summary: 1 suite, 4 tests

BUILD SUCCESSFUL

Total time: 1 mins 15.116 secs
```
</comment><comment author="jasontedor" created="2016-02-02T23:25:31Z" id="178883702">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_type has no mapping properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16389</link><project id="" key="" /><description>This has come up in the context of kibana:
# https://github.com/elastic/kibana/issues/5684

For new indicies created in ES 2.x, kibana sees _type as unindexed.
It looks like the mapping properties are empty. This seems to affect Kibana's ability to properly discover the mapping.

I have a feeling this is to do with the change not allowing mapping options for metafields like _type, _id etc.

I have provided examples below using the mapping API against a pre 2.x index and a post 2.x index both running inside ES 2.1

Pre 2.x index
/oldindex-2014.12.28/_mapping/logs/field/_type?include_defaults=true

```
{
  "oldindex-2014.12.28": {
    "mappings": {
      "logs": {
        "_type": {
          "full_name": "_type",
          "mapping": {
            "_type": {
              "store": false,
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}
```

New 2.x index:
/newindex_2-2015.01.01/_mapping/logs/field/_type?include_defaults=true

```
{
  "newindex_2-2015.01.01": {
    "mappings": {
      "logs": {
        "_type": {
          "full_name": "_type",
          "mapping": {

          }
        }
      }
    }
  }
}

```
</description><key id="130841363">16389</key><summary>_type has no mapping properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johncollaros</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-02-02T22:34:07Z</created><updated>2016-04-26T14:06:00Z</updated><resolved>2016-04-26T14:06:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T22:10:29Z" id="183765005">The metadata fields have been locked down and (mostly) are no longer configurable.  Mappings emitted by this API can't be roundtripped (ie used to configure the mapping for a new index) because the metadata fields won't accept these parameter.

I think the solution is for kibana to hard code the mapping for metadata fields.
</comment><comment author="johncollaros" created="2016-02-16T11:36:15Z" id="184644957">That's a bit of a shame because it breaks discovery of mappings.
How hard is it to have the mapping settings to be read-only for these meta-fields?
</comment><comment author="rashidkpc" created="2016-03-10T14:28:20Z" id="194871585">@clintongormley I'm not sure I agree here:

```
I think the solution is for kibana to hard code the mapping for metadata fields.
```

The problem is if the mappings for those fields ever change in any way, or the names change, or new ones are added. Kibana would have to keep the same list that Elasticsearch is already keeping, but do it manually. Its a really error prone process, as we've already proved :-)
</comment><comment author="brwe" created="2016-03-10T14:35:30Z" id="194874047">&gt; Mappings emitted by this API can't be roundtripped (ie used to configure the mapping for a new index) because the metadata fields won't accept these parameter.

Do people use the field mapping api for round tripping mappings or do they use regular mappings api? Because if Kibana uses field mapping api to get this kind of information but for round tripping people use mapping api then there is no conflict anyway.
</comment><comment author="rashidkpc" created="2016-03-10T14:37:49Z" id="194875593">I was under the impression that the field mapping API was read-only anyway
</comment><comment author="GlenRSmith" created="2016-04-25T21:50:26Z" id="214538185">Couldn't there be either a metadata API or an include_metadata parameter? Or some other method of exposure? "Self documenting" would be a good thing in general.
</comment><comment author="clintongormley" created="2016-04-26T14:06:00Z" id="214756412">While not changing the output for metadata fields, this change will solve the issue for Kibana: https://github.com/elastic/elasticsearch/issues/17750
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove DeDotProcessor from Ingest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16388</link><project id="" key="" /><description /><key id="130836085">16388</key><summary>remove DeDotProcessor from Ingest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T22:17:01Z</created><updated>2016-02-08T09:25:11Z</updated><resolved>2016-02-02T22:20:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-02T22:18:00Z" id="178857255">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Imports Automatically in Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16387</link><project id="" key="" /><description>The ANTLR generated files currently have star imports in them when the files are regenerated.  Need to make it part of the ANT task to replace these imports automatically with non-star imports.
</description><key id="130808959">16387</key><summary>Fix Imports Automatically in Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>:Scripting</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T20:33:21Z</created><updated>2016-02-13T22:20:08Z</updated><resolved>2016-02-03T03:03:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-02T20:38:10Z" id="178808848">We could also skip these files entirely with checkstyle.
</comment><comment author="nik9000" created="2016-02-02T20:38:34Z" id="178808960">I mean, we are using checkstyle to check this now. We could skip the automatically generated files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove detect_noop from REST spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16386</link><project id="" key="" /><description>Unless this should be supported as a query string parameter instead, right now it only works when specified in the body.
</description><key id="130808624">16386</key><summary>Remove detect_noop from REST spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:REST</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T20:32:21Z</created><updated>2016-02-08T09:20:32Z</updated><resolved>2016-02-02T20:41:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-02T20:37:35Z" id="178808718">LGTM. I'm sure this is my fault. Thanks for fixing it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix imports to not have wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16385</link><project id="" key="" /><description>Accidentally did not merge the final change to fix the imports to not have wildcards in the ANTLR generated files.  This fixes that.
</description><key id="130807504">16385</key><summary>Fix imports to not have wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T20:28:50Z</created><updated>2016-02-13T21:57:34Z</updated><resolved>2016-02-02T20:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-02T20:30:05Z" id="178806111">LGTM. Can you open an issue so we remember to fix antlr generation so it does not use wildcard imports?
</comment><comment author="nik9000" created="2016-02-02T20:30:17Z" id="178806223">LGTM

Is this a manual thing you have to do after running the ant step?
</comment><comment author="jdconrad" created="2016-02-02T20:31:34Z" id="178806878">This is manual right now.  I'm making an issue to automate this task as part of there regeneration cycle.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin cli: Improve maven coordinates detection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16384</link><project id="" key="" /><description>Identifying when a plugin id is maven coordinates is currently done by
checking if the plugin id contains 2 colons. However, a valid url could
have 2 colons, for example when a port is specified. This change adds
another check, ensuring the plugin id with maven coordinates does not
contain a slash, which only a url would have.

closes #16376
</description><key id="130805157">16384</key><summary>Plugin cli: Improve maven coordinates detection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T20:20:25Z</created><updated>2016-02-08T09:18:09Z</updated><resolved>2016-02-02T20:27:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-02T20:22:05Z" id="178802007">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix calling ensureOpen() on the wrong directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16383</link><project id="" key="" /><description>Also removes `ensureCanWrite` since this already passes the
`TRUNCATE_EXISTING` flag when opening.

Adds a REST test that fails without this fix due to the classloader
isolation.

Additionally, move `SmbDirectoryWrapper` into an elasticsearch package instead of a lucene one, so this would be found at compile time instead of runtime.
</description><key id="130803502">16383</key><summary>Fix calling ensureOpen() on the wrong directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Plugin Cloud Azure</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label></labels><created>2016-02-02T20:15:03Z</created><updated>2016-02-21T20:22:27Z</updated><resolved>2016-02-02T22:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-02T20:15:22Z" id="178798548">This is only for 2.x, since a lot of files were moved all over the place I will forward-port separately.
</comment><comment author="rmuir" created="2016-02-02T20:16:44Z" id="178799765">+1
</comment><comment author="dakrone" created="2016-02-03T00:04:04Z" id="178898247">Actually, I believe this is also a problem is 2.2.0, so I will backport this to the `2.2` branch so it's available in the 2.2.1 release (whenever that is)
</comment><comment author="rmuir" created="2016-02-03T01:26:22Z" id="178944317">It might be beneficial to consider https://github.com/elastic/elasticsearch/pull/16325 as well. IMO its a serious bug :)
</comment><comment author="dakrone" created="2016-02-03T16:09:07Z" id="179316506">@rmuir I agree, I'm going to backport it to 2.2.1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Extra String Concat Token</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16382</link><project id="" key="" /><description>Renamed no-semicolon tests to work when running gradle.  Also removed the ..= token entirely from the grammar as string concatenation now uses the overloaded += operator.
</description><key id="130797797">16382</key><summary>Remove Extra String Concat Token</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T19:55:25Z</created><updated>2016-04-05T11:07:15Z</updated><resolved>2016-02-02T19:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-02T19:57:18Z" id="178788378">LGTM
</comment><comment author="nik9000" created="2016-02-02T20:00:20Z" id="178789551">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow aggregations on parent document fields from a nested aggregation without changing scope using reverse nested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16381</link><project id="" key="" /><description>I think this could be a very practical solutions for whole set of issues where aggregation needs to be done on nested documents but be able to group on its parent document properties. It i snot the same as using reverse nested because reverse nested changes document scope to groupp and then it will needs to be changed back to nested again introducing double-counting.
Also from ease of use it would be a significant step forward comparing to using reverse nested just to be able to access parent document properties (even if were equivalent)

Here is an example I used in another issue:

Imagine having Teams that handle Requests of certain Priority. Each request may have multiple teams assigned to it and each team request assignment has number of hours allocated for this team on this request

``` javascript
request:{
  priority:1,
  teams:[
    {name:'team1', hours:10},
    {name:'team2', hours:20},
  ]
}
```

I need a report that gives me number of hours by team by priority. Doing nested aggregation on **teams.name** and then reverse nested to group on **priority** and then nested to sum **teams.hours** double-counts hours because second nesting on teams knows nothing about  upstream nesting as it is executed in context of request and as result it will lump hours for each team on request under the top level team aggregation 

If i could access "../priority" from teams nested document context I would not have to use reverse nested and lose my aggregation context and everything would have worked like a charm

please see also #16380 
</description><key id="130773655">16381</key><summary>Allow aggregations on parent document fields from a nested aggregation without changing scope using reverse nested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2016-02-02T18:31:52Z</created><updated>2016-02-13T21:51:10Z</updated><resolved>2016-02-13T21:51:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T21:51:10Z" id="183763230">Duplicate of https://github.com/elastic/elasticsearch/issues/16380
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested Aggregations - Filtering of child aggs on parent agg's key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16380</link><project id="" key="" /><description>Let me start with an example to explain the issue which is a major pain point for us when using nested aggregations.

Imagine having Teams that handle Requests of certain Priority. Each request may have multiple teams assigned to it and each team request assignment has number of hours allocated for this team on this request

``` javascript
request:{
  priority:1,
  teams:[
    {name:'team1', hours:10},
    {name:'team2', hours:20},
  ]
}
```

I need a report that gives me number of hours by team by priority. Doing nested aggregation on **teams.name** and then reverse nested to group on **priority** and then nested to sum **teams.hours** double-counts hours because second nesting on teams knows nothing about  upstream nesting as it is executed in context of request and as result it will lump hours for each team on request under the top level team aggregation. 

Please do not suggest :-) that I can aggregate on Priority first and then on Teams and then flip/collate my nested result. Yes I can but this example is a small portion of a report full of similar cases not to mention that it would not work in our generic solution where users can pick and choose any group by and metrics at any level of nesting

A **very practical**, generic, concise and easy to use  solution would be to allow aggregating on properties of owner document(s) from a nested aggregation without having to change context back to parent document buy using reverse nested aggregation. In our example as simple as doing terms sub aggregation on "../priority" from teams nested document context.

Another solution would be to support filtering expressions that are based on upstream aggregation values so that i can filter lowest teams nested aggregation calculating hours based on top level team aggregation. it could be useful for many scenarious but for this one it is a bit artificial. I think it would be  a great capability to be able to use parent agg bukets as input for child bucket filters etc 

Another, and in my opinion **pretty powerful** solution would be ability to flatten/de-normalize nested aggregation (like Cartesian join) prior to applying filters/aggregations. This will be much more useful in very many cases than aggregation on nested data as it is available now.

Sorry for a rant below please do read it:-)

As a person who has to deal with lots of nested data and produce complicated analytic on it I can tell you that this is the single most difficult part. After promoting elastic heavily instead of traditional star-schema based data-mart we suffer a lot trying to produce reports on nested data. One of selling point was that we can store complex data in its natural form and filter/search it, report/export it  and also produce analytic from it without creating dozens of fragmented single purpose de-normalized data-marts. As far as searching it works fairly well except for a similar issue of not being able to filter nested data when one of the criteria is on that nested data element and users do not wish to see nested records that do not match the criteria (this would have happened naturally and for free with relational database). With aggregation it is even worse - we cant escape this kind of double counting without all sorts of shenanigans. 
If only handling of nested fields were richer we could have gotten much further with it. You may say de-normalize all your nested relationships into separate indices. yes it could be done but it defeats the purpose and given lack of joins it will lead to enormous data duplication and won't be a generic solution anyways. I would love if elastic could do de-normalization on the fly on specific nested fields and apply filtering/aggregation logic to the denormalized dataset  

It (the nested de-normalization) would also be extremely useful for plain search scenario particularly when combined with ability to return selected snippets of both owner and nested objects 
</description><key id="130759616">16380</key><summary>Nested Aggregations - Filtering of child aggs on parent agg's key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-02-02T17:48:13Z</created><updated>2016-02-13T21:42:11Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-13T21:41:39Z" id="183760673">Hi @roytmana 

As you know, nested docs are independent of their parent docs, so there is no way to provide access to the parents from the nested docs in any way that is going to perform well.

&gt; Please do not suggest :-) that I can aggregate on Priority first and then on Teams and then flip/collate my nested result. 

Suggest that you design your data to fit with your access pattern?  It would seem the most obvious (and most efficient) thing to do, no?  

Your "practical" and "powerful" suggestions would be great, except that they don't fit at all with how aggregations work - it'd be a horrible and expensive hack to implement this.

If you want access to a value inside a document, then it needs to be added to that document at index time. eg as follows:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "teams": {
          "type": "nested"
        }
      }
    }
  }
}

PUT t/t/1
{
  "priority": 1,
  "teams": [
    {
      "name": "team1",
      "hours": 10,
      "priority": 1
    },
    {
      "name": "team2",
      "hours": 20,
      "priority": 1
    }
  ]
}

GET t/_search?size=0
{
  "aggs": {
    "teams": {
      "nested": {
        "path": "teams"
      },
      "aggs": {
        "team_name": {
          "terms": {
            "field": "teams.name"
          },
          "aggs": {
            "priority": {
              "terms": {
                "field": "teams.priority"
              },
              "aggs": {
                "hours": {
                  "sum": {
                    "field": "teams.hours"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

The one thing that _may_  be worth considering is allowing `copy_field` to add fields from the parent into each nested doc (a request I previously rejected), something like the following:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "priority": {
          "type": "integer",
          "copy_to": "teams.priority"
        },
        "teams": {
          "type": "nested"
        }
      }
    }
  }
}

PUT t/t/1
{
  "priority": 1,
  "teams": [
    {
      "name": "team1",
      "hours": 10
    },
    {
      "name": "team2",
      "hours": 20
    }
  ]
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch won't start on Mac OS 10.10.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16379</link><project id="" key="" /><description>Hi,
I've had a working install of Elasticsearch for some time now and it eventually stopped working.
I uninstalled it then reinstalled it with brew. Here's what happens: 

```
 ct@inferno ~  $ brew uninstall elasticsearch --force
Uninstalling elasticsearch... (50 files, 30.9M)
 ct@inferno ~  $ brew install elasticsearch
==&gt; Downloading https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distributi
Already downloaded: /Library/Caches/Homebrew/elasticsearch-2.1.1.tar.gz
==&gt; Caveats
Data:    /usr/local/var/elasticsearch/elasticsearch_ct/
Logs:    /usr/local/var/log/elasticsearch/elasticsearch_ct.log
Plugins: /usr/local/Cellar/elasticsearch/2.1.1/libexec/plugins/
Config:  /usr/local/etc/elasticsearch/
plugin script: /usr/local/Cellar/elasticsearch/2.1.1/libexec/bin/plugin

To have launchd start elasticsearch at login:
  ln -sfv /usr/local/opt/elasticsearch/*.plist ~/Library/LaunchAgents
Then to load elasticsearch now:
  launchctl load ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist
Or, if you don't want/need launchctl, you can just run:
  elasticsearch
==&gt; Summary
&#127866;  /usr/local/Cellar/elasticsearch/2.1.1: 50 files, 30.9M, built in 1 second
 ct@inferno ~  $ elasticsearch
[2016-02-02 11:34:03,823][INFO ][node                     ] [Vindaloo] version[2.1.1], pid[15869], build[40e2c53/2015-12-15T13:05:55Z]
[2016-02-02 11:34:03,823][INFO ][node                     ] [Vindaloo] initializing ...
Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.NoSuchFileException: /usr/local/var/lib/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:317)
    at java.nio.file.Files.newByteChannel(Files.java:363)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:380)
    at java.nio.file.Files.newInputStream(Files.java:108)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:302)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

I've searched for the likely root cause, to no avail.
Any idea ?
</description><key id="130738598">16379</key><summary>Elasticsearch won't start on Mac OS 10.10.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">edorgeville</reporter><labels /><created>2016-02-02T16:35:41Z</created><updated>2016-02-02T17:23:02Z</updated><resolved>2016-02-02T16:47:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-02T16:47:28Z" id="178682514">Do you have a stale plugin from an old installation sitting around and have your config pointing to that old plugin directory? That's what it looks like.

```
Likely root cause: java.nio.file.NoSuchFileException: /usr/local/var/lib/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties
```

The solution then is to alter your config to not point to this plugin location _or_ to remove the old plugins that are not compatible with Elasticsearch 2.1.1 from `/usr/local/var/lib/elasticsearch/plugins`. You can then install the appropriate version of the plugin after you've repaired this.

&gt; I've had a working install of Elasticsearch

What version? If you're coming from the 1.x line, I think the above explanation is what's going on. If not, let us know and we'll investigate further.
</comment><comment author="jasontedor" created="2016-02-02T17:07:17Z" id="178692320">I can replicate your issue confirming that my [theory](https://github.com/elastic/elasticsearch/issues/16379#issuecomment-178682514) is plausible:

```
11:59:34 3d [jason:~] $ brew install elasticsearch17
[...]
11:59:38 3d [jason:~] $ /usr/local/Cellar/elasticsearch17/1.7.4/bin/plugin --install elasticsearch/elasticsearch-analysis-icu/2.7.0
[...]
12:00:10 3d [jason:~] $ elasticsearch
[2016-02-02 12:00:13,537][INFO ][node                     ] [Ka-Zar] version[1.7.4], pid[88711], build[0d3159b/2015-12-15T11:25:18Z]
[2016-02-02 12:00:13,537][INFO ][node                     ] [Ka-Zar] initializing ...
[2016-02-02 12:00:13,603][INFO ][plugins                  ] [Ka-Zar] loaded [analysis-icu], sites []
[2016-02-02 12:00:13,631][INFO ][env                      ] [Ka-Zar] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [104.9gb], net total_space [232.6gb], types [hfs]
[2016-02-02 12:00:15,156][INFO ][node                     ] [Ka-Zar] initialized
[2016-02-02 12:00:15,158][INFO ][node                     ] [Ka-Zar] starting ...
[2016-02-02 12:00:15,242][INFO ][transport                ] [Ka-Zar] bound_address {inet[/127.0.0.1:9300]}, publish_address {inet[/127.0.0.1:9300]}
[2016-02-02 12:00:15,257][INFO ][discovery                ] [Ka-Zar] elasticsearch_jason/69KwMa0fQlSKlymxaASeDw
[2016-02-02 12:00:19,030][INFO ][cluster.service          ] [Ka-Zar] new_master [Ka-Zar][69KwMa0fQlSKlymxaASeDw][...][inet[/127.0.0.1:9300]], reason: zen-disco-join (elected_as_master)
[2016-02-02 12:00:19,046][INFO ][http                     ] [Ka-Zar] bound_address {inet[/127.0.0.1:9200]}, publish_address {inet[/127.0.0.1:9200]}
[2016-02-02 12:00:19,046][INFO ][node                     ] [Ka-Zar] started
[2016-02-02 12:00:19,051][INFO ][gateway                  ] [Ka-Zar] recovered [0] indices into cluster_state
^C[2016-02-02 12:00:26,751][INFO ][node                     ] [Ka-Zar] stopping ...
[2016-02-02 12:00:26,767][INFO ][node                     ] [Ka-Zar] stopped
[2016-02-02 12:00:26,767][INFO ][node                     ] [Ka-Zar] closing ...
[2016-02-02 12:00:26,771][INFO ][node                     ] [Ka-Zar] closed
12:00:26 3d [jason:~] 130 $ brew remove elasticsearch17
Uninstalling /usr/local/Cellar/elasticsearch17/1.7.4... (36 files, 29.7M)
12:00:35 3d [jason:~] $ brew install elasticsearch
[...]
12:00:42 3d [jason:~] $ elasticsearch --path.plugins /usr/local/var/lib/elasticsearch/plugins/
[2016-02-02 12:00:54,171][INFO ][node                     ] [Baron Mordo] version[2.1.1], pid[88898], build[40e2c53/2015-12-15T13:05:55Z]
[2016-02-02 12:00:54,171][INFO ][node                     ] [Baron Mordo] initializing ...
Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.NoSuchFileException: /usr/local/var/lib/elasticsearch/plugins/analysis-icu/plugin-descriptor.properties
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
        at java.nio.file.Files.newByteChannel(Files.java:361)
        at java.nio.file.Files.newByteChannel(Files.java:407)
        at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
        at java.nio.file.Files.newInputStream(Files.java:152)
        at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
        at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:302)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

Ideally you should just remove the old plugin directory:

```
$ rm -rf /usr/local/var/lib/elasticsearch/plugins
```

as none of the plugins there will work when upgrading from Elasticsearch 1.x to Elasticsearch 2.x because of a change in the plugin format. But if you have some reason to keep them around, change the config for `path.plugins`.
</comment><comment author="edorgeville" created="2016-02-02T17:22:06Z" id="178698344">Works perfectly ! Thank you ! :smile: 
</comment><comment author="jasontedor" created="2016-02-02T17:23:02Z" id="178698918">&gt; Works perfectly ! Thank you ! :smile:

Excellent. You're very welcome. :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field name missing from invalid mapping error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16378</link><project id="" key="" /><description>When trying to create a mapping for master I got the error:

```
Failed to parse mapping [_default_]: Can't parse [index] value [not_analyzed], expected [true] or [false]
```

It took a while to track down that this was being caused by numeric fields with the `index: not_analyzed` setting. It would have been much easier to debug if the error message contained the field name.
</description><key id="130730349">16378</key><summary>Field name missing from invalid mapping error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spalger</reporter><labels><label>:Exceptions</label><label>enhancement</label></labels><created>2016-02-02T16:11:31Z</created><updated>2016-02-09T15:17:33Z</updated><resolved>2016-02-09T15:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spalger" created="2016-02-05T19:57:36Z" id="180534211">ping
</comment><comment author="spalger" created="2016-02-05T19:59:15Z" id="180534621">A new variation on this error message that showed up today, it doesn't list the expected values but does mention the field.

```
Failed to parse mapping [_default_]: wrong value for index [true] for field [lastname]
```
</comment><comment author="s1monw" created="2016-02-06T18:32:20Z" id="180830534">@spalger I will take a look at these problems
</comment><comment author="clintongormley" created="2016-02-08T08:35:41Z" id="181255124">@spalger `index` now takes `true`/`false`:  https://github.com/elastic/elasticsearch/pull/16161

What did you do to get the second message? You sure that was on master?
</comment><comment author="clintongormley" created="2016-02-08T08:36:08Z" id="181255268">Ah, I see this is about the exception message, rather than this change. OK
</comment><comment author="s1monw" created="2016-02-08T16:45:15Z" id="181458763">&gt; A new variation on this error message that showed up today, it doesn't list the expected values but does mention the field.

i think this one is stale - I can't fine it anymore.... I will have a fix for the others.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix plugins integration tests on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16377</link><project id="" key="" /><description>closes #16376
</description><key id="130717453">16377</key><summary>Fix plugins integration tests on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T15:31:09Z</created><updated>2016-02-02T20:28:10Z</updated><resolved>2016-02-02T20:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BigFunger" created="2016-02-02T16:45:30Z" id="178681607">This functionally fixes the issue on windows. Did not review code.
</comment><comment author="tlrx" created="2016-02-02T20:28:10Z" id="178804941">closed in favor of #16384
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integration tests for plugins fail on Windows platforms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16376</link><project id="" key="" /><description>Integration tests for plugins are failing on Windows platforms because:
- it tries to print message using a wrong format (see #16367)
- when installing the plugin using a file, it mixes up the file path (ex: `file://C:\whatever`) and thinks it refers to a plugin to download on Maven Central (ex: `org.elasticsearch:mapper-attachments:3.0.0`) because of the number of `:` in the name

```
....
bin\plugin install file:/Y:/jenkins/workspace/es_core_master_window-2012/plugins/analysis-icu/build/cluster/integTest%20node0/plugins%20tmp/analysis-icu-3.0.0-SNAPSHOT.zip
Successfully started process 'command 'cmd''
Plugins directory [Y:\jenkins\workspace\es_core_master_window-2012\plugins\analysis-icu\build\cluster\integTest node0\elasticsearch-3.0.0-SNAPSHOT\plugins] does not exist. Creating...
Exception in thread "main" java.util.IllegalFormatWidthException: 20
    at java.util.Formatter$FormatSpecifier.checkText(Formatter.java:3044)
    at java.util.Formatter$FormatSpecifier.&lt;init&gt;(Formatter.java:2733)
    at java.util.Formatter.parse(Formatter.java:2560)
    at java.util.Formatter.format(Formatter.java:2501)
    at java.util.Formatter.format(Formatter.java:2455)
    at java.lang.String.format(String.java:2981)
    at org.elasticsearch.common.cli.Terminal$SystemTerminal.doPrint(Terminal.java:161)
    at org.elasticsearch.common.cli.Terminal.print(Terminal.java:110)
    at org.elasticsearch.common.cli.Terminal.println(Terminal.java:105)
    at org.elasticsearch.common.cli.Terminal.println(Terminal.java:93)
    at org.elasticsearch.plugins.InstallPluginCommand.download(InstallPluginCommand.java:168)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:140)
    at org.elasticsearch.common.cli.CliTool.execute(CliTool.java:145)
    at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:74)
:plugins:analysis-icu:integTest#installAnalysisIcuPlugin FAILED
:plugins:analysis-icu:integTest#installAnalysisIcuPlugin (Thread[main,5,main]) completed. Took 1.304 secs.

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':plugins:analysis-icu:integTest#installAnalysisIcuPlugin'.
&gt; Process 'command 'cmd'' finished with non-zero exit value 1
```
</description><key id="130716310">16376</key><summary>Integration tests for plugins fail on Windows platforms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>bug</label><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T15:28:57Z</created><updated>2016-02-02T20:27:54Z</updated><resolved>2016-02-02T20:27:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Output version when running bin/plugin list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16375</link><project id="" key="" /><description>Currently, we only get the plugin list without versions:

```
Installed plugins in /Users/Gabriel/Documents/ElasticSearch/elasticsearch-2.1.1/plugins:
    - license
    - mapper-size
    - marvel-agent
    - watcher
```

It will be nice to print the version of each plugin so we can verify that the correct version is installed.
</description><key id="130715814">16375</key><summary>Output version when running bin/plugin list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-02-02T15:27:57Z</created><updated>2016-05-10T15:33:40Z</updated><resolved>2016-05-10T15:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-02T15:31:35Z" id="178638321">@gmoskovicz Note that for the examples you gave, version will be for all those plugins `2.1.1` as we release official plugins using elasticsearch version number.
</comment><comment author="gmoskovicz" created="2016-02-02T15:36:32Z" id="178640795">@dadoonet Correct! However i think that for community plugins this can vary.
</comment><comment author="electrical" created="2016-02-02T15:48:42Z" id="178648887">Would be nice to have since community plugins can have different versions.
</comment><comment author="alehane" created="2016-02-02T15:48:50Z" id="178648946">@gmoskovicz many thanks for raising this, I'd like to see something along the lines of:

```
Installed plugins in /Users/Gabriel/Documents/ElasticSearch/elasticsearch-2.1.1/plugins:
    - license (2.0.0)
    - mapper-size (2.1.1)
    - marvel-agent (2.1.1)
    - watcher (2.0.1)
```

Note: versions are examples rather than real values :)
</comment><comment author="gmoskovicz" created="2016-02-02T15:49:17Z" id="178649201">+1 for @electrical and @alehane 
</comment><comment author="dadoonet" created="2016-02-02T16:55:09Z" id="178684897">&gt; However i think that for community plugins this can vary.

Sure. I was just talking about the examples you mentioned.

Note that the `_cat/plugins` API provides such information but you already know that it requires to have elasticsearch running and I guess here you need to have that information without elasticsearch running.
</comment><comment author="gmoskovicz" created="2016-02-02T17:06:43Z" id="178692000">&gt; Note that the _cat/plugins API provides such information but you already know that it requires to have elasticsearch running and I guess here you need to have that information without elasticsearch running.

Yep. The useful information from this is **before** starting Elasticsearch, so you can play a little bit with versions and so on. I believe that this will be useful for automated releases and version upgrade.
</comment><comment author="rjernst" created="2016-02-03T02:48:55Z" id="178968141">What would you have the format look like? Should it be only when using "verbose" mode, kind of like `ls` vs `ls -l`? It seems like it should either be just the name, or the name and all the info from the plugin properties file?
</comment><comment author="dadoonet" created="2016-02-03T05:20:13Z" id="179018069">+1 for verbose mode
Not sure if we should print all properties though.
</comment><comment author="rjernst" created="2016-02-03T05:22:09Z" id="179018280">Well, that is what verbose means. :)
</comment><comment author="rjernst" created="2016-02-03T05:23:45Z" id="179018429">I would at least expect the description with it? And java version? So at that point, why not all the info we have about the plugin. 
</comment><comment author="dadoonet" created="2016-02-03T05:26:10Z" id="179019129">Indeed. 
</comment><comment author="gmoskovicz" created="2016-02-03T11:28:11Z" id="179177143">+1 for verbose mode!
</comment><comment author="marek-obuchowicz" created="2016-04-26T14:17:30Z" id="214760075">We are at the moment parsing `plugin-descriptor.properties` file as a workaround, but it seems very logic and natural to be able to get this information from `plugin` tool. In the era of API's and configuration management systems, it would be very helpful to be able to get this information in "straight" way. Lots of third-party plugins are built for very specific ES versions, so we should be able to manage it in easiest possible way.
</comment><comment author="damianpfister" created="2016-04-26T14:18:18Z" id="214760314">+1 for more info on plugins without the need to start Elasticsearch
</comment><comment author="gmoskovicz" created="2016-04-26T14:30:42Z" id="214764311">@rjernst @dadoonet looks like the [ListPluginsCommand](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/ListPluginsCommand.java) class is the one that is returning this information.

Currently is just looking at the directory and printing the name of the folder (which is the plugin name). With verbosity maybe we can print the full file name and read the `.properties` file and print the `description` and `version` ?
</comment><comment author="AttilaForgacs" created="2016-04-26T15:39:44Z" id="214786826">+1 this comes handy
</comment><comment author="gmoskovicz" created="2016-04-28T12:40:31Z" id="215410959">@AttilaForgacs @marek-obuchowicz @dadoonet @damianpfister please add your `+1` as the &#128077;  in the first comment so we can then get some noise :)
</comment><comment author="gmoskovicz" created="2016-05-10T15:33:40Z" id="218195771">Closed by #18051 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed double curl in the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16374</link><project id="" key="" /><description /><key id="130708858">16374</key><summary>Fixed double curl in the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sezinkarli</reporter><labels /><created>2016-02-02T15:04:12Z</created><updated>2016-02-02T15:20:57Z</updated><resolved>2016-02-02T15:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T15:20:56Z" id="178629960">@sezinkarli it is correct as it stands - the first line creates the index, the second puts the mapping

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch master and 2.x require Lucene snapshot upgrades</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16373</link><project id="" key="" /><description>The backwards compatibility test `OldIndexBackwardsCompatibilityIT#testOldIndexes` is failing because Elasticsearch 2.2.0 is built against version 5.4.1 of Lucene. However, Elasticsearch master and 2.x are currently built against Lucene snapshots that do not contain the Lucene version 5.4.1 field.
</description><key id="130706824">16373</key><summary>Elasticsearch master and 2.x require Lucene snapshot upgrades</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>blocker</label><label>build</label></labels><created>2016-02-02T14:58:35Z</created><updated>2016-02-03T12:12:20Z</updated><resolved>2016-02-03T12:12:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-03T12:12:20Z" id="179191343">Closed by #16400 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed double curl in the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16372</link><project id="" key="" /><description /><key id="130706667">16372</key><summary>Fixed double curl in the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sezinkarli</reporter><labels /><created>2016-02-02T14:58:10Z</created><updated>2016-02-02T15:20:30Z</updated><resolved>2016-02-02T15:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T15:20:30Z" id="178629737">@sezinkarli it is correct as it stands - the first line creates the index, the second puts the mapping
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the discovery-multicast plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16371</link><project id="" key="" /><description>Relates to #16310 
Relates to #16326
</description><key id="130696477">16371</key><summary>Deprecate the discovery-multicast plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Discovery Multicast</label><label>deprecation</label><label>v2.2.0</label></labels><created>2016-02-02T14:21:58Z</created><updated>2016-02-05T10:49:56Z</updated><resolved>2016-02-02T14:22:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix minor typo in migrate_3_0.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16370</link><project id="" key="" /><description>Remove extra `on`
</description><key id="130673041">16370</key><summary>Fix minor typo in migrate_3_0.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pra85</reporter><labels /><created>2016-02-02T12:47:31Z</created><updated>2016-02-02T15:15:35Z</updated><resolved>2016-02-02T15:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T15:15:35Z" id="178627133">Thanks @pra85 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JvmGcMonitorServiceSettingsTests.testMissingSetting fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16369</link><project id="" key="" /><description>Failed on my CI. It is fortunately reproducible:

```
gradle :core:test -Dtests.seed=D405DF76E636C6C8 -Dtests.class=org.elasticsearch.monitor.jvm.JvmGcMonitorServiceSettingsTests -Dtests.method="testMissingSetting" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.jvms=10 -Dtests.locale=de-DE -Dtests.timezone=Asia/Yerevan
```

Error message:

```
Expected: a string containing "missing gc_threshold for [monitor.jvm.gc.collector.LaaXU."
   &gt;      but: was "invalid gc_threshold [0s] for [monitor.jvm.gc.collector.LaaXU.warn]"
```
</description><key id="130672909">16369</key><summary>JvmGcMonitorServiceSettingsTests.testMissingSetting fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T12:47:08Z</created><updated>2016-02-02T13:51:05Z</updated><resolved>2016-02-02T13:50:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-02T12:47:20Z" id="178555654">Can you have a look @jasontedor ?
</comment><comment author="jasontedor" created="2016-02-02T13:51:05Z" id="178582245">Thanks @ywelsch. I pushed 69a3f7f590afb3e29b33f9918175e8d1e4004e64 and 1e6b2d4f1d21442b176bff81279231316ec1ad23.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accessing parent properties in script_fields seems no more possible in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16368</link><project id="" key="" /><description>I have a request running on ES 1.4.5 which does not work at all on ES 2.x (tested on 2.1.2, but I guess it's more a 2.x problem) : 
- given a parent `parent` with a property `foo` in the mapping
- given a child `child`with a property `bar` in the mapping
- given a mapping linking the `child` with the `parent`
- given a search query on `child` with a `has_parent` filter

With ES 1.4.5, I could use in groovy `script_fields`, use query function_score functions `script_score` and use filter script `script` code like `doc['parent.foo'].value` as well as `doc['child.bar'].value` and it worked very well.

Whith ES 2.1.2, I have a script error with both `doc['parent.foo'].value` and `doc['child.bar'].value` refering to 'parent.foo' or 'child.bar' not existing in the mapping. The same request would work perfectly on 1.4.5.

So, I've then changed it to, `doc['foo'].value` and `doc['bar'].value` so it would run the scripts without an error :
-  `doc['bar'].value` is ok ( the child value )
-  `doc['foo'].value` is always null ( the parent value )

As I need such a feature, how do I fix this ?
</description><key id="130653720">16368</key><summary>Accessing parent properties in script_fields seems no more possible in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">temsa</reporter><labels /><created>2016-02-02T11:27:27Z</created><updated>2016-02-17T15:34:20Z</updated><resolved>2016-02-02T15:14:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T15:14:04Z" id="178626303">You can no longer use type names as a prefix for field paths.  And you can't access a child document's values when you're in the context of the parent (and vice versa).  They are completely separate documents.
</comment><comment author="temsa" created="2016-02-02T15:41:16Z" id="178643306">@clintongormley I need this feature... If not I will be stuck with a ES 1.x ?

Should I be able to create a plugin to support accessing parent document ? I could maybe add `parent['foo'].value` in the scripts, the same way there is `doc['bar'].value`, is it achievable ? Will there be a huge cost in term of performance or should it be not too far from 1.x performances?

Where should I start to add something available inside scripts ? I've been reading the source of elasticsearch core for about an hour and the source of 2 plugins too, I'm on the way but I did not really found where to start for now...
</comment><comment author="temsa" created="2016-02-04T14:16:36Z" id="179860868">@clintongormley  ?
</comment><comment author="clintongormley" created="2016-02-13T23:41:04Z" id="183771737">@temsa the parent is in a completely separate document. accessing it will perform horribly. this is why we don't support it.  Denormalise your data instead.
</comment><comment author="temsa" created="2016-02-14T16:30:56Z" id="183916004">@clintongormley it would mean updating about ~500 000 documents rather than ~500 every ~30s. I'm not sure it would perform that much less horribly, are you ?

BTW it works well enough on 1.x, how differently would it perform in 2.x with a good enough plugin ? What is the difference in the design between 1.x and 2.x preventing 2.x to have it ?

Thx
</comment><comment author="clintongormley" created="2016-02-15T12:27:34Z" id="184186399">@temsa if it worked on 1.x, it was by chance, and only sometimes.  this was never supported
</comment><comment author="temsa" created="2016-02-15T20:03:40Z" id="184367204">@clintongormley okay, I guess I have a lot of chance as it worked for several versions (1.4.5 up to 1.7x with no issue but something totally unrelated concerning geopoint mapping in script_fields, which seems fixed in 2.x, and not backported to 1.x ...).

From an external point of view it does not look like that it is much different from `inner fields`, is that ?

BTW, your only role here seems to be telling me explicitly its an abuse of ElasticSearch, that this is bad, and this is not a feature, just something incidental, like a bug, etc. Ok, understood, really.

Maybe you can albeit redirect me to someone else or some documentation, or even some code to look at ? This would be a bit more helpful, and would answer a bit to my questions, so I can make this horribly performing plugin, at least for myself ? or give me an alternative to this plugin ?
</comment><comment author="clintongormley" created="2016-02-17T15:34:20Z" id="185257006">@temsa here's an example demonstrating what i'm talking about.  This works in exactly the same way on 1.7.5 and 2.2.0:

```
PUT test
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT test/parent/1
{
  "parent": 5
}

PUT test/child/2?parent=1
{
  "child": 10
}
```

Now we run a `has_parent` query - the main query only returns values in the child document (as only child documents have a parent, while inner hits in the `has_parent` clause returns only values in the parent document:

```
GET test/_search
{
  "script_fields": {
    "parent": {
      "script": "doc['parent'].value"
    },
    "child": {
      "script": "doc['child'].value"
    }
  },
  "query": {
    "has_parent": {
      "type": "parent",
      "inner_hits": {
        "script_fields": {
          "parent": {
            "script": "doc['parent'].value"
          },
          "child": {
            "script": "doc['child'].value"
          }
        }
      },
      "query": {
        "match_all": {}
      }
    }
  }
}
```

And the reverse is true when running a `has_child` query:

```
GET test/_search
{
  "script_fields": {
    "parent": {
      "script": "doc['parent'].value"
    },
    "child": {
      "script": "doc['child'].value"
    }
  },
  "query": {
    "has_child": {
      "type": "child",
      "inner_hits": {
        "script_fields": {
          "parent": {
            "script": "doc['parent'].value"
          },
          "child": {
            "script": "doc['child'].value"
          }
        }
      },
      "query": {
        "match_all": {}
      }
    }
  }
}
```

So unless you are talking about something else (which would have been instantly apparent had you provided a curl example of what you were doing) then your statement that you could access parent and child values within the same query context seems to be false.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CliTool: Messages printed in Terminal should have percent char escaped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16367</link><project id="" key="" /><description>Messages printed using `Terminal` should always escape the `%` char in their messages. Otherwise this char might be interpreted as a String Format specifier and might induce a formatting error that will hide the real message.
</description><key id="130649777">16367</key><summary>CliTool: Messages printed in Terminal should have percent char escaped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T11:09:31Z</created><updated>2016-02-03T13:09:11Z</updated><resolved>2016-02-02T18:59:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-02T12:42:54Z" id="178553027">This is a classic bug IMO, and I think the only correct fix is:

```
printf("%s", someString)
```

calling `printf(someString)` is just trouble, and fixing up `someString` with escaping is fragile. For example the current logic in this PR only escapes the first occurrence, what if there are two?
</comment><comment author="tlrx" created="2016-02-02T14:16:02Z" id="178593147">&gt; For example the current logic in this PR only escapes the first occurrence, what if there are two?

`Strings.replace(msg, "%", "%%")` should replace all occurrences, no?

But I agree we can't escape all format specifiers... I updated the code according to your comment.
</comment><comment author="rmuir" created="2016-02-02T18:51:25Z" id="178754785">+1
</comment><comment author="tlrx" created="2016-02-02T18:59:49Z" id="178759122">@rmuir thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Splits AggregatorFactory implementation into AggregatorFactory and AggregatorBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16366</link><project id="" key="" /><description /><key id="130646518">16366</key><summary>Splits AggregatorFactory implementation into AggregatorFactory and AggregatorBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-02-02T10:56:02Z</created><updated>2016-02-08T14:53:20Z</updated><resolved>2016-02-05T12:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-05T12:11:08Z" id="180322831">superseded by https://github.com/elastic/elasticsearch/pull/16473
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make settings validation strict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16365</link><project id="" key="" /><description>This commit enableds strict settings validation on node startup. All settings
passed to elasticsearch either through system properties, yaml files or any other
way to pass settings must be registered and valid. Settings that are unknown ie. due to
typos or due to deprecation or removal will cause the node to NOT start up. Plugins
have to declare all their settings on the `SettingsModule#registerSetting` and settings for
plugins that are not installed must be removed.

This commit also removes the ability to specify the nodes name via `-Des.name` or just `name` in the
configuration files. The node name must be prefixed with the node prexif like `node.name: Boom`. Left over
usage of `name` will also cause startup to fail.
</description><key id="130642971">16365</key><summary>Make settings validation strict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>das awesome</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T10:39:17Z</created><updated>2016-02-03T13:29:28Z</updated><resolved>2016-02-03T13:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-02T14:12:19Z" id="178591236">AWESOME. LGTM.  left some nitty comments. no need for another review... 
</comment><comment author="ywelsch" created="2016-02-02T14:27:16Z" id="178598061">I'm all ++ on this change, just a few questions:
- How will this affect users upgrading from 2.x to 3.x? How can they remove settings that were useful in 2.x but have been removed in 3.x?
- Can you add documentation for the breaking change?
</comment><comment author="s1monw" created="2016-02-02T16:23:23Z" id="178669426">&gt; Can you add documentation for the breaking change?

I will open followups for all documentation around how to register them etc.

&gt; How will this affect users upgrading from 2.x to 3.x? How can they remove settings that were useful in 2.x but have been removed in 3.x?

settings are only validated on startup and never from the clusterstate. if you ahve something that was useful in 2.x you can just remove it from the yaml file
</comment><comment author="s1monw" created="2016-02-03T10:25:05Z" id="179149619">@bleskes @ywelsch I pushed updates
</comment><comment author="clintongormley" created="2016-02-03T10:32:11Z" id="179155477">thanks for the doc changes @s1monw, i left a couple of comments
</comment><comment author="bleskes" created="2016-02-03T10:58:31Z" id="179163261">thx @s1monw . LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexShardTests.testStressRelocated fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16364</link><project id="" key="" /><description>The test IndexShardTests.testStressRelocated fails on master: timeout while trying to acquire a primary relocation lock on shard:

http://build-us-00.elastic.co/job/es_core_master_oracle_6/4540/
</description><key id="130636501">16364</key><summary>IndexShardTests.testStressRelocated fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Allocation</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T10:11:57Z</created><updated>2016-02-02T11:59:29Z</updated><resolved>2016-02-02T11:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Simplify azure settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16363</link><project id="" key="" /><description>In this PR we have simplified the parsing logic in `AzureStorageSettings` by leveraging the new settings infrastructure. Note that we are now also more strict w.r.t. to wrong config values: We will ensure that there is exactly one primary data store defined. To me it makes no sense to be lenient about this setting and it can be easily fixed by users in their config file.

I have also noticed that different flavors of keys ("normal" ones, group keys, list keys ...) are implicitly defined in `Setting`. As this PR adds yet another flavor of keys, I have introduced a setting key as a dedicated abstraction used internally in the `Setting` class but opted to still expose the string based keys to clients by default to keep the API simpler. As this is not entirely related to the `AzureStorageSettings` (but it benefits in my opinion from the new implementation) I have made this change in a separate commit.

Wdyt about these changes @s1monw?
</description><key id="130627832">16363</key><summary>Simplify azure settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T09:39:39Z</created><updated>2016-03-03T09:05:21Z</updated><resolved>2016-03-03T09:05:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-02T14:46:23Z" id="191266164">this looks awesome! @danielmitterdorfer the only thing that I would ask you todo before pushing is to add an assertion to `AbstractScopedSettings` like this:

``` JAVA
private boolean assertMatcher(String key, int numComplexMatchers) {
  List&lt;Setting&lt;?&gt;&gt; list = new ArrayList&lt;&gt;();
  for (Map.Entry&lt;String, Setting&lt;?&gt;&gt; entry : complexMatchers.entrySet()) {
    if (entry.getValue().match(key)) {
      list.add(entry.getValue().getConcreteSetting(key));
    }
  }
  assert list.size() == numComplexMatchers : "Expected " + numComplexMatchers + " complex matcher but got: "  + list.toString();
  return true;
}
```

and call it in:

``` JAVA
   public Setting&lt;?&gt; get(String key) {
        Setting&lt;?&gt; setting = keySettings.get(key);
        if (setting != null) {
            assert assertMatcher(key, 0);
            return setting;
        }
        for (Map.Entry&lt;String, Setting&lt;?&gt;&gt; entry : complexMatchers.entrySet()) {
            if (entry.getValue().match(key)) {
                assert assertMatcher(key, 1);
                return entry.getValue().getConcreteSetting(key);
            }
        }
        return null;
    }
```

that way we can ensure we don't have any matcher ambiguity?!
</comment><comment author="danielmitterdorfer" created="2016-03-03T08:38:53Z" id="191653118">@s1monw: Thanks for the review! I've added the assertions and the first one (i.e. `assertMatcher(key, 0)`) triggers for a few settings:
- `logger.` clashes with `logger.level`
- `node.` clashes with `node.mode`, `node.name`, `node.client`, `node.data`, `node.max_local_storage_nodes`, `node.add_id_to_custom_path`, `node.enable_lucene_segment_infos_trace`, `node.portsfile`, `node.master`, `node.local`, `node.ingest`
- `indices.analysis.hunspell.dictionary.` clashes with `indices.analysis.hunspell.dictionary.lazy`, `indices.analysis.hunspell.dictionary.ignore_case`

(Theoretically), I see four options to avoid this problem:
1. Rename all affected group keys (i.e. `logger.`, `node.`, ...) avoiding the name clash alltogether. This would break existing configurations.
2. Add a new complex key implementation which is not only based on a prefix but explicitly excludes all suffixes what would clash. This can work but is fragile: Every time we add a new specific key (somewhere in the code base), we also have to exclude the concrete key (suffix) in the group key.
3. Add explicit exclude rules in infrastructure code: After system bootstrap, we could check explicitly for name clashes and add some exclude rules. However, this asks for trouble: too much magic and code complexity.
4. Remove the first assertion. As far as I can see, complex matchers play no role when a setting can be found based on an exact key so it would be safe to remove it.

I'd opt for option 4 but maybe you can think of another alternative.
</comment><comment author="danielmitterdorfer" created="2016-03-03T08:47:57Z" id="191657176">As discussed, I'll go with option 4 and remove the first assert.
</comment><comment author="s1monw" created="2016-03-03T08:48:09Z" id="191657243">&gt; Remove the first assertion. As far as I can see, complex matchers play no role when a setting can be found based on an exact key so it would be safe to remove it.

++ I was adding that in my example by accident - not sure what I was thinking :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fail to build the master branch of elasticsearch using gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16362</link><project id="" key="" /><description>I am using the gradle 2.8 and java 1.8.0_20 to build the master branch of the elasticsearch and get the error:
**AutoExpandReplicas.java:44 : cannot assign to the final variable min**
**min = Integer.parseInt(sMin);**
I have searched the google and there is no answer.
</description><key id="130604558">16362</key><summary>fail to build the master branch of elasticsearch using gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jerry-sjtu</reporter><labels /><created>2016-02-02T07:49:57Z</created><updated>2016-02-02T10:11:42Z</updated><resolved>2016-02-02T10:11:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-02-02T08:41:11Z" id="178447722">You are probably hitting https://bugs.openjdk.java.net/browse/JDK-8051958. Can you update to a newer JDK 8 version?
</comment><comment author="jerry-sjtu" created="2016-02-02T09:08:13Z" id="178460261">I tried the java 1.8.0_40,java 1.8.0_72 and get the error: :core:forbiddenApisTest FAILED
</comment><comment author="ywelsch" created="2016-02-02T09:13:30Z" id="178464829">can you do a `gradle clean` first? Also, add `-s` to the gradle invocation so we get the full stacktrace.
</comment><comment author="jerry-sjtu" created="2016-02-02T09:57:20Z" id="178485911">## 2: Task failed with an exception.
- What went wrong:
  Execution failed for task ':distribution:integ-test-zip:integTest#stop'.
  
  &gt; Process 'Taskkill /PID 6724 /F' finished with non-zero exit value 128
- Try:
  Run with --info or --debug option to get more log output.
- Exception is:
  org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':distrib
  ution:integ-test-zip:integTest#stop'.
      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.ex
  ecuteActions(ExecuteActionsTaskExecuter.java:69)
      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.ex
  ecute(ExecuteActionsTaskExecuter.java:46)
      at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExec
  uter.execute(PostExecutionAnalysisTaskExecuter.java:35)
      at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.exec
  ute(SkipUpToDateTaskExecuter.java:64)
      at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execut
  e(ValidatingTaskExecuter.java:58)
      at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecu
  ter.execute(SkipEmptySourceFilesTaskExecuter.java:52)
      at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter
  .execute(SkipTaskWithNoActionsExecuter.java:52)
      at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execut
  e(SkipOnlyIfTaskExecuter.java:53)
      at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter
  .execute(ExecuteAtMostOnceTaskExecuter.java:43)
      at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTa
  skWorker.execute(DefaultTaskGraphExecuter.java:203)
      at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTa
  skWorker.execute(DefaultTaskGraphExecuter.java:185)
      at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorW
  orker.processTask(AbstractTaskPlanExecutor.java:62)
      at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorW
  orker.run(AbstractTaskPlanExecutor.java:50)
      at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(Defaul
  tTaskPlanExecutor.java:25)
      at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(Defau
  ltTaskGraphExecuter.java:110)
      at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTask
  ExecutionAction.java:37)
      at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecute
  r.java:37)
      at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExec
  uter.java:23)
      at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecu
  ter.java:43)
      at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildEx
  ecutionAction.java:32)
      at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecute
  r.java:37)
      at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecute
  r.java:30)
      at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLa
  uncher.java:154)
      at org.gradle.internal.Factories$1.create(Factories.java:22)
      at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(Defaul
  tBuildOperationExecutor.java:90)
      at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(Defaul
  tBuildOperationExecutor.java:52)
      at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(Default
  GradleLauncher.java:151)
      at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGra
  dleLauncher.java:32)
      at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradl
  eLauncher.java:99)
      at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradl
  eLauncher.java:93)
      at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(Defaul
  tBuildOperationExecutor.java:90)
      at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(Defaul
  tBuildOperationExecutor.java:62)
      at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradle
  Launcher.java:93)
      at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLaun
  cher.java:82)
      at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildCon
  troller.run(InProcessBuildActionExecuter.java:94)
      at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(Exe
  cuteBuildActionRunner.java:28)
      at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildA
  ctionRunner.java:35)
      at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProce
  ssBuildActionExecuter.java:43)
      at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProce
  ssBuildActionExecuter.java:28)
      at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(Contin
  uousBuildActionExecuter.java:77)
      at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(Contin
  uousBuildActionExecuter.java:47)
      at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.exe
  cute(DaemonUsageSuggestingBuildActionExecuter.java:51)
      at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.exe
  cute(DaemonUsageSuggestingBuildActionExecuter.java:28)
      at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43)
      at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.jav
  a:170)
      at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.
  execute(CommandLineActionFactory.java:237)
      at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.
  execute(CommandLineActionFactory.java:210)
      at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRunti
  meValidationAction.java:35)
      at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRunti
  meValidationAction.java:24)
      at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(
  CommandLineActionFactory.java:206)
      at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(
  CommandLineActionFactory.java:169)
      at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionRep
  ortingAction.java:33)
      at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionRep
  ortingAction.java:22)
      at org.gradle.launcher.Main.doAction(Main.java:33)
      at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
  java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
  sorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:497)
      at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBoots
  trap.java:54)
      at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.j
  ava:35)
      at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
  Caused by: org.gradle.api.GradleException: Process 'Taskkill /PID 6724 /F' finis
  hed with non-zero exit value 128
      at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  
  ```
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstruct
  ```
  
  orAccessorImpl.java:62)
      at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingC
  onstructorAccessorImpl.java:45)
      at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
      at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstru
  ctor.java:80)
      at org.codehaus.groovy.reflection.CachedConstructor.doConstructorInvoke(
  CachedConstructor.java:74)
      at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteN
  oUnwrap.callConstructor(ConstructorSite.java:84)
      at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstru
  ctor(CallSiteArray.java:60)
      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor
  (AbstractCallSite.java:235)
      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor
  (AbstractCallSite.java:247)
      at org.elasticsearch.gradle.LoggedExec$_closure1.doCall(LoggedExec.groov
  y:37)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
  java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
  sorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:497)
      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:
  93)
      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(C
  losureMetaClass.java:294)
      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019)
      at groovy.lang.Closure.call(Closure.java:426)
      at groovy.lang.Closure.call(Closure.java:442)
      at org.gradle.api.internal.AbstractTask$ClosureTaskAction.execute(Abstra
  ctTask.java:554)
      at org.gradle.api.internal.AbstractTask$ClosureTaskAction.execute(Abstra
  ctTask.java:535)
      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.ex
  ecuteAction(ExecuteActionsTaskExecuter.java:80)
      at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.ex
  ecuteActions(ExecuteActionsTaskExecuter.java:61)
      ... 61 more
</comment><comment author="jasontedor" created="2016-02-02T10:11:41Z" id="178493241">Duplicates #16097
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce plugin zip does not contain zip entries outside of the plugin dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16361</link><project id="" key="" /><description>When unzipping a plugin zip, the zip entries are resolved relative to
the directory being unzipped into. However, there are currently no
checks that the entry name was not absolute, or relatively points
outside of the plugin dir. This change adds a check for those two cases.
</description><key id="130569382">16361</key><summary>Enforce plugin zip does not contain zip entries outside of the plugin dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T04:20:38Z</created><updated>2016-03-11T22:54:47Z</updated><resolved>2016-03-11T22:54:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-11T22:40:12Z" id="195587393">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop `@Nullable` on `type` in `Script`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16360</link><project id="" key="" /><description>`Script.type` is not nullable: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/script/Script.java#L94
</description><key id="130558019">16360</key><summary>Drop `@Nullable` on `type` in `Script`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">izeye</reporter><labels /><created>2016-02-02T03:11:20Z</created><updated>2016-03-17T05:44:51Z</updated><resolved>2016-03-17T04:13:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-02-02T03:53:30Z" id="178354745">I'm not sure about this because scripts can also be de-serialized off the wire and the de-serialization logic [allows for the possibility](https://github.com/elastic/elasticsearch/blob/d91a898f6a339a48d546d9205bcbd7c09a390631/core/src/main/java/org/elasticsearch/script/Script.java#L143-L145) of a null `ScriptType`; this will require some careful investigation. Then there are things like `Script#getType` which also has [logic in it](https://github.com/elastic/elasticsearch/blob/d91a898f6a339a48d546d9205bcbd7c09a390631/core/src/main/java/org/elasticsearch/script/Script.java#L118-L120) to allow for a null type, so there is a lot more to doing this than just removing the annotation. At the end of the day the annotation does not really do anything, but it does communicate intent so we don't want to be too hasty and we definitely want to make sure that if we are going to get in there and clean something like this up that all the bread crumbs are followed.
</comment><comment author="jasontedor" created="2016-03-17T04:14:04Z" id="197685514">I think this annotation should stay; please correct me if you think otherwise @izeye.
</comment><comment author="izeye" created="2016-03-17T05:44:51Z" id="197716792">@jasontedor Ok, no problem. Thanks for the feedback!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CliTool: Allow unexpected exceptions to propagate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16359</link><project id="" key="" /><description>Cli tools currently catch all exceptions, and only print the exception
message, except when a special system property is set. Even with this
flag set, certain exceptions, like IOException, are captured and their
stack trace is always lost.

This change adds a UserError class, which can be used a cli tools to
specify a message to the user, as well as an exit status. All other
exceptions are propagated out of main, so java will exit with non-zero
and print the stack trace.
</description><key id="130529590">16359</key><summary>CliTool: Allow unexpected exceptions to propagate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-02T00:36:14Z</created><updated>2016-02-02T01:38:28Z</updated><resolved>2016-02-02T01:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-02-02T00:41:42Z" id="178274845">this is an awesome cleanup. thanks for removing that now-unneeded jdk workaround!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to disable importing dangling indices on shared filesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16358</link><project id="" key="" /><description>We already have issues open for the behavior of importing indices when a node is out of the cluster while an index is deleted, however, in the case of shadow replicas on a shared filesystem, if the index is deleted while a node is not part of the cluster, when the node comes back it will try to re-import the index as dangling without any actual data, leading to exceptions like:

```
[2016-01-14 23:17:23,303][WARN ][indices.cluster ] [data1] [[my-index][2]] marking and sending shard failed due to [failed recovery]
[my-index][[my-index][2]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: IndexShardRecoveryException[shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(default(mmapfs(/tmp/foo/my-index/2/index),niofs(/tmp/foo/my-index/2/index))): files: []];
```

It would be nice to have an option to disable this dangling import behavior _only_ for indices on a shared filesystem, since this is likely to cause much more confusion than dangling imports on non-shared filesystems.

Related #13298
</description><key id="130517213">16358</key><summary>Add ability to disable importing dangling indices on shared filesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Shadow Replicas</label><label>enhancement</label></labels><created>2016-02-01T23:21:35Z</created><updated>2016-04-25T20:16:08Z</updated><resolved>2016-04-25T20:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-21T23:52:43Z" id="186947393">Discussing the cause issue (index delete while node is off the cluster) I think we should consider this in a different way, which may help in the normal use case (non-shared FS) as well.  Dangling indices are added as a safety guard against the case where a cluster goes down (or the master nodes) and because of mis configuration or because of bad operation practices (kill all masters, start fresh ones with a different data folders). In these cases the new master publishes a cluster state which is empty and we don't want to delete the data. These days we have a cluster UUID as part of the cluster meta data. That uuid is created once and never change in the duration of the cluster.  That means that we can detect deletion while node left if:

1) Store the cluster uuid in the index meta data, so we know the cluster this index belongs to.
2) When a node gets a cluster state from a master and it doesn't contain the index it has on disk, the node can check whether the index used to be belong to the same cluster (by uuid) . If it is, we know it's deleted.
3) If the uuid in the index is different than the uuid in the cluster, we import it.

How does this sound?
</comment><comment author="dakrone" created="2016-02-22T01:31:46Z" id="186961890">&gt; How does this sound?

I think this sounds great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't cancel allocation when a new sync id is found on shared filesystems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16357</link><project id="" key="" /><description>In `ReplicaShardAllocator.processExistingRecoveries`, if we find a "better" match, we cancel allocation of a replica:

``` java
// we found a better match that has a full sync id match, the existing allocation is not fully synced
// so we found a better one, cancel this one
it.moveToUnassigned(new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA,
        "existing allocation of replica to [" + currentNode + "] cancelled, sync id match found on node [" + nodeWithHighestMatch + "]"));
```

However, when on a shared filesystem, all data nodes have the same data, so we should not cancel allocation if a new node pops up.
</description><key id="130507022">16357</key><summary>Don't cancel allocation when a new sync id is found on shared filesystems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>:Shadow Replicas</label><label>enhancement</label></labels><created>2016-02-01T22:38:16Z</created><updated>2017-05-26T18:53:34Z</updated><resolved>2017-05-26T18:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-02-01T22:38:41Z" id="178235219">@bleskes I spoke with @brwe about this and I think we agreed it was worth doing, but I'm curious about your input on this as well.
</comment><comment author="dakrone" created="2017-05-26T18:53:34Z" id="304361528">Shadow replicas have been removed and this is no longer applicable</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add task status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16356</link><project id="" key="" /><description>Implements a simple task status for superclasses of ReplicationRequest to
show how you can do use the status.

Closes #16344
</description><key id="130496814">16356</key><summary>Add task status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>blocker</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T21:56:36Z</created><updated>2016-02-08T09:47:56Z</updated><resolved>2016-02-03T23:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-01T21:56:52Z" id="178214662">@imotov more task management!
</comment><comment author="nik9000" created="2016-02-02T21:37:13Z" id="178837178">@imotov I've moved the integration test bit into TasksIT and used the MockTaskManager to do what I needed without any looping. Yay!
</comment><comment author="nik9000" created="2016-02-03T21:36:57Z" id="179482814">@imotov removed &lt;?&gt; from Task.Status. It makes me much happier now.
</comment><comment author="imotov" created="2016-02-03T21:42:01Z" id="179485245">@nik9000 me too. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>revert PipelineFactoryError handling with throwing ElasticsearchParseException in ingest pipeline creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16355</link><project id="" key="" /><description>From discussion found here: https://github.com/elastic/elasticsearch/pull/16276#issuecomment-177910583

After some discussion, leveraging headers in `ElasticsearchException` is favored over wrapping structured response objects for errors.
</description><key id="130493134">16355</key><summary>revert PipelineFactoryError handling with throwing ElasticsearchParseException in ingest pipeline creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T21:42:58Z</created><updated>2016-02-08T09:31:35Z</updated><resolved>2016-02-02T22:11:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-02T08:56:50Z" id="178454910">@talevy looks good, I think we can remove `ConfigurationPropertyException` too?
</comment><comment author="talevy" created="2016-02-02T18:48:09Z" id="178753324">removed `ConfigurationPropertyException` and made ConvertProcessor throw ElasticsearchParseException when looking up `Type` from config.
</comment><comment author="martijnvg" created="2016-02-02T20:45:27Z" id="178811454">@talevy left one small comment. OTT LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add parsing from xContent to PhraseSuggestionBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16354</link><project id="" key="" /><description>This adds parsing from xContent to PhraseSuggestionBuilder and changes properties of PhraseSuggestionBuilder to using default values form PhraseSuggestionContext where possible. Also adding access to the global Suggesters via QueryParseContext so we can retrieve the right prototype for parsing the suggestions.
</description><key id="130486801">16354</key><summary>Add parsing from xContent to PhraseSuggestionBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T21:16:47Z</created><updated>2016-03-10T18:58:32Z</updated><resolved>2016-02-08T11:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-03T11:30:00Z" id="179177459">@cbuescher I am concerned about this change making suggesters required everywhere we have queries. Queries are used a lot outside of the Search API and I think it will make other modules (e.g. percolator and aggs) difficult to maintain if they need to pass in suggesters when they don't use them. Personally I think the suggester should have a separate registry like the Aggregations do.
</comment><comment author="cbuescher" created="2016-02-03T12:11:37Z" id="179190907">@colings86 thanks, I will take a look at alternatives, Suggesters already is the central registry, I only need a way to access it from the SuggestionBuilder fromXContent method. Maybe directly adding it to the context there would be an option. Any other ideas welcome.
</comment><comment author="javanna" created="2016-02-03T13:41:37Z" id="179238999">Colin has a good point, would it be possible to treat Suggesters as the registry and instead of adding it to the existing IndicesQueriesRegistry, inject that directly to the parse context? That said the naming is a bit off at the moment, I am not clear on whether QueryParseContext will become SearchParseContext at some point .
</comment><comment author="cbuescher" created="2016-02-03T16:02:28Z" id="179312951">@colings86 @javanna thanks for the comments, I tried to solve the suggestion builder lookup differently now. The lookup is needed in the common parsing code for fields all SuggestionBuilder share, so initially I put it in the abstract superclass, but by putting it into SuggestParseElement it can directly use the registry that is already wired there. The downside is, we don't have the parsing code in the builder, but thats also not true for the queries. Also the duplicated code we temporarily have in SuggestParseElement will go away after the complete refactoring (also we don't need to call it SuggestParseElement then).
</comment><comment author="cbuescher" created="2016-02-04T18:50:49Z" id="179998093">I went a step back and optionally added the suggesters registry I need for parsing to the QueryParseContext directly. That way, when we need it can be add it (e.g. in SearchService when starting the whole parsing chain) and its not part of IndicesQueriesRegistry.
</comment><comment author="colings86" created="2016-02-08T10:42:00Z" id="181300511">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `discovery.zen.rejoin_on_master_gone`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16353</link><project id="" key="" /><description>That setting was built as an escape hatch for the work done in 1.4. I never heard it being used once. We can safely remove it.
</description><key id="130485107">16353</key><summary>Remove `discovery.zen.rejoin_on_master_gone`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>non-issue</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T21:11:26Z</created><updated>2016-02-02T11:30:27Z</updated><resolved>2016-02-02T11:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-01T21:11:51Z" id="178193359">@martijnvg can you take a look?
</comment><comment author="martijnvg" created="2016-02-01T21:17:22Z" id="178197034">Yes, this setting was a escape hatch for when the then new rejoin didn't pan out. Luckily I have also never seen the new rejoin logic to not work out, so LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added ingest processor combination tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16352</link><project id="" key="" /><description>- Added an ingest qa that tests processor real world like configurations
- Renamed `ingest-with-mustache` to `smoke-test-ingest-with-all-dependencies` and made it also depend on `ingest-grok` and `ingest-geoip`.
- Also renamed `ingest-disabled` to `smoke-test-ingest-disabled` so that the name is more inline with other qa smoke test modules.
</description><key id="130480699">16352</key><summary>Added ingest processor combination tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T20:56:04Z</created><updated>2016-02-08T09:21:54Z</updated><resolved>2016-02-02T21:38:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-02-02T14:37:15Z" id="178601248">this is sweet! should we add more examples of mutations on fields? like uppercase, etc. 
</comment><comment author="talevy" created="2016-02-02T21:34:55Z" id="178835349">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats API raises index_closed_exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16351</link><project id="" key="" /><description>The fact that I can't find anyone else with this problem suggests the problem is on my end, but based on the docs and the response I get, [this looks like a bug](https://discuss.elastic.co/t/index-closed-exception/38424).  1451407412544 is roughly Dec 29 09:43:32 MST 2015 and 1452012212544 is roughly Jan  5 09:43:32 MST 2016.  My first thought was that maybe I have documents in the closed indices that fall with this range.  If that were the case, shouldn't the field stats response include those indices in its reply after I open them?
</description><key id="130478241">16351</key><summary>Field stats API raises index_closed_exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erik-stephens</reporter><labels /><created>2016-02-01T20:47:11Z</created><updated>2016-02-01T22:51:00Z</updated><resolved>2016-02-01T22:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-01T21:14:44Z" id="178195048">What is the exact ES version that you're using? 
Are you by any chance specifying indices that are closed in the field stats api?

Closed indices are never queried if field stats executes of all indices, so this is weird if you're seeing these exceptions. I quickly tries to re-create a scenario with a closed index, but the field stats api just skips that closed index.
</comment><comment author="erik-stephens" created="2016-02-01T22:51:00Z" id="178239709">The version is 2.1.1.  Took me a while, but I was finally able to reproduce on a test cluster with test data.  Looks like my problem is that I have lingering aliases for closed indices.  Now I'm seeing unexpected results around aliases but that's maybe for another thread.  Thanks for the reply - it helped me narrow it down.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mapper is used by multiple types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16350</link><project id="" key="" /><description>Have Upgraded ES From 1.7 to ES 2.1. While Indexing, facing the issue mentioned below. Any help would be highly appreciated:

mapper [field] is used by multiple types. Set update_all_types to true to update [format] across all types.]]
[5813]: index [index_name], type [index_type], id [13344701], message [java.lang.IllegalArgumentException: Mapper for [field] conflicts with existing mapping in other types
</description><key id="130471397">16350</key><summary>mapper is used by multiple types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sandesh2014</reporter><labels /><created>2016-02-01T20:19:49Z</created><updated>2016-02-02T01:33:12Z</updated><resolved>2016-02-02T01:33:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sandesh2014" created="2016-02-01T21:34:18Z" id="178204771">Can anyone from elastic team help out regarding this issue. Do let me know if this is a bug or something else is wrong.
</comment><comment author="rjernst" created="2016-02-02T01:33:12Z" id="178297641">It is hard to give a better explanation of what is happening without seeing the mappings. However, this is expected behavior with 2.0+ when you try to have a field with the same name, but different data types, in two different document types.

Please join us on https://discuss.elastic.co for questions like these.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace percolate APIs with a percolator query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16349</link><project id="" key="" /><description>By replacing the percolator APIs by a percolator query means that percolating will be done via the search API. This has many advantages:
- Removing a lot of code
- A lot of requested features are now supported.
- There will never be a need to sync the search and percolate apis.

Using the percolator query in the search API:

```
"query" : {
   "percolator" : {
        "doc_type" : "type",
        "source" : {
            "field1" : "value",
            ...
        }
    }
}
```

Percolating an existing document:

```
"query" : {
   "percolator" : {
        "doc_type" : "type",
        "get" : {
            "index" : "_index",
            "type" : "_type",
            "id" : "_id"
        }
    }
}
```

The search response will now include hits of type `.percolator`.

The percolate and mpercolate APIs still exist in this PR, but just these APIs just build a search request and delegates the request to the search API behind the scene for bwc reason. In the next major version after 3.0 these APIs can be removed.

This PR builds, but is not ready yet. Tests need to be added and the entire percolator docs need to be revised.

Closes #10741
Closes #7297
Closes #13176
Closes #13978
Closes #11264
Closes #10741
Closes #4317
</description><key id="130426382">16349</key><summary>Replace percolate APIs with a percolator query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T17:27:22Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2016-02-05T14:26:05Z" id="180378526">This will also add lots of confusion, since percolation is a very confusing concept for people new to Elasticsearch and also to experienced users. Instead of having "percolator" a type of a query, can it be a sibling to query and highlight when using the search API?

```
POST _search
{
   "percolator" : {
        "doc_type" : "type",
        "source" : {
            "field1" : "value",
            ...
        }
    }
}
```

I think this will go a long way in removing confusion. Another option is to leave the percolate API as is but internally use redirects so it will essentially use the search API via it's own Lucene Query type.
</comment><comment author="martijnvg" created="2016-02-05T14:59:18Z" id="180390097">I think part of this confusion is how the percolator feature is exposed right now? In the end what the percolator does is return documents that have a query defined under the `query` field. All of these documents are being evaluated if its query matches with the provided document.

I think this change rather removes confusion. Percolator queries are just document with a query field that is mapped as percolator field type in the mapping. The `percolator` query can be used to match these queries.  
</comment><comment author="synhershko" created="2016-02-06T18:19:14Z" id="180828328">The confusion stems from the difference between what the percolator does (indexing a single doc via MemoryIndex and hitting it with many stored queries) to what 99.9% of Elasticsearch is about (indexing many documents and using a single query for search across them) to how percolator is presented to the public ("reverse search"; "indexing queries" and so on).

The fact that stored queries can be filtered before execution via a query, doesn't make it much of a search. Just a filter stage before executing percolation (even if underneath a search is executed).

Honestly, that's nothing a good documentation and careful working can't solve, and the current percolator documentation is quite good. What's missing is a clearer message (IMO, stop using the "reverse" terminology. Just explain technically what it does, the usage of the `.percolator` type is confusing enough), and then also keeping this in a separate API. Bringing it within the search API would IMO grow the confusion even bigger.
</comment><comment author="martijnvg" created="2016-02-08T08:03:37Z" id="181248752">Right, the reverse part is that queries are stored as part of a document and a document is used to query queries. I still think that percolating via a query doesn't confuse things more. I think that form an api perspective the percolator doesn't need separate APIs just because how the percolator matches with queries (MemoryIndex, pre searching etc.). The `query` field is a special field that can only be matched with the source of a document, which can be specified in the `percolator` query. 

I think that the special `.percolator` type should go away eventually and if someone wants to store queries they would need to configure the `percolator` field type in the mapping.
</comment><comment author="djschny" created="2016-02-09T13:56:02Z" id="181873636">I agree with @synhershko, keeping a clear distinction of `_percolate` really is much more elegant. Throwing everything under `_search` really muddies the water and the line of separation. We are placing internal concerns ahead of usability in this situation. The advantages of this are declared as:
- Removing a lot of code
- A lot of requested features are now supported.
- There will never be a need to sync the search and percolate apis.

But these have almost zero advantages to end users and instead actually causes unnecessary changes, confusion, etc. Instead I would suggest exploring other ways so that the `_percolate` endpoint code is extremely simple and behind the scenes delegates to existing backend business logic, so that way the duplicated code, syncing, etc. is avoided.
</comment><comment author="s1monw" created="2016-02-09T14:22:30Z" id="181884550">&gt; The fact that stored queries can be filtered before execution via a query, doesn't make it much of a search. Just a filter stage before executing percolation (even if underneath a search is executed).

I think you have to take a step back and forget about the implementation here. If you think about what the percolator does is:
- give me all the queries that match a given document

now if you think of queries are documents (JSON) it's a perfect match for a search. It's conceptually exactly the same as MLT. You pass it a document and we process the terms in a way and return documents like this. Now the percolator query can process a document in a structured way and has a potentially costly collect method. It's like a geohash for prefiltering and then use a slower method to get exact matches. 

you have to plan the mind game of what would happen if we haven't had this API before. I think it's a very neat idea and might transport to users much better than a dedicated API. Anyway having a dedicated API is kinda required for BWC purposes but I think the implementation should go the path of using the search infra. We are also going that way somehow with suggest which is also a search at the end of the day.

&gt; But these have almost zero advantages to end users and instead actually causes unnecessary changes, 

I think this is far from correct!
- _A lot of requested features are now supported._  this is huge
- _There will never be a need to sync the search and percolate apis._ has been a massive source of bugs in the past
- _Removing a lot of code_ this is very important growth wise for the project. calling this not an advantage to the user is short sighted.
</comment><comment author="kimchy" created="2016-02-09T14:27:08Z" id="181887053">I really like where this is going, using the search infrastructure to execute it is sooo much cleaner. As @s1monw said, we can keep the sugar percolator API on top of it, but on my end, I would have used it in the context of the search API, feels more natural (as much as possible, percolate is a mind bender :) )
</comment><comment author="javanna" created="2016-02-09T14:45:15Z" id="181897165">I don't think keeping the percolate endpoint as a shortcut is a problem, and we will do it for bw comp anyways. But I think "A lot of requested features are now supported." is a big one for users, maybe we should list what these requested features are.
</comment><comment author="martijnvg" created="2016-02-09T15:10:11Z" id="181908450">I know that returning the percolator document source or part of it (via source filtering) is highly requested. Also pagination is a highly requested feature for the percolator. All these features would be supported.

Also the following issues can be closed if this PR gets merged:
https://github.com/elastic/elasticsearch/issues/10741
https://github.com/elastic/elasticsearch/issues/7297
https://github.com/elastic/elasticsearch/issues/13176
https://github.com/elastic/elasticsearch/issues/13978
https://github.com/elastic/elasticsearch/issues/11264
https://github.com/elastic/elasticsearch/issues/10741
https://github.com/elastic/elasticsearch/issues/4317
</comment><comment author="martijnvg" created="2016-03-08T10:57:47Z" id="193728391">I've updated this PR (added tests &amp; first stab at updating docs). I think it is ready for a review.
</comment><comment author="martijnvg" created="2016-03-10T12:57:17Z" id="194830616">I've updated the PR based on @clintongormley's comments.
</comment><comment author="jpountz" created="2016-03-16T20:57:17Z" id="197546394">I like the change in general, especially how it removes the duplication between the search and percolation code. But the change is large so I'll need to give it a deeper look, if you could point me the the parts that need a more careful review, I would appreciate. :) I also saw that you moved the percolate queries back to memory, which I think (not certain though) is a better trade-off as it help elasticsearch avoid to reparse all queries at search time.
</comment><comment author="martijnvg" created="2016-03-16T21:21:17Z" id="197554142">Thank you @jpountz for looking at this! 

&gt; if you could point me the the parts that need a more careful review, I would appreciate. :)

Off course :)
- I think the `PercolatorQueryCache` needs a more careful look, because that is a big change how we did this before. In this PR all the queries get loaded via an index warmer. Now in master we have the `PercolatorQueryRegistry` that in realtime gets changes from either index and delete operations.
- The entire percolator infra structure has been replaces by the `PercolatorQueryBuilder`. I think that this is important to look at to.
- Getting highlighting to work for the matching percolator required adding a special sub fetch phase: `HighlightSubFetchPhase`.
- The `PercolatorQuery` does is now cacheable too. I think this is a nice improvement, but needs a good look too.

&gt; I also saw that you moved the percolate queries back to memory, which I think (not certain though) is a better trade-off as it help elasticsearch avoid to reparse all queries at search time.

Yes, I think in general this is a better trade off. Maybe at some point we can look into not caching all queries and only keeping the most used percolator queries in memory.
</comment><comment author="jpountz" created="2016-03-18T08:38:10Z" id="198256520">I just did a 2nd deeper review and left some comments.
</comment><comment author="martijnvg" created="2016-03-18T13:20:36Z" id="198352254">@jpountz I've updated the PR.
</comment><comment author="jpountz" created="2016-03-18T21:10:22Z" id="198543075">I left some minor comments, otherwise LGTM. I'm looking forward to the follow-up PRs. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic mapping does not work with more than one type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16348</link><project id="" key="" /><description>Hello guys,

This config worked in ES v1.7.2 but since i tried to use the configuration in ES v2.1 I have a error... So :

I use this mapping :

``` json
PUT /my_index/_mapping/_default_
{
    "_default_": {
        "dynamic_date_formats" : [
            "yyyy-MM-dd HH:mm:ss",
            "yyyy-MM-dd HH:mm:ss||yyyyMMdd-HH:mm:ss||yyyyMMdd-HH:mm:ss.SSS"
        ],
        "dynamic_templates": [
            {
                "template_1" : {
                    "match" : "*",
                    "match_mapping_type" : "string",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            }, {
                "template_2" : {
                    "match" : "*@expire",
                    "match_mapping_type" : "date",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            }
        ]
    }
}
```

And after a first type insert, I get the expected result. But if I try to insert a second or third type like :

``` json
POST /my_index/my_type
{
   "source":"my_source",
   "country":"my_country",
   "company":"my_company",
   "server":"my_server",
   "@timestamp":"2016-01-01 00:07:10"
}
```

Result return is :

``` json
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse [@timestamp]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "failed to parse [@timestamp]",
      "caused_by": {
         "type": "illegal_argument_exception",
         "reason": "Invalid format: \"2016-01-01 00:07:10\" is malformed at \" 00:07:10\""
      }
   },
   "status": 400
}
```

If I try to insert it on the first type, this is working... Perhaps bug, or just my mistake but i do not understand.

FYI: if i swap the order of the type inserted, it is always the first that it work correctly.
</description><key id="130424170">16348</key><summary>Dynamic mapping does not work with more than one type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AColmant</reporter><labels /><created>2016-02-01T17:20:38Z</created><updated>2016-02-02T16:16:47Z</updated><resolved>2016-02-02T11:38:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-02-02T11:38:35Z" id="178521318">@AColmant thanks for the report, you are right this is a bug and you can find the fix for ES v2.3.0 via https://github.com/elastic/elasticsearch/issues/15568 and https://github.com/elastic/elasticsearch/issues/15138. 
</comment><comment author="AColmant" created="2016-02-02T16:16:47Z" id="178663807">Alright ! Sorry for the issue duplicate and thanks for your job.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify ToXContent contract</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16347</link><project id="" key="" /><description>ElasticsearchException implements ToXContent but it doens't look like it works with XContentBuilder#writeValue because ElasticsearchException doesn't output its start and end object markers. Is this a bug? If so we should probably fix it and add more comments around toXContent's contract and probably make some simple way to test that implementers comply with the contract.
</description><key id="130422231">16347</key><summary>Clarify ToXContent contract</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-02-01T17:13:10Z</created><updated>2017-01-07T11:26:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-12T11:12:02Z" id="183279958">This is a good one. The reason why some ToXContent implementations don't call start and end object is that sometime you want to output within another context, where you for example have added some information for context. For an example look at org.elasticsearch.rest.BytesRestResponse#convert where we enrich the top level error. I'm not saying this is the only solution, but this is why it's like that...
</comment><comment author="nik9000" created="2016-02-12T13:36:54Z" id="183332285">Thanks for clarifying. I think it'd be more clear if we had two interfaces for the different contracts. We frequently use "innerToXContent" when we want to support this. I get that we have a bazillion interfaces already but it'd be nice to know more about the things that implement ToXContent. We could even have consistent tests for them!
</comment><comment author="javanna" created="2016-02-12T13:58:02Z" id="183342102">Would be great to clarify and clean up this contract for sure, in this case using a class that extends `ToXContent` doesn't say enough, you have to look at the implementation to see if you have to open a new object yourself or not.
</comment><comment author="javanna" created="2016-12-16T09:52:12Z" id="267557095">We have been encountering this inconsistency while writing parsing code for the High Level REST client. We could fix it as we go through all the `toXContent` methods (in separate PRs would be wise). I see that most of the objects don't open a start object, and output essentially fragments that are not valid without an ancestor. On the other hand, our `ActionResponse`s should always print out a complete object without the need to start and end the object externally, which some REST actions do and some others do not.

How about having `ToXContent` for complete objects and `ToXContentFragment` for fragments?</comment><comment author="javanna" created="2016-12-16T11:18:30Z" id="267573185">Discussed in FixItFriday, we agreed that this needs to be fixed. We are going to differentiate between the two contracts (self-contained objects and fragments) using two different interfaces.</comment><comment author="javanna" created="2017-01-07T11:26:36Z" id="271077949">#22387 is in, so we now have `ToXContent` for fragments and `ToXContentObject` for complete objects. What is left to do is moving more classes over from `ToXContent` to `ToXContentObject`.  That's why I'm keeping this issue open for now.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use allocation ids to prevent repeated recovery of failed shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16346</link><project id="" key="" /><description>code simplification
</description><key id="130414555">16346</key><summary>Use allocation ids to prevent repeated recovery of failed shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T16:44:16Z</created><updated>2016-02-02T12:55:55Z</updated><resolved>2016-02-02T12:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-01T16:50:26Z" id="178064060">WOW awesome. LGTM but I think @bleskes will need to look too
</comment><comment author="ywelsch" created="2016-02-01T17:54:39Z" id="178094976">I simplified the loop a bit (so we do not need goto to outer loop). can you have a look @bleskes?
</comment><comment author="bleskes" created="2016-02-01T19:34:30Z" id="178145698">This is great! I left some comments simplifying it even further..
</comment><comment author="ywelsch" created="2016-02-02T08:46:16Z" id="178451401">Thanks @bleskes. Java 8 streams to the rescue. Looks way better! I made one minor improvement to your suggestion:

```
if (shardRoutingTable.assignedShards().stream()
        .noneMatch(shr -&gt; shr.isSameAllocation(failedShardRouting))) {
    iterator.remove();
}
```
</comment><comment author="bleskes" created="2016-02-02T08:50:50Z" id="178453103">That noneMatch is sexy indeed. LGTM. thanks @ywelsch .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changes "that is" to "for example".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16345</link><project id="" key="" /><description>"Derivative of a derivative" and "second derivative" are synonyms, so the text should read id est (i.e.) instead of exempli gratia (e.g.).
</description><key id="130381944">16345</key><summary>Changes "that is" to "for example".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lbrito1</reporter><labels><label>docs</label></labels><created>2016-02-01T14:55:55Z</created><updated>2016-02-02T14:13:02Z</updated><resolved>2016-02-02T14:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T14:13:02Z" id="178591734">thanks @lbrito1 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tasks's should grow an XContent-able status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16344</link><project id="" key="" /><description /><key id="130371195">16344</key><summary>Tasks's should grow an XContent-able status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Task Manager</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T14:08:04Z</created><updated>2016-02-03T23:30:30Z</updated><resolved>2016-02-03T23:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-01T14:10:52Z" id="177988477">On second thought, no. Tasks descriptions should stay method references because they call toString that are only needed if the task's status is fetched which is rare.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>https://maven.elasticsearch.org certficate expired</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16343</link><project id="" key="" /><description>The certificate from https://maven.elasticsearch.org/releases is expired.
</description><key id="130353295">16343</key><summary>https://maven.elasticsearch.org certficate expired</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mrsolo/following{/other_user}', u'events_url': u'https://api.github.com/users/mrsolo/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mrsolo/orgs', u'url': u'https://api.github.com/users/mrsolo', u'gists_url': u'https://api.github.com/users/mrsolo/gists{/gist_id}', u'html_url': u'https://github.com/mrsolo', u'subscriptions_url': u'https://api.github.com/users/mrsolo/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/96771?v=4', u'repos_url': u'https://api.github.com/users/mrsolo/repos', u'received_events_url': u'https://api.github.com/users/mrsolo/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mrsolo/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mrsolo', u'type': u'User', u'id': 96771, u'followers_url': u'https://api.github.com/users/mrsolo/followers'}</assignee><reporter username="">brackxm</reporter><labels /><created>2016-02-01T12:43:53Z</created><updated>2016-02-01T18:17:29Z</updated><resolved>2016-02-01T18:17:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-01T13:00:13Z" id="177962289">Thanks for reporting the certificate expiry. We're working on it.
</comment><comment author="mrsolo" created="2016-02-01T18:17:29Z" id="178107732">Fixed

&lt;img width="1419" alt="art cert" src="https://cloud.githubusercontent.com/assets/96771/12726528/f0e76d38-c8cc-11e5-823b-9b0a341eb0e9.png"&gt;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the Network Addresses community plugin link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16342</link><project id="" key="" /><description>This time to the old docs :)
</description><key id="130329841">16342</key><summary>Added the Network Addresses community plugin link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofir123</reporter><labels><label>docs</label></labels><created>2016-02-01T10:59:12Z</created><updated>2016-02-02T14:08:50Z</updated><resolved>2016-02-02T14:08:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T14:08:50Z" id="178588292">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert PageCacheRecycler settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16341</link><project id="" key="" /><description /><key id="130323273">16341</key><summary>Convert PageCacheRecycler settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T10:35:03Z</created><updated>2016-02-02T14:10:03Z</updated><resolved>2016-02-01T12:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-02-01T10:36:46Z" id="177903693">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend fsprobe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16340</link><project id="" key="" /><description>Depends on https://github.com/elastic/elasticsearch/pull/16017 and replaces https://github.com/elastic/elasticsearch/pull/15330
</description><key id="130322852">16340</key><summary>Extend fsprobe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">beiske</reporter><labels><label>:Stats</label><label>low hanging fruit</label></labels><created>2016-02-01T10:33:08Z</created><updated>2016-05-12T12:50:33Z</updated><resolved>2016-05-12T12:50:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T17:01:38Z" id="206465405">@tlrx I think you wrote the FsProbe stuff when moving it from Sigar (is that correct?) maybe you can take a look at this?
</comment><comment author="beiske" created="2016-04-08T09:45:44Z" id="207352381">Backporting this to 2.x would also be preferable, but the implementation would likely have to be a little different due to more extensive use of guice in 2.x.
</comment><comment author="tlrx" created="2016-04-08T11:19:48Z" id="207385143">I think we should not allow to directly set/override the FS probe but we should use a dedicated extension point so that plugins can register their own `FsProbe`.

To do that, we need to create a `MonitorModule` that handles extension points for `FsProbe`. It could have a setting like `probe.fs.type` with a default value `default` that fallback to the current `FsProbe` implementation.

Then, plugins will be able to register their own probes using something like:

``` java
    public void onModule(MonitorModule module) {
        module.registerFsProbe("custom_fs", CustomFsProbe.class);
    }
```

Few things will also need to be changed in `NodeModule` and `Node` but it should be OK.
</comment><comment author="beiske" created="2016-05-12T12:50:33Z" id="218747200">Closing because this because intended usage doesn't scale at all.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GroovyScriptIT fails with SIGSEGV</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16339</link><project id="" key="" /><description>We've encountered a segment violation in [build 663 for ES 2.0](http://build-us-00.elastic.co/job/elasticsearch-20-medium/663) in `GroovyScriptIT`:

```
[ERROR] ERROR: JVM J0 ended with an exception: Forked process returned with error code: 134. Very likely a JVM crash.  See process stdout at: /mnt/jenkins/workspace/elasticsearch-20-medium/core/target/junit4-J0-20160131_224721_430.sysout
```
- JDK: 8.0_60-b27 (build 1.8.0_60-b27)
- OS: Ubuntu 12.04.3 LTS (Kernel 3.2.0-52)
- VM Args: -Xmx512m -Xms512m -XX:MaxDirectMemorySize=512m -Des.logger.prefix= -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/jenkins/workspace/elasticsearch-20-medium
- Variables: -Dtests.prefix=tests -Dtests.seed=7C91414F00CB3DCD -Des.logger.level=DEBUG -Des.node.local= -Des.node.mode=local -DhaltOnFailure= -Djava.awt.headless=true -Djava.io.tmpdir=./temp -Dtests.appendseed= -Dtests.assertion.disabled=false -Dtests.awaitsfix= -Dtests.badapples= -Dtests.bwc= -Dtests.bwc.path= -Dtests.bwc.version= -Dtests.class= -Dtests.client.ratio= -Dtests.cluster= -Dtests.compatibility= -Dtests.config= -Dtests.enable_mock_modules= -Dtests.failfast= -Dtests.filter= -Dtests.heap.size=512m -Dtests.iters= -Dtests.jvm.argline= -Dtests.locale=random -Dtests.maven=true -Dtests.maxfailures= -Dtests.method= -Dtests.network=true -Dtests.nightly= -Dtests.project=org.elasticsearch:elasticsearch -Dtests.rest= -Dtests.rest.blacklist=cat.recovery/10_basic/\* -Dtests.rest.load_packaged= -Dtests.rest.spec= -Dtests.rest.suite= -Dtests.security.manager=true -Dtests.showSuccess= -Dtests.thirdparty= -Dtests.timeoutSuite=500000 -Dtests.timezone=random -Dtests.verbose=false -Dtests.verify.phase=true -Dtests.version=2.0.3-SNAPSHOT -Dtests.weekly= -Djunit4.childvm.cwd=/mnt/jenkins/workspace/elasticsearch-20-medium/core/target/J0 -Djunit4.childvm.id=0 -Djunit4.childvm.count=1

Core dumps were disabled on the build machine. Find attached stdout ([junit4-J0-20160131_224721_430.sysout](https://github.com/elastic/elasticsearch/files/112030/junit4-J0-20160131_224721_430.sysout.txt)) and the JVM crash log ([hs_err_pid3786.log](https://github.com/elastic/elasticsearch/files/112031/hs_err_pid3786.log.txt)).
</description><key id="130318838">16339</key><summary>GroovyScriptIT fails with SIGSEGV</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Scripting</label><label>test</label><label>v2.0.3</label></labels><created>2016-02-01T10:16:18Z</created><updated>2016-12-19T17:00:10Z</updated><resolved>2016-12-19T16:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-12-19T11:22:30Z" id="267941569">The Groovy scripting language was deprecated in 5.0 and removed in 6.0 with #21607 so I'd keep this ticket open at most as long as we run scheduled builds for ES 5.x.</comment><comment author="jasontedor" created="2016-12-19T15:01:44Z" id="267985373">Can we just close it? It doesn't look like it's a recurring issue, and I don't think anyone is digging/is going to dig into this unless it recurs frequently?</comment><comment author="danielmitterdorfer" created="2016-12-19T16:53:16Z" id="268016104">Sure, I'm fine with closing it; I was just conservatively adding a comment instead of closing it preliminary. :)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FieldValueFactorFunction should use 'missing' when field does't exist in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16338</link><project id="" key="" /><description>It's common case for continuous deployment to change elasticsearch's client and index independently. This leads to the case when `fieldValueFactorFunction` is added to the client but index is not rebuilt yet. I use `missing` property. Unfortunately [FieldValueFactorFunctionParser](https://github.com/elastic/elasticsearch/blob/528f6481eaadd2a0585dc6731a94d7a024b8ce29/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java#L89) demands to have a field in a mapping, it throws exception otherwise. This means we can't deploy an client before index is updated despite having `missing` property set. The suggestion here is to look as `missing` in case of field in the document or mapping are missing.
</description><key id="130286328">16338</key><summary>FieldValueFactorFunction should use 'missing' when field does't exist in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edudar</reporter><labels /><created>2016-02-01T07:51:10Z</created><updated>2016-02-02T14:03:10Z</updated><resolved>2016-02-02T14:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T14:03:09Z" id="178585634">Duplicate of #12016
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch tribe node'ip address is public ip or private?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16337</link><project id="" key="" /><description>Elasticsearch tribe node'ip address is public ip or private?

Thasnks U ...
</description><key id="130279596">16337</key><summary>Elasticsearch tribe node'ip address is public ip or private?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2016-02-01T07:05:16Z</created><updated>2016-02-01T07:12:53Z</updated><resolved>2016-02-01T07:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-01T07:12:53Z" id="177817659">Please ask questions on discuss.elastic.co. We'll help you better there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce complexity of plugin cli</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16336</link><project id="" key="" /><description>The plugin cli currently is extremely lenient, allowing most errors to
simply be logged. This can lead to either corrupt installations (eg
partially installed plugins), or confused users.

This change rewrites the plugin cli to have almost no leniency.
Unfortunately it was not possible to remove all leniency, due in
particular to how config files are handled.

The following functionality was simplified:
- The format of the name argument to install a plugin is now an official
  plugin name, maven coordinates, or a URL.
- Checksum files are required, and only checked, for official plugins
  and maven plugins. Checksums are also only SHA1.
- Downloading no longer uses a separate thread, and no longer has a timeout.
- Installation, and removal, attempts to be atomic. This only truly works
  when no config or bin files exist.
- config and bin directories are verified before copying is attempted.
- Permissions and user/group are no longer set on config and bin files.
  We rely on the users umask.
- config and bin directories must only contain files, no subdirectories.
- The code is reorganized so each command is a separate class. These
  classes already existed, but were embedded in the plugin cli class, as
  an extra layer between the cli code and the code running for each command.
</description><key id="130226934">16336</key><summary>Reduce complexity of plugin cli</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-02-01T01:41:29Z</created><updated>2016-02-01T15:20:08Z</updated><resolved>2016-02-01T15:20:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-01T01:54:55Z" id="177697321">Note that testing of installing official plugins by name, or plugins via maven coordinates is still TODO, and will require an https fixture. However, this was not really tested before (the tests were skipped and relied on actually hitting eg github or our real download service). This can happen in a follow up.
</comment><comment author="rjernst" created="2016-02-01T02:01:19Z" id="177702206">Also note that this PR is the beginning of what will allow us to separate the plugin cli into its own jar, which will allow us to run plugin cli under security manager, as well as better test it (eg move tests which need extra permissions like modifying file permissions from evil-tests next to the plugin cli code).
</comment><comment author="s1monw" created="2016-02-01T09:25:33Z" id="177872816">I love the strictness and how structured this is now.This is a huge improvement. I looked over the code in src/main and I think we should get it in as is. It's a huge improvemnet! thanks LGTM
</comment><comment author="dadoonet" created="2016-02-01T09:34:24Z" id="177875142">Nice stats! And much easier to read.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logs added for support Issue #17983</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16335</link><project id="" key="" /><description>We had a hard time finding on why our custom plugins were not working. A log would have been very useful. Adding a useful log which no parent file is found.

For Eg: 

sudekumar@LM-BLR-00668412:/tmp/check$./elasticsearch-3.0.0-SNAPSHOT/bin/plugin install file:///Users/sudekumar/GIT/EXAMPLE/elasticsearch-ebay-managed-plugin/target/releases/elasticsearch-ebay-managed-2.0.0.zip  -Des.cli.debug=true
-&gt; Installing from file:/Users/sudekumar/GIT/EXAMPLE/elasticsearch-ebay-managed-plugin/target/releases/elasticsearch-ebay-managed-2.0.0.zip...
Trying file:/Users/sudekumar/GIT/EXAMPLE/elasticsearch-ebay-managed-plugin/target/releases/elasticsearch-ebay-managed-2.0.0.zip ...
Downloading ....................................................................................................................................................................................................................................................................DONE
Verifying file:/Users/sudekumar/GIT/EXAMPLE/elasticsearch-ebay-managed-plugin/target/releases/elasticsearch-ebay-managed-2.0.0.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
Cannot find parent for / .Ignoring file...
</description><key id="130172290">16335</key><summary>Logs added for support Issue #17983</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sudekumar</reporter><labels /><created>2016-01-31T19:28:37Z</created><updated>2016-02-02T14:55:09Z</updated><resolved>2016-02-02T14:55:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-02T04:26:01Z" id="178365424">@sudekumar Thank you for the PR. While I understand the frustration this must have caused, finding an entry like that indicates a broken plugin zip.  Imagine if it was `/absolute/path/to/necessary.jar`. I've opened another PR to fail hard on this, along with a test. Please see #16361.
</comment><comment author="clintongormley" created="2016-02-02T14:55:09Z" id="178614665">Closing in favour of https://github.com/elastic/elasticsearch/pull/16361
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Equal default min and max heap settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16334</link><project id="" key="" /><description>Today we encourage users to set their minimum and maximum heap settings
equal to each other to prevent the heap from resizing. Yet, the default
heap settings do not start Elasticsearch in this fashion. This commit
addresses this discrepancy by setting the default min heap to '1g' and
the default max heap to the default min heap.
</description><key id="130171710">16334</key><summary>Equal default min and max heap settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2016-01-31T19:23:21Z</created><updated>2016-05-13T17:23:50Z</updated><resolved>2016-05-11T14:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-31T19:25:24Z" id="177593484">@mikemccand @rmuir This is why the Elasticsearch trace that we were looking at a few days ago showed the committed heap not starting at 1g but instead started at 256m and stair-stepped up to 1g.
</comment><comment author="dadoonet" created="2016-01-31T19:58:03Z" id="177597449">++
</comment><comment author="mikemccand" created="2016-02-01T10:43:19Z" id="177907050">Hmm, thinking about this more, I'm not sure we should do this?

Yes, we do make this recommendation, to maximize performance for e.g. production installations.

But when someone is just launching ES at defaults, playing with a node or three on their laptop, I think the current default is maybe better, such that java only asks the OS for a smallish up front chunk of RAM for each java process?
</comment><comment author="bleskes" created="2016-02-01T10:55:03Z" id="177913381">&gt; But when someone is just launching ES at defaults, playing with a node or three on their laptop, I think the current default is maybe better, such that java only asks the OS for a smallish up front chunk of RAM for each java process?

I'm not sure what the background of the PR is (it seems to be a follow up on another discussion), but +1 to what Mike said - our current defaults are geared towards downloading ES and just trying it out/dev mode. As such, we shouldn't just claim 1GB of memory.. 
</comment><comment author="jasontedor" created="2016-04-27T14:13:50Z" id="215097017">@clintongormley I'd like to reconsider this pull request since we so strongly encourage this to be the case and since #17728 we warn on startup if it is not the case (and even fail if bound to a non-loopback interface).

```
[2016-04-27 10:03:59,228][WARN ][bootstrap                ] [Songbird] initial heap size [268435456] not equal to maximum heap size [1073741824]; this can cause resize pauses and prevents mlockall from locking the entire heap
```

I think one possibility is to set the default min and max to 256m or 512m if the concern is that 1g is too much?
</comment><comment author="clintongormley" created="2016-04-29T08:04:35Z" id="215653803">&gt; since #17728 we warn on startup if it is not the case (and even fail if bound to a non-loopback interface).

If we set min/max to the same value at startup, we lose the ability to detect that the heap hasn't been set correctly when moving into production.  Given that we fail hard in production if not set, what is the benefit of making this change for the OOB case?
</comment><comment author="jasontedor" created="2016-04-29T13:17:36Z" id="215708319">&gt; If we set min/max to the same value at startup, we lose the ability to detect that the heap hasn't been set correctly when moving into production.

That's a good point, but easily adjusted to. We could fail if non-equal or if the heap is small (&lt; 1 GB)?

&gt; Given that we fail hard in production if not set, what is the benefit of making this change for the OOB case?

I continue to find it odd that we so strongly encourage the practice but do not ship with it configured as such.
</comment><comment author="clintongormley" created="2016-04-29T16:25:41Z" id="215793290">&gt; I continue to find it odd that we so strongly encourage the practice but do not ship with it configured as such.

We ship for a good OOB dev experience.  We don't know if somebody is going to run one instance and try to fill it with data, or start 4 nodes on their laptop to check out clustering...

Personally I like the flexibility of the current min/max, but I may be missing something
</comment><comment author="clintongormley" created="2016-04-29T16:26:08Z" id="215793551">One thing is for sure, we expect people to set the heap to something other than whatever we default to.
</comment><comment author="jasontedor" created="2016-04-29T16:30:34Z" id="215795626">&gt; If we set min/max to the same value at startup, we lose the ability to detect that the heap hasn't been set correctly when moving into production.

@clintongormley Reflecting on this further, I see the point of the min/max size check differently. I see the point as ensuring that the min/max are the same, not that the values have been tuned from their defaults. On any reasonable production workload, the user is going to find out very quickly if they haven't tuned away from the defaults, especially if we tune down to 256m or 512m like I'm proposing.

&gt; We ship for a good OOB dev experience. We don't know if somebody is going to run one instance and try to fill it with data, or start 4 nodes on their laptop to check out clustering...

That's why I'm proposing to tune it down to 256m or 512m.

&gt; Personally I like the flexibility of the current min/max, but I may be missing something

I think I'm missing what the flexibility is? What I see is us repeatedly communicating to set min equal to max, and then not shipping as such.
</comment><comment author="jasontedor" created="2016-04-29T16:31:43Z" id="215796158">&gt; One thing is for sure, we expect people to set the heap to something other than whatever we default to.

Exactly. And if they don't and are doing anything serious, they will know _very_ quickly (especially if we tune it down to satisfy running multiple nodes on a laptop).
</comment><comment author="clintongormley" created="2016-05-05T08:53:29Z" id="217105876">I've been thinking about this some more and am swinging around to the other side.  With things as they are today, if a user needs to bind to anything other than localhost in their dev environment, then suddenly they have to specify a bunch of stuff: min master nodes, min/max heap size, etc...

We now have an escape hatch for this situation (https://github.com/elastic/elasticsearch/pull/18088) but I am wondering if it would be better for us to ship with min=max heap as a default, perhaps 500M?

&gt; &gt; If we set min/max to the same value at startup, we lose the ability to detect that the heap hasn't been set correctly when moving into production.
&gt; 
&gt; That's a good point, but easily adjusted to. We could fail if non-equal or if the heap is small (&lt; 1 GB)?

This would kinda defeat the reason that I've suggested for shipping with min=max...

Still undecided
</comment><comment author="jasontedor" created="2016-05-05T09:12:20Z" id="217108385">&gt; We now have an escape hatch for this situation (#18088)

Note that that only covers "system" checks, not checks that the end-user definitely has complete control over. 

&gt; but I am wondering if it would be better for us to ship with min=max heap as a default, perhaps 500M?

Yes. :smile:

&gt; &gt; That's a good point, but easily adjusted to. We could fail if non-equal or if the heap is small (&lt; 1 GB)?
&gt; &gt; This would kinda defeat the reason that I've suggested for shipping with min=max...

Yeah that's a good point; I implicitly backed away from this in a previous [comment](https://github.com/elastic/elasticsearch/pull/16334#issuecomment-215795626) for different reasons. 

&gt; Still undecided 

That's fair. Thanks for still being open to the proposal. (Note that I haven't pushed a commit to make the min equal to the max equal to 512 MB, but will if we come to agreement.)
</comment><comment author="jasontedor" created="2016-05-11T14:39:26Z" id="218479918">@clintongormley I've updated the PR as discussed.
</comment><comment author="clintongormley" created="2016-05-11T14:45:54Z" id="218481927">&gt; I've updated the PR as discussed.

Some background to the discussion.  We currently ship with min != max, which means that we log a warning about bad settings out of the box.  Also, binding to anything other than localhost immediately requires editing a bunch of things to make elasticsearch work, even if we're actually still in development mode.  Shipping with min==max means we (a) don't log a warning by default and (b) there's one less thing to edit.

We've chosen 512m as it's enough space to hold a fair bit of data while not so much that you can't start a few nodes on your dev machine.  If there isn't enough memory available, the node will fail at startup so the problem will be obvious.

LGTM
</comment><comment author="rmuir" created="2016-05-12T12:04:52Z" id="218737377">is 512 MB really enough memory for ES to run? https://benchmarks.elastic.co/index.html
</comment><comment author="jasontedor" created="2016-05-12T12:11:17Z" id="218738589">&gt; is 512 MB really enough memory for ES to run?

It's not enough for the benchmarks to run at their previous level of throughput, but I don't think that should dictate the defaults that we use here because these defaults are oriented towards the out-of-the-box experience, not production and not the benchmarks.
</comment><comment author="rmuir" created="2016-05-12T12:18:22Z" id="218740017">I ask if its enough memory for ES to run when GC times increase **25x**

The dataset being used there is tiny.
</comment><comment author="clintongormley" created="2016-05-12T12:20:04Z" id="218740354">@rmuir what's your take on setting min==max vs what we do today (min=250m, max=1g)
</comment><comment author="rmuir" created="2016-05-12T12:20:55Z" id="218740512">I think we should use as little RAM as possible: but in this case I am saying that something is wrong.
</comment><comment author="rmuir" created="2016-05-12T12:24:39Z" id="218741281">and @clintongormley yes, min == max does not really matter to me at all. I don't agree with the mlockall and all that, I think everyone knows that. We should keep that stuff separate.

Here, I think we should look at that old GC chart. Something is wrong. Remember ES is a _ton_ of java classes with a _lot_ of dependencies. There are various RAM pigs swimming around in the code too. It may very well be that 512MB is simply not enough.
</comment><comment author="rmuir" created="2016-05-12T12:30:16Z" id="218742487">&gt; It's not enough for the benchmarks to run at their previous level of throughput, but I don't think that should dictate the defaults that we use here because these defaults are oriented towards the out-of-the-box experience, not production and not the benchmarks.

Besides the fact that something is truly wrong, I dont agree with this. People are going to benchmark the out of box configuration no matter what we tell them.

But this statement also disagrees with the change you made. mlockall is a (bad) thing that we recommend for production, its a production change. so I don't think it should dictate the defaults, just like you said :)
</comment><comment author="uschindler" created="2016-05-12T13:21:45Z" id="218754928">Hi, I don't handle Logstash customers; but my real fulltext customers get the following advice:
- Disable mlockall in really any case. Bad bad bad (sorry, like @rmuir I disagree here completely. mlockall is stuff only for very special _small_ processes that need realtime features; but not monster processes with tens of gigabytes of heap)
- Set swappiness to 1 (not 0 !!!)
- Enable MMAP from the beginning (nuke the hybrid one) and use Docvalues for sorting or facetting.

Customer is happy, their infrastructure does not complain and the system is stable also when somebody tries some heavy query or starts some amok process on the machine. With swappiness=1 the system at least has an option to recover and not crush with kernel panic or OOM killer killing your SSH daemon.

Nothing more to say from my side!
</comment><comment author="jasontedor" created="2016-05-12T22:16:25Z" id="218901838">&gt; People are going to benchmark the out of box configuration no matter what we tell them.

The benchmarks allocate around 335 GB in the TLABs and 40 GB out of of the TLABs during the indexing phase alone with 512m and 1g heaps. These heaps (whether it be 512m or 1g) are _tiny_ for sustaining high throughput here (442.44 MB/s on the 512m heap and 1.15 GB/s on the 1g for the sustained allocation rate in the TLABs).

If I increase the heap size for the benchmarks to 2g, the allocations shift to even more into the TLABs at a sustained rate of 1.8 GB/s during indexing. And if I increase the heap size for the benchmarks to 4g, the allocation profile is basically unchanged from 2g (roughly the same amount in the TLABs and rate during indexing).

So my conclusion is that 512m and 1g are just too small to begin with for the benchmark workload.

Also, this issue has nothing to do with `mlockall` et al. so I think we should put that discussion aside for another time?
</comment><comment author="rmuir" created="2016-05-12T22:23:30Z" id="218903215">&gt; So my conclusion is that 512m and 1g are just too small to begin with for the benchmark workload

I index this particular dataset all the time with lucene, on ancient computers. I do this because its tiny and fast.
</comment><comment author="jasontedor" created="2016-05-12T22:28:39Z" id="218904203">&gt; I index this particular dataset all the time with lucene, on ancient computers. I do this because its tiny and fast.

That's not a valid comparison because there are no networking buffers and all of the other functionality of Elasticsearch that sits on top of Lucene. And more importantly, it does not negate what I've shown here. The benchmarks want to sustain 1.8 GB/s of allocation here&lt;sup&gt;1&lt;/sup&gt;, and 512m and 1g heaps are not going to allow them to do that.

&lt;sup&gt;1&lt;/sup&gt;:  On my workstation with i7-5930K, 2133 MHz DDR 4, Samsung 950 Pro M.2 SSD.
</comment><comment author="rmuir" created="2016-05-12T22:32:31Z" id="218904843">Its totally valid. The workload is trivial.
</comment><comment author="rmuir" created="2016-05-12T22:52:45Z" id="218908360">By the way... It is not that i am against your change. It is just that defaults are obviously broken. Defaults need to support indexing a trivial dataset. I have indexed this one over the years with many many configurations... Including 256mb heap with solr. I will open a separate blocker issue about this, because it can be fixed in two ways. We can give more ram by default... Or we can use less of it. But the defaults are broken.
</comment><comment author="jasontedor" created="2016-05-13T01:22:35Z" id="218929382">&gt; It is just that defaults are obviously broken. Defaults need to support indexing a trivial dataset.

I do not disagree with this. That was the point of my work, to show that even 1g is too small _for this dataset_.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make single node utility methods non-static</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16333</link><project id="" key="" /><description>With the recent change to allow ESSingleNodeTestCase to specify plugins
and Version for the node, node creation became lazy, where it now
happens before the first test to run. However, there were many static
methods on ESSingleNodeTestCase that still try to access the node. This
change makes those methods non-static.
</description><key id="130160842">16333</key><summary>Make single node utility methods non-static</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-31T18:01:25Z</created><updated>2016-02-01T16:46:58Z</updated><resolved>2016-02-01T16:46:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-01T08:31:53Z" id="177843856">left some comments @rjernst 
</comment><comment author="s1monw" created="2016-02-01T16:46:41Z" id="178062822">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Security Manager and scripting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16332</link><project id="" key="" /><description>Added docs explaining the impact of the Java Security Manager on scripting
languages, how to disable the JSM, and how to customise the classloader
whitelist.

Closes https://github.com/elastic/elasticsearch/issues/16094
Closes https://github.com/elastic/elasticsearch/issues/14290
</description><key id="130129556">16332</key><summary>Java Security Manager and scripting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v2.2.0</label></labels><created>2016-01-31T13:57:43Z</created><updated>2016-02-02T12:19:44Z</updated><resolved>2016-02-01T08:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-31T13:58:01Z" id="177506240">@rmuir please could you review?
</comment><comment author="rmuir" created="2016-01-31T16:10:41Z" id="177538410">+1, this looks great.
</comment><comment author="clintongormley" created="2016-02-01T08:51:54Z" id="177859826">thanks @rmuir 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>when set "index.max_result_window" to Long.MAX_VALUE get error "Failed to parse int setting"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16331</link><project id="" key="" /><description>When set `index.max_result_window` to `Long.MAX_VALUE` there is no problem.
index settings: 

``` json
    "settings" : {
      "index" : {
        "number_of_shards" : "5",
        "max_result_window" : "9223372036854775807",
        "creation_date" : "1454230598724",
        "number_of_replicas" : "0",
        "uuid" : "xSkP-DuMTdOE7tIvSpAnlw",
        "version" : {
          "created" : "2010199"
        }
      }
    }
```

but when i want to search in that index i get this error

``` json
{
  "error": {
    "root_cause": [
      {
        "type": "settings_exception",
        "reason": "Failed to parse int setting [index.max_result_window] with value [9223372036854775807]"
      }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
      {
        "shard": 0,
        "index": "alarm-2016-01-31",
        "node": "Z-Zz7yE_RJ6dnO3Uezo4og",
        "reason": {
          "type": "settings_exception",
          "reason": "Failed to parse int setting [index.max_result_window] with value [9223372036854775807]",
          "caused_by": {
            "type": "number_format_exception",
            "reason": "For input string: \"9223372036854775807\""
          }
        }
      }
    ]
  },
  "status": 500
}
```

If user can't set this value to `Long.MAX_VALUE`, don't set the value.
</description><key id="130106656">16331</key><summary>when set "index.max_result_window" to Long.MAX_VALUE get error "Failed to parse int setting"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arman1371</reporter><labels /><created>2016-01-31T09:25:26Z</created><updated>2016-02-02T19:14:45Z</updated><resolved>2016-01-31T11:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-31T11:59:39Z" id="177482127">Sorry about that! This is already fixed in master as part of the great setting rewrite (allows resetting settings to default, listing them along with their default, atomically applying changes, etc). Keep the value under Integer.MAX_VALUE and you should be fine.
</comment><comment author="clintongormley" created="2016-02-02T13:58:16Z" id="178584338">While @nik9000 is technically correct, setting this setting to MAXINT is really damaging to your system.  Expect it to fall over.  This is why we added this limit in the first place.
</comment><comment author="nik9000" created="2016-02-02T19:14:45Z" id="178766048">&gt; While @nik9000 is technically correct, setting this setting to MAXINT is really damaging to your system. Expect it to fall over. This is why we added this limit in the first place.

Right. When I wrote "you should be fine" I meant, "the setting won't break anything" but @clintongormley is right, it is a bad idea to set it that high.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start to break lines at 140 characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16330</link><project id="" key="" /><description>Its what we say our maximum line length is in CONTRIBUTING.md and it'd be
nice to have a check on that. Unfortunately, we don't actually wrap all lines
at 140.
</description><key id="130061014">16330</key><summary>Start to break lines at 140 characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-31T01:23:35Z</created><updated>2016-02-02T02:41:47Z</updated><resolved>2016-02-02T02:41:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-31T01:24:09Z" id="177353282">If you want to review this I suggest userstyles and the github wide style.
</comment><comment author="dakrone" created="2016-02-01T21:08:12Z" id="178190848">I left some bikesheds, but other than that, LGTM
</comment><comment author="nik9000" created="2016-02-01T21:15:18Z" id="178195531">Thanks for looking @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uppercase ells ('L') in long literals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16329</link><project id="" key="" /><description>This commit removes and forbids the use of lowercase ells ('l') in long
literals because they are often hard to distinguish from the digit
representing one ('1').

Relates #16279
</description><key id="130046037">16329</key><summary>Uppercase ells ('L') in long literals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-30T22:29:01Z</created><updated>2016-02-01T10:27:24Z</updated><resolved>2016-01-31T03:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-30T22:42:48Z" id="177322799">LGTM
</comment><comment author="nik9000" created="2016-01-30T23:50:06Z" id="177339675">I caught one stowaway but otherwise LGTM. The stowaway change won't hurt anything either.

I admit to only reading 20% of the change and then unfocusing my eyes and using pattern recognition to scan for errors. I didn't see any during that phase but I'm pretty sure this is safe.
</comment><comment author="jasontedor" created="2016-01-31T01:11:14Z" id="177352715">&gt; I caught one stowaway but otherwise LGTM. The stowaway change won't hurt anything either.

@nik9000 I wrote a second more complicated regex and caught one more that should not have been caught in SearchScrollIT.java (it also didn't matter if it was changed, but I'd rather not). I'll fix the two that were caught but should not have been before integrating.

For posterity, the substitution I used is `s{(?&lt;![^-+*/ ,\(&gt;&lt;])(\d+)l(?![^-+*/ ,\)&gt;&lt;;\n])}{$1L}g`.

&gt; I didn't see any during that phase but I'm pretty sure this is safe.

Yeah, `gradle check` passes.
</comment><comment author="nik9000" created="2016-01-31T01:25:34Z" id="177353338">Sure. Merge when you like.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>which operation has more priority in query_string?  AND or OR </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16328</link><project id="" key="" /><description>i tested query_stirng like script below attached.(test ES version: 1.7.2)
but, i don't know why this result happened.

(1) age:13 OR type:1 AND age:35 OR type:1       &lt;strong&gt;result=&gt;  {"name":"Paul", "age":35, "type":1} &lt;/strong&gt;
(2) ((age:13 OR type:1) AND age:35) OR type:1   &lt;strong&gt;result=&gt;  {"name":"Paul", "age":35, "type":1},{"name":"Paul", "age":13, "type":1}&lt;/strong&gt;
(3) age:13 OR (type:1 AND age:35) OR type:1     &lt;strong&gt;result=&gt;  {"name":"Paul", "age":35, "type":1},{"name":"Paul", "age":13, "type":1}&lt;/strong&gt;
(4) (age:13 OR type:1) AND (age:35 OR type:1)     &lt;strong&gt;result=&gt;  {"name":"Paul", "age":35, "type":1},{"name":"Paul", "age":13, "type":1}&lt;/strong&gt;

'(1)' query's result is different from '(2)', '(3)', '(4)'.
if operation order is left to right, '(1)' query's result needes to be equal to be '(2)'.
and if AND opeation has more priority,  '(1)' query's result needes to be equal to be '(3)'.
but nothing is matched. any reason?

&lt;pre&gt;[root@hadoop1 bin]# curl -XPUT http://127.0.0.1:9200/test
{"acknowledged":true}
[root@hadoop1 bin]# curl -XPUT http://127.0.0.1:9200/test/mytype/1 -d '{"name":"Paul", "age":35, "type":1}'
{"_index":"test","_type":"mytype","_id":"1","_version":1,"created":true}
[root@hadoop1 bin]# curl -XPUT http://127.0.0.1:9200/test/mytype/2 -d '{"name":"Paul", "age":13, "type":1}'
{"_index":"test","_type":"mytype","_id":"2","_version":1,"created":true}
[root@hadoop1 bin]# curl -XPUT http://127.0.0.1:9200/test/mytype/3 -d '{"name":"Paul", "age":35, "type":2}'
{"_index":"test","_type":"mytype","_id":"3","_version":1,"created":true}
[root@hadoop1 bin]# curl -XPOST localhost:9200/test/_search?pretty=true -d '{
  "query": {
    "query_string": {
      "query": "age:13 OR type:1 AND age:35 OR type:1"
    }
  }
}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.125,
    "hits" : [ {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "1",
      "_score" : 1.125,
      "_source":{"name":"Paul", "age":35, "type":1}
    } ]
  }
}
[root@hadoop1 bin]# curl -XPOST localhost:9200/test/_search?pretty=true -d '{
&gt;   "query": {
&gt;     "query_string": {
&gt;       "query": "((age:13 OR type:1) AND age:35) OR type:1"
&gt;     }
&gt;   }
&gt; }'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.25,
    "hits" : [ {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "1",
      "_score" : 1.25,
      "_source":{"name":"Paul", "age":35, "type":1}
    }, {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "2",
      "_score" : 0.25,
      "_source":{"name":"Paul", "age":13, "type":1}
    } ]
  }
}
[root@hadoop1 bin]# curl -XPOST localhost:9200/test/_search?pretty=true -d '{
  "query": {
    "query_string": {
      "query": "age:13 OR (type:1 AND age:35) OR type:1"
    }
  }
}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name":"Paul", "age":35, "type":1}
    }, {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "2",
      "_score" : 0.6666667,
      "_source":{"name":"Paul", "age":13, "type":1}
    } ]
  }
}
[root@hadoop1 bin]# curl -XPOST localhost:9200/test/_search?pretty=true -d '{
  "query": {
    "query_string": {
      "query": "(age:13 OR type:1) AND (age:35 OR type:1)"
    }
  }
}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.25,
    "hits" : [ {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "1",
      "_score" : 1.25,
      "_source":{"name":"Paul", "age":35, "type":1}
    }, {
      "_index" : "test",
      "_type" : "mytype",
      "_id" : "2",
      "_score" : 1.25,
      "_source":{"name":"Paul", "age":13, "type":1}
    } ]
  }
}&lt;/pre&gt;
</description><key id="129999585">16328</key><summary>which operation has more priority in query_string?  AND or OR </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gm06041</reporter><labels /><created>2016-01-30T15:36:34Z</created><updated>2016-01-31T16:19:22Z</updated><resolved>2016-01-31T16:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-31T16:19:22Z" id="177540938">A [query string query](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/query-dsl-query-string-query.html#query-string-syntax) is effectively a wrapper to classic [Lucene query parser syntax](http://lucene.apache.org/core/5_4_1/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description). There are some [notoriously tricky](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/query-dsl-query-string-query.html#_boolean_operators) issues surrounding the precedence rules (cf. [LUCENE-1823](https://issues.apache.org/jira/browse/LUCENE-1823) and [`BooleanQuerySyntax`](https://wiki.apache.org/lucene-java/BooleanQuerySyntax) for historical background) and so if there's any possible confusion for precedence, you should just use [grouping](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/query-dsl-query-string-query.html#_grouping) so as to remove all such confusion.

For future reference, questions like this are best asked on the [Elastic Discourse forums](https://discuss.elastic.co) whereas bug reports and feature requests are best reported here. If you do have a followup question, please open a discussion there and the community will be very happy to engage with you. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch not search on not_analysed indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16327</link><project id="" key="" /><description>I am unable to figure out why elasticsearch not searching with `not_analysed` indexes. I have following settings in my model,

``` ruby
settings index: { number_of_shards: 1 } do
  mappings dynamic: 'false' do
    indexes :id
    indexes :name, index: 'not_analyzed'
    indexes :email, index: 'not_analyzed'
    indexes :contact_number
  end
 end
```

And my mapping at elasticsearch is right, as below.

``` ruby
{
  "users-development" : {
    "mappings" : {
      "user" : {
        "dynamic" : "false",
        "properties" : {
          "contact_number" : {
            "type" : "string"
          },
          "email" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "id" : {
            "type" : "string"
          },
          "name" : {
            "type" : "string",
            "index" : "not_analyzed"
          }
        }
      }
    }
  }
}
```

Following are the documents that have been indexed and should match

``` ruby

[{
      "_index" : "users-development",
      "_type" : "user",
      "_id" : "670",
      "_score" : 1.0,
      "_source":{"id":670,"email":"john@monkeyofdoom.com","name":"john","contact_number":null}
    },
{
      "_index" : "users-development",
      "_type" : "user",
      "_id" : "671",
      "_score" : 1.0,
      "_source":{"id":671,"email":"human@monkeyofdoom.com","name":"John","contact_number":null}
    }

, {
      "_index" : "users-development",
      "_type" : "user",
      "_id" : "736",
      "_score" : 1.0,
      "_source":{"id":736,"email":"tiger@monkeyofdoom.com","name":"tigher", "contact_number":null}
    } ]

```

But issue is when I make search on not analyzed fields (name and email, as I wanted them to be not analyzed) it never found any record. While on the contact index it works like charm.

I am searching as below

``` ruby
settings = {
    query: {
      filtered: {
        filter: {
          bool: {
            must: [
              { terms: { name: [ "john", "tiger" ] } },
            ]
          }
        }
      }
    },
    size: 10
  }

  User.__elasticsearch__.search(settings).records
```
</description><key id="129990758">16327</key><summary>Elasticsearch not search on not_analysed indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awais545</reporter><labels /><created>2016-01-30T14:11:27Z</created><updated>2016-02-01T13:44:00Z</updated><resolved>2016-02-01T13:43:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-01T13:43:03Z" id="177976820">Hi @awais545,

please note that we have a separate [discussion forum](http://discuss.elastic.co/) for such questions.

To your example: I get search results when I do this (tested with latest ES 2.x development version):
1. Create an index:

``` json
PUT /users-development
{
   "mappings": {
      "user": {
         "dynamic": "false",
         "properties": {
            "contact_number": {
               "type": "string"
            },
            "email": {
               "type": "string",
               "index": "not_analyzed"
            },
            "id": {
               "type": "string"
            },
            "name": {
               "type": "string",
               "index": "not_analyzed"
            }
         }
      }
   }
}
```

Add some data (note that you have added a user named "tigher" and I have corrected it to "tiger" which matches also your search query):

``` json
POST /_bulk
{"index":{"_index":"users-development","_type":"user"}}
{"id":670,"email":"john@monkeyofdoom.com","name":"john","contact_number":null}
{"index":{"_index":"users-development","_type":"user"}}
{"id":671,"email":"human@monkeyofdoom.com","name":"John","contact_number":null}
{"index":{"_index":"users-development","_type":"user"}}
{"id":736,"email":"tiger@monkeyofdoom.com","name":"tiger", "contact_number":null}

```

Search for users:

``` json
GET /users-development/user/_search
{
   "query": {
      "filtered": {
         "filter": {
            "bool": {
               "must": [
                  {
                     "terms": {
                        "name": [
                           "john",
                           "tiger"
                        ]
                     }
                  }
               ]
            }
         }
      }
   }
}
```

This returns the two users with the name "tiger" and "john":

``` json
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "users-development",
            "_type": "user",
            "_id": "AVKdDO3qnwqtR_2FR2c-",
            "_score": 1,
            "_source": {
               "id": 670,
               "email": "john@monkeyofdoom.com",
               "name": "john",
               "contact_number": null
            }
         },
         {
            "_index": "users-development",
            "_type": "user",
            "_id": "AVKdDO3qnwqtR_2FR2dA",
            "_score": 1,
            "_source": {
               "id": 736,
               "email": "tiger@monkeyofdoom.com",
               "name": "tiger",
               "contact_number": null
            }
         }
      ]
   }
}
```

But note that the user "John" is not found (as it is a not-analyzed field).

Maybe you're missing something when building the query? You can also check whether the mapping of the `name` field matches your expectation:

```
GET /users-development/_mapping/user/field/name?include_defaults=true
```

Closing as this is not an issue.

If you have further questions don't hesitate to post your question to the [discussion forum](http://discuss.elastic.co/).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove multicast plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16326</link><project id="" key="" /><description>closes #16310.
</description><key id="129931540">16326</key><summary>Remove multicast plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-01-30T02:42:24Z</created><updated>2016-02-02T14:25:29Z</updated><resolved>2016-02-01T15:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-30T02:44:01Z" id="177052485">@clintongormley I'm happy to do the deprecation part for 2.x as well, but I'm not sure "how" you want to do that (just in docs? with a warning?)
</comment><comment author="jasontedor" created="2016-01-31T03:27:17Z" id="177372390">LGTM.
</comment><comment author="clintongormley" created="2016-02-02T14:25:29Z" id="178597621">@rjernst i added the deprecation notice in https://github.com/elastic/elasticsearch/pull/16371
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Exception class name to message in `NotSerializableExceptionWrapper`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16325</link><project id="" key="" /><description>Previously it's possible to get errors like:

```
Caused by: NotSerializableExceptionWrapper[d:\shared_data\afs-issue-1-1\23]
```

Which doesn't tell us what the underlying exception type was that could
not be serialized.

This changes the message to prepend the
`ElasticsearchException.getExceptionName()` of the exception (which is a
underscore case of the class with the leading 'Elasticsearch' removed)
</description><key id="129910567">16325</key><summary>Add Exception class name to message in `NotSerializableExceptionWrapper`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T23:26:58Z</created><updated>2016-02-04T23:06:47Z</updated><resolved>2016-01-31T01:20:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-29T23:27:32Z" id="177016322">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add processor tags to on_failure metadata in ingest pipeline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16324</link><project id="" key="" /><description>adds an `on_failure_processor_tag` ingest metadata field that is accessible via templating within failure processors in pipelines.

closes #16202
</description><key id="129902104">16324</key><summary>Add processor tags to on_failure metadata in ingest pipeline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T22:38:22Z</created><updated>2016-02-08T09:11:20Z</updated><resolved>2016-02-01T18:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-01T17:53:35Z" id="178094528">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tell users how to tell if they're running init or systemd.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16323</link><project id="" key="" /><description>I made this matching PR for Kibana docs https://github.com/elastic/kibana/pull/6052
</description><key id="129895630">16323</key><summary>Tell users how to tell if they're running init or systemd.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeeDr</reporter><labels><label>docs</label><label>review</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T22:06:32Z</created><updated>2016-02-01T10:28:01Z</updated><resolved>2016-01-31T18:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-29T22:08:02Z" id="176991139">LGTM
</comment><comment author="LeeDr" created="2016-01-29T23:22:00Z" id="177014376">@nik9000 I don't know if this only goes in master or where it should go?
</comment><comment author="jasontedor" created="2016-01-31T18:56:39Z" id="177584048">Thanks @LeeDr! I integrated this into master and backported it to 2.x., 2.2., 2.1, 2.0, and 1.7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move DeDotProcessor into its own plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16322</link><project id="" key="" /><description>In light of new developments around how Elasticsearch is going to handle "."s in fields, This processor is not as required and can be moved into its own plugin.

open question: should this be moved into its own repository, and removed from ES entirely?
</description><key id="129895360">16322</key><summary>move DeDotProcessor into its own plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>non-issue</label></labels><created>2016-01-29T22:05:33Z</created><updated>2016-02-08T09:25:01Z</updated><resolved>2016-02-02T22:12:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-02-02T22:12:03Z" id="178854461">After some discussion, we determined that we can simply remove it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent index closing while snapshot is restoring the indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16321</link><project id="" key="" /><description>On 1.7.1.  This may be related to https://github.com/elastic/elasticsearch/issues/15432 , but it is unclear in #15432 what the cause was.   So I am filing a separate ticket on this.

In short, it looks like when snapshot restore is running, if the indices are closed while it is operating on a shard, it will leave the snapshot restore request in a STARTED state.

```
    "restore" : {
      "snapshots" : [ {
        "snapshot" : "snapshot_name",
        "repository" : "repository_name",
        "state" : "STARTED",
```

And shards that are in INIT state as reported by the restore request:

```
{
          "index" : "pricing_2015121502",
          "shard" : 1,
          "state" : "INIT"
}
```

So that when the end user tries to kick off another restore, it will fail:

```
failed to restore snapshot
org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [repository_name:snapshot_name] Restore process is already running in this cluster
    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:174)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:374)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:196)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:162)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Because only 1 snapshot restore request can be run at any point in time.  

It will be a good idea for us to implement a check to prevent users from closing indices while the restore operation is working on those shards which will help prevent this type of issue.
</description><key id="129883856">16321</key><summary>Prevent index closing while snapshot is restoring the indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2016-01-29T21:12:33Z</created><updated>2016-03-10T17:51:04Z</updated><resolved>2016-03-10T17:51:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-02T13:53:31Z" id="178582789">@ywelsch please could you take a look at this
</comment><comment author="ywelsch" created="2016-02-03T17:11:04Z" id="179352288">I verified the issue on master. It is present on all ES versions.

To fix this, there are two possible ways:
- Fail closing an index if there are shards of that index still being restored.
- Always let the index close operation succeed but fail the restore process of the shards of that index. This is more in line with what we currently do for index deletes that happen during the snapshot restore process.

@clintongormley wdyt?
</comment><comment author="ppf2" created="2016-02-03T17:32:23Z" id="179363622">&gt; Fail closing an index if there are shards of that index still being restored.

At least for this particular use case, this option is desirable because they didn't realize that the restore is still in progress (would like the restore to proceed), and not because they want to cancel a specific restore.
</comment><comment author="ywelsch" created="2016-02-04T07:58:09Z" id="179699305">@ppf2 what's your take on closing the index during snapshot (in contrast to restore)? Currently, the close operation succeeds but we fail the shards that are closed.
</comment><comment author="ppf2" created="2016-02-04T18:19:12Z" id="179979210">&gt; Currently, the close operation succeeds but we fail the shards that are closed.

For this specific report from the field, the indices did close, but the restore status from the cluster state shows a shard apparently stuck in INIT state - reopening the index did not resolve the issue, but deleting the index helped get the restore out of that stuck state.  So somehow the close operation did not successfully fail the shards, but left them the restore procedure in a started state, thinking that it is still initializing the restore on one of the shards.

```
{
          "index" : "pricing_2015121502",
          "shard" : 1,
          "state" : "INIT"
}
```
</comment><comment author="ywelsch" created="2016-02-04T19:42:08Z" id="180018923">@ppf2 I think you misunderstood me, I'm not claiming that the close operation successfully failed the restore process. My question was related to the comment of yours saying that it would be preferable to fail closing an index if there are shards of that index still being restored. I wanted to know whether you think the same applies to the snapshot case. So my question was: Should closing an index fail while the index is being snapshotted?

My goal here is to find out what the expected behavior should be in the following situations:
- Deleting an index during snapshot operation of that index
- Closing an index during snapshot operation of that index
- Deleting an index during restore operation of that index
- Closing an index during restore operation of that index

@clintongormley care to chime in?
</comment><comment author="ppf2" created="2016-02-04T20:33:52Z" id="180037992">@ywelsch Ah sorry misinterpreted :) 

I am thinking for deletions, we can cancel that snapshot operation (only on shards of that index) and perform necessary cleanup so that it doesn't end up with partial files in the repository.  Same thing with restore, cancel the restore of the shards for that index, and do some cleanup in the target cluster.  For closing, maybe we can just prevent them from doing so and allow the snapshot or restore operation to complete?   But yah, let's get input from @clintongormley @imotov 
</comment><comment author="ywelsch" created="2016-02-05T15:53:39Z" id="180412299">Here are my initial thoughts:

For restore:
- Closing an index that is being restored from a snapshot should fail the close operation. There is no good reason to do allow this, as closing an index that is being restored makes the index unusable (it cannot be recovered). What the user probably intends to do is to delete the index.
- Deleting an index that is being restored should cancel the restore operation for that index and delete the index. Deleting an index that is being restored would be a simple approach to cancel the restore process for that index. An open point for me is whether the restore information returned after finishing the restore process should take the deleted index into account in its statistics and status.

For snapshot:
- Closing an index that is being snapshotted should only succeed if snapshot is marked as partial. If so, index should be closed and snapshot should abort snapshotting the shards of that index. If shards of the deleted index have been already successfully snapshotted in the meantime, they will remain in the snapshot.
- Deleting an index that is being snapshotted should only succeed if snapshot is marked as partial. If so, index should be deleted and snapshot should abort snapshotting the shards of that index. If shards of the deleted index have been already successfully snapshotted in the meantime, they will remain in the snapshot.

Note: If the user does not want to wait for non-partial snapshot to finish before executing close/delete, he has to cancel the snapshot operation by deleting the snapshot in progress.
</comment><comment author="imotov" created="2016-02-05T16:16:18Z" id="180425491">+1 on the restore part but I don't think we should abort the snapshot if an index is closed or deleted. That might lead to unexpected data loss. When users use partial snapshot they have certain set of partially available indices that they have in mind (basically indices that were partially available at the beginning of the snapshot). The proposed behavior arbitrary extends this to any index that happened to be copied over when the close or delete operation is performed, I think we should keep a lock on the shard and finish the snapshot operation before closing it.
</comment><comment author="ywelsch" created="2016-02-05T16:25:33Z" id="180428495">@imotov Can you elaborate a bit more on the use cases for partial snapshots? The only one I have in mind is the following: As part of a general backup mechanism, hourly / daily snapshots are triggered (e.g. by cron job). The idea of partial snapshots would then be to snapshot as much as possible, even if some indices / shards are unavailable at that point in time. My proposed solution would be in line with that idea.
</comment><comment author="imotov" created="2016-02-09T14:00:32Z" id="181874857">@ywelsch I see. I thought about partial snapshot as an emergency override that someone would deploy during a catastrophic event when they have a half working cluster and would like to make a snapshot of whatever they have left before taking some drastic recovery measures. During these highly stressful events the user might inadvertently close a half backed up index while thinking that they have a full copy.
</comment><comment author="clintongormley" created="2016-02-13T20:36:03Z" id="183749208">@ywelsch I agree with your proposals for restore, but I think we should fail to close or delete an index while a snapshot is in progress (partial or otherwise).

I realise that this might mean that the user is blocked from deleting an index while a background cron job is doing a snapshot.  Perhaps the exception can include a message about how to cancel the current snapshot, and provide the snapshot_id etc required to perform the cancellation in a script friendly format.  That way, if the delete/close happens in a script, the user can code around the blocked action.
</comment><comment author="ywelsch" created="2016-03-03T15:06:54Z" id="191804693">@imotov @clintongormley: Let me just recapitulate a bit:

For restore, we all agree that:
- deleting an index that is being restored should cancel the restore operation for that index and delete the index. Same behavior as before.
- closing an index that is being restored from a snapshot should fail the close operation. The previous behavior was to force-close the index and render it thereby unusable as it could not be recovered anymore upon opening.

For snapshot, we disagree. Currently, deleting an index that is being snapshotted results in two outcomes, depending on whether the snapshot was started as partial or not:
- If it was started as partial and an index is deleted during snapshotting, the index is deleted and the snapshot still completes but has the snapshot state partial.
- If it was started as non-partial, the index is deleted and the snapshot completes with the snapshot state failed.

**In both cases, the delete operation succeeds and takes priority over snapshotting.** We currently even have a test (`SharedClusterSnapshotRestoreIT.testDeleteIndexDuringSnapshot`) that checks exactly both of these scenarios and asserts the current behavior.

In light of that, let me make my case again:
- My suggestion is to change the behavior only for the case where the snapshot is not started as partial. In that case I want snapshotting to take priority and the delete to fail as the user explicitly requested to have a full snapshot of all the specified stuff. I apply the same reasoning to close.
- Your suggestions break way harder with the current way of doing snapshots. You're essentially saying that snapshots should always take priority (independently of whether started as partial or not) over deletes / closing. As I said before this does not play nicely with daily background snapshots (e.g. on cloud services). As for the disaster scenario outlined by @imotov, I don't think that doing snapshots **during** a disaster is exactly the scenario we want to optimize for. @clintongormley I'm missing any kind of argument why you think we should go this way.

In conclusion, the snapshot/delete-close case needs more discussion before I feel comfortable with implementing it. To not block too long on this discussion, I can in the meanwhile make a PR for the restore case.
</comment><comment author="clintongormley" created="2016-03-08T08:46:09Z" id="193665471">@ywelsch has convinced me of his argument: 
- Deleting an index during restore should cancel the restore of that index
- Closing an index during restore should fail the close request
- Closing or deleting an index during a full snapshot should fail the close/delete request
- Closing or deleting an index during a partial snapshot should succeed and mark the snapshot as partial.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add task cancellation mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16320</link><project id="" key="" /><description>Only tasks that extend CancellableTask can be cancelled using this mechanism. If a cancellable task has children it can elect to cancel all child tasks as well. In this case a special ban parent request is sent to all nodes. This request does two things: 1) it prevents any tasks with the banned parent task from being started, and 2) it cancels all currently running tasks that have the banned task as a parent. The ban is lifted as soon as the coordinating node notifies all other nodes that the cancelled task has finished executing. If the coordinating node leaves the cluster before it has a chance to lift its bans, all bans set by this coordinating node are automatically removed.

As an option a task can elect to automatically cancel all child tasks if their parent task was running on a node that just left the cluster. This option makes sense for cancellable heavy tasks that have no side-effects and only return results to the coordinating node. With the coordinating node gone, it doesn't make sense to run such tasks any longer since their results will be most likely discarded.
</description><key id="129875058">16320</key><summary>Add task cancellation mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>feature</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T20:33:47Z</created><updated>2016-02-10T04:18:34Z</updated><resolved>2016-02-10T04:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-01T09:14:51Z" id="177869659">I left a bunch of comments @imotov 
</comment><comment author="imotov" created="2016-02-02T22:03:35Z" id="178850069">@s1monw I still need to come up with a better task selection/matching approach in TransportTasksAction, but I would really appreciate it if you could take a look at the refactoring that I have done so far.
</comment><comment author="s1monw" created="2016-02-03T08:45:13Z" id="179095287">I think the changes so far look great @imotov I left some minor comments
</comment><comment author="s1monw" created="2016-02-05T16:34:24Z" id="180430844">left some minors LGTM otherwise
</comment><comment author="imotov" created="2016-02-08T19:23:20Z" id="181529580">@s1monw I pushed a new version. I have removed doCancel, but I cannot make it final because I need to make it extensible for other services to add task features. For example reindex API will need an ability to add throttling on the top of cancelling. I have also fixed a race condition in TransportCancelTasksAction, that I found while running additional tests. 
</comment><comment author="s1monw" created="2016-02-09T15:35:54Z" id="181916941">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ShardId equality and hash code inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16319</link><project id="" key="" /><description>This commit fixes an inconsistency between ShardId#equals(Object),
ShardId#hashCode, and ShardId#compareTo(ShardId). In particular,
ShardId#equals(Object) compared only the numerical shard ID and the
index name, but did not compare the index UUID; a similar situation
applies to ShardId#compareTo(ShardId). However, ShardId#hashCode
incorporated the indexUUID into its calculation. This can lead to
situations where two ShardIds compare as equal yet have different hash
codes.

Relates #16217 
</description><key id="129861804">16319</key><summary>ShardId equality and hash code inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T19:35:59Z</created><updated>2016-02-02T14:31:55Z</updated><resolved>2016-02-01T16:58:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-30T19:42:07Z" id="177282230">Lgtm , thanks @jasontedor . Let's wait till Monday so CI will give us a quite weekend ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryProfilerIT.testNoProfile failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16318</link><project id="" key="" /><description>Failed once:
http://build-us-00.elastic.co/job/es_core_master_strong/6362/testReport/junit/org.elasticsearch.search.profile/QueryProfilerIT/testNoProfile/
http://build-us-00.elastic.co/job/es_core_master_strong/6362/testReport/junit/org.elasticsearch.search.profile/QueryProfilerIT/testNoProfile/

I was unable to reproduce it with:

```
gradle :core:integTest -Dtests.seed=FB14D25BF47858F1 -Dtests.class=org.elasticsearch.search.profile.QueryProfilerIT -Dtests.method="testNoProfile" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:-UseCompressedOops" -Dtests.locale=ar-LB -Dtests.timezone=America/Argentina/ComodRivadavia
```

Error message:

```
Error Message

 Expected: an empty iterable      but: [&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [17s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s0][local[419]][indices:data/write/index]]; nested: RemoteTransportException[[node_s0][local[412]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [30s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s2][local[418]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index]]; nested: RemoteTransportException[[node_s0][local[412]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [30s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s2][local[418]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [29s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [29s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [17s]];&gt;]
Stacktrace

java.lang.AssertionError: 
Expected: an empty iterable
     but: [&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [17s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s0][local[419]][indices:data/write/index]]; nested: RemoteTransportException[[node_s0][local[412]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [30s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s2][local[418]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index]]; nested: RemoteTransportException[[node_s0][local[412]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [30s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s2][local[418]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [29s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index]]; nested: RemoteTransportException[[node_s2][local[414]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [29s]];&gt;,&lt;RemoteTransportException[[transport_client_node_s1][local[417]][indices:data/write/index]]; nested: RemoteTransportException[[node_s1][local[413]][indices:data/write/index[p]]]; nested: NotSerializableExceptionWrapper[Failed to acknowledge mapping update within [17s]];&gt;]
    at __randomizedtesting.SeedInfo.seed([FB14D25BF47858F1:F64D2836A28F927]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1433)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1353)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1337)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1313)
    at org.elasticsearch.search.profile.QueryProfilerIT.testNoProfile(QueryProfilerIT.java:602)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="129810175">16318</key><summary>QueryProfilerIT.testNoProfile failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>jenkins</label><label>test</label></labels><created>2016-01-29T16:07:10Z</created><updated>2016-09-14T16:55:32Z</updated><resolved>2016-09-14T16:55:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-31T19:10:28Z" id="177587843">I looked at this and the failure is caused by two nodes taking ~40s to process mapping updates from the master. They do not hang but rather just take to long to complete. The only thing that sticks out is that the cluster state update contains mapping for two types at once. I tried to simulated this using a count down latch in the MetaDataMappingService, causing batching of types but to no avail, it doesn't reproduce. I suggest we wait and see if this failures comes back and try to get more information. 
</comment><comment author="polyfractal" created="2016-02-02T15:22:02Z" id="178630237">Sorry for the delay, been traveling + internet issues.  Thanks for looking into this @bleskes, I'll defer to your judgement and just sit on this for a while :)
</comment><comment author="javanna" created="2016-09-14T15:54:48Z" id="247060849">this failed once last January and never again. Shall we close while we see if it happens again? Also from the explanation above it seemed not directly related to profiling so maybe similar things have happened in other tests too... not sure. WDYT @bleskes ?
</comment><comment author="bleskes" created="2016-09-14T16:52:36Z" id="247079261">I&#8217;m good with closing.

&gt; On 14 Sep 2016, at 17:55, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; this failed once last January and never again. Shall we close while we see if it happens again? Also from the explanation above it seemed not directly related to profiling so maybe similar things have happened in other tests too... not sure. WDYT @bleskes ?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local Discovery - don't create a local DiscoNode, but use the one from cluster service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16317</link><project id="" key="" /><description>Long ago (#7834) the ownership of the local disco node was centralized to the cluster service. LocalDiscovery is still created it's own disco node, which is not used by the cluster service and thus creating confusion (two nodes same name but different ids).

This commit also removes and optimization where when joining a new master we would first copy the master's metadata and only then pull in the rest of the cluster state (and it's nodes).

Note: LocalDiscovery is only used for testing
</description><key id="129797572">16317</key><summary>Local Discovery - don't create a local DiscoNode, but use the one from cluster service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T15:29:13Z</created><updated>2016-02-01T10:09:19Z</updated><resolved>2016-02-01T10:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-29T15:29:30Z" id="176815016">@jasontedor care to have a look?
</comment><comment author="jasontedor" created="2016-01-29T18:17:27Z" id="176894258">One correction, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure all resources are closed on Node#close()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16316</link><project id="" key="" /><description>We are leaking all kinds of resources if something during Node#close() barfs.
This commit cuts over to a list of closeables to release resources that
also closed remaining services if one or more services fail to close.

Closes #13685
</description><key id="129793780">16316</key><summary>Ensure all resources are closed on Node#close()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T15:19:29Z</created><updated>2016-02-01T10:28:19Z</updated><resolved>2016-02-01T09:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-29T15:20:15Z" id="176806161">@rmuir can you take a look at this?
</comment><comment author="rmuir" created="2016-01-29T15:29:56Z" id="176815766">+1, thanks for cleaning this up
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use index name rather than the index object to check against existence in the set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16315</link><project id="" key="" /><description>This is an issue due to some refactoring we did along the lines of Index.java etc.
We pass the index object to Set#contains which should actually be only it's name.

Closes #16299
</description><key id="129776431">16315</key><summary>Use index name rather than the index object to check against existence in the set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T14:11:21Z</created><updated>2016-01-29T14:19:45Z</updated><resolved>2016-01-29T14:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-29T14:12:27Z" id="176774780">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split templates from scripts?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16314</link><project id="" key="" /><description>Templates exist to resolve placeholders in pieces of text and can only be used in indexed scripts and ingest. While scripts are used in many other places and exist to flexible execute custom logic related to the context it is running in.

Templates are integrated into the script infrastructure (mustache implements `ScriptEngineService` interface, `Template` extends from `Script` and all places that use templates are hardcoded to use mustache language). I think it makes sense to strip templates from the script infrastructure and let it be its own thing. For example `Template` should be not extend from Script, template implementations would implement `TemplateEngineService` and all of the template features would be accessible from a `TemplateService`.

The following is a list of tasks to work towards a separated `TemplateService`:
* [x] Add `ScriptService#compileTemplate` and cut some script compilations to that method (#24280)
* [x] Cut all script compilation to `ScriptService#compileTemplate` (#24280)
* [ ] Create a `Template` class similar to the `Script` class that we can use to parse templates and use that with `ScriptService#compileTemplate`
* [ ] Make a replacement for `ScriptEngineService` for templates, us it in `ScriptService#compileTemplate`, and make mustache implement that instead.
* [ ] Remove template specific stuff from `Script` and `ScriptEngineService` (I'm starting to get fuzzy on exactly what this is so we'll have to clarify later)
</description><key id="129771482">16314</key><summary>Split templates from scripts?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Indexed Scripts/Templates</label><label>Meta</label></labels><created>2016-01-29T13:53:30Z</created><updated>2017-05-11T12:53:40Z</updated><resolved>2017-05-11T12:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T13:31:52Z" id="190725424">+1
</comment><comment author="martijnvg" created="2016-04-11T13:59:20Z" id="208357048">Maybe all template related features/infrastructure should move to the `lang-mustache` module? The main reason being here that templating is hard code to use mustache anyway.
- The transport, rest actions, `template` query builder should move to this module.
- We should extract the templating related code from the search api and move it to a dedicated api. This api's goal is to template any search request that it receives and then delegate it to the search api.
</comment><comment author="jdconrad" created="2017-02-08T16:07:56Z" id="278372243">Another +1 from me for separating templates completely from scripts using a TemplateEngineService because while some of the high level code would be duplicated, the details are quite different from parsing to execution.</comment><comment author="nik9000" created="2017-02-28T15:08:19Z" id="283064541">I talked to @jdconrad about this a while back. This is in the way or his work on "script contexts" so I'm going to work on this. "Script contexts" don't have an issues (I thought they did!) but the short description is that it'll allow scripts to be compiled for the context in which they should be run. There are lots of cool things this'll allow, one of which is that it'll let us avoid a box/unbox step in search scripts which should help to close the [gap](https://elasticsearch-benchmarks.elastic.co/geonames/index.html#search_latency_queries) between Painless and expressions.</comment><comment author="nik9000" created="2017-03-03T14:18:15Z" id="283964406">I'm about half way through a large change that should separate the two somewhat. The goal is to provide a framework for further separation rather than accomplish all the separation that we want in one go.</comment><comment author="nik9000" created="2017-04-20T18:36:20Z" id="295849862">I took a stab at this and even with my best efforts I really wasn't able to get it small enough. So I did some brainstorming with @rjernst and @jdconrad and we decided it'd be best to have a running list of things we're going to to do complete the split. This is the list, I expect we'll add to it as we come up with more things.

Edit: Moved list to the issue description.</comment><comment author="rjernst" created="2017-04-20T18:39:21Z" id="295850942">@nik9000 I moved your list to the issue description and marked this as a meta issue. Thanks!</comment><comment author="jdconrad" created="2017-05-10T18:29:03Z" id="300572525">@martijnvg I think we can close this issue based on our discussions yesterday.  With custom contexts we will just make a template context that should be able to execute against any language as long as the language supports the context.</comment><comment author="martijnvg" created="2017-05-11T12:53:40Z" id="300779839">@jdconrad Agreed, with contexts we will be able to isolate template specifics better.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Monitor settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16313</link><project id="" key="" /><description>This pull requests converts the monitor settings to the new settings
infrastructure.
</description><key id="129762954">16313</key><summary>Monitor settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T13:14:30Z</created><updated>2016-02-02T13:50:02Z</updated><resolved>2016-01-29T15:21:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-29T13:23:15Z" id="176751900">LGTM
</comment><comment author="jasontedor" created="2016-01-29T15:05:34Z" id="176800978">@s1monw I pushed a1fd01d4ccb91a3a10d7d19446da13010413cb86 removing some leniency from the settings and adding some tests for the handling of the settings.
</comment><comment author="s1monw" created="2016-01-29T15:21:15Z" id="176806904">LGTM still thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow direct reading and writing of NamedWritables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16312</link><project id="" key="" /><description>When the serialization infrastructure for NamedWritable objects was introduced, the generic read/writeNamedWritable methods in StreamInput and StreamOutput had limited access. Instead of using it directly we exposed public readFoo/writeFoo methods for each NamedWritable category (which was only QueryBuilders then).

Now with the ongoing search refactoring, we got a lot more categories of NamesWritable objects (shapes, rescore, function scores, soon also suggesters, aggs, sorts), which all require additional read/write methods in StreamInput/StreamOutput, thereby bloating it.

This PR allows direct access to read/writeNamedWritable in the streaming classes, thereby reducing the number of specialized methods we need (and will need in the future)
</description><key id="129755972">16312</key><summary>Allow direct reading and writing of NamedWritables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2016-01-29T12:47:54Z</created><updated>2016-02-22T21:51:07Z</updated><resolved>2016-01-29T13:17:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-29T12:49:11Z" id="176735436">Opening this mainly for discussion, maybe @javanna or @colings86 have an opinion on this?
</comment><comment author="colings86" created="2016-01-29T12:57:03Z" id="176741419">I'm -1 on exposing the read/writeNamedWriteable methods directly as it makes the stream more difficult to use. I don't personally have a problem with having a method for each type of object we are reading and writing to/from the stream since these methods male it easy to see what can be serialised and we decrease the likelyhood of us ending up with multiple subtly different ways of serialising the same objects. Having a big class with lots of methods isn't always a bad thing as long as the methods make sense in the context of. The class which I think these do here. 
</comment><comment author="javanna" created="2016-01-29T13:03:14Z" id="176744200">I see the problem, but I tend to agree with @colings86 and I am not sure it is really a problem. The main idea behind this decision (I was in favour of having a single public method in the very beginning but I later changed my mind) was also that we don't want to expose a public method that allows to serialize any arbitrary object through StreamInput and StreamOutput, but we want to keep this under control, at the cost of having one method for each supported type. Maybe @s1monw has something to say about this too ;)
</comment><comment author="cbuescher" created="2016-01-29T13:17:16Z" id="176749655">Thanks for the quick feedback, greatly appreciated. I only wanted to bring this up quickly, since it crossed my mind several times but understand the arguments against it. Will close this again then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut over tribe node settings to new settings infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16311</link><project id="" key="" /><description /><key id="129748371">16311</key><summary>Cut over tribe node settings to new settings infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T12:04:26Z</created><updated>2016-01-29T13:23:50Z</updated><resolved>2016-01-29T13:23:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-29T13:16:09Z" id="176749206">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate and remove the multicast plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16310</link><project id="" key="" /><description>Multicast has some serious issues (eg see https://github.com/elastic/elasticsearch/issues/12993 and https://github.com/elastic/elasticsearch/issues/12914) which are JDK and OS dependent, and essentially unfixable.  In https://github.com/elastic/elasticsearch/pull/12999 we changed to default to using unicast for discovery and in https://github.com/elastic/elasticsearch/issues/13019 we moved multicast to a plugin to give users a chance to migrate their code.

We are not seeing many downloads of the multicast plugin which indicates that almost everybody has switched to unicast.  I think we should deprecate this plugin in 2.2 and remove it in 3.0.
</description><key id="129743408">16310</key><summary>Deprecate and remove the multicast plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Discovery Multicast</label><label>adoptme</label><label>deprecation</label></labels><created>2016-01-29T11:48:19Z</created><updated>2016-11-01T19:07:58Z</updated><resolved>2016-02-01T15:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-29T17:01:48Z" id="176861735">+1
</comment><comment author="djschny" created="2016-03-08T14:14:04Z" id="193798722">Since the 2.0 adoption rate is low still, I don't believe a small of amount of downloads of the multicast plugin can be used as good indication that "almost everybody has switched to unicast". I would suggest proceeding very cautiously here.
</comment><comment author="dadoonet" created="2016-03-08T18:15:53Z" id="193897943">For the record, I met someone today at BigDataParis who is deploying elasticsearch in Google cloud, within a private network, and they are using multicast and are happy with it (ES &lt; 2.0).
I told about the multicast plugin and that it will be removed in 5.0. Does not seem to be a big deal. Just complicates a bit the deployment.
</comment><comment author="macdjord" created="2016-11-01T18:43:32Z" id="257656576">Uh, this is a problem for us. We use Dockerized elasticsearch, and the way our deployments are set up makes giving our nodes fixed IP addresses non-trivial. We depend on multicast discovery to allow our nodes to connect under those conditions.

We will probably not be upgrading to 5.0 until this is fixed.

(We were aware of the deprecation, but were under the impression that it meant the multicast-discovery _core module_ was being deprecated in favour of the multicast-discovery _plugin_ - not that muticast support was being dropped entirely.)
</comment><comment author="nik9000" created="2016-11-01T18:44:55Z" id="257656946">&gt; We will probably not be upgrading to 5.0 until this is fixed.

I don't know anyone who plans on bringing it back.
</comment><comment author="jasontedor" created="2016-11-01T19:07:58Z" id="257663166">&gt; I don't know anyone who plans on bringing it back.

It's not coming back, but there are solutions to this problem. I suggest opening a topic on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add SmoothingModel and candidate generator serialization to PhraseSuggestionBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16309</link><project id="" key="" /><description>With SmoothingModels and CandidateGenerator now beeing able to be serialized via NamedWritable infrastructure, PhraseSuggestionBuilder can now properly delegate read/write and equals/hashCode to their implementations. Also adding randomization of smoothing model and candidate generators to tests.
</description><key id="129743342">16309</key><summary>Add SmoothingModel and candidate generator serialization to PhraseSuggestionBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T11:48:10Z</created><updated>2016-03-10T18:57:25Z</updated><resolved>2016-02-01T16:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-02-01T15:04:29Z" id="178006030">Left one comment but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent interruption while store checks lucene files for consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16308</link><project id="" key="" /><description>This can cause premature closing of the underlying file descriptor immediately
if at the same time the thread is blocked on IO. The file descriptor will remain closed
and subsequent access to NIOFSDirectory will throw a ClosedChannelException.
</description><key id="129737001">16308</key><summary>Prevent interruption while store checks lucene files for consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T11:21:04Z</created><updated>2016-01-29T12:04:55Z</updated><resolved>2016-01-29T12:04:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-29T11:41:15Z" id="176707195">Lgtm. good catch 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate the rest of NettyTransport settings to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16307</link><project id="" key="" /><description>Also does some consistency clean up, renaming trasnport.netty.\* settings to transport.*
</description><key id="129731661">16307</key><summary>Migrate the rest of NettyTransport settings to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T10:53:12Z</created><updated>2016-02-01T09:51:20Z</updated><resolved>2016-02-01T09:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-29T15:22:53Z" id="176808516">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16306</link><project id="" key="" /><description>i try to upgrade from a centos 
1.5 to 2.1
i can't start elasticsearch 
CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
</description><key id="129728268">16306</key><summary>CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luluprat</reporter><labels /><created>2016-01-29T10:37:55Z</created><updated>2016-03-25T14:44:28Z</updated><resolved>2016-01-29T10:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-29T10:43:24Z" id="176688134">Yes it's part of the breaking changes. See https://github.com/elastic/elasticsearch/pull/13772
</comment><comment author="luluprat" created="2016-01-29T13:51:18Z" id="176762638">but wath i have to do so to get elasticsearch work ?
</comment><comment author="dadoonet" created="2016-01-29T13:57:37Z" id="176765230">If you need help, you can describe all the steps you made to upgrade to 2.1 on discuss.elastic.co

We can help you there.

Check that you updated all scripts (like bin/elasticsearch...) as well.
</comment><comment author="ekovacs" created="2016-03-22T13:21:59Z" id="199811284">same happend to me. 

Problem:
i did not want to do anything else but install the 2.2.1 version through yum. no other magic, only a simple `yum install ./elasticsearch-2.2.1.rpm`. then starting it gave me the blues `CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.`

The solution: 
`/etc/sysconfig/elasticsearch` had the `CONF_FILE=/etc/elasticsearch/elasticsearch.yml` line it it, that the rpm install process didn't update. after a couple hours of searching where the hell did i define this environment variable i tripped upon that location. after commenting out that line, the elasticsearch service started. 

@dadoonet Please update the rpm/deb install process so it patches that file so that the service could start without a problem after the upgrade. if not, then please update your documentation to clearly state this possible problem and this solution.
</comment><comment author="dadoonet" created="2016-03-22T13:23:58Z" id="199812705">Thanks @ekovacs. @spinscale WDYT? Should we open an issue?
</comment><comment author="clintongormley" created="2016-03-25T14:44:28Z" id="201318980">@ekovacs Presumably your `/etc/sysconfig/elasticsearch` file was not replaced because it had been edited locally.  You should have had a `.rpm` version (or similar) alongside it.  This is how RPM works.  We can't override your local edits.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a hard check to ensure we are running with the expected lucene version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16305</link><project id="" key="" /><description>Closes #16301
</description><key id="129727828">16305</key><summary>Add a hard check to ensure we are running with the expected lucene version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T10:36:36Z</created><updated>2016-01-29T11:39:51Z</updated><resolved>2016-01-29T11:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-29T10:37:28Z" id="176686697">LGTM
</comment><comment author="s1monw" created="2016-01-29T11:00:26Z" id="176693854">@jasontedor pushed a new commit 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add expectThrows utility to base test case</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16304</link><project id="" key="" /><description>In junit5, a neat assertion method is added which makes testing expected
failures a little more straightforward. The block of code that is
expected to throw is passed in with a lambda expression, and the caught
exception returned for inspection.

This change adds an implementation of expectThrows to ESTestCase. When
junit5 is eventually releassed and we switch to it, we can remove.
</description><key id="129718946">16304</key><summary>Tests: Add expectThrows utility to base test case</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-29T09:55:38Z</created><updated>2016-01-29T17:01:21Z</updated><resolved>2016-01-29T17:01:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-29T14:02:24Z" id="176767111">LGTM. I was thinking of adding one of these but didn't want to diverge from junit 5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move mapper attachments plugin to ingest-tika plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16303</link><project id="" key="" /><description>With Ingest Node coming, it now makes sense to make text extraction (and may be OCR extraction as well) in Ingest nodes and not anymore as a mapper plugin.

Proposed names for the plugin: 
- `ingest-tika` plugin
- `ingest-attachments` plugin
- `ingest-extract` plugin
</description><key id="129706656">16303</key><summary>Move mapper attachments plugin to ingest-tika plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>:Plugin Mapper Attachment</label><label>breaking</label></labels><created>2016-01-29T08:57:17Z</created><updated>2016-02-09T16:16:50Z</updated><resolved>2016-02-09T16:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Some questions about Tribe node log </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16302</link><project id="" key="" /><description>Two clusters are to be installed  Marvel plugin, Tribe node also need to install the license plugin, Otherwise, could not connect the two cluster&#65292;There have been some quite understand the message log&#65292;But Tribe node  can provide the service.

```

[2016-01-29 11:52:35,167][DEBUG][action.admin.indices.create] [elk-etribe01-105] no known master node, scheduling a retry
[2016-01-29 11:52:35,196][DEBUG][action.admin.indices.create] [elk-etribe01-105] no known master node, scheduling a retry

[2016-01-29 11:56:48,030][WARN ][cluster.service          ] [elk-etribe01-105/e100] failed to notify ClusterStateListener
java.lang.ClassCastException: org.elasticsearch.license.plugin.core.LicensesMetaData cannot be cast to org.elasticsearch.license.plugin.core.LicensesMetaData
    at org.elasticsearch.license.plugin.core.LicensesService.clusterChanged(LicensesService.java:462)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
...


```
</description><key id="129677921">16302</key><summary>Some questions about Tribe node log </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zcola</reporter><labels /><created>2016-01-29T06:15:17Z</created><updated>2016-01-29T09:52:28Z</updated><resolved>2016-01-29T09:52:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T09:52:27Z" id="176669909">Hi @zcola 

This has been improved in 2.2.0 - the exception now reads:

```
IllegalArgumentException[No custom metadata prototype registered for type [licenses], node like missing plugins]; ]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene version check prevents people updating Lucene to get bugfixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16301</link><project id="" key="" /><description>Lucene 5.3.2 came out, so we updated it to get the fix.

Elasticsearch then stopped working, because there is this check in `Version.java`:

```
        assert CURRENT.luceneVersion.equals(Lucene.VERSION) : "Version must be upgraded to [" + Lucene.VERSION + "] is still set to [" + CURRENT.luceneVersion + "]";
```

This basically prevents us updating Lucene to get any bugfixes at all unless we turn off assertions to work around it, which is probably the better option, but still irritates me somehow.

If Elasticsearch does depend on 5.3.1 specifically then it should probably be using Version.LUCENE_5_3_1 and not checking Version.LATEST and then complaining if it happens to be newer.
</description><key id="129617007">16301</key><summary>Lucene version check prevents people updating Lucene to get bugfixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">trejkaz</reporter><labels><label>:Core</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-28T23:30:43Z</created><updated>2016-01-29T13:45:04Z</updated><resolved>2016-01-29T11:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-29T10:10:11Z" id="176679191">&gt; If Elasticsearch does depend on 5.3.1 specifically then it should probably be using Version.LUCENE_5_3_1 and not checking Version.LATEST and then complaining if it happens to be newer.

This is exactly how it's set up. The goal of this assertion to make sure we run on the lucene version we expect to be there, so we will not forget to update the pinned lucene version.

I'm not sure what exactly you are trying to do. It seems you are replacing the lucene jars in the elasticsearch lib folder? If so this is not supported as many times it requires code changes as well. I'm going to close this assuming, I understood it correctly. If not feel free to reopen.
</comment><comment author="s1monw" created="2016-01-29T10:15:16Z" id="176680180">&gt; I'm not sure what exactly you are trying to do. It seems you are replacing the lucene jars in the elasticsearch lib folder? If so this is not supported as many times it requires code changes as well. I'm going to close this assuming, I understood it correctly. If not feel free to reopen.

I think we should make this even a hard check instead of an assertion maybe in Bootstrap.java?
</comment><comment author="bleskes" created="2016-01-29T10:23:02Z" id="176682572">+1 

&gt; On 29 Jan 2016, at 11:15, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; I'm not sure what exactly you are trying to do. It seems you are replacing the lucene jars in the elasticsearch lib folder? If so this is not supported as many times it requires code changes as well. I'm going to close this assuming, I understood it correctly. If not feel free to reopen.
&gt; 
&gt; I think we should make this even a hard check instead of an assertion maybe in Bootstrap.java?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2016-01-29T10:37:21Z" id="176686640">there is a PR for this here #16305 
</comment><comment author="trejkaz" created="2016-01-29T13:45:04Z" id="176760958">We use Lucene in the same application and we are now trying to add in Elasticsearch.

If Elasticsearch wants to use a specific version, Lucene's Version class does allow it to do that. Checking that LATEST is equal to a specific version just hinders people updating Lucene. I do understand that some updates to Lucene require code changes, but this was a bugfix release and we wanted the fix. You are now depriving us from getting that fix.

I wouldn't mind so much if Elasticsearch were to release a corresponding bugfix update which uses the same updated Lucene, but there was no such update.

If your goal is to inconvenience people who want to use your library, you might also consider aiming for the maximum score and locking down the version of Java you run against, so that people are forced to run an out of date JRE as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Name eclipse projects to gradle path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16300</link><project id="" key="" /><description>This groups like projects together which is nice. It creates two weirdly
named projects:
1. buildSrc - its still just called buildSrc and it doesn't match. I don't
   see why we import it into Eclipse anyway. Its groovy and easier to just edit
   in vim or whatever.
2. elasticsearch - this is the name of the root project. It's also not
   particularly useful to import into eclipse but we've always named it this way
   and the name ':' was even more confusing so we just kept the name.
</description><key id="129606421">16300</key><summary>Name eclipse projects to gradle path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T22:43:22Z</created><updated>2016-02-11T17:53:11Z</updated><resolved>2016-02-11T17:53:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-28T22:43:47Z" id="176461552">For posterity: I've edited the description of the PR above. I used to name the root project ':' which is the path for the root project but was funky. I special cased it like @rjernst asked me to do in the below comment.

Original comment at this position:
Opinions?
</comment><comment author="rjernst" created="2016-01-29T02:35:59Z" id="176532746">It seems a little odd to have the root project as just colon, but you could special case it? And I don't think buildSrc is bad if it is on its own, it is really a special separate project in the same repo. But I don't care about any of this, since I don't use eclipse. :)
</comment><comment author="ywelsch" created="2016-01-29T09:37:59Z" id="176664907">@nik9000 isn't what you want a nested/hierarchical view of projects? If you use Eclipse Mars, this can be easily set up in the IDE (see Nested/Hierarchical view of projects on https://eclipse.org/mars/noteworthy/
or a presentation about it: http://ppalaga.github.io/presentations/150207-devconf-brno/nested-projects-in-eclipse.html#_about_me ). With such a feature, I would prefer we keep project names as is.
</comment><comment author="nik9000" created="2016-01-29T14:30:33Z" id="176781926">&gt; If you use Eclipse Mars, this can be easily set up in the IDE

Hmmmm. I tried that feature this morning and it seems a bit buggy to me. Like when I delete the root project all the other projects disappear but them are still there when I switch back to the flat view. With the amount of times that I nuke and recreate my projects that isn't going to work....

Personally I like the flat view better anyway. I guess I'm just more used to it though.
</comment><comment author="nik9000" created="2016-02-10T14:30:43Z" id="182398012">@ywelsch I tried the hierarchical view and it doesn't seem ready - I commented above about issues I'd had with it. I think this still might be useful.
</comment><comment author="ywelsch" created="2016-02-10T14:51:22Z" id="182405366">@nik9000 Don't see me as a blocker on this, I'm using IntelliJ anyway :-)
</comment><comment author="nik9000" created="2016-02-10T15:19:49Z" id="182421599">&gt; @nik9000 Don't see me as a blocker on this, I'm using IntelliJ anyway :-)

Sweet! Ok then. I'm going to give any other Eclipse users 24 hours. If no one objects I'll merge around this time tomorrow. Any opinion on what I should special case the root project to? `root`? In my experience it actually isn't worth importing the root project into eclipse anyway, much less sorting it to the top of the project list.
</comment><comment author="rjernst" created="2016-02-10T16:45:03Z" id="182472916">&gt; Any opinion on what I should special case the root project to? root?

I would keep it `elasticsearch`? This would match what we have in intellij.
</comment><comment author="nik9000" created="2016-02-10T17:08:45Z" id="182485024">&gt; I would keep it elasticsearch? This would match what we have in intellij.

Will do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TribeIT.testOnConflictDrop still failing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16299</link><project id="" key="" /><description>Failed a few times yesterday and today (after Simon's fix):
http://build-us-00.elastic.co/job/es_core_master_regression/4416/testReport/junit/org.elasticsearch.tribe/TribeIT/testOnConflictDrop/

I was unable to reproduce it with: 

```
gradle :core:integTest -Dtests.seed=2B576DFA8D61DCF9 -Dtests.class=org.elasticsearch.tribe.TribeIT -Dtests.method="testOnConflictDrop" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=de-GR -Dtests.timezone=Africa/Lubumbashi
```

The error message is:

```
Error Message

 Expected: &lt;false&gt;      but: was &lt;true&gt;
Stacktrace

java.lang.AssertionError: 
Expected: &lt;false&gt;
     but: was &lt;true&gt;
    at __randomizedtesting.SeedInfo.seed([2B576DFA8D61DCF9:15024283C5329C8C]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.tribe.TribeIT.testOnConflictDrop(TribeIT.java:248)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="129596713">16299</key><summary>TribeIT.testOnConflictDrop still failing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Tribe Node</label><label>test</label></labels><created>2016-01-28T22:07:20Z</created><updated>2016-01-29T14:19:43Z</updated><resolved>2016-01-29T14:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-29T12:02:30Z" id="176721463">@javanna I think this started failing after you added the path.conf stuff to tribe?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>omit_norms: true PUT _mapping exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16298</link><project id="" key="" /><description>When using the PUT _mapping api in 1.x builds (tested with 1.5.2, 1.6.0, 1.6.1, 1.6.2, 1.7.4.  Does not repo with 2.1.1) and

`{
"_default_": {
  "_all": {
     "enabled": true,
     "omit_norms": true
  }
 }
}`

is set (logstash template: https://github.com/logstash-plugins/logstash-output-elasticsearch/commit/e4590a2315b23fb9e94638534b62e49a779532d1); adding a mapping for a new field of an existing type will throw the following error:

`{
   "error": "MergeMappingException[Merge failed with failures {[mapper [_all] cannot enable norms (`norms.enabled`)]}]",
   "status": 400
}`

Steps to reproduce:

```
PUT logstash-example

PUT logstash-example/_mapping/_default_
{
"_default_": {
  "_all": {
     "enabled": true,
     "omit_norms": true
  }
 }
}

PUT logstash-example/_mapping/syslog
{
   "syslog": {
  }
}

PUT logstash-example/_mapping/syslog
{
  "properties": {
    "tags": {
      "type": "string"
    }
  }
}
```
</description><key id="129590056">16298</key><summary>omit_norms: true PUT _mapping exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpcarey</reporter><labels /><created>2016-01-28T21:37:17Z</created><updated>2016-01-29T09:41:16Z</updated><resolved>2016-01-29T09:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T09:41:16Z" id="176665560">Hi @jpcarey 

Yes, this is a bug which has been fixed by the great mapping rewrite (#8870) in 2.x.  These changes are way too big to backport (and break bwc).  As a workaround, specify the `_all` mapping again:

```
PUT logstash-example/_mapping/syslog
{
  "_all": {
    "enabled": true,
    "omit_norms": true
  },
  "properties": {
    "tags": {
      "type": "string"
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexStatsIT.testSegmentsStats fails intermittently </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16297</link><project id="" key="" /><description>The failed test on Jenkins:
http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2403/testReport/junit/org.elasticsearch.indices.stats/IndexStatsIT/testSegmentsStats/

It has failed 3 times in January on master.  This time on Windows and the other two times on Centos and Metal:
http://build-us-00.elastic.co/job/es_core_21_metal/327/testReport/org.elasticsearch.indices.stats/IndexStatsIT/testSegmentsStats/
http://build-us-00.elastic.co/job/es_core_21_centos/944/testReport/junit/org.elasticsearch.indices.stats/IndexStatsIT/testSegmentsStats/

I was not able to reproduce it with:

```
gradle :core:integTest -Dtests.seed=B194B6C9CB02B4D8 -Dtests.class=org.elasticsearch.indices.stats.IndexStatsIT -Dtests.method="testSegmentsStats" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=816m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:-UseCompressedOops" -Dtests.locale=lv -Dtests.timezone=Poland
```

The error message is:

```
Error Message

 Expected: a value greater than &lt;0L&gt;      but: &lt;0L&gt; was equal to &lt;0L&gt;
Stacktrace

java.lang.AssertionError: 
Expected: a value greater than &lt;0L&gt;
     but: &lt;0L&gt; was equal to &lt;0L&gt;
    at __randomizedtesting.SeedInfo.seed([B194B6C9CB02B4D8:6BF089F569E838C]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.indices.stats.IndexStatsIT.testSegmentsStats(IndexStatsIT.java:534)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="129552762">16297</key><summary>IndexStatsIT.testSegmentsStats fails intermittently </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">abeyad</reporter><labels><label>:Stats</label><label>test</label></labels><created>2016-01-28T19:27:36Z</created><updated>2016-05-21T11:57:21Z</updated><resolved>2016-05-21T11:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-05-21T11:57:21Z" id="220773844">closing - I think this was fixed a while ago
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index if not deleted / non-partial updates / index if doc exists / consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16296</link><project id="" key="" /><description>Hi,

a) for the sake of consistency, it would be super awesome if ES could permanently store a list of IDs of deleted docs, such that one could ensure to not re-index already deleted docs. E.g. PUT /.../_create?if_not_deleted=true. It seems the only current possible way to achieve consistency regarding deleted records is to not really delete these docs but instead mark them as deleted and exclude them from search queries,

b) Similarly, i don't find a way to do non-partial updates, i.e. use the update api without merging the already existing attributes into the provided ones. E.g.

```
PUT /my-index/my-type/1/_update
{
  "full_doc": {
    ...
  }
}
```

I could of course use the index api, but for the sake of consistency i want to be sure to not re-index already deleted documents. Something like

```
PUT /my-index/my-type/1?if_not_exists=true
{
  ...
}
```

Thanks
</description><key id="129526770">16296</key><summary>index if not deleted / non-partial updates / index if doc exists / consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrkamel</reporter><labels /><created>2016-01-28T17:58:48Z</created><updated>2016-01-29T09:26:12Z</updated><resolved>2016-01-29T09:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrkamel" created="2016-01-28T18:06:02Z" id="176309595">the last one should of course be

```
PUT /my-index/my-type/1?if_exists=true
{
  ...
}
```
</comment><comment author="clintongormley" created="2016-01-29T09:26:12Z" id="176660388">Hi @mrkamel 

If you want this behaviour, then you need to do exactly what you're doing: use "soft" deletes by adding a flag to the document instead of deleting it.  The semantics we have are way too baked in to support this change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert multcast plugin settings to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16295</link><project id="" key="" /><description /><key id="129486659">16295</key><summary>Convert multcast plugin settings to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T15:38:54Z</created><updated>2016-01-29T09:20:30Z</updated><resolved>2016-01-28T16:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-28T15:53:16Z" id="176245122">I left a very valuable comment :) LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.7.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16294</link><project id="" key="" /><description>Jackson 2.7.0 has been released.

Release notes: https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.7
</description><key id="129483320">16294</key><summary>Upgrade to Jackson 2.7.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Core</label><label>adoptme</label><label>upgrade</label></labels><created>2016-01-28T15:24:49Z</created><updated>2016-02-26T21:18:56Z</updated><resolved>2016-02-26T21:18:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Migrate AWS settings to new settings infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16293</link><project id="" key="" /><description>For EC2 discovery and S3 repository
</description><key id="129474378">16293</key><summary>Migrate AWS settings to new settings infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>blocker</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T14:50:48Z</created><updated>2016-02-11T18:48:24Z</updated><resolved>2016-02-11T18:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-02-10T15:15:39Z" id="182419270">Reopening the issue as I reverted the PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert `request.headers.*` to the new settings infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16292</link><project id="" key="" /><description /><key id="129459481">16292</key><summary>Convert `request.headers.*` to the new settings infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T13:56:14Z</created><updated>2016-01-28T14:05:02Z</updated><resolved>2016-01-28T14:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-28T14:01:25Z" id="176197237">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate Azure settings to new settings infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16291</link><project id="" key="" /><description>With this commit we migrate all Azure related settings to
the new settings infrastructure.

During the implementation I have spotted a few things which I have improved along the way:
- Made some classes immutable
- Removed usage of outdated classes like `StringBuffer` and `Hashtable`
- Removed leniency in `AzureComputeServiceImpl` which allowed any garbled value as key store type and just assumed some default
- Fixed some tests that have apparently not been executed in CI

Can you have a look at it @s1monw?
</description><key id="129459462">16291</key><summary>Migrate Azure settings to new settings infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T13:56:05Z</created><updated>2016-02-04T15:04:46Z</updated><resolved>2016-02-01T15:35:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-28T14:02:07Z" id="176197356">this looks awesome!
LGTM thank you so much
</comment><comment author="danielmitterdorfer" created="2016-01-28T14:12:25Z" id="176201390">Thanks for the feedback. I'll look whether I can migrate `AzureStorageSettings` similarly to [`EsLoggerFactory`](https://github.com/elastic/elasticsearch/pull/16289/files#diff-7117bca712d8a68a5aa84783e75d902aR40) though.
</comment><comment author="danielmitterdorfer" created="2016-01-29T16:35:51Z" id="176852231">I've integrated the review comments by @dadoonet now. 

Apart from that, I found the implementation in `AzureStorageSettings#parse()` very hard to understand. Here's what the method basically does:

Suppose we have the following properties defined in the config file (please ignore that some properties are missing):

```
cloud.azure.storage.azure1.timeout=30s
cloud.azure.storage.azure1.account=demo
cloud.azure.storage.azure1.default=true

cloud.azure.storage.azure2.timeout=45s
cloud.azure.storage.azure2.account=demo2

cloud.azure.storage.azure3.timeout=45s
cloud.azure.storage.azure3.account=demo3
```

For each "group" ("azure1", "azure2", "azure3"), it will create an instance of `AzureStorageSettings`, verify a few constraints (e.g. there must be only one group with `default` set to `true`) and return the "primary" (which is the group with `default` set to `true`) and a list of secondaries (which are all other groups).

I have added an experimental reimplementation of this method based on the following approach:
- Partition the provided settings by group key (will create three groups "azure1", "azure2" and "azure3" considering the example above). The respective setting keys will be "normalized" by group. So instead of `cloud.azure.storage.azure3.timeout`, the setting key will be `cloud.azure.storage.timeout` internally. This allows us to use the settings infrastructure here without creating `Setting` instances for each group.
- Based on these settings, it creates preliminary `AzureStorageSettings`, one for each group
- It will validate global configuration rules not on raw config values but based on the `AzureStorageSettings` instances created before. It is also now more restrictive, in the sense that it will fail if multiple groups have `default` set to `true`.

For more details, please see also my commit comment in 7e4c6578462afe45855738448bea9eb4ed8e9ded. I have implemented the experimental parsing logic in the separate commit 7e4c6578462afe45855738448bea9eb4ed8e9ded, so it is easy to revert if the implementation does not make too much sense.

I'd like to get your feedback (especially from @dadoonet as you know a lot about this module and @s1monw as you've written the settings infrastructure). If this doesn't make sense for you then I'm happy to revert the commit and merge the PR without this change but I wanted to give it at least a try.

Advantages from my point of view:
- The code is easier to understand (at least from my point of view) although it is now more code
- It is stricter and rules are explicitly checked on domain objects instead of implicit checks during parsing
- We can leverage the settings infrastructure

Disadvantages:
- We introduce "synthetic" setting keys which could mislead users on configuration errors :(
- I'd prefer if we could integrate this better into the settings infrastructure but I didn't see how this could fit in, even though we have now `Setting#dynamicKeySetting()` as we'd need to be able to group multiple values by infix.

Open issues:
- Tests for the new code (I didn't want to burn too much time and just wanted to get early feedback from you before proceeding)
</comment><comment author="danielmitterdorfer" created="2016-02-01T15:37:00Z" id="178026611">I've merged the PR now without the refactored parse method (see my previous comment) and will do this in a separate PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner_hits returns "ip" type values in strange (byte?) format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16290</link><project id="" key="" /><description>I am running nested queries like this:

```
body={
    "query" : {
        "nested" : {
            "path" : "network.http.packet",
            "query" : {
                "match" : {"network.http.packet.dstip" : "184.28.188.99"}
            },
            "inner_hits" : {"_source":False,"fielddata_fields":["network.http.packet.dstip"]} 
        }
    }
}
```

The query returns the fielddata_fields in some strange (byte?) representation instead of ip, like this:
`{u'network.http.packet.dstip': [386205091, 386205091, 386205091, 386205091, 386206491, 386206491,....
` 

I have set include_in_root to true for network.http.packet, and if I query a document with 
`_source="network.http.packet.dstip"`
the IPs are returned as expected (Dot-decimal notation).

Looks like a bug to me.
</description><key id="129431871">16290</key><summary>inner_hits returns "ip" type values in strange (byte?) format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abulhol</reporter><labels /><created>2016-01-28T12:05:07Z</created><updated>2016-01-29T08:04:25Z</updated><resolved>2016-01-28T16:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-28T16:24:40Z" id="176261484">This is  caused by the use of `fielddata_fields`, not inner hits, which return the internal representation of the data in fielddata. And ip addresses happen to be backed by 32-bits numerics.
</comment><comment author="abulhol" created="2016-01-28T20:27:23Z" id="176396652">This is good to know - I suggest to also explain it in detail in the reference guide. 
The other question is: how can I return the dot-decimal notation for inner_hits instead?
I can see that I could use Source filtering probably like this:
`"inner_hits" : {"_source":["network.http.packet.dstip"]}`
and to NOT return the _source for the containing document, I have to set 
`"_source":["network.http.packet.*"]`
or so for the whole query. Will give that a try tomorrow. 
In any case, the reference guide could be improved here. 
</comment><comment author="abulhol" created="2016-01-29T08:04:25Z" id="176632760">No solution found so far, have posted this in the ES forum: 
https://discuss.elastic.co/t/return-ip-field-from-inner-hit-in-dot-decimal-format/40452
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate logger settings and allow them to be reset via API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16289</link><project id="" key="" /><description>Today our logger settings are treated differently than all other
settings. This was a shortcut / workaround since it's quite messy but now
that we have strict validation and transactional updates we should integrate the
logger settings there as well. This commit makes the logger settings a build-in
settings updater and removes all the special code-paths we had for these settings.
Logger settings now also updateable and resetttable by passing `null`
</description><key id="129422372">16289</key><summary>Validate logger settings and allow them to be reset via API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T11:21:34Z</created><updated>2016-01-28T16:00:08Z</updated><resolved>2016-01-28T16:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-28T15:53:47Z" id="176245285">LGTM thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the Network Addresses community plugin link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16288</link><project id="" key="" /><description>Updated analysis.asciidoc with a link to the Network Addresses Analysis Plugin.
</description><key id="129408621">16288</key><summary>Added the Network Addresses community plugin link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofir123</reporter><labels /><created>2016-01-28T10:23:07Z</created><updated>2016-01-28T11:23:03Z</updated><resolved>2016-01-28T11:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T11:23:03Z" id="176130935">thanks @ofir123 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use es2.1.1 and meet many thread waiting all the same (not blocked)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16287</link><project id="" key="" /><description>Actually I use es1.3 and then es 1.7 , but meet many threads waiting. So I read several issues here , like 
#10766 , #10940 , #11467 , #10704 , then I read this commit https://github.com/kimchy/elasticsearch/commit/fc483a7cbff03de07cf96b46ec00a8aa6ef16002 (Automatically thread client based action listeners), hope this will be fix in es 2.0 + .  But when I upgrade to es 2.1.1 , meet the same thread waiting :

```
"catalina-exec-35" daemon prio=10 tid=0x00007fdb08020800 nid=0x887a waiting on condition [0x00007fdabedeb000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  &lt;0x00000007f0ca71c8&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:276)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:116)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:42)

        ... here is my code to invoke search api of es

        at sun.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.springframework.web.servlet.mvc.multiaction.MultiActionController.invokeNamedMethod(MultiActionController.java:471)
```

The whole thread dump is here: 
https://gist.github.com/node/773dc44273d0c78680ed

So how can I fix this ? Is this only an issues about es transport client ?  
</description><key id="129408215">16287</key><summary>Use es2.1.1 and meet many thread waiting all the same (not blocked)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">node</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2016-01-28T10:20:34Z</created><updated>2016-01-29T12:30:50Z</updated><resolved>2016-01-29T09:54:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T11:21:53Z" id="176130430">Could you provide the code that you are using to run your search requests?
</comment><comment author="node" created="2016-01-29T06:46:54Z" id="176603780">@clintongormley  here is it:

```
SearchResponse response = esClient
                .prepareSearch(esManager.getIndexName())
                .setTypes(esManager.getIndexType())
                .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                .setQuery(mmQuery).setPostFilter(andSearchFilter)
                .setTrackScores(true)
                .setMinScore(MyConst.SEARCH_ES_MINSCORE)
                .addSort(SortBuilders.scoreSort())
                .addSort("official", SortOrder.DESC)
                .addSort("downloadCount", SortOrder.DESC)
                .setFrom(pageFrom).setSize(pageSize)
                .setExplain(true).execute().actionGet();

        SearchHits hits = response.getHits();
```

And esClient is from 

```
esClient = TransportClient.builder().settings(settings).build();
```

And mmQuery is MultiMatchQueryBuilder object, and andSearchFilter is a BoolQueryBuilder object.

Note: We got this issue under 500/1000/2000 concurrent performance test.
</comment><comment author="clintongormley" created="2016-01-29T09:54:41Z" id="176671320">it sounds like you are overloading your cluster.  Check the size of the queue in the search thread pool.  I think the forum is probably a better place to continue this discussion as it doesn't sound like a bug: http://discuss.elastic.co/
</comment><comment author="node" created="2016-01-29T12:30:18Z" id="176729993">Thanks all the same. @clintongormley   I will post this to discuss.elastic.co .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skipping hidden files compilation for script service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16286</link><project id="" key="" /><description>If hidden files are detected during the ScriptService initialization, they will be ignored and a warning message will be logged.

Closes #15269 
</description><key id="129406934">16286</key><summary>Skipping hidden files compilation for script service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fforbeck</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T10:15:17Z</created><updated>2016-03-16T19:41:04Z</updated><resolved>2016-03-09T22:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T11:19:49Z" id="176129703">@rjernst could you review please?
</comment><comment author="rjernst" created="2016-02-11T06:17:40Z" id="182734319">@fforbeck Thanks for the update. In the future, you should post a message after you push new commits, otherwise we don't know they exist without checking in on the PR once in a while. :)

Changing the return value of `getScriptNameExt` seems ok. However, it looks like you moved it out of the ScriptChangesListener just so it can be tested. I think it would be better to keep it where it was, and write the test like other existing tests of file based scripts. See for example `ScriptServiceTests.testScriptsWithoutExtensions()`. While the test you added checks `getScriptNameExt` works correctly, it does not check the scripts are actually skipped correctly when found on disk. So I think a test actually using the `resourceWatcherService` in `ScriptServiceTests` would better ensure we don't accidentally break skipping over hidden files in future refactorings.
</comment><comment author="fforbeck" created="2016-02-16T00:51:12Z" id="184456726">Hi @rjernst thanks for your reply. Okay, next time I will make sure to post a message.

Regarding the test, in my first PR I had added the test to check if the file was properly skipped using the `resourceWatcherService`. 

@jasontedor pointed out that it wasn't actually testing the change because we will never have a cached key named "hidden_file" if the script file is called `.hidden_file`. The cached key would be an empty string, because the file has no name and the test case was trying to retrieve the file by `hidden_file` cached key.

Then I decided to move the `getScriptNameExt` the method out of the listener class and drop the modifier to write the test. In addition to hidden files, I got another case which we have an issue -
if the file does not have any extension.

So, I have updated the code to test the `getScriptNameExt` independently with valid and invalid file names, and added back the test for the `ScriptChangesListener` to check if the hidden script file is skipped.

Please let me know what you think.

Thank you!
</comment><comment author="rjernst" created="2016-02-25T20:08:33Z" id="188958550">@fforbeck I wasn't suggesting a test like you had before. I agree with @jasontedor that checking for non-existence of a script with an empty name isn't really testing anything. However, I think there a test which ensures an error does not occur when an arbitrary dot file exists, and that other scripts are still loaded, would be good. As I mentioned in my previous comment, your tests right now are not actually testing that the ScriptService works, only that this method (which really should stay internal to the file change listener) works by itself.
</comment><comment author="fforbeck" created="2016-03-01T12:40:51Z" id="190707256">@rjernst 
Alright! Got it! I have added the test to load scripts even if dot files are found and I moved that method to the inner class as you recommended. Could you review it please?

Thanks a lot!
</comment><comment author="rjernst" created="2016-03-09T22:46:13Z" id="194550332">Looks good, I merged, thanks @fforbeck! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term vector APIs should no longer update mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16285</link><project id="" key="" /><description>Before if an unmaps field was found by the term vector APIs, this missing field would be added to the mappings. This PR changes that and doesn't modify the mapping any more if an unmapped field is found.
</description><key id="129395326">16285</key><summary>Term vector APIs should no longer update mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>:Term Vectors</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T09:33:55Z</created><updated>2016-01-29T09:06:52Z</updated><resolved>2016-01-29T09:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-28T09:53:08Z" id="176093045">hell yeah!
</comment><comment author="jpountz" created="2016-01-28T16:37:32Z" id="176273298">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ParseContext#flyweight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16284</link><project id="" key="" /><description>This removes the ParseContext#flyweight option.  It was only really used for the id and source field mapper, but in those places we can also work without it. This cleans things up.
</description><key id="129393564">16284</key><summary>Remove ParseContext#flyweight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-28T09:25:10Z</created><updated>2016-01-29T09:17:56Z</updated><resolved>2016-01-29T09:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-28T16:31:02Z" id="176265805">LGTM. Thanks for simplifying the parse context.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch failed to create index after upgradation to 2.1.1 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16283</link><project id="" key="" /><description>[logstash-2016.01.28] failed to create
MapperParsingException[Failed to parse mapping [_default_]: Mapping definition for [geoip] has unsupported parameters:  [path : full]]; nested: MapperParsingException[Mapping definition for [geoip] has unsupported parameters:  [path : full]];
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:369)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="129374500">16283</key><summary>Elasticsearch failed to create index after upgradation to 2.1.1 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hiteshmahajan</reporter><labels /><created>2016-01-28T08:06:09Z</created><updated>2016-01-28T11:14:20Z</updated><resolved>2016-01-28T11:14:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hiteshmahajan" created="2016-01-28T08:18:34Z" id="176047776">Its resolved, added template_overwrite =&gt; true to conf file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can not set default analyzer except those which are builtin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16282</link><project id="" key="" /><description>log

```
IndexCreationException[failed to create index]; nested: IllegalArgumentException[Unknown Analyzer type [ik] for [default]]
```

I'm using Elasticsearch 2.1.1,

config in
`elasticsearch.yml`
is only have

```
index.analysis.analyzer.default.type: ik
```

```
root@xxx:/opt/elasticsearch-jdbc-2.1.1.2/bin# /usr/share/elasticsearch/bin/plugin list
Installed plugins in /usr/share/elasticsearch/plugins:
    - elasticsearch-analysis-mmseg-1.7.0
    - elasticsearch-analysis-pinyin-1.5.2
    - elasticsearch-analysis-stconvert-1.6.1
    - elasticsearch-analysis-ik-1.7.0
```

if i set config using cluster setting api

```
GET /xxx/_analyze
{
    "text": "&#20320;&#22909;",
    "analyzer": "ik"
}
```

it works fine.
</description><key id="129329377">16282</key><summary>can not set default analyzer except those which are builtin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">denghongcai</reporter><labels /><created>2016-01-28T04:09:56Z</created><updated>2016-01-29T02:52:03Z</updated><resolved>2016-01-28T11:13:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T11:13:01Z" id="176126718">It looks to me like the analyzer is called either `ik_smart` or `ik_max_word` (see https://github.com/medcl/elasticsearch-analysis-ik)

Either way, i tried this with the smartcn analysis plugin and it works as expected.  The `ik` plugin is not maintained by elasticsearch so if my suggestion doesn't work, you should open an issue there.
</comment><comment author="medcl" created="2016-01-29T02:52:03Z" id="176544166">actually you have already set the default analyzer to `ik`, but the _analyze api is broken,see the issue here: https://github.com/elastic/elasticsearch/issues/16219
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JAVA API[1.5]: ImmutableSettings.settingsBuilder.put(map) doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16281</link><project id="" key="" /><description>Elasticsearch[1.5]
Unable to connect to nodes when using any map to pass key-value properties to immutable settings builder. 
</description><key id="129277490">16281</key><summary>JAVA API[1.5]: ImmutableSettings.settingsBuilder.put(map) doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shashank1029</reporter><labels /><created>2016-01-27T21:39:35Z</created><updated>2016-01-27T22:09:04Z</updated><resolved>2016-01-27T22:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve documentation on snapshot restore using URL (read-only) repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16280</link><project id="" key="" /><description>A few documentation improvement suggestions based on user experience in the field:
- Will be nice to add a cross reference to setting up [read only url repository](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-snapshots.html#_read_only_url_repository) from the [restore](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-snapshots.html#_restore) section of the guide.
- Providing an example (eg. for file protocol) will be nice.

```
PUT /_snapshot/my_restore_repo
{
  "type": "url",
  "settings": {
    "url": "file:///nfs/path"
  }
}
```
- When setting up a read-only repository, only setting up repositories.url.allowed_urls is sufficient.  The statements below suggests that path.repo is also required.

&gt; "URL Repository supports the following protocols: "http", "https", "ftp", "file" and "jar". URL repositories with http:, https:, and ftp: URLs has to be whitelisted by specifying allowed URLs in the repositories.url.allowed_urls setting. "
&gt; "URL repositories with file: URLs can only point to locations registered in the path.repo setting similar to shared file system repository."  
</description><key id="129274488">16280</key><summary>Improve documentation on snapshot restore using URL (read-only) repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>docs</label></labels><created>2016-01-27T21:28:45Z</created><updated>2016-11-05T15:41:45Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="frederikbosch" created="2016-11-05T09:16:22Z" id="258600182">I dont get the purpose of this method at all. Could someone dxplain it?
</comment><comment author="clintongormley" created="2016-11-05T14:04:01Z" id="258613593">@frederikbosch you can make snapshots available to the public (or privately to others) via a read-only URL repository.
</comment><comment author="frederikbosch" created="2016-11-05T14:49:18Z" id="258616355">@clintongormley Alright, but how is that achieved? Is the data pushed to some other location?
</comment><comment author="frederikbosch" created="2016-11-05T14:50:52Z" id="258616452">My case is that I want to push the backup directly (without hitting the local fs) to another location (over HTTPS over FTPS/SFTP). And I thought that this backup type might be useful. For me S3, HDFS and Azure are not an option.
</comment><comment author="clintongormley" created="2016-11-05T15:10:52Z" id="258617691">@frederikbosch no, it's read only.  it's for making read only copies available to others, not for making the snapshots yourself. we don't have an http only writable repository
</comment><comment author="frederikbosch" created="2016-11-05T15:41:45Z" id="258619626">@clintongormley Thanks for your answers! That is really helpful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Checkstyle!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16279</link><project id="" key="" /><description>This duplicates all of the checks that we have in ForbiddenPatterns with
checkstyle to demonstrate that it is possible. We should either remove
them from checkstyle or remove java from forbidden patterns with a note
to go look in checkstyle. Or just live with a little duplication.

It removes a few tests that were in forbidden patterns and moves them to
checkstyle because they are only appropriate to java. In some cases they are
better - the java.io.Serializable test now doesn't fail if you put
`java.io.Serializable` in comments.

Adds a few basic checks that already pass as well as some commented out checks
that don't pass. Things like "lines wrap at 140 characters", which is part
of CONTRIBUTING.md. Its true for all but about 3500 lines.
</description><key id="129273496">16279</key><summary>Checkstyle!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>discuss</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T21:26:03Z</created><updated>2016-01-30T20:07:39Z</updated><resolved>2016-01-30T20:07:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-27T21:27:46Z" id="175863192">@rjernst we talked about this a while back and I figured I could mock it up and we could bat it around. Its reasonably simple to plug together and would allow us to turn on tons of checks.

Its pretty quick from my perspective.
</comment><comment author="rjernst" created="2016-01-27T21:57:55Z" id="175880368">Since checkstyle clearly subsumes what we could do with the forbidden patterns task, I'm ok with this change (and removing forbidden patterns altogether). But I would like to hear others that have worked on the buildsystem here for there comments. e.g. there was checkstyle configuration in maven, but I don't believe it was used, so why was a regex check used then instead of checkstyle?
</comment><comment author="nik9000" created="2016-01-27T22:09:40Z" id="175885925">One thing: forbiddenPatterns works fine on all test file. I don't believe checkstyle is going to work well on non-java files. I'll have to play with it some more.
</comment><comment author="nik9000" created="2016-01-28T14:25:55Z" id="176206732">There are _tons_ of things we can add to this. You can look around [here](http://checkstyle.sourceforge.net/google_style.html) for an attempt to enforce Google's rather well known style.

If we do decide to use checkstyle I don't think we should go nuts adding lots of things right away. If you are just starting a project that might be a good idea but we've got zillions of lines which might violate. Also Elasticsearch has traditionally been pretty diverse in styles and I don't think its worth spending lots of time removing the diversity. But we can add obvious stuff like the line length check.
</comment><comment author="nik9000" created="2016-01-28T22:49:48Z" id="176466111">I'm going to poke this issue because conversation's stalled for a day:

I propose we run checkstyle on java files and forbidden patterns on all files. I'll remove the checks that I duplicated from forbidden patterns (tabs, nocommit) but keep the ones that I moved from forbidden patterns (java.io.Serializable, wildcard imports). I'll keep the commented out checks for things like line length. We can work on those later.
</comment><comment author="javanna" created="2016-01-29T08:18:08Z" id="176635165">+1 sounds like a good improvement
</comment><comment author="nik9000" created="2016-01-30T14:38:39Z" id="177193558">Having received positive reviews so far but no LGTM I'm going to make the executive decision to put a fuse on this one: I'll merge it on my Tuesday morning unless I hear otherwise. If we hate it we can just revert it. It should be simple to revert.
</comment><comment author="jasontedor" created="2016-01-30T15:04:22Z" id="177201146">Just have a couple questions, LGTM. Let's get this in and then consider adding some more (e.g., `UpperEll` because lower-case `l` in `long` literals make me sad, `EmptyBlock`, and do the work to get `LineLength` in). I'm happy to share in the burden here.
</comment><comment author="nik9000" created="2016-01-30T20:02:06Z" id="177287392">&gt; LGTM

OK! I've rebased and I'm the precommit checks once again to make sure I don't bust anything.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored the term suggestion builder for the query refactoring effort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16278</link><project id="" key="" /><description>Added the term suggestion builder's serialization/deserialization and
equals/hashCode methods.
</description><key id="129268811">16278</key><summary>Refactored the term suggestion builder for the query refactoring effort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-01-27T21:08:17Z</created><updated>2016-02-03T17:01:32Z</updated><resolved>2016-02-03T17:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-29T14:11:58Z" id="176774588">Looks great already, I left some comments and maybe @areek wants to have another look too.
</comment><comment author="abeyad" created="2016-01-30T04:30:46Z" id="177064470">@cbuescher I made the changes we discussed.
</comment><comment author="cbuescher" created="2016-02-01T14:27:59Z" id="177993300">@abeyad thanks, looks very good now. I left two minor comments, also lets wait on input on the NamedEnum, then I think this is ready for merging.
</comment><comment author="abeyad" created="2016-02-01T22:21:37Z" id="178226238">@cbuescher @colings86 @areek I updated the PR with the latest commits that address the code review comments.  One thing to note is that I also added a `AbstractWriteableEnumTestCase` that reduces the amount of boilerplate code for writing tests for enums that implement `Writeable`.
</comment><comment author="abeyad" created="2016-02-02T22:29:59Z" id="178864470">@cbuescher @colings86 @areek Pushed changes based on discussion earlier.
</comment><comment author="areek" created="2016-02-03T05:58:10Z" id="179028089">thanks @abeyad this looks great! I left some minor comments. 
</comment><comment author="abeyad" created="2016-02-03T15:39:02Z" id="179298547">@areek @cbuescher @colings86 I removed `fromUnderlying` and fixed the error messages per Areek's comments.  If its good otherwise, should we merge?
</comment><comment author="areek" created="2016-02-03T16:56:52Z" id="179344200">Thanks @abeyad LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests.testUpdateSettingsCanNotChangeThreadPoolType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16277</link><project id="" key="" /><description>Test Failure:
org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests.testUpdateSettingsCanNotChangeThreadPoolType

Repro:
gradle :core:test -Dtests.seed=F06F5C97649014C1 -Dtests.class=org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests -Dtests.method="testUpdateSettingsCanNotChangeThreadPoolType" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=552m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:-UseCompressedOops" -Dtests.locale=sr-BA -Dtests.timezone=Europe/Chisinau

Build:
http://build-us-00.elastic.co/job/es_core_master_small/6738/

Description:
http://build-us-00.elastic.co/job/es_core_master_small/6738/testReport/junit/org.elasticsearch.threadpool/UpdateThreadPoolSettingsTests/testUpdateSettingsCanNotChangeThreadPoolType/
</description><key id="129253666">16277</key><summary>Test Failure: org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests.testUpdateSettingsCanNotChangeThreadPoolType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T20:06:21Z</created><updated>2016-03-10T18:38:38Z</updated><resolved>2016-01-27T20:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-27T20:19:22Z" id="175835307">@s1monw says he pushed a fix for this. Probably https://github.com/elastic/elasticsearch/commit/7ef762c8f0758d08fb136a4fbecf873f0fdb95ec
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>catch processor/pipeline factory exceptions and return structured error responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16276</link><project id="" key="" /><description>fixes: https://github.com/elastic/elasticsearch/issues/16010
</description><key id="129253275">16276</key><summary>catch processor/pipeline factory exceptions and return structured error responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T20:05:05Z</created><updated>2016-03-18T13:23:46Z</updated><resolved>2016-01-29T21:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-29T21:04:57Z" id="176969193">Left two small comments. OTT LGTM
</comment><comment author="s1monw" created="2016-02-01T10:51:42Z" id="177910583">@talevy @martijnvg I think this PR should be reverted and we should use out existing exception infrastructure for this. You can set headers on all ElasticsearchExceptions that get's rendered in a strucutred way. There is no reason to add so much custom code we have a streamlined way to do this.

you can just do something like this:

``` JAVA
try {
 // configure
} catch (Exception ex) {
 ElasticsearchException e = new ElasticsearchException("failed to configure pipeline", ex);
 e.addHeader("processor", "grok");
 e.addHeader("what_have_you", "boom");
  throw e;
}
```

this will get rendered as:

``` JSON
{
  "type" : "exception",
  "reason" : "failed to configure pipeline",
  "caused_by" : {
    "type" : "illegal_argument_exception",
    "reason" : "something is wrong"
  },
  "header" : {
    "what_have_you" : "boom",
    "processor" : "grok"
  }
}
```
</comment><comment author="talevy" created="2016-02-01T16:29:39Z" id="178056925">interesting, I was not familiar with this `header` functionality. thanks for the suggestion!

do we want to do this. shall I preserve the `ConfigurationPropertyException` and then wrap this during the same time that we have all the `PipelineFactoryError` logic? that would really clean up that level, you're right. I'll start off with that, and we can push it down further if that is suitable.
</comment><comment author="martijnvg" created="2016-02-01T17:00:58Z" id="178068851">&gt; do we want to do this. shall I preserve the ConfigurationPropertyException and then wrap this during the same time that we have all the PipelineFactoryError logic? that would really clean up that level, you're right. I'll start off with that, and we can push it down further if that is suitable.

Discussed this we only throw `SearchParseException` and add headers from the processors and `ConfigurationUtils`. The `PipelineFactoryError` logic and `ConfigurationPropertyException` are no longer needed.
</comment><comment author="s1monw" created="2016-02-01T20:06:19Z" id="178165470">@martijnvg I think `ElasticsearchParseException` is what we should throw
</comment><comment author="s1monw" created="2016-02-01T20:07:08Z" id="178165664">oh and @talevy @martijnvg by reverting I mean not back it out just use the existing mechanism. :)

thanks
</comment><comment author="martijnvg" created="2016-02-01T20:08:28Z" id="178166023">&gt; @martijnvg I think ElasticsearchParseException is what we should throw

Agreed `ElasticsearchParseException` is better otherwise we're forced to pass down null arguments.
</comment><comment author="talevy" created="2016-02-01T21:25:46Z" id="178201344">confirming here that I am working on migrating to `ElasticsearchParseException`! thanks
</comment><comment author="talevy" created="2016-02-01T21:43:36Z" id="178207642">PR for this fix can be found here: #16355
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Illegal shard failure requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16275</link><project id="" key="" /><description>Today, shard failure requests are blindly handled on the master without
any validation that the request is a legal request. A legal request is a
shard failure request for which the shard requesting the failure is
either the local allocation or the primary allocation. This is because
shard failure requests are classified into only two sets: requests that
correspond to shards that exist, and requests that correspond to shards
that do not exist. Requests that correspond to shards that do not exist
are immediately marked as successful (there is nothing to do), and
requests that correspond to shards that do exist are sent to the
allocation service for handling the failure.

This pull request adds a third classification for shard failure requests
to separate out illegal shard failure requests and enables the master to
validate shard failure requests. The master communicates the illegality
of a shard failure request via a new exception:
`IllegalShardFailureException`. This exception can be used by shard
failure listeners to discover when they've sent a shard failure request
that they were not allowed to send (e.g., if they are no longer the
primary allocation for the shard).
</description><key id="129230350">16275</key><summary>Illegal shard failure requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T18:36:31Z</created><updated>2016-02-05T10:38:41Z</updated><resolved>2016-02-02T22:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-27T18:37:52Z" id="175786245">@bleskes Note that this does not yet add support to `TransportReplicationAction` to handle situations where a node is acting as if it's a primary, tries to fail a replica but by the time the request arrives on the master the node is no longer the primary; the handling of such a situation is unchanged by this pull request. This is merely getting the plumbing in place to enable handling on the `TransportReplicationAction` side which will come in a later pull request.
</comment><comment author="jasontedor" created="2016-01-29T18:24:33Z" id="176896362">@bleskes I've pushed a new commit to use allocation IDs as identity instead of node IDs. I think this is ready for a formal review?
</comment><comment author="bleskes" created="2016-02-01T10:53:32Z" id="177912091">Thanks @jasontedor . Left some comments
</comment><comment author="jasontedor" created="2016-02-01T16:46:13Z" id="178062674">@bleskes I've pushed more commits addressing all of your comments.
</comment><comment author="bleskes" created="2016-02-02T13:31:42Z" id="178575352">left a bunch of minor improvement suggestions... 
</comment><comment author="jasontedor" created="2016-02-02T20:02:40Z" id="178790236">&gt; left a bunch of minor improvement suggestions...

Thanks @bleskes, I pushed more commits so this is ready for another round.
</comment><comment author="bleskes" created="2016-02-02T20:19:23Z" id="178800982">LGTM. Thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent TransportReplicationAction to route request based on stale local routing table</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16274</link><project id="" key="" /><description>Relates to #12573

When relocating a primary shard, there is a cluster state update at the end of relocation where the active primary is switched from the relocation source to the relocation target. If relocation source receives and processes this cluster state before the relocation target, there is a time span where relocation source believes active primary to be on relocation target and relocation target believes active primary to be on relocation source. This results in index/delete/flush requests being sent back and forth and can end in an OOM on both nodes.

This PR adds a field to the index/delete/flush request that helps detect the case where we locally have stale routing information. In case this staleness is detected, we wait until we have received an up-to-date cluster state before rerouting the request.

I have included the test from #12574 in this PR to demonstrate the fix in an integration test. That integration test will not be part of the final commit, however.
</description><key id="129226756">16274</key><summary>Prevent TransportReplicationAction to route request based on stale local routing table</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T18:19:19Z</created><updated>2016-02-02T15:16:41Z</updated><resolved>2016-02-02T12:59:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-01-27T18:25:13Z" id="175781726">@bleskes instead of using the cluster state version, we could as well use the index metadata version. The index metadata version is updated whenever a new shard is started (thanks to active allocation ids). wdyt?

On a related note, we could use this field as well to wait for dynamic mapping updates to be applied. (for that the update mappings api would have to return the current index metadata version).
</comment><comment author="ywelsch" created="2016-02-01T17:43:44Z" id="178090868">@bleskes renamed the field and removed integration test.
</comment><comment author="bleskes" created="2016-02-01T17:53:26Z" id="178094455">LGTM . Thanks @ywelsch  - Left some minor comments, no need for another cycle.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch NodeEnvironment's settings to new settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16273</link><project id="" key="" /><description>And add a few more tests.
</description><key id="129220502">16273</key><summary>Switch NodeEnvironment's settings to new settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T17:54:18Z</created><updated>2016-01-28T17:09:03Z</updated><resolved>2016-01-28T17:09:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-27T17:54:25Z" id="175768895">Like this @s1monw ?
</comment><comment author="s1monw" created="2016-01-28T13:19:42Z" id="176183081">left one comment - LGTM otherwise
</comment><comment author="nik9000" created="2016-01-28T15:24:37Z" id="176234731">I've addressed the comments and rebased to work around the merge conflicts. I'm running tests once again locally and I'll push if they pass.
</comment><comment author="s1monw" created="2016-01-28T15:37:59Z" id="176239713">LGTM
</comment><comment author="nik9000" created="2016-01-28T16:41:28Z" id="176275792">I just noticed my tests passed. And merge conflicts! Yay.
</comment><comment author="nik9000" created="2016-01-28T16:43:43Z" id="176276747">Rebase was easy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make SuggestionBuilder fieldname mandatory constructor argument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16272</link><project id="" key="" /><description>The fieldname seems to be mandatory for all suggestion builder implementations. This removes the setter and makes it a mandatory constructor argument.
</description><key id="129208261">16272</key><summary>Make SuggestionBuilder fieldname mandatory constructor argument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>WIP</label></labels><created>2016-01-27T17:05:25Z</created><updated>2017-06-10T15:46:04Z</updated><resolved>2016-03-02T17:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-26T13:07:42Z" id="189266069">@cbuescher Can you resolve conflicts and ping me afterwards? Then I'm happy to review your PR.
</comment><comment author="cbuescher" created="2016-02-26T17:42:17Z" id="189387068">@danielmitterdorfer thanks a lot, this is quiet stale and needs some additional work after refactoring the remaining two suggester implementations, so I'll need to extend this quiet a bit. Will ping you once its ready and added the WIP label for now.
</comment><comment author="danielmitterdorfer" created="2016-02-26T17:45:14Z" id="189388320">@cbuescher Ok, thanks for the feedback. Does it make sense to remove the review label in the meantime?
</comment><comment author="cbuescher" created="2016-03-02T17:09:23Z" id="191328776">Closed in favour of https://github.com/elastic/elasticsearch/pull/16913
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for search after</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16271</link><project id="" key="" /><description>- Removes search_after from the query string param of the rest api spec.
- Handle null values when sorting on strings.
- Wait for green status in the integ tests.
</description><key id="129207461">16271</key><summary>Fix for search after</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T17:02:09Z</created><updated>2016-01-27T18:58:48Z</updated><resolved>2016-01-27T18:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-27T17:06:36Z" id="175748840">@jpountz this is the follow up of the chat with @clintongormley. 
I've added a fix for the sporadic error in the integ tests but I am not sure if it is enough to call ensureGreen after each create or update index.
</comment><comment author="nik9000" created="2016-01-27T17:18:40Z" id="175753851">Null handling LGTM. I left a few minor nits. I haven't read the first search_after PR so @jpountz should probably review as well.
</comment><comment author="jpountz" created="2016-01-27T17:22:52Z" id="175755291">Nothing to add besides what @nik9000 said.
</comment><comment author="jimczi" created="2016-01-27T17:39:29Z" id="175763111">I think I've covered all the comments.
Thanks for the review @nik9000 and @jpountz.
</comment><comment author="nik9000" created="2016-01-27T17:46:21Z" id="175765306">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify AutoCreateIndex and add more tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16270</link><project id="" key="" /><description>Remove the globallyDisabled internal flag which simply replicated needToCheck. Also kept a single of list of indices around, with the information of whether each one of them needs to be included or excluded. Taken the chance to add a few more tests.
</description><key id="129192274">16270</key><summary>Simplify AutoCreateIndex and add more tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T16:15:29Z</created><updated>2016-01-27T16:23:50Z</updated><resolved>2016-01-27T16:23:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-27T16:17:55Z" id="175719399">looks great!!! thanks @javanna LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert several pending settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16269</link><project id="" key="" /><description>@jpountz  can you look 
</description><key id="129176587">16269</key><summary>Convert several pending settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T15:26:38Z</created><updated>2016-01-28T10:18:36Z</updated><resolved>2016-01-27T16:57:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-27T16:39:44Z" id="175734572">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow the query cache to be disabled.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16268</link><project id="" key="" /><description>This feature already existed but was neither tested nor documented.

Closes #15802
</description><key id="129169932">16268</key><summary>Allow the query cache to be disabled.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-01-27T15:05:57Z</created><updated>2016-04-11T16:06:50Z</updated><resolved>2016-04-11T16:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-27T15:46:40Z" id="175696069">LGTM
</comment><comment author="rjernst" created="2016-01-27T16:59:14Z" id="175744155">Is having a custom query cache really something we should be promoting? It seems like allowing to disable it is one thing, and should be done with a boolean. 
</comment><comment author="jpountz" created="2016-01-27T17:14:39Z" id="175751707">I enabled it the way it was already implemented but I agree we probably want to keep things more contained.
</comment><comment author="jpountz" created="2016-04-07T13:56:34Z" id="206918515">Thanks @rjernst for the suggestion. I replaced the setting with a flag and removed the ability to configure custom query caches.
</comment><comment author="rjernst" created="2016-04-11T15:21:57Z" id="208399198">This LGTM as is. I have 2 thoughts for the future:
- Now that "none" is gone as far as the user is concerned, I wonder if we could rename the impl to a name that makes sense. Perhaps DisabledQueryCache?
- It would be nice if we removed the force method and used a package private member that tests could override, like we do with other test only overrides.
</comment><comment author="jpountz" created="2016-04-11T15:27:59Z" id="208401200">&gt; DisabledQueryCache

I like the name better too so I will rename before merging.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate query caching settings to the new settings infra.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16267</link><project id="" key="" /><description /><key id="129159036">16267</key><summary>Migrate query caching settings to the new settings infra.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T14:26:38Z</created><updated>2016-01-28T10:10:06Z</updated><resolved>2016-01-27T15:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-27T14:29:34Z" id="175656155">left one comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API cluster health: Add support for the level request parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16266</link><project id="" key="" /><description>Currently the _ClusterHealthRequest_ class (and therefore _ClusterHealthRequestBuilder_) does not support the [**level** request parameter](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params).

Each time the cluster health is requested using the Java API

``` java
//cluster health at shard level is retrieved
client.admin().cluster().prepareHealth().execute().actionGet();
```

The cluster health is retrieved at _shard_ level. (Tested on version 2.0.0)

This improvement would be very nice for saving memory on cases where the amount of indices/shards is too high and the user only needs the general cluster health. Also it's currently not following the REST API defaults (its default level is _cluster_)
</description><key id="129157361">16266</key><summary>Java API cluster health: Add support for the level request parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">augusto-altman</reporter><labels><label>:Java API</label><label>:Stats</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-27T14:19:21Z</created><updated>2016-01-28T10:17:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T10:17:35Z" id="176106986">The stats have to be collected at the shard level anyway, in order to summarise it at the index and cluster level.  Really the only change would be to not populate the maps in the response.  This may be worth doing for a small saving when sending the response to the client.

Note: changing the default level to `cluster` for the java api would be a breaking change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] re-enable and merge cluster settings REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16265</link><project id="" key="" /><description>We used to have a disabled test around cluster put settings as it left cluster settings behind without a way to remove them. That has been in fixed in the cluster put settings api, so the test can be re-enabled.
</description><key id="129156321">16265</key><summary>[TEST] re-enable and merge cluster settings REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T14:14:29Z</created><updated>2016-01-27T16:38:42Z</updated><resolved>2016-01-27T16:38:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-27T16:30:12Z" id="175727214">@s1monw does this look ok?
</comment><comment author="s1monw" created="2016-01-27T16:31:11Z" id="175727622">looks great
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put mapping operations must update metadata of all types.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16264</link><project id="" key="" /><description>Today put mapping operations only update metadata of the type that is being
modified, which is not enough since some modifications may have side-effects
on other types.

Closes #16239
</description><key id="129147128">16264</key><summary>Put mapping operations must update metadata of all types.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T13:39:35Z</created><updated>2017-05-16T10:56:53Z</updated><resolved>2016-02-10T09:41:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-02-08T15:45:12Z" id="181432567">@rjernst could you give it a look?
</comment><comment author="rjernst" created="2016-02-08T19:30:10Z" id="181531816">LGTM
</comment><comment author="rjernst" created="2016-02-10T22:43:59Z" id="182616760">@jpountz This is a pretty bad bug for mapping validation, and the fix is relatively simple. Should we put it into 2.2.1?
</comment><comment author="s1monw" created="2016-02-11T08:16:10Z" id="182758867">+1 to 2.2.1 and also port it to 2.1 in case we do a bugfix release there too?
</comment><comment author="jpountz" created="2016-02-11T13:03:52Z" id="182854439">The problem is that 2.2 has another bug: when you update a mapping with `update_all_types=true`, it will not refresh the mapping source of other types (we had this bug since 2.0 and it was fixed as a side-effect of #15539) so this fix alone won't help, we would also need to fix mapping updates to refresh the source of all types. If the consensus is that we should do it, I will do it, but these are big changes so I'm not comfortable pushing them to a bugfix release.
</comment><comment author="travisjeffery" created="2016-03-03T00:03:04Z" id="191501758">Is there a workaround for 2.2? This bug is a really bad bug to leave like it is.
</comment><comment author="rjernst" created="2016-03-03T00:31:37Z" id="191508004">@travisjeffery the workaround would be to explicitly send updates to every type that contains the field you are changing.
</comment><comment author="clintongormley" created="2016-03-03T15:59:35Z" id="191826466">@rjernst @travisjeffery there is no way to do that.  The update-mapping API only works on a single type at a time. The only time you can set the mapping for multiple types is at index creation.
</comment><comment author="rjernst" created="2016-03-03T17:52:09Z" id="191885341">@clintongormley It does work. Updating them in one internal method call doesn't matter. We just need to get the mappings in memory back into the correct state.

```
DELETE test_index

PUT test_index
{
  "mappings": {
    "type1": {
      "properties": {
        "field": {
          "type": "string"
        }
      }
    },
    "type2": {
      "properties": {
        "field": {
          "type": "string"
        }
      }
    }
  }
}

PUT test_index/type1/_mapping?update_all_types=true
{
  "properties": {
    "field": {
      "type": "string",
      "search_analyzer": "keyword",
      "analyzer": "default"
    }
  }
}

GET test_index/_mapping?pretty
{
  "test_index" : {
    "mappings" : {
      "type1" : {
        "properties" : {
          "field" : {
            "type" : "string",
            "analyzer" : "default",
            "search_analyzer" : "keyword"
          }
        }
      },
      "type2" : {
        "properties" : {
          "field" : {
            "type" : "string"
          }
        }
      }
    }
  }
}

PUT test_index/type2/_mapping?update_all_types=true
{
  "properties": {
    "field": {
      "type": "string",
      "search_analyzer": "keyword",
      "analyzer": "default"
    }
  }
}

GET test_index/_mapping?pretty
{
  "test_index" : {
    "mappings" : {
      "type1" : {
        "properties" : {
          "field" : {
            "type" : "string",
            "analyzer" : "default",
            "search_analyzer" : "keyword"
          }
        }
      },
      "type2" : {
        "properties" : {
          "field" : {
            "type" : "string",
            "analyzer" : "default",
            "search_analyzer" : "keyword"
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-03-04T09:04:19Z" id="192193163">Ah I see what you mean: using multiple calls to update mapping, ok
</comment><comment author="fengzhi" created="2017-05-16T10:56:53Z" id="301747177">es 5.2 , I have many types. How can I add my new field to each type . I tryed  put mapping with `update_all_types` but it does not actually update all types.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert `action.auto_create_index` and `action.master.force_local` to the new settings infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16263</link><project id="" key="" /><description /><key id="129146198">16263</key><summary>Convert `action.auto_create_index` and `action.master.force_local` to the new settings infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T13:35:07Z</created><updated>2016-01-27T15:25:54Z</updated><resolved>2016-01-27T14:18:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-27T13:35:16Z" id="175631209">@javanna can you take a look?
</comment><comment author="javanna" created="2016-01-27T13:46:14Z" id="175634354">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>min_score doesn't work with query string?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16262</link><project id="" key="" /><description>```
{
    min_score: 0.2,
    query_string: {
        query: {
            ...
        }
    }
}
```

doesn't seem to work...
am i missing something?

thanks!
</description><key id="129142437">16262</key><summary>min_score doesn't work with query string?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SkateFreak</reporter><labels /><created>2016-01-27T13:21:39Z</created><updated>2016-01-28T09:39:11Z</updated><resolved>2016-01-28T09:39:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T09:39:11Z" id="176086344">Hi @SkateFreak 

I tried it and it works just fine.  Please ask questions like these in the forums: https://discuss.elastic.co/ and i suggest that you provide more information when you do.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes json generation for scriptsort w/ deprecated params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16261</link><project id="" key="" /><description>Fixes #16260

@colings86 Could you have a quick look, as you did the original modification?
</description><key id="129126098">16261</key><summary>Fixes json generation for scriptsort w/ deprecated params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Scripting</label><label>:Search Refactoring</label><label>bug</label><label>v2.3.0</label></labels><created>2016-01-27T12:16:01Z</created><updated>2016-01-27T13:21:47Z</updated><resolved>2016-01-27T13:21:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-27T13:18:29Z" id="175620445">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ScriptSortBuilder:toXContent uses wrong parameter object when writing nested filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16260</link><project id="" key="" /><description>This was discovered on master and fixed there (see #16153) but already existed in 2.2:

https://github.com/elastic/elasticsearch/blob/2.2/core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java#L192 ... here we are passing the parameters that actually belong to the script itself when writing out the nested query filter. Instead this should use the method's parameter "builderParams".

I tried modifying the [SimpleSortTests](https://github.com/elastic/elasticsearch/blob/209860854ddc87ff71a49d9ced1186a8e2c8e3b8/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java#L575), one of our messy tests, to demonstrate the issue - except apparently the additional json that's being generated doesn't bother [SortParseElement's](https://github.com/elastic/elasticsearch/blob/76fa9023b6378681de34672d2e94227e8b464cfd/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java) parsing method so the test above remained green. So not sure if this should be considered a bug and fixed for 2.x.

Will post a test as PR showing the difference in json generated depending on whether script params are set or not shortly.
</description><key id="129125486">16260</key><summary>ScriptSortBuilder:toXContent uses wrong parameter object when writing nested filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Scripting</label><label>:Search Refactoring</label><label>v2.3.0</label></labels><created>2016-01-27T12:13:00Z</created><updated>2016-01-27T13:22:10Z</updated><resolved>2016-01-27T13:22:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-01-27T13:22:10Z" id="175623445">Closed by  #16261
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API to allow queries to bypass the query cache policy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16259</link><project id="" key="" /><description>We have currently a performance issue with the new query cache policy. We have queries that are quite heavy to construct and compute, even on small segments. The `UsageTrackingQueryCachingPolicy` (which use `CacheOnLargeSegments`) will always discard the caching of our queries on small segments. This leads to a significant drop of performance (5x to 10x) in our scenarios. 
Another limitation of the `UsageTrackingQueryCachingPolicy` is that there is no easy way to indicate him that our queries are costly to build, apart from subclassing our queries with MultiTermQuery so that it is picked up by the `UsageTrackingQueryCachingPolicy#isCostly`.
At the moment, the only solution we have is to configure elasticsearch to switch back to the `QueryCachingPolicy.ALWAYS_CACHE` cache policy.
</description><key id="129117945">16259</key><summary>API to allow queries to bypass the query cache policy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rendel</reporter><labels><label>:Cache</label><label>:Search</label><label>discuss</label></labels><created>2016-01-27T11:38:19Z</created><updated>2016-01-29T12:07:58Z</updated><resolved>2016-01-27T18:29:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rendel" created="2016-01-27T12:01:54Z" id="175585836">Related to #16031 
</comment><comment author="clintongormley" created="2016-01-27T12:24:34Z" id="175591537">@rendel i'm curious as to how you figured out that your queries are heavy to construct on small segments?  That seems counterintuitive.  Could you provide some examples?
</comment><comment author="rendel" created="2016-01-27T12:37:12Z" id="175597258">Hi @clintongormley 

we have developed a custom query which embeds a large number of terms to perform a semi-join between indexes (see [siren-join plugin](https://github.com/sirensolutions/siren-join)). The terms are encoded in a byte array for performance consideration, and decoded lazily at query execution time. The decoding of the terms is the heavy part. We are caching them using a cache key. The issue now is that this decoding is always done for small segments. 
</comment><comment author="jpountz" created="2016-01-27T18:29:49Z" id="175783551">If a query is slow when it is not cached, I don't think the cache is to blame. It is something that users would hit anyway after a merge or a restart. I actually think not caching on small segments is _very_ important as:
- it does not affect performance with regular queries
- it makes memory accounting more accurate (it is easier to account memory usage for a few large cache entries that many tiny entries)
- it avoids cache churn due to NRT search.

While I think there are things to improve based on the feedback that was given in #16031, I don't think we should make it possible to cache on all segments.
</comment><comment author="rendel" created="2016-01-29T12:07:58Z" id="176724060">@jpountz I would agree that for mainstream cases - the standard Lucene queries - should not be cached on small segments and that the new caching policy is well adapted for those kind of queries. However, there exist very legitimate cases, when this policy is too restrictive. We are not asking to change the high-level api (e.g., query dsl) but just to give that option at low level for advanced users that - like us - are building on top of Elasticsearch.

Something at the java Lucene Query api level, where people creating a new custom Lucene Query can have somehow some control on the cache policy. Maybe this is something that should be implemented at a Lucene level instead of Elasticsearch ?

Without such a control, we would have to fallback to alternative options that are not very optimal:
- tell users to activate the `index.queries.cache.everything: true` setting (but this means that standard queries will not benefit anymore from the cache optimisations introduced by the new caching policy)
- add and manage a secondary query cache that will cache our custom queries (but this adds unnecessary complexity)
- be able to change the cache implementation of elasticsearch to introduce our own (but this does not look possible at the moment - we would have to fork elasticsearch)
  What would be the other fallback options available to us ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node: pass path.conf to inner tribe clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16258</link><project id="" key="" /><description>If we don't do this, and some path.conf is set when starting the tribe node, that path.conf will be ignored and the inner tribe clients will try to read elsewhere, where they most likely don't have permissions to read from.

Closes #16253
</description><key id="129117617">16258</key><summary>Tribe node: pass path.conf to inner tribe clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Tribe Node</label><label>bug</label><label>review</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T11:36:38Z</created><updated>2016-01-27T14:15:00Z</updated><resolved>2016-01-27T12:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-27T11:38:13Z" id="175573801">The fix for this is easy. The problem is testing it. I spent a few hours trying to write a good test for this but I failed. I tried with an integration test to add to TribeIT, but I couldn't make it fail. I also looked into unit testing it but the TribeService would need to be refactored to allow for that, as it has way too much logic and dependencies on its constructor. Suggestions are welcome.
</comment><comment author="javanna" created="2016-01-27T11:42:03Z" id="175574877">one more thing: `HunspellService` is quite peculiar and it's the only component that would cause this type of problem I think, because it eagerly loads the dictionaries in each client node (not sure it's the best thing to do but it's another problem) when the node gets created. Same doesn't happen for synonyms for instance as loading happens when the token filter factory gets created. I thought also something like this could happen for logging, but there loading happens only once from a static block.
</comment><comment author="s1monw" created="2016-01-27T11:42:22Z" id="175575007">LGTM - maybe @rjernst has an idea with test-fixtures
</comment><comment author="javanna" created="2016-01-27T12:38:34Z" id="175598161">I will get this in for now, as a followup we should definitely look at the tribe tests and rewrite them and add more coverage. Whatever we fix here is not properly tested at the moment because the tribe node is so hard to test...the least we can do is add some qa tests.
</comment><comment author="clintongormley" created="2016-01-27T12:42:59Z" id="175600377">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the ability to fsync on every operation and only schedule fsync task if really needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16257</link><project id="" key="" /><description>This commit limits the `index.translog.sync_interval` to a value not less than `100ms` and
removes the support for fsync on every operation which used to be enabled if `index.translog.sync_interval` was set to `0s`
Now this pr also only schedules an async fsync if the durability is set to `async`. By default not async task is scheduled.

Closes #16152
</description><key id="129116503">16257</key><summary>Remove the ability to fsync on every operation and only schedule fsync task if really needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>:Translog</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T11:32:39Z</created><updated>2016-01-27T14:51:23Z</updated><resolved>2016-01-27T14:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-27T14:07:27Z" id="175647685">LGTM
</comment><comment author="jpountz" created="2016-01-27T14:51:23Z" id="175665361">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added note about url encoding for date math support in index names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16256</link><project id="" key="" /><description>Updated examples and added note about the fact that the `/` for date rounding in date math index names should be url encoded as `%2F`
</description><key id="129108228">16256</key><summary>Added note about url encoding for date math support in index names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>docs</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T10:58:20Z</created><updated>2016-01-27T14:15:00Z</updated><resolved>2016-01-27T12:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-27T11:24:00Z" id="175568988">LGTM thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix serialization of `search_analyzer`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16255</link><project id="" key="" /><description>We currently have a bug that it will be omitted if the index analyzer is the
default analyzer.
</description><key id="129096022">16255</key><summary>Fix serialization of `search_analyzer`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T10:18:10Z</created><updated>2016-02-08T15:54:10Z</updated><resolved>2016-02-08T15:54:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-01-27T10:20:35Z" id="175531490">Does that fix https://github.com/elastic/elasticsearch/issues/16239 ?
</comment><comment author="jpountz" created="2016-01-27T10:24:48Z" id="175533706">Not entirely, there are other things to fix to completely fix #16239 that I still need to figure out.
</comment><comment author="rjernst" created="2016-01-27T15:10:57Z" id="175677089">Can you add tests for each part of the boolean clauses? Seems like there are still cases missed by tests (eg this underscore check?).

LGTM
</comment><comment author="jpountz" created="2016-01-27T17:58:23Z" id="175771386">@rjernst I pushed an alternative that doesn't require that we check for analyzer names that start with an underscore.
</comment><comment author="rjernst" created="2016-01-27T18:06:23Z" id="175775779">Much better, thank you for the simplification! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] docs s/processor_id/tag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16254</link><project id="" key="" /><description /><key id="129084213">16254</key><summary>[Ingest] docs s/processor_id/tag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-27T09:30:33Z</created><updated>2016-02-08T09:31:58Z</updated><resolved>2016-01-27T09:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-27T09:41:48Z" id="175512951">LGTM thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node clients using wrong config path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16253</link><project id="" key="" /><description>This is a duplicate of https://github.com/elastic/elasticsearch/issues/15880

To replicate, copy the config directory to a new location and add a simple tribe config, eg:

```
cp -a elasticsearch-2.2.0/config tribe
echo "tribe.t1.cluster.name: foo" &gt;&gt; tribe/elasticsearch.yml
```

Start the tribe node as follows:

```
./elasticsearch-2.2.0/bin/elasticsearch --path.conf tribe
```

The process dies with:

```
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/Users/clinton/workspace/servers/elasticsearch-2.2.0/config/hunspell" "read")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
at java.security.AccessController.checkPermission(AccessController.java:884)
at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
at java.nio.file.Files.readAttributes(Files.java:1737)
at java.nio.file.Files.isDirectory(Files.java:2192)
at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:127)
at org.elasticsearch.indices.analysis.HunspellService.&lt;init&gt;(HunspellService.java:102)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at &lt;&lt;&lt;guice&gt;&gt;&gt;
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
at org.elasticsearch.tribe.TribeClientNode.&lt;init&gt;(TribeClientNode.java:35)
at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:141)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at &lt;&lt;&lt;guice&gt;&gt;&gt;
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

The reason for this is that, when the tribe node tries to create a client to connect to the cluster, it doesn't pass on the `path.conf` setting to the client, so that when HunspellService tries to [resolve the hunspell directory](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java#L114) it uses `path.home` instead of the custom `path.conf`.

Adding the absolute config path to the tribe node configuration works around this:

```
echo "tribe.t1.path.conf: /Users/clinton/workspace/servers/tribe/" &gt;&gt; tribe/elasticsearch.yml
```
</description><key id="129082456">16253</key><summary>Tribe node clients using wrong config path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2016-01-27T09:23:23Z</created><updated>2016-01-29T06:28:55Z</updated><resolved>2016-01-27T12:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gbjuno" created="2016-01-28T03:05:15Z" id="175933369">@clintongormley hi. I do as you said, but it doesn't work!

```
elasticsearch@repository:/$ cat /etc/elasticsearch/elasticsearch.yml 
transport.tcp.port: 9300
http.port: 9200
node.name: es-center
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 0.0.0.0
tribe:
    t1:
        path.conf: /etc/tribe-client/
        cluster.name: ydnj-es
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.2.89.182:50002"]
    t2:
        path.conf: /etc/tribe-client/
        cluster.name: ytyq-es
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.2.89.182:50003"]
```

elasticsearch.yml in /etc/tribe-client/  without anything in it.

```
elasticsearch@repository:/$ ls -al /etc/tribe-client/elasticsearch.yml 
-rw-r--r-- 1 elasticsearch root 0 Jan 27 20:01 /etc/tribe-client/elasticsearch.yml
```

HOWEVER, I get the result.

```
elasticsearch@repository:/$ /usr/share/elasticsearch/bin/elasticsearch --path.conf /etc/elasticsearch
[2016-01-28 11:00:06,314][INFO ][node                     ] [es-center] version[2.1.1], pid[126326], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-28 11:00:06,314][INFO ][node                     ] [es-center] initializing ...
[2016-01-28 11:00:06,353][INFO ][plugins                  ] [es-center] loaded [], sites []
[2016-01-28 11:00:07,421][INFO ][node                     ] [es-center/t1] version[2.1.1], pid[126326], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-28 11:00:07,421][INFO ][node                     ] [es-center/t1] initializing ...
[2016-01-28 11:00:07,480][INFO ][plugins                  ] [es-center/t1] loaded [], sites []
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/etc/tribe-client/hunspell" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
    at java.nio.file.Files.readAttributes(Files.java:1737)
    at java.nio.file.Files.isDirectory(Files.java:2192)
    at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:127)
    at org.elasticsearch.indices.analysis.HunspellService.&lt;init&gt;(HunspellService.java:102)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
```

seems that the path.conf send to the tribe. but it still search for hunspell.

Enviroment: es 2.1.1; java 1.8.0_72
</comment><comment author="gbjuno" created="2016-01-28T03:08:05Z" id="175933827">the same configuration with environment: es 2.0.2

```
[2016-01-28 11:07:17,016][INFO ][node                     ] [es-center] version[2.0.2], pid[127071], build[6abf5d8/2015-12-16T12:49:58Z]
[2016-01-28 11:07:17,017][INFO ][node                     ] [es-center] initializing ...
[2016-01-28 11:07:17,064][INFO ][plugins                  ] [es-center] loaded [], sites []
[2016-01-28 11:07:18,040][ERROR][bootstrap                ] Guice Exception: java.security.AccessControlException: access denied ("java.io.FilePermission" "/etc/tribe-client/elasticsearch.yml" "read")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
        at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
        at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
        at java.nio.file.Files.exists(Files.java:2385)
```
</comment><comment author="clintongormley" created="2016-01-28T10:41:36Z" id="176116531">@gbjuno for the workaround to work, the `path.conf` that you specify on the command line must be the same as the `path.conf` that you specify in the tribe node config.
</comment><comment author="gbjuno" created="2016-01-29T06:28:55Z" id="176600333">@clintongormley ,thank you.After changing the path.conf to /etc/elasticsearch, the tribe node started running.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documents lost on replica shard after replicating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16252</link><project id="" key="" /><description>#### Overview:

After relocating shards as soon as updating index, the documents are not replicated partially on replica shard 
#### Settings:

Elasticsearch 2.1.1
RHEL 6.2
#### Cluster:

Nodes 4
Shards 4
Replica 1
#### Procedure:
1. Make empty index as cluster settings above.
   
   ```
   node 1 : test 0(r), test 3(r)
   node 2 : test 1(r), test 2(r)
   node 3 : test 0(p), test 1(p)
   node 4 : test 2(p), test 3(p)
   *test x = shard number
   ```
2. Update documents by Bulk api. (Update it to "primary node", node 3)
   
   ```
   { "index" : { "_index" : "test", "_type" : "test", "_id" : "1" } }
   {"dummy" : true}
   { "index" : { "_index" : "test", "_type" : "test", "_id" : "2" } }
   {"dummy" : true}
   { "index" : { "_index" : "test", "_type" : "test", "_id" : "3" } }
   {"dummy" : true}
   .....
   ```
3. On finishing updating documents, restart node with replica shard.
   
   -&gt; restart node 1.
4. Wait relocating replica shards.
5. See the number of docs with cat shards api.
   
   ```
   test   3 r STARTED  0  131b {IP} node1  (*)
   test   3 p STARTED 11 6.7kb {IP} node4 
   test   2 p STARTED 10 6.6kb {IP} node4 
   test   2 r STARTED 10 6.6kb {IP} node2 
   test   1 r STARTED  4 6.3kb {IP} node2 
   test   1 p STARTED  4 6.3kb {IP} node3 
   test   0 r STARTED  0 3.2kb {IP} node1  (*)
   test   0 p STARTED  5 6.3kb {IP} node3 
   ```

See the number of docs of test 1 and test 3 shard on node 1 (*) are 0 after relocation.
The docs of primary shards are properly updated.

In addition, with not empty index, after restarting the node while updating docs by bulk api continuously,
the number of primary shard and replica shard are not identical.
</description><key id="129018214">16252</key><summary>Documents lost on replica shard after replicating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yfujita</reporter><labels><label>:CRUD</label><label>discuss</label><label>feedback_needed</label><label>resiliency</label></labels><created>2016-01-27T03:28:43Z</created><updated>2016-02-01T18:06:54Z</updated><resolved>2016-02-01T18:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-27T19:01:27Z" id="175795228">@yfujita I've tried several times to reproduce this based off of your description but it has not reproduced for me. Do you have a script that reliably reproduces the issue that you describe (it does not have to be 100% reliable, just frequent enough that we can use it as a basis for assessing the situation)?
</comment><comment author="yfujita" created="2016-02-01T06:59:03Z" id="177814605">@jasontedor Thank you for reply.

I already discarded the Cluster. So, I set up same settings cluster and tried to reproduce to make script. But It could not reproduce. 

This issue always reproduced when this issue occured. There may be another triggers for this issue.

Before this issue occured, I had repeated Rolling Restart while updating docs by bulk api continuously. At that time, I got another issue that the num_doc of primary shards and replica shards are not identical. 

Proedure of rolling restat:

```
1. Node1 restart.
2. Wait for green status of cluster health api.
3. Node2 restart.
4. Wait for green status of cluster health api.
5. Node3 restart.
...
```

After that, I tried a variety of procedures. As a result, I found the procedures of this issue.
</comment><comment author="jasontedor" created="2016-02-01T18:05:56Z" id="178101642">The procedure you describe is identical to the procedure that we use to reproduce #14671, but with a different outcome. However, I have tried _many_ times to get the reproduction for #14671 to reproduce your issue but it does not reproduce. 

&gt; I already discarded the Cluster. So, I set up same settings cluster and tried to reproduce to make script. But It could not reproduce.

If you're able to produce a script that reliably reproduces the issue, please let us know.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update test to verify that documents are deep-copied between verbose results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16251</link><project id="" key="" /><description>updated this test to reflect changes in #16248 which fix #16246 around `_simulate?verbose` results mutating between processor executions because `IngestDocument`s were not properly being copied.
</description><key id="128959513">16251</key><summary>update test to verify that documents are deep-copied between verbose results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T22:15:19Z</created><updated>2016-01-27T11:31:40Z</updated><resolved>2016-01-26T22:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-26T22:20:28Z" id="175262330">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mark shard active during recovery; push settings after engine finally inits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16250</link><project id="" key="" /><description>This is the master port for #16209 ... it's simpler because in master
we already pushed `lastWriteNanos` down to engine, so it's already being
set properly on translog replay.

I carried forward setting `IndexShard.active` to `true` when translog
recovery starts; while this won't impact indexing buffer decisions
(it's totally different in master with #14121), I think this is still
important so sync'd flush will later notice there is something to
sync, if all the shard does is wake up, play translog, but then do no
indexing ops.

I also carried forward the "refresh engine settings after engine is
done starting" change.  While it's not important for indexing buffer
(since in master indexing buffer is always a huge value), I think if
other settings changes happened (e.g. merge scheduler) they may need
to be pushed as well.
</description><key id="128958605">16250</key><summary>Mark shard active during recovery; push settings after engine finally inits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T22:12:20Z</created><updated>2016-01-28T14:36:55Z</updated><resolved>2016-01-28T11:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-27T08:46:10Z" id="175487781">LGTM. Thanks @mikemccand 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documents are not returned after disk full</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16249</link><project id="" key="" /><description>Hi,
      I am seeing this issue where query doesn't return any documents after disk full and cluster state is red. This is a single node cluster. Is this expected when disk full condition is reached ?

Here is the relevant info. 

[admin@CentOS7 log]$ curl '172.31.193.192:9200/_cat/indices?v'
health status index  pri rep docs.count docs.deleted store.size pri.store.size
red    open   monkey   5   1

[admin@CentOS7 log]$ curl '172.31.193.192:9200/_cat/shards?v'
index  shard prirep state        docs store ip             node
monkey 3     p      INITIALIZING            172.31.193.192 Mad Jack
monkey 3     r      UNASSIGNED
monkey 4     p      UNASSIGNED
monkey 4     r      UNASSIGNED
monkey 1     p      UNASSIGNED
monkey 1     r      UNASSIGNED
monkey 2     p      UNASSIGNED
monkey 2     r      UNASSIGNED
monkey 0     p      UNASSIGNED
monkey 0     r      UNASSIGNED

 Please find attached  ES log file as well.

I see this in the log file.

[2016-01-25 19:54:56,312][INFO ][cluster.routing.allocation.decider] [Mad Jack] rerouting shards: [high disk watermark exceeded on one or more nodes]
[2016-01-25 19:55:01,507][ERROR][index.engine             ] [Mad Jack] [monkey][0] failed to merge
java.io.IOException: No space left on device

[elasticsearch.log-2016-01-25.txt](https://github.com/elastic/elasticsearch/files/105755/elasticsearch.log-2016-01-25.txt)

[admin@CentOS7 log]$ curl '172.31.193.192:9200'
{
  "name" : "Mad Jack",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.1.1",
    "build_hash" : "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
    "build_timestamp" : "2015-12-15T13:05:55Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  },
  "tagline" : "You Know, for Search"
}

Let me know if any other info is needed.

Thanks,
Ravindra
</description><key id="128958066">16249</key><summary>Documents are not returned after disk full</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rathir</reporter><labels /><created>2016-01-26T22:09:59Z</created><updated>2016-01-27T11:43:53Z</updated><resolved>2016-01-27T11:43:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-27T11:43:53Z" id="175575953">Hi @rathir 

If your disk is full and you try to write more documents, writing will fail, which will fail the shards, so yes, you will get no search results until you find more disk space and your shards can recover.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The IngestDocument copy constructor should make a deep copy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16248</link><project id="" key="" /><description>PR for #16246
</description><key id="128951116">16248</key><summary>The IngestDocument copy constructor should make a deep copy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T21:39:03Z</created><updated>2016-02-01T09:46:29Z</updated><resolved>2016-01-26T21:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-01-26T21:43:37Z" id="175245948">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations don't work if a type has the same name as a field in an other type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16247</link><project id="" key="" /><description>Hi,

Here is a bug that is always reproducible on Elasticsearch 1.7.4 (no problem using 2.1.1). It concerns aggregations that return no results (while it should) when a type has the same name as a field name of an other type.

Here is a way to systematically reproduce the issue:

```
# If you just change "state" with "state2" in the second type, everything will work fine
curl -XPUT 'http://localhost:9200/agg_analysis' -d '
{
  "mappings": {
    "data": {
      "properties": {
        "state" : {
          "type": "string",
          "fields": {
            "raw" : {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    },
    "state": {
      "properties": {
        "name" : {
          "type": "string",
          "fields": {
            "raw" : {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}
'
```

```
curl -XPOST 'http://localhost:9200/agg_analysis/data/_bulk' -d '
{ "index": {}}
{ "state" : "New York" }
{ "index": {}}
{ "state" : "New Mexico" }
{ "index": {}}
{ "state" : "New York" }
'
```

```
curl -XGET 'http://localhost:9200/agg_analysis/data/_search?pretty' -d '
{
  "size" : 0,
  "aggs" : {
    "states" : {
        "terms" : {
            "field" : "state.raw"
        }
    }
  }
}
'
```

This should return a list of 2 states, but returns none. Note that `state` is:
- The name of a field in the `data` type.
- And also the name of an other type.

Changing the type name from `state` to `state2` in the mapping will make the aggregation on the `data` type work which is really weird.

To check this out, you can run all the steps again by replacing from `state` with `state2` in the mapping step.
</description><key id="128944919">16247</key><summary>Aggregations don't work if a type has the same name as a field in an other type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michaelperrin</reporter><labels /><created>2016-01-26T21:17:53Z</created><updated>2016-01-27T09:26:17Z</updated><resolved>2016-01-27T07:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-27T07:01:12Z" id="175450995">This was fixed with 2.0, in #8872, and was part of a very large set of breaking changes to the mappings api.
</comment><comment author="michaelperrin" created="2016-01-27T08:17:59Z" id="175475652">Thanks @rjernst for your answer!

As I am stuck with Elasticsearch 1.7.4 for now (as the plugin (aka. bundle) for Symfony is not entirely ready yet for 2.x), I will use the full path with the type prepended in my aggregation (in my example: `data.state.raw`), even if it is incompatible with ES 2.x . I will change it back to `data.state` when I upgrade to ES 2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>node ingest - simulate - uppercase processor affecting output of previous processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16246</link><project id="" key="" /><description>Processors appear to be firing out of sequence. 

In example 1, when the geoip processor is the only processor in the pipeline, the resulting object has a continent_name property with the value of  "North America". Note that is it not all uppercase.

In example 2, the same geoip processor and input doc is used, but there is an additional uppercase processor after the geoip processor. Note that the output of BOTH processors in the pipeline have a continent_name property that is all uppercase.
## Example 1
### Request Body

```
{
  "pipeline": {
    "processors": [
      {
        "geoip": {
          "processor_id": "processor_1",
          "source_field": "_raw",
          "target_field": "geoip"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "_raw": "64.242.88.10"
      }
    }
  ]
}
```
### Response Body

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "doc": {
            "_type": "_type",
            "_routing": null,
            "_ttl": null,
            "_index": "_index",
            "_timestamp": null,
            "_parent": null,
            "_id": "_id",
            "_source": {
              "geoip": {
                "continent_name": "North America",
                "city_name": "Chesterfield",
                "country_iso_code": "US",
                "region_name": "Missouri",
                "location": [
                  -90.5771,
                  38.6631
                ]
              },
              "_raw": "64.242.88.10"
            },
            "_ingest": {
              "timestamp": "2016-01-26T20:41:59.806+0000"
            }
          }
        }
      ]
    }
  ]
}
```
## Example 2
### Request Body

```
{
  "pipeline": {
    "processors": [
      {
        "geoip": {
          "processor_id": "processor_1",
          "source_field": "_raw",
          "target_field": "geoip"
        }
      },
      {
        "uppercase": {
          "processor_id": "processor_2",
          "field": "geoip.continent_name"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "_raw": "64.242.88.10"
      }
    }
  ]
}
```
### Response Body

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "doc": {
            "_type": "_type",
            "_routing": null,
            "_ttl": null,
            "_index": "_index",
            "_timestamp": null,
            "_parent": null,
            "_id": "_id",
            "_source": {
              "geoip": {
                "continent_name": "NORTH AMERICA",
                "city_name": "Chesterfield",
                "country_iso_code": "US",
                "region_name": "Missouri",
                "location": [
                  -90.5771,
                  38.6631
                ]
              },
              "_raw": "64.242.88.10"
            },
            "_ingest": {
              "timestamp": "2016-01-26T20:39:10.407+0000"
            }
          }
        },
        {
          "processor_id": "processor_2",
          "doc": {
            "_type": "_type",
            "_routing": null,
            "_ttl": null,
            "_index": "_index",
            "_timestamp": null,
            "_parent": null,
            "_id": "_id",
            "_source": {
              "geoip": {
                "continent_name": "NORTH AMERICA",
                "city_name": "Chesterfield",
                "country_iso_code": "US",
                "region_name": "Missouri",
                "location": [
                  -90.5771,
                  38.6631
                ]
              },
              "_raw": "64.242.88.10"
            },
            "_ingest": {
              "timestamp": "2016-01-26T20:39:10.407+0000"
            }
          }
        }
      ]
    }
  ]
}
```
</description><key id="128937571">16246</key><summary>node ingest - simulate - uppercase processor affecting output of previous processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BigFunger</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-26T20:47:00Z</created><updated>2016-01-26T21:50:38Z</updated><resolved>2016-01-26T21:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BigFunger" created="2016-01-26T20:50:11Z" id="175223628">Seems to be a bigger issue. The same effect happens with the lowercase, and split processors. Is it related to the fact that I am operating on a nested object? ie: geoip.continent_name

Am I doing this incorrectly?
</comment><comment author="talevy" created="2016-01-26T20:56:34Z" id="175226084">I believe the copy constructor for `IngestDocument` does not properly do a deep copy of the underlying source Map.
</comment><comment author="martijnvg" created="2016-01-26T20:56:39Z" id="175226126">@BigFunger Usage is correct, there seems to be something wrong with the verbose version of the simulate api.
</comment><comment author="martijnvg" created="2016-01-26T21:50:38Z" id="175248686">@BigFunger Bug has been fixed in the master branch. Make sure when you're going to work from master, that you need to change  `processor_id` to `tag`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename Plan A to Painless</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16245</link><project id="" key="" /><description>Not actually a huge change, just renamed the Plan A scripting language back to Painless by popular request :)
</description><key id="128922591">16245</key><summary>Rename Plan A to Painless</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T19:47:51Z</created><updated>2016-02-01T21:04:02Z</updated><resolved>2016-01-27T18:38:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-27T14:33:32Z" id="175657695">LGTM
</comment><comment author="nik9000" created="2016-01-27T14:33:58Z" id="175657922">Sorry about all the conflicts. Renames are the most conflict-y things....
</comment><comment author="jdconrad" created="2016-01-27T18:38:53Z" id="175786500">Thanks for the review @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add dedicated client nodes as an option for TransportClient sniffing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16244</link><project id="" key="" /><description>Currently TransportClient's sniffing feature [only adds data nodes](https://github.com/elastic/elasticsearch/blob/2.1/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L483-L485) to the list of available hosts.  For clusters that have dedicated client nodes, it will be nice for the sniffing feature to be able to honor client nodes so that the reduce phase can occur on the dedicated client node machines.  
</description><key id="128920662">16244</key><summary>Add dedicated client nodes as an option for TransportClient sniffing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-26T19:38:53Z</created><updated>2016-01-29T10:19:58Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2016-01-27T04:00:53Z" id="175378706">pelase also consider gateway node
</comment><comment author="javanna" created="2016-01-27T08:51:00Z" id="175489493">would you add client nodes and data nodes, or client nodes only if there's at least a certain number of them (how many?) ? Or did you mean to add an option to the transport client to control this? I think we should do here what the language clients do, not sure if they add client nodes to their internal list. @clintongormley can comment on this.
</comment><comment author="clintongormley" created="2016-01-27T12:09:10Z" id="175587892">The lang clients are inconsistent here.  Really you want a number of options available, eg:
- client nodes
- data nodes
- tribe nodes
- nodes with a particular tag (or tags) 

and you probably want to prefer eg client nodes but allow fallback to eg data nodes...
</comment><comment author="clintongormley" created="2016-01-29T10:18:28Z" id="176680893">Discussed in FiF - we should add an overridable method which allows the user to specify whether a sniffed node should be used or not.  Currently we only sniff data notes, but the predicate should allow client nodes to be accepted too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove version in ShardRouting (now obsolete)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16243</link><project id="" key="" /><description>With the changes in #14739, allocation ids are now being used to select primary shards instead of version information stored with the shard state on disk. As this was the only use case so far to carry version information in shard routing, we can safely remove it.
</description><key id="128881686">16243</key><summary>Remove version in ShardRouting (now obsolete)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T17:09:37Z</created><updated>2016-02-08T09:52:49Z</updated><resolved>2016-02-04T14:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-27T09:23:50Z" id="175506216">This is great. Left some minor comments here and there...
</comment><comment author="ywelsch" created="2016-01-27T14:53:26Z" id="175666076">@bleskes pushed a new set of changes
</comment><comment author="ywelsch" created="2016-02-02T15:00:19Z" id="178618071">@bleskes I rebased on current master (there were lots of conflicts due to the changes in Index class)
</comment><comment author="bleskes" created="2016-02-03T09:53:00Z" id="179134520">Thanks @ywelsch . Left some extremely minor comments. Feel free to address them and push when ready (or ping me if you feel another cycle is needed).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest: move get/put/delete pipeline methods to ClusterAdminClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16242</link><project id="" key="" /><description> These methods allow to modify and retrieve the content of pipelines, which are stored in the cluster state. Their actions names were already correct under the category `cluster:admin/ingest/pipeline/*` , the corresponding methods should be moved under `client.admin().cluster()` .
</description><key id="128881108">16242</key><summary>Ingest: move get/put/delete pipeline methods to ClusterAdminClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T17:07:28Z</created><updated>2016-01-27T13:04:14Z</updated><resolved>2016-01-27T13:04:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-26T17:09:46Z" id="175121406">What remains to be discussed here is what to do with the simulate api. It allows to simulate pipelines, which would mean only retrieving existing ones and executing them. The index api and bulk api allow as well to retrieve pipelines and execute them before indexing. Seems consistent to keep simulate in the `Client` , but its action name is `cluster:admin/ingest/pipeline/simulate` which suggests it is an admin action. That said I don't see any other existing action category that would suit it better. @jaymode what do you think?
</comment><comment author="javanna" created="2016-01-27T12:30:24Z" id="175593090">We have discussed this internally and decided to move simulate pipeline to ClusterAdminClient too. This is ready for review then, @martijnvg can you please have a look?
</comment><comment author="martijnvg" created="2016-01-27T13:03:58Z" id="175609327">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initial refactoring for phrase suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16241</link><project id="" key="" /><description>Adding initial serialization methods (readFrom, writeTo) to the PhraseSuggestionBuilder, also adding the base test framework for serialiazation testing, equals and hashCode. Moving SuggestionBuilder out of the global SuggestBuilder for better readability.

Still missing: Handling of Smoothing Model (depends on https://github.com/elastic/elasticsearch/pull/16130) and DirectCandidateGenerator (depends on https://github.com/elastic/elasticsearch/pull/16185). Also Test needs to be extended to handle all parameters.
</description><key id="128877326">16241</key><summary>Initial refactoring for phrase suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2016-01-26T16:54:28Z</created><updated>2016-02-22T21:51:24Z</updated><resolved>2016-01-27T17:22:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-01-27T13:56:42Z" id="175638610">Left a few comments. Feel free to ignore the ones about methods being empty. I didn't realize this wasn't going against master.
</comment><comment author="cbuescher" created="2016-01-27T15:24:59Z" id="175687758">@areek @abeyad thanks for the review, I hope I adressed all of your comments, any objections about merging this then?
</comment><comment author="abeyad" created="2016-01-27T15:36:20Z" id="175692161">@cbuescher LGTM ++
</comment><comment author="areek" created="2016-01-27T15:45:09Z" id="175695488">@cbuescher LGTM. This looks great and thanks for the test infra :)
</comment><comment author="cbuescher" created="2016-01-27T17:22:48Z" id="175755268">Didn't merge this with master but with feature-suggest-refactoring branch in c00c0fa0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add convenience method to RoutingNodes for shard by allocation ID </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16240</link><project id="" key="" /><description>Code like

```
            RoutingNodes.RoutingNodeIterator routingNodeIterator =
                currentState.getRoutingNodes().routingNodeIter(task.getShardRouting().currentNodeId());
            if (routingNodeIterator != null) {
                for (ShardRouting maybe : routingNodeIterator) {
                    if (task.getShardRouting().isSameAllocation(maybe)) {
                        return true;
                    }
                }
            }
            return false;
```

for finding whether or not a given shard matches a shard in the routing table is clunky. We should just get this in a convenience method on the routing table. Via a [comment](https://github.com/elastic/elasticsearch/pull/16089#discussion_r50836646) from @bleskes.
</description><key id="128868484">16240</key><summary>Add convenience method to RoutingNodes for shard by allocation ID </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>adoptme</label><label>non-issue</label></labels><created>2016-01-26T16:19:21Z</created><updated>2017-01-12T10:45:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PUT mapping with with search_analyzer and `update_all_types` does not actually update all types </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16239</link><project id="" key="" /><description>Or `toXContent` is broken.

Reproduce:

```
DELETE test_index

PUT test_index
{
  "mappings": {
    "type1": {
      "properties": {
        "field": {
          "type": "string"
        }
      }
    },
    "type2": {
      "properties": {
        "field": {
          "type": "string"
        }
      }
    }
  }
}

GET test_index/_mapping/field/field?include_defaults

PUT test_index/type1/_mapping?update_all_types
{
  "properties": {
    "field": {
      "type": "string",
      "search_analyzer": "keyword",
      "analyzer": "default"
    }
  }
}

```

In the mapping I retrieve with the GET _mapping API in the mapping only type1 is updated, although with the GET field mapping API I get the correct mapping.

```
GET test_index/_mapping

{
  "test_index": {
    "mappings": {
      "type1": {
        "properties": {
          "field": {
            "type": "string",
            "analyzer": "default",
            "search_analyzer": "keyword"
          }
        }
      },
      "type2": {
        "properties": {
          "field": {
            "type": "string"
          }
        }
      }
    }
  }
}

GET test_index/_mapping/field/field


{
  "test_index": {
    "mappings": {
      "type2": {
        "field": {
          "full_name": "field",
          "mapping": {
            "field": {
              "type": "string",
              "analyzer": "default",
              "search_analyzer": "keyword"
            }
          }
        }
      },
      "type1": {
        "field": {
          "full_name": "field",
          "mapping": {
            "field": {
              "type": "string",
              "analyzer": "default",
              "search_analyzer": "keyword"
            }
          }
        }
      }
    }
  }
}

```

When I restart the node the change has vanished from the field mapping result but still is in the get mapping result.
</description><key id="128867306">16239</key><summary>PUT mapping with with search_analyzer and `update_all_types` does not actually update all types </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-01-26T16:14:49Z</created><updated>2016-02-10T09:41:02Z</updated><resolved>2016-02-10T09:41:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-01-26T16:16:17Z" id="175097066">tested only 2.1.1
</comment><comment author="rjernst" created="2016-01-27T06:14:49Z" id="175423563">I tried on 2.2 and see the same behavior. The get mapping api is implemented by looking at the cluster state from the master, and doing a manual inspection of the cluster state shows the same inconsistency.

I have a hunch this is a problem in `MetaDataMappingService.PutMappingExecutor`, where I believe it should now simply be regenerating the entire mappings for the given index instead of the current complicated merging into the existing `MappingMetaData` of the index based on the type in the request. /cc @jpountz 
</comment><comment author="jpountz" created="2016-01-27T13:41:24Z" id="175632722">Good catch @rjernst, this was indeed one of the causes of the problem. I opened #16264 for it.

But there was also a serialization bug as Britta initially thought, which would be fixed by #16255.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert `cluster.routing.allocation.type` and `processors` to the new settings infra.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16238</link><project id="" key="" /><description>this also removes some unused / unsupported settings from the tests.
</description><key id="128865793">16238</key><summary>Convert `cluster.routing.allocation.type` and `processors` to the new settings infra.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T16:09:03Z</created><updated>2016-01-27T10:38:01Z</updated><resolved>2016-01-27T10:38:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-26T16:18:36Z" id="175098543">LGTM
</comment><comment author="javanna" created="2016-01-26T16:20:48Z" id="175099252">left a small comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate tribe node settings on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16237</link><project id="" key="" /><description /><key id="128851887">16237</key><summary>Validate tribe node settings on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T15:20:15Z</created><updated>2016-01-27T10:11:43Z</updated><resolved>2016-01-27T10:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-27T05:27:54Z" id="175406645">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better wording</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16236</link><project id="" key="" /><description /><key id="128851163">16236</key><summary>Better wording</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dlangille</reporter><labels><label>docs</label></labels><created>2016-01-26T15:18:18Z</created><updated>2016-01-26T16:55:39Z</updated><resolved>2016-01-26T16:54:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T16:55:39Z" id="175114083">thanks @dlangille 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighter: Move writing of top-level highlight field to SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16235</link><project id="" key="" /><description>Most elements in SearchSourceBuilder (e.g. aggs, queries) write their top-level ParseField name in toXContent(), while HighlightBuilder used to do it in its own toXContent() method. Moved this up so SeachSourceBuilder for consistency.
</description><key id="128850240">16235</key><summary>Highlighter: Move writing of top-level highlight field to SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T15:15:34Z</created><updated>2016-03-10T18:57:29Z</updated><resolved>2016-02-22T19:03:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-26T15:17:15Z" id="175070435">Stumbled upon this while going through the SearchSourceBuilder, I'd be interested to learn if there are any reasons for writing the top-level field name in the Highlighter itself so far, so just opening this PR as a question.
</comment><comment author="nik9000" created="2016-01-27T14:51:56Z" id="175665553">I don't know why it did that. Those reasons might be lost to the depths of time. I say if `git blame` doesn't give you any hints on another person to ask then just merge this to make it consistent.
</comment><comment author="cbuescher" created="2016-01-27T15:04:29Z" id="175672398">@nik9000 I was just wondering if highlighter#toXContent was used anywhere else besides the SearchSourceBuilder, couldn't find anything really.
</comment><comment author="nik9000" created="2016-02-10T14:31:21Z" id="182398165">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate node.mode in favour of node.local</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16234</link><project id="" key="" /><description>The `node.mode` setting supports two values: `local` or `network`and was introduced by #3713. The `node.local` setting is a boolean. We read both as part of `DiscoveryNode#localNode` static method which gives the precedence to `node.local`, then reads `node.mode`, if none of the two is set it returns `false`.

I am not sure why we have `node.mode` anymore, I think these two settings have the same effect and result in controlling which transport impl (local or netty) and which discovery impl (local or zen) is used. Unless I am overlooking something, we can deprecate one of the two. The boolean one seems the simpler the keep, what do people think about this?
</description><key id="128842778">16234</key><summary>Deprecate node.mode in favour of node.local</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>adoptme</label><label>deprecation</label></labels><created>2016-01-26T14:49:44Z</created><updated>2016-07-14T11:21:25Z</updated><resolved>2016-07-14T11:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-26T18:34:00Z" id="175163044">+1 on just have node.local . I think long term we want this to be a test only setting anyway. Maybe we can get rid of it and just set both network and discovery type from the test framework as we want them&#8230;

&gt; On 26 Jan 2016, at 15:50, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; The node.mode setting supports two values: local or networkand was introduced by #3713. The node.local setting is a boolean. We read both as part of DiscoveryNode#localNode static method which gives the precedence to node.local, then reads node.mode, if none of the two is set it returns false.
&gt; 
&gt; I am not sure why we have node.mode anymore, I think these two settings have the same effect and result in controlling which transport impl (local or netty) and which discovery impl (local or zen) is used. Unless I am overlooking something, we can deprecate one of the two. The boolean one seems the simpler the keep, what do people think about this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2016-01-27T11:36:05Z" id="175573104">+1 @javanna 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java mget is much slower than http mget when in high concurrency environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16233</link><project id="" key="" /><description>Hi, when i test the performance of mget method with HP Loadrunning, i found that java's mget method is much slower than http's mget method, java's mget method is almost 5 times slower than http's mget method.

here is my test method.

``` java
MultiGetResponse multiGetResponse = client.prepareMultiGet()
                .add(index, type, ids)
                .execute()
                .actionGet();
```

curl -XGET 'http://127.0.0.1:9200/test/test/_mget' -d '{
    "ids" : ["1", "2", "3", ...]
}'

But the weird thing is that i send a single request respectively, their average time spent is the same.
</description><key id="128830821">16233</key><summary>java mget is much slower than http mget when in high concurrency environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henghanan</reporter><labels /><created>2016-01-26T14:11:23Z</created><updated>2016-01-26T14:23:57Z</updated><resolved>2016-01-26T14:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T14:23:57Z" id="175039979">Hi @henghanan 

They use the same mechanism internally so I can only assume that you're doing something incorrect (and we'd need more information to figure out what that is).  The right place to ask questions like this is in the forum: http://discuss.elastic.co/

If, after discussing there, you find a bug then please reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Version#id to parse version from settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16232</link><project id="" key="" /><description>When a Version is passed to `Settings#put(String, Version)` it's id is used as
an integer which should also be used to deserialize it on the consumer end.
Today AssertinLocalTransport expects Version#toString() to be used which can lead
to subtile bugs in tests.
</description><key id="128830278">16232</key><summary>Use Version#id to parse version from settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T14:09:22Z</created><updated>2016-01-27T12:13:12Z</updated><resolved>2016-01-27T10:10:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-26T14:09:33Z" id="175032659">@bleskes  FYI
</comment><comment author="bleskes" created="2016-01-26T18:51:59Z" id="175175203">That's a pita. I think we should use strings in our settings tbh - will be more consistent with all the rest and those integers are failure opaque. That said I don't mind much. Just to double check - there wasn't a bug caused by this to date right? 
</comment><comment author="rjernst" created="2016-01-27T05:16:57Z" id="175402631">There was a test bug:
https://build-us-01.elastic.co/job/elastic+elasticsearch+master+periodic/160/console
</comment><comment author="bleskes" created="2016-01-27T07:59:14Z" id="175470682">Thanks Ryan. To be clear - the fix is good and consistent with what we do right now with other  version setting. I just don't see where it went wrong... the only usage of those setting used the string variant as well. I feel I miss something. 
</comment><comment author="s1monw" created="2016-01-27T10:10:25Z" id="175525893">@bleskes ryan fixed the problem last week here https://github.com/elastic/elasticsearch/commit/b349746bbe6767b527d9410f3d6faa0313b377f4 it didn't use `toString()` in order to reduce the trap I am making it consistent. This commit introduced it ec31feca93fdfad9c8b4ef7a757b408ecf18d274
</comment><comment author="bleskes" created="2016-01-27T12:13:12Z" id="175588757">Thx @s1monw . That was the missing piece. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Align the regex implementation in SignificantTermsBuilder and TermsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16231</link><project id="" key="" /><description>In TermsBuilder we switched from `java.util.Regex` to `org.apache.lucene.util.automaton.RegExp` but `SignificantTermsBuilder` still uses `java.util.Regex`. We should align both implementations and use `org.apache.lucene.util.automaton.RegExp` consistently.
</description><key id="128827786">16231</key><summary>Align the regex implementation in SignificantTermsBuilder and TermsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-26T13:59:07Z</created><updated>2016-09-08T10:37:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-26T15:10:12Z" id="175066927">This should be done once the agg refactoring has been merged. Marking as stalled for now until that is complete
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move node.client, node.data, node.master, node.local and node.mode to new settings infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16230</link><project id="" key="" /><description>I did this while working on ingest node as we added node.ingest which uses the new settings infrastructure.

I am wondering if we still need both node.mode and node.local which seem to do the same thing.

Also, there is a breaking change as we now parse all these boolean values in a strict manner, but I thought we may want to add a general note to the 2.0 migration guide, to mention that we do this for every boolean setting, let's discuss that.
</description><key id="128827668">16230</key><summary>Move node.client, node.data, node.master, node.local and node.mode to new settings infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T13:58:40Z</created><updated>2016-01-26T17:16:56Z</updated><resolved>2016-01-26T17:16:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-26T14:04:51Z" id="175029998">LGTM - regarding the break, we will soon fail hard with an exception so I think we are ok here?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Remove the fact that ingest was a plugin from the docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16229</link><project id="" key="" /><description /><key id="128826342">16229</key><summary>[Ingest] Remove the fact that ingest was a plugin from the docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T13:53:31Z</created><updated>2016-02-08T09:26:01Z</updated><resolved>2016-01-26T14:50:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-26T13:57:37Z" id="175027890">thanks @martijnvg left a few minor comments, LGTM otherwise
</comment><comment author="javanna" created="2016-01-26T14:20:52Z" id="175038213">++ thanks @martijnvg 
</comment><comment author="talevy" created="2016-01-26T15:13:20Z" id="175068220">thanks! missed that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add regex flags to TermsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16228</link><project id="" key="" /><description>Note: This ticket originates from https://discuss.elastic.co/t/includeflags-in-termbuilders-for-elasticsearch-2-0/39923

In https://github.com/elastic/elasticsearch/commit/aecd9ac51527fa6ac57788c23527ec4e0dc32636 we switched from `java.util.Regex` to `org.apache.lucene.util.automaton.RegExp` in `TermsBuilder` but don't expose the regex flags. The related [Regexp query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html) exposes these flags, so we should also add them to `TermsBuilder` for consistency.
</description><key id="128820963">16228</key><summary>Add regex flags to TermsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-26T13:34:56Z</created><updated>2016-09-08T10:37:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-26T15:10:19Z" id="175066953">This should be done once the agg refactoring has been merged. Marking as stalled for now until that is complete
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove string interning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16227</link><project id="" key="" /><description>Keep it simple and just use normal strings

Relates #16217 
</description><key id="128816605">16227</key><summary>remove string interning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>non-issue</label></labels><created>2016-01-26T13:14:17Z</created><updated>2016-07-12T08:40:54Z</updated><resolved>2016-07-11T19:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-26T13:25:23Z" id="175009487">Thanks! I am tempted to make string interning a forbidden API.
</comment><comment author="s1monw" created="2016-01-26T13:27:56Z" id="175010444">+1 to forbid
</comment><comment author="s1monw" created="2016-01-26T13:32:44Z" id="175013108">make is a release highlight :)
</comment><comment author="jasontedor" created="2016-01-26T13:42:32Z" id="175018923">Why? My intuition is that we should keep this, but at least do a study before making the change.
</comment><comment author="s1monw" created="2016-01-26T13:47:51Z" id="175021807">&gt; Why? My intuition is that we should keep this, but at least do a study before making the change.

I disagree there is no reason for this to be there.
</comment><comment author="s1monw" created="2016-01-26T15:32:08Z" id="175079587">I spoke to jason about this face to face and explained my bad experience with `intern()` on lucene. Yet, we have a bunch of more intern calls here which I personally think we should get rid of as well but from @jasontedor experience it might be wise to visit them case by case so I think this is a bigger issue and we should not block #16217
</comment><comment author="bleskes" created="2016-01-26T15:57:49Z" id="175089768">OK. I went ahead and added string intern to the forbidden API. Also removed all usages in ES. @jasontedor  seems you had some concerns around this - can you check as well?
</comment><comment author="s1monw" created="2016-01-26T16:07:57Z" id="175093749">@bleskes lets discuss this internally first. I think there are some caveats to this
</comment><comment author="dakrone" created="2016-07-11T16:59:01Z" id="231796480">@bleskes how did this turn out? Can we close this PR?
</comment><comment author="bleskes" created="2016-07-11T19:46:01Z" id="231843306">yeah, simon asked me to wait with it post 5.0 and it was controversial how useful it is anyway. I'll close and reopen if it's relevant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"indices.filter_cache" cluster statistic not present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16226</link><project id="" key="" /><description>I'm using an external monitoring system which collects metrics from Elasticsearch and tries to collect the `indices.filter_cache` values as shown in https://www.elastic.co/guide/en/elasticsearch/reference/2.1/cluster-stats.html:

```
{
   "indices": {
...
      "filter_cache": {
         "memory_size": "0b",
         "memory_size_in_bytes": 0,
         "evictions": 0
      },
...
```

For some reason, I don't have these values when I try to query the different clusters I have (running Elasticsearch 2.1 with `cloud-aws` as the only configured plugin). Grepping the code doesn't shown any relevant information about this value, so I wonder where the problem is?
Is this a value which is returned by a plugin (which I don't have enabled)? Is this an obsolete value? Is this activated only under some specific circumstances?
</description><key id="128800145">16226</key><summary>"indices.filter_cache" cluster statistic not present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">multani</reporter><labels /><created>2016-01-26T12:00:20Z</created><updated>2016-01-26T12:50:24Z</updated><resolved>2016-01-26T12:49:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T12:49:30Z" id="174997429">The filter cache was renamed to the query cache in 2.0.0 - I've updated the docs to reflect that
</comment><comment author="clintongormley" created="2016-01-26T12:50:24Z" id="174997829">Closed in 4e5316591a95b28528335e1905558bfa1c0c4c6a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move missing() from SortBuilder interface to class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16225</link><project id="" key="" /><description>As mentioned by @cbuescher on #16151 this method is really implemented only in
the FieldSortBuilder. Moving the method down.

Relates to #15178
</description><key id="128799003">16225</key><summary>Move missing() from SortBuilder interface to class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T11:52:44Z</created><updated>2016-03-09T09:27:27Z</updated><resolved>2016-03-09T09:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-01-26T12:01:54Z" id="174969818">@jpountz @cbuescher suggested the small change, I made it, would be nice to have someone independent give the final LGTM. Maybe you can help here?
</comment><comment author="cbuescher" created="2016-03-08T12:22:36Z" id="193763107">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch get stuck while indexing a few record</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16224</link><project id="" key="" /><description>I have an embedded instance of an `elasticsearch` node (no tcp) used to provide a fast indexed search engine on a big and complex relational database.

The index is kept aligned with the db by a background thread (this is a very common usage pattern for ES).

The problem is that after a while we start seeing exception like the following, and we have to stop and restart the server to make it work again:

```
Caused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1@1133ea1 on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1ff3ea[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 5]]]
at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.routeRequestOrPerformLocally(TransportReplicationAction.java:444)
at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:386)
at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:120)
at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:135)
at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:119)
at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:66)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
```

No other exception are visibile in the logs except this one. Seams like something il locked inside ES and when it reaches the fatal number of 200 stuck "things" it will stop working.

Elastic search is version : 2.1.1
</description><key id="128798975">16224</key><summary>Elasticsearch get stuck while indexing a few record</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dam0vm3nt</reporter><labels /><created>2016-01-26T11:52:34Z</created><updated>2016-03-01T08:57:12Z</updated><resolved>2016-01-26T12:13:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-26T12:13:47Z" id="174976727">This is the index thread pool inside Elasticsearch rejecting requests because its task queue is full. The `EsRejectedExecutionException` is a way of communicating to your client thread that it needs to back off. It sounds like your client thread is not properly handling these exceptions.
1. You need to catch and handle requests that fail from an `EsRejectedExecutionException`; this means that you need to back off, and retry your request later (this is true whether you're asynchronously or synchronously firing off indexing requests).
2. Investigate increasing the size of the index queue, but only within reason. Increasing queue size is not a panacea for constantly-full queues. If you're shoving data into Elasticsearch faster than your hardware can handle, increasing the queue size will not solve your problem because that new queue size will just fill up. Stuffed queues are almost always a sign of _another_ problem.
3. Are you sure that you have sufficient hardware here? Are your disks and CPU fast enough? Are the database and the embedded Elasticsearch on the same box competing for resources?

&gt; Seams like something il locked inside ES and when it reaches the fatal number of 200 stuck "things" it will stop working.

I'm interpreting this statement as you saying that Elasticsearch locks up (i.e., no longer responding to requests) when this happens. This shouldn't be the case, but if you do in fact have evidence that Elasticsearch is locking up here (thread dumps would be great!), that would be helpful.
</comment><comment author="dam0vm3nt" created="2016-01-26T12:49:48Z" id="174997541">Hello, thank you for the response.

We have recreated the index with `number_of_replicas=1` and it seams to work on a test machine. Can it be that using the default value of 2 replicas with 1 single node can cause timeout happening on the indexing phase making the queue grow until it explodes ?

Other than that none of the points you enilisted seams to apply :
1. As I have said no exception are thrown while indexing until that one indicating the queue is full. Is there something else we can do to check if something fails ?
2.  I agree that resizing queue is not the answer because eventually it will fill up no matter the size
3. We tryied even indexing 1 record every 5 secs and it will explode. The hardware is not a problem (4 - core i7 + 8 gb ram)
</comment><comment author="dam0vm3nt" created="2016-01-26T12:55:53Z" id="174999099">As for point 1: after reading twice I understood what you mean, sorry. We can back off but this problem seams to happen way to often and with very little loads so there should be something else.

Can it be the `number_of_repilcas` value as written in my previous answer ?
</comment><comment author="jasontedor" created="2016-01-26T13:03:22Z" id="175001320">&gt; We have recreated the index with number_of_replicas=1 and it seams to work on a test machine. 

If you have a single embedded node, you should have `number_of_replicas` set to zero. Positive replicas are not helpful when you only have a single node because there is no home for the replica.

&gt; Can it be that using the default value of 2 replicas

The default is one replica. To be clear, the default is one replica meaning that every shard will by default have a primary copy and a replica copy. However, in the case of a single embedded node, there is no home for the replica copy so just set it to zero (if you need backups of your indices, take a snapshot using the snapshot API).

&gt; with 1 single node can cause timeout happening on the indexing phase making the queue grow until it explodes ?

No, the replica will just be unassigned and have no impact on the indexing.

&gt; Is there something else we can do to check if something fails ?

You have to check the index response (if you're doing synchronous calls) or handle the failure callback (if you're doing asynchronous calls).

&gt; We tryied even indexing 1 record every 5 secs and it will explode. 

Can you provide more details? How are you doing the indexing? What is the size of the request? 

&gt; The hardware is not a problem (4 - core i7 + 8 gb ram)

What is the disk situation, and is it sharing the same hardware with the database? How much heap have you allocated to the Elasticsearch process?
</comment><comment author="jasontedor" created="2016-01-26T13:06:15Z" id="175002372">&gt; Can it be the number_of_repilcas value as written in my previous answer ?

Nope. There is something else going on.
</comment><comment author="dam0vm3nt" created="2016-01-26T13:26:37Z" id="175009975">ouch! I was so hopefull! 

Ok then we will set `number_of_replicas` to 0 according to your suggestion (obviously you're right about the default value beiing 1).
At the moment we will implement a back-off policy and see what's happening.

Disks are a bit slow and shared with the db so that may be the cause. 

For now thanks for your help.
</comment><comment author="dam0vm3nt" created="2016-01-26T13:32:41Z" id="175013081">I forgot to mention one important thing : at the moment we have a single thread that will index the data.

It is indexing syncronously one record at a time.

In this situation I expect the indexer task queue should be always almost empty. 

Is this assumption of mine wrong ?
</comment><comment author="jasontedor" created="2016-01-26T14:46:14Z" id="175053907">&gt; Disks are a bit slow and shared with the db so that may be the cause.

That definitely won't help performance, but given your later statements about indexing synchronously one record at at time, it's not the cause of the issue.

&gt; It is indexing syncronously one record at a time.
&gt; 
&gt; In this situation I expect the indexer task queue should be always almost empty.
&gt; 
&gt; Is this assumption of mine wrong ?

Given your stated setup, this assumption is not wrong. Can you share more detail about how exactly you're executing the indexing requests synchronously?
</comment><comment author="dam0vm3nt" created="2016-01-26T15:09:15Z" id="175066530">This is what I am doing in the indexer:

```
 client.prepareIndex("elys", "history", String.valueOf(record.getId()))
            .setSource(objectMapper.writeValueAsBytes(record)).get(new TimeValue(5,TimeUnit.SECONDS));
```

The node is started like this:

```
node = NodeBuilder.nodeBuilder()
    .settings(settings)
    .clusterName(ELYS_ES_CLUSTER)
    .data(true).local(true).node();

    node.start();

    client = node.client();
```
</comment><comment author="jasontedor" created="2016-01-27T19:16:11Z" id="175805079">At a very quick glance, that looks correct, but can you also share the `settings` that you're passing in?
</comment><comment author="dam0vm3nt" created="2016-01-28T10:08:24Z" id="176101074">Of course here they are:

```
     // Start indexer
    Settings.Builder settings = Settings.settingsBuilder();

    settings.put("node.name", ELYS_ES_CLUSTER);

    settings.put("path.data", configuratorBridge.getProperty("es.data.dir"));
    settings.put("path.home", configuratorBridge.getProperty("es.home.dir"));
    settings.put("logger._root","DEBUG");
    settings.put("threadpool.index.queue_size",INDEX_QUEUE_SIZE);

    settings.put("http.enabled", false);
    // Init log if missing
    try {
        java.io.File h = new File(configuratorBridge.getProperty("es.home.dir"));
        java.io.File f = new File(h, "config");
        f = new File(f, "logging.yml");
        if (!f.exists()) {
            f.getParentFile().mkdirs();
            FileWriter fw = new FileWriter(f);
            fw.write("file:\n" +
                    "    type: org.apache.log4j.rolling.RollingFileAppender\n" +
                    "    file: ${path.home}/logs/es.log\n" +
                    "    rollingPolicy: org.apache.log4j.rolling.TimeBasedRollingPolicy\n" +
                    "    rollingPolicy.FileNamePattern: ${path.home}/logs/es.log.%d{yyyy-MM-dd}.gz\n" +
                    "    layout:\n" +
                    "      type: pattern\n" +
                    "      conversionPattern: \"%d{ISO8601}[%-5p][%-25c] %m%n\"");
            fw.close();

            f = new File(h, "logs");
            f.mkdirs();
        }


        LogConfigurator.configure(settings.build(), true);
    } catch (Exception e) {
        e.printStackTrace();
    }
```

Recentlly we have added the `INDEX_QUEUE_SIZE` that's now set to `2000`. This is to stabilize production but it's only a temporary solution.

Another info I have to add is that we had to "osgify" ES to avoid class collition (our framework was already using an older version of lucene). For the same reason we had to `shade` the `guava` library. Having said that, no class loading problem or exception are ever thrown. And apart from this problem everything seams to work.

There are also some machine where indexing works well and that exception is never thrown.

I have a jstack dump of a "locked" jvm if this can help. We're also thinking to enable JMX monitoring to see if we can have other usefull info too. 
</comment><comment author="jasontedor" created="2016-02-25T14:24:28Z" id="188806371">&gt; I have a jstack dump of a "locked" jvm if this can help.

Can you share this?
</comment><comment author="dam0vm3nt" created="2016-03-01T08:57:12Z" id="190619323">In a few day we are going to reproduce the event and see if in the
meanwhile something has changed (we fixed some issues and changed some
parameters). I'll let you know how it has gone.
Thanks,

Il giorno gio 25 feb 2016 alle ore 15:25 Jason Tedor &lt;
notifications@github.com&gt; ha scritto:

&gt; I have a jstack dump of a "locked" jvm if this can help.
&gt; 
&gt; Can you share this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/16224#issuecomment-188806371
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> WIP initial phrase suggester refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16223</link><project id="" key="" /><description>This is a very early WIP that add basic serialization and tests to the PhraseSuggestionBuilder, inteded to initial discussion.
</description><key id="128796455">16223</key><summary> WIP initial phrase suggester refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2016-01-26T11:36:57Z</created><updated>2016-02-22T21:51:43Z</updated><resolved>2016-01-26T16:12:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-26T11:56:16Z" id="174968827">Left two minor comments but LGTM
</comment><comment author="cbuescher" created="2016-01-26T16:12:35Z" id="175095357">Closing and will reopen again vs. new feature branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query and top level inner hit definitions shouldn't overwrite each other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16222</link><project id="" key="" /><description>PR for #16218
</description><key id="128796104">16222</key><summary>Query and top level inner hit definitions shouldn't overwrite each other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-26T11:34:31Z</created><updated>2016-01-29T10:25:17Z</updated><resolved>2016-01-29T09:52:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-28T16:42:13Z" id="176276239">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to filter duplicate field?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16221</link><project id="" key="" /><description>I have created a "test1" index, and then inserted two document to index. This two documents have the same "name" value:

{
  "_index": "test1",
  "_type": "type",
  "_id": "2",
  "_score": 1,
  "_source": {
    "name": "&#30456;&#22768;&#24191;&#25773;"
  }
}

{
  "_index": "test1",
  "_type": "type",
  "_id": "1",
  "_score": 1,
  "_source": {
    "name": "&#30456;&#22768;&#24191;&#25773;"
  }
}

If I want to query the distinct result for the "name" field, how should I to filter duplicate document using query statement?
</description><key id="128770151">16221</key><summary>How to filter duplicate field?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin820224</reporter><labels /><created>2016-01-26T09:28:22Z</created><updated>2016-01-26T09:46:27Z</updated><resolved>2016-01-26T09:46:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-26T09:46:27Z" id="174931471">Please use discuss.elastic.co for questions.

You can't really de duplicate data when searching. I'd try if possible to do that at index time.

But let's follow up on discuss...

Not an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when query if we have boost in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16220</link><project id="" key="" /><description>Hi,
here is the reproduce:

```
curl -XDELETE http://localhost:9200/index

curl -XPUT http://localhost:9200/index

curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'
{
    "fulltext": {
        "properties": {
            "content": {
                "type": "string",
                "boost": 8
            }
        }
    }
}'

curl -XPOST http://localhost:9200/index/fulltext/1 -d'
{"content":"test"}
'

curl -XGET http://localhost:9200/index/_search?q=test


curl -XPOST http://localhost:9200/index/_search -d'
{
  "query": {
    "term": {
      "_all": "test"
    }
  }
}'
```

```
{"took":10,"timed_out":false,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"shard":3,"index":"index","node":"phYI6RmCSLu8VWrVeOSCow","reason":{"type":"null_pointer_exception","reason":null}}]},"hits":{"total":0,"max_score":null,"hits":[]}}
```

here is the error in the console

```
[2016-01-26 15:54:36,104][DEBUG][action.search.type       ] [Rodstvow] [index][3], node[phYI6RmCSLu8VWrVeOSCow], [P], v[2], s[STARTED], a[id=yCs33OxKRmycU_mPfEZ5Mg]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1d24fcc4]
RemoteTransportException[[Rodstvow][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
```

but this works:

```
curl -XGET http://localhost:9200/index/_search\?q\=content:test
```

but if we remove the `"boost": 8` from mapping, the NPE is gone.
</description><key id="128753574">16220</key><summary>NPE when query if we have boost in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2016-01-26T08:16:07Z</created><updated>2016-01-26T11:27:57Z</updated><resolved>2016-01-26T11:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-26T11:27:56Z" id="174963347">This is fixed by #15506. The release with the fix is coming soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis API doesn't respect node level default analyzer setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16219</link><project id="" key="" /><description>Hi,
I tried to set the default analyzer at node level by config the `elasticsearch.yml`, but without success, here is the config i used to test, under elasticsearch version 2.0 and 2.1.*

```
index :
  analysis :
    analyzer :
      default :
        tokenizer : keyword
```

and 

```
index.analysis.analyzer.default.type: keyword
```
</description><key id="128744473">16219</key><summary>Analysis API doesn't respect node level default analyzer setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">medcl</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-26T07:21:02Z</created><updated>2016-04-26T14:17:28Z</updated><resolved>2016-04-26T14:17:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T12:36:33Z" id="174988387">Hi @medcl 

I tested this in 2.1.0 and 2.2.0 and it works as expected.  Try this:

```
PUT t/t/1
{
  "name": "Foo Bar"
}

GET _search
{
  "fielddata_fields": ["name"]
}
```

It also works for the `_all` field.
</comment><comment author="medcl" created="2016-01-26T14:09:35Z" id="175032673">@clintongormley  here is the method what i tried 

```
curl http://localhost:9200/_analyze\?text\=medcl+test1234
{"tokens":[{"token":"medcl","start_offset":0,"end_offset":5,"type":"&lt;ALPHANUM&gt;","position":0},{"token":"test1234","start_offset":6,"end_offset":14,"type":"&lt;ALPHANUM&gt;","position":1}]}
```
</comment><comment author="clintongormley" created="2016-01-26T14:20:28Z" id="175037945">@medcl ah ok - that's a different issue, it's a bug in the analyze API and has never worked..  I'll change the title and reopen
</comment><comment author="johtani" created="2016-01-28T09:35:12Z" id="176083665">Note: The `standard` is specified the request without index.
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java#L234
</comment><comment author="clintongormley" created="2016-01-28T11:17:48Z" id="176128886">@johtani perhaps it should look for that setting first, before defaulting to standard.
</comment><comment author="johtani" created="2016-01-28T13:06:18Z" id="176175219">@clintongormley Agreed
</comment><comment author="johtani" created="2016-04-26T05:58:51Z" id="214623885">Noted: In 5.0, we can't set node level default analyzer. See: https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_settings_changes.html#_index_level_settings
</comment><comment author="clintongormley" created="2016-04-26T14:17:28Z" id="214760066">agreed - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Top level inner hits override query level inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16218</link><project id="" key="" /><description>In the presence of top level inner hits query level inner hits are missing from the result, even if there are no name collisions. This necessitates rewriting query level inner hits as top level inner hits with a duplicate of the main query, which I assume is suboptimal.

ES 2.1.1.
</description><key id="128744418">16218</key><summary>Top level inner hits override query level inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zygfryd</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2016-01-26T07:20:34Z</created><updated>2016-01-29T09:52:47Z</updated><resolved>2016-01-29T09:52:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-26T10:16:41Z" id="174942415">@zygfryd Thanks for reporting this. This is indeed suboptimal and not expected behaviour.

Simple reproduction:

```
PUT index
{
  "mappings": {
    "parent" : {
    },
    "child" : {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT /index/parent/1
{ 
}

PUT /index/child/1?parent=1
{ 
}

GET /index/_search
{
  "query": {
    "has_child": {
      "type": "child",
      "query": {
        "match_all": {}
      },
      "inner_hits" : {}
    }
  },
  "inner_hits" : {
    "my-inner-hits" : {
      "type" : {
        "child" : {

        }
      }
    }
  }
}
```

Two inner hits are expected here, but instead only the top level inner hit is returned.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make index uuid available in Index, ShardRouting &amp; ShardId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16217</link><project id="" key="" /><description>In the early days Elasticsearch used to use the index name as the index identity. Around 1.0.0 we introduced a unique index uuid which is stored in the index setting. Since then we used that uuid in a few places but it is by far not the main identifier when working with indices, partially because it's not always readily available in all places.

This PR start to make a move in the direction of using uuids instead of name by making sure that the uuid is available on the Index class (currently just a wrapper around the name) and as such also available via ShardRouting and ShardId. 

Note that this is by no means an attempt to do the right thing with the uuid in all places. In almost all places it falls back to the name based comparison that was done before. It is meant as a first step towards slowly improving the situation. 

PS. This came from starting to think about implementing storing indices on disk using folders `NAME_UUID` pattern (instead of just name as we do today) and thus being able to avoid collision between indices that are named the same (but deleted and recreated). The hope is that this will als remove the need for the in memory shard locks we have now. The problem was that I didn't have easy access to the uuid in all places.

While the tests are not all passing ( 99% of them are). I think this is ready to get initial feedback.
</description><key id="128664908">16217</key><summary>Make index uuid available in Index, ShardRouting &amp; ShardId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-01-25T22:54:24Z</created><updated>2016-01-29T19:08:53Z</updated><resolved>2016-01-28T08:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-26T12:34:39Z" id="174986325">I love this - infact I started the same effort last week but didn't get too far time wise. I am +100 on this. I really think we have to try to get rid of the `_na_` as much as possible or at least use assertions to ensure we never get it when we really need a real uuid! please get this in ASAP :)
</comment><comment author="bleskes" created="2016-01-26T21:33:13Z" id="175240961">@s1monw I rebased on master and addressed your comments. All test pass now. Can you take another look?
</comment><comment author="s1monw" created="2016-01-27T13:06:57Z" id="175612346">@bleskes LGTM I left minor comments in the code, can we maybe also add an assertion that the uuid is a real UUID when we create IndexService? that would be awesome. No need for another review!!! thanks so much for doing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] update ingest docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16216</link><project id="" key="" /><description>- move ingest plugin docs to core reference docs
- move geoip processor docs to plugins/ingest-geoip.asciidoc
- add missing options tables for some processors
- add description of pipeline definition
- add description of processor definitions including common parameters
  like "tag" and "on_failure"
</description><key id="128626248">16216</key><summary>[Ingest] update ingest docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-01-25T20:09:37Z</created><updated>2016-02-08T09:32:29Z</updated><resolved>2016-01-25T20:40:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-25T20:36:59Z" id="174654997">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update filters-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16215</link><project id="" key="" /><description>Small correction to other_bucket/other_bucket_key usage in filters aggregation example
</description><key id="128624244">16215</key><summary>Update filters-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eemp</reporter><labels><label>docs</label></labels><created>2016-01-25T20:01:28Z</created><updated>2016-01-26T11:55:52Z</updated><resolved>2016-01-26T11:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T11:55:52Z" id="174968662">thanks @eemp 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove redundant `_score *` in `script_score` examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16214</link><project id="" key="" /><description>```
"script_score" : {
    "script" : "_score * doc['my_numeric_field'].value"
}
```

for `script_score`, isn't the `_score *` implied for the defaults? (which would mean it gets squared by this example?)  That is my reading of

&gt; Note that unlike the custom_score query, the score of the query is multiplied with the result of the script scoring. If you wish to inhibit this, set "boost_mode": "replace"
</description><key id="128621625">16214</key><summary>remove redundant `_score *` in `script_score` examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nezda</reporter><labels><label>Awaiting CLA</label><label>docs</label><label>feedback_needed</label></labels><created>2016-01-25T19:48:03Z</created><updated>2016-05-07T14:40:59Z</updated><resolved>2016-05-07T14:40:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T11:53:22Z" id="174968194">Hi @nezda 

I agree that the docs are not optimal here.  If you use `script_score` as the single function, then the result of the script is combined with the query score using `boost_mode: multiply` which is the default.  So in this case, including `_score` in the script means that it is multiplied twice.  

But `script_score` can also be used as one of many functions (eg decay, field_value_factor etc) and `boost_mode` could be set to `replace`.  So we also want to demonstrate that the script has access to the current `_score`...

I don't think the change you suggest is the right one, given the above, but would be open to other suggestions.  (Also, you'll need to sign the CLA before we can merge an PRs in).
</comment><comment author="clintongormley" created="2016-05-07T14:40:59Z" id="217641720">CLA not signed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>issue starting elasticsearch on windows </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16213</link><project id="" key="" /><description>I am running Windows10 and have Java 1.8.0.71 installed. On clicking the `elasticsearch.bat` file in the freshly extracted elasticsearch 2.1.1 folder, the window flashes for the briefest instant and disappears. Any thoughts would be appreciated. 
</description><key id="128582811">16213</key><summary>issue starting elasticsearch on windows </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Ij888</reporter><labels /><created>2016-01-25T16:44:05Z</created><updated>2016-01-25T17:28:39Z</updated><resolved>2016-01-25T17:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-25T17:28:39Z" id="174594483">Run it from the command Line and it Will tell what's wrong.

Also please ask questions on discuss.elastic.co

We can help there...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.x Small error in the Serial Differencing Aggregation example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16212</link><project id="" key="" /><description>If I have well understood how the _buckets_path_ works, it has to refer to the aggregation path. So in the last example in the Serial Differencing Aggregation documentation, the _thirtieth_difference_ pipeline aggregation might define its _buckets_path_ as "the_sum", not "lemmings".
(I'm sorry, I accidentally made a wrong pull request. It is actually an issue request.)
</description><key id="128557277">16212</key><summary>2.x Small error in the Serial Differencing Aggregation example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RCinna</reporter><labels /><created>2016-01-25T15:09:15Z</created><updated>2016-01-25T15:23:27Z</updated><resolved>2016-01-25T15:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Deprecate fuzzy query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16211</link><project id="" key="" /><description>With this commit we deprecate the widely misunderstood fuzzy query but will still allow the fuzziness parameter in match queries and suggesters.

Relates to #15760 

This is similar to PR #16121 but I did it once again on master as we decided that we will not remove it before 4.0. Doing the deprecation on master first is then more in line with our development model (backporting).
</description><key id="128547649">16211</key><summary>Deprecate fuzzy query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Search</label><label>deprecation</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-25T14:27:24Z</created><updated>2016-02-01T17:28:40Z</updated><resolved>2016-01-27T07:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T19:28:43Z" id="174629770">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Setting class to register node.ingest setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16210</link><project id="" key="" /><description>Boolean parsing is now strict. Also added isIngestNode methods to DiscoveryNode to align this setting with the existing node.data node.master and node.client. Removed NodeModule#isIngestEnabled methods that are not needed anymore.
</description><key id="128543826">16210</key><summary>Use Setting class to register node.ingest setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-25T14:05:49Z</created><updated>2016-02-08T09:32:20Z</updated><resolved>2016-01-25T15:24:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-25T15:11:40Z" id="174538053">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure IndexShard is active during recovery so it gets its fair share of the indexing buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16209</link><project id="" key="" /><description>This PR is based on 2.2.  It's a start for #16206, but I still need to make a test case.

I fixed `IndexShard.recovering` to forcefully activate the shard (and have IMC re-budget indexing buffers), and also fixed `IndexShard.checkIdle` to always return `false` (still active) if the shard is still recovering.
</description><key id="128543076">16209</key><summary>Make sure IndexShard is active during recovery so it gets its fair share of the indexing buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-25T14:01:25Z</created><updated>2016-01-26T20:52:48Z</updated><resolved>2016-01-26T20:52:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-25T14:54:07Z" id="174530719">@bleskes I pushed changes based on your feedback (pushing activation "lower", to when recovery actually begins ... and I changed `markLastWrite` to use `System.nanoTime` instead of the operation's start time)  can you look?  Thanks!
</comment><comment author="bleskes" created="2016-01-25T15:01:25Z" id="174533968">LGTM. I wonder if we can add an easy unit test test to make sure this doesn't regress. Maybe check that a shard is active post recovery from store in `testRecoverFromStore`?

Also - although not needed in master, I wonder what parts of this should go there as well?  (i.e., mark the shard as active up recovery from primary)
</comment><comment author="mikemccand" created="2016-01-25T15:11:17Z" id="174537896">@bleskes I added a couple unit tests ^^

They fail before the change and pass with the change.

I'll think about master ...
</comment><comment author="bleskes" created="2016-01-25T15:15:25Z" id="174539256">Awesome. Thanks mike
</comment><comment author="mikemccand" created="2016-01-25T15:42:43Z" id="174547791">Thanks @bleskes, I'll run all tests and push to 2.2 and 2.1.x.

@dadoonet can you maybe test this with your script and see if recovery is as fast as the original indexing again?  Thanks!
</comment><comment author="mikemccand" created="2016-01-25T16:11:44Z" id="174557649">Well, some tests are angry ... digging.
</comment><comment author="mikemccand" created="2016-01-25T16:56:40Z" id="174581377">So the rest test (`Rest6IT`) is angry because, immediately after the local recovery, which just does `activate()` but does not set the `lastWriteNS`, the shard goes to inactive and a sync flush is done, giving the shard a `sync_id` when the test doesn't want one (since it didn't flush).

If I change the `activate()` to `markLastWrite()` in `internalPerformTranslogRecovery` the test is happy again (no sync flush happens, since the shard never goes to inactive again during the test).  Or I can fix the test (`10_basic.yaml`) to accept a sync_id....

Other tests are tripping on the new `assert` I added, which is spooky, but I fear it could be a concurrency issue where one thread is activating, and then another deactivates, before the first thread gets to its assert ... these failures don't repro ... still digging.
</comment><comment author="mikemccand" created="2016-01-25T19:40:32Z" id="174634910">&gt; If I change the `activate()` to `markLastWrite()` in `internalPerformTranslogRecovery`

I'm going to push this ... it simplifies things again because `activate()` can be folded into `markLastWrite()` again ... I think the "purity" of separating "I'm writing via my local translog" vs "I'm writing via a peer's xlog" is not worth the strange "active -&gt; inactive -&gt; active" that would happen as a result.

But I'm still struggling with:

&gt; Other tests are tripping on the new assert I added,

Many tests seem to trip on this, where a shard becomes active, asks IMC to set its buffer, but after that it still has an inactive buffer ... but only intermittently ... I'm still not sure why.  I'll dig some more, but this is possibly a pre-existing issue (I'll confirm) and we can perhaps decouple from this blocker.
</comment><comment author="bleskes" created="2016-01-25T19:55:02Z" id="174639547">+1 to just use markLastWrite.. Make sense to me.

On 25 jan. 2016 8:41 PM +0100, Michael McCandlessnotifications@github.com, wrote:

&gt; &gt; If I change theactivate()tomarkLastWrite()ininternalPerformTranslogRecovery
&gt; 
&gt; I'm going to push this ... it simplifies things again becauseactivate()can be folded intomarkLastWrite()again ... I think the "purity" of separating "I'm writing via my local translog" vs "I'm writing via a peer's xlog" is not worth the strange "active -&gt;inactive -&gt;active" that would happen as a result.
&gt; 
&gt; But I'm still struggling with:
&gt; 
&gt; &gt; Other tests are tripping on the new assert I added,
&gt; 
&gt; Many tests seem to trip on this, where a shard becomes active, asks IMC to set its buffer, but after that it still has an inactive buffer ... but only intermittently ... I'm still not sure why. I'll dig some more, but this is possibly a pre-existing issue (I'll confirm) and we can perhaps decouple from this blocker.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/16209#issuecomment-174634910).
</comment><comment author="dadoonet" created="2016-01-26T08:10:44Z" id="174884295">@mikemccand here is what I did:
- checkout 2.1
- cherry-picked your commits in 2.1
- build the distribution
- run my scenario

I can confirm that everything works perfectly now! Thanks for fixing this Mike
</comment><comment author="mikemccand" created="2016-01-26T09:07:56Z" id="174914096">Thank you for confirming @dadoonet!

I pushed a fix to restrict the new assertion to only the one thread that actually did the activation (invoked IMC's `forceCheck`), and `mvn verify` is passing (well at least once), but this is hard to explain the test failures I was seeing because local recovery should be single threaded...

The assert does NOT appear trip if I add it to a clean 2.2 clone, meaning it was only ever tripping when  activating during translog replay.

If `mvn verify` passes again I'm going to push!
</comment><comment author="mikemccand" created="2016-01-26T10:40:56Z" id="174950814">OK I pushed this fix to 2.2 with a disastrously accidental commit message: https://github.com/elastic/elasticsearch/commit/9c612d4a7baa8c1ced0eb5ed0bfc0daac05337c6

Working on 2.1.x backport now ...
</comment><comment author="mikemccand" created="2016-01-26T11:13:06Z" id="174960027">I pushed the fix to 2.1 and 2.2, but the new assert still intermittently trips, e.g.: https://build-us-01.elastic.co/job/elastic+elasticsearch+2.2+multijob-intake/128/console

I've commented it out for now, but I still can't explain it.
</comment><comment author="mikemccand" created="2016-01-26T11:14:11Z" id="174960203">Another example failure: http://build-us-00.elastic.co/job/es_core_22_oracle_6/70/consoleFull
</comment><comment author="mikemccand" created="2016-01-26T11:43:15Z" id="174966149">The failures look like this:

```
 &gt; Caused by: java.lang.AssertionError: active=true state=RECOVERING shard=[test][0]
   &gt;    at __randomizedtesting.SeedInfo.seed([582017D9CD0B9CD1]:0)
   &gt;    at org.elasticsearch.index.shard.IndexShard.markLastWrite(IndexShard.java:1005)
   &gt;    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:923)
   &gt;    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:897)
   &gt;    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
   &gt;    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
   &gt;    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2016-01-26 03:08:21,706][INFO ][index          
```

My only guess at this point is when IMC asks `IndicesService` for all indices + shards, that somehow this index is not yet visible?  I.e., that somehow the `StoreRecoveryService` is recovering this shard before `IndicesService` knows about the index ... is that possible?
</comment><comment author="mikemccand" created="2016-01-26T12:17:33Z" id="174977733">Hmm I think for the `IndexWithShadowReplicasIT.testIndexWithFewDocuments` failure, the node is being closed while (concurrently) an index shard is trying to recover, and so when IMC iterates all active shards, there are none.
</comment><comment author="bleskes" created="2016-01-26T13:37:13Z" id="175015982">I think your theory about closing while recovering is correct. An index is recovered on a concurrent thread (to not block the cluster service thread). If the shard is removed before it's recovered (which I guess is likely on test cleanup), the shard is first removed from the lookup tables and then shut down. In between those the state of the shard is not closed, but the IMC ignores it. Not sure what a simple fix would be - we don't have access to indexservice from the shard (good!).  we can add "pending delete" flag to the indexshard marking it as being deleted (while still in the pool), we can change the the assert to wait on close on failure. All of this feels like an overkill to just removing the assert and just relying on the unit tests.
</comment><comment author="mikemccand" created="2016-01-26T15:27:01Z" id="175077288">Thanks @bleskes ... my inclination would be to just leave the assert off, as long as we can convince ourselves these cases when it trips are not important in practice ...

I'm chasing this failure now, which is different...: https://build-us-01.elastic.co/job/elastic+elasticsearch+2.2+multijob-os-compatibility/os=elasticsearch&amp;&amp;debian/107/console
</comment><comment author="mikemccand" created="2016-01-26T18:31:09Z" id="175161994">&gt; I'm chasing this failure now, which is different.

Alright, I've explained this failure, and I'm running tests for a simple fix now.

The issue is that when a shard creates its `Engine` it pulls the current indexing buffer (and other settings) from its `EngineConfig`, but then possibly a lot of time can elapse (e.g. replaying local translog) before it finally sets the `Engine` instances into the shard's `AtomicReference currentEngineReference`.

During that window, if IMC goes and updates the indexing buffer for this shard, since the current engine reference is still null, that shard will update the `EngineConfig` but not the engine, and the change is "lost" in the sense that unless IMC further changes the indexing buffer, `IndexShard.updateBufferSize`'s change detection logic will think nothing had actually changed and won't push the change down to the (now initialized) engine.

This is really a pre-existing issue, but my change here exacerbated it by invoking IMC more frequently, especially when N shards are being initialized which happens nearly concurrently.

My proposed fix is this is to call `onSettingsChanged()` after we finally set the engine instance:

```
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index 69f575b..26bc3ed 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -1433,6 +1433,12 @@ public class IndexShard extends AbstractIndexShardComponent {
             assert this.currentEngineReference.get() == null;
             this.currentEngineReference.set(newEngine(skipTranslogRecovery, config));
         }
+
+        // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during which
+        // IMC may have updated our indexing buffer, e.g. if N other shards are being created and applying their translogs, and we would
+        // otherwise not realize the index buffer had changed while we were concurrently "startint up" so here we forcefully push any config
+        // chagnes to the new engine:
+        engine().onSettingsChanged();
     }

     protected Engine newEngine(boolean skipTranslogRecovery, EngineConfig config) {
```

This fixes the above (reproducible, yay!) test failure, and `mvn verify` seems happy at least once as well...
</comment><comment author="bleskes" created="2016-01-26T18:40:29Z" id="175166283">+1 on this move. Strictly speaking we are still missing the update to the translog buffer, which is a pain in the... . The current infra doesn't allow fixing it directly without doing something ugly and the IMC will set it right in it's next run. I'm good with letting this go and proceed with your suggestion.
</comment><comment author="mikemccand" created="2016-01-26T20:52:18Z" id="175224337">Thanks @bleskes, I pushed that change (I added `null` check just in case ;) ).

I think this change is done for 2.1, 2.2, 2.x.

I'll open a separate PR for master changes, and resolve this ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reproducible randoms in SearchWhileCreatingIndexIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16208</link><project id="" key="" /><description>This commit removes non-reproducible randomness from
SearchWhileCreatingIndexIT. The cause of the non-reproducible randomness
is the use of a random draws for the shard preference inside of a
non-deterministic while loop. Because the inner while loop executed a
non-deterministic number of times, the draws for the next iteration of
the outer loop would be impacted by this making the random draws
non-reproducible. The solution is to move the random draws outside of
the while loop (just make a single draw for the prefernce and increment
with a counter), and remove the outer loop iteration instead using an
annotation to get the desired repetitions.
</description><key id="128515260">16208</key><summary>Reproducible randoms in SearchWhileCreatingIndexIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-25T11:20:26Z</created><updated>2016-01-25T13:42:50Z</updated><resolved>2016-01-25T13:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-25T12:06:52Z" id="174486646">LGTM. I'm wondering if we should just drop the repeat. People can add if if they want to stress test this one...
</comment><comment author="jasontedor" created="2016-01-25T12:11:27Z" id="174487410">&gt; LGTM. I'm wondering if we should just drop the repeat. People can add if if they want to stress test this one...

@bleskes I wondered about that too, and I ultimately decided to keep it because this test is currently failing, but _rarely_. Reducing the number of times this test (`SearchWhileCreatingIndexIT#testTwoReplicas`) runs by a factor of twenty just gives us less opportunities to solve it. I think we just need to get one run from CI with reproducible output.
</comment><comment author="jasontedor" created="2016-01-25T13:27:54Z" id="174509427">@bleskes @s1monw pointed out to me that `@Repeat` is verboten.
</comment><comment author="jasontedor" created="2016-01-25T13:42:33Z" id="174512754">Integrated via 4c1e93bd89cfbf97c25e84e0cdcf9818e9cd97f6 without the `@Repeat` annotation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow deletion of document types from an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16207</link><project id="" key="" /><description>This is in the reference of issue https://github.com/elastic/elasticsearch/issues/8877
@clintongormley I agree that there are internal issue of deleting types but it is one of the widely used feature too. Since, if an index contains more than one type and you are not allowing the users to delete a single type from it, users are forced to re-index complete data of other types first in a new index. I think this features should be enabled with putting some constraints rather than completely removing it from elasticsearch. In any data store one should be able to delete a collection/table/type at any time. 
</description><key id="128502675">16207</key><summary>Allow deletion of document types from an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bharvidixit</reporter><labels /><created>2016-01-25T10:10:40Z</created><updated>2016-01-26T12:19:00Z</updated><resolved>2016-01-26T11:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T19:45:06Z" id="174636518">The reason why we removed the ability to delete types is that Lucene does not support removing fields. We could potentially add it back if we divorced mappings from types (as mentioned on #15613), but I suspect this would just change your problem from "Allow deletion of types" to "Allow deletion of fields", which we can't solve: it is a natural limitation of the data-structures that Lucene is using to store data.
</comment><comment author="clintongormley" created="2016-01-26T11:47:31Z" id="174966871">Instead we're adding a reindex API which will make it easier for you to do the right thing.  See https://github.com/elastic/elasticsearch/issues/15201
</comment><comment author="jasontedor" created="2016-01-26T12:18:26Z" id="174977885">&gt; In any data store one should be able to delete a collection/table/type at any time.

The analogy to database tables, while one that has been made in the past, is broken. See [Index vs. Type](https://www.elastic.co/blog/index-vs-type).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog replay is very slow because indexing buffer is stuck at 500 KB (inactive)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16206</link><project id="" key="" /><description>#13918 caused this issue, by having a shard start up in "inactive" state and only activating once it receives indexing operations.

However, translog replay happens at a lower level (directly, by the `InternalEngine`) and bypasses the `active` boolean on the shard, meaning when a translog is replaying, Lucene is only using a tiny (`500 KB`) indexing buffer the whole time, making translog replay take far too long!
</description><key id="128501982">16206</key><summary>Translog replay is very slow because indexing buffer is stuck at 500 KB (inactive)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label></labels><created>2016-01-25T10:07:11Z</created><updated>2016-01-26T20:54:59Z</updated><resolved>2016-01-26T20:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-25T10:19:51Z" id="174461427">I updated the title: I think this issue also affects replaying ops from a peer's translog.
</comment><comment author="ywelsch" created="2016-01-25T10:22:35Z" id="174461991">yes, the same applies for peer recovery (That was the initial scenario David saw).
</comment><comment author="makeyang" created="2016-01-25T10:38:08Z" id="174465449">any plan or schedule when 2.1.2 will be released?
</comment><comment author="mikemccand" created="2016-01-26T20:53:42Z" id="175224710">Fixed in #16209 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get the type of aggregation result in java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16205</link><project id="" key="" /><description>Hi, I am querying elastic search using a java client. I am able to get the result but how do know the type of the aggregation result so that I can cast accordingly and get the values?
</description><key id="128485578">16205</key><summary>Get the type of aggregation result in java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hardvain</reporter><labels /><created>2016-01-25T08:21:41Z</created><updated>2016-01-25T08:49:11Z</updated><resolved>2016-01-25T08:39:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-25T08:39:09Z" id="174437262">Well I guess that when you send a request, you know which kind of aggregation you are asking for?

You could otherwise may be use `instanceof` before casting?

But please can you ask this kind of question on discuss.elastic.co? We keep this place for confirmed issues.
</comment><comment author="hardvain" created="2016-01-25T08:49:11Z" id="174439654">@dadoonet I was not aware of discuss.elastic.co. I wanted a better solution than instanceof. I will ask there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update mapper-attachments.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16204</link><project id="" key="" /><description>content -&gt; file for correcting mapping
</description><key id="128478516">16204</key><summary>Update mapper-attachments.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tungdx</reporter><labels><label>:Plugin Mapper Attachment</label><label>Awaiting CLA</label><label>feedback_needed</label></labels><created>2016-01-25T07:26:50Z</created><updated>2016-01-25T09:58:27Z</updated><resolved>2016-01-25T09:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-25T07:49:56Z" id="174430009">Thank you for your PR but I don't think this is true in master and 2.x releases.

Are you sure about your change?

BTW if you want to contribute (which is encouraged :) ), please sign the CLA.
</comment><comment author="tungdx" created="2016-01-25T09:57:11Z" id="174455452">Sorry for my mistake, I am using ElasticSearch 1.5.2. Thank you for your response! 
</comment><comment author="dadoonet" created="2016-01-25T09:58:27Z" id="174455897">No problem. Thanks for contributing! :D 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] add docs for on_failure support in ingest pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16203</link><project id="" key="" /><description>just some docs to describe `on_failure` and how it is used in pipelines
</description><key id="128453490">16203</key><summary>[Ingest] add docs for on_failure support in ingest pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-01-25T03:53:46Z</created><updated>2016-01-25T14:50:06Z</updated><resolved>2016-01-25T14:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-25T09:09:41Z" id="174443357">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Update on_failure processor metadata to include a processor's tag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16202</link><project id="" key="" /><description>Now that we introduced the concept of a `tag` to identify processors in a pipeline, it would be useful to add this information into the `on_failure` metadata.

should be added here: https://github.com/elastic/elasticsearch/blob/feature/ingest/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java#L89
</description><key id="128452749">16202</key><summary>[Ingest] Update on_failure processor metadata to include a processor's tag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2016-01-25T03:41:50Z</created><updated>2016-02-01T18:13:26Z</updated><resolved>2016-02-01T18:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expensive include regex is not captured by the circuit breaker logic in ES 1.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16201</link><project id="" key="" /><description>If we run this type of query with an expensive regex (think 10 000 eventIds), it takes 10 minutes to process and is not caught by the circuit breaker.

`"aggs":{"terms":{"terms":{"field":"eventId","include":"123|456|789|...|987|654|321"}}}`
</description><key id="128440413">16201</key><summary>Expensive include regex is not captured by the circuit breaker logic in ES 1.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Asimov4</reporter><labels /><created>2016-01-25T01:14:34Z</created><updated>2016-01-26T12:38:45Z</updated><resolved>2016-01-26T12:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T19:50:10Z" id="174638222">I am confused by your bug report: the circuit breaker provides (limited) protection against out-of-memory errors, while the problem your are noticing here is a long processing time, which should rather be solved by setting a timeout?
</comment><comment author="Asimov4" created="2016-01-26T06:28:48Z" id="174852783">You are right. That said, a circuit breaker as a concept does not necessarily imply that memory should be the only trigger. Is there an opportunity to also trip the circuit breaker on CPU?
I did a bit more research and found this: https://github.com/elastic/elasticsearch/issues/9156
</comment><comment author="jpountz" created="2016-01-26T10:38:22Z" id="174950253">This is related indeed and that would probably apply to this issue as well: I suspect that this will be less of an issue with elasticsearch 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Begin migration of NettyTransport setting to the new platform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16200</link><project id="" key="" /><description>Note for many of the settings we had a netty specific override to a generic transport setting. I have removed those as they are not needed imo (ex. "transport.connections_per_node.recovery" was overriden by "transport.netty.connections_per_node.recovery", which I removed). The netty prefix was kept for settings that are tied to the netty universe - ex. "transport.netty.max_cumulation_buffer_capacity"

There are more settings in this class, I will pick them up  as a followup.
</description><key id="128390390">16200</key><summary>Begin migration of NettyTransport setting to the new platform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-24T11:34:12Z</created><updated>2016-01-28T14:03:06Z</updated><resolved>2016-01-28T14:03:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-28T10:16:58Z" id="176106726">@s1monw @rmuir I've update the PR with a separate class for the settings used before security kicks in. Can you confirm this is what  you meant?
</comment><comment author="bleskes" created="2016-01-28T10:23:36Z" id="176108711">thinking about this more, since we have more settings in Security.java - maybe we should have a "SecureSetting" class and put all settings that are needed there.. 
</comment><comment author="s1monw" created="2016-01-28T11:16:10Z" id="176128347">&gt; thinking about this more, since we have more settings in Security.java - maybe we should have a "SecureSetting" class and put all settings that are needed there..

I guess we should put them in bootstrap or so just to mark them as being used there? WDYT?
</comment><comment author="rmuir" created="2016-01-28T11:21:43Z" id="176130379">I would go the route of less indirection/abstraction not more. The netty problem can be solved completely by moving it to a module.

Still we load thousands of classes before security is initialized because of all the stuff happening (e.g. giant libraries for parsing elasticsearch.yml).

I don't think we should engineer anything elaborate, just tread water instead: try to contain third party code, netty especially (since we know it does **crazy** stuff), try to keep security code from 'reaching out' to other code, try to reduce/remove abstractions and keep it all simple.
</comment><comment author="bleskes" created="2016-01-28T12:14:40Z" id="176150646">@rmuir I agree with the principles. what are you suggesting concretely ? put the settings in bootstrap like simon suggested ?
</comment><comment author="rmuir" created="2016-01-28T12:24:42Z" id="176154452">The only concrete suggestion i have is the long-term one: modules will help us so we don't have so much on the classpath and we control when things are loaded exactly.

in the short term i have no concrete suggestion. I don't think there is really a good one since netty is on the classpath, so I really mean tread water, its a big problem already, we just "try" to prevent bootstrap package from reaching out to other code as much as we can. 
</comment><comment author="s1monw" created="2016-01-28T13:21:35Z" id="176183637">LGTM - we should just move netty in a module
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch with OAuth2 and OpenId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16199</link><project id="" key="" /><description>How do you recommend using Elasticsearch with OAuth2 and OpenId?

Shield does not support this or have I missed something?

Greetings
</description><key id="128385012">16199</key><summary>Elasticsearch with OAuth2 and OpenId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dottilotti</reporter><labels /><created>2016-01-24T09:53:18Z</created><updated>2016-01-26T10:33:52Z</updated><resolved>2016-01-26T10:33:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T10:33:52Z" id="174947971">Hi @dottilotti 

Please ask questions like these in the forum instead: http://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increasing `path.data` causes problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16198</link><project id="" key="" /><description>It seems that changing the *.yml file from

```
path.data: /data1, /data2, /data3
```

to

```
path.data: /data1, /data2, /data3, /data4
```

Causes primary shards on the node to never get assigned.  Reverting the *.yml file solves this problem, for most nodes.

I would expect adding to the `path.data` would not cause data loss

[1]  https://discuss.elastic.co/t/how-to-get-back-my-primary-shards/39954
</description><key id="128351674">16198</key><summary>Increasing `path.data` causes problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klahnakoski</reporter><labels /><created>2016-01-23T20:49:45Z</created><updated>2016-01-24T05:28:37Z</updated><resolved>2016-01-24T05:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-24T00:23:40Z" id="174238199">@klahnakoski Which version of elasticsearch?
</comment><comment author="klahnakoski" created="2016-01-24T02:42:17Z" id="174244590">version 1.7.1
</comment><comment author="rjernst" created="2016-01-24T05:28:37Z" id="174255083">This was fixed in 2.0, where we now maintain a shard must exist on a single data path.  Upgrading to 2.0 or after will reorganize the shard data so it satisfies the new behavior. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16197</link><project id="" key="" /><description>This pull requests converts the cluster-level script settings to the new
settings infrastructure.
</description><key id="128326369">16197</key><summary>Script settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-01-23T12:37:37Z</created><updated>2016-04-05T11:00:44Z</updated><resolved>2016-01-27T11:54:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T10:31:20Z" id="174946518">heya @jasontedor - i'm torn about which values these settings should accept.  `script.inline: on` reads well (and it is how it is documented), but everywhere else we use just `true|false`. Wondering if we should just standardise on true/false and be done with it?
</comment><comment author="jasontedor" created="2016-01-26T10:43:10Z" id="174951628">&gt; Wondering if we should just standardize on true/false and be done with it?

@clintongormley: I'm good with that. I only chose `on` and `off` since it is the value used throughout our docs, so _probably_ the least likely to be a breaking change for people. Shall I update the PR?
</comment><comment author="javanna" created="2016-01-26T11:01:13Z" id="174957476">++ I think we should standardize on proper booleans (true or false that's it) everywhere at this point. `on` and `off` were nice but we should leave them behind and simplify :)
</comment><comment author="s1monw" created="2016-01-26T13:35:44Z" id="175014786">@jasontedor @javanna @clintongormley I already use stict boolean parsing in the new infra, I wonder if we should deprecate the on|off stuff and upgrade all these settings internally. I can make this happen for 3.0 but it would require some work to upgrade it.
</comment><comment author="jasontedor" created="2016-01-26T13:36:28Z" id="175015390">@s1monw This setting can't be a boolean because `sandbox` is also a possible value.
</comment><comment author="s1monw" created="2016-01-26T13:44:01Z" id="175019400">I left some cosmetic comments LGTM I think we should do all cleanups as followups
</comment><comment author="javanna" created="2016-01-26T13:49:21Z" id="175023379">I think #15469 might simplify things here, the conclusion seems to be that we should only support true or false... that way the setting could become a simple boolean.
</comment><comment author="clintongormley" created="2016-01-26T14:02:33Z" id="175029405">++ to strict booleans 
</comment><comment author="jasontedor" created="2016-01-27T02:49:34Z" id="175357068">@javanna @clintongormley @s1monw The script mode settings now accept _only_ `true`, `false` and `sandbox` after 74c61848f82f167ed84dc497eeb33f86b905ae11.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security permissions for Groovy closures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16196</link><project id="" key="" /><description>The purpose behind this pull request is to add some permissions that Groovy needs to use closures in its current state. While granting `java.lang.reflect.ReflectPermission "supressAccessChecks"` is undesirable, there is an upstream pull request in apache/groovy#248 to modify the Groovy compiler so that these permissions are not necessary. Therefore, the long-term plan is to grant these permissions so as to not break existing scripts, and remove these permissions when the upstream pull request is accepted (and make a breaking change in a major release if not). These permissions are not staying around forever.

Closes #16194 
</description><key id="128284515">16196</key><summary>Security permissions for Groovy closures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Scripting</label><label>blocker</label><label>bug</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-23T01:58:36Z</created><updated>2016-01-26T17:53:57Z</updated><resolved>2016-01-26T17:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-23T06:21:55Z" id="174153531">There is also a huge historical security risk. I don't think we should do this at all: closures aren't necessary for the kind of scripting happening here. This scripting is _deadly_ and I don't think we should add even one single permission beyond what is absolutely required.
</comment><comment author="rmuir" created="2016-01-23T06:38:03Z" id="174154182">Just to put another way, to justify my -1 technically:

Closures have not been supported for **months** in master or 2.2: nobody has noticed. There are **hundreds** of tests that use and abuse groovy, not a single one uses a closure. This speaks volumes: they simply aren't necessary.

The `suppressAccessChecks` is over the top here, it has wide implications. Taking on this enormous risk just to add some unnecessary syntactic sugar is the wrong tradeoff.
</comment><comment author="nik9000" created="2016-01-24T20:01:39Z" id="174335615">If we can't get closures without a suppressacesschecks can we throw a
meaningful error message?

Can we make a test with a complex groovy script that we can point people
that want closures to to say "sorry, we can't support closures because
groovy needs dangerous permissions for them to work but if you structure
your script like this it'll work."

Is that good enough given painless's progress?
On Jan 23, 2016 1:38 AM, "Robert Muir" notifications@github.com wrote:

&gt; Just to put another way, to justify my -1 technically:
&gt; 
&gt; Closures have not been supported for _months_ in master or 2.2: nobody
&gt; has noticed. There are _hundreds_ of tests that use and abuse groovy, not
&gt; a single one uses a closure. This speaks volumes: they simply aren't
&gt; necessary.
&gt; 
&gt; The suppressAccessChecks is over the top here, it has wide implications.
&gt; Taking on this enormous risk just to add some unnecessary syntactic sugar
&gt; is the wrong tradeoff.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/16196#issuecomment-174154182
&gt; .
</comment><comment author="rmuir" created="2016-01-25T16:59:45Z" id="174583073">We discussed this after @ywelsch dug into exactly why groovy is in this situation. He has a potential patch to improve the situation in groovy.

Today we have no way to separate "basic scripting users" from "advanced users" (people who choose to use groovy for a reason, e.g. because they like features of it). The two different use cases will cause tension for a while.

But for now, we can optimistically commit this, with the intent we will "remove" it again one way or another: best case, our fix to groovy gets accepted, we upgrade, and then its gone. worst case, it takes longer, until groovy becomes "opt-in" as a plugin or whatever (and we have a different default language).

I do worry about subsequent commits that will enable more "long-tail" features of groovy, e.g. `@Grab` or similar, in the meantime we need to be careful and judicious to avoid relying on this permission as its really no good at all. And we should expect plenty of "long-tail" feature issues just like this to be opened after 2.2.
</comment><comment author="ywelsch" created="2016-01-26T16:45:47Z" id="175108561">@jasontedor LGTM
</comment><comment author="jasontedor" created="2016-01-26T17:53:49Z" id="175141364">Integrated via bdddea2dd0ecd3624e5484b3a34e668ca74cd6d0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Visibility problem when cluster.routing.allocation.awareness.attributes is misconfigured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16195</link><project id="" key="" /><description>Scenario: If a cluster is misconfigured such that all nodes have `cluster.routing.allocation.awareness.attributes=some_missing_attribute` and zero nodes actually set this attribute, then shard allocation will fail and new indexes will have unallocated shards. 

The symptoms are:
- One or more indices are red

The challenge is that it is very difficult to debug this scenario. @dakrone was kind and pointed me at a nifty (advanced!) trick to ask Elasticsearch why it's allocation decision was made, and the decision is unhelpful (details below).

To reproduce this:

1) Run Elasticsearch with the _default_ configuration file and the following additional settings:
- `cluster.routing.allocation.awareness.attributes=foobar`

2) Create a new index:

```
% curl -DPUT localhost:9200/example -d '{}'
{"acknowledged":true}
```

3) Check the status:

```
% curl -s localhost:9200/_cat/indices|grep example
red open example               5 1
```

4) Troubleshooting: Check the elasticsearch logs (at default level) and I don't see any information hinting at allocation issues.

5) Debugging: Try a dry run allocation via _cluster/reroute:

```
% curl -s 'localhost:9200/_cluster/reroute?dry_run&amp;explain&amp;pretty' -d '{ "commands": [ { "
allocate": { "index": "example", "shard": 0, "node": "happynode" } } ] }' | jq '.explanati
ons'
[
  {
    "command": "allocate",
    "parameters": {
      "index": "example",
      "shard": 0,
      "node": "happynode",
      "allow_primary": false
    },
    "decisions": [
      {
        "decider": "allocate_allocation_command",
        "decision": "NO",
        "explanation": "trying to allocate a primary shard [example][0], which is disabled
"
      }
    ]
  }
]
```

---

Overall, I believe Elasticsearch to be acting correctly (It has nowhere to route shards because of our configuration!). However, my concern is the lack of visibility into this issue for users:

1) The debugging trickery via `_cluster/reroute` uses phrasing that I interpret to mean that allocation on is _disabled_.
2) It's unclear how, without the reroute trick, how to ask Elasticsearch for hints on troubleshooting the misconfiguration

For a user, the correction would be to have at least one data node with `node.foobar: whatever` (more generally, that an awareness attribute _must_ exist on at least one data node), and with some more clear logging and/or response/hinting to tell users something along the lines of "Could not allocate shard on any nodes because no nodes match the criteria: has attribute `foobar`"
</description><key id="128265759">16195</key><summary>Visibility problem when cluster.routing.allocation.awareness.attributes is misconfigured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels><label>:Allocation</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-22T22:55:00Z</created><updated>2016-01-26T10:26:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-26T10:25:16Z" id="174945222">Related to #14593 and #12412
</comment><comment author="clintongormley" created="2016-01-26T10:26:23Z" id="174945465">The shard allocation explain API (https://github.com/elastic/elasticsearch/issues/14593) would be a big win here, but I agree that the logging and failure messages can be improved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Closures inaccessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16194</link><project id="" key="" /><description>This starts with ES 2.2 as we move to the script modules. The below script works in ES 2.1, but not ES 2.2 (the impact of which could be pretty big):

``` http
PUT /test/type/1
{
  "message": "This is a message!",
  "numbers" : [1, 2, 3, 4]
}

PUT /test/type/2
{
  "message": "This is a message! too",
  "numbers" : [4, 2, 3, 5]
}

GET /test/_search
{
  "script_fields": {
    "use_closure": {
      "script": {
        "inline": "doc['numbers'].values.findAll { it % 2 == 0 }"
      }
    }
  }
}
```

I noticed that the issue seemed to be ClassNotFound errors related to the security manager, so I started adding permissions until I hit a wall because it could not access the generated closure's class.

```
  permission org.elasticsearch.script.ClassPermission "groovy.lang.Closure";
  permission org.elasticsearch.script.ClassPermission "org.codehaus.groovy.reflection.CachedMethod";
  permission org.elasticsearch.script.ClassPermission "org.codehaus.groovy.reflection.ClassInfo";
  permission org.elasticsearch.script.ClassPermission "org.codehaus.groovy.runtime.DefaultGroovyMethodsSupport";
  permission org.elasticsearch.script.ClassPermission "org.codehaus.groovy.runtime.GeneratedClosure";
```

cc @rasroh 
</description><key id="128248711">16194</key><summary>Groovy Closures inaccessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Scripting</label><label>blocker</label><label>bug</label><label>v2.2.0</label></labels><created>2016-01-22T21:20:38Z</created><updated>2016-01-26T17:56:27Z</updated><resolved>2016-01-26T17:56:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-26T11:40:51Z" id="174965756">Relates apache/groovy#248.
</comment><comment author="jasontedor" created="2016-01-26T17:56:27Z" id="175143132">Closed via bdddea2dd0ecd3624e5484b3a34e668ca74cd6d0 from #16196.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Allow to get size of array in mustache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16193</link><project id="" key="" /><description>This adds support for returning the size of an array
or an collection, in addition to access fields via
`array.0` you can specify `array.size` to get its size.
</description><key id="128246443">16193</key><summary>Scripting: Allow to get size of array in mustache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T21:08:35Z</created><updated>2016-01-25T06:59:44Z</updated><resolved>2016-01-25T06:59:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2016-01-22T22:11:36Z" id="174069443">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16192</link><project id="" key="" /><description>The following build failed due to a test failure: (http://build-us-00.elastic.co/job/es_core_master_medium/3130/)

```
[12:04:04][~/code/esxp/elasticsearch]$ gradle :core:integTest -Dtests.seed=45D0AE38C41F1521 -Dtests.class=org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=539m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:-UseCompressedOops" -Dtests.locale=en_US -Dtests.timezone=Etc/UTC
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:sourcesJar UP-TO-DATE
:buildSrc:signArchives SKIPPED
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.8
  OS Info               : Mac OS X 10.10.5 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_40 [Java HotSpot(TM) 64-Bit Server VM 25.40-b25]
:core:compileJava UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:jar UP-TO-DATE
:test:framework:compileJava UP-TO-DATE
:test:framework:processResources UP-TO-DATE
:test:framework:classes UP-TO-DATE
:test:framework:jar UP-TO-DATE
:core:compileTestJava UP-TO-DATE
:core:processTestResources UP-TO-DATE
:core:testClasses UP-TO-DATE
:core:integTest
   [junit4] &lt;JUnit4&gt; says g'day! Master seed: 45D0AE38C41F1521
==&gt; Test Info: seed=45D0AE38C41F1521; jvm=1; suite=1
Suite: org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT
  1&gt; [2016-01-22 12:07:25,871][DEBUG][org.elasticsearch.bootstrap] BSD RLIMIT_NPROC initialization successful
  1&gt; [2016-01-22 12:07:26,019][DEBUG][org.elasticsearch.bootstrap] OS X seatbelt initialization successful
  1&gt; [2016-01-22 12:07:26,023][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
  1&gt; [2016-01-22 12:07:26,023][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
  1&gt; 186877882f8b68f031f610bd7ab8c5c1fb/lucene-core-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-analyzers-common/5.5.0-snapshot-1725675/528a695bb8882dbc3d9866335ac1bb3905cba4e3/lucene-analyzers-common-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-backward-codecs/5.5.0-snapshot-1725675/3fb1bcc1001a10b74ae91848c8558572891c1409/lucene-backward-codecs-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-grouping/5.5.0-snapshot-1725675/6e6253936522f27b35ba4d8485806f517ef2df45/lucene-grouping-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-highlighter/5.5.0-snapshot-1725675/8a313aa34b0070d3f7d48005e7677b680db1b09d/lucene-highlighter-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-join/5.5.0-snapshot-1725675/bf4c5a17cfb265d321ef4cfb0f3d7c1a6a6651de/lucene-join-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-memory/5.5.0-snapshot-1725675/2713a319d0aa696c65a32a36fda830bc482a5880/lucene-memory-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-misc/5.5.0-snapshot-1725675/88251ecdbf877c15a94d4013aa5157f5b5ce4cea/lucene-misc-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queries/5.5.0-snapshot-1725675/bf9e522244c7c4eee6c3bcc3212ff057f7b88000/lucene-queries-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queryparser/5.5.0-snapshot-1725675/12d71cf10a4b79231dc488af16d723dfca5ab64b/lucene-queryparser-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-sandbox/5.5.0-snapshot-1725675/f903d67d042904527a7e2e8a75c55afe36a04251/lucene-sandbox-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial/5.5.0-snapshot-1725675/2f5758bbcf97048ab62d2d4ae73867d06f1ed03f/lucene-spatial-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial3d/5.5.0-snapshot-1725675/2cc29e4658be151658fac6e5ed7915982b6de861/lucene-spatial3d-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-suggest/5.5.0-snapshot-1725675/f490a09ca056aba42e8751a469ef114df64aae0d/lucene-suggest-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securesm/1.0/c0c6cf986ba0057390bfcc80c366a0e3157f944b/securesm-1.0.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.3.1/1303efbc4b181e5a58bf2e967dc156a3132b97c0/commons-cli-1.3.1.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.7.1/8b5057f74ea378c0150a1860874a3ebdcb713767/hppc-0.7.1.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.8.2/d27c24204c5e507b16fec01006b3d0f1ec42aed4/joda-time-2.8.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org./caches/modules-2/files-2.1/commons-cli/commons-cli/1.3.1/1303efbc4b181e5a58bf2e967dc156a3132b97c0/commons-cli-1.3.1.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.7.1/8b5057f74ea378c0150a1860874a3ebdcb713767/hppc-0.7.1.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.8.2/d27c24204c5e507b16fec01006b3d0f1ec42aed4/joda-time-2.8.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.joda/joda-convert/1.2/35ec554f0cd00c956cc69051514d9488b1374dec/joda-convert-1.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.6.2/123f29333b2c6b3516b14252b6e93226bfcd6e37/jackson-core-2.6.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-smile/2.6.2/395d18c1a1dd730b8026ee59c4067e5d2b45ba6e/jackson-dataformat-smile-2.6.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.6.2/4ae23088dd3fae47c66843f2e4251d7255ee140e/jackson-dataformat-yaml-2.6.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-cbor/2.6.2/1e13c575f914c83761bb8e2aca7dfd9e4c647579/jackson-dataformat-cbor-2.6.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.10.5.Final/9ca7d55d246092bddd29b867706e2f6c7db701a0/netty-3.10.5.Final.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.tdunning/t-digest/3.0/84ccf145ac2215e6bfa63baa3101c0af41017cfc/t-digest-3.0.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.6/7495feb7f71ee124bd2a7e7d83590e296d71d80e/HdrHistogram-2.1.6.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.spatial4j/spatial4j/0.5/6e16edaf6b1ba76db7f08c2f3723fce3b358ecc3/spatial4j-0.5.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.vividsolutions/jts/1.13/3ccfb9b60f04d71add996a666ceb8902904fd805/jts-1.13.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/apache-log4j-extras/1.2.17/85863614d82185d7e51fe21c00aa9117a523a8b6/apache-log4j-extras-1.2.17.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.2/8619e95939167fb37245b5670135e4feb0ec7d50/slf4j-api-1.6.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/4.1.0/1c12d070e602efd8021891cdd7fd18bc129372d4/jna-4.1.0.jar:/Users/jdconrad/code/esxp/elasticsearch/test/framework/build/libs/framework-3.0.0-SNAPSHOT.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/randomizedtesting-runner/2.3.2/307965917fe8a22b7ee72deba39ef4b8e6ebc069/randomizedtesting-runner-2.3.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-test-framework/5.5.0-snapshot-1725675/75e8b2c40b6fc7274d248c0ccdd1af5ce6cf904a/lucene-test-framework-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-codecs/5.5.0-snapshot-1725675/6cb9611a9aad77934bc991f3b5825240c415eece/lucene-codecs-5.5.0-snapshot-1725675.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.3.6/4c47155e3e6c9a41a28db36680b828ced53b8af4/httpclient-4.3.6.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.3.3/f91b7a4aadc5cf486df6e4634748d7dd7a73f06d/httpcore-4.3.3.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.3/f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f/commons-logging-1.1.3.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.10/4b95f4897fa13f2cd904aee711aeafc0c5295cd8/commons-codec-1.10.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securemock/1.2/98201d4ad5ac93f6b415ae9172d52b5e7cda490e/securemock-1.2.jar:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/junit4-ant/2.3.2/dc8f03f6111974092491f35b8269eb0fc57f52f7/junit4-ant-2.3.2.jar
  1&gt; [2016-01-22 12:07:26,148][DEBUG][org.elasticsearch.bootstrap] sun.boot.class.path: /Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/classes
  1&gt; [2016-01-22 12:07:26,150][DEBUG][org.elasticsearch.bootstrap] classloader urls: [file:/Users/jdconrad/code/esxp/elasticsearch/core/build/classes/test/, file:/Users/jdconrad/code/esxp/elasticsearch/core/build/resources/test/, file:/Users/jdconrad/code/esxp/elasticsearch/core/build/classes/main/, file:/Users/jdconrad/code/esxp/elasticsearch/core/build/resources/main/, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-core/5.5.0-snapshot-1725675/9eff7f186877882f8b68f031f610bd7ab8c5c1fb/lucene-core-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-analyzers-common/5.5.0-snapshot-1725675/528a695bb8882dbc3d9866335ac1bb3905cba4e3/lucene-analyzers-common-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-backward-codecs/5.5.0-snapshot-1725675/3fb1bcc1001a10b74ae91848c8558572891c1409/lucene-backward-codecs-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-grouping/5.5.0-snapshot-1725675/6e6253936522f27b35ba4d8485806f517ef2df45/lucene-grouping-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-highlighter/5.5.0-snapshot-1725675/8a313aa34b0070d3f7d48005e7677b680db1b09d/lucene-highlighter-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-join/5.5.0-snapshot-1725675/bf4c5a17cfb265d321ef4cfb0f3d7c1a6a6651de/lucene-join-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-memory/5.5.0-snapshot-1725675/2713a319d0aa696c65a32a36fda830bc482a5880/lucene-memory-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-misc/5.5.0-snapshot-1725675/88251ecdbf877c15a94d4013aa5157f5b5ce4cea/lucene-misc-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queries/5.5.0-snapshot-1725675/bf9e522244c7c4eee6c3bcc3212ff057f7b88000/lucene-queries-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queryparser/5.5.0-snapshot-1725675/12d71cf10a4b79231dc488af16d723dfca5ab64b/lucene-queryparser-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-sandbox/5.5.0-snapshot-1725675/f903d67d042904527a7e2e8a75c55afe36a04251/lucene-sandbox-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial/5.5.0-snapshot-1725675/2f5758bbcf97048ab62d2d4ae73867d06f1ed03f/lucene-spatial-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial3d/5.5.0-snapshot-1725675/2cc29e4658be151658fac6e5ed7915982b6de861/lucene-spatial3d-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-suggest/5.5.0-snapshot-1725675/f490a09ca056aba42e8751a469ef114df64aae0d/lucene-suggest-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securesm/1.0/c0c6cf986ba0057390bfcc80c366a0e3157f944b/securesm-1.0.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.3.1/1303efbc4b181e5a58bf2e967dc156a3132b97c0/commons-cli-1.3.1.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.7.1/8b5057f74ea378c0150a1860874a3ebdcb713767/hppc-0.7.1.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.8.2/d27c24204c5e507b16fec01006b3d0f1ec42aed4/joda-time-2.8.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.joda/joda-convert/1.2/35ec554f0cd00c956cc69051514d9488b1374dec/joda-convert-1.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.6.2/123f29333b2c6b3516b14252b6e93226bfcd6e37/jackson-core-2.6.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-smile/2.6.2/395d18c1a1dd730b8026ee59c4067e5d2b45ba6e/jackson-dataformat-smile-2.6.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.6.2/4ae23088dd3fae47c66843f2e4251d7255ee140e/jackson-dataformat-yaml-2.6.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-cbor/2.6.2/1e13c575f914c83761bb8e2aca7dfd9e4c647579/jackson-dataformat-cbor-2.6.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.10.5.Final/9ca7d55d246092bddd29b867706e2f6c7db701a0/netty-3.10.5.Final.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.tdunning/t-digest/3.0/84ccf145ac2215e6bfa63baa3101c0af41017cfc/t-digest-3.0.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.6/7495feb7f71ee124bd2a7e7d83590e296d71d80e/HdrHistogram-2.1.6.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.spatial4j/spatial4j/0.5/6e16edaf6b1ba76db7f08c2f3723fce3b358ecc3/spatial4j-0.5.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.vividsolutions/jts/1.13/3ccfb9b60f04d71add996a666ceb8902904fd805/jts-1.13.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/apache-log4j-extras/1.2.17/85863614d82185d7e51fe21c00aa9117a523a8b6/apache-log4j-extras-1.2.17.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.2/8619e95939167fb37245b5670135e4feb0ec7d50/slf4j-api-1.6.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/4.1.0/1c12d070e602efd8021891cdd7fd18bc129372d4/jna-4.1.0.jar, file:/Users/jdconrad/code/esxp/elasticsearch/test/framework/build/libs/framework-3.0.0-SNAPSHOT.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/randomizedtesting-runner/2.3.2/307965917fe8a22b7ee72deba39ef4b8e6ebc069/randomizedtesting-runner-2.3.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-test-framework/5.5.0-snapshot-1725675/75e8b2c40b6fc7274d248c0ccdd1af5ce6cf904a/lucene-test-framework-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-codecs/5.5.0-snapshot-1725675/6cb9611a9aad77934bc991f3b5825240c415eece/lucene-codecs-5.5.0-snapshot-1725675.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.3.6/4c47155e3e6c9a41a28db36680b828ced53b8af4/httpclient-4.3.6.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.3.3/f91b7a4aadc5cf486df6e4634748d7dd7a73f06d/httpcore-4.3.3.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.3/f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f/commons-logging-1.1.3.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.10/4b95f4897fa13f2cd904aee711aeafc0c5295cd8/commons-codec-1.10.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securemock/1.2/98201d4ad5ac93f6b415ae9172d52b5e7cda490e/securemock-1.2.jar, file:/Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/junit4-ant/2.3.2/dc8f03f6111974092491f35b8269eb0fc57f52f7/junit4-ant-2.3.2.jar]
  1&gt; [2016-01-22 12:07:26,165][DEBUG][org.elasticsearch.bootstrap] java.home: /Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre
  1&gt; [2016-01-22 12:07:26,166][DEBUG][org.elasticsearch.bootstrap] examining directory: /Users/jdconrad/code/esxp/elasticsearch/core/build/classes/test
  1&gt; [2016-01-22 12:07:26,623][DEBUG][org.elasticsearch.bootstrap] examining directory: /Users/jdconrad/code/esxp/elasticsearch/core/build/resources/test
  1&gt; [2016-01-22 12:07:26,664][DEBUG][org.elasticsearch.bootstrap] examining directory: /Users/jdconrad/code/esxp/elasticsearch/core/build/classes/main
  1&gt; [2016-01-22 12:07:27,307][DEBUG][org.elasticsearch.bootstrap] examining directory: /Users/jdconrad/code/esxp/elasticsearch/core/build/resources/main
  1&gt; [2016-01-22 12:07:27,312][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-core/5.5.0-snapshot-1725675/9eff7f186877882f8b68f031f610bd7ab8c5c1fb/lucene-core-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,355][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-analyzers-common/5.5.0-snapshot-1725675/528a695bb8882dbc3d9866335ac1bb3905cba4e3/lucene-analyzers-common-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,371][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-backward-codecs/5.5.0-snapshot-1725675/3fb1bcc1001a10b74ae91848c8558572891c1409/lucene-backward-codecs-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,373][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-grouping/5.5.0-snapshot-1725675/6e6253936522f27b35ba4d8485806f517ef2df45/lucene-grouping-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,376][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-highlighter/5.5.0-snapshot-1725675/8a313aa34b0070d3f7d48005e7677b680db1b09d/lucene-highlighter-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,379][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-join/5.5.0-snapshot-1725675/bf4c5a17cfb265d321ef4cfb0f3d7c1a6a6651de/lucene-join-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,381][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-memory/5.5.0-snapshot-1725675/2713a319d0aa696c65a32a36fda830bc482a5880/lucene-memory-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,381][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-misc/5.5.0-snapshot-1725675/88251ecdbf877c15a94d4013aa5157f5b5ce4cea/lucene-misc-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,383][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queries/5.5.0-snapshot-1725675/bf9e522244c7c4eee6c3bcc3212ff057f7b88000/lucene-queries-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,387][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queryparser/5.5.0-snapshot-1725675/12d71cf10a4b79231dc488af16d723dfca5ab64b/lucene-queryparser-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,398][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-sandbox/5.5.0-snapshot-1725675/f903d67d042904527a7e2e8a75c55afe36a04251/lucene-sandbox-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,400][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial/5.5.0-snapshot-1725675/2f5758bbcf97048ab62d2d4ae73867d06f1ed03f/lucene-spatial-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,401][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial3d/5.5.0-snapshot-1725675/2cc29e4658be151658fac6e5ed7915982b6de861/lucene-spatial3d-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,403][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-suggest/5.5.0-snapshot-1725675/f490a09ca056aba42e8751a469ef114df64aae0d/lucene-suggest-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,404][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securesm/1.0/c0c6cf986ba0057390bfcc80c366a0e3157f944b/securesm-1.0.jar
  1&gt; [2016-01-22 12:07:27,405][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.3.1/1303efbc4b181e5a58bf2e967dc156a3132b97c0/commons-cli-1.3.1.jar
  1&gt; [2016-01-22 12:07:27,406][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.7.1/8b5057f74ea378c0150a1860874a3ebdcb713767/hppc-0.7.1.jar
  1&gt; [2016-01-22 12:07:27,456][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.8.2/d27c24204c5e507b16fec01006b3d0f1ec42aed4/joda-time-2.8.2.jar
  1&gt; [2016-01-22 12:07:27,473][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.joda/joda-convert/1.2/35ec554f0cd00c956cc69051514d9488b1374dec/joda-convert-1.2.jar
  1&gt; [2016-01-22 12:07:27,475][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.6.2/123f29333b2c6b3516b14252b6e93226bfcd6e37/jackson-core-2.6.2.jar
  1&gt; [2016-01-22 12:07:27,477][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-smile/2.6.2/395d18c1a1dd730b8026ee59c4067e5d2b45ba6e/jackson-dataformat-smile-2.6.2.jar
  1&gt; [2016-01-22 12:07:27,479][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.6.2/4ae23088dd3fae47c66843f2e4251d7255ee140e/jackson-dataformat-yaml-2.6.2.jar
  1&gt; [2016-01-22 12:07:27,483][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-cbor/2.6.2/1e13c575f914c83761bb8e2aca7dfd9e4c647579/jackson-dataformat-cbor-2.6.2.jar
  1&gt; [2016-01-22 12:07:27,485][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.10.5.Final/9ca7d55d246092bddd29b867706e2f6c7db701a0/netty-3.10.5.Final.jar
  1&gt; [2016-01-22 12:07:27,580][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.tdunning/t-digest/3.0/84ccf145ac2215e6bfa63baa3101c0af41017cfc/t-digest-3.0.jar
  1&gt; [2016-01-22 12:07:27,581][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.6/7495feb7f71ee124bd2a7e7d83590e296d71d80e/HdrHistogram-2.1.6.jar
  1&gt; [2016-01-22 12:07:27,582][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.spatial4j/spatial4j/0.5/6e16edaf6b1ba76db7f08c2f3723fce3b358ecc3/spatial4j-0.5.jar
  1&gt; [2016-01-22 12:07:27,584][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.vividsolutions/jts/1.13/3ccfb9b60f04d71add996a666ceb8902904fd805/jts-1.13.jar
  1&gt; [2016-01-22 12:07:27,597][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar
  1&gt; [2016-01-22 12:07:27,601][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/log4j/apache-log4j-extras/1.2.17/85863614d82185d7e51fe21c00aa9117a523a8b6/apache-log4j-extras-1.2.17.jar
  1&gt; [2016-01-22 12:07:27,606][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.2/8619e95939167fb37245b5670135e4feb0ec7d50/slf4j-api-1.6.2.jar
  1&gt; [2016-01-22 12:07:27,608][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/4.1.0/1c12d070e602efd8021891cdd7fd18bc129372d4/jna-4.1.0.jar
  1&gt; [2016-01-22 12:07:27,610][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/code/esxp/elasticsearch/test/framework/build/libs/framework-3.0.0-SNAPSHOT.jar
  1&gt; [2016-01-22 12:07:27,614][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/randomizedtesting-runner/2.3.2/307965917fe8a22b7ee72deba39ef4b8e6ebc069/randomizedtesting-runner-2.3.2.jar
  1&gt; [2016-01-22 12:07:27,672][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar
  1&gt; [2016-01-22 12:07:27,712][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar
  1&gt; [2016-01-22 12:07:27,720][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-test-framework/5.5.0-snapshot-1725675/75e8b2c40b6fc7274d248c0ccdd1af5ce6cf904a/lucene-test-framework-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,739][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-codecs/5.5.0-snapshot-1725675/6cb9611a9aad77934bc991f3b5825240c415eece/lucene-codecs-5.5.0-snapshot-1725675.jar
  1&gt; [2016-01-22 12:07:27,748][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.3.6/4c47155e3e6c9a41a28db36680b828ced53b8af4/httpclient-4.3.6.jar
  1&gt; [2016-01-22 12:07:27,757][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.3.3/f91b7a4aadc5cf486df6e4634748d7dd7a73f06d/httpcore-4.3.3.jar
  1&gt; [2016-01-22 12:07:27,770][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.3/f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f/commons-logging-1.1.3.jar
  1&gt; [2016-01-22 12:07:27,773][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.10/4b95f4897fa13f2cd904aee711aeafc0c5295cd8/commons-codec-1.10.jar
  1&gt; [2016-01-22 12:07:27,781][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securemock/1.2/98201d4ad5ac93f6b415ae9172d52b5e7cda490e/securemock-1.2.jar
  1&gt; [2016-01-22 12:07:27,823][DEBUG][org.elasticsearch.bootstrap] examining jar: /Users/jdconrad/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/junit4-ant/2.3.2/dc8f03f6111974092491f35b8269eb0fc57f52f7/junit4-ant-2.3.2.jar
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=45D0AE38C41F1521 -Dtests.class=org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=539m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:-UseCompressedOops" -Dtests.locale=en_US -Dtests.timezone=America/Los_Angeles
  2&gt; Jan 22, 2016 12:07:30 PM com.carrotsearch.randomizedtesting.RandomizedRunner runSuite
  2&gt; SEVERE: Panic: RunListener hook shouldn't throw exceptions.
  2&gt; java.lang.NullPointerException
  2&gt;  at org.apache.lucene.util.RunListenerPrintReproduceInfo.printDebuggingInformation(RunListenerPrintReproduceInfo.java:132)
  2&gt;  at org.apache.lucene.util.RunListenerPrintReproduceInfo.testRunFinished(RunListenerPrintReproduceInfo.java:119)
  2&gt;  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:706)
  2&gt;  at com.carrotsearch.randomizedtesting.RandomizedRunner.access$200(RandomizedRunner.java:140)
  2&gt;  at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:591)
ERROR   0.00s | DateHistogramOffsetIT (suite) &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.util.IllformedLocaleException: Invalid subtag: en_US [at index 0]
   &gt;  at __randomizedtesting.SeedInfo.seed([45D0AE38C41F1521]:0)
   &gt;  at java.util.Locale$Builder.setLanguageTag(Locale.java:2426)
   &gt;  at org.apache.lucene.util.LuceneTestCase.localeForLanguageTag(LuceneTestCase.java:1534)
   &gt;  at java.lang.Thread.run(Thread.java:745)
Completed [1/1] in 5.29s, 0 tests, 1 error &lt;&lt;&lt; FAILURES!

Tests with failures:
  - org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT (suite)

   [junit4] JVM J0:     0.75 ..     7.75 =     6.99s
   [junit4] Execution time total: 7.78 sec.
   [junit4] Tests summary: 1 suite, 0 tests, 1 suite-level error
:core:integTest FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:integTest'.
&gt; There were test failures: 1 suite, 0 tests, 1 suite-level error [seed: 45D0AE38C41F1521]

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 1 mins 6.943 secs

BUILD SUCCESSFUL
Total time: 9 seconds
```
</description><key id="128235253">16192</key><summary>Test Failure: org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>jenkins</label><label>test</label><label>v5.0.0-beta1</label></labels><created>2016-01-22T20:11:40Z</created><updated>2016-09-17T09:02:50Z</updated><resolved>2016-09-16T09:58:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T14:07:02Z" id="216538810">This is still broken in master, but looks like a test bug.  The important bit is:

```
ERROR   0.00s | DateHistogramOffsetIT (suite) &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.util.IllformedLocaleException: Invalid subtag: en_US [at index 0]
   &gt;  at __randomizedtesting.SeedInfo.seed([45D0AE38C41F1521]:0)
   &gt;  at java.util.Locale$Builder.setLanguageTag(Locale.java:2426)
   &gt;  at org.apache.lucene.util.LuceneTestCase.localeForLanguageTag(LuceneTestCase.java:1582)
   &gt;  at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="javanna" created="2016-09-16T09:58:10Z" id="247563039">I had a look as this seemed easy to fix (seems like we try to parse an unsupported locale), but I couldn't find where the failure came from. Also there were no further failures on this so I assume it got fixed meanwhile.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Where is boolFilter().cache, hasParentFilter() and includeFlags in ES 2.x?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16191</link><project id="" key="" /><description>Following is a snippet that is compatible with ES 1.x:
`BoolFilterBuilder boolFilterBuilder = boolFilter().cache(true);`

This is a function present in `FilterBuilders.java`:
`hasParentFilter(String parentType, QueryBuilder query)`

How can I find `includeFlags` that was in Elasticsearch `TermBuilders`?
This is only present in `SignificantTermBuilders`!!

How do I change the syntax to make it compatible with ES 2.x?
I could not find any examples?
</description><key id="128231339">16191</key><summary>Where is boolFilter().cache, hasParentFilter() and includeFlags in ES 2.x?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apanimesh061</reporter><labels /><created>2016-01-22T19:54:19Z</created><updated>2016-01-25T19:58:38Z</updated><resolved>2016-01-25T19:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T19:58:38Z" id="174640555">&gt; boolFilter().cache(true)

Caching is not configurable anymore, see https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_query_dsl_changes.html#_filter_auto_caching

&gt; This is a function present in FilterBuilders.java: hasParentFilter(String parentType, QueryBuilder query)

Queries and filters have been merged, you should now use QueryBuilders.hasParentQuery. https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_query_dsl_changes.html#_queries_and_filters_merged

&gt; How can I find includeFlags that was in Elasticsearch TermBuilders?

The `terms` aggregation moved to a different regex engine that allows to run such aggregations more efficiently.
https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_aggregation_changes.html#_including_excluding_terms
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.1 - Multi field issue?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16190</link><project id="" key="" /><description>Prior to upgrading to ES 2.1, we have been using multi fields to store strings in multiple ways:

https://www.elastic.co/guide/en/elasticsearch/reference/master/_multi_fields.html
https://www.elastic.co/guide/en/elasticsearch/guide/current/multi-fields.html

To access the "raw" version of the field, we simply use "field.raw".

However, in ES 2.1, I get an exception:

```
MapperParsingException[Field name [first_name.raw] cannot contain '.']
```

Is it possible to use multi fields in ES 2.1? Is there some other way I should be trying to call the multi field?
</description><key id="128225616">16190</key><summary>Elasticsearch 2.1 - Multi field issue?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevkon3</reporter><labels /><created>2016-01-22T19:24:42Z</created><updated>2016-02-02T21:51:25Z</updated><resolved>2016-01-25T19:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevkon3" created="2016-01-22T20:53:33Z" id="174043222">Looks related to https://github.com/elastic/elasticsearch/issues/14957 which was closed without a resolution. Which led me to https://github.com/elastic/elasticsearch-mapper-attachments/issues/169
</comment><comment author="kevkon3" created="2016-01-22T21:44:10Z" id="174060836">It looks like this should be fixed as of ES 2.2. Is there a release date plan for 2.2?
</comment><comment author="jpountz" created="2016-01-25T19:52:26Z" id="174638820">The 2.2 release should happen soon. I can't promise anything but we are in the process of resolving the last blockers for this release.
</comment><comment author="kri5" created="2016-02-02T20:56:14Z" id="178814578">@kevkon3 is it fixed for you? i am experiencing the same issue.
</comment><comment author="kevkon3" created="2016-02-02T21:51:25Z" id="178845775">@kri5 We haven't tried out elasticsearch 2.2 yet. We tabled the upgrade for now, waiting on 2.2. I imagine we will try it out in about a week?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warn user when mapper attachments has to extract an unknown format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16189</link><project id="" key="" /><description>With this change: https://github.com/elastic/elasticsearch-mapper-attachments/issues/163 we removed support for many formats.

For example, we don't extract text from XML anymore. See https://discuss.elastic.co/t/search-attachment-content/39559/7

We should warn the user (either in logs) or reject the document is such a case. The end user has no way to know if the parsing has been done correctly or no.

For example:

``` js
DELETE /test
PUT /test
PUT /test/type/_mapping
{
  "type": {
    "properties": {
      "file": {
        "type": "attachment",
        "fields": {
          "content": {
            "type": "string",
            "store": true
          }
        }
      }
    }
  }
}
PUT /test/type/1?refresh=true
{
  "file": "PD94bWwgdmVyc2lvbj0iMS4wIj8+PGNhdGFsb2c+PGJvb2sgaWQ9ImJrMTAxIj48YXV0aG9yPlBhY288L2F1dGhvcj48dGl0bGU+U3ByaW5nLUJvb3Q8L3RpdGxlPjwvYm9vaz48Ym9vayBpZD0iYmsxMDEiPjxhdXRob3I+QXV0aG9yIFRlc3Q8L2F1dGhvcj48dGl0bGU+VGl0bGUgVGVzdDwvdGl0bGU+PC9ib29rPjxib29rIGlkPSJiazEwMSI+PGF1dGhvcj5QYWNvPC9hdXRob3I+PHRpdGxlPkVsYXN0aWMgU2VhcmNoPC90aXRsZT48L2Jvb2s+PGJvb2sgaWQ9ImJrMTAxIj48YXV0aG9yPk5vYm9keTwvYXV0aG9yPjx0aXRsZT5Ob3RoaW5nPC90aXRsZT48L2Jvb2s+"
}
GET /test/type/_search
{
  "fields": [ "file.content" ]
}
```

Gives no content at all. Although the source text is:

``` xml
&lt;?xml version="1.0"?&gt;&lt;catalog&gt;&lt;book id="bk101"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Spring-Boot&lt;/title&gt;&lt;/book&gt;&lt;book id="bk101"&gt;&lt;author&gt;Author Test&lt;/author&gt;&lt;title&gt;Title Test&lt;/title&gt;&lt;/book&gt;&lt;book id="bk101"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Elastic Search&lt;/title&gt;&lt;/book&gt;&lt;book id="bk101"&gt;&lt;author&gt;Nobody&lt;/author&gt;&lt;title&gt;Nothing&lt;/title&gt;&lt;/book&gt;
```

But this:

``` js
DELETE /test
PUT /test
PUT /test/type/_mapping
{
  "type": {
    "properties": {
      "file": {
        "type": "string",
        "store": true
      }
    }
  }
}
PUT /test/type/1?refresh=true
{
  "file": "&lt;?xml version=\"1.0\"?&gt;&lt;catalog&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Spring-Boot&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Author Test&lt;/author&gt;&lt;title&gt;Title Test&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Elastic Search&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Nobody&lt;/author&gt;&lt;title&gt;Nothing&lt;/title&gt;&lt;/book&gt;"
}
GET /test/type/_search
{
  "fields": [ "file" ]
}
```

Works as expected.
</description><key id="128201245">16189</key><summary>Warn user when mapper attachments has to extract an unknown format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Ingest Attachment</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-22T17:12:49Z</created><updated>2017-02-06T23:31:44Z</updated><resolved>2017-02-06T23:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pakkk" created="2016-01-22T18:41:35Z" id="174005478">Many thanks for your help @dadoonet :)

Yes, it is strange, the Base64 string comes from:

MongoDB (GridFS field) -&gt; mongo-connector plugin -&gt; attachment plugin -&gt; ES

For us, it is impossible to have an XML String in MongoDB because it is too large (&gt; 16 MB ***), so we need to store it as Binary (using GridFS) and then, we use ES to do indexed search onto this XML.

Do you know another way to work with XML which is in Base64? If I have to help you to implement this converter in your API, let me know ;)

Many thanks again!

Regards,
Paco.

**\* MongoDB has a maximum document size of 16 MB.
</comment><comment author="dadoonet" created="2016-01-22T19:01:13Z" id="174012893">I think you should create a batch which extracts your docs then parse them with logstash or whatever to have structured data.
</comment><comment author="pakkk" created="2016-01-22T19:12:22Z" id="174015467">Or...before inserting to MongoDB, we parse the data and convert to one of the expected:

MS Office docs: .doc, .docx, .xls, .xlsx, .ppt, .pptx
TXT docs: .rtf, .txt, .csv
oOo docs: .odt, .sxw, .ods, .sxc, .odp, .sxi
PDF docs: .pdf

...And then inserts it as GridFS.

Do you agree with this other way?

Many thanks again!
</comment><comment author="dadoonet" created="2016-01-22T19:23:12Z" id="174017871">But then why would you need to use the mapper plugin? 

Just send the data as a string to elasticsearch as I explained:

``` js
PUT /test/type/1?refresh=true
{
  "file": "&lt;?xml version=\"1.0\"?&gt;&lt;catalog&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Spring-Boot&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Author Test&lt;/author&gt;&lt;title&gt;Title Test&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Paco&lt;/author&gt;&lt;title&gt;Elastic Search&lt;/title&gt;&lt;/book&gt;&lt;book id=\"bk101\"&gt;&lt;author&gt;Nobody&lt;/author&gt;&lt;title&gt;Nothing&lt;/title&gt;&lt;/book&gt;"
}
```
</comment><comment author="pakkk" created="2016-01-22T19:37:53Z" id="174022177">Hello,

Mapper plugin is the only component which can upload elements from MongoDB to ES when the field content is binary. The following project "mongo-connector":

https://github.com/mongodb-labs/mongo-connector/wiki/Usage%20with%20ElasticSearch

It interacts between MongoDB and ElasticSearch when there is any change in the database. So, when the field type is binary (GridFS), "mongo-connector" needs any plugin to convert to "readable" field in ES (this is your attachment plugin).

As I explained previously, our problem is because the size of the XML is greater than 16 MB, so we are forced to convert to GridFS (binary type).

Finally, we thought we could use ES for all the fields because we have a problem with this strange case, so we could take it to do the search for all the fields in ES, and not only for one specific field like that.

Do you agree with my comment?

Many thanks again.

Regards,
Paco.
</comment><comment author="clintongormley" created="2016-01-29T10:33:28Z" id="176684516">Let's add logging for failure cases, and possibly for partial failures (eg truncation) and leave it at that.  When we move this functionality to node ingest we can be more flexible. #16303
</comment><comment author="clintongormley" created="2016-03-01T13:23:20Z" id="190723179">Moving to the ingest attachment plugin, given the mapper attachment plugin is deprecated
</comment><comment author="pozhidaevak" created="2017-02-06T21:26:06Z" id="277818647">
I modified this example to work with Ingest plugin
And in the case of invalid XML (as in the example) I receive parsing error, so the user is warned.
```HTTP
PUT /_ingest/pipeline/attachment HTTP/1.1
{
  "description" : "Extract attachment information",
  "processors" : [
    {
      "attachment" : {
        "field" : "data"
    }
    }
  ]
}

PUT /my_index/my_type/my_id?pipeline=attachment
{
  "data": "PD94bWwgdmVyc2lvbj0iMS4wIj8+PGNhdGFsb2c+PGJvb2sgaWQ9ImJrMTAxIj48YXV0aG9yPlBhY288L2F1dGhvcj48dGl0bGU+U3ByaW5nLUJvb3Q8L3RpdGxlPjwvYm9vaz48Ym9vayBpZD0iYmsxMDEiPjxhdXRob3I+QXV0aG9yIFRlc3Q8L2F1dGhvcj48dGl0bGU+VGl0bGUgVGVzdDwvdGl0bGU+PC9ib29rPjxib29rIGlkPSJiazEwMSI+PGF1dGhvcj5QYWNvPC9hdXRob3I+PHRpdGxlPkVsYXN0aWMgU2VhcmNoPC90aXRsZT48L2Jvb2s+PGJvb2sgaWQ9ImJrMTAxIj48YXV0aG9yPk5vYm9keTwvYXV0aG9yPjx0aXRsZT5Ob3RoaW5nPC90aXRsZT48L2Jvb2s+PC9jYXRhbG9nPg=="
}
```


If XML is valid I receive next answer
```json
{
  "_index": "my_index",
  "_type": "my_type",
  "_id": "my_id",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
  },
  "created": true
}
```

If it is not valid (as in this issue) then the parse error will be received: 
```json
{
  "error": {
    "root_cause": [
      {
        "type": "exception",
        "reason": "java.lang.IllegalArgumentException: ElasticsearchParseException[Error parsing document in field [data]]; nested: TikaException[XML parse error]; nested: SAXParseException[XML document structures must start and end within the same entity.];",
        "header": {
          "processor_type": "attachment"
        }
      }
    ],
    "type": "exception",
    "reason": "java.lang.IllegalArgumentException: ElasticsearchParseException[Error parsing document in field [data]]; nested: TikaException[XML parse error]; nested: SAXParseException[XML document structures must start and end within the same entity.];",
    "caused_by": {
      "type": "illegal_argument_exception",
      "reason": "ElasticsearchParseException[Error parsing document in field [data]]; nested: TikaException[XML parse error]; nested: SAXParseException[XML document structures must start and end within the same entity.];",
      "caused_by": {
        "type": "parse_exception",
        "reason": "Error parsing document in field [data]",
        "caused_by": {
          "type": "tika_exception",
          "reason": "XML parse error",
          "caused_by": {
            "type": "s_a_x_parse_exception",
            "reason": "XML document structures must start and end within the same entity."
          }
        }
      }
    },
    "header": {
      "processor_type": "attachment"
    }
  },
  "status": 500
}
```
The behaviour seems to be correct. So, I'd suggest to close this issue. 

PS
Sorry if misunderstood something. This is my first triage :)</comment><comment author="dadoonet" created="2017-02-06T23:31:44Z" id="277849100">Agreed. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moved http settings to the new settings infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16188</link><project id="" key="" /><description>Moved http settings to the new settings infrastructure

```
The following settings were moved to Setting contents in HttpTransportSettings:

* http.cors.allow-origin
* http.port
* http.publish_port
* http.detailed_errors.enabled
* http.max_content_length
* http.max_chunk_size
* http.max_header_size
* http.max_initial_line_length

The following settings were removed:

* http.port
* http.netty.port
* http.netty.publish_port
* http.netty.max_content_length
* http.netty.max_chunk_size
* http.netty.max_header_size
* http.netty.max_initial_line_length
```
</description><key id="128194685">16188</key><summary>Moved http settings to the new settings infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T16:41:52Z</created><updated>2016-02-01T13:04:01Z</updated><resolved>2016-02-01T13:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-26T13:45:17Z" id="175019903">@rmuir the static initialization comment you made on the other PR from boaz applies here too? --&gt; https://github.com/elastic/elasticsearch/pull/16200
</comment><comment author="rmuir" created="2016-01-26T14:15:23Z" id="175034726">Yeah. We really need to factor netty to a module...
</comment><comment author="s1monw" created="2016-01-26T14:29:57Z" id="175044508">@bleskes @colings86 can you move all these settings into a separate class such that we don't load any netty code on static init?
</comment><comment author="colings86" created="2016-01-28T17:01:00Z" id="176283225">@s1monw I pushed a commit that moves the HTTP settings constants into a dedicated class
</comment><comment author="s1monw" created="2016-02-01T10:33:22Z" id="177901516">left one comment - LGTM 
</comment><comment author="s1monw" created="2016-02-01T12:50:55Z" id="177957601">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate network service to the new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16187</link><project id="" key="" /><description>This commit migrates all the settings under network service to the new settings infra.

It also adds some chaining utils to make fall back settings slightly less verbose.

Breaking (but I think acceptable)  - network.tcp.no_delay and network.tcp.keep_alive used to accept the value `default` which make us not set them at all on netty. Our default was true so we weren't using this feature. I removed it and now we only accept a true boolean.
</description><key id="128187183">16187</key><summary>Migrate network service to the new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T16:02:54Z</created><updated>2016-01-22T16:55:49Z</updated><resolved>2016-01-22T16:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T16:12:11Z" id="173964481">LGTM thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date processor: simplify switch to identify the specified date format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16186</link><project id="" key="" /><description>Date processor: simplify switch to identify the specified date format
</description><key id="128181072">16186</key><summary>Date processor: simplify switch to identify the specified date format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>non-issue</label><label>review</label></labels><created>2016-01-22T15:36:33Z</created><updated>2016-02-08T09:32:51Z</updated><resolved>2016-01-22T15:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-22T15:49:33Z" id="173958258">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor PhraseSuggestionBuilder.DirectCandidateGenerator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16185</link><project id="" key="" /><description>As a prerequisite for refactoring the whole PhraseSuggestionBuilder to be able to be parsed and streamed from the coordinating node, the DirectCandidateGenerator must implement Writeable, be able to parse a new instance (fromXContent()) and later when transported to the shard to generate a PhraseSuggestionContext.DirectCandidateGenerator. Also adding equals/hashCode and tests and moving DirectCandidateGenerator to its own DirectCandidateGeneratorBuilder class.

Relates to #10217 
</description><key id="128177009">16185</key><summary>Refactor PhraseSuggestionBuilder.DirectCandidateGenerator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T15:19:20Z</created><updated>2016-03-10T18:58:37Z</updated><resolved>2016-02-01T12:17:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-29T16:20:34Z" id="176842867">left a couple of minor comments but it LGTM
</comment><comment author="cbuescher" created="2016-02-01T12:26:26Z" id="177949401">@colings86 thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added jmx agent support to elasticsearch.in.sh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16184</link><project id="" key="" /><description>Enable JMX Through environment variables.
</description><key id="128165250">16184</key><summary>added jmx agent support to elasticsearch.in.sh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BobVanB</reporter><labels /><created>2016-01-22T14:18:11Z</created><updated>2016-06-24T09:34:42Z</updated><resolved>2016-06-24T09:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BobVanB" created="2016-01-22T14:39:45Z" id="173937515">I did signed the CLA!
</comment><comment author="jasontedor" created="2016-06-24T09:34:31Z" id="228302023">Thanks for the pull request! However, all JVM flags have been migrated away from the scripts and are instead in the jvm.options files. Yet, I do not think that we need to add these there (in a commented form, of course) because most users will not need this and it just complicates the default configuration file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert client.transport settings to new infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16183</link><project id="" key="" /><description /><key id="128164690">16183</key><summary>Convert client.transport settings to new infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T14:14:50Z</created><updated>2016-01-22T14:25:56Z</updated><resolved>2016-01-22T14:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-22T14:22:27Z" id="173932984">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move discovery.* settings to new Setting infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16182</link><project id="" key="" /><description /><key id="128157781">16182</key><summary>Move discovery.* settings to new Setting infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T13:33:24Z</created><updated>2016-01-22T16:51:24Z</updated><resolved>2016-01-22T14:36:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-01-22T13:36:01Z" id="173923207">@dadoonet I changed the semantics of "discovery.azure.host.type" to fail if it is incorrectly set instead of falling back to the default. Now that these settings are checked on startup, we should fail early if a host is wrongly-configured. If you don't agree, I can revert that change.
</comment><comment author="danielmitterdorfer" created="2016-01-22T13:59:53Z" id="173927781">I've left a few comments. Btw, I agree with your proposal of failing early on improperly configured host types.
</comment><comment author="s1monw" created="2016-01-22T14:01:52Z" id="173928165">I love it! thanks @ywelsch LGTM aside of @danielmitterdorfer comments
</comment><comment author="dadoonet" created="2016-01-22T14:22:04Z" id="173932928">@ywelsch I do agree that it's definitely better to fail sooner than later.
</comment><comment author="ywelsch" created="2016-01-22T14:28:14Z" id="173934928">pushed a change addressing the 2 comments. @danielmitterdorfer can you have a quick look again?
</comment><comment author="danielmitterdorfer" created="2016-01-22T14:31:27Z" id="173935667">@ywelsch: LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change over to o.e.common.settings.Setting for http settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16181</link><project id="" key="" /><description>Added support for these: 

http.cors.allow-credentials
http.cors.enabled
http.detailed_errors.enabled
http.enabled
http.pipelining
http.cors.max-age
</description><key id="128157318">16181</key><summary>Change over to o.e.common.settings.Setting for http settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T13:30:24Z</created><updated>2016-01-22T17:37:54Z</updated><resolved>2016-01-22T15:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T14:03:03Z" id="173928587">LGTM @markharwood 
</comment><comment author="markharwood" created="2016-01-22T15:20:25Z" id="173948108">Pushed in https://github.com/elastic/elasticsearch/commit/d371ef35f415a5b44c06a726e926e43318edf7e7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert "path.*" and "pidfile" to new settings infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16180</link><project id="" key="" /><description /><key id="128157193">16180</key><summary>Convert "path.*" and "pidfile" to new settings infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T13:29:33Z</created><updated>2016-02-01T17:28:40Z</updated><resolved>2016-01-22T14:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T13:45:47Z" id="173925015">left a minor comment LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make disabled fielddata loading fail earlier.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16179</link><project id="" key="" /><description>Currently this fails when loading data from a segment, which means that it will
never fail on an empty index since it does not have segments.

Closes #16135
</description><key id="128154810">16179</key><summary>Make disabled fielddata loading fail earlier.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>bug</label><label>review</label><label>v2.3.0</label></labels><created>2016-01-22T13:17:28Z</created><updated>2016-01-27T08:13:32Z</updated><resolved>2016-01-27T08:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-27T06:43:08Z" id="175434138">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate repository settings to the new settings API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16178</link><project id="" key="" /><description /><key id="128154544">16178</key><summary>Migrate repository settings to the new settings API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T13:15:20Z</created><updated>2016-01-22T15:44:03Z</updated><resolved>2016-01-22T15:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T13:39:57Z" id="173923886">left some comments @tlrx thanks
</comment><comment author="tlrx" created="2016-01-22T15:28:00Z" id="173950088">@s1monw Thanks. I updated the code with your comment: do you want to have another look or can I push this?
</comment><comment author="s1monw" created="2016-01-22T15:33:43Z" id="173952453">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert "indices.*" settings to new infra.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16177</link><project id="" key="" /><description /><key id="128151179">16177</key><summary>Convert "indices.*" settings to new infra.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T12:54:04Z</created><updated>2016-01-22T14:08:58Z</updated><resolved>2016-01-22T14:08:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T13:20:59Z" id="173919092">left some comments - LGTM otherwise
</comment><comment author="s1monw" created="2016-01-22T13:46:21Z" id="173925198">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make security non-optional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16176</link><project id="" key="" /><description>2.x has show so far that running with security manager is the way to go.
This commit make this non-optional. Users that need to pass their own rules
can still do this via the system configuration for the security manager. They
can even opt out of all security that way.
</description><key id="128143676">16176</key><summary>Make security non-optional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>das awesome</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T12:08:27Z</created><updated>2016-02-12T17:08:05Z</updated><resolved>2016-02-11T16:37:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-22T14:54:20Z" id="173941001">+1
</comment><comment author="rjernst" created="2016-01-22T15:18:16Z" id="173947567">LGTM
</comment><comment author="clintongormley" created="2016-02-01T08:53:25Z" id="177860059">@s1monw You'll want to make the appropriate changes here too: https://github.com/elastic/elasticsearch/blob/master/docs/reference/modules/scripting/security.asciidoc#disable-the-java-security-manager
</comment><comment author="s1monw" created="2016-02-09T15:48:14Z" id="181921830">@clintongormley can you take another look?
</comment><comment author="clintongormley" created="2016-02-11T15:57:17Z" id="182931824">LGTM
</comment><comment author="uschindler" created="2016-02-12T17:08:05Z" id="183415361">Cool, thanks! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate gateway settings to the new settings API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16175</link><project id="" key="" /><description /><key id="128142379">16175</key><summary>Migrate gateway settings to the new settings API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T11:58:45Z</created><updated>2016-01-22T12:06:08Z</updated><resolved>2016-01-22T12:06:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T12:03:02Z" id="173900211">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to disable jarhell check for tests on 2.2 with `test.jarhell.check`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16174</link><project id="" key="" /><description>This relates to #16042 where we agreed on adding an opt-out on 2.2 for test
to disable jarhell checks in the BootstrapForTesting.java - for other branches we
will use different solutions.
</description><key id="128138571">16174</key><summary>Allow to disable jarhell check for tests on 2.2 with `test.jarhell.check`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-22T11:37:02Z</created><updated>2016-03-13T13:41:51Z</updated><resolved>2016-01-22T11:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-22T11:37:22Z" id="173892769">@clintongormley can you take a look?
</comment><comment author="clintongormley" created="2016-01-22T11:51:36Z" id="173897398">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geoip processor: remove redundant latitude and longitude fields and make location an object with lat and lon subfields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16173</link><project id="" key="" /><description>Geoip processor: remove redundant latitude and longitude fields and make location an object with lat and lon subfields
</description><key id="128136680">16173</key><summary>Geoip processor: remove redundant latitude and longitude fields and make location an object with lat and lon subfields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T11:27:47Z</created><updated>2016-02-08T09:28:05Z</updated><resolved>2016-01-22T11:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-22T11:58:26Z" id="173899469">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert several node and test level settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16172</link><project id="" key="" /><description>this PR converts  `search.default_keep_alive` and `search.keep_alive_interval` to the new infra, special cases `tests.portfile` and removes  `test.cluster.node.seed` entirely
</description><key id="128135434">16172</key><summary>Convert several node and test level settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T11:21:21Z</created><updated>2016-01-22T16:53:53Z</updated><resolved>2016-01-22T15:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-22T13:11:31Z" id="173916806">LGTM
</comment><comment author="danielmitterdorfer" created="2016-01-22T13:18:18Z" id="173917899">Left one minor comment. Apart from that: LGTM.
</comment><comment author="s1monw" created="2016-01-22T14:50:50Z" id="173940162">@ywelsch  @danielmitterdorfer I pushed a new rebased commit - sorry I messed too many things up :)
</comment><comment author="ywelsch" created="2016-01-22T14:55:01Z" id="173941141">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix missing break line before unordered item list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16171</link><project id="" key="" /><description /><key id="128126005">16171</key><summary>Fix missing break line before unordered item list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmydlarz</reporter><labels><label>docs</label></labels><created>2016-01-22T10:24:40Z</created><updated>2016-01-22T13:13:52Z</updated><resolved>2016-01-22T13:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-22T13:00:54Z" id="173914507">thanks @dmydlarz 
</comment><comment author="dmydlarz" created="2016-01-22T13:13:52Z" id="173917166">You're welcome :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reverted new completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16170</link><project id="" key="" /><description>The new completion suggester is a breaking change that we should not introduce in a minor version. Instead it will be introduced in version 3.0
</description><key id="128119614">16170</key><summary>Reverted new completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-22T09:46:33Z</created><updated>2016-01-22T15:07:55Z</updated><resolved>2016-01-22T10:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-22T10:38:43Z" id="173874243">merged into 2.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Malformed multi-match query doesn't throw exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16169</link><project id="" key="" /><description>In our installation, based on Elasticsearch 2.1.1, the number of results depends on the position of the 'size' key in the request.
e.g.

```
curl -XPOST localhost:9200/hep/_search -d '{
    "size": 25,
    "query": {
        "multi_match": {
            "message": {
                "fields": [
                    "title^3",
                    "title.raw^10",
                    "abstract^2",
                    "abstract.raw^4",
                    "author^10",
                    "author.raw^15",
                    "reportnumber^10",
                    "eprint^10",
                    "doi^10"
                ],
                "operator": "or",
                "query": "ellis ",
                "zero_terms_query": "all"
            }
        }
    }
}' -o /tmp/size
```

Vs.

```
curl -XPOST localhost:9200/hep/_search -d '{
    "query": {
        "multi_match": {
            "message": {
                "fields": [
                    "title^3",
                    "title.raw^10",
                    "abstract^2",
                    "abstract.raw^4",
                    "author^10",
                    "author.raw^15",
                    "reportnumber^10",
                    "eprint^10",
                    "doi^10"
                ],
                "operator": "or",
                "query": "ellis ",
                "zero_terms_query": "all"
            }
        }
    },
    "size":25
}' -o /tmp/size2
```

Gives two different results:

```
wc /tmp/size*
0 2293415 24129993 /tmp/size
0 697658 7396394 /tmp/size2
```

Somehow when `size` is last, it is ignored (and default value of 10 is used).

Note that on ES 2.0.0 the `size argument` is respected in both cases.

cc: @jalavik, @jmartinm
</description><key id="128117585">16169</key><summary>Malformed multi-match query doesn't throw exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">kaplun</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-22T09:33:25Z</created><updated>2016-02-13T22:06:55Z</updated><resolved>2016-02-13T22:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-22T12:52:19Z" id="173912293">Hi @kaplun 

Your multi_match query is malformed. It shouldn't have the `message` level.  The bug is that we should throw an exception when we see a malformed multi_match query.  
</comment><comment author="kaplun" created="2016-01-22T12:59:42Z" id="173914115">Hi @clintongormley,

but would that explain the fact that we still obtain results, and the number of them depends on where `size` is placed?
</comment><comment author="clintongormley" created="2016-01-22T13:06:47Z" id="173915771">@kaplun yes - the parser for multi-match is stepping into the `message` section, but then isn't stepping out of it properly, so it ignores everything after that.
</comment><comment author="cbuescher" created="2016-02-02T22:16:55Z" id="178856712">@clintongormley Just checked, we catch this kind of error on master already. Does it make sense to fix this on the 2.x branch? It's a small fix I think.
</comment><comment author="clintongormley" created="2016-02-13T22:06:54Z" id="183764820">@cbuescher it's been like this since 0.x - i think it is ok to leave it as it is. thanks for checking
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run Metadata upgrade tool on every version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16168</link><project id="" key="" /><description>Today we run the metadata upgrade only on the current major version
but this should run on every upgrade at least once to ensure we don't miss
an important check or upgrade.
</description><key id="128107359">16168</key><summary>Run Metadata upgrade tool on every version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T08:31:27Z</created><updated>2016-01-22T09:04:54Z</updated><resolved>2016-01-22T09:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-22T08:45:23Z" id="173847267">LGTM
</comment><comment author="jpountz" created="2016-01-22T08:52:46Z" id="173850154">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ap-northeast-2 (seoul) endpoints for EC2 discovery and S3 snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16167</link><project id="" key="" /><description>Fix for #16166 - added endpoint for AWS region ap-northeast-2
</description><key id="128064145">16167</key><summary>Add ap-northeast-2 (seoul) endpoints for EC2 discovery and S3 snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">malpani</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>enhancement</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-22T01:55:17Z</created><updated>2016-01-25T07:47:58Z</updated><resolved>2016-01-25T07:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-25T07:35:39Z" id="174426943">Merged in master with b24dde88de8ea4fd7d62e4518e36c99a84abb2e7 and in 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ec2-discovery support for Seoul</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16166</link><project id="" key="" /><description>AWS recently launched in Seoul and the ec2-discovery plugin needs an endpoint update for the same. I will share a pull request for the same
</description><key id="128062109">16166</key><summary>ec2-discovery support for Seoul</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">malpani</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label></labels><created>2016-01-22T01:38:53Z</created><updated>2016-01-25T07:36:02Z</updated><resolved>2016-01-25T07:36:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-25T07:36:02Z" id="174427122">Fixed by #16167 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update function-score-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16165</link><project id="" key="" /><description>Make explanatory text match value used in example body
</description><key id="128044569">16165</key><summary>Update function-score-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlenRSmith</reporter><labels><label>docs</label></labels><created>2016-01-21T23:19:53Z</created><updated>2016-01-22T12:02:37Z</updated><resolved>2016-01-22T12:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-22T12:02:36Z" id="173900140">thanks @GlenRSmith - I mage the change I suggested and merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Third level of aggregation sometimes results in missing sub-bucket data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16164</link><project id="" key="" /><description>When I ask ES for an aggregation, by time, of the sum of a size by ID; I get a result where some events work correctly (end up in bucket with the correct term key), but other data, seemingly randomly, ends up with a blank term key

For example, with this as my aggs

```
 "aggs": {
    "2": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1m",
        "time_zone": "America/Los_Angeles",
        "min_doc_count": 1,
        "extended_bounds": {
          "min": 1452209723842,
          "max": 1452214986379
        }
      },
      "aggs": {
        "3": {
          "terms": {
            "field": "collection_id.raw",
            "size": 0,
            "order": {
              "1": "desc"
            }
          },
          "aggs": {
            "1": {
              "sum": {
                "field": "size"
              }
            }
          }
        }
      }
    }
  }
```

I get the the following result 
...

```
 "aggregations": {
    "2": {
      "buckets": [
        {
          "3": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
-----&gt;      "buckets": []         &lt;---- NOTE: This is where the data is missing
          },
          "key_as_string": "2016-01-07T15:38:00.000-08:00",
          "key": 1452209880000,
          "doc_count": 1
        },
        {
          "3": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
              {
                "1": {
                  "value": 140355866
                },
                "key": "19488",
                "doc_count": 43
              }
            ]
          },
          "key_as_string": "2016-01-07T16:59:00.000-08:00",
          "key": 1452214740000,
          "doc_count": 43
        },
        {
          "3": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
              {
                "1": {
                  "value": 63037240
                },
                "key": "19488",
                "doc_count": 8
              }
            ]
          },
          "key_as_string": "2016-01-07T17:01:00.000-08:00",
          "key": 1452214860000,
          "doc_count": 8
        }
      ]
    }
  }
```

Note that I get three buckets. Two of them have a value in their "sub" `buckets`, one of them does not. 

If i dig into the Event data, I see nothing odd about them, Here is a snippet for an event that does get into a well defined bucket, 

```
@timestamp      January 7th 2016, 16:59:32.000
t@version       1
t_id        AVIevu0_l2HWhQn5VZHu
t_index     logstash-2016.01.08
...     
collection_id       19488
```

and one that does not. Note they have all the same keys, and very similar values. 

```
@timestamp      January 7th 2016, 15:38:33.000
t@version       1
t_id        AVIedMH2l2HWhQn5U0QH
t_index     logstash-2016.01.07
...
collection_id       19456
```

No idea what is going on, and what makes some data special and others not so much.
</description><key id="128044560">16164</key><summary>Third level of aggregation sometimes results in missing sub-bucket data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdrummond</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-01-21T23:19:49Z</created><updated>2016-02-28T22:26:08Z</updated><resolved>2016-02-24T17:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-22T11:47:08Z" id="173895448">Hi @zdrummond 

You're using extended bounds, so there may well be buckets with no data in them.  Are you absolutely sure that there are docs that should be in that bucket?  Could you post a simple curl recreation showing: the create index/put mapping command, indexing some docs, then running the agg which shows the problem?

Also, what version of ES are you on?
</comment><comment author="zdrummond" created="2016-01-22T19:11:14Z" id="174015188">Thanks @clintongormley for the quick response.

We are on 2.1.0

Unfortunately I can not reliable reproduce the problem, I wish I could.

As for the ' absolutely sure that there are docs that should be in that bucket'.. as sure as I can be. If you note above my aggs are on **timestamp** by **sum(size)** by **collection_id**. The **timestamp** By **size** always works. The **collection_id** is the item that sometimes works and sometimes does not. 

The example above also shows two events I picked from the same time range. Their keys are all the same (but values differ). They both have collection_id. Yet one ends up in a bucket, and the other ends up in a null bucket. 

Note: this is at a high level ELK... I have a chart that has missing data, but i was able to narrow down the issue to the empty bucket issue with aggs in ES. 
</comment><comment author="clintongormley" created="2016-01-26T10:27:27Z" id="174945693">Hiya @zdrummond 

Is there any chance that you could provide us (privately) with a copy of the index and the full query that is being sent?
</comment><comment author="zdrummond" created="2016-01-26T17:45:36Z" id="175138041">I think so. How would I get it to you?
</comment><comment author="clintongormley" created="2016-01-26T17:48:53Z" id="175139417">You could put it in dropbox or S3 and send me the link?  clinton at elastic dot co
</comment><comment author="colings86" created="2016-02-10T11:48:09Z" id="182328618">@zdrummond I took a look at the data you provided us. It looks like there are documents that have a `collection_id` field but do not have a `collection_id.raw` field. You can see this if you add a missing aggregation alongside `"3"` looking at the field `collection_id.raw`.

Did you add the multifield for `collection_id.raw` after you have done some indexing into 07-01-2016? Or maybe the template mapping for your logstash indices is not being applied correctly anymore?
</comment><comment author="zdrummond" created="2016-02-10T21:17:57Z" id="182585288">So the good news is, removing `.raw` in fact makes the query return the correct results. 

The bad news is, we have made no any mapping changes, and the issue is persistent, not just on that date. Also, if there is a mapping issue, I would understand it across different sources, but this is coming from the same source (a single server responsible for one job)

That said, we have pretty much just set up ELK and started sending it data. We likely need to clean up the mappings holistically. Maybe if we nail down the mappings across all sources this issue would clear up.                                                                                                                    
</comment><comment author="colings86" created="2016-02-11T09:17:59Z" id="182777526">Do you use a custom template in your logstash output at all?
</comment><comment author="pmorton" created="2016-02-12T23:06:09Z" id="183520196">@colings86 We allowed logstash to use the default dynamic template. We did not specify a mapping we have logstash setup to tag these messages as the same type so they should share the same schema.
</comment><comment author="clintongormley" created="2016-02-13T12:10:39Z" id="183655232">I wonder if the docs with the missing `.raw` field are all from the first documents in the index, ie whether there was some race condition.  Or alternatively, whether all the documents missing the `.raw` field are on the same shard.

A number of mapping bugs have been cleaned up in 2.2.0 - also wonder if this problem has already been solved.  @zdrummond any chance you can upgrade and see if the issue disappears on new indices?
</comment><comment author="zdrummond" created="2016-02-17T17:07:16Z" id="185301554">Sure. We will plan a roll out of 2.2
</comment><comment author="zdrummond" created="2016-02-24T17:32:11Z" id="188368161">@clintongormley and @colings86 We upgraded to 2.2 and we also did some hand mapping of data types and now the issue is resolved. Thanks!
</comment><comment author="clintongormley" created="2016-02-28T22:26:08Z" id="189957725">thanks @zdrummond 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added plumbing for compile time script parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16163</link><project id="" key="" /><description>Back port for #15464 PR to 2.x branch.
</description><key id="128038783">16163</key><summary>Added plumbing for compile time script parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v2.3.0</label></labels><created>2016-01-21T22:42:42Z</created><updated>2016-01-22T11:35:06Z</updated><resolved>2016-01-21T22:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-01-21T22:46:16Z" id="173740039">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integrity checks for read-only indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16162</link><project id="" key="" /><description>The checksum verification that we perform on merge or relocation has proved very useful to detect corruption but it is only triggered on merge. So static indices or segments that already reached the maximum segment size are never checked, even though they are as likely to get corrupted.

It would be nice to have an API, a command-line tool or even to figure out a way to run integrity checks automatically (anything that is easier than running CheckIndex on the shard directories)?
</description><key id="128028008">16162</key><summary>Integrity checks for read-only indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>feature</label></labels><created>2016-01-21T21:51:33Z</created><updated>2017-07-19T08:56:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T10:55:21Z" id="176692265">+1 - should probably integrate with the task management api
</comment><comment author="martijnvg" created="2017-07-19T08:56:17Z" id="316318973">When discussing whether the experimental tag should be removed from `index.shard.check_on_startup` index setting in #19798, we came to the conclusion that performing a checkindex when a shard is starting is maybe not the best time. It can slow down recovery significantly. 

Instead like this issue suggested an api is better, because there is full control when a check index is performed. The api should mimic the checkindex tool as close as possible. Exposing its `verbose`, `fast`, `segment` and `crossCheckTermVectors` options:

```
POST /{index}/_check_index?verbose=true|false&amp;fast=true|false&amp;segement=[segment_id]&amp;crossCheckTermVectors=true|false
```

The `exorcise` option should not be exposed as it modifies the index by removing segments that have issues. 

The check index api should be executed on a closed index. Making this api work on open indices is tricky, because the underlying Lucene index can change while check index is running. However I think we can make this api work on read only indices.

The check index api should replace the `index.shard.check_on_startup` index setting.

@s1monw If I remember correctly you prefer a command line utility instead of an api for things like check index. However I think in this case check index I think an api is preferred? The current proposal doesn't modify shards and checking the integrity of an index via an api can be useful for curator like tools. Do you agree?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the `index` property a boolean.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16161</link><project id="" key="" /><description>With the split of `string` into `text` and `keyword`, the `index` property can
only have two values and should be a boolean.
</description><key id="128008618">16161</key><summary>Make the `index` property a boolean.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T20:07:13Z</created><updated>2016-01-27T08:10:50Z</updated><resolved>2016-01-27T08:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T20:30:20Z" id="173698901">Looks good. Maybe add a backcompat test for the old ways that were supported, and see it still works with created version before 3.0?
</comment><comment author="jpountz" created="2016-01-22T09:37:35Z" id="173861740">@rjernst @jasontedor Thanks for the reviews, I pushed a new commit.
</comment><comment author="jasontedor" created="2016-01-23T15:24:05Z" id="174193494">LGTM.
</comment><comment author="jpountz" created="2016-01-27T08:10:49Z" id="175473285">Fixed via 209860854ddc87ff71a49d9ced1186a8e2c8e3b8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.4.1.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16160</link><project id="" key="" /><description /><key id="128008538">16160</key><summary>Upgrade to lucene-5.4.1.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label></labels><created>2016-01-21T20:06:39Z</created><updated>2016-01-22T12:38:40Z</updated><resolved>2016-01-22T08:35:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T20:26:12Z" id="173697918">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default standard output to the journal in systemd</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16159</link><project id="" key="" /><description>This commit modifies the default setting for standard output in the
systemd configuration to the journal instead of /dev/null. This is to
address a user pain point where Elasticsearch would fail to start but
the error message would be sent to standard output and therefore
/dev/null leading to difficult-to-debug situations.

Relates #15315, relates #16134 
</description><key id="128007242">16159</key><summary>Default standard output to the journal in systemd</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T19:59:42Z</created><updated>2016-01-22T13:02:19Z</updated><resolved>2016-01-22T11:13:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T20:25:12Z" id="173697689">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot to Azure Blob-storage is failing with IndexShardSnapshotFailedException (nested: IOException; nested: StorageException)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16158</link><project id="" key="" /><description>I am trying to take a snapshot of our existing indices to Azure Cloud Storage and it is failing with `IndexShardSnapshotFailedException (nested: IOException; nested: StorageException)`

We are using:
`ES Version: 2.1.1`
`Cloud-Azure Version: Latest (2.1.1)` - installed using `plugin install cloud-azure`

Configuration in `elasticsearch.yml` is:

```
cloud:
    azure:
        storage:
            account: "azurecloudstoragenameXXX"
            key: "azurecloudstoragekeyXXX"
```

I am trying to create a new repository using following:

```
PUT _snapshot/onazure?verify=false
{
  "type" : "azure",
  "settings" : {
    "container" : "esbackup",
    "compressed" : true
  }
}
```

Once the repository has been created, I am trying to take a snapshot using following:

```
PUT _snapshot/onazure/snapshot-1-01212016-1920
{
  "indices": [
    "index1",
    "index2",
    "index3",
    "index4"
  ]
}
```

Every time, I am trying to take a snapshot, it is failing with the following exception and fails to take a snapshot. Exception:

```
[2016-01-21 19:19:03,328][WARN ][snapshots                ] [node-1] [[index1][1]] [onazure:snapshot-1-01212016-1920] failed to create snapshot
[index1][[index1][1]] IndexShardSnapshotFailedException[Failed to perform snapshot (index files)]; nested: IOException; nested: StorageException[Value for one of the query parameters specified in the request URI is invalid.];
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:606)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:191)
    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:340)
    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:76)
    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:296)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException
    at com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:598)
    at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:374)
    at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:358)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    ... 3 more
Caused by: com.microsoft.azure.storage.StorageException: Value for one of the query parameters specified in the request URI is invalid.
    at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)
    at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)
    at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)
    at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlockInternal(CloudBlockBlob.java:714)
    at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlock(CloudBlockBlob.java:686)
    at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:362)
    ... 7 more
[2016-01-21 19:19:04,407][INFO ][snapshots                ] [node-1] snapshot [onazure:snapshot-1-01212016-1920] is done
```

I tried running the snapshot with only one index which has almost (250 GB) of data, and it is still failing.

However, if I use an index (individually or in group while taking a snapshot) which is almost ~5MB in size, I do not get any exception from log. It says: snapshot is done - however, if I check the `esbackup` container underneath the blob storage, it does not contain any blobs.

Also, tried with `"compressed": false` setting and it is not working with that setting as well.
</description><key id="128003803">16158</key><summary>Snapshot to Azure Blob-storage is failing with IndexShardSnapshotFailedException (nested: IOException; nested: StorageException)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tejasavora</reporter><labels><label>:Plugin Repository Azure</label><label>discuss</label></labels><created>2016-01-21T19:42:21Z</created><updated>2016-07-04T11:10:46Z</updated><resolved>2016-07-04T11:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T13:22:12Z" id="190722533">@dadoonet any idea what is going on here?
</comment><comment author="dadoonet" created="2016-03-01T17:07:41Z" id="190817419">I have no idea. Never seen this message from Azure before. I know they have strict policy regarding naming but I don't see anything wrong here.
I need to reproduce it locally. Assigning to myself.
</comment><comment author="dadoonet" created="2016-03-05T12:23:01Z" id="192632516">@tejasavora Could you please run that again and add in `logging.yml` file:

``` yml
cloud.azure.storage: TRACE
```

Or send

```
PUT /_cluster/settings
{
    "transient" : {
        "logger.cloud.azure.storage" : "TRACE"
    }
}
```

Then do the same operation again and paste the logs on gist.github.com or here if not too big?
If you have some critical data inside your logs (like credentials), feel free to hide them.

Thanks.
</comment><comment author="dadoonet" created="2016-07-04T11:10:45Z" id="230266632">No further news. Closing. Feel free to reopen if you can come with more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] rename processor_tag to tag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16157</link><project id="" key="" /><description>`processor_tag` is a bit verbose given that the key name is already within the scope of a processor's config. so `tag` is sufficient.
</description><key id="127970001">16157</key><summary>[Ingest] rename processor_tag to tag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2016-01-21T16:59:12Z</created><updated>2016-02-08T09:32:59Z</updated><resolved>2016-01-21T19:20:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-21T19:14:26Z" id="173678097">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for invalid index settings on metadata upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16156</link><project id="" key="" /><description>this change allows us to open existing IndexMetaData that contains invalid, removed settings
or settings with invalid values and instead of filling up the users disks with exceptions we _archive_
the settings with and `archive.` prefix. This allows us to warn the user via logs (once it's archived) as
well as via external tools like the upgrade validation tool since those archived settings will be preserved
even over restarts etc. It will prevent indices from failing during the allocaiton phase but instead will
print a prominent warning on index metadata recovery from disk.
</description><key id="127964998">16156</key><summary>Check for invalid index settings on metadata upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>das awesome</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T16:36:29Z</created><updated>2016-01-22T08:18:18Z</updated><resolved>2016-01-22T08:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T19:28:47Z" id="173682881">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not apply minimum_should_match on auto generated boolean query if the coordination factor is disabled.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16155</link><project id="" key="" /><description>Affects match, multi_match, query_string and simple_query_string queries.
Direct bool queries are not affected anymore (minimum_should_match is applied even if the coord factor is disabled).
Fixes https://github.com/elastic/elasticsearch/pull/16078 which wrongly applies this logic to every bool query.
</description><key id="127962606">16155</key><summary>Do not apply minimum_should_match on auto generated boolean query if the coordination factor is disabled.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T16:25:41Z</created><updated>2016-01-22T08:29:40Z</updated><resolved>2016-01-22T08:29:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-21T16:27:08Z" id="173624413">@jpountz it's the followup from yesterday's chat:

&gt; Adrien Grand [2:54 PM] 
&gt; hmmI agree this is buggy
&gt; [2:55] 
&gt; this should only happen this way for parsers that generate a query out of a string
&gt; [2:55] 
&gt; like match, query_string or simple_query_string
</comment><comment author="jpountz" created="2016-01-21T19:20:57Z" id="173680601">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Refactoring of term suggester for the new query API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16154</link><project id="" key="" /><description /><key id="127953946">16154</key><summary>WIP: Refactoring of term suggester for the new query API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Query Refactoring</label><label>WIP</label></labels><created>2016-01-21T15:47:36Z</created><updated>2016-02-09T20:43:45Z</updated><resolved>2016-02-09T20:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-25T17:16:10Z" id="174590336">@abeyad Thanks, I left a few general comments, I'm also still trying to wrap my head around the steps we need to take here since the structure of the suggesters is again different from the other parts we did so far, so this should only be seen as directions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated parameters from ScriptSortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16153</link><project id="" key="" /><description>This is in preparation of refactoring.

Relates to #15178
</description><key id="127946640">16153</key><summary>Remove deprecated parameters from ScriptSortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T15:14:26Z</created><updated>2016-01-26T10:38:46Z</updated><resolved>2016-01-26T10:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-22T15:43:55Z" id="173956294">@MaineC left a small comment to investigate, also I'm not sure if we collect java api changes like these anywhere, even if those methods were deprecated in 2.x. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove option to fsync translog on every operation (index.translog.sync_interval: 0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16152</link><project id="" key="" /><description>We still have the option to sync the translog after every operation by setting `index.translog.sync_interval` to 0. This kills performance and does not seem to add any value. We had a few test failures because of that too on 2.1, like http://build-us-00.elastic.co/job/es_core_21_metal/338/ It should be sufficient to sync the translog on each request.

If we want to remove this option we should also think about how to schedule the translog fsyncing. We probably want a lower limit to the interval and decide what that should be. Then we also need a way to handle settings in already created indices that are lower that the chosen lower limit and how to handle old templates that have that.
</description><key id="127944793">16152</key><summary>Remove option to fsync translog on every operation (index.translog.sync_interval: 0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2016-01-21T15:06:57Z</created><updated>2016-01-27T14:48:31Z</updated><resolved>2016-01-27T14:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-21T17:07:37Z" id="173638588">+1
</comment><comment author="jpountz" created="2016-01-21T19:21:40Z" id="173680830">+1
</comment><comment author="brwe" created="2016-01-21T19:26:15Z" id="173682195">&gt; Then we also need a way to handle settings in already created indices that are lower that the chosen lower limit and how to handle old templates that have that.

will be easier on master once #16156 is in
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor GeoSortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16151</link><project id="" key="" /><description>- adds json parsing,
- refactors json serialisation,
- adds writable parsing and serialisation,
- adds json and writable roundtrip test

This relates to #15178

Caveat: Tests are still running locally, so expect changes to fix any failures to come.
</description><key id="127943702">16151</key><summary>Refactor GeoSortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T15:01:52Z</created><updated>2016-02-02T15:10:04Z</updated><resolved>2016-02-02T11:14:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-22T17:25:25Z" id="173984529">@MaineC I wasn't sure if this was ready to review or you want to include the build method as well, but I went ahead and left a couple of minor comments. Already look very good to me besides the missing method that creates the SortField.
</comment><comment author="MaineC" created="2016-01-26T12:54:48Z" id="174998891">@cbuescher Went over your comments and tried to address as many as possible. Left a few questions as well. I kept the fixes in response to your comments in a separate commit, might makes things easier to review.

Does it make sense to merge this and do the switchover to a to be implemented build method in a separate PR?
</comment><comment author="cbuescher" created="2016-01-27T10:03:02Z" id="175523478">@MaineC Thanks, I went through the changes, left some small remarks and idea about relation between the "mode" and "order" parmeter but this is indeed a tricky detail. Maybe setting the default is the right thing here, I'm still wondering what we are then missing on the builder side though.
</comment><comment author="MaineC" created="2016-01-27T13:17:31Z" id="175619650">@cbuescher Made a few changes, feel free to have another look.
</comment><comment author="cbuescher" created="2016-01-27T13:34:32Z" id="175630801">Thanks, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If use an unknown analyzer es 2.1.1 throw  NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16150</link><project id="" key="" /><description>When I try to visit http://my-es-host:9200/_analyze?text=NBA 2K&amp;analyzer=ik  everything is OK, but when I change "analyzer=ik" to "analyzer=blahblahblah" , there is a NullPointerException.  Maybe an exception named "UnknownAnalyzerException"  is better ?  If  other guys meet this NullPointerException , actually they can not understand what to happen .

```
  [2016-01-21 22:53:52,836][ERROR][transport                ] [es2-181-9500] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@43726337]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="127943522">16150</key><summary>If use an unknown analyzer es 2.1.1 throw  NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">node</reporter><labels /><created>2016-01-21T15:01:02Z</created><updated>2016-01-21T19:11:59Z</updated><resolved>2016-01-21T19:11:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-21T19:11:59Z" id="173677496">Fixed by https://github.com/elastic/elasticsearch/pull/15447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop multi data path upgrade tool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16149</link><project id="" key="" /><description>Since we require upgrades to 2.x we don't need this anymore on master.
</description><key id="127943107">16149</key><summary>Drop multi data path upgrade tool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T14:59:07Z</created><updated>2016-01-21T16:50:13Z</updated><resolved>2016-01-21T16:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T16:09:46Z" id="173619481">LGTM
</comment><comment author="s1monw" created="2016-01-21T16:23:20Z" id="173623316">@rjernst thx - I missed one place, can you take another look?
</comment><comment author="rjernst" created="2016-01-21T16:25:44Z" id="173624022">Still looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timezone: use forward slash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16148</link><project id="" key="" /><description>Using a backslash causes errors when querying elasticsearch, but changing the back slash to forward slash on the timezone fixes it.
</description><key id="127937388">16148</key><summary>Timezone: use forward slash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chevin99</reporter><labels><label>docs</label></labels><created>2016-01-21T14:37:15Z</created><updated>2016-01-22T16:49:37Z</updated><resolved>2016-01-22T13:27:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-22T13:27:09Z" id="173921683">thanks @chevin99 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the ability to enable doc values with the `fielddata.format` setting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16147</link><project id="" key="" /><description>Doc values can now only be enabled by setting `doc_values: true` in the
mappings. Removing this feature also means that we can now fail mapping updates
that try to disable doc values.
</description><key id="127932481">16147</key><summary>Remove the ability to enable doc values with the `fielddata.format` setting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T14:13:20Z</created><updated>2016-03-01T23:26:12Z</updated><resolved>2016-01-27T08:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T16:07:34Z" id="173618837">Great change, +1! I think we can do even better as a followup by removing the doc values as a boolean, since field type already has docvalues type?
</comment><comment author="jpountz" created="2016-01-21T17:03:05Z" id="173636284">Agreed this would simplify...
</comment><comment author="jpountz" created="2016-01-27T08:11:10Z" id="175473378">Fixed via 2aaa5e6448fb96dd80c37827b292592d607398bc
</comment><comment author="ppf2" created="2016-03-01T18:52:48Z" id="190850443">I just saw a mapping file out there in production that has fielddata.format set to doc_values.  By removing this option entirely, will this be a breaking change?  If so, can we add the tagging so we can also document this in the breaking changes doc for 5.0? 
</comment><comment author="jpountz" created="2016-03-01T23:12:24Z" id="190955857">@ppf2 This is breaking in the sense that for new indices you will have to use `doc_values: true` instead. However it is backward compatible: existing 2.x indices that use fielddata.format=doc_values will keep working on 5.0.
</comment><comment author="ppf2" created="2016-03-01T23:26:12Z" id="190961466">@jpountz ok thx for clarifying!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be stricter about parsing boolean values in mappings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16146</link><project id="" key="" /><description>Parsing is currently very lenient, which has the bad side-effect that if you
have a typo and pass eg. `store: fasle` this will actually be interpreted as
`store: true`. Since mappings can't be changed after the fact, it is quite bad
if it happens on an index that already contains data.

Note that this does not cover all settings that accept a boolean, but since the
PR was quite hard to build and already covers some main settings like `store`
or `doc_values` this would already be a good incremental improvement.
</description><key id="127926446">16146</key><summary>Be stricter about parsing boolean values in mappings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T13:39:42Z</created><updated>2016-01-27T08:10:18Z</updated><resolved>2016-01-27T08:10:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T16:02:11Z" id="173617266">LGTM
</comment><comment author="jpountz" created="2016-01-27T08:10:18Z" id="175473097">Fixed via 35709f62b65cde39f8ba420559d091e247a6adce
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix IngestMetadata parsing and add unittest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16145</link><project id="" key="" /><description /><key id="127922629">16145</key><summary>Fix IngestMetadata parsing and add unittest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-21T13:20:44Z</created><updated>2016-01-21T13:40:39Z</updated><resolved>2016-01-21T13:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-21T13:36:09Z" id="173572109">Thx @s1monw! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failing replica relocation source should not fail replica relocation target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16144</link><project id="" key="" /><description>If the relocation source fails during the relocation of a shard from one node to another, the relocation target is currently failed as well. For replica shards this is not necessary, however, as the actual shard recovery of the relocation target is done via the primary shard.
</description><key id="127899981">16144</key><summary>Failing replica relocation source should not fail replica relocation target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-21T11:04:19Z</created><updated>2016-05-27T13:28:57Z</updated><resolved>2016-05-27T13:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-29T10:58:54Z" id="176693416">+1 good point
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix the use of nested `inner_hits` in query dsl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16143</link><project id="" key="" /><description>Fix support for using inner hits hierarchically when using nested `has_child`, `has_parent` or `nested` queries in the query dsl.

PR for #11118
</description><key id="127895478">16143</key><summary>Fix the use of nested `inner_hits` in query dsl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2016-01-21T10:41:13Z</created><updated>2017-04-28T08:56:15Z</updated><resolved>2016-04-18T09:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T12:42:53Z" id="174495938">I am not very happy about the added complexity to QueryShardContext. If I understand the issue correctly, the bug only applies when `inner_hits` are specified per query object instead of as a top-level element? If this is true then I would be leaning towards not supporting nested inner hits definitions except in the case that they are specified as a top-level element? 
</comment><comment author="martijnvg" created="2016-01-25T13:09:28Z" id="174504239">The issue only occurs at least two nested inner_hits are specified in an hierarchical way. So this works:

``` json
{
  "query" : {
     "has_child" : {
         "type" : "child",
          "inner_hits" : {},
         "query" : { "match_all" : {} }
     }
  }
}
```

but this does't:

``` json
{
  "query" : {
     "has_child" : {
         "type" : "child",
          "inner_hits" : {}
         "query" : {
             "has_child" : {
                  "type" : "grand_child",
                  "inner_hits" : {},
                  "query" : { "match_all" : {}
             }
         }
     }
  }
}
```

I'm happy with not supporting these kind of `inner_hits` usage to avoid further complexity of the `QueryShardContext`. If this nested `inner_hits` usage is required then top level inner hits can be used: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-inner-hits.html#top-level-inner-hits

I can change this PR to instead of supporting this nested `inner_hits` usage, make the inner hits sub fetch phase fail with a descriptive error when trying the fetch inner hits for top level matches that aren't a direct child or parent of the current hit?
</comment><comment author="jpountz" created="2016-01-25T19:20:48Z" id="174627472">+1 on this plan. @clintongormley could you confirm whether it makes sense to you?
</comment><comment author="clintongormley" created="2016-01-26T11:45:45Z" id="174966579">As far as I can tell, inner hits on grand children aren't supported at all, even with the top-level syntax:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "one": {
          "type": "nested",
          "properties": {
            "two": {
              "type": "nested"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "one": [
    {
      "name": "foo",
      "two": [
        {
          "name": "foo"
        },
        {
          "name": "bar"
        }
      ]
    },
    {
      "name": "bar",
      "two": [
        {
          "name": "foo"
        },
        {
          "name": "bar"
        }
      ]
    }
  ]
}

GET t/_search?_source=false
{
  "query": {
    "nested": {
      "path": "one",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "one.name": "foo"
              }
            },
            {
              "nested": {
                "path": "one.two",
                "inner_hits": {},
                "query": {
                  "match": {
                    "one.two.name": "foo"
                  }
                }
              }
            }
          ]
        }
      }
    }
  }
}

GET t/_search?_source=false
{
  "query": {
    "nested": {
      "path": "one",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "one.name": "foo"
              }
            },
            {
              "nested": {
                "path": "one.two",
                "query": {
                  "match": {
                    "one.two.name": "foo"
                  }
                }
              }
            }
          ]
        }
      }
    }
  },
  "inner_hits": {
    "grandkids": {
      "path": {
        "one.two": {
          "query": {
            "nested": {
              "path": "one",
              "query": {
                "bool": {
                  "must": [
                    {
                      "match": {
                        "one.name": "foo"
                      }
                    },
                    {
                      "nested": {
                        "path": "one.two",
                        "query": {
                          "match": {
                            "one.two.name": "foo"
                          }
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="martijnvg" created="2016-01-26T12:58:08Z" id="174999777">@clintongormley that last search request is incorrect usage. This works:

```
GET t/_search?_source=false
{
  "query": {
    "nested": {
      "path": "one",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "one.name": "foo"
              }
            },
            {
              "nested": {
                "path": "one.two",
                "query": {
                  "match": {
                    "one.two.name": "foo"
                  }
                }
              }
            }
          ]
        }
      }
    }
  },
  "inner_hits": {
    "kids": {
      "path": {
        "one": {
          "query": {
            "match": {
              "one.name": "foo"
            }
          },
          "inner_hits": {
            "grand_kids": {
              "path": {
                "one.two": {
                  "query": {
                    "match": {
                      "one.two.name": "foo"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-01-26T13:05:19Z" id="175001748">ah ok - that wasn't clear from the docs (in fact i struggled to understand the correct syntax for the top-level inner hits)

The inline inner hits is so much easier to write and understand, I'm sad that we have to resort to the top-level version.  I'd much prefer to get inline working, but I understand your reluctance to add complexity to already complex code.  I bow to your decision.
</comment><comment author="martijnvg" created="2016-01-26T13:07:42Z" id="175003142">@clintongormley what I can do is to add more examples in docs regarding this. Also I will improve the error messages. It should be clear what is wrong, which wasn't the case when the error was returned from the initial search request you sent.
</comment><comment author="abulhol" created="2016-02-01T13:33:28Z" id="177973852">Is this not also related to this issue?:
https://github.com/elastic/elasticsearch/issues/11118
I hate to say it, but the top-level inner hits syntax drives me mad.
</comment><comment author="martijnvg" created="2016-02-01T16:07:38Z" id="178045845">@abulhol yes, that is the related issue.
</comment><comment author="clintongormley" created="2016-02-02T14:28:46Z" id="178598462">@abulhol i agree with you - i think we should take another look at that syntax
</comment><comment author="martijnvg" created="2016-02-15T16:58:29Z" id="184300429">Reports like #16664 make doubt about removing the `inner_hits` usage inside the query dsl. The tricky part of top level inner hits is that certain parts of the query need to be repeated (the main query needs to be taken apart and each level needs to be applied correct in top level inner hits). 

What if we instead of removing inner hits in the query dsl, we remove top level inner hits? We would basically get this change in and remove the entire top level inner hit from the search api. Ok, so there is a bit extra complexity in the query dsl, but by removing top level inner hits we would remove complexity too.
</comment><comment author="rpedela" created="2016-02-15T19:58:09Z" id="184363503">+1

I think having only one way to do `inner_hits` is a win for usability and the nested version is more logical in my opinion. The only reason I currently use the top-level version is because I need inner hits for grandchildren.
</comment><comment author="abulhol" created="2016-02-16T10:31:16Z" id="184616387">Since I have understood how the top level inner hits work, I am fine with them. But the documentation (ES reference guide) is really not helpful in its current version, so probably almost anybody will spend a lot of time with trial and error to figure out how they work. 
Please extend this part of the docs and give detailed examples. 
I am happy to review them. 
</comment><comment author="abulhol" created="2016-02-16T10:37:16Z" id="184617713">If you make any changes to the query DSL, please make sure that it is still possible to retrieve inner hits for grandchildren (and grand-grandchildren.... ;-), also if no queries are made on the children/intermediate nested nodes (i.e. the cases where you have to repeat the grandchild query in the child query because the child query will otherwise return all childs, of which 0 to n may have no inner hits for grandchildren).
</comment><comment author="martijnvg" created="2016-04-18T09:15:36Z" id="211289846">Closing in favour of #17816
</comment><comment author="gmoskovicz" created="2016-06-21T16:12:02Z" id="227490446">@martijnvg @jpountz @clintongormley 

Are we planning to bring this to the 2.x branch ? Or until 5.x inner child for a second level will not work?
</comment><comment author="martijnvg" created="2016-06-21T16:18:03Z" id="227492168">@gmoskovicz No, this change is too controversial to push to any branch. For 2.x top level inner hits is the only way multi level nested or parent child works correctly. 
</comment><comment author="gmoskovicz" created="2016-06-21T16:32:36Z" id="227496311">Roger that. Thanks @martijnvg !
</comment><comment author="abhishek5678" created="2017-04-27T14:34:20Z" id="297731737">@martijnvg 
your example is not working for me.
I am also running the same nested example but it's not working.In this data when i am trying with one level nested data then it's working fine but when i am adding one more nested data then it's not working if anybody will be knows about that then please reply me.I am sending my document data and mapping file also.

POST /test_word14/doc/1
{
"name": "Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism (D50-D89)",
"depth": 1,
"children": [{
"name": "Nutritional anemias (D50-D53)",
"depth": 2,
"children": [{
"code": "D50",
"name": "Iron deficiency anemia",
"depth": 3,
"children": [{
"code": "D50.0",
"name": "Iron deficiency anemia secondary to blood loss (chronic)",
"depth": 4
}, {
"code": "D50.1",
"name": "Sideropenic dysphagia",
"depth": 4
}, {
"code": "D50.8",
"name": "Other iron deficiency anemias",
"depth": 4
}, {
"code": "D50.9",
"name": "Iron deficiency anemia, unspecified",
"depth": 4
}]
}]
}]
}

(mapping file)
PUT /test_word16
{
"mappings": {
"doc": {
"properties": {
"name": { "type": "string" },
"depth": {"type":"long"},
"children": {
"type": "nested",
"properties": {
"name": { "type": "string" },
"depth": { "type": "long" },
"children": {
"type": "nested",
"properties": {
"code": { "type": "string" },
"name": { "type": "string" },
"depth": { "type": "long" },
"children": {
"type": "nested",
"properties": {
"code": { "type": "string" },
"name": { "type": "string" },
"depth": { "type": "long" },
}
}
}
}
}
}
}
}
}
}

please tell me what is the query i am applying here.i applied the query like this:
GET /icd10_codes/doc/_search
{
"_source": false,
"query": {
"nested": {
"path": "children",
"query": {
"nested": {
"path": "children.children",
"query": {
"nested": {
"path": "children.children.children",
"query": {
"bool": {
"must": [
{
"match": {
"children.children.name": "Iron"
}
}
]
}
},
"inner_hits": {
"_source": {
"excludes":["name"]
}
}
}
}
}
}
}
}</comment><comment author="gmoskovicz" created="2017-04-27T14:46:23Z" id="297735253">@abhishek5678 This PR was moved to https://github.com/elastic/elasticsearch/pull/17816 . 

For any questions, please feel free to visit https://discuss.elastic.co and add a question there. PR and Issues are reserved for bugs. Thanks for understanding.</comment><comment author="abhishek5678" created="2017-04-28T04:44:01Z" id="297905695">@martijnvg 
your nested inner_hits example is not working for me.when i am running then it's showing the error like that:
"type": "parsing_exception",
  "reason": "Unknown key for a START_OBJECT in [inner_hits]."</comment><comment author="clintongormley" created="2017-04-28T08:56:15Z" id="297945663">@abhishek5678 if you don't stop hijacking issues I will be forced to ban you.  You have been told several times to use the https://discuss.elastic.co forums.  </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog: close channel on failures while converting a writer to a reader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16142</link><project id="" key="" /><description>TranslogWriter.closeIntoReader transfers the file ownership from a writer to a reader and closes the writer. If the transfer fails, we need to make sure we closed the underlying channel as the writer is already closes.

See: http://build-us-00.elastic.co/job/es_core_master_regression/4355
</description><key id="127883901">16142</key><summary>Translog: close channel on failures while converting a writer to a reader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>bug</label></labels><created>2016-01-21T09:44:42Z</created><updated>2016-01-22T09:42:13Z</updated><resolved>2016-01-22T09:42:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-21T10:09:33Z" id="173524110">left some comments especially regarding logging
</comment><comment author="bleskes" created="2016-01-21T10:20:34Z" id="173527185">@s1monw I pushed the change to closeWhileHandlingException . Regarding the logging - I personally find them invaluable in tracing this kind of issues. They are all trace logs and therefore shouldn't be any disturbance until you need them and turn them on. That upside greatly outweighs, at least for the way I work, the extra lines in the code.
</comment><comment author="s1monw" created="2016-01-21T10:33:34Z" id="173530720">&gt; @s1monw I pushed the change to closeWhileHandlingException . Regarding the logging - I personally find them invaluable in tracing this kind of issues. They are all trace logs and therefore shouldn't be any disturbance until you need them and turn them on. That upside greatly outweighs, at least for the way I work, the extra lines in the code.

I managed to keep this part of the code clean from these unnecessary code lines. I really don't wanna bloat it up with logger dependencies and shard ids what have you. Really please don't add them everywhere we suffer from the massive logging problem all over the place.
</comment><comment author="s1monw" created="2016-01-21T14:54:08Z" id="173594984">can we please get this bug fixed with the minimal changes needed here I'd really appreciate it.
</comment><comment author="GlenRSmith" created="2016-01-21T18:08:38Z" id="173658106">Without diving too deep into the code here - assuming the logging threshold is set to TRACE for sufficient scope, would it be possible to identify the shard at these code points without actually logging the shard_id in these functions/methods whose signatures must be changed exclusively for logging?

That is, is the shard_id already logged somewhere higher in the stack?
</comment><comment author="s1monw" created="2016-01-22T08:00:48Z" id="173838134">@bleskes this test case fails locally for me can you remove all the unnecessary bloat here and push this. I will wait an hour then push a fix myself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix default doc values to be enabled when a field is not indexed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16141</link><project id="" key="" /><description>Doc values currently default to `true` if the field is indexed and not analyzed.
So setting `index:no` automatically disables doc values, which is not explicit
in the documentation.

This commit makes doc values default to true for numerics, booleans regardless
of whether they are indexed. Not indexed strings still don't have doc values,
since we can't know whether it is rather a text or keyword field. This
potential source of confusion should go away when we split `string` into `text`
and `keyword`.
</description><key id="127883740">16141</key><summary>Fix default doc values to be enabled when a field is not indexed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-21T09:43:53Z</created><updated>2016-01-27T08:09:57Z</updated><resolved>2016-01-27T08:09:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-21T15:43:45Z" id="173611203">LGTM
</comment><comment author="jpountz" created="2016-01-27T08:09:52Z" id="175472909">Closed via f959d39ac3a7dbc54f93764c4b3408d7c6f346d6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"/etc/init.d/elasticsearch stop" takes 86400 seconds in RHEL6.x.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16140</link><project id="" key="" /><description>This is same problem with #15520.

Cause of this problem is that behavior of killproc is different between RHEL 6 and 7. 

Line 129 of /etc/init.d/elasticsearch:

```
killproc -p $pidfile -d 86400 $prog
```

In RHEL 6, killproc execute "sleep $delay" if target process does not stop within 1sec. So, "/etc/init.d/elasticsearch stop" often takes 86400 seconds, even if elasticsearch is stopped.
</description><key id="127873989">16140</key><summary>"/etc/init.d/elasticsearch stop" takes 86400 seconds in RHEL6.x.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yfujita</reporter><labels /><created>2016-01-21T08:47:32Z</created><updated>2016-01-28T03:10:09Z</updated><resolved>2016-01-21T11:19:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-21T11:19:57Z" id="173541754">Duplicates #15520.
</comment><comment author="marevol" created="2016-01-28T03:10:09Z" id="175934775">@jasontedor I think that this problem is to use killproc in /etc/init.d/elasticsearch.
For RHEL 6, elasticsearch waits 86400 sec(24h) even if a process of elasticsearch is stopped.
For RHEL 7, elasticsearch is stopped if the process is stopped within 24h.
This behavior  for _RHEL 6_ annoys us in our production environment.
I think it's better not to remove/replace killproc in elasticsearch's init script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping conflicts cause the infinite shard allocation loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16139</link><project id="" key="" /><description>I'm using the latest elasticsearch 2.1.1 as a part of ELK stack in the environment that difficult to control the consistency of the field type and mapping. 

I noticed the mapping breaking change from 1.x version. I tried testing this to ensure the stability in 2.x cluster and found this unexpected behaviour. 

I started the fresh elasticsearch 2.1.1 cluster with no data. I got the infinite loop of shard allocation failed and it kept retrying but never success when indexing data below using the bulk API.

Reproduce step:
1) Using the logstash template in attached.
2) run curl to elasticsearch using below data. Sometime it's success but sometime it failed with infinite loop of shard allocation failure. (I'm usually found this issue 1 in 3 times)

curl -s -XPOST localhost:9200/_bulk --data-binary "@/tmp/a"

the /tmp/a contains
{ "index" : { "_index" : "logstash-2016.02.01", "_type" : "tweet" } }
{ "message" : 0, "test" : 20, "status" : 0 }
{ "index" : { "_index" : "logstash-2016.02.01", "_type" : "tweet" } }
{ "message" : "value1", "test" : "test", "status" : "0" }
{ "index" : { "_index" : "logstash-2016.02.01", "_type" : "twoot" } }
{ "message" : "value1", "test" : "test", "status" : "0" }
{ "index" : { "_index" : "logstash-2016.02.01", "_type" : "twaat" } }
{ "message" : "value1", "test" : "test", "status" : "0" }

Once run you will get the error like this

![image](https://cloud.githubusercontent.com/assets/11056237/12473071/5700c04a-c041-11e5-8cdd-d3fb867f62af.png)

I've also attached 
1)  error from elasticsearch log [error.txt](https://github.com/elastic/elasticsearch/files/98769/error.txt)
2) logstash template [logstash_template.txt](https://github.com/elastic/elasticsearch/files/98766/logstash_template.txt) 
3) logstash-2016.02.01 index mapping after run the command [logstash-2016.02.01_mapping.txt](https://github.com/elastic/elasticsearch/files/98767/logstash-2016.02.01_mapping.txt)

I noticed something wrong in the mapping as follows:
1) The tweet type have the correct mapping ( all fields are having type: long) 
2) Somehow the mapping of twaat type for all fields became string and this is the cause I got the infinite loop of shard recovering.

![image](https://cloud.githubusercontent.com/assets/11056237/12472955/3c8e9288-c040-11e5-990a-841eab1f6d19.png)

Once it happened the only way to resolve this is to delete that index which means if this happened in production environment, I need to delete the index and lose all data?

Your earliest help on this issue would be highly appreciated.

Thank you.
Sombut
</description><key id="127855536">16139</key><summary>Mapping conflicts cause the infinite shard allocation loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">sombut</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-01-21T06:22:58Z</created><updated>2016-01-27T07:13:19Z</updated><resolved>2016-01-26T16:48:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-21T18:26:15Z" id="173664539">Hi @sombut 

Thanks for the report - I've spent a few hours playing with the example and so far have been unable to get the shard failures that you're seeing.  I've tried with a cluster of two and three nodes.  What does your cluster look like?

Are you sure that it is ES 2.1.1?  I've seen this happen on 2.1.0, but that was supposed to be fixed in 2.1.1.  (There are a number of other mapping fixes coming in 2.2 and 2.3 as well.)

Something else I've seen on 2.1.1 while running your recreation (which I can't replicate on 2.2.0) is that the bulk request will sometimes hang indefinitely.

Btw, you can make your dynamic mappings better to avoid this issue by deleting the `match_mapping_type` parameter from the template for `message_field`, and you can delete all of the templates for double/float/integer/date/etc - those fields default to use doc_values already.
</comment><comment author="clintongormley" created="2016-01-21T18:27:00Z" id="173664728">Recreation here:

```
DELETE *
DELETE _template/*
PUT _template/logstash
{
  "order": 0,
  "template": "logstash-*",
  "settings": {
    "index": {
      "mapping": {
        "ignore_malformed": "true"
      },
      "refresh_interval": "30s"
    }
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "message_field": {
            "mapping": {
              "fielddata": {
                "format": "disabled"
              },
              "index": "analyzed",
              "omit_norms": true,
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "message"
          }
        },
        {
          "string_fields": {
            "mapping": {
              "fielddata": {
                "format": "disabled"
              },
              "index": "analyzed",
              "omit_norms": true,
              "type": "string",
              "fields": {
                "raw": {
                  "ignore_above": 256,
                  "index": "not_analyzed",
                  "type": "string",
                  "doc_values": true
                }
              }
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        },
        {
          "float_fields": {
            "mapping": {
              "type": "float",
              "doc_values": true
            },
            "match_mapping_type": "float",
            "match": "*"
          }
        },
        {
          "double_fields": {
            "mapping": {
              "type": "double",
              "doc_values": true
            },
            "match_mapping_type": "double",
            "match": "*"
          }
        },
        {
          "byte_fields": {
            "mapping": {
              "type": "byte",
              "doc_values": true
            },
            "match_mapping_type": "byte",
            "match": "*"
          }
        },
        {
          "short_fields": {
            "mapping": {
              "type": "short",
              "doc_values": true
            },
            "match_mapping_type": "short",
            "match": "*"
          }
        },
        {
          "integer_fields": {
            "mapping": {
              "type": "integer",
              "doc_values": true
            },
            "match_mapping_type": "integer",
            "match": "*"
          }
        },
        {
          "long_fields": {
            "mapping": {
              "type": "long",
              "doc_values": true
            },
            "match_mapping_type": "long",
            "match": "*"
          }
        },
        {
          "date_fields": {
            "mapping": {
              "type": "date",
              "doc_values": true
            },
            "match_mapping_type": "date",
            "match": "*"
          }
        },
        {
          "geo_point_fields": {
            "mapping": {
              "type": "geo_point",
              "doc_values": true
            },
            "match_mapping_type": "geo_point",
            "match": "*"
          }
        }
      ],
      "_all": {
        "omit_norms": true,
        "enabled": true
      },
      "properties": {
        "@timestamp": {
          "type": "date",
          "doc_values": true
        },
        "geoip": {
          "dynamic": true,
          "type": "object",
          "properties": {
            "ip": {
              "type": "ip",
              "doc_values": true
            },
            "latitude": {
              "type": "float",
              "doc_values": true
            },
            "location": {
              "type": "geo_point",
              "doc_values": true
            },
            "longitude": {
              "type": "float",
              "doc_values": true
            }
          }
        },
        "@version": {
          "index": "not_analyzed",
          "type": "string",
          "doc_values": true
        }
      }
    },
    "tornado_iis_advanced": {
      "properties": {
        "args": {
          "properties": {
            "version": {
              "norms": {
                "enabled": false
              },
              "type": "string",
              "fields": {
                "raw": {
                  "ignore_above": 256,
                  "index": "not_analyzed",
                  "type": "string"
                }
              }
            }
          }
        }
      }
    }
  },
  "aliases": {}
}

DELETE *
POST /_bulk
{"index":{"_index":"logstash-2016.02.01","_type":"tweet"}}
{"message":0,"test":20,"status":0}
{"index":{"_index":"logstash-2016.02.01","_type":"tweet"}}
{"message":"value1","test":"test","status":"0"}
{"index":{"_index":"logstash-2016.02.01","_type":"twoot"}}
{"message":"value1","test":"test","status":"0"}
{"index":{"_index":"logstash-2016.02.01","_type":"twaat"}}
{"message":"value1","test":"test","status":"0"}

GET _mapping/field/message,status
```
</comment><comment author="sombut" created="2016-01-22T02:54:46Z" id="173787876">Hi @clintongormley 

Thanks for your help.

My cluster has 1 master and 2 data nodes. I'm sure it is the 2.1.1 version. You can see the full log since start the elasticsearch until getting this error.
[compass-dev-esmn-01.zip](https://github.com/elastic/elasticsearch/files/100208/compass-dev-esmn-01.zip)
You recreation steps is right and I can recreate the issue using yours too.

Below is my new template after match_mapping_type removed. It really helps avoid this issue but only when the conflicted field name is the `message`.

```
PUT _template/logstash
{
  "order": 0,
  "template": "logstash-*",
  "settings": {
    "index": {
      "mapping": {
        "ignore_malformed": "true"
      },
      "refresh_interval": "30s"
    }
  },
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "message_field": {
            "mapping": {
              "fielddata": {
                "format": "disabled"
              },
              "index": "analyzed",
              "omit_norms": true,
              "type": "string"
            },
            "match": "message"
          }
        }
      ],
      "_all": {
        "omit_norms": true,
        "enabled": true
      },
      "properties": {
        "@timestamp": {
          "type": "date",
          "doc_values": true
        },
        "geoip": {
          "dynamic": true,
          "type": "object",
          "properties": {
            "ip": {
              "type": "ip",
              "doc_values": true
            },
            "latitude": {
              "type": "float",
              "doc_values": true
            },
            "location": {
              "type": "geo_point",
              "doc_values": true
            },
            "longitude": {
              "type": "float",
              "doc_values": true
            }
          }
        },
        "@version": {
          "index": "not_analyzed",
          "type": "string",
          "doc_values": true
        }
      }
    },
    "tornado_iis_advanced": {
      "properties": {
        "args": {
          "properties": {
            "version": {
              "norms": {
                "enabled": false
              },
              "type": "string",
              "fields": {
                "raw": {
                  "ignore_above": 256,
                  "index": "not_analyzed",
                  "type": "string"
                }
              }
            }
          }
        }
      }
    }
  },
  "aliases": {}
}
```

But when I change the field `message` to something else for example `size`, the problem happens again.

```
{"index":{"_index":"logstash-2016.02.01","_type":"tweet"}}
{"size":0,"test":20,"status":0}
{"index":{"_index":"logstash-2016.02.01","_type":"tweet"}}
{"size":"value1","test":"test","status":"0"}
{"index":{"_index":"logstash-2016.02.01","_type":"twoot"}}
{"size":"value1","test":"test","status":"0"}
{"index":{"_index":"logstash-2016.02.01","_type":"twaat"}}
{"size":"value1","test":"test","status":"0"}
```

![image](https://cloud.githubusercontent.com/assets/11056237/12502801/c62eede8-c0ff-11e5-9694-cceedadb5074.png)
</comment><comment author="clintongormley" created="2016-01-22T12:19:01Z" id="173902616">Thanks @sombut - with one master and two data nodes I'm able to recreate this issue in 2.1.1 and in 2.1.2..  The good news is that I can't recreate it on 2.2.0 (which we are close to releasing).
</comment><comment author="sombut" created="2016-01-26T07:09:39Z" id="174866010">Thank you @clintongormley for the good news. I'll wait for the 2.2.0 release. Do you know when will it be release please?
</comment><comment author="jpountz" created="2016-01-26T16:48:01Z" id="175109802">This seems to be fixed by #15142.

&gt; I'll wait for the 2.2.0 release. Do you know when will it be release please?

It will be out "soon" after we fix the last remaining issues: https://github.com/elastic/elasticsearch/labels/v2.2.0
</comment><comment author="sombut" created="2016-01-27T07:13:19Z" id="175456062">Thank you @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node stats api shows both query_cache and request_cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16138</link><project id="" key="" /><description>Is this expected?  Looking at the node stats api output for 2.1.1, it shows both query_cache and request_cache sections.  For a query with request_cache enabled, it increments the request_cache section (expected).  

Is the query_cache section obsolete now that we have renamed [query_cache to request_cache](https://github.com/elastic/elasticsearch/pull/12478)?

```
"query_cache" : {
          "memory_size_in_bytes" : 0,
          "total_count" : 832147,
          "hit_count" : 29390,
          "miss_count" : 802757,
          "cache_size" : 0,
          "cache_count" : 1943,
          "evictions" : 1943
        }

        "request_cache" : {
          "memory_size_in_bytes" : 3395,
          "evictions" : 0,
          "hit_count" : 30,
          "miss_count" : 25
        }
```
</description><key id="127846104">16138</key><summary>Node stats api shows both query_cache and request_cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2016-01-21T05:09:02Z</created><updated>2016-01-21T07:43:23Z</updated><resolved>2016-01-21T07:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-21T07:26:05Z" id="173482383">I believe the query cache is now the previous filter cache, given that filters became queries... will wait for others to confirm though before closing, just to make sure.
</comment><comment author="ppf2" created="2016-01-21T07:43:23Z" id="173485233">Oh makes sense, for this query_cache is sitting between warmer and fielddata, where filter_cache and id_cache used to be located.  Just got confused because of the naming :)  Thanks for the pointer.  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_point ignore ignore_malformed setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16137</link><project id="" key="" /><description>even with enable `ignore_malformed` on mapping

```
"geoip_coordinate" : {
    "type" : "geo_point",
    "ignore_malformed" : true
},
```

Bulk insert(by fluentd elasticsearch plugin) something like`"geoip_coordinate":"-,-"`still throw exception

```
[2016-01-20 12:05:09,274][DEBUG][action.bulk              ] [my server name] [my index name][2] failed to execute bulk item (index) index {[my index name][nginx][AVJdNRawFNa9of9WENEs], source[{"request_time":"0
.500","remote_addr":"172.0.0.1","server_port":"8080","geoip_coordinate":"-,-","@timestamp":"2016-0
1-20T08:21:26+08:00"}]}
MapperParsingException[failed to parse]; nested: NumberFormatException[For input string: "-"];
        at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:159)
        at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:304)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:516)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:507)
        at org.elasticsearch.action.support.replication.TransportReplicationAction.prepareIndexOperationOnPrimary(TransportReplicationAction.java:1052)
        at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1060)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:338)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:131)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "-"
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
        at java.lang.Double.parseDouble(Double.java:538)
        at org.elasticsearch.common.geo.GeoPoint.resetFromString(GeoPoint.java:71)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parsePointFromString(GeoPointFieldMapper.java:694)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parse(GeoPointFieldMapper.java:669)
        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:314)
        at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:441)
        at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:267)
        at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:127)
        ... 14 more
```
</description><key id="127825431">16137</key><summary>geo_point ignore ignore_malformed setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yijui</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-21T01:35:29Z</created><updated>2016-05-04T16:29:21Z</updated><resolved>2016-05-04T16:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-06T11:49:17Z" id="206333085">Closed by 3651854bf6d0b1e9fb4ad8a954f476ba9f6df367.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run the elasticsearch.bat - in some versions you need use "Run as administrator"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16136</link><project id="" key="" /><description /><key id="127819420">16136</key><summary>Run the elasticsearch.bat - in some versions you need use "Run as administrator"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aig-</reporter><labels /><created>2016-01-21T00:45:18Z</created><updated>2016-04-06T17:10:14Z</updated><resolved>2016-04-06T17:10:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-21T14:28:50Z" id="173584851">Hi @aig- 

In what situations would you need to run as an admin?
</comment><comment author="dakrone" created="2016-04-06T17:10:14Z" id="206468669">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs with fielddata disabled fails inconsistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16135</link><project id="" key="" /><description>&gt; Reported by @jimmyjones2 in https://github.com/elastic/kibana/issues/3335#issuecomment-168061914

While indexing a small amount of data to test https://github.com/elastic/kibana/issues/3335 we discovered a strange inconsistency with how elasticsearch validated aggregations. What we observed:

After creating a fresh index (two shards and a `nofielddata` field) any aggregation using the `nofielddata` field can be sent to the index and responses will come back as "successful". As shards start to get documents these shards will start to fail with `"java.lang.IllegalStateException: Field data loading is forbidden on nofielddata"`, but responses will still be formatted as partial successes. Once the final shard has a document requests will start to actually fail (response with a top level "error" property).

This feels like a serious edge-case, but I figured there isn't any harm in reporting it. 

---

Here is a sense script to replicate this:

``` sh
DELETE /test?ignore_unavailable=true

POST /test
{
  "settings": {
    "number_of_shards": 2
  },
  "mappings": {
    "test" : {
      "properties": {
        "nofielddata" : {
          "type": "string",
          "fielddata" : { "format" : "disabled" }
        }
      }
    }
  }
}

POST /test/test
{ "noindex" : "foo", "nofielddata" : "foo" }

GET /test/test/_search
{
  "query": {
    "match": {
      "_id": "AVJhGARbdsS1hY8oBX1Z"
    }
  },
  "aggs" : {
    "test" : {
      "terms" : { "field" : "nofielddata" }
    }
  }
}

POST /test/test
{ "noindex" : "bar", "nofielddata" : "bar" }

GET /test/test/_search
{
  "query": {
    "match": {
      "_id": "AVJhGARbdsS1hY8oBX1Z"
    }
  },
  "aggs" : {
    "test" : {
      "terms" : { "field" : "nofielddata" }
    }
  }
}
```
</description><key id="127801043">16135</key><summary>Aggs with fielddata disabled fails inconsistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels><label>:Fielddata</label><label>adoptme</label><label>bug</label></labels><created>2016-01-20T22:39:49Z</created><updated>2016-01-27T08:13:30Z</updated><resolved>2016-01-27T08:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T22:54:22Z" id="173390255">Agreed this is a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>systemd-pre-exec script fails silently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16134</link><project id="" key="" /><description>On systemd-based distros (CentOS in this case), there is a script `bin/elasticsearch-systemd-pre-exec` which runs before starting the main process. This script ensures the `CONF_FILE` setting is empty, as it has apparently been removed. However, the error message is echoed to stdout, not stderr, so systemd does not pick up the message, resulting in a very confusing, silent failure.

```
$ systemctl start elasticsearch
Job for elasticsearch.service failed because the control process exited with error code. See "systemctl status elasticsearch.service" and "journalctl -xe" for details.

$ systemctl status elasticsearch
&#9679; elasticsearch.service - Elasticsearch
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Wed 2016-01-20 14:42:39 CST; 3s ago
     Docs: http://www.elastic.co
  Process: 10768 ExecStartPre=/usr/share/elasticsearch/bin/elasticsearch-systemd-pre-exec (code=exited, status=1/FAILURE)

Jan 20 14:42:39 db_host systemd[1]: Starting Elasticsearch...
Jan 20 14:42:39 db_host systemd[1]: elasticsearch.service: control process exited, code=exited status=1
Jan 20 14:42:39 db_host systemd[1]: Failed to start Elasticsearch.
Jan 20 14:42:39 db_host systemd[1]: Unit elasticsearch.service entered failed state.
Jan 20 14:42:39 db_host systemd[1]: elasticsearch.service failed.
```
</description><key id="127781075">16134</key><summary>systemd-pre-exec script fails silently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">royaldark</reporter><labels /><created>2016-01-20T20:51:37Z</created><updated>2016-01-21T20:01:12Z</updated><resolved>2016-01-21T11:58:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-21T11:58:34Z" id="173549295">In [/usr/lib/systemd/system/elasticsearch.service](https://github.com/elastic/elasticsearch/blob/eaad8924d87306f56b7c5d1f6add5fbf481b4c5d/distribution/src/main/packaging/systemd/elasticsearch.service#L30) is `StandardOutput=null`. If you change this to `StandardOutput=journal`, attempt to start Elasticsearch to reproduce the error, and then use `journalctl` to check the journal, you will see:

```
Jan 21 11:55:14 vagrant-ubuntu-vivid-64 elasticsearch-systemd-pre-exec[11751]: CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
```
</comment><comment author="royaldark" created="2016-01-21T19:17:42Z" id="173679080">Sure, but why is `StandardOutput=journal` not the default then? It seems this message should be visible with a default installation, especially when an upgrade from &lt; 2.0 is pretty likely to run into this issue. 
</comment><comment author="jasontedor" created="2016-01-21T20:01:12Z" id="173691939">&gt; Sure, but why is `StandardOutput=journal` not the default then?

@royaldark I agree it should be as I think this situation trips a lot of people up. Changing it will produce more logging in the journal than some system operators are use to, but that's an easier problem to diagnose and address than the opposite problem of not having any logging at all at the critical moment when the system is starting up.

I opened #16159.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Mapping] Several MappingService cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16133</link><project id="" key="" /><description>- Simplified `MapperService#searchFilter(...)` by removing obsolete logic. Relates to #15924 
- Removed `resolveClosestNestedObjectMapper(...)` method as it was no longer used.
- Removed `DocumentTypeListener` infrastructure. Before it was used by the percolator and parent/child, but these features no longer use it.

The breaking part relates to the first cleanup and that is that we no longer exclude percolator documents from the search response.
</description><key id="127777611">16133</key><summary>[Mapping] Several MappingService cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T20:32:28Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-01-20T21:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-20T20:38:33Z" id="173352285">LGTM great cleanups! Perhaps as a future tweak we can move the now simplified impl of searchFilter into DefaultSearchContext.searchFilter, which is the only caller of MapperService.searchFilter?
</comment><comment author="martijnvg" created="2016-01-20T21:16:54Z" id="173361690">@rjernst I moved the searchFilter() logic from the MapperService to DefaultSearchContext. This change was small enough to be done in this PR.
</comment><comment author="rjernst" created="2016-01-20T21:22:01Z" id="173362878">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] add an AbstractProcessor to help hold the re-used processorTag variable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16132</link><project id="" key="" /><description>from this discussion: https://github.com/elastic/elasticsearch/pull/16049#discussion_r49984437
</description><key id="127776965">16132</key><summary>[Ingest] add an AbstractProcessor to help hold the re-used processorTag variable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2016-01-20T20:29:22Z</created><updated>2016-01-21T15:30:33Z</updated><resolved>2016-01-21T15:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-21T08:39:47Z" id="173498668">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API for listing index file sizes (revisited)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16131</link><project id="" key="" /><description>It looks like #6728 is closed via #8832 - but #8832 reports on the _memory_ usage of these items, eg. the doc_values_memory_in_bytes reports on the memory we use for efficient access to the doc values on disk.    So I think 8832 only addresses part of this request here.  The other part is that it will be helpful to report on the size on disk usage (eg. ls -l) of the Lucene files, in particular, the ones that tend to use the most storage like files associated to term dictionaries, norms, doc values.  The use case is that one can use this information to help size the amount of OS cache space (probably not 1-1 since its unlikely that it will use all of the norms, doc values, terms, etc.. but it will be helpful to get an idea of the worst case scenario when it comes to sizing).   Since this may end up being a separate API, I am filing this as a new ticket.
</description><key id="127750319">16131</key><summary>API for listing index file sizes (revisited)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-20T18:15:42Z</created><updated>2016-03-03T16:15:10Z</updated><resolved>2016-03-03T16:15:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T22:56:20Z" id="173391040">+1
</comment><comment author="mikemccand" created="2016-01-21T09:28:24Z" id="173513792">+1
</comment><comment author="camilojd" created="2016-01-28T05:12:39Z" id="175983207">Hi, I'd like to add support for these stats as another member of CommonStats. Already did some tinkering and would like to go further :-)
</comment><comment author="camilojd" created="2016-02-08T23:16:14Z" id="181615602">Please see my implementation at https://github.com/camilojd/elasticsearch/commit/f80b6271870eb4a9e82649d985c0fa691de4831f. Some comments/questions follow.

I'm consolidating file sizes using the same criteria of `SegmentsStats` to expose disk usage of Lucene index files: `Terms`, `TermVectors`, `StoredFields`, `Norms`, `DocValues` (class `IndexResources` in `SegmentsStats`).

`SegmentsStats` currently exposes memory consumption as bytes of each of the aforementioned so it's possible to add these as part of a class (`IndexResources`) and expose it as two fields of `SegmentsStats`, one for memory resources (existing stats) and other for disk resources. This is what I implemented in my branch, but I'm not sure if it would be better to expose this as another "sister" class of `SegmentsStats`, member of `CommonStats`, and leave `SegmentsStats` as it is. I'm however aware that my `toXContent` serialization could break clients expecting certain fields that now are placed elsewhere.

Some other couple questions I've got:
1. Is it necessary to consolidate sizes by the same criteria of `SegmentsStats` or may be better doing it using the file extensions? This last option has the advantage that would include additional information about postings, i.e., separate size info for positions, payloads, etc.
2. The meat of the matter: Is the way I open the Directory from the `SegmentInfo` and the compound reader a sensible approach? I'm not sure if this could introduce issues with the `Store`.
</comment><comment author="clintongormley" created="2016-02-13T20:22:51Z" id="183747332">@camilojd i'd suggest opening a PR to discuss this, even if it is a WIP
</comment><comment author="camilojd" created="2016-02-14T20:56:33Z" id="183976590">Will do! thanks @clintongormley 
</comment><comment author="clintongormley" created="2016-03-03T16:15:10Z" id="191832083">Closed by https://github.com/elastic/elasticsearch/pull/16661
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable serialization and parsing from xContent for SmoothingModels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16130</link><project id="" key="" /><description>PhraseSuggestionBuilder uses three smoothing models internally. In order to enable proper serialization / parsing from xContent to the phrase suggester for the search builder refactoring, this change starts by making the smoothing models writable, adding hashCode/equals and fromXContent.
</description><key id="127742636">16130</key><summary>Enable serialization and parsing from xContent for SmoothingModels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T17:43:01Z</created><updated>2016-03-10T18:57:35Z</updated><resolved>2016-01-29T09:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-28T13:41:21Z" id="176189538">@areek thanks for the review, I removed the type parameter from SmoothingModel, also added an aditional buildWordScorerFactory() method that we will need later in the Phrase suggesters build method. Can you take another look if this now look okay to you?
</comment><comment author="areek" created="2016-01-28T20:42:37Z" id="176404476">@cbuescher this looks great, LGTM! I added a minor style comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>port fix for windows command line options to master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16129</link><project id="" key="" /><description>This pr https://github.com/elastic/elasticsearch/pull/15320
cannot be easily ported to master because we need to translate mvn/ant to gradle which might take me a little so I open an issue here.
</description><key id="127717068">16129</key><summary>port fix for windows command line options to master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-01-20T16:11:10Z</created><updated>2016-05-25T12:57:30Z</updated><resolved>2016-05-25T12:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-20T16:40:52Z" id="173266471">I don't think the change will be that difficult, but I would also wait until I've finished refactoring ES into a fixture. Part of that is to use ProcessBuilder instead of ant exec, at which point we will no longer use the pidfiles for correct test execution, and the command line qa test you have here can set the options and check the pid file itself.
</comment><comment author="brwe" created="2016-01-20T16:41:55Z" id="173266830">&gt; I would also wait until I've finished refactoring ES into a fixture

ok
</comment><comment author="clintongormley" created="2016-05-03T14:05:40Z" id="216538070">@rjernst does this still need doing?
</comment><comment author="jasontedor" created="2016-05-03T14:16:30Z" id="216542289">&gt; does this still need doing?

The fix has been done in master in e32da555aa56b20f914610282392f51c2d12b8ac by @ywelsch after the command-line parsing refactoring in #17088 but the tests have not been forward-ported.
</comment><comment author="clintongormley" created="2016-05-07T15:33:51Z" id="217645243">@brwe would you be able to port the tests to master please?
</comment><comment author="brwe" created="2016-05-09T08:58:09Z" id="217811372">ok
</comment><comment author="brwe" created="2016-05-19T18:57:35Z" id="220419945">We discussed it in #18437 and came to the conclusion that might make more sense to invest in bats tests for windows (https://github.com/elastic/elasticsearch/issues/18475) instead of having this one test in gradle or the likes.
</comment><comment author="brwe" created="2016-05-25T12:22:53Z" id="221557862">superseded by #18475
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory Leak using InternalSearchHit.sourceAsMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16128</link><project id="" key="" /><description>I'm experiencing an odd behavior in a project I'm currently working on. What I'm seeing is that when I reference an InternalSearchHit.sourceAsMap directly I'm experiencing a memory leak. The code is doing some graph traversal and so there are references to other InternalSearchHit.sourceAsMap documents as well.

I'm currently writing groovy code, and so part of what I'm seeing is:

```
def docMap = hit.sourceAsMap() 
//Add data into hash map
//Add other InternalSearchHit.sourceAsMap as child of map
```

   I end up with a memory leak.

If I copy the map using the HashMap constructor:

```
def docMap = new HashMap(hit.sourceAsMap())
//Add data into hash map
//Add other InternalSearchHit.sourceAsMap as child of map
```

Then the memory leak goes away.

I took a look at the source for InternalSearchHit and I didn't see anything glaring. At best what I can guess, is that referencing the sourceAsMap object retains a hold on the SearchHit which in turn retains a hold on something else.
</description><key id="127706094">16128</key><summary>Memory Leak using InternalSearchHit.sourceAsMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arciisine</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2016-01-20T15:24:32Z</created><updated>2016-02-05T18:09:18Z</updated><resolved>2016-02-05T18:07:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-21T13:53:21Z" id="173575529">@dakrone any idea what might be happening here?
</comment><comment author="dakrone" created="2016-01-21T20:12:24Z" id="173694667">@timothysoehnlin when you say you are writing groovy code, do you mean you are using the Groovy ES client to interact with Elasticsearch, or are you talking about using the ES scripting API (which is also Groovy)? It _sounds_ like the former so please correct me if I'm mistaken.
</comment><comment author="arciisine" created="2016-01-21T23:52:15Z" id="173753139">I am writing Groovy code that is leveraging the Java Elasticsearch client (nothing do to with scripting inside of an ES query).

I also posted a question on discuss.elastic.co at https://discuss.elastic.co/t/memory-leak-using-internalsearchhit-sourceasmap/39668 which has a bit more information.  As I'm not certain this is an ES issue I thought it might be useful to cross post.
</comment><comment author="jasontedor" created="2016-01-29T01:06:24Z" id="176505836">@timothysoehnlin Do you have a small script that reliably reproduces the issue, preferably a script that works on a bare installation of Elasticsearch?
</comment><comment author="jasontedor" created="2016-02-05T18:07:46Z" id="180474476">@timothysoehnlin I've poked around in the code a bit, and I do not see the possibility of a memory leak. These sorts of issues are notoriously tricky though, so that is not to say there is not an issue on our side, just to say there is not an obvious one. :)

I'm closing this since we haven't heard from you; if you do have a script that reproduces the situation, please feel free to attach and reopen. I'd be more than happy to take a look.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor FieldSortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16127</link><project id="" key="" /><description>- adds json parsing,
- refactors json serialisation,
- adds writable parsing and serialisation,
- adds json and writable roundtrip test

This relates to #15178

Caveat: Tests are still running locally, so expect changes to fix any failures to come.
</description><key id="127700403">16127</key><summary>Refactor FieldSortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T14:59:11Z</created><updated>2016-03-15T09:48:21Z</updated><resolved>2016-03-15T09:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-25T11:05:03Z" id="174472372">@MaineC left a few comments, left out the ones where I already commented similar thoughts in #16151. 
</comment><comment author="MaineC" created="2016-02-08T14:28:43Z" id="181395484">@cbuescher thanks for the comments. I tried to address them in the last two commit. Not sure I correctly understood your comment wrt. how to implement the copy method. Would welcome another round of review comments.
</comment><comment author="cbuescher" created="2016-02-08T20:01:35Z" id="181542180">@MaineC thanks, I did another round of review, left a couple of comments, some questions for clarification.
</comment><comment author="MaineC" created="2016-03-09T09:44:24Z" id="194213124">Rebased to master after getting #16573 in separately. @cbuescher can you take a final look if there's anything left to fix?
</comment><comment author="cbuescher" created="2016-03-09T16:23:09Z" id="194373893">@MaineC since it was a while since I last looked at this I did another round. Looks great, left only a few minor comments and rebasing or merge, then it should be ready.
</comment><comment author="MaineC" created="2016-03-10T12:01:05Z" id="194812634">@cbuescher addressed your comments, mind to have one final look?
</comment><comment author="cbuescher" created="2016-03-10T13:11:15Z" id="194835071">@MaineC left one minor comment, but otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix "analyzer" and "size" parsing and move to shared ParseFields in CompletionSuggestParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16126</link><project id="" key="" /><description>While looking at CompletionSuggestParser I saw that the "analyzer" parameter seems to be declared twice, once for `::setAnalyzer` and once for `::setField`, which seems wrong. Also the "size" parameter seems to be declared twice (once for `::setSize` but also for `::setShardSize`). While correcting this I also moved the ParseFields to their respective builders, so they get also shared by the corresponding "toXContent" methods.
Another small inconsistency to check: RegexOptionsBuilder seems to write its flags parameter to a field called "flags_value", while the parser seems to have tried to pick it up as "flags". I added a ParseField using both options but deprecating "flags_value" since I could find anything in the docs.
</description><key id="127698801">16126</key><summary>Fix "analyzer" and "size" parsing and move to shared ParseFields in CompletionSuggestParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T14:51:21Z</created><updated>2017-06-10T15:43:29Z</updated><resolved>2016-01-21T10:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T22:51:07Z" id="173388514">Good catch. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `search_after` parameter in the SearchAPI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16125</link><project id="" key="" /><description>Pagination of results can be done by using the `from` and `size` parameters but it can be very costly for indices with more than one shard if the `from` parameter is big. The `search_after` parameter has been added to address this problem, it provides a way to efficiently paginate from one page to the next. This parameter accepts an array of sort values, those values are then used by the searcher to sort the top hits from the first document that is equal to the sort values.
This parameter must be used in conjunction with the sort parameter, it must contain exactly the same number of values than the number of fields to sort on.

NOTE: A field with one unique value per document should be used as the last element of the sort specification. Otherwise the sort order for documents that have the same sort values would be undefined. The recommended way is to use the field `_uid` which is certain to contain one unique value for each document.

Relates to https://github.com/elastic/elasticsearch/issues/8192
</description><key id="127696829">16125</key><summary>Add `search_after` parameter in the SearchAPI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T14:42:11Z</created><updated>2016-01-27T18:58:29Z</updated><resolved>2016-01-27T10:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-20T15:22:46Z" id="173236372">@jpountz this is a work in progress, I still need to write more tests to cover cases with scripts, geo and nested sort. I called it "search_from" rather than "search_after" because searching from somewhere (rather than search after) is also a use case and also because "search_from" with from=1 is equivalent to a search after.
</comment><comment author="rjernst" created="2016-01-20T19:36:54Z" id="173335371">`search_from` is a very confusing name, given the existing `from` parameter.
</comment><comment author="jimczi" created="2016-01-20T21:28:03Z" id="173364366">@rjernst, `search_after` with a parameter `&#236;nclude_top` to indicate if the top document (the one matching exactly the search_from values) must be included in the result ?

```
"search_after": {
    "values": ["tweet#3465"]
    "include_top": true|false
}
```
</comment><comment author="jpountz" created="2016-01-20T22:44:47Z" id="173386326">I would need to dive in a bit more but this looks rather good for a WIP! I agree with Ryan `search_from` would be confusing. Maybe we would need @clintongormley 's naming skills here. Something else I was wondering about is whether we want it as a top-level element or not (thinking of putting it under `sort` right now).

&gt; The recommended way is to use the field _uuid which is certain to contain one unique value for each document.

This recommendation is problematic given that _uid does not have doc values and that adding doc values to _uid is controversial: #11887. I'm not sure what to say besides requiring that the set of provided sort values should uniquely identify a document given that documents that compare equal will be skipped?
</comment><comment author="clintongormley" created="2016-01-21T14:18:25Z" id="173582094">&gt; search_from is a very confusing name, given the existing from parameter.
&gt; 
&gt;  I called it "search_from" rather than "search_after" because searching from somewhere (rather than search after) is also a use case and also because "search_from" with from=1 is equivalent to a search after.

Hmmm I don't like the interaction between `search_from` and `from`.  These settings should be exclusive.  I think that, if you want to include the previous last result as the first result in the next page, then you should just choose the values from the previous second-last doc, instead of the last doc.

&gt; Something else I was wondering about is whether we want it as a top-level element or not (thinking of putting it under sort right now).

`sort` is pretty overloaded already - I don't think it is the right place to put it.

`from` today means "how many docs should I skip before i start returning results, and defaults to 0.

Two possibilities:
-  support `"from": ["val1","val2"]` to mean "start returning results after these sort values", or
- `"search_after": ["val1", "val2"]` and prohibit the use of `from` in combination with `search_after`

I'm leaning towards the second option
</comment><comment author="jimczi" created="2016-01-21T14:54:42Z" id="173595109">@clintongormley I agree, I'll change the PR with the second option (`search_after` and prohibit `from`\+ `search_after` combination).
</comment><comment author="jimczi" created="2016-01-26T11:11:37Z" id="174959774">@jpountz @clintongormley I renamed `search_from`to `search_after` and prohibited the usage of `from`!= 0 when `search_after` is used. 

&gt; This recommendation is problematic given that _uid does not have doc values and that adding doc values to _uid is controversial: #11887. I'm not sure what to say besides requiring that the set of provided sort values should uniquely identify a document given that documents that compare equal will be skipped?

What do we recommend then ? It's not only for search_after but for pagination in general. For a pure search use case we should have an easy way to provide a deterministic sort. IMO _uid is the simplest and the safest solution. Now, should we activate the doc_values per default on this field ? Maybe not but I don't see why it could not be activated by the user. Bottom line is that the sort would work on _uid even if the doc_values are not activated, are we planning to remove the ability to sort on a field without doc_values ?
</comment><comment author="clintongormley" created="2016-01-26T12:42:36Z" id="174994634">We recommend using `_uid` as a tiebreaker (and really there is no other readily available field that can be used), plus the `random_score` function in the function score query relies on the `_uid`...  I think that using the `_uid` is the best advice we can give at the moment, and rather than blocking this feature, we should see if we can make progress on #11887
</comment><comment author="jpountz" created="2016-01-26T14:51:10Z" id="175056419">I left some comments but it looks good overall!
</comment><comment author="jimczi" created="2016-01-26T17:05:34Z" id="175119320">@jpountz thanks for the review, I think I've covered all your comments.
</comment><comment author="jpountz" created="2016-01-26T17:10:53Z" id="175122006">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sort order enum writable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16124</link><project id="" key="" /><description>Related to #15178
</description><key id="127690426">16124</key><summary>Make sort order enum writable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T14:11:14Z</created><updated>2016-01-25T09:25:27Z</updated><resolved>2016-01-21T13:59:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T22:48:25Z" id="173387268">I'm still not totally comfortable with the use of enum ordinals for serialization but we do it everywhere else so LGTM.
</comment><comment author="MaineC" created="2016-01-21T13:58:57Z" id="173577247">&gt; I'm still not totally comfortable with the use of enum ordinals for serialization but we do it everywhere 
&gt; else so LGTM.

That's pretty much why I added it. Should I open a ticket to discuss alternatives?

Other than that added a unit test to at least check the ordinals remain stable.
</comment><comment author="jpountz" created="2016-01-21T14:03:31Z" id="173578236">&gt; Should I open a ticket to discuss alternatives?

No this is fine. This is just my French genes that made me complain. Thanks for the test!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException during Query phase </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16123</link><project id="" key="" /><description>Hi, I am getting the following error from the `elasticsearch-ruby` gem while query'ing elasticsearch. I am using JRuby-1.7.22 

```
Elasticsearch::Transport::Transport::Errors::InternalServerError: [500] {"error":{"root_cause":[{"type":"null_pointer_exception","reason":null}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query_fetch","grouped":true,"failed_shards":[{"shard":0,"index":"ct_80e958881295065b_language_packs","node":"b2ol3POOSpiYCeD94ZCQuA","reason":{"type":"null_pointer_exception","reason":null}}]},"status":500}
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/elasticsearch-transport-1.0.15/lib/elasticsearch/transport/transport/base.rb:146:in `__raise_transport_error'
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/elasticsearch-transport-1.0.15/lib/elasticsearch/transport/transport/base.rb:256:in `perform_request'
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/elasticsearch-transport-1.0.15/lib/elasticsearch/transport/transport/http/faraday.rb:20:in `perform_request'
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/elasticsearch-transport-1.0.15/lib/elasticsearch/transport/client.rb:125:in `perform_request'
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/elasticsearch-api-1.0.15/lib/elasticsearch/api/actions/search.rb:167:in `search'
    /home/ericu/community/lib/searchable.rb:1029:in `raw_search'
    /home/ericu/.rvm/gems/jruby-1.7.22@community/gems/statsd-ruby-1.2.1/lib/statsd.rb:232:in `time'
    /home/ericu/community/lib/searchable.rb:1028:in `raw_search'
    /home/ericu/community/lib/searchable.rb:1416:in `raw_search'
    /home/ericu/community/lib/searchable.rb:1530:in `raw_search'
    /home/ericu/community/app/models/search/language_pack.rb:59:in `search'
    test/unit/search/language_pack_test.rb:56:in `LanguagePackTest'
```

Here is the HTTP request + response for the query

```
GET http://localhost:9200/ct_80e958881295065b_language_packs/_search [status:500, request:0.176s, query:N/A]
&gt; {"query":{"filtered":{"filter":{"bool":{"must":[{"term":{"killed":false}},{"term":{"listed":true}}]}},"query":{"match":{"_all":"foo"}}}},"highlight":{"fields":{"name":{},"description":{},"user_name":{}},"pre_tags":["&lt;strong&gt;"],"post_tags":["&lt;/strong&gt;"],"require_field_match":false},"size":10}
&lt; {"error":{"root_cause":[{"type":"null_pointer_exception","reason":null}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query_fetch","grouped":true,"failed_shards":[{"shard":0,"index":"ct_80e958881295065b_language_packs","node":"b2ol3POOSpiYCeD94ZCQuA","reason":{"type":"null_pointer_exception","reason":null}}]},"status":500}
[500] {"error":{"root_cause":[{"type":"null_pointer_exception","reason":null}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query_fetch","grouped":true,"failed_shards":[{"shard":0,"index":"ct_80e958881295065b_language_packs","node":"b2ol3POOSpiYCeD94ZCQuA","reason":{"type":"null_pointer_exception","reason":null}}]},"status":500}
```

This is how I made the index

```
PUT http://localhost:9200/ct_80e958881295065b_language_packs [status:200, request:1.079s, query:n/a]
&gt; {"mappings":{"language_pack":{"properties":{"user_id":{"type":"integer"},"current_shared_plugin_version_id":{"type":"integer"},"shared_plugin_category_id":{"type":"integer"},"featured_image_id":{"type":"integer"},"original_current_shared_plugin_version_id":{"type":"integer"},"id":{"type":"integer","index":"not_analyzed"},"name":{"type":"string","analyzer":"snowball","boost":2.0},"description":{"type":"string","analyzer":"snowball"},"created_at":{"type":"date"},"updated_at":{"type":"date"},"downloads":{"type":"integer"},"download_count":{"type":"integer"},"times_rated":{"type":"integer"},"avg_rating":{"type":"float"},"user_name":{"type":"string","analyzer":"lowercase_keyword"},"killed":{"type":"boolean"},"public":{"type":"boolean"},"min_app_version":{"type":"string"},"max_app_version":{"type":"string"},"min_app_version_integer":{"type":"integer"},"max_app_version_integer":{"type":"integer"},"extension_type":{"type":"string"},"extension_category":{"type":"string"},"icon_small":{"type":"string","index":"not_analyzed"},"icon_medium":{"type":"string","index":"not_analyzed"},"icon_large":{"type":"string","index":"not_analyzed"},"url":{"type":"string","index":"not_analyzed"},"last_update":{"type":"date"},"featured":{"type":"boolean"},"listed":{"type":"boolean"},"taxon_names":{"type":"string"}}}},"settings":{"analysis":{"analyzer":{"autocomplete_indexing":{"tokenizer":"whitespace","filter":["lowercase","word_delimiter","autocomplete_filter"]},"autocomplete_search":{"tokenizer":"whitespace","filter":["lowercase","word_delimiter","truncate"]},"keyword_only":{"type":"custom","tokenizer":"keyword","filter":["trim"]},"lowercase_keyword":{"type":"custom","tokenizer":"keyword","filter":["lowercase"]},"autocomplete":{"type":"custom","tokenizer":"keyword","filter":["lowercase","keyword_filter"]},"letters":{"type":"custom","tokenizer":"letter","filter":["lowercase"],"char_filter":["html_strip"]},"standard_no_html":{"type":"standard","char_filter":["html_strip"]},"snowball_plus":{"tokenizer":"standard","filter":["standard","lowercase","stop","word_delimiter","snowball"]}},"filter":{"autocomplete_filter":{"type":"edge_ngram","min_gram":1,"max_gram":20},"synonym":{"type":"synonym","ignore_case":"true","synonyms":["computer, pc, desktop, workstation","laptop, notebook","ram, memory","cpu, processor","gpu, graphics card","hdd, hard drive","ssd, solid state drive"]},"keyword_filter":{"type":"edge_ngram","min_gram":3,"max_gram":20}}},"index":{"number_of_replicas":0,"number_of_shards":1}}}
&lt; {"acknowledged":true}
```

Here is what I could find in my `elasticsearch.log`

```
RemoteTransportException[[Emma Frost][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:467)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-20 07:53:09,790][DEBUG][action.search.type       ] [Emma Frost] All shards failed for phase: [query_fetch]
RemoteTransportException[[Emma Frost][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:467)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-20 07:53:09,790][INFO ][rest.suppressed          ] /ct_a26d4338e39cc61b_language_packs/_search Params: {index=ct_a26d4338e39cc61b_language_packs}
Failed to execute phase [query_fetch], all shards failed; shardFailures {[5yggsqv3SuWB2EiQ2YFCsw][ct_a26d4338e39cc61b_language_packs][0]: RemoteTransportException[[Emma Frost][127.0.0.1:9300][indices:data/read/search[phase/query+fetch]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:228)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ; nested: NullPointerException;
    at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:382)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
    at java.lang.Throwable.printStackTrace(Throwable.java:665)
    at java.lang.Throwable.printStackTrace(Throwable.java:721)
    at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
    at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
    at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
    at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
    at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
    at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
    at org.apache.log4j.Category.callAppenders(Category.java:206)
    at org.apache.log4j.Category.forcedLog(Category.java:391)
    at org.apache.log4j.Category.log(Category.java:856)
    at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo(Log4jESLogger.java:125)
    at org.elasticsearch.common.logging.support.AbstractESLogger.info(AbstractESLogger.java:90)
    at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:131)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:96)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:87)
    at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAction.java:316)
    ... 10 more
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:467)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    ... 3 more
```

Here is what my elasticsearch version:

```
[ericu-destroyer-of-worlds] community$ curl localhost:9200
{
  "name" : "Savage Steel",
  "cluster_name" : "ericu-dow-es",
  "version" : {
    "number" : "2.1.1",
    "build_hash" : "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
    "build_timestamp" : "2015-12-15T13:05:55Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  },
  "tagline" : "You Know, for Search"
}

```

If you need additional information please let me know, this is easy to reproduce.
</description><key id="127690075">16123</key><summary>NullPointerException during Query phase </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hydrogen18</reporter><labels /><created>2016-01-20T14:09:25Z</created><updated>2016-01-20T14:27:46Z</updated><resolved>2016-01-20T14:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T14:27:45Z" id="173220325">Fixed via #15506. A release containing this fix should be out soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make DistanceUnit writable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16122</link><project id="" key="" /><description>This relates to #15178

Decided to split the sort refactoring into smaller steps to avoid one huge PR that's tricky to review (and tricky to keep up to date until it's finally done).
</description><key id="127689028">16122</key><summary>Make DistanceUnit writable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T14:03:53Z</created><updated>2016-01-25T09:24:53Z</updated><resolved>2016-01-21T13:03:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T14:21:00Z" id="173217814">Let's write a simple unit test? Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate fuzzy query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16121</link><project id="" key="" /><description>With this commit we deprecate the widely misunderstood
fuzzy query but will still allow the fuzziness
parameter in match queries and suggesters.

Relates to #15760
</description><key id="127688361">16121</key><summary>Deprecate fuzzy query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Search</label><label>deprecation</label></labels><created>2016-01-20T14:00:10Z</created><updated>2016-01-25T14:48:20Z</updated><resolved>2016-01-25T14:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-01-20T14:02:05Z" id="173212589">This is a separate PR for 2.x just deprecating fuzzy query. I'll create another one for 3.0 where I'll remove it.
</comment><comment author="jpountz" created="2016-01-20T14:13:52Z" id="173215400">Should we wait for 4.0 to remove? Deprecating queries is a bit more tricky than other parts of the api since they might be stored for the percolator.
</comment><comment author="danielmitterdorfer" created="2016-01-20T14:17:16Z" id="173216166">Sure, there's no need to rush it. We can always extend the deprecation period. In that case I'd hold back the PR removing it until after the 3.0 release.

Can/should we take any additional precautions already?
</comment><comment author="danielmitterdorfer" created="2016-01-25T14:28:30Z" id="174523016">Closing unmerged as a result of this discussion. I have create #16211 based on the master branch in favor of this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate the settings key if it's simple chars separated by `.`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16120</link><project id="" key="" /><description>This adds additional safety to the settings since we don't have a full list of them
yet and it caught bugs already in tests.
</description><key id="127677970">16120</key><summary>Validate the settings key if it's simple chars separated by `.`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T13:05:24Z</created><updated>2016-01-20T13:25:45Z</updated><resolved>2016-01-20T13:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-20T13:19:51Z" id="173200527">@clintongormley I pushed updates :)
</comment><comment author="clintongormley" created="2016-01-20T13:24:32Z" id="173202472">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0.0 - Filter Query returns empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16119</link><project id="" key="" /><description>Hello,
I am facing issues while using filter to query using ES 2.0.0

I have an index named user_details with 2 values as below,
{
"_index": "user_details",
"_type": "user",
"_id": "2",
"_score": 1,
"_source": {
"name": "Daniel",
"age": 50
}
}
,
{
"_index": "user_details",
"_type": "user",
"_id": "1",
"_score": 1,
"_source": {
"name": "James",
"age": 30
}
}

Now when I run my filter query as below, it returns empty.
 curl -XGET http://localhost:9200/user_details/_search -d '{"query":{"bool":{"filter":{"term":{"name":"James"}}}}}'

I have also tried adding a "must" clause. that doesn't run at all.
curl -XGET http://localhost:9200/user_details/_search -d '{"query":{"bool":{"must":{"age":30},"filter":{"term":{"name":"James"}}}}}'

Any help to fix this issue is appreciated.
</description><key id="127657789">16119</key><summary>ES 2.0.0 - Filter Query returns empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HarishRamanathan</reporter><labels /><created>2016-01-20T10:59:38Z</created><updated>2016-01-20T13:37:40Z</updated><resolved>2016-01-20T13:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-01-20T13:15:23Z" id="173199658">Please note that we have a separate [discussion forum](http://discuss.elastic.co/) for these questions. In the future, please ask your questions there. The community over there is very helpful.

As a hint though, your problem is that the `name` field is analyzed but the filter looks for the exact term. You probably want to use a match query instead. This works perfectly fine:

```
GET _search
{
   "query": {
      "bool": {
         "must": {
            "match": {
               "name": "James"
            }
         },
         "filter": {
            "term": {
               "age": 30
            }
         }
      }
   }
}
```

You can read more about [match queries](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/query-dsl-match-query.html) and [term queries](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/query-dsl-term-query.html) in the docs.

Closing as this is not a bug.
</comment><comment author="HarishRamanathan" created="2016-01-20T13:35:34Z" id="173205155">@danielmitterdorfer Thanks for helping with this. 

Sorry, wasn't aware about the discussion forum. Will use the forum instead for further questions.
</comment><comment author="danielmitterdorfer" created="2016-01-20T13:37:40Z" id="173206297">You're welcome; glad I could help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't guard IndexShard#refresh calls by a check to isRefreshNeeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16118</link><project id="" key="" /><description>While conceptually correct this call can be confusing since if a
realtime GET is exectued after refresh is called with realtime=true it might
fetch the wrong document out of the translog since the isRefreshNeeded guard
might return false once the new searcher is published but it will not
wait until the verision map is flushed and that can cause a slight race condition
if a subsequent call GETs a document that it expects to come from the index.

Ie. the `_size` mapper tests do this and expecte the GET call to return the document
from the index but it comes from the t-log due to this race.

This change only uses the guard in an async refresh that we schedule to prevent unnecessary
refresh calls but will allow API Refresh calls to have the somewhat less confusing semantics.

This was introduces lately only on unrelease major version branches.

here is a related test failure http://build-us-00.elastic.co/job/es_core_master_regression/4349/
</description><key id="127656090">16118</key><summary>Don't guard IndexShard#refresh calls by a check to isRefreshNeeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T10:48:54Z</created><updated>2016-01-20T15:24:55Z</updated><resolved>2016-01-20T12:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-20T11:25:00Z" id="173176561">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't suppress exceptions when closing unref'd translog readers and deleting unref'd translog files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16117</link><project id="" key="" /><description>`IOUtils.close/deleteWhileHandlingException` should only be used when you are actually handling an exception because otherwise it can suppress a legitimate exception.

I think if we hit exceptions while closing a reader, or while trying to delete old translog files, we (users) should know about it since it could mean something is wrong w/ the IO system.
</description><key id="127649911">16117</key><summary>Don't suppress exceptions when closing unref'd translog readers and deleting unref'd translog files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-01-20T10:18:41Z</created><updated>2016-09-12T22:42:26Z</updated><resolved>2016-09-12T22:42:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T14:19:13Z" id="173216851">I don't know this part of the code well but is it fine if an exception prevents further readers to be closed? Or should we try to close and delete all readers and propagate the first exception that has been thrown (with other exceptions as suppressed exceptions)?
</comment><comment author="bleskes" created="2016-01-20T15:10:59Z" id="173232925">I'm +1 on closing as much as we can and suppressing future exception. 

Also, reviewing this I noticed that we leak files if we crash between committing to lucene and trimming the translog (or when we have an  error deleting them, fail the shard and the re-allocate the shard). We should clean up all translogs with id &lt; comitted  when opening up the translog... (different change).
</comment><comment author="mikemccand" created="2016-01-21T16:58:31Z" id="173634044">Hmm, I don't think we should try to be heroic (do the whole try / finally / firstException / addSuppressed / remove each unref'd reader separately dance) here?

This just adds non-trivial code complexity in order to try for perfection in the exceptional cases where your file system is in trouble.  We really should not design our code based on the exceptional cases?

And, the risk/danger is low for those exceptional cases: calling `close` on a given reader more than once is fine.  Yes, `Files.delete` will give NSFE if we try again to delete the same file.  We could `Files.deleteIfExists` but again I don't think we should add such leniency just for the exceptional cases?

We could also go and remove the affected reader individually from `readers` even on any exception, but that would then mean you see the exception just once vs seeing it repeatedly later (whenever commit occurs or a view is closed, which is really not that frequent) ... I would prefer the latter (be loud about it!).
</comment><comment author="mikemccand" created="2016-02-02T09:55:56Z" id="178484977">OK it seems that I'm outvoted on heroic exception handling ;)

So I pushed another commit, with heroics, doing the `firstException`/`addSuppressed` thing.

I remove each unused reader immediately after its close/deletes, just on the remote chance the `logger.trace` (or the two lines above it) throw some exception.

I think it's ready...
</comment><comment author="s1monw" created="2016-03-23T15:03:23Z" id="200384348">@mikemccand I left a comment
</comment><comment author="dakrone" created="2016-04-06T17:11:14Z" id="206468986">@mikemccand is this still going in to 5.0? It's been a few months since it was opened so just wanted to check :)
</comment><comment author="mikemccand" created="2016-04-06T17:55:42Z" id="206487827">@dakrone I removed the 5.0.0 target ... I don't like any of the options here :(
</comment><comment author="dakrone" created="2016-09-12T21:24:15Z" id="246498773">@mikemccand do you think we should move this to be an issue rather than a PR? It's been quite a while (unless we're in a place we can merge this now)
</comment><comment author="mikemccand" created="2016-09-12T22:42:26Z" id="246518748">Thanks for the reminder @dakrone.  I'll close it for now.  We can start again if we find a cleaner approach.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor tweak to 2.x _reroute documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16116</link><project id="" key="" /><description>Closes #16113
</description><key id="127645623">16116</key><summary>minor tweak to 2.x _reroute documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2016-01-20T10:00:40Z</created><updated>2016-03-01T13:11:21Z</updated><resolved>2016-03-01T13:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-01-20T10:03:32Z" id="173152013">Personally I'd be explicit and say primary and replica, rather than just shard, then there's no question.
</comment><comment author="bleskes" created="2016-01-20T10:04:40Z" id="173152247">Tweak away and make a suggestion :) I felt that it makes the sentence cumbersome. 

&gt; On 20 Jan 2016, at 11:03, markwalkom notifications@github.com wrote:
&gt; 
&gt; Personally I'd be explicit and say primary and replica, rather than just shard, then there's no question.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index buffer - clarify min and max settings relative to size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16115</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/indexing-buffer.html) it's not immediately clear how min and max relate when we also have size. @javanna was nice enough to check and mentioned;

&gt; the min and max are taken into account only when index_buffer_size is a percentage.  In that case the min and max may override index_buffer_size depending on how much memory is available

Thanks to this thread [here](https://discuss.elastic.co/t/index-buffer-size-vs-max-index-buffer-size/39586) for prompting the question.
</description><key id="127633975">16115</key><summary>Index buffer - clarify min and max settings relative to size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Settings</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2016-01-20T08:59:58Z</created><updated>2016-03-03T16:17:46Z</updated><resolved>2016-03-03T16:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-20T09:01:29Z" id="173134447">See https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/indices/IndexingMemoryController.java#L101 .
</comment><comment author="s1monw" created="2016-02-01T20:08:06Z" id="178165930">@mikemccand is this still relevant?
</comment><comment author="mikemccand" created="2016-02-02T00:21:12Z" id="178270638">It is still relevant, even in master, because those settings determine the size of the total indexing buffer for the node.  Only the per-shard min and max settings were removed in master from #14121.

Are there specific suggestions on improving the docs?  (They do already state that they only take effect when `index_buffer_size` is a percentage already...).
</comment><comment author="clintongormley" created="2016-03-01T13:19:26Z" id="190721126">Honestly I think the docs are pretty clear.  @markwalkom if you can think of a way of making this clearer, feel free to send a PR but otherwise I'm going to close this
</comment><comment author="markwalkom" created="2016-03-02T02:14:38Z" id="191019804">There ya are mate! :D
</comment><comment author="clintongormley" created="2016-03-03T16:17:46Z" id="191833613">Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update lucene to r1725675</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16114</link><project id="" key="" /><description>Adds DFI (divergence from independence) provider.
Fixes test bugs passing invalid values for BM25 parameters.
</description><key id="127630384">16114</key><summary>Update lucene to r1725675</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T08:35:17Z</created><updated>2016-01-20T15:19:58Z</updated><resolved>2016-01-20T10:04:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T09:36:22Z" id="173145304">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reroute and replica shards that return</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16113</link><project id="" key="" /><description>The warning at the bottom of [this page](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html] states;

&gt; The allow_primary parameter will force a new empty primary shard to be allocated without any data. If a node which has a copy of the original primary shard (including data) rejoins the cluster later on, that data will be deleted: the old shard copy will be replaced by the new live shard copy.

If there are no applicable replicas to promote and a user force allocates the primary, we will then create a new replica containing the same amount of data as this new primary (ie probably empty).

But if we had a copy of the replica on another unavailable node - eg if we had multiple nodes drop out that happened to have the primary and replica of the same shard - I presume we also ignore the old replica? Should we update this warning to include this?
</description><key id="127623239">16113</key><summary>Reroute and replica shards that return</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-01-20T07:36:59Z</created><updated>2016-01-20T10:01:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-20T08:06:51Z" id="173123770">I'm not sure I follow you...

&gt;  if we had multiple nodes drop out that happened to have the primary and replica of the same shard 

In this case we wait for the nodes to come back. If someone force allocate an empty primary in that period the same thing will happen indeed - a new empty primary will be created, and all future copies will be identical (i.e., empty).

How would you suggest adapting the text?

Note that this has changed in master [where this flag has been replaced with explicit commands](https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-reroute.html).
</comment><comment author="markwalkom" created="2016-01-20T09:04:04Z" id="173134972">&gt; In this case we wait for the nodes to come back. If someone force allocate an empty primary in that period the same thing will happen indeed - a new empty primary will be created, and all future copies will be identical (i.e., empty).

Yeah, which means that if a node that had the original replica shard came back, we'd delete that as we have an existing one (that is empty).
That's what I mean - by force allocating a primary and recreating it empty, we also do the same for the corresponding replicas.
</comment><comment author="bleskes" created="2016-01-20T10:01:04Z" id="173151521">oh, I see now. I made #16116. Is that what you mean?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrading elastic search from 0.90.x to 1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16112</link><project id="" key="" /><description>Both [breaking changes page](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes.html) and  [Upgrading](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html) sections miss the guide or breaking changes about upgrading from 0.90.x to 1.x. Could somebody please add this related info?
Thank you.
</description><key id="127614158">16112</key><summary>Upgrading elastic search from 0.90.x to 1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SummerSun</reporter><labels /><created>2016-01-20T06:18:21Z</created><updated>2016-01-20T14:29:34Z</updated><resolved>2016-01-20T14:29:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T14:29:34Z" id="173220738">To have this information you need to go to the 1.x docs: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/breaking-changes-1.0.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EC2 discovery docs point to old repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16111</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-discovery-ec2.html) we point to https://github.com/elastic/elasticsearch-cloud-aws, instead of https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws.html
</description><key id="127607012">16111</key><summary>EC2 discovery docs point to old repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-20T05:21:27Z</created><updated>2016-01-20T10:16:57Z</updated><resolved>2016-01-20T10:16:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-20T05:56:51Z" id="173097507">+1 same for azure and GCE
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] split string into ArrayList so it can be appended to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16110</link><project id="" key="" /><description>Fixes #16109.
</description><key id="127605172">16110</key><summary>[Ingest] split string into ArrayList so it can be appended to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-20T05:00:12Z</created><updated>2016-01-21T17:04:21Z</updated><resolved>2016-01-21T17:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-20T08:21:32Z" id="173126061">left a comment, LGTM otherwise. Good catch, I wonder if other processors have the same problem, will double check.
</comment><comment author="javanna" created="2016-01-20T08:27:46Z" id="173127109">I quickly double checked, seems to me that only the geoip processor might have the same processor as it uses `Arrays.asList` for the location array. It will not be possible to add anything to that array, less of a problem compared to split but we should change that I think.
</comment><comment author="s1monw" created="2016-01-20T08:29:03Z" id="173127326">this is likely the most inefficient way to do this that I can think of? If we wanna support that level of analysis why don't we use analyzers? Or if we really need to do this add support for `Appendable.java` and put it in a StringBuilder?
</comment><comment author="s1monw" created="2016-01-20T10:21:18Z" id="173161158">&gt; this is likely the most inefficient way to do this that I can think of? If we wanna support that level of analysis why don't we use analyzers? Or if we really need to do this add support for Appendable.java and put it in a StringBuilder?

I guess that is what we have to deal with - nevermind 
</comment><comment author="martijnvg" created="2016-01-21T16:59:56Z" id="173634809">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] SplitProcessor does not interoperate well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16109</link><project id="" key="" /><description>After a split processor sets a list value, it is of type `Arrays$ArrayList`, and the underlying `add` that is inherited is from `AbstractList` which does not support `add(Object o)`.

here is a sample pipeline that leads to the processor interoperability issue

``` json
{
  "pipeline": {
    "processors": [
      {
        "set": {
          "processor_tag": "processor_1",
          "field": "flags",
          "value": "new|hot|super|fun|interesting"
        }
      },
      {
        "split": {
          "processor_tag": "processor_2",
          "field": "flags",
          "separator": "\\|"
        }
      },
      {
        "append": {
          "processor_tag": "processor_3",
          "field": "flags",
          "value": [
            "additional_flag"
          ]
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {}
    }
  ]
}
```

here is a test to help recreate this:

``` java
public void testAppend() throws Exception {
        TemplateService templateService = TestTemplateService.instance();

        Map setConfig = new HashMap&lt;&gt;();
        setConfig.put("field", "flags");
        setConfig.put("value", "new|hot|super|fun|interesting");
        Processor p1 = (new SetProcessor.Factory(templateService)).create(setConfig);

        Map splitConfig = new HashMap&lt;&gt;();
        splitConfig.put("field", "flags");
        splitConfig.put("separator", "\\|");
        Processor p2 = (new SplitProcessor.Factory()).create(splitConfig);

        Map appendConfig = new HashMap&lt;&gt;();
        appendConfig.put("field", "flags");
        appendConfig.put("value", Collections.singletonList("additional_flag"));
        Processor p3 = (new AppendProcessor.Factory(templateService)).create(appendConfig);

        CompoundProcessor compoundProcessor = new CompoundProcessor(p1, p2, p3);

        IngestDocument ingestDocument = new IngestDocument(new HashMap&lt;&gt;(), new HashMap&lt;&gt;());
        compoundProcessor.execute(ingestDocument);
}
```

exception thrown:

```
java.lang.UnsupportedOperationException
    at __randomizedtesting.SeedInfo.seed([73BE8BBD9217E836:9B6A3F6987AE9240]:0)
    at java.util.AbstractList.add(AbstractList.java:148)
    at java.util.AbstractList.add(AbstractList.java:108)
    at org.elasticsearch.ingest.core.IngestDocument$$Lambda$1/1854908422.accept(Unknown Source)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
    at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
    at org.elasticsearch.ingest.core.IngestDocument.appendValues(IngestDocument.java:415)
    at org.elasticsearch.ingest.core.IngestDocument.appendValues(IngestDocument.java:407)
    at org.elasticsearch.ingest.core.IngestDocument.setFieldValue(IngestDocument.java:358)
    at org.elasticsearch.ingest.core.IngestDocument.appendFieldValue(IngestDocument.java:264)
    at org.elasticsearch.ingest.core.IngestDocument.appendFieldValue(IngestDocument.java:281)
    at org.elasticsearch.ingest.processor.AppendProcessor.execute(AppendProcessor.java:60)
    at org.elasticsearch.ingest.core.CompoundProcessor.execute(CompoundProcessor.java:73)
```

thanks to @BigFunger for reporting this!
</description><key id="127604098">16109</key><summary>[Ingest] SplitProcessor does not interoperate well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-20T04:52:36Z</created><updated>2016-01-26T12:41:36Z</updated><resolved>2016-01-26T12:41:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Caching of filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16108</link><project id="" key="" /><description>Hi,
As I can see in [Filter Auto Caching](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking_20_query_dsl_changes.html#_filter_auto_caching) section, the control has been taken away from users for disabling caching in known cases.
This would cause a lot of problems in situations where elasticsearch is being used as a timeseries database. Typically analysts might run some one-off queries over older time ranges causing the filter cache to blow up without any reason. In previous versions, we would have turned of caching for the time range filter for queries over older ranges. Getting the data would be slower, but given that the data is coming for older date range, people could live with it.
I might be missing something, but right now it seems impossible to mimic this behaviour due to lack of configurability in choosing which queries to cache in the filter segment and which queries not to. The strategy  discussed in the aforementioned documentation make sense in most of the general situation, but not 100% of the time. I feel that the option should at-least be present so that people can use it in times of need.
If you don't mind my asking, what was the rationale behind the decision to remove the filter caching configuration, and are there any chances that this will be brought back in the future? 
</description><key id="127602329">16108</key><summary>Caching of filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">santanusinha</reporter><labels><label>:Cache</label><label>:Search</label><label>discuss</label></labels><created>2016-01-20T04:32:16Z</created><updated>2016-01-20T11:07:02Z</updated><resolved>2016-01-20T11:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-20T10:00:49Z" id="173151476">&gt; As I can see in Filter Auto Caching section, the control has been taken away from users for disabling caching in known cases.

That's partly true, in a boolean query the `filter` clauses are cacheable whereas `must` and `should`clauses are not. 
That's also partly true ( ;) ) because the should and must clauses are cacheable if they appear in a context where the score is not needed. 
I agree that the fine grain tuning of the cache is not easy in 2.x and that we'll need to add some documentations about it. @jpountz I also agree that it could be useful to add (re-add ?)  a clear statement "not to cache" for each query that could be eligible to the cache. It is sometimes difficult to control it especially because some rewrite of the queries are not controllable by the user (constant score query are sometimes added under the hood).
</comment><comment author="santanusinha" created="2016-01-20T10:27:03Z" id="173163666">@jimferenczi hmm .. yes, query rewrites might cause an issue here, but if the intent of the user is to _not_ do caching for a particular query, maybe cache=false should be set in the rewritten queries also .. right?
</comment><comment author="jimczi" created="2016-01-20T10:44:50Z" id="173168539">@santanusinha yes this is my point. If the user knows that the query should not be cached (even if the score is not needed) then we should have something in the query that state clearly that we don't want this part of the query to enter the filter cache.
</comment><comment author="clintongormley" created="2016-01-20T11:00:00Z" id="173171419">&gt; Typically analysts might run some one-off queries over older time ranges causing the filter cache to blow up without any reason

This should not longer happen as filters will only be cached after repeated use - this is one of the reasons for the rewrite, to stop overcaching filters by default.  Really, this is something that the user should't have to think about; Elasticsearch should be smart enough to figure it out for itself.  Of course, these algorithms need iteration to improve.

Another feature which is already there, but needs improvement, is [shard request caching](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/shard-request-cache.html)... to explain: a typical use case is showing page views per hour for the last month.  Using the index-per-day model, only the data for today's index is changing.  The request cache (you need to turn on caching) will cache the aggregation results for all of the other indices, and only recalculate the results for today's index - huge improvement.

But there are a couple of issues that we are working on fixing. The first is that the JSON request must be exactly the same in order to retrieve the cached version.  This can be tricky because the order of keys in JSON can vary.  The search refactoring happening in https://github.com/elastic/elasticsearch/issues/10217 will fix this because we'll use the parsed representation of the query for caching instead of the JSON.

The second is that these queries usually use a time range.  If you use `now`, that time will change on every request and so won't use the cache.  If you use `now/h` (now rounded to the nearest hour) then it can use the cached entry for the whole hour.

Once the search refactoring is done, we can improve this situation by checking whether the min and max values in range query are lower/higher respectively than the min/max values for a particular shard and, if so, rewrite the range query as a match_all. This would mean that, even though `now` is used with millisecond resolution, the request cache would still work.
</comment><comment author="santanusinha" created="2016-01-20T11:06:15Z" id="173173123">Thanks for the explanation. Will keep my eyes out for issues and report back if we see anything.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup the raw generics in ActionFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16107</link><project id="" key="" /><description>Mostly these were pretty easy to clean up by insisting that the request
and response stays consistent across the filter. There are a few places
where we have to make assumptions in tests but those are valid assumptions
for the test.
</description><key id="127593155">16107</key><summary>Cleanup the raw generics in ActionFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-20T02:57:11Z</created><updated>2016-01-21T02:54:49Z</updated><resolved>2016-01-21T02:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-20T02:57:44Z" id="173069870">More fancy java incantation to make the compiler happy. They really do help me relax.
</comment><comment author="jpountz" created="2016-01-20T13:12:36Z" id="173199084">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi-fields query could not match any document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16106</link><project id="" key="" /><description>I have created an index naming "test1", which included a msg property. The msg property used  smartcn analyzer, it was include an field naming "original" that was not analyzed.  And then inserted an document. The command likes following:

PUT http://localhost:9200/test1
{
  "mappings": {
    "my_type": {
      "properties": {
        "msg": {
          "type": "string",
          "analyzer" :"smartcn",
          "fields": {
            "original": { 
              "type":  "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT http://localhost:9200/test1/mytype/1
{
    "msg" : "&#25105;&#26159;&#20013;&#22269;&#20154;"
}

When I used the prefix query to match msg.original field, I could not match my document.  How could I match this document using prefix query for msg.original field? The commend like this:

POST http://localhost:9200/test1/_search?pretty
{
    "query": {
        "prefix": {
            "msg.original": "&#25105;"
        }
    }
}
</description><key id="127588896">16106</key><summary>multi-fields query could not match any document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin820224</reporter><labels /><created>2016-01-20T02:16:54Z</created><updated>2016-01-20T05:03:17Z</updated><resolved>2016-01-20T03:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2016-01-20T03:28:21Z" id="173074441">@robin820224  You create `my_type` mapping, but you register the data to `mytype`.

Please ask these questions in the forums: http://discuss.elastic.co/
</comment><comment author="selfchanger" created="2016-01-20T03:45:29Z" id="173077068">&#35828;&#20013;&#22269;&#35805;&#23601;&#26159;&#65306;&#20320;&#30340;my_type&#26159;&#20010;&#26144;&#23556;&#65292;&#19981;&#26159;&#32034;&#24341;&#65292;&#20320;&#24212;&#35813;&#24448;&#32034;&#24341;&#37324;&#23384;&#25968;&#25454;
</comment><comment author="robin820224" created="2016-01-20T05:03:17Z" id="173088813">yes, it works when I re-register the data to my_type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes info and stats output do not show tribe node clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16105</link><project id="" key="" /><description>/_cat/nodes?v run against the tribe node shows the following:

```
host      ip        heap.percent ram.percent load node.role master name                   
127.0.0.1 127.0.0.1                               c         x      tribe_cluster_node1/t2 
127.0.0.1 127.0.0.1                               c         x      tribe_cluster_node1/t1 
127.0.0.1 127.0.0.1                               c         x      tribe_cluster_node1    
127.0.0.1 127.0.0.1            5         100 2.69 d         x      cluster2_node1         
127.0.0.1 127.0.0.1            5         100 2.69 d         x      cluster1_node1  
```

You will see 1 node for each downstream cluster (there are 2 downstream clusters here each with a single node), the tribe node itself, and its corresponding tribe node clients that joined both clusters.

But if you hit the `_nodes` or `_nodes/stats` api against the tribe node, you will see that it only returns cluster1_node1, cluster2_node1 and tribe_cluster_node1 in the output.  The 2 tribe node clients are missing from the output (unlike the `_cat/nodes` output).  
</description><key id="127581006">16105</key><summary>Nodes info and stats output do not show tribe node clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>:Tribe Node</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-20T01:20:05Z</created><updated>2016-03-01T16:57:35Z</updated><resolved>2016-03-01T16:57:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T13:09:46Z" id="190717065">Related to https://github.com/elastic/elasticsearch/issues/16815
</comment><comment author="javanna" created="2016-03-01T16:57:35Z" id="190811566">This doesn't necessarily have to do with tribe nodes, as Clint pointed out above, it's more to do with client nodes. 

`_cat/nodes` reads first from the local cluster state, which is why you see there all the nodes. Note that if you run the same call on any other node in the cluster apart from the tribe node, you'll see the same nodes apart from the tribe node itself, as that one doesn't really join the cluster.

On the other hand nodes info and stats actually connect to all nodes in the cluster to gather information, and a client node cannot at the moment cannot to another client node as explained in #16815. Closing in favour of #16815.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>set -Djna.nosys when running tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16104</link><project id="" key="" /><description>This is consistent with what happens in elasticsearch.sh/.bat, and it means
tests will work even if there is a crazy "system" JNA installed on the machine.
</description><key id="127567550">16104</key><summary>set -Djna.nosys when running tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T23:50:57Z</created><updated>2016-01-20T03:59:55Z</updated><resolved>2016-01-20T03:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-20T02:12:09Z" id="173061490">Lgtm
</comment><comment author="rmuir" created="2016-01-20T03:59:55Z" id="173078731">I backported this to the maven 2.x build (https://github.com/elastic/elasticsearch/commit/b2d0ba21024440a71b4e3a19302549dcbb3c1e04) as well. 

cc: @drewr , let me know if you see this problem again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use our standard xlint with standalone-test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16103</link><project id="" key="" /><description>We were not changing the xlint settings there at all. Also cleans up some
generic array warnings that this found by switching them to an ArrayList.
</description><key id="127558806">16103</key><summary>Use our standard xlint with standalone-test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T22:57:07Z</created><updated>2016-01-19T23:28:13Z</updated><resolved>2016-01-19T23:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-19T22:57:47Z" id="173015243">These tests seem to have dodged our xlint settings. In my role as warning smasher I've brought them into the fold.
</comment><comment author="rjernst" created="2016-01-19T23:02:54Z" id="173016385">Looks good.
</comment><comment author="nik9000" created="2016-01-19T23:03:38Z" id="173016537">Thanks for the quick review @rjernst .
</comment><comment author="nik9000" created="2016-01-19T23:27:41Z" id="173021989">Noops. Had an error and of course these tests run late in the build and I didn't wait for it to get to them. I'm getting too confident.....
</comment><comment author="nik9000" created="2016-01-19T23:28:13Z" id="173022095">I pushed a fix: 3178d24beaf16d5ac12781727be012c6fba77c68
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bulk api: opt out individual item responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16102</link><project id="" key="" /><description>Is it possible to hide the individual item responses from the bulk response via some param? Often i'm just interested into the all or nothing response, such that checking errors: true/false is enough, but i have to parse the full json response just to do that.
</description><key id="127547986">16102</key><summary>bulk api: opt out individual item responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrkamel</reporter><labels /><created>2016-01-19T21:54:46Z</created><updated>2016-01-19T23:08:08Z</updated><resolved>2016-01-19T23:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-19T23:01:14Z" id="173016018">Have you tried [response filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering)? I've not tried it myself but I imagine it should work at shrinking the response. You should be careful and make sure that you keep enough information so you can be sure that the data made it and one of the responses wasn't some kind of error.
</comment><comment author="mrkamel" created="2016-01-19T23:08:08Z" id="173017534">perfect, thx, didn't know about that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add infrastructure to wrap/filter all clients exposed by the test clusters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16101</link><project id="" key="" /><description>This commit allows an integ test to wrap all clients that are exposed by the
InternalTestCluster which is useful for request interception or to add general headers
to request or to intercept client to server call even if the client calls are done from within
the test framework.
</description><key id="127517311">16101</key><summary>Add infrastructure to wrap/filter all clients exposed by the test clusters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T19:16:43Z</created><updated>2016-03-10T18:36:24Z</updated><resolved>2016-01-20T08:34:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-01-19T19:19:06Z" id="172957157">LGTM
</comment><comment author="rjernst" created="2016-01-19T19:19:31Z" id="172957291">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add wait_for_completion to reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16100</link><project id="" key="" /><description>It defaults to false and when false it returns a task identifier. Right
now all you can do is get the task to see if it is still running. Once
the task finishes it vanishes and you can't get any information about it.
</description><key id="127515358">16100</key><summary>Add wait_for_completion to reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>enhancement</label><label>review</label></labels><created>2016-01-19T19:05:47Z</created><updated>2016-02-02T14:29:07Z</updated><resolved>2016-02-01T16:13:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-19T19:07:24Z" id="172953953">@imotov this uses your recent change to get the task back.

We can't cancel the task or get information after its finished but one things at a time!
</comment><comment author="nik9000" created="2016-01-25T00:45:27Z" id="174358465">@imotov can you review this?
</comment><comment author="imotov" created="2016-01-26T03:07:07Z" id="174793035">@nik9000 I left one minor comment. Otherwise it looks good to me. There are a couple of general issues that I would love to discuss when you are back: 1) I am not sure if `false` is a good default for wait_for_completion and 2) should we try to provide a bit more friendly way to address tasks? I know that separate `node id` and `task id` is what I originally suggested. But I am thinking maybe we should use something like `node_id:task_id` or encode it somehow. I am open to ideas.
</comment><comment author="nik9000" created="2016-01-27T14:09:36Z" id="175649193">&gt; 1) I am not sure if false is a good default for wait_for_completion

Fair enough. Its the default that we have for snapshots and I figure it'd be nice to have the same default for `wait_for_completion` everywhere it is available.

&gt; node_id:task_id

What about the `node_id/task_id`. Its the format on the url. I can return all three:

```
{
  "task": {
    "node_id": "foobar",
    "task_id": 12345,
    "path": "foobar/12345"
  }
}
```

which is more in keeping with how some of our other responses look or I can do

```
{
   "task": "foobar/12345"
}
```

Either way the user has all the data they need.
</comment><comment author="clintongormley" created="2016-01-28T09:46:37Z" id="176090190">&gt; I know that separate node id and task id is what I originally suggested. But I am thinking maybe we should use something like node_id:task_id or encode it somehow. I am open to ideas.

From  a user perspective, the user is going to extract the task ID from the request and pass it directly to GET task-status.  If the task id has the format `node_id/task_id` then it will be encoded as `node_id%2Ftask_id`, so it won't match the `node_id/task_id` in the URL.

Perhaps the answer is a URL handler for the encoded form?
</comment><comment author="nik9000" created="2016-01-28T17:42:15Z" id="176298345">@clintongormley and @imotov - I've defaulted `wait_for_completion` to true. Let me know if you have any more opinions on the result format. I'm tempted to merge this as is and change the result format when we make a decision.
</comment><comment author="nik9000" created="2016-01-28T20:07:04Z" id="176379448">Ok - I had a look at logging the result like @imotov mentioned but that'll be easier once I get https://github.com/elastic/elasticsearch/pull/16029 in in some form so I'm going to skip it for this PR.
</comment><comment author="nik9000" created="2016-01-29T18:06:08Z" id="176890601">@imotov I've reduced the wrapping.
</comment><comment author="nik9000" created="2016-02-01T15:46:00Z" id="178033398">&gt; @imotov I've reduced the wrapping.

Any chance you could have a look at this soonish? It'd be nice to get this in.
</comment><comment author="imotov" created="2016-02-01T15:56:03Z" id="178040710">LGTM
</comment><comment author="nik9000" created="2016-02-01T16:13:23Z" id="178048079">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create default for ExecutableScript#unwrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16099</link><project id="" key="" /><description>Tons of scripts just return the variable they are passed and that is the
most intuitive behavior so that may as well be the default implementation.
</description><key id="127507473">16099</key><summary>Create default for ExecutableScript#unwrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T18:23:09Z</created><updated>2016-01-19T19:05:10Z</updated><resolved>2016-01-19T19:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-19T18:24:19Z" id="172941324">@jdconrad I mentioned this one to you so I figured I may as well do it.
</comment><comment author="jdconrad" created="2016-01-19T18:44:44Z" id="172947020">Awesome, thanks.  LGTM.
</comment><comment author="nik9000" created="2016-01-19T19:05:10Z" id="172953398">Thanks for the review @jdconrad .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency in multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16098</link><project id="" key="" /><description>There is a discrepancy in the behaviour of `multi_match` query when it comes to resolving field names containing wildcard characters.

The following query will return zero results if the field `subject` does not exists.

```
{
  "multi_match" : {
    "query":    "this is a test", 
    "fields": [ "subject" ] 
  }
}
```

But the following query will fail with an exception saying `"No fields specified for multi_match query"` if there is no field whose name starts with "ms" in the index being queried. Ideally it should return zero results too instead of failing right?

```
{
  "multi_match" : {
    "query":    "this is a test", 
    "fields": [ "ms*" ] 
  }
}
```

I have a use case where fields are dynamically added to the mapping and users can provide the field name to match search text on. Fields added to the mapping also have a metadata string appended to their names which the user may not be aware about. Hence I need to use wildcards in the field name in a `multi_match` query and there are chances that no field name matches too.
</description><key id="127499216">16098</key><summary>Inconsistency in multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-19T17:37:58Z</created><updated>2016-11-08T09:08:39Z</updated><resolved>2016-11-08T09:08:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-20T14:21:14Z" id="173217924">I agree.  Related to https://github.com/elastic/elasticsearch/issues/12016
</comment><comment author="michel-kraemer" created="2016-01-30T10:49:25Z" id="177145617">I bumped into this too. Is there a workaround that I can apply until this issue has been solved?
</comment><comment author="gmridul" created="2016-03-04T22:39:42Z" id="192504389">Is this fixed now?
I just pulled the code from master and the second query is returning zero results (I ensured that there is no field starting with 'ms').
</comment><comment author="qwerty4030" created="2016-07-20T03:04:20Z" id="233826454">@clintongormley Confirmed this is **not** reproducible on latest master with the following:

```
POST test-index/test-type/test-doc1
{
  "test-key":"test-value"
}

POST test-index/_search
{
  "query": {
    "multi_match": {
      "query": "test-query",
      "fields": [
        "doesnt-match*"
      ]
    }
  }
}
results in 0 hits
```

However the same query fails on 2.3.2. Which release will the changes from #12016 land in?
</comment><comment author="clintongormley" created="2016-07-21T10:35:29Z" id="234217562">This is fixed in master thanks to a massive refactoring of how searches are parsed - this obviously can't be backported.  If somebody wants to contribute a quick fix for 2.4, then please submit a PR.
</comment><comment author="andrewla" created="2016-11-04T15:43:37Z" id="258467086">One workaround this that I've found is to add a dummy field to the `fields` specification, like `"fields": ["dummydummydummy", "ms*"]`, which will trigger the behavior on missing fields without causing the wildcard misfiring.  Not sure if this makes the query significantly more expensive.
</comment><comment author="michel-kraemer" created="2016-11-07T20:18:32Z" id="258950328">This issue seems to be fixed in 5.0.0. As far as I am concerned it can be closed.
</comment><comment author="clintongormley" created="2016-11-08T09:08:39Z" id="259083141">Fixed in 5.0, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build on Windows 2012 fails with compilation failure from assigning to final variables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16097</link><project id="" key="" /><description>Compile issue that seems only to happen on Windows jdk as seen here:

http://build-us-00.elastic.co/job/es_core_master_window-2012/2315/

```
Compiling with Java command line compiler 'y:\jdk8\8u11\bin\javac'.
Starting process 'command 'y:\jdk8\8u11\bin\javac''. Working directory: Y:\jenkins\workspace\es_core_master_window-2012\core Command: y:\jdk8\8u11\bin\javac -J-Xmx1g @Y:\jenkins\workspace\es_core_master_window-2012\core\build\tmp\compileJava\java-compiler-args.txt
Successfully started process 'command 'y:\jdk8\8u11\bin\javac''
Y:\jenkins\workspace\es_core_master_window-2012\core\src\main\java\org\elasticsearch\cluster\metadata\AutoExpandReplicas.java:44: error: cannot assign a value to final variable min
            min = Integer.parseInt(sMin);
            ^
Y:\jenkins\workspace\es_core_master_window-2012\core\src\main\java\org\elasticsearch\cluster\metadata\AutoExpandReplicas.java:50: error: cannot assign a value to final variable max
            max = Integer.MAX_VALUE;
            ^
Y:\jenkins\workspace\es_core_master_window-2012\core\src\main\java\org\elasticsearch\cluster\metadata\AutoExpandReplicas.java:53: error: cannot assign a value to final variable max
                max = Integer.parseInt(sMax);
                ^
```

The same code compiles fine on other systems. I'll go ahead and remove the offending `final` modifier for now, but leave a comment linking to this issue.
</description><key id="127494655">16097</key><summary>Build on Windows 2012 fails with compilation failure from assigning to final variables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>build</label><label>jenkins</label></labels><created>2016-01-19T17:14:07Z</created><updated>2016-01-20T21:51:34Z</updated><resolved>2016-01-20T19:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-19T17:17:26Z" id="172923236">isn't it a jvm bug? shouldn't we upgrade to a more recent jdk instead of changing the code? I wonder if  the fact that it's a windows machine matters.
</comment><comment author="ywelsch" created="2016-01-19T17:28:53Z" id="172926289">JDK Version identifies as `Oracle Corporation 1.8.0_11 [Java HotSpot(TM) 64-Bit Server VM 25.11-b03]`. Can we update this to a more recent version @drewr ?
</comment><comment author="cbuescher" created="2016-01-19T17:40:37Z" id="172929324">Could be related to http://bugs.java.com/view_bug.do?bug_id=8051958, since it's code inside a lambda.
Pushing the fix wasn't meant as a permanent solution, @s1monw suggested to do it to get the windows build working again, following up on it with this issue, but I'm also happy to wait with it to see if updating the jdk helps.
</comment><comment author="cbuescher" created="2016-01-19T18:45:44Z" id="172947311">Master also doesn't compile on my machine (Mac OS) with jdk 1.8.0_25 and 1.8.0_31. More recent version (I tried u60 and u66) work.
</comment><comment author="cbuescher" created="2016-01-19T18:49:59Z" id="172949114">Same here, that machine probably also needs a fresher jdk version:

http://build-us-00.elastic.co/job/es_core_master_window-2008/
</comment><comment author="jasontedor" created="2016-01-19T18:50:58Z" id="172949465">&gt; Master also doesn't compile on my machine (Mac OS) with jdk 1.8.0_25 and 1.8.0_31. More recent version (I tried u60 and u66) work.

@cbuescher The same compiler bug as in this issue, or are you seeing type inference errors?
</comment><comment author="cbuescher" created="2016-01-19T18:52:28Z" id="172949888">@jasontedor Looks the same to me. I'd say I push the fix to remove the `final` modifier for now, can be re-added once those jdks have been updated, wdyt?
</comment><comment author="jasontedor" created="2016-01-19T19:11:46Z" id="172955163">@cbuescher Okay, I think that it is fixed in 8u40 by a backport of the bug you linked to in [JDK-8052388](http://bugs.java.com/view_bug.do?bug_id=8052388), and I confirmed that it fails on 8u31 on my machine too, but succeeds on 8u40.
</comment><comment author="cbuescher" created="2016-01-19T19:27:48Z" id="172959413">Pushed temporary fix with df5f1b1. We can revert this once infra has upgraded those servers.
</comment><comment author="cbuescher" created="2016-01-20T21:51:34Z" id="173372261">@jasontedor thanks for taking care of this, while http://build-us-00.elastic.co/job/es_core_master_window-2008/ looks good now, last build on http://build-us-00.elastic.co/job/es_core_master_window-2012/ still seems to hang. Will ping infra in above ticket, maybe they can abort the hanging build
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prefer nodes that previously held primary shard for primary shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16096</link><project id="" key="" /><description>Allocation ids are currently used during primary shard allocation to decide which nodes have non-stale copies of the shard data. Instead of just having a set of allocation ids representing the shard copies that have been last active, we can order this set by putting allocation ids of active primaries first. This allows the primary shard allocator to prefer allocation on nodes that previously held the active primary shard.

Relates to #14739
</description><key id="127488612">16096</key><summary>Prefer nodes that previously held primary shard for primary shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T16:46:32Z</created><updated>2016-01-20T15:32:56Z</updated><resolved>2016-01-20T15:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-19T20:27:04Z" id="172975914">Thanks @ywelsch . I was originally thinking of a simpler approach (at least, I think so) - extend NodeGatewayStartedShards to include the primary flag, which we store and use that. This means the change is limited to `PrimaryShardAllocator`. I like the simplicity of having a Set of active allocation ID, no preference, no ordering. 
</comment><comment author="ywelsch" created="2016-01-20T09:00:30Z" id="173134273">My concern for relying on `ShardStateMetaData` is that it can be out-of-sync with cluster state. I agree though that for this specific use case (which is just a soft preference) perfect synchrony does not matter and that your approach better scopes the change.
</comment><comment author="ywelsch" created="2016-01-20T10:05:28Z" id="173152502">@bleskes I updated the PR. can you have a look?
</comment><comment author="bleskes" created="2016-01-20T10:15:43Z" id="173158757">LGTM. Love the stats +81 -47 (most of it is tests).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient should use updated setting for initialization of modules and service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16095</link><project id="" key="" /><description>Today the TransportClient uses the given settings rather than the updated setting from the plugin
service to pass on to it's modules etc. It should use the updates settings instead.
</description><key id="127486871">16095</key><summary>TransportClient should use updated setting for initialization of modules and service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T16:39:25Z</created><updated>2016-01-19T17:03:48Z</updated><resolved>2016-01-19T17:03:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-19T16:39:32Z" id="172910414">@jaymode FYI
</comment><comment author="jaymode" created="2016-01-19T16:55:59Z" id="172916453">LGTM. Thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>document classloader whitelisting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16094</link><project id="" key="" /><description>As of 2.0, we restrict groovy scripts to run with minimal permissions

For 2.2 we apply same restrictions to python and javascript (https://github.com/elastic/elasticsearch/pull/13924), and we whitelist classloaders for all scripting engines by default (https://github.com/elastic/elasticsearch/pull/15262)

I expect a long tail of issues opened (e.g. crazy things happening in scripts), but the whitelisting is customizable for extreme cases via java's security configuration, as friendly as I could make it (e.g. supports wildcards), just not documented. 

I can provide some examples (e.g. disable whitelisting completely, add classes/packages, whatever). The whitelisting also doesn't happen if security manager is disabled.
</description><key id="127479090">16094</key><summary>document classloader whitelisting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>docs</label></labels><created>2016-01-19T16:04:28Z</created><updated>2016-02-01T08:39:08Z</updated><resolved>2016-02-01T08:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-19T16:28:27Z" id="172907254">good call @rmuir thanks for picking this up @clintongormley can you take a look at this and maybe come up with some good way of documenting it?
</comment><comment author="clintongormley" created="2016-01-19T16:58:43Z" id="172917283">@rmuir if you could provide some examples, i'd be happy to write it up as docs and find somewhere to put it.  thanks
</comment><comment author="rmuir" created="2016-01-19T17:04:39Z" id="172919449">Do you think the simplest solution is to just document disabling security manager if you are unhappy with the whitelisting? Its easier to think about, less risky as far as breaking everything in confusing ways (e.g. invalid syntax or whatever in security configuration).

I really think we should avoid using it as a cop-out, but we have a new scripting language as a long term solution?
</comment><comment author="clintongormley" created="2016-01-19T17:09:31Z" id="172920819">I think most people would prefer to run with the security manager enabled, unless they really can't make things work, so it is probably worth the effort documenting whitelisting.  I'm assuming it's not too hard to do, and it'll be for the minority of users.
</comment><comment author="rmuir" created="2016-01-20T07:34:45Z" id="173118651">ok, i was just thinking if you were security-conscious at all, you wouldn't be wanting scripts to do even more :) And enable/disable security manager completely would be simpler than changing java security config.

if someone wants to tweak this in a fine-grained way, then they need to add lines to their policy file, either system wide (`$JAVA_HOME/lib/security/java.policy`), or in their user's home directory (`~/.java.policy`), or from a file specified via commandline (`-Djava.security.policy=someURL`). See http://docs.oracle.com/javase/7/docs/technotes/guides/security/PolicyFiles.html for more information.

grant entries would look like this:

```
grant {
    permission org.elasticsearch.script.ClassPermission "java.util.Base64"; // allow class
    permission org.elasticsearch.script.ClassPermission "java.util.*"; // allow package
    permission org.elasticsearch.script.ClassPermission "*"; // allow all (disables filtering basically)
};
```

See https://github.com/elastic/elasticsearch/blob/01ce49e94ee7dcd9a8696b53a84e2a0184284a93/core/src/main/java/org/elasticsearch/script/ClassPermission.java#L31-L73 for more details of that.
</comment><comment author="clintongormley" created="2016-01-20T15:10:01Z" id="173232622">thanks @rmuir - i'll give it a shot in a pr
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Retry search and bulk rejections in reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16093</link><project id="" key="" /><description>Right now any bulk and search rejections due to full thread pools just terminate the whole thing. That is silly. We should backoff and retry.
</description><key id="127474429">16093</key><summary>Retry search and bulk rejections in reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-01-19T15:45:09Z</created><updated>2016-03-16T19:44:01Z</updated><resolved>2016-02-26T21:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-26T21:33:51Z" id="189490334">Fixed by https://github.com/elastic/elasticsearch/pull/16556
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More robust handling of CORS HTTP Access Control</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16092</link><project id="" key="" /><description>Uses a refactored version of Netty's CORS implementation to provide more
robust cross-origin resource request functionality.  The CORS specific
Elasticsearch parameters remain the same, just the underlying
implementation has changed.

It has also been refactored in a way that allows dropping in Netty's
CORS handler as a replacement once Elasticsearch is upgraded to Netty 4.
</description><key id="127464753">16092</key><summary>More robust handling of CORS HTTP Access Control</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T15:02:54Z</created><updated>2016-02-13T23:28:25Z</updated><resolved>2016-02-03T20:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-01-22T06:22:50Z" id="173823104">@spinscale I made some changes as overriding the MessageEvent didn't work.  One change from how CORS handling previously worked is, before if an origin was not allowed, it would return a 200 but omit the "allow-origin" value in the response header.  Now, we return a 403 - this how Netty implements it too.
</comment><comment author="spinscale" created="2016-01-29T11:32:01Z" id="176704456">left a few comments, I think we can speed up tests and clean it up a little. Is this rebased against the latest master? Just want to make sure all the settings changed are in, and you're not surprised by those
</comment><comment author="abeyad" created="2016-01-29T20:58:02Z" id="176966928">@spinscale I implemented the suggested changes.  If you could take a look and let me know if its ready to merge.  Thanks!
</comment><comment author="spinscale" created="2016-02-02T14:58:35Z" id="178616996">left minor comments, apart from that LGTM.

I think it might make sense to take the next step here and move netty into its own module, then we can easily get rid of this again in our own codebase once we upgrade to netty4, by using the netty cors handler.
</comment><comment author="abeyad" created="2016-02-02T15:02:22Z" id="178619248">From what I could tell, the Netty CORS handler does not support regular expressions, which is something ES supports.  So I had to modify the Netty CORS handler as such and if we want to continue to support regular expressions for the origin value (other than "*" for any origin), then we can't just use a drop in replacement (unless of course Netty is adding regular expression capability to their CORS handler).
</comment><comment author="abeyad" created="2016-02-03T00:05:24Z" id="178898803">@spinscale I pushed up changes and also rebased with master, let me know what you think.
</comment><comment author="spinscale" created="2016-02-03T20:42:11Z" id="179454511">tests passed on local run, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate known global settings on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16091</link><project id="" key="" /><description>Today we already validate all index level setting on startup. For global
settings we are not fully there yet since not all settings are registered.
Yet we can already validate the ones that are know if their values are parseable/correct.
This is better than nothing and an improvement to what we had before. Over time there will
be more an dmore setting converted and we can finally flip the switch.
</description><key id="127457801">16091</key><summary>Validate known global settings on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T14:28:39Z</created><updated>2016-01-20T13:57:56Z</updated><resolved>2016-01-19T15:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-19T14:32:10Z" id="172870739">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse aggregations on coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16090</link><project id="" key="" /><description /><key id="127456942">16090</key><summary>Parse aggregations on coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>WIP</label></labels><created>2016-01-19T14:24:14Z</created><updated>2016-02-09T12:16:13Z</updated><resolved>2016-02-09T12:16:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Shard failure requests for non-existent shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16089</link><project id="" key="" /><description>This commit adds handling on the master side for shard failure requests
for shards that do not exist at the time that they are processed on the
master node (whether it be from errant requests, duplicate requests, or
both the primary and replica notifying the master of a shard
failure). This change is made because such shard failure requests should
always be considered successful (the failed shard is not there anymore),
but could be marked as failed if batched with a shard failure request
that does in fact fail.

Relates #14252 
</description><key id="127448233">16089</key><summary>Shard failure requests for non-existent shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T13:37:30Z</created><updated>2016-01-26T21:41:38Z</updated><resolved>2016-01-26T21:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-20T16:31:20Z" id="173262634">@bleskes I've updated this pull request.
</comment><comment author="bleskes" created="2016-01-21T14:01:35Z" id="173577759">Thanks @jasontedor . Left some comments/questions...
</comment><comment author="jasontedor" created="2016-01-23T20:00:47Z" id="174216356">@bleskes I pushed two commits.
</comment><comment author="bleskes" created="2016-01-26T16:15:19Z" id="175096574">LGTM. Left some suggestions for the tests, non of it is blocking pushing this.
</comment><comment author="jasontedor" created="2016-01-26T21:41:38Z" id="175245354">Thanks for another great review @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more tests for Azure Repository client selection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16088</link><project id="" key="" /><description>One test we forgot in #14843 and #13779 is the default client selection.

Most of the time, users won't define explicitly which client they want to use because they are providing only one connection to Azure storage:

``` yml
cloud:
    azure:
        storage:
            my_account:
                account: your_azure_storage_account
                key: your_azure_storage_key
```

Then using the default client like this:

``` sh
# This one will use the default account (my_account1)
curl -XPUT localhost:9200/_snapshot/my_backup1?pretty -d '{
  "type": "azure"
}'
```

This commit adds tests to check that the right client is still selected when no client is explicitly set when creating the snapshot.
</description><key id="127445502">16088</key><summary>Add more tests for Azure Repository client selection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>test</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T13:24:36Z</created><updated>2016-01-19T16:32:09Z</updated><resolved>2016-01-19T16:23:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-01-19T16:02:03Z" id="172899205">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace server side timeout by client side timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16087</link><project id="" key="" /><description>This commit replaces server side timeout (which is BTW not correctly implemented in azure client 2.0.0 but fixed later #16084) with a client side timeout.

As a consequence, for each request sent to azure, azure client will raise an exception after a given amount of time (timeout).

Closes #12567.
</description><key id="127440532">16087</key><summary>Replace server side timeout by client side timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>enhancement</label><label>review</label></labels><created>2016-01-19T12:57:38Z</created><updated>2016-01-20T15:14:22Z</updated><resolved>2016-01-19T17:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-01-19T15:53:33Z" id="172896028">LGTM
</comment><comment author="craigwi" created="2016-01-19T18:58:52Z" id="172951653">Based on reading the sources for azure-storage-java, calling setTimeoutIntervalInMs() will cause both a server side timeout and a client side timeout to be set up.  The server side timeout is rounded down to a whole number of seconds; the client side timeout is calculated in Utility.getRemainingTimeout() and is equal to "timeoutIntervalInMs + Constants.DEFAULT_READ_TIMEOUT" (the later constant is equal to 5 minutes).

Getting timeouts right is really challenging.  My original goal was to have some timeout rather than none and choose a default timeout that is longer than any single read or write during snapshots.

This can be done, of course, with either client or server side timeouts.  I would be cautious about using setMaximumExecutionTimeInMs because it is not clear (from reading the source) what the interaction is with retry policies and not clear when the clock starts ticking.
</comment><comment author="dadoonet" created="2016-01-19T19:32:54Z" id="172960739">So that explains why my test does not fail after 1 s but probably after 1s + 5 mn.
</comment><comment author="dadoonet" created="2016-01-20T09:49:23Z" id="173148127">@craigwi So as I wrote there: https://github.com/elastic/elasticsearch/pull/16084#issuecomment-173147292 I think we should:
- revert this change
- merge #16084 

I'll update #16084 accordingly. cc @tlrx 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search not recognizing --cluster.name and --node.name parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16086</link><project id="" key="" /><description>Hi Team,

I had downloaded elasticsearch-2.1.1.zip and tried to change the node name through command line as given in the following documentation page:
https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html

&gt; elasticsearch.bat --cluster.name my_cluster_name --node.name my_node_name

I get the following error:
Unrecognized option: --cluster.name
</description><key id="127437387">16086</key><summary>Elastic search not recognizing --cluster.name and --node.name parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harishkannarao</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-01-19T12:41:19Z</created><updated>2017-01-04T12:11:04Z</updated><resolved>2016-02-05T16:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-01-20T14:27:42Z" id="173220312">This is a bug. Pull request to fix it is here: https://github.com/elastic/elasticsearch/pull/15320
</comment><comment author="clintongormley" created="2016-02-05T16:27:41Z" id="180429044">Closed by #15320
</comment><comment author="chandankmr" created="2017-01-04T05:59:36Z" id="270300477">@brwe this bug persists in version 5.5.1 as well. i have tested it on ubuntu 15.04 .Please suggesst a fix for this.  </comment><comment author="jasontedor" created="2017-01-04T12:11:04Z" id="270357365">@chandankmr The short answer to your question is that parameters of the form expressed here are not supported in Elasticsearch 5.x. This is covered in the [migration docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking_50_settings_changes.html#_removed_using_double_dashes_to_configure_elasticsearch) and elsewhere in the docs. If you require additional general help, please ask on the [forum](https://discuss.elastic.co).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for MatchQueryBuilderTests.testToQuery random fuzziness generation caused test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16085</link><project id="" key="" /><description>For all query testing we offer the option to initialise them with random
fuzziness objects. So far there is a chance to generate an edit distance based
fuzziness for non-string fields. This is fixed by this change.

Fixes the following test failures:
- http://build-us-00.elastic.co/job/es_core_master_small/6606/
- http://build-us-00.elastic.co/job/es_core_master_centos/9555/
</description><key id="127434281">16085</key><summary>Fix for MatchQueryBuilderTests.testToQuery random fuzziness generation caused test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T12:23:48Z</created><updated>2016-01-25T09:25:57Z</updated><resolved>2016-01-19T12:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-19T14:00:30Z" id="172861625">Thanks for cleaning up my mess! I should have run this test another couple hundred times....
</comment><comment author="MaineC" created="2016-01-20T08:55:41Z" id="173133325">@nik9000 This particular problem was actually not caused by your change - it was just your change that highlighted an issue in how random fuzziness was generated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Azure Storage client to 4.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16084</link><project id="" key="" /><description>We are using `2.0.0` today but Azure team now recommends:

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-storage&lt;/artifactId&gt;
    &lt;version&gt;4.0.0&lt;/version&gt;
&lt;/dependency&gt;
```

This new version fix the timeout issues we have seen with azure storage although #15080 adds a timeout support.
Azure storage client 2.0.0 was not passing correctly this value when it was calling Azure services.

Note that the timeout is a server side timeout and not client side timeout.
It means that it will raise only a timeout when:
- upload of blob is complete
- if azure service is not able to process the blob (and store it) within a given time range.

In which case it will raise an exception which elasticsearch can deal with:

```
java.io.IOException
    at __randomizedtesting.SeedInfo.seed([91BC11AEF16E073F:6886FA5308FCE4D8]:0)
    at com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:643)
    at com.microsoft.azure.storage.blob.BlobOutputStream.writeBlock(BlobOutputStream.java:444)
    at com.microsoft.azure.storage.blob.BlobOutputStream.access$000(BlobOutputStream.java:53)
    at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:388)
    at com.microsoft.azure.storage.blob.BlobOutputStream$1.call(BlobOutputStream.java:385)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: com.microsoft.azure.storage.StorageException: Operation could not be completed within the specified time.
    at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)
    at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:305)
    at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:175)
    at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlockInternal(CloudBlockBlob.java:1006)
    at com.microsoft.azure.storage.blob.CloudBlockBlob.uploadBlock(CloudBlockBlob.java:978)
    at com.microsoft.azure.storage.blob.BlobOutputStream.writeBlock(BlobOutputStream.java:438)
    ... 9 more
```

The following code was used to test this against Azure platform:

``` java
public void testDumb() throws URISyntaxException, StorageException, IOException, InvalidKeyException {
    String connectionString = "MY-AZURE-STRING";

    CloudStorageAccount storageAccount = CloudStorageAccount.parse(connectionString);
    CloudBlobClient client = storageAccount.createCloudBlobClient();
    client.getDefaultRequestOptions().setTimeoutIntervalInMs(1000);
    CloudBlobContainer container = client.getContainerReference("dumb");
    container.createIfNotExists();
    CloudBlockBlob blob = container.getBlockBlobReference("blob");

    File sourceFile = File.createTempFile("sourceFile", ".tmp");

    try {
        int fileSize = 10000000;

        byte[] buffer = new byte[fileSize];
        Random random = new Random();
        random.nextBytes(buffer);

        logger.info("Generate local file");
        FileOutputStream fos = new FileOutputStream(sourceFile);
        fos.write(buffer);
        fos.close();
        logger.info("End generate local file");

        FileInputStream fis = new FileInputStream(sourceFile);

        logger.info("Start uploading");
        blob.upload(fis, fileSize);
        logger.info("End uploading");

    }
    finally {
        if (sourceFile.exists()) {
            sourceFile.delete();
        }
    }
}
```

With 2.0.0, the above code was not raising any exception. With 4.0.0, the exception is now thrown correctly.

Closes #12567.

Release notes from 2.0.0:
- Removed deprecated table AtomPub support.
- Removed deprecated constructors which take service clients in favor of constructors which take credentials.
- Added support for "Add" permissions on Blob SAS.
- Added support for "Create" permissions on Blob and File SAS.
- Added support for IP Restricted SAS and Protocol SAS.
- Added support for Account SAS to all services.
- Added support for Minute and Hour Metrics to FileServiceProperties and added support for File Metrics to CloudAnalyticsClient.
- Removed deprecated startCopyFromBlob() on CloudBlob. Use startCopy() instead.
- Removed deprecated Credentials and StorageKey classes. Please use the appropriate methods on StorageCredentialsAccountAndKey instead.
- Fixed a bug in table where a select on a non-existent field resulted in a null reference exception if the corresponding field in the TableEntity was not nullable.
- Fixed a bug in table where JsonParser was automatically closing the response stream before it was completely drained causing socket exhaustion.
- Fixed a bug in StorageCredentialsAccountAndKey.updateKey(String) which prevented valid keys from being set.
- Added CloudBlobContainer.listBlobs(final String, final boolean) method.
- Fixed a bug in blob where using AccessConditions on block blob uploads larger than 64MB done with the upload\* methods or block blob uploads done openOutputStream with would fail if the blob did not already exist.
- Added support for setting a proxy per request. Proxy can be set on an OperationContext instance and will be used when that instance is passed to the request method.
- Added support for SAS to the Azure File service.
- Added support for Append Blob.
- Added support for Access Control Lists (ACL) to File Shares.
- Added support for getting and setting of CORS rules to File service.
- Added support for ShareStats to File Shares.
- Added support for copying an Azure File to another Azure File or a Block Blob asynchronously, and aborting Azure File copy operations asynchronously.
- Added support for copying a Blob to an Azure File asynchronously.
- Added support for setting a maximum quota property on a File Share.
- Removed deprecated AuthenticationScheme and its getter and setter. In the future only SharedKey will be used.
- Removed deprecated getter/setters for all request option properties on the service clients. Please use the default request options getter/setters instead.
- Removed getSubDirectoryReference() for blob directories and file directories. Use getDirectoryReference() instead.
- Removed getEntityClass() in TableQuery. Please use getClazzType() instead.
- Added client-side verification for lease duration and break periods.
- Deprecated the setters in table for timestamp as this property is only modifiable by the service.
- Deprecated startCopyFromBlob() on CloudBlob. Use startCopy() instead.
- Deprecated the Credentials and StorageKey classes. Please use the appropriate methods on StorageCredentialsAccountAndKey instead.
- Deprecated constructors which take service clients in favor of constructors which take credentials.
- Fixed a bug where the DateBackwardCompatibility flag was not applied if set on the CloudTableClient default request options.
- Changed library behavior to retry all exceptions thrown when parsing a response object.
- Changed behavior to stop removing query parameters passed in with the resource URI if that URI contains a SAS token. Some query parameters such as comp, restype, snapshot and api-version will still be removed.
- Added support for logging StringToSign to SharedKey and SAS.
- **Added a connect timeout to prevent hangs when establishing the network connection.**
- **Made performance enhancements to the BlobOutputStream class.**
- Fixed a bug where maximum execution time was ignored for file, queue, and table services.
- **Changed the socket timeout to be set to the service side timeout plus 5 minutes when maximum execution time is not set.**
- **Changed the socket timeout to default to 5 minutes rather than infinite when neither service side timeout or maximum execution time are set.**
- Fixed a bug where MD5 was calculated for commitBlockList even though UseTransactionalMD5 was set to false.
- Fixed a bug where selecting fields that did not exist returned an error rather than an EntityProperty with a null value.
- Fixed a bug where table entities with a single quote in their partition or row key could be inserted but not operated on in any other way.
- Fixed a bug for all listing API's where next() would sometimes throw an exception if hasNext() had not been called even if there were more elements to iterate on.
- Added sequence number to the blob properties. This is populated for page blobs.
- Creating a page blob sets its length property.
- Added support for page blob sequence numbers and sequence number access conditions.
- Fixed a bug in abort copy where the lease access condition was not sent to the service.
- Fixed an issue in startCopyFromBlob where if the URI of the source blob contained certain non-ASCII characters they would not be encoded appropriately. This would result in Authorization failures.
- Fixed a small performance issue in XML serialization.
- Fixed a bug in BlobOutputStream and FileOutputStream where flush added data to a request pool rather than immediately committing it to the Azure service.
- Refactored to remove the blob, queue, and file package dependency on table in the error handling code.
- Added additional client-side logging for REST requests, responses, and errors.

Closes #15976.
</description><key id="127421547">16084</key><summary>Upgrade Azure Storage client to 4.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>upgrade</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T11:06:22Z</created><updated>2016-03-02T09:11:19Z</updated><resolved>2016-02-29T14:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-19T11:08:13Z" id="172819381">Note that we need to backport it to https://github.com/elastic/elasticsearch-cloud-azure/ as a follow up for https://github.com/elastic/elasticsearch-cloud-azure/issues/109
</comment><comment author="tlrx" created="2016-01-20T09:31:01Z" id="173143978">Why did we set a default timeout of `5m`? (see https://github.com/elastic/elasticsearch/issues/14277). According to the documentation I'm wondering how it handles the case when a blob upload takes more than 5 m:

&gt; The server timeout interval begins at the time that the complete request has been received by the service, and the server begins processing the response. If the timeout interval elapses before the response is returned to the client, the operation times out. 
</comment><comment author="dadoonet" created="2016-01-20T09:46:22Z" id="173147292">@tlrx So according to this thread https://github.com/elastic/elasticsearch/pull/16087#issuecomment-172951653 we should revert my last change #16087 and use only `setTimeoutIntervalInMs` and merge the current PR. 

Clock starts when the blob is finished uploaded.

Which means that it's the time for Azure to "process" the blob (store it basically - I don't know the internals).
In that case, a default value of `5m` looks reasonable to me because for now we are only uploading small chunks of data (&lt; 64mb): #12448

Then the global request timeout is set to `5m` + `Constants.DEFAULT_READ_TIMEOUT` which is `5m`. It means that after 10 minutes by default, elasticsearch will raise an exception (remember that we are talking about 64mb max here).

So it looks conservative to me and a good default.

We might want to revisit this default value once we will have implemented #12448.

Make sense?
</comment><comment author="dadoonet" created="2016-01-20T09:59:06Z" id="173151120">@tlrx I added a new commit.
</comment><comment author="dadoonet" created="2016-01-22T15:27:42Z" id="173949940">@tlrx and I chatted about this today and we came to those conclusions:
- elasticsearch should use the default timeouts set in the azure client. We should not set any default on our end
- we should let user modify this default azure timeout if they really need to

To do that, we need to:
- upgrade the azure client to 4.0.0 as it comes with a default socket timeout (5 minutes).
- only set socket timeout with `setMaximumExecutionTimeInMs` to a value if the user explicitly defined one
- update the documentation accordingly

I'll update the current PR based on this.
</comment><comment author="dadoonet" created="2016-01-24T17:40:15Z" id="174322828">@tlrx I added a new commit (and rebased BTW)
</comment><comment author="dadoonet" created="2016-02-21T23:28:33Z" id="186943619">@tlrx I rebased on master. Could you give a final look please?
</comment><comment author="tlrx" created="2016-02-29T09:42:07Z" id="190125099">Left a minor comment, otherwise LGTM
</comment><comment author="dadoonet" created="2016-02-29T15:07:35Z" id="190250669">@clintongormley I merged this change in `2.2`, `2.x` and `master`. I'm not sure if I should merge it in 2.1 and 2.0. I removed the labels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16083</link><project id="" key="" /><description>Hello,

I've had such an issue for 3 times now for the last 3 days. For me, It's absolutely random. Have no clues where to start investigating. 

Elasticsearch 2.1 is running on software linux RAID1. Disks are fine (according to S.M.A.R.T.), fsck passes succesfully each time, there are plenty of free disk space. System's dmesg and syslog are clean.

Thanks for help!

[2016-01-19 03:48:29,354][ERROR][index.engine             ] [kibana4] [logstash-2016.01.19][1] failed to merge
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=9ea446d9 actual=fb46cdc (resource=BufferedChecksumIndexInput(NIOFSIndexInput
(path="/data/elasticsearch2/data/kibana4/nodes/0/indices/logstash-2016.01.19/1/index/_xo.cfs") [slice=_xo_Lucene50_0.doc]))
        at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:335)
        at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:449)
        at org.apache.lucene.codecs.lucene50.Lucene50PostingsReader.checkIntegrity(Lucene50PostingsReader.java:1299)
        at org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.checkIntegrity(BlockTreeTermsReader.java:336)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.checkIntegrity(PerFieldPostingsFormat.java:317)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:96)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:193)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:95)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrent
</description><key id="127420155">16083</key><summary>org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mobidevadmin</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2016-01-19T10:57:50Z</created><updated>2016-07-19T10:27:39Z</updated><resolved>2016-02-02T14:04:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-20T13:05:54Z" id="173197718">Hi @mobidevadmin 

While we had issues with bad legacy checksums back in 1.4, the only time I've seen reports like this recently has been from real corruption.  It's always possible that some new bug has been introduced, but I think on balance that the likeliest issue is with the file system/hardware.

Could I suggest migrating to a non-raided, locally mounted filesystem and see if the issue disappears?
</comment><comment author="mobidevadmin" created="2016-01-20T13:50:23Z" id="173210064">Hello,

thanks for the response! The migration is possible (and i will most likely go this way) but I have another elasticsearch indices storage on same RAID massive, and it's totally fine, but it's version is 1.7.3. And also other files from same RAID are perfectly fine. Unlike this ELK installation which fails once a day (at night in my case).
</comment><comment author="jpountz" created="2016-01-20T14:31:44Z" id="173221300">For the record, we only added checks that checksums match on merge in 2.0, so it might be that your 1.7 data is corrupted as well but that the corruption has not surfaced yet.
</comment><comment author="mobidevadmin" created="2016-01-20T14:38:19Z" id="173222864">Hi @jpountz,

thanks for the info. Sorry for noobie question, is there a way to check elasticsearch 1.7 for such errors manually?
</comment><comment author="jpountz" created="2016-01-20T14:58:24Z" id="173228460">I think the easier way would be to use the lucene jar that is in the lib directory of elasticsearch (lucene-core-4.x.jar) and use it to run the CheckIndex utility. The command should look something like `java -cp /usr/share/elasticsearch/lib/lucene-core-4.10.4.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex $datadir/nodes/0/indices/$index/$shard/index/`. Note that it is a bit tedious as it needs to be run for every shard. It might be slow as well given that this command will not only verify checksums but also make sure that the index can actually be read and that its datastructures are consistent with ach other.
</comment><comment author="Lin-Brian" created="2016-02-01T08:31:49Z" id="177843806">Hi  @jpountz   ,
After I used CheckIndex 
WARNING: 4 broken segments (containing 16202693 documents) detected
WARNING: would write new segments file, and 16202693 documents would be lost, if -fix were specified
And -fix it
But my index status is also Unassigned , cant use cluster.reroute API  
`nested: ElasticsearchIllegalArgumentException[[allocate] trying to allocate a primary shard [logstash-2015.12.12][1], which is disabled]; ","status":400}`
How to enable the index?
</comment><comment author="clintongormley" created="2016-02-02T14:04:35Z" id="178586260">@Lin-Brian your index is corrupt.  Either reindex your documents from source or use the -fix option (and lose the corrupted documents).
</comment><comment author="suiyuan2009" created="2016-07-19T09:45:45Z" id="233583009">I use ubuntu to run lucene(not ES), and nfs to mount disk(actually is rocketstor) of macmini. I get this error constantly. @clintongormley any advise? I have already used `org.apache.lucene.index.CheckIndex`, but will still crash after restart program. 

```
Exception in thread "main" org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
        at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:724)
        at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:738)
        at org.apache.lucene.index.IndexWriter.numDocs(IndexWriter.java:1198)
        at xxxx.xxxxx.search.XxxxxxxxIndexer.close(XxxxxxxxIndexer.java:184)
        at xxxx.xxxxx.search.ThreadedXxxxxxxxIndexer.close(ThreadedXxxxxxxxIndexer.java:59)
        at xxxx.xxxxx.search.ThreadedXxxxxxxxIndexer.main(ThreadedXxxxxxxxIndexer.java:136)

Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=51fbdb5c actual=6e964d17 (resource=BufferedChecksumIndexInput(MMapIndexInput(path="/mnt/HPT8_56T/xxxxxxxxx-index/index1/_mq.cfs") [slice=_mq_Lucene50_0.pos]))
        at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:365)
        at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:469)
        at org.apache.lucene.codecs.lucene50.Lucene50PostingsReader.checkIntegrity(Lucene50PostingsReader.java:1286)
        at org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.checkIntegrity(BlockTreeTermsReader.java:336)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.checkIntegrity(PerFieldPostingsFormat.java:317)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:96)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:211)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3679)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)
```
</comment><comment author="clintongormley" created="2016-07-19T10:23:57Z" id="233591575">@suiyuan2009 Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="suiyuan2009" created="2016-07-19T10:27:39Z" id="233592348">@clintongormley thanks, I will move to the forum
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Still getting "high disk watermark exceeded on one or more nodes, rerouting shards"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16082</link><project id="" key="" /><description>With reference to #15919, I have changed the share path to SAN volume instead of NAS but still getting the same error. Is there any issue with Elasticsearch if path.data is shared volume?
</description><key id="127416155">16082</key><summary>Still getting "high disk watermark exceeded on one or more nodes, rerouting shards"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">najamss</reporter><labels /><created>2016-01-19T10:35:15Z</created><updated>2016-01-20T08:09:09Z</updated><resolved>2016-01-19T19:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-19T16:59:51Z" id="172917752">@najamss do you mean this warning?

```
 [WARN ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark [0b] exceeded on [O2-Ef7fET9S_MJNAL-q_yA][Desmond Pitt] free: -1b[100%], shards will be relocated away from this node
```

If so, that means that your JVM is still not reporting disk numbers for that mount
</comment><comment author="najamss" created="2016-01-19T19:43:34Z" id="172963922">Well, I got the same reply before as well. What can I do to fix it?
</comment><comment author="dakrone" created="2016-01-19T19:56:06Z" id="172967634">@najamss you can disable the disk-based allocation using `cluster.routing.allocation.disk.threshold_enabled`, see https://www.elastic.co/guide/en/elasticsearch/reference/2.1/disk-allocator.html

In the future though, these questions should be opened on https://discuss.elastic.co
</comment><comment author="najamss" created="2016-01-20T08:08:55Z" id="173124078">In fact, I did open this question over there but they asked me to open here. To disable the setting, is this command correct?

curl -XPUT localhost:9200/_cluster/settings -d '{"persistent" : {"cluster.routing.allocation.disk.threshold_enabled": False}}'

Second, if these warnings are being generated, will it stop dumping the logs to disk because this is what happening right now?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>backport repository-hdfs to 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16081</link><project id="" key="" /><description>works as it does on master for the most part. The way in that it works is pretty evil, but it is what it is, its maven.

Similar to mapper-attachments, pom.xml has autogenerated dependencies.

The miniHDFS fixture runs via ant. Its not perfect, maybe even _screwed up_ the way it works, but I'm not investing any more time in this, here it is. :)

@rjernst tried to help with ESSingleNodeTestCase stuff, well its working here, but causes tests in core/ to fail. So that needs to get straightened out. In general i only got the hdfs plugin working and didnt bother with any other qa tests (especially given the SingleNodeTestCase issues)

I put adoptme on this, because I don't plan on seeing it through anymore, I just wanted to help with the annoying parts.
</description><key id="127413079">16081</key><summary>backport repository-hdfs to 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Repository HDFS</label><label>adoptme</label></labels><created>2016-01-19T10:20:47Z</created><updated>2016-08-26T13:27:06Z</updated><resolved>2016-06-27T13:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ability to have init script for scripted fields or scripted filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16080</link><project id="" key="" /><description>I have have a set of documents which have a field of dates when a user did an action.  I want to look through and count the number of dates they have in a specified range. I was  make the requests with string dates like "2015-12-01" and using a `SimpleDateFormat` to convert the string to a Long for conversion.  The groovy script is:

```
sdf = new java.text.SimpleDateFormat('yyyy-MM-dd');
sdf.setTimeZone(TimeZone.getTimeZone('GMT+0')); 
start = sdf.parse(start_date).getTime();
end = sdf.parse(end_date).getTime();
dates = doc['dates'].values.findAll { it&gt;=start &amp;&amp; it&lt;=end  }; 
return dates.size()&gt;limit
```

The performance is actually pretty good - this runs on around 3 seconds for 1.5M documents on a 3 machine cluster.

But really on each document I'm creating a new SimpleDateFormat and parsing the string dates, even though it's the same for each document.

If I instead convert this to a timestamp before, the script becomes

```
dates = doc['dates'].values.findAll { it&gt;=start_ts &amp;&amp; it&lt;=end_ts}}  }; 
return dates.size()&gt;limit ;
```

and it runs in less than half the time.  This works and is fine, but it loses the convenience of having an easy to read date like `2015-12-01` and instead uses `1448928000000`.  It also would be nice to be able to specify the range with single date and number of days before - if I had a script that could run first and then setup the variables, this would be easy and fast.

My particular case is using templated queries, so perhaps it could even been added as a script to modify the variables before the template (I need to run this type of query with other permutations so the time savings add up.)  But even for simple scripted fields queries this can be valuable when I need have a common setup.  

I currently feel this most with dates - but it could have implications for other queries where I want need to build a common situation for comparison.  
</description><key id="127412585">16080</key><summary>ability to have init script for scripted fields or scripted filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yehosef</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2016-01-19T10:19:14Z</created><updated>2016-02-12T11:18:49Z</updated><resolved>2016-02-12T11:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-20T13:01:01Z" id="173196805">Hi @yehosef 

I understand what you're asking for and I can see the benefits, but it would end up making the scripting API horribly over complicated.  I think, if you need to do this, then the right approach would be to make a native (ie Java) script which already supports this kind of initialization.
</comment><comment author="yehosef" created="2016-01-20T15:44:17Z" id="173244018">I've been considering making a native script.   I hear what you're saying about making the API complicated since it might be different per scripted application (eg, scripted filters vs fields vs aggregations).  Would it be possible to allow such an approach with either indexed or file-based scripts? 

Would that simplify things at all?  It seems unfortunate that I need to resort to writing and compiling a java class in order to initialize some variables.
</comment><comment author="jpountz" created="2016-02-12T11:18:49Z" id="183283029">We discussed this issue in fixit friday and agreed with @clintongormley's analysis: we understand the need for such a feature but don't want to make the scripting API more complicated in order to support an initialization phase.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.engine.internal  exception  and many full gc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16079</link><project id="" key="" /><description>my ES cluster version is 1.4.2. 

print so many excption like this:

[2016-01-19 18:05:30,882][INFO ][index.engine.internal    ] [es_culrs [es_culrs.index][0] now throttling indexing: numMergesInFlight=6, maxNumMerges=5

and so many full gc. 

what's the problem? 
</description><key id="127410994">16079</key><summary>index.engine.internal  exception  and many full gc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tianzongqi</reporter><labels /><created>2016-01-19T10:09:24Z</created><updated>2016-01-20T12:30:35Z</updated><resolved>2016-01-20T12:30:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-20T12:30:35Z" id="173190669">Hi @tianzongqi 

Please ask questions like these on the forum instead: https://discuss.elastic.co/

The issues list is for bug reports and feature requests.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not apply minimum-should-match on a boolean query if the coords are disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16078</link><project id="" key="" /><description>Fixes #15858
</description><key id="127395588">16078</key><summary>Do not apply minimum-should-match on a boolean query if the coords are disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T08:56:45Z</created><updated>2016-01-19T14:07:35Z</updated><resolved>2016-01-19T14:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-19T09:54:37Z" id="172797262">@jimferenczi I suspect that this does not fully address #15858. For instance if you pass a min should match of 2 on a match query that only emits a asingle term then the min should match will simply be ignored?
</comment><comment author="jimczi" created="2016-01-19T10:03:28Z" id="172801592">@jpountz yes but not because of this commit, a match query that emits a single term is translated into a TermQuery and not a BooleanQuery. The match query builder ignores TermQuery (and any Query that is not a BooleanQuery after parsing) when the minimum should match is applied. 
</comment><comment author="jpountz" created="2016-01-19T10:10:55Z" id="172803683">OK, so let's merge this in and work on another PR to make the min should match apply in all cases?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the percolate api to not dynamically add fields to mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16077</link><project id="" key="" /><description>The percolate api shouldn't add field mappings for fields inside the document being percolated that are unmapped. In the end this is a read api and a read api shouldn't change the mapping or anything else.

Closes #15751
</description><key id="127392144">16077</key><summary>Change the percolate api to not dynamically add fields to mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T08:37:27Z</created><updated>2016-01-26T09:48:26Z</updated><resolved>2016-01-26T09:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-25T11:34:36Z" id="174479599">LGTM. This is a great change!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting by _id field is bouncing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16076</link><project id="" key="" /><description>I have a problem with consistency of ES sorting. When I order for example by date, ES gives different orderings from one query to another because of duplicate dates. To fix this we usually perform sorting by date + sorting by _id, which meant to be unique, but it isn't. Sorting by _id field only acts the same, it is bouncing. We have config of ES 1.4.1, 1 shard + 1 replica per index. When I disable replica it seems that sorting bouncing stops. 
</description><key id="127385942">16076</key><summary>Sorting by _id field is bouncing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2016-01-19T07:53:12Z</created><updated>2016-01-20T12:11:38Z</updated><resolved>2016-01-20T12:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-20T12:11:38Z" id="173185393">Correct - the `_id` field is neither indexed nor stored by default.  You will see that its entry in the `sort` array is `null`.  You can sort on `_uid` instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce scroll timeout in example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16075</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.1/java-search-scrolling.html) we have `60000ms` as the timeout, but [here](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-request-scroll.html#scroll-search-context) we state;

&gt; Its value (e.g. 1m, see the section called &#8220;Time unitsedit&#8221;) does not need to be long enough to process all data&#8201;&#8212;&#8201;it just needs to be long enough to process the previous batch of results

Which seems at opposites.

I'd suggest we update the java docs and reduce the figure in the example.

(Reported via twitter - https://twitter.com/xavierfacq/status/689116646124957696)
</description><key id="127384800">16075</key><summary>Reduce scroll timeout in example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-01-19T07:44:15Z</created><updated>2016-01-19T13:50:03Z</updated><resolved>2016-01-19T13:44:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-19T10:54:07Z" id="172816673">`60000ms` is `1m`, I think there was an initial misread in the tweet. No big deal, it happens. :)
</comment><comment author="xavierfacq" created="2016-01-19T11:00:27Z" id="172818038">The first TimeValue is well setted to 1 minute (60000ms) but the second, in the while-loop is setted to 600000ms = 10 minutes.  
</comment><comment author="jasontedor" created="2016-01-19T13:43:24Z" id="172857889">@xavierfacq Thanks for clarifying, sorry for the confusion. It happens. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread hangs while Bulk Insert Operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16074</link><project id="" key="" /><description>Hi, ES Team

we have an analytic application which uses underneath ES. our application collects various info. from diff sources and push into ES for further processing. we are using bulk insert operation to gain the optimal performance and throughput. but frequently our threads get hang or stuck while doing bulk insert operation.

can you please throw some lights on  this ??? or do we need to create some watcher that monitors the thread and if  bulk operations does not get finished within x time then watcher would kill or interrupt the thread which gets stuck or hang ??? 

it's a show stopper issue for us.
</description><key id="127380929">16074</key><summary>Thread hangs while Bulk Insert Operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adhamelia</reporter><labels><label>:Bulk</label></labels><created>2016-01-19T07:10:21Z</created><updated>2016-01-22T16:12:29Z</updated><resolved>2016-01-21T13:18:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adhamelia" created="2016-01-19T07:14:08Z" id="172761318">we are using 1.7.3 ES having Linux and Windows OS
</comment><comment author="danielmitterdorfer" created="2016-01-19T07:35:08Z" id="172764572">Hi Alpesh,

can you describe your situation in more detail? Some questions to ponder:
- Are you using the Java client or do you use another language client?
- How do you perform bulk operations? I see you are using the 1.7 line. Starting with 2.0 we provide a helper class, called `BulkProcessor` (see [docs](https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/java-docs-bulk-processor.html)) in the Java API. I'd be best to upgrade to the 2.x line of Elasticsearch anyway but if this is not an option you can base your implementation on our `BulkProcessor` class. Then there is no need to manage those low level details by yourself.

If all of this does not help you, can you please provide an isolated example project that demonstrates the problem?
</comment><comment author="adhamelia" created="2016-01-19T10:45:20Z" id="172814847">Thanks Daniel for your prompt response.

we are using java api [Node Client] with following bulk processor settings.

 .setBulkSize(new ByteSizeValue(TraceOrgServerConstants.ES_BULK_REQUEST_FLUSH_SIZE_IN_MB, ByteSizeUnit.MB))
                        .setFlushInterval(TimeValue.timeValueSeconds(TraceOrgServerConstants.ES_BULK_REQUEST_FLUSH_TIME_IN_SECOND))
                        .setConcurrentRequests(1)
                        .build();

ES_BULK_REQUEST_FLUSH_SIZE_IN_MB =100

ES_BULK_REQUEST_FLUSH_TIME_IN_SECOND =30
</comment><comment author="adhamelia" created="2016-01-19T10:49:45Z" id="172815736">Our  Testing System Conf

1 DATA NODE [8 GB RAM/6 CORE CPU] [VIRTUAL MACHINE] [CENT OS 6.6]

1 SHARD

0 REPLICATION 
</comment><comment author="adhamelia" created="2016-01-19T11:01:57Z" id="172818289">if we migrate from 1.x to 2.x then will require so much time and resources :(

there is an huge api changes in 2.x 
</comment><comment author="s1monw" created="2016-01-19T11:17:17Z" id="172820927">&gt; there is an huge api changes in 2.x

I do apologize for this but that's the right time to add breaking improvements. I highly recommend you to go to 2.x asap.
</comment><comment author="danielmitterdorfer" created="2016-01-19T12:19:35Z" id="172835324">How did you arrive at a bulk index size of 100MB? You can have a look at the section [Using and Sizing Bulk Requests](https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-performance.html#_using_and_sizing_bulk_requests) in the docs. A few thoughts:
- You should look where the bottleneck is: network, CPU usage, I/O
- You could benefit from a smaller bulk size
- You may also increase the number of concurrent requests (depending on the outcome of your investigation of the bottleneck)

For future reference, we have a [discussion forum](https://discuss.elastic.co/) for such questions.

And as Simon said, it also makes sense to upgrade to 2.x soon. There are a ton of improvements in there.
</comment><comment author="adhamelia" created="2016-01-19T13:45:12Z" id="172858210">Thanks guys !!!

ya, I know 100 MB is too large. in past,we tried on 10-15 MB bulk size but still it did not work.

for concurrency, if  we increase it,  ES behaves strange.

right now we are using  concurrent request = 1 and it works for  a while but if load gets increased then bulk index eats our all threads.

we are definitely thinking about 2.x but right now, we are looking for a cheaper alternative by tweaking some ES conf. 
</comment><comment author="danielmitterdorfer" created="2016-01-19T13:55:04Z" id="172860420">&gt; ya, I know 100 MB is too large. in past,we tried on 10-15 MB bulk size but still it did not work.

What happened, when you used a smaller bulk size?

&gt; for concurrency, if  we increase it,  ES behaves strange.

Can you describe what type of strangeness you experience then?

&gt; right now we are using  concurrent request = 1 and it works for  a while but if load gets increased then bulk index eats our all threads.

Have you looked at resource usage? There must be something causing blocked threads. I suggest you go through these guides:
- [Indexing Performance Tips](https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-performance.html)
- [Performance Considerations for Elasticsearch Indexing](https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing)
</comment><comment author="adhamelia" created="2016-01-21T13:17:27Z" id="173565421">thanks guys ... finally the issue has been fixed... :)

we stopped sharing bulk processor instance across threads. now,each thread has own copy of bulk processor 

5k docs/sec achieved !!!
</comment><comment author="danielmitterdorfer" created="2016-01-22T07:35:00Z" id="173834529">Hi, Alpesh,

good to hear that you've resolved your problem. Nevertheless, this sounds to me that you have (or had?) a problem elsewhere in your code. You should also get decent throughput if you have just one bulk processor instance so you might want to take a second look at your code. Did you lock on the bulk processor instance previously, i.e. did you have something like this in your code?

``` java
synchronized (bulkProcessor) {
  // ... do something here
}
```

This shouldn't be necessary and would explain why you had problems with throughput previously.
</comment><comment author="adhamelia" created="2016-01-22T10:34:24Z" id="173873460">no we did not do that.. we never block our akka actor ...

even i am surprised when came to know that threads were blocking on bulk processor method 

private synchronized void internalAdd(ActionRequest request, @Nullable Object payload) {
        bulkRequest.add(request, payload);
       executeIfNeeded();
   } 
</comment><comment author="danielmitterdorfer" created="2016-01-22T12:55:23Z" id="173913338">Well, it seems we have found your contention point. Yes, bulk processor is blocking, so your other actors will not be able to make progress in case the bulk request limit is hit (if you share the bulk processor instance). The usage model for bulk processor is just not a good fit for actors.

With your solution you have eliminated this contention point. Another idea you could try is to create a dedicated actor that owns the sole bulk processor instance and just send messages to this actor. To me this feels more natural than giving each actor its own bulk processor instance and I think it's also easier to test.
</comment><comment author="bleskes" created="2016-01-22T16:12:29Z" id="173964549">&gt; The usage model for bulk processor is just not a good fit for actors.

Just a note that the bulk processor still gives you nice retry properties and back pressure logic from ES. You can use async handling and set the semaphore limiting concurrent requests to something that's high enough to fit your need. Also, having one processor means that data will be sent faster to ES as messages from all actors count towards the quota to send a bulk
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Attachment mapper plugin: attempting to retrieve stored fields realtime produces incorrect results or exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16073</link><project id="" key="" /><description>Overview (ES 2.1.1):
- Index an attachment-mapped field
- Attempt to retrieve stored sub-fields real time, before refresh
- Observe that:
  - if string-type sub-fields (i.e. `content_type`) are requested, their content is the base-64 encoded attachment
  - if number-type sub-fields (i.e. `content_length`) an exception is thrown and the query fails
- If the refresh interval elapses, the same query succeeds and produces correct results.

---

Mapping

```
"data" : {
  "type" : "attachment",
  "fields" : {
    "content_type" : {
      "type" : "string",
      "store" : true
    },
    "content_length" : {
      "type" : "integer",
      "store" : true
    }
    // rest omitted 
  }
}
```

Using the JS client:

```
es.create({
  index: 'test',
  type: 'attachment',
  parent: 'someID',
  body: {
    data: ''e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0="
  }
}).then(
  ({ _id }) =&gt; es.get({
    index: 'test',
    type: 'attachment',
    id: _id,
    parent: 'someID',
    fields: ['data.content_type', 'data.content_length']
  })
)
```

Executing this query results in the following exception:

```
[2016-01-19 00:09:22,656][DEBUG][action.get               ] [MODOK] null: failed to execute [get [live][attachment][AVJYSZJK_w5v_dJkuVcb]: routing [AVJYSZIQ_w5v_dJkuVca]]
RemoteTransportException[[MODOK][127.0.0.1:9300][indices:data/read/get[s]]]; nested: NumberFormatException[For input string: "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0="];
Caused by: java.lang.NumberFormatException: For input string: "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0="
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:580)
        at java.lang.Integer.parseInt(Integer.java:615)
        at org.elasticsearch.index.mapper.core.IntegerFieldMapper$IntegerFieldType.value(IntegerFieldMapper.java:161)
        at org.elasticsearch.index.mapper.core.IntegerFieldMapper$IntegerFieldType.value(IntegerFieldMapper.java:124)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper$NumberFieldType.valueForSearch(NumberFieldMapper.java:150)
        at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:247)
        at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:86)
        at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:101)
        at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:44)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
```

Performing the same query but only requesting the `data.content_type` stored field yields 

```
'data.content_type': [ 'e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=' ] }
```

as the value of `content_type` in `fields` in the response.

I've read the stored fields documentation and my understanding is that they are not available to realtime APIs like Get. So, I was expecting that for queries requesting stored fields that were submitted before the elapsing of the refresh interval, the fields would simply not be available. (Incidentally, submitting `realtime:true` or `realtime:false` with the request had no effect and I am not sure what this option does.)
</description><key id="127369327">16073</key><summary>Attachment mapper plugin: attempting to retrieve stored fields realtime produces incorrect results or exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dminkovsky</reporter><labels><label>:Plugin Mapper Attachment</label><label>adoptme</label><label>bug</label></labels><created>2016-01-19T05:30:05Z</created><updated>2016-03-01T13:08:30Z</updated><resolved>2016-03-01T13:08:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T13:08:29Z" id="190716828">This issue is resolved by using the ingest attachment plugin instead. Closing as the mapper attachment plugin is deprecated
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove remaining xlints from plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16072</link><project id="" key="" /><description /><key id="127358973">16072</key><summary>Remove remaining xlints from plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T03:32:18Z</created><updated>2016-01-19T15:54:15Z</updated><resolved>2016-01-19T15:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-19T04:34:54Z" id="172734683">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Xlint from lang-groovy and discovery-azure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16071</link><project id="" key="" /><description>discovery-azure didn't actually need it.

This removes all non-default Xlints from modules.
</description><key id="127343015">16071</key><summary>Remove Xlint from lang-groovy and discovery-azure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T01:25:04Z</created><updated>2016-01-19T02:06:16Z</updated><resolved>2016-01-19T02:06:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-19T01:39:58Z" id="172701520">LGTM
</comment><comment author="nik9000" created="2016-01-19T02:06:16Z" id="172708968">Thanks @rjernst !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Make rest test framework accept http directly for the test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16070</link><project id="" key="" /><description>The rest test framework, because it used to be tightly integrated with
ESIntegTestCase, currently expects the addresses for the test cluster to
be passed using the transport protocol port. However, it only uses this
to then find the http address.

This change makes ESRestTestCase extend from ESTestCase instead of
ESIntegTestCase, and changes the sysprop used to tests.rest.cluster,
which now takes the http address.

closes #15459
</description><key id="127339403">16070</key><summary>Test: Make rest test framework accept http directly for the test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-19T00:44:30Z</created><updated>2016-01-19T01:38:24Z</updated><resolved>2016-01-19T01:38:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-19T00:53:34Z" id="172694626">+1. this is a nice step towards improving rest tests in the build.
</comment><comment author="nik9000" created="2016-01-19T01:04:30Z" id="172695988">++. I like minimizing the places that use ESIntegTest. The rest test stuff used to need it because they were spinning up the cluster. But master hasn't needed or supported that for forever. So yay! Less ESIntegTest!

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Attachment mapper plugin: configuring sub-fields per documentation results in "ClassCastException: java.lang.Integer cannot be cast to java.lang.String"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16069</link><project id="" key="" /><description>Re: Attachment mapper plugin

Migrating this issue from [elastic/elasticsearch-mapper-attachments](https://github.com/elastic/elasticsearch-mapper-attachments/issues/202):

Regarding sub-field mappings, [documentation says](https://www.elastic.co/guide/en/elasticsearch/plugins/master/mapper-attachments-usage.html):

&gt; The other fields map to their respective metadata names, but there is no need to specify the type (like string or date) since it is already known.

This is either incorrect or misleading because when I set

```
"content_length" : { "store" : true }
```

in my mapping, `content_length` was not indexed. Instead, I got (with errors ignored):

```
[2016-01-18 17:33:46,053][DEBUG][mapper.attachment        ] Ignoring MapperParsingException catch while parsing content_length: [failed to parse [data.content_length]]: [55]
```

or (with errors not ignored):

```
[2016-01-18 17:38:54,660][DEBUG][action.index             ] [Red Ronin] [live][3], node[4-sneDHnRr-n_I9iZpGmEA], [P], v[8], s[STARTED], a[id=HRjjSSzORZKvyrsV3DlDHQ]: Failed to execute [index {[live][attac
hment][1], source[{ "data": "IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg==" }]}]
MapperParsingException[failed to parse [data.content_length]]; nested: ClassCastException[java.lang.Integer cannot be cast to java.lang.String];
        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:339)
        at org.elasticsearch.index.mapper.attachment.AttachmentMapper.parse(AttachmentMapper.java:605)
```

This happened because in the resulting mapping, `type` of `content_length` is `string`.

```
"content_length" : {
    "type" : "string",
    "store" : true
}
```

It appears that if you specify any properties of a subfield and omit `type`, type is `string`. So in order to store `content_length` and have it work, it must be specified completely as 

```
"content_length" : {
    "type" : "integer",
    "store" : true
}
```

This appears to be the opposite of what the documentation states.
</description><key id="127331589">16069</key><summary>Attachment mapper plugin: configuring sub-fields per documentation results in "ClassCastException: java.lang.Integer cannot be cast to java.lang.String"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dminkovsky</reporter><labels><label>:Plugin Mapper Attachment</label><label>adoptme</label><label>docs</label></labels><created>2016-01-18T23:35:38Z</created><updated>2016-03-01T13:06:45Z</updated><resolved>2016-03-01T13:06:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-19T01:21:41Z" id="172697963">Thanks for raising it. Do you want to contribute a doc fix?
</comment><comment author="dminkovsky" created="2016-01-19T05:32:55Z" id="172743520">This looks like a wonderful opportunity to get acquainted with a simple aspect of the codebase and to contribute, but I simply have no time at all right now :(. Thank you, but I cannot. 
</comment><comment author="clintongormley" created="2016-03-01T13:06:45Z" id="190716455">The mapper attachment plugin is deprecated in favour of the ingest attachment plugin, so I'm going to close this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single IPv4 addresses in IP field term queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16068</link><project id="" key="" /><description>This commit modifies IpFieldMapper#termQuery to permit single IPv4
addresses for use in match and query_string queries.

Closes #16058 
</description><key id="127323786">16068</key><summary>Single IPv4 addresses in IP field term queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Query DSL</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T22:32:30Z</created><updated>2016-01-21T19:17:41Z</updated><resolved>2016-01-18T22:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-18T22:45:51Z" id="172674033">LGTM
</comment><comment author="jasontedor" created="2016-01-18T22:46:39Z" id="172674158">Thanks @jpountz for a very quick review!
</comment><comment author="spalger" created="2016-01-18T23:36:31Z" id="172682043">:dancer: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in doc values docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16067</link><project id="" key="" /><description /><key id="127316311">16067</key><summary>Fix typo in doc values docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">R4CHI7</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T21:39:23Z</created><updated>2016-01-19T11:15:35Z</updated><resolved>2016-01-19T11:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-18T21:43:21Z" id="172661990">Thank you for noticing this and taking the time to fix! I'll integrate this soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a minor typo.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16066</link><project id="" key="" /><description /><key id="127314536">16066</key><summary>Fixed a minor typo.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">R4CHI7</reporter><labels /><created>2016-01-18T21:28:09Z</created><updated>2016-01-18T21:28:47Z</updated><resolved>2016-01-18T21:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cleanup ContextAndHeaderTransportTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16065</link><project id="" key="" /><description>We have two similar tests with the same name, ContextAndHeaderTransportTests.
They shared lots of common code so I extracted much of it into
ActionRecordingPlugin, a plugin which records all action requests for later
inspection.

I also removed all the warnings from both tests. That made lang-mustache
compile cleanly without any custom -Xlint so I removed those. To remove
the warnings I had to add type parameters to ActionFilter which seemed
like a good idea anyway.
</description><key id="127312785">16065</key><summary>Cleanup ContextAndHeaderTransportTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T21:18:07Z</created><updated>2016-01-18T21:35:48Z</updated><resolved>2016-01-18T21:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-18T21:34:24Z" id="172660216">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there a way of analyzing the context on the Completion Suggester?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16064</link><project id="" key="" /><description>We know this is a great feature for the completion suggester, but as far as I know is too serious with the argument it takes. My question is if it has any way of analyzing the context so cases like the examples below cool match.

_Example 1:_

``` JSON
GET my_index/_suggest
{
  "Autocomplete": {
    "text": "T",
    "completion": {
      "field": "autocomplete_field",
      "size": 15,
      "context": {
        "color": "red"
      }
    }
  }
}
```

_Example 2:_

``` JSON
GET my_index/_suggest
{
  "Autocomplete": {
    "text": "T",
    "completion": {
      "field": "autocomplete_field",
      "size": 15,
      "context": {
        "color": "Red"
      }
    }
  }
}
```

Thanks
</description><key id="127309120">16064</key><summary>Is there a way of analyzing the context on the Completion Suggester?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abrahamduran</reporter><labels /><created>2016-01-18T20:55:32Z</created><updated>2016-01-20T16:26:47Z</updated><resolved>2016-01-19T13:15:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-19T13:15:06Z" id="172848862">Hi @AIsaac08 

No there isn't.  Contexts are intended as simple filters - the kind of thing you'd select from a prepopulated list on a website.
</comment><comment author="abrahamduran" created="2016-01-19T20:27:27Z" id="172976034">Ok, thanks. By the way, I'm facing issues when some elements in the `context` contains accents, of course I'm querying with the accent, but they fail to match however.
</comment><comment author="clintongormley" created="2016-01-20T14:45:00Z" id="173224862">Hi @AIsaac08 

I've just tried out a context like `r&#233;d` and confirm that it is not working in 2.1 or before.  We're not going to fix this as, in ES 2.2.0, we have [completely rewritten](https://www.elastic.co/guide/en/elasticsearch/reference/2.x/search-suggesters-completion.html) the completion suggester. I can confirm that contexts with accents work correctly there.
</comment><comment author="abrahamduran" created="2016-01-20T16:26:47Z" id="173260995">Thanks! When will ES 2.2 be available as a release? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix warnings in lang-plan-a</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16063</link><project id="" key="" /><description>Removes all Xlint skips in lang-plan-a. Warnings were just some extra
casts which were removed, some raw types which we just needed to &lt;?&gt;, and
a couple of unchecked casts in data that we know to be Map&lt;String, Object&gt;
because that structure is super-ultra-common in scripts.
</description><key id="127305604">16063</key><summary>Fix warnings in lang-plan-a</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Scripting</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T20:30:50Z</created><updated>2016-01-18T21:40:21Z</updated><resolved>2016-01-18T21:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-18T21:20:34Z" id="172656568">LGTM
</comment><comment author="nik9000" created="2016-01-18T21:40:21Z" id="172661402">Thanks for review @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add default separator test to dedot rest test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16062</link><project id="" key="" /><description /><key id="127274886">16062</key><summary>[Ingest] Add default separator test to dedot rest test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label><label>test</label></labels><created>2016-01-18T17:28:28Z</created><updated>2016-01-19T09:06:12Z</updated><resolved>2016-01-18T22:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-18T20:55:41Z" id="172651277">LGTM Maybe also add a unit test for dedot with default separator?
</comment><comment author="talevy" created="2016-01-18T22:48:54Z" id="172674519">@martijnvg there is one in the factory tests for when `separator` is not provided.
</comment><comment author="martijnvg" created="2016-01-19T09:06:12Z" id="172781810">@talevy Cool, I thought it was missing, but I was wrong.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Normalize unavailable load average</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16061</link><project id="" key="" /><description>This pull request cleans up the handling of load averages in various places.
1. The first cleanup is to normalize the value of unavailable load averages to `-1`; the `OperatingSystemMXBean#getSystemLoadAverage` method at best guarantees that the return value is negative when the load average is not available but the value of `-1` is utilized in various places as "not available".
2. The second cleanup is to prevent builds on FreeBSD from failing. On FreeBSD systems it is either the case that `linprocfs` is mounted at `/compat/linux/proc` and all load averages are available, or this is not the case and at best the one-minute load average is available.
3. The third cleanup is to tighten the test assertions by adding OS X to the list of systems where we can make definitive statements about the availability of load averages.
4. The fourth cleanup is to just set load averages to null on Windows because load average is never available there.
5. The final cleanup is to not output the `load_average` object if none of the load average values are meaningful.

Relates #12049, #14741, #15907, #15932, #15934 
</description><key id="127271629">16061</key><summary>Normalize unavailable load average</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T17:11:13Z</created><updated>2016-01-29T09:28:15Z</updated><resolved>2016-01-28T20:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-01-28T20:36:13Z" id="176402616">@jasontedor LGTM!
</comment><comment author="jasontedor" created="2016-01-28T20:38:47Z" id="176403569">Thank you @abeyad!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes Aggregation Builders in place of AggregatorFactory implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16060</link><project id="" key="" /><description /><key id="127265955">16060</key><summary>Removes Aggregation Builders in place of AggregatorFactory implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-01-18T16:47:45Z</created><updated>2016-01-26T15:59:28Z</updated><resolved>2016-01-26T15:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-20T14:49:44Z" id="173226115">I think AggregatorFactories.countAll vs AggregatorFactories.count is a bit confusing. I somehow wish it was easier to tell from the name what they include or not.

NestedIT.java, HDRPercentileRanksTests.java have some indentation issues.

Otherwise LGTM.
</comment><comment author="colings86" created="2016-01-21T14:11:39Z" id="173580131">@jpountz I pushed two new commits. The first fixes the indentation issues you spotted and the second renames the count() and countAll() methods to countAggegators() and countPipelineAggregators() as well as fixing some failing integration tests which I had missed before
</comment><comment author="jpountz" created="2016-01-26T15:10:34Z" id="175067010">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose the reason why a mapping merge is issued.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16059</link><project id="" key="" /><description>This would be useful in order to only perform some validations in the case of
a mapping update and in cases when a mapping is restored eg. after a restart,
such as discussed in #15989.

This replaces the current `applyDefault` parameter which can be derived from
the mapping merge reason: the default mapping should be applied only in case of
a mapping update, if the mapping does not exist yet and if this is not the
default mapping.
</description><key id="127265045">16059</key><summary>Expose the reason why a mapping merge is issued.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T16:43:00Z</created><updated>2016-01-19T08:27:36Z</updated><resolved>2016-01-19T08:27:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-18T19:19:43Z" id="172627316">LGTM
</comment><comment author="ywelsch" created="2016-01-19T07:43:55Z" id="172766250">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match query on IP requires CIDR notation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16058</link><project id="" key="" /><description>Consider the following Sense script:

``` sh
DELETE /test-index

POST /test-index
{
  "mappings": {
    "test-type": {
      "properties": {
        "ip": {
          "type": "ip"
        }
      }
    }
  }
}

GET test-index/_mapping/field/ip

POST /test-index/test-type/1
{
  "ip": "192.168.0.1"
}

POST test-index/test-type/_search?size=1

POST logstash-0/apache/_search
{
  "query": {
    "match": {
      "clientip": "192.168.0.1"
    }
  }
}
```

This executes just fine in 2.x but fails at the final step in master. Since https://github.com/elastic/elasticsearch/pull/14874 the `match` query now requires that IP addresses use valid CIDR notation (`"192.168.0.1"` must be `"192.168.0.1/32"`). The pr seems clear on the point that ES shouldn't be lenient when it comes to parsing CIDR notation, but should the match query on IP address fields really only accept CIDR notation? It seems like it should accept a single IPV4 address as well.
</description><key id="127253848">16058</key><summary>Match query on IP requires CIDR notation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">spalger</reporter><labels><label>bug</label></labels><created>2016-01-18T15:49:07Z</created><updated>2016-01-18T22:46:51Z</updated><resolved>2016-01-18T22:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-18T17:50:01Z" id="172604304">Agreed that it should.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard state action channel exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16057</link><project id="" key="" /><description>This pull request addresses an issue in the handling of transport
exceptions while handling master channel exceptions when failing a
shard. The underlying issue is a distinction between local transport
exceptions versus remote transport exceptions. This pull request
corrects the handling of these exceptions, and adds an integration test
that simulates the situation.

Relates #15748
</description><key id="127242101">16057</key><summary>Shard state action channel exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T14:50:50Z</created><updated>2016-01-20T12:21:12Z</updated><resolved>2016-01-19T09:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-18T15:47:52Z" id="172566712">The fix is good. I left some comments about simplifying and making the test more generic
</comment><comment author="bleskes" created="2016-01-19T09:32:09Z" id="172791077">LGTM. Thanks @jasontedor . I'm merging this one in to stabilize the build.
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>