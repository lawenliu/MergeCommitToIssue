<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Allow sorting on nested sub generated field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6151</link><project id="" key="" /><description>When you have a nested document and want to sort on its fields, it's perfectly doable on regular fields but not on "generated" sub fields.

Here is a SENSE recreation:

```
DELETE /tmp

PUT /tmp

PUT /tmp/doc/_mapping
{
  "properties": {
    "flat": {
      "type": "string",
      "index": "analyzed",
      "fields": {
        "sub": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    },
    "nested": {
      "type": "nested",
      "properties": {
        "foo": {
          "type": "string",
          "index": "analyzed",
          "fields": {
            "sub": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT /tmp/doc/1
{
  "flat":"bar",
  "nested":{
    "foo":"bar"
  }
}
```

When sorting on `flat.sub` sub field, everything is fine:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "flat.sub": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               "bar"
#            ]
```

When sorting on `nested` field, everything is fine:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "nested.foo": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               "bar"
#            ]
```

But when sorting on `nested.sub` field, sorting is incorrect:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "nested.foo.sub": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               null
#            ]
```

Closes #6150.
</description><key id="33400373">6151</key><summary>Allow sorting on nested sub generated field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Nested Docs</label><label>blocker</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T13:57:18Z</created><updated>2015-06-07T20:06:57Z</updated><resolved>2014-05-14T12:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-14T10:33:52Z" id="43065209">and now for the PR: LGTM

I think this fix should also be back ported to 1.1 and 1.0 branches.
</comment><comment author="dadoonet" created="2014-05-14T12:29:03Z" id="43073815">Thanks @martijnvg.

Fix pushed in master, 1.x (1.2), 1.1 and 1.0 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow sorting on nested sub generated field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6150</link><project id="" key="" /><description>When you have a nested document and want to sort on its fields, it's perfectly doable on regular fields but not on "generated" sub fields.

Here is a SENSE recreation:

```
DELETE /tmp

PUT /tmp

PUT /tmp/doc/_mapping
{
  "properties": {
    "flat": {
      "type": "string",
      "index": "analyzed",
      "fields": {
        "sub": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    },
    "nested": {
      "type": "nested",
      "properties": {
        "foo": {
          "type": "string",
          "index": "analyzed",
          "fields": {
            "sub": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT /tmp/doc/1
{
  "flat":"bar",
  "nested":{
    "foo":"bar"
  }
}
```

When sorting on `flat.sub` sub field, everything is fine:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "flat.sub": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               "bar"
#            ]
```

When sorting on `nested` field, everything is fine:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "nested.foo": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               "bar"
#            ]
```

But when sorting on `nested.sub` field, sorting is incorrect:

```
GET /tmp/doc/_search
{
  "sort": [
    {
      "nested.foo.sub": {
        "order": "desc"
      }
    }
  ]
}

# gives
# "sort": [
#               null
#            ]
```
</description><key id="33391673">6150</key><summary>Allow sorting on nested sub generated field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-05-13T12:04:47Z</created><updated>2014-05-14T12:22:21Z</updated><resolved>2014-05-14T12:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow 0 as a valid external version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6149</link><project id="" key="" /><description>Until now all version types have officially required the version to be a positive long number. Despite of this has being documented, ES versions &lt;=1.0 did not enforce it when using the `external` version type. As a result people have succesfully indexed documents with 0 as a version. In 1.1. we introduced validation checks on incoming version values and causing indexing request to fail if the version was set to 0. While this is strictly speaking OK, we effectively have a situation where data already indexed does not match the version invariant.

To be lenient and adhere to spirit of our data backward compatibility policy, we have decided to allow 0 as a valid external version type. This is somewhat complicated as 0 is also the internal value of `MATCH_ANY`, which indicates requests should succeed regardles off the current doc version. To keep things simple, this commit changes the internal value of `MATCH_ANY` to `-3` for all version types.

Since we're doing this in a minor release (and because versions are stored in the transaction log), the default `internal` version type still accepts 0 as a `MATCH_ANY` value. This is not a problem for other version types as `MATCH_ANY` doesn't make sense in that context.

Closes #5662
</description><key id="33389364">6149</key><summary>Allow 0 as a valid external version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels /><created>2014-05-13T11:26:18Z</created><updated>2014-06-15T11:42:23Z</updated><resolved>2014-05-18T07:20:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-14T11:02:45Z" id="43067334">I left a question about whether we should keep using VLong here but other than that this looks good to me.
</comment><comment author="bleskes" created="2014-05-14T14:31:16Z" id="43087713">@jpountz I pushed an update. Your comment made me think some more about the writeLong version and it opened a can of worms... 
</comment><comment author="jpountz" created="2014-05-15T09:09:23Z" id="43185597">LGTM
</comment><comment author="s1monw" created="2014-05-15T11:39:18Z" id="43199328">this goes also into master no?
</comment><comment author="bleskes" created="2014-05-15T11:40:28Z" id="43199401">@s1monw yes. forgot a label. Adding.
</comment><comment author="imotov" created="2014-05-15T19:45:55Z" id="43256104">LGTM, but I think it might be cleaner to have another PR the will go only into master and will stop accepting `0` as `MATCH_ANY` there. Thoughts?
</comment><comment author="bleskes" created="2014-05-16T21:20:08Z" id="43380807">Thanks all for reviewing. I just pushed it  master (9f10547f4b60e2e55318a955f4ace6496ef32cf3) and 1.x (dbcf2b1f1dc61d589e8f09844fbb5a46d81cec50). I'll make another PR in a second to remove the BW code from master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Read full message on free context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6148</link><project id="" key="" /><description>Since #5730 we write a boolean in the FreeContextResponse which should be deserialized

Closes #6147
</description><key id="33385476">6148</key><summary>Read full message on free context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T10:25:09Z</created><updated>2015-06-07T20:07:38Z</updated><resolved>2014-05-13T10:44:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-13T10:30:18Z" id="42939432">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lot's of warn logs related to FreeContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6147</link><project id="" key="" /><description>I see tons of these logs when I run some tests:

```
[2014-05-13 12:17:52,428][WARN ][org.elasticsearch.transport.netty] [node_1] Message not fully read (response) for [91] handler org.elasticsearch.transport.EmptyTransportResponseHandler@7bfccc86, error [false], resetting
[2014-05-13 12:17:52,432][WARN ][org.elasticsearch.transport.netty] [node_2] Message not fully read (response) for [50] handler org.elasticsearch.transport.EmptyTransportResponseHandler@7bfccc86, error [false], resetting
...
```

I think this is related to #5730 since we now actually send a non-empty response back on freeContext.
</description><key id="33385258">6147</key><summary>Lot's of warn logs related to FreeContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T10:22:02Z</created><updated>2014-05-13T10:44:54Z</updated><resolved>2014-05-13T10:44:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Benchmark list() should return empty result if no bench nodes are available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6146</link><project id="" key="" /><description>today we fail a list benchmark request which seems odd to me. Should we just return an empty list in that case?
</description><key id="33382924">6146</key><summary>Benchmark: Benchmark list() should return empty result if no bench nodes are available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Benchmark</label><label>enhancement</label></labels><created>2014-05-13T09:46:21Z</created><updated>2015-03-20T16:13:26Z</updated><resolved>2014-05-19T18:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Some minor cleanups related to bench API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6145</link><project id="" key="" /><description>I found that some of the Bench tests leave benchmarks behind which can have an impact on other tests. We should shutdown all the benchmarks once tests are done. This PR adds some infra that detects this.
</description><key id="33382423">6145</key><summary>Some minor cleanups related to bench API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-05-13T09:39:52Z</created><updated>2014-07-16T21:45:14Z</updated><resolved>2014-05-13T10:35:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-13T10:09:41Z" id="42937899">@jpountz I pushed a review commit
</comment><comment author="jpountz" created="2014-05-13T10:12:01Z" id="42938089">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE when initializing an accepted socket in NettyTransport.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6144</link><project id="" key="" /><description>NettyTransport's ChannelPipelineFactory uses the instance variable
serverOpenChannels in order to create sockets. However, this instance variable
is set to null when stoping the netty transport, so if the transport tries to
stop and to initialize a socket at the same time you might hit the following
NullPointerException:

[2014-05-13 07:33:47,616][WARN ][netty.channel.socket.nio.AbstractNioSelector] Failed to initialize an accepted socket.
java.lang.NullPointerException: handler
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.&lt;init&gt;(DefaultChannelPipeline.java:725)
    at org.jboss.netty.channel.DefaultChannelPipeline.init(DefaultChannelPipeline.java:667)
    at org.jboss.netty.channel.DefaultChannelPipeline.addLast(DefaultChannelPipeline.java:96)
    at org.elasticsearch.transport.netty.NettyTransport$2.getPipeline(NettyTransport.java:327)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.registerAcceptedChannel(NioServerBoss.java:134)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.process(NioServerBoss.java:104)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

This fix ensures that the ChannelPipelineFactory always uses the channels that
have been used upon start, even if a stop request is issued concurrently.
</description><key id="33380321">6144</key><summary>Fix NPE when initializing an accepted socket in NettyTransport.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Network</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T09:09:00Z</created><updated>2015-06-07T20:07:48Z</updated><resolved>2014-05-13T12:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-13T10:54:17Z" id="42941198">LGTM
</comment><comment author="s1monw" created="2014-05-18T10:04:27Z" id="43435940">should we get this into `1.1.2` as well?
</comment><comment author="jpountz" created="2014-05-18T19:40:11Z" id="43449290">Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `shard_min_doc_count` parameter to terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6143</link><project id="" key="" /><description>This was discussed in issue #6041 and #5998 . The parameter already exists for significant terms aggregation.

There are also two refactoring commits:

I tried to extract the parsing of common parameters for of significant terms and terms aggregation to get rid of some duplicate code.

Also, I refactored terms and significant terms aggregation a little: `requiredSize`, `shardSize`, `minDocCount` and `shardMinDocCount` are stored in a class called `BucketCountThresholds`. Before, every class using these parameters had their own member where these four are stored. This clutters the code. Because they are mostly needed together it might make sense to group them. I hope that this makes the code more readable.
</description><key id="33376222">6143</key><summary>Add `shard_min_doc_count` parameter to terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T08:01:54Z</created><updated>2015-06-08T14:37:05Z</updated><resolved>2014-05-14T12:11:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-05-13T08:22:11Z" id="42929053">@markharwood Maybe we should wait with the review here until you pushed your changes and I resolved all the conflicts? 
</comment><comment author="markharwood" created="2014-05-13T08:23:26Z" id="42929159">Would make sense I think. Just running the tests before pushing...
</comment><comment author="jpountz" created="2014-05-13T08:37:01Z" id="42930261">This looks good to me, I like the `BucketCountsThresholds` refactoring and the documentation is clear. I just left a minor comment about the handling of default values in the builders.
</comment><comment author="brwe" created="2014-05-13T08:56:01Z" id="42931861">Ok, I rebased on 1e560b0d92e8 , will work on @jpountz comments next 
</comment><comment author="brwe" created="2014-05-13T10:23:29Z" id="42938903">I added two commits to implement the comments. I was wondering: Would it make sense to add the logic in TermsParser to SignificantTermsParser as well, @markharwood ? I mean the

```
if (bucketCountThresholds.requiredSize == 0) {
           bucketCountThresholds.requiredSize = Integer.MAX_VALUE;
}
```

which is  [here](https://github.com/elasticsearch/elasticsearch/pull/6143/files#diff-85113edbf4bc80d49d1b7ea494e70518R53)
</comment><comment author="jpountz" created="2014-05-13T15:32:37Z" id="42970992">&gt; I was wondering: Would it make sense to add the logic in TermsParser to SignificantTermsParser as well

I think it would be nice to make it consistent.
</comment><comment author="brwe" created="2014-05-14T08:19:02Z" id="43054102">Updated with new commits.
</comment><comment author="jpountz" created="2014-05-14T08:32:51Z" id="43055268">LGTM
</comment><comment author="markharwood" created="2014-05-14T09:21:18Z" id="43059305">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use t-digest as a dependency.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6142</link><project id="" key="" /><description>Our improvements to t-digest have been pushed upstream and t-digest also got
some additional nice improvements around memory usage and speedups of quantile
estimation. So it makes sense to use it as a dependency now.

This also allows to remove the test dependency on Apache Mahout.
</description><key id="33375559">6142</key><summary>Use t-digest as a dependency.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T07:49:37Z</created><updated>2015-06-07T13:30:31Z</updated><resolved>2014-05-13T08:45:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-13T08:29:19Z" id="42929619">LGTM, +1
</comment><comment author="uboness" created="2014-05-13T08:29:27Z" id="42929632">+++++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard rebalance not obeying cluster_concurrent_rebalance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6141</link><project id="" key="" /><description>During full cluster restart relocating shards steadily increased and did not respond to setting cluster.routing.allocation.cluster_concurrent_rebalance. This behavior is pretty clearly cause by having disk.threshold_enabled set.
- 00:30: Relocating shards was around 86 causing us to get tight on disk space (a node ran out of space and we deleted a number of shards with rm before restarting it).
- 00:57: Set cluster_concurrent_rebalance to 10 (trying to limit relocating shards)
- 01:00 Set to 4 (after realizing it was already set to 6 in the config).
- 01:33 Set to 2 (in case something had changed and it was now per node not for the whole cluster)
- 02:00 Relocating shards hit its peak of ~140

![image](https://cloud.githubusercontent.com/assets/820871/2953053/8521b826-da4c-11e3-8b56-749eca93943a.png)

I'm guessing there is an interaction with disk threshold and/or shard allocation. We're seeing a lot of disk space taken up by old shards that haven't yet been cleaned up (note this is from 03:38):

![image](https://cloud.githubusercontent.com/assets/820871/2953176/4722b9f4-da50-11e3-8469-b661b9d9ff9f.png)

System config:
- 175 primary shards (plus 2 replicas per shard) = 525 shards total
- 36 data nodes (3 master nodes).
- Total disk space in cluster is 26TB. Total of all primaries is about 4.6TB (x3 = 13.8TB). Should be plenty of space.
- Some shards are pretty big. Max of about 45GB. Generally range from 15-40GB. 
- Just ran a full cluster restart to go from v0.90.12 to v1.1.1 (went pretty smoothly otherwise)
- Shard allocation awareness: DC = dfw, iad, sat; PARITY=0,1 (so 6 zones with 6 servers in each)

Relevant config settings:

```
cluster:
  routing:
    allocation:
      node_initial_primaries_recoveries: 8
      node_concurrent_recoveries: 15
      cluster_concurrent_rebalance: 6
      awareness.attributes: dc, parity
      disk.threshold_enabled: true

indices:
  recovery:
    max_bytes_per_sec: 100mb
    concurrent_streams: 5
```

```
{
  "cluster_name" : "es_glbl_cluster",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 39,
  "number_of_data_nodes" : 36,
  "active_primary_shards" : 175,
  "active_shards" : 469,
  "relocating_shards" : 135,
  "initializing_shards" : 56,
  "unassigned_shards" : 0
}
```
</description><key id="33366767">6141</key><summary>Shard rebalance not obeying cluster_concurrent_rebalance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">gibrown</reporter><labels><label>:Allocation</label></labels><created>2014-05-13T03:47:29Z</created><updated>2015-10-27T16:01:04Z</updated><resolved>2015-10-27T16:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gibrown" created="2014-05-13T03:55:32Z" id="42914744">There are some odd relocations occurring. Moving many shards off of a node while some similar sized ones are being moved onto the node:

```
global-2-0m-10m  15 p RELOCATING   4543831 21.6gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 192.0.82.77 es4.global.search.dfw.wordpress.com
global-2-50m-60m 11 p RELOCATING   5630907 31.5gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 66.155.10.57 es12.global.search.iad.wordpress.com
global-2-50m-60m 3  p RELOCATING   7115231   40gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 66.155.10.57 es12.global.search.iad.wordpress.com
global-2-50m-60m 8  r RELOCATING   6527571 37.2gb 192.0.81.196    es12.global.search.dfw.wordpress.com -&gt; 76.74.248.139 es2.global.search.sat.wordpress.com
global-2-40m-50m 4  r STARTED      6645253 27.9gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-40m-50m 10 r RELOCATING   5868871 28.1gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 192.0.82.110 es5.global.search.dfw.wordpress.com
global-2-20m-30m 19 r RELOCATING   3871159 19.1gb 66.155.10.56    es11.global.search.iad.wordpress.com -&gt; 76.74.248.139 es2.global.search.sat.wordpress.com
global-2-20m-30m 3  p RELOCATING   3299537   15gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 192.0.82.77 es4.global.search.dfw.wordpress.com
global-2-20m-30m 8  p RELOCATING   3174868 13.1gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 76.74.248.148 es8.global.search.sat.wordpress.com
global-2-10m-20m 24 r STARTED      3518521 15.4gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-10m-20m 11 r RELOCATING   3901551 19.2gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 66.155.9.135 es8.global.search.iad.wordpress.com
global-2-10m-20m 15 r RELOCATING   3533270 14.4gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 66.155.10.57 es12.global.search.iad.wordpress.com
global-2-10m-20m 12 r RELOCATING   3746370 19.5gb 76.74.248.159   es5.global.search.sat.wordpress.com -&gt; 76.74.248.139 es2.global.search.sat.wordpress.com
global-2-60m-70m 20 r STARTED      5211373 21.5gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-60m-70m 14 r STARTED      4354790 18.9gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-60m-70m 23 p RELOCATING   4194335 19.3gb 76.74.248.139   es2.global.search.sat.wordpress.com -&gt; 192.0.81.164 es1.global.search.dfw.wordpress.com
global-2-60m-70m 13 p STARTED      5044100 23.6gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-30m-40m 1  r STARTED      4409239 21.4gb 76.74.248.139   es2.global.search.sat.wordpress.com
global-2-30m-40m 8  p STARTED      4748723 26.8gb 76.74.248.139   es2.global.search.sat.wordpress.com
```

At this time this node has 121 of 703 GB free.
</comment><comment author="dakrone" created="2014-05-28T15:47:46Z" id="44425213">@gibrown just wanted to let you know I haven't missed this, it's on my list of issues to look into!
</comment><comment author="gibrown" created="2014-05-29T07:06:34Z" id="44500835">Great, thanks. We have been able to avoid the problem by not using disk threshold allocation for the moment.
</comment><comment author="dakrone" created="2015-02-24T00:36:36Z" id="75674470">@gibrown do you know if this is still an issue for you guys? I know we have fixed some issues in the disk threshold decider with regard to taking shard sizes into account, and I am wondering if this might also have been related. Have you seen this again recently?
</comment><comment author="gibrown" created="2015-03-06T20:46:28Z" id="77632520">We've had the disk threshold allocation disabled since this incident so we wouldn't run into it.

We're not currently on 1.4.x yet either, but are planning an upgrade in a month or two. I'll add trying the disk allocation to our list.
</comment><comment author="bsandvik" created="2015-03-19T19:50:11Z" id="83737276">When upgrading from 1.4.2 to 1.4.4 I also observed that the settings we have in elasticsearch.yml related to allocation are not being honored:

```
cluster.routing.allocation.cluster_concurrent_rebalance: 16
cluster.routing.allocation.node_concurrent_recoveries: 16
```

After a node restart when ES decided it was time to rebalance the cluster, I only saw two concurrent rebalancings going on. But when I manually set it to 16 using the API

```
curl -XPUT localhost:9200/_cluster/settings -d '{"transient":{"cluster.routing.allocation.cluster_concurrent_rebalance":16}}'
```

.... it works as expected.

We do not specify any disk thresholds.
</comment><comment author="clintongormley" created="2015-04-04T12:48:18Z" id="89566996">@bsandvik did you just change those settings in the config file on an already running cluster?  Or did you do a full cluster restart?
</comment><comment author="bsandvik" created="2015-04-05T15:53:28Z" id="89795919">@clintongormley I observed this during a rolling restart of the cluster.
</comment><comment author="clintongormley" created="2015-04-05T19:46:32Z" id="89839161">@bsandvik ok - so existing cluster settings where taken into account, rather than new settings from config files.  what you describe makes sense.
</comment><comment author="sawyercade" created="2015-08-23T17:03:23Z" id="133879164">Bringing some attention back to this bug, we're experiencing the same issue as well. We set cluster_concurrent_rebalance to 0 using the cluster settings api, and relocations are still being allowed to happen. What's really interesting is that the cluster_concurrent_rebalance setting is being checked both in the allocator (I'm using the datarank/tempest allocator plugin) and in the ConcurrentRebalanceAllocationDecider, and neither of them seem to be receiving the updated transient value, despite the following:

```
$ curl -XGET localhost:9200/_cluster/settings
{"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"enable":"all","cluster_concurrent_rebalance":"0"}}}}}

```

/_cat/shards shows 4 of our shards started relocating well after we updated the setting.
</comment><comment author="ywelsch" created="2015-10-27T16:01:04Z" id="151550975">This is fixed now by #14259
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up Java-API docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6140</link><project id="" key="" /><description>Fixed some awkward wording and subject/verb agreement while reading through the Java API documentation.
</description><key id="33363741">6140</key><summary>Clean up Java-API docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">taidan19</reporter><labels><label>docs</label></labels><created>2014-05-13T02:11:28Z</created><updated>2014-06-13T04:39:21Z</updated><resolved>2014-06-05T17:30:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:52:18Z" id="43061984">Hi @taidan19 

Thanks for all the fixes. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="taidan19" created="2014-05-14T11:51:57Z" id="43070965">You're very welcome, and thank you for accepting my changes. I've just
signed the agreement this morning.

-christian
On May 14, 2014 5:52 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; Hi @taidan19 https://github.com/taidan19
&gt; 
&gt; Thanks for all the fixes. Please could I ask you to sign the CLA so that
&gt; we can get your commit merged in?
&gt; http://www.elasticsearch.org/contributor-agreement
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/6140#issuecomment-43061984
&gt; .
</comment><comment author="javanna" created="2014-06-05T17:30:41Z" id="45249416">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to snapshot replicating primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6139</link><project id="" key="" /><description>This change adds a new cluster state that waits for the replication of a shard to finish before starting snapshotting process. Because this change adds a new snapshot state, it will not be possible to do rolling upgrade from 1.1 branch to 1.2 with snapshot is being executed in the cluster.

Closes #5531
</description><key id="33355910">6139</key><summary>Add ability to snapshot replicating primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T23:13:40Z</created><updated>2015-06-07T22:09:08Z</updated><resolved>2014-05-20T13:40:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-13T12:21:13Z" id="42947577">I want to do a deeper review but how do we handle bw compat here if a node is from a prev version and reads the "WAITING" state which is doesn't know about it just fails?
</comment><comment author="imotov" created="2014-05-13T15:33:34Z" id="42971131">Yes, the node with old version will fail on trying to process new cluster state if there is a running snapshot that was started while a primary shard participating in the snapshot was relocated. That's why, as I mentioned in the comment, we cannot have snapshots during 1.1-&gt;1.2 rolling upgrade. We will need to add this to release notes. Alternatively, I could also add a check for the presence of older version nodes and if they are present - fallback to the old behavior. That's not going to be bulletproof solution though, since an older version node can suddenly show up later and will not be able to join the cluster. What do you think?
</comment><comment author="s1monw" created="2014-05-13T16:09:34Z" id="42976088">+1 for best effort but It might make sense to have this as a separate commit so it's easier to review? I will do a review of this in the meanwhile.
</comment><comment author="imotov" created="2014-05-14T01:33:07Z" id="43033302">Added version check as a separate commit.
</comment><comment author="s1monw" created="2014-05-14T08:00:16Z" id="43052559">so bascially if we snapshot an index that has a primary that is initializing or relocating we just wait for the shards to start up. I think we should documentt this behavior and also test that we can abort such a waiting snapshot just in case it never get's to a state where we are able to snapshot. I wonder if we also should add tests where we have snapshots in multiple states ie. a chaos snapshotting while indexing and relocating. And just make sure we are not seeing any failures or broken snapshots?
</comment><comment author="imotov" created="2014-05-16T21:22:15Z" id="43380992">@s1monw I've implemented suggested changes. Thanks!
</comment><comment author="s1monw" created="2014-05-19T09:30:33Z" id="43482020">FYI - I removed the labels in favor of the issue
</comment><comment author="s1monw" created="2014-05-20T12:34:05Z" id="43619265">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix recovery percentage &gt; 100%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6138</link><project id="" key="" /><description>The recovery API was sometimes misreporting the recovered byte
percentages of index files. This was caused by summing up total file
lengths on each file chunk transfer. It should have been summing the
lengths of each transfer request.

Closes #6113
</description><key id="33355736">6138</key><summary>Fix recovery percentage &gt; 100%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Stats</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T23:10:34Z</created><updated>2015-06-07T20:09:11Z</updated><resolved>2014-05-13T16:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-13T11:01:35Z" id="42941783">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MetaData#concreteIndices to throw exception with a single index argument if allowNoIndices == false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6137</link><project id="" key="" /><description>Fixed MetaData#concreteIndices to throw exception with a single index argument in case allowNoIndices == false and ignoreUnavailable == true.

The current behaviour is inconsistent as 2 or more non existing indices would cause an exception to be thrown:

```
GET /a,b/_search?allow_no_indices=false&amp;ignore_unavailable=true

{
   "error": "IndexMissingException[[[a, b]] missing]",
   "status": 404
}
```

On the other hand 

```
GET /a/_search?allow_no_indices=false&amp;ignore_unavailable=true
```

returns 0 results and no exception regardless of the `allow_no_indices` value.

Took also the chance to improve the javadocs for `IndicesOptions` class.
</description><key id="33346725">6137</key><summary>MetaData#concreteIndices to throw exception with a single index argument if allowNoIndices == false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T21:03:44Z</created><updated>2015-06-07T20:07:58Z</updated><resolved>2014-05-12T21:28:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-12T21:24:39Z" id="42890340">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid query results with . (dot)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6136</link><project id="" key="" /><description>Request (correct results):

```
post fc751ed1-9118-4c25-bdbf-d36eb527493a/asset/_search
{
  "query": {
    "query_string": {
      "query": "ContentStatus:1"
    }
  },
  "_source":[
      "Id",
      "Title"
    ]
}
```

Response (correct results):

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 645,
      "max_score": 1,
      "hits": [
         {
            "_index": "fc751ed1-9118-4c25-bdbf-d36eb527493a",
            "_type": "asset",
            "_id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
            "_score": 1,
            "_source": {
               "Id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
               "Title": "English Riverbank / American Ranch"
            }
         },
...
```

Request (correct results):

```
post fc751ed1-9118-4c25-bdbf-d36eb527493a/asset/_search
{
  "query": {
    "query_string": {
      "query": "Title:."
    }
  },
  "_source":[
      "Id",
      "Title"
    ]
}
```

Response (correct results):

```
{
   "took": 0,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

Request (incorrect results):

```
post fc751ed1-9118-4c25-bdbf-d36eb527493a/asset/_search
{
  "query": {
    "query_string": {
      "query": "(Title:. AND ContentStatus:1)"
    }
  },
  "_source":[
      "Id",
      "Title"
    ]
}
```

Response (incorrect results):

```
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 645,
      "max_score": 1,
      "hits": [
         {
            "_index": "fc751ed1-9118-4c25-bdbf-d36eb527493a",
            "_type": "asset",
            "_id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
            "_score": 1,
            "_source": {
               "Id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
               "Title": "English Riverbank / American Ranch"
            }
         },
...
```

Request (incorrect results):

```
post fc751ed1-9118-4c25-bdbf-d36eb527493a/asset/_search
{
  "query": {
    "query_string": {
      "query": "(Title:\".\" AND ContentStatus:1)"
    }
  },
  "_source":[
      "Id",
      "Title"
    ]
}
```

Response (incorrect results):

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 645,
      "max_score": 1,
      "hits": [
         {
            "_index": "fc751ed1-9118-4c25-bdbf-d36eb527493a",
            "_type": "asset",
            "_id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
            "_score": 1,
            "_source": {
               "Id": "b70b3b6a-51b5-45af-8aa2-be5d3dc2d654",
               "Title": "English Riverbank / American Ranch"
            }
         },
...
```
</description><key id="33344304">6136</key><summary>Invalid query results with . (dot)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">VKhazinGladiator</reporter><labels /><created>2014-05-12T20:35:20Z</created><updated>2014-05-13T14:10:15Z</updated><resolved>2014-05-12T20:40:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-12T20:40:40Z" id="42885155">Hi @VKhazinGladiator 

This issues list is for bugs and feature requests. Please ask questions about how to use Elasticsearch in the forum instead.

thanks
</comment><comment author="VKhazinGladiator" created="2014-05-12T20:43:48Z" id="42885530">Could you please advise how it is not a bug?
</comment><comment author="clintongormley" created="2014-05-12T20:52:23Z" id="42886577">Because a `.` is not even indexed.  It is removed by the analyzer.  

Reading this will help you to understand why: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/getting-started.html
</comment><comment author="VKhazinGladiator" created="2014-05-12T21:24:00Z" id="42890260">I appreciate that . (dot) is not even indexed and I understand that searching for .(dot) never returns results as one of my correct results 'Title:.' above demonstrates.

What does not see to behave as I expect is a combined criteria 'Title:. AND ContentStatus:1' that seem to match all the results for 'ContentStatus:1' while ignoring the 'Title:.' portion of the query.
</comment><comment author="clintongormley" created="2014-05-12T21:40:10Z" id="42892021">Hi @VKhazinGladiator 

OK, I've reformatted your JSON to make it easier to read. The details of what you were trying to explain were not easily readable before.

This is just how the query string query works.  If analyzes the query string for each field and builds a query clause out of the terms produced by the analyzer.  You can see this in action with the validate-query API, eg:

```
PUT /test
GET /test/_validate/query?explain
{
  "query": {
    "query_string": {
      "query": "title:."
    }
  }
}
```

Returns an explanation of `""`, which is mapped to the `MatchNoDocsQuery`.

This one, however:

```
GET /test/_validate/query?explain
{
  "query": {
    "query_string": {
      "query": "title:. AND foo:bar"
    }
  }
}
```

returns an explanation of `"foo:bar"`. Note how it has removed the `title` clause because no terms were generated.  

Elasticsearch uses the Lucene query parser syntax for this query, and it is a complicated beast.  It is sensitive to syntax errors, surprising interpretation of boolean logic, and a host of other gotchas. 

Frankly, it is much better to write real queries with the query DSL. For instance, here is the explanation for the equivalent query in the DSL:

```
GET /test/_validate/query?explain
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "title": "."
          }
        },
        {
          "match": {
            "foo": "bar"
          }
        }
      ]
    }
  }
}
```

which returns the explanation: `"+MatchNoDocsQuery +foo:bar"`, which is exactly what you want.

I definitely wouldn't expose the `query_string` query directly to your users.
</comment><comment author="VKhazinGladiator" created="2014-05-12T21:47:04Z" id="42892701">Thank you for the detailed comments! Let me re-group on what's next.

P.S. I don't really see a cost-effective alternative exposing query functionality in open-ended Web Api where client app e.g. web app, mobile app, xbox app, and etc. compiles the query string based on its specific requirements. Asking client app to compose elastic search json query would be quite tedious and no less dangerous in my opinion.
</comment><comment author="clintongormley" created="2014-05-12T23:41:46Z" id="42902120">You shouldn't be exposing Elasticsearch directly to the outside anyway. Why not put a proxy in front of it that knows how to parse query strings from your apps into something you're happy to pass to Elasticsearch?  Then you do it once for all apps, and at the same time you prevent them from running queries that could take down your server.
</comment><comment author="VKhazinGladiator" created="2014-05-13T14:10:15Z" id="42959522">I do have a proxy in front of elastic search, the part I am trying to avoid is building a query language to 'parse query strings' as you have also suggested. So far I was happy with exposing lucene as the query language instead of building a custom implementation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for benchmark test timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6135</link><project id="" key="" /><description>Lower number of random requests generated for each test so as not to
timeout on heavy tests.

Addresses #6094
</description><key id="33337536">6135</key><summary>Fix for benchmark test timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-12T19:15:02Z</created><updated>2014-10-21T23:43:32Z</updated><resolved>2014-05-12T21:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-12T21:21:01Z" id="42889941">LGTM. This looks more reasonable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Track the number of times the CircuitBreaker has been tripped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6134</link><project id="" key="" /><description>Fixes #6130
</description><key id="33319731">6134</key><summary>Track the number of times the CircuitBreaker has been tripped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T15:46:07Z</created><updated>2015-06-08T14:37:14Z</updated><resolved>2014-05-13T19:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-13T12:04:31Z" id="42946276">LGTM
</comment><comment author="jpountz" created="2014-05-13T14:16:09Z" id="42960299">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support empty fields array in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6133</link><project id="" key="" /><description>could you also change src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java (line 66) to support empty array for "fields"?

```
   } else if (fieldName.equals("fields")) {
       Map&lt;String, Object&gt; fieldsNode = (Map&lt;String, Object&gt;) fieldNode;
```

similar to #5887 
</description><key id="33316297">6133</key><summary>Support empty fields array in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edast</reporter><labels /><created>2014-05-12T15:09:37Z</created><updated>2014-09-06T14:33:51Z</updated><resolved>2014-09-06T14:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cfontes" created="2014-07-23T09:00:20Z" id="49849048">I could try... Any details about it besides that statement?
</comment><comment author="edast" created="2014-07-23T09:08:17Z" id="49849739">not that i know of...
</comment><comment author="clintongormley" created="2014-07-23T10:40:59Z" id="49858184">@cfontes see #5887 - it had the same issue
</comment><comment author="edast" created="2014-07-23T10:52:01Z" id="49858972">yes - I've reported that and it was solved - it is referenced in description
</comment><comment author="clintongormley" created="2014-07-23T15:14:07Z" id="49887942">No, it's similar, not a duplicate.
</comment><comment author="cfontes" created="2014-07-23T15:15:10Z" id="49888090">Yeah I got that the second after the post was sent. So I deleted it. 

I will give it a shot, see if I can come up with something.
</comment><comment author="jpountz" created="2014-08-01T08:50:03Z" id="50862696">@cfontes That would be great! Have you been able to make some progress? If you have any questions, feel free to ask!
</comment><comment author="cfontes" created="2014-08-01T09:26:17Z" id="50865648">Still haven't had time to do it. 
But I plan to.
It's just going to take a while because I need to get more used to the code
</comment><comment author="cfontes" created="2014-08-05T02:11:43Z" id="51142060">In order to be on the safe side. I was thinking about creating a few unit tests.
I am a little confused about the best way to do that or if I am aiming at the right place to test this.

Which currently is

``` java
TypeParser.Mapper.TypeParser multiFieldConverterTypeParser
```

Any tips about what is the proper way on ElasticSearch to test this without having to use a real ParserContext (which has a massive constructor param list)? Mocking doens't look like an option as I couldn't find a framework for that on the deps.
</comment><comment author="clintongormley" created="2014-08-05T11:39:16Z" id="51185604">@cfontes ++ for tests. Have a look at the test that was added to the related issue: https://github.com/elasticsearch/elasticsearch/pull/6006/files#diff-9a44deb15da4051f9980d8d6889d1684R56
</comment><comment author="cfontes" created="2014-08-06T02:13:13Z" id="51286086">Ok, cheers!
</comment><comment author="cfontes" created="2014-08-14T08:04:42Z" id="52154622">Sorry for the delay.

@clintongormley thanks for the tip, it helped a lot.

So I ran the whole test suite 2x (took 40 minutes each!) 2 tests failed, but one is a **@BadApple** and the other one is marked as **@LuceneTestCase.AwaitsFix**

Tests with failures:
- org.elasticsearch.indices.leaks.IndicesLeaksTests.testIndexShardLifecycleLeak
- org.elasticsearch.discovery.DiscoveryWithNetworkFailuresTests.failWithMinimumMasterNodesConfigured

Is it ok to send the pull like that? I cannot run it again. It just takes to long and locks my machine.

Are they both @BadApples or the second is my fault?

Cheers!
</comment><comment author="clintongormley" created="2014-08-14T08:30:45Z" id="52156601">Hi @cfontes 

I doubt either of those are your fault :)

I'd send the PR (mentioning this ticket number) and somebody will review it.  thanks for the effort!
</comment><comment author="cfontes" created="2014-08-14T08:40:57Z" id="52157453">Nice,

Here it is #7271, if it needs any more work just ask.

Glad I could help, ES is great!
</comment><comment author="clintongormley" created="2014-09-06T14:33:51Z" id="54713891">Fixed by #7271
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>allow to choose garbage collectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6132</link><project id="" key="" /><description>We've been having some issues with java dying on GC, and wanted to try out switching to G1GC, and tried to set:
ES_JAVA_OPTS="-XX:+UseG1GC" in sysconfig
But this conflicts with the settings the startup script sets :(

As far as I can see, there's no variable I can set in sysconfig file - to make it switch to another collector? 
</description><key id="33315183">6132</key><summary>allow to choose garbage collectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KlavsKlavsen</reporter><labels /><created>2014-05-12T14:57:58Z</created><updated>2014-12-30T18:27:02Z</updated><resolved>2014-12-30T18:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-05-13T15:08:58Z" id="42967650">Just want to mention that when elasticsearch is dying on GC, it typically means that it runs out of heap space. Changing GC or its parameters is not going to be useful in this condition, you simply need to add more memory or revise your queries. Changing garbage collector to G1GC might actually make situation worse. We heard many reports of G1GC-related JVM crashes.
</comment><comment author="KlavsKlavsen" created="2014-05-13T19:21:54Z" id="43000078">I'll try to find some tool, that can pull out the JVM numbers, and put them into graphite.. I'm currently using graphite, but it's elasticsearch plugin is broken :(

Can I make elasticsearch enable jmx? then I could peak at heap space etc. through that?

All hints appreciated :)
</comment><comment author="imotov" created="2014-05-15T00:12:58Z" id="43155775">[Hint](http://www.elasticsearch.org/overview/marvel/download/) :)
</comment><comment author="clintongormley" created="2014-12-30T18:27:02Z" id="68382249">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Making rpm update init.d script even if its been locally changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6131</link><project id="" key="" /><description>since otherwise, you get a broken elasticsearch install, whenever the elasticsearch binary, that the init script runs (/usr/share/elasticsearch/bin/elasticsearch) changes something (like the startup needing -d - for it to be daemonized in 1.1.1 :) which makes the old init script not work.
</description><key id="33314974">6131</key><summary>Making rpm update init.d script even if its been locally changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">KlavsKlavsen</reporter><labels><label>:Packaging</label></labels><created>2014-05-12T14:55:43Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-12-15T11:57:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wilkinsbrian" created="2014-05-14T11:13:49Z" id="43068166">Does it create a .rpmsave or .rpmnew? You should be diff'ing these files with the existing init.d script to see if you need to pull in changes. 
</comment><comment author="KlavsKlavsen" created="2014-05-14T12:12:24Z" id="43072484">odd thing is it created neither.. just didn't overwrite it :(
</comment><comment author="wilkinsbrian" created="2014-05-14T12:43:43Z" id="43075112">What happens if you try a yum reinstall?

On Wed, May 14, 2014 at 8:12 AM, Klavs Klavsen notifications@github.comwrote:

&gt; odd thing is it created neither.. just didn't overwrite it :(
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/6131#issuecomment-43072484
&gt; .
</comment><comment author="KlavsKlavsen" created="2014-05-14T12:56:21Z" id="43076344">it doesn't update the init file and does not make any .rpmnew or .rpmsave.

You can reproduce easily:
yum install elasticsearch-1.0.2-1.noarch
vim /etc/init.d/elasticsearch  #alter content
yum update
vim /etc/init.d/elasticsearch #verify still your old version
ls -lort /etc/init.d/elasticsearch\* #verify it didn't put any .rpmnew etc. :) 
yum reinstall elasticsearch
vim /etc/init.d/elasticsearch  #still old version
</comment><comment author="wilkinsbrian" created="2014-05-14T13:15:23Z" id="43078204">Yep, after researching the rpm-maven-plugin documentation here (http://mojo.codehaus.org/rpm-maven-plugin/map-params.html#configuration), this is a bug. Unless it is intended by design.
</comment><comment author="KlavsKlavsen" created="2014-05-14T13:48:41Z" id="43082041">I don't get how maven gets rpm to not create an .rpmnew atleast, when it upgrades.. I'd like to capture the .spec file that maven generates to see how it actually looks.
The way it gets rpm to note nothing - and still not overwrite - should not be possible AFAIK (and also not according to one explanation of it, that I found: http://www-uxsup.csx.cam.ac.uk/~jw35/docs/rpm_config.html - although I do realize that rpm comes in many incarnations.. it's probably one of the most forked pieces of software :)

But since this does happen - IMHO it would be a better option to not mark init script as config.
with other packages - it's my understanding that init is normally not marked as config - although I do not have actual statistical data - and haven't downloaded a mirror of specs and grep'ed to see what the stats said :)
</comment><comment author="wilkinsbrian" created="2014-05-14T14:42:22Z" id="43089228">Actually, I think it should be set to &lt;configuration&gt;noreplace&lt;/configuration&gt;. Look here (https://docs.fedoraproject.org/en-US/Fedora_Draft_Documentation/0.1/html/RPM_Guide/ch09s05s03.html):

"Use this option to help protect local modifications. If you use %config(noreplace), the file **will not overwrite an existing file that has been modified**. If the file has not been modified on disk, the rpm command will overwrite the file. But, **if the file has been modified on disk**, the rpm command will **copy the new file with an extra file-name extension of .rpmnew.**"
</comment><comment author="KlavsKlavsen" created="2014-05-19T08:49:39Z" id="43478781">I'd actually prefer .rpmsave (ie. just config) - but that doesn't work when maven does it appearently.
That way - elasticsearch will start - when you have modified something in the init script or something in the script it actually uses to start elasticsearch.
</comment><comment author="wilkinsbrian" created="2014-05-19T10:57:30Z" id="43488730">It's not maven. It's RPM itself. See here - "If a config file has been edited on disk, but is not actually different from one RPM to another then the edited version will be silently left in place"

http://www-uxsup.csx.cam.ac.uk/~jw35/docs/rpm_config.html
</comment><comment author="KlavsKlavsen" created="2014-05-19T11:21:56Z" id="43490489">yes. but the init script was different from 1.0.2 to 1.1.1 AFAIK? (the -d option was added atleast)
</comment><comment author="wilkinsbrian" created="2014-05-19T12:57:32Z" id="43498127">Maybe a bug in the version of RPM you are running?
</comment><comment author="KlavsKlavsen" created="2014-05-19T13:22:52Z" id="43501652">red hat 6 - latest (6.5). ohh well. I'd generally prefer not having init scripts marked as config (you're not supposed to make local changes - so if such exist - IMHO it's fine they are moved to .rpmsave) - but that's up to you :)
</comment><comment author="wilkinsbrian" created="2014-05-19T13:25:18Z" id="43502014">If I had my druthers, it should do a replace with no .rpmnew or .rpmsave. 
</comment><comment author="KlavsKlavsen" created="2014-05-19T13:52:01Z" id="43506190">well according to the rpm docs linked to here - the pull request I sent does exactly that (ie. it's not marked as config).
</comment><comment author="s1monw" created="2014-12-05T09:51:20Z" id="65767938">sorry for the long delay - we are leaning towards accepting this PR as it is - @spinscale is still verifying the impact of it.
</comment><comment author="spinscale" created="2014-12-08T09:22:39Z" id="66042044">So I tested our current behaviour by installed the 1.3.6 rpm, changing the init script and then upgrading to 1.4.1 (to make sure to not run into bugs of our older packages). This gets printed out

```
# rpm -Uvh elasticsearch-1.4.1.noarch.rpm
Preparing...                ########################################### [100%]
   1:elasticsearch          warning: /etc/rc.d/init.d/elasticsearch saved as /etc/rc.d/init.d/elasticsearch.rpmsave
```

Reading the fedora package guidelines which are drafted here, we are behaving accordingly, see https://docs.fedoraproject.org/en-US/Fedora_Draft_Documentation/0.1/html/RPM_Guide/ch09s05s03.html

they also mark the init script as configuration like its done in our package. I also tried to update packages and had a `.rpmsave` file created, so to me everything looks as if it is working.

Can you reproduce this with two recent packages, because I could not.

If you check this table in http://www-uxsup.csx.cam.ac.uk/~jw35/docs/rpm_config.html I am leaning towards leaving as it is, because this means, that the new init script  gets installed with a new package. Which is important on our side to add fixes to it.
</comment><comment author="KlavsKlavsen" created="2014-12-15T11:57:49Z" id="66984413">odd.. reading http://www-uxsup.csx.cam.ac.uk/~jw35/docs/rpm_config.html - it says it should have updated the file.. why it didn't in that case is odd. If you can't reproduce it with later packages I probably cannot either.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Track the number of times the circuit breaker has tripped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6130</link><project id="" key="" /><description>We should add a stat for the number of times that the circuit breaker has been tripped.
</description><key id="33314177">6130</key><summary>Track the number of times the circuit breaker has tripped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2014-05-12T14:47:38Z</created><updated>2014-05-13T19:26:04Z</updated><resolved>2014-05-13T19:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Switch memory breaking in process requests to use CircuitBreaker infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6129</link><project id="" key="" /><description>Followup for the work from #6050, we should use the circuit breaker infrastructure for the memory limiting.
</description><key id="33313029">6129</key><summary>Switch memory breaking in process requests to use CircuitBreaker infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2014-05-12T14:35:25Z</created><updated>2014-07-11T09:05:21Z</updated><resolved>2014-07-11T09:05:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T09:05:21Z" id="48709206">Closing in favour of #6739 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deferred aggregations prevent combinatorial explosion </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6128</link><project id="" key="" /><description>New BucketCollector classes to aid the recording and subsequent playback of "collect" streams in aggs  to reduce combinatorial explosions where pruning of parent buckets should occur before calculating child aggs. 
Aggregator base class now wraps the subAgg BucketCollectors with any required caching of collect streams for sub aggregations that are indicated as being deferred. Aggregator subclasses should now override `shouldDefer` to indicate any aggs that are expensive to compute and in the BuildAggregation call should subsequently call `runDeferredCollections` with the subset of bucket ordinals that represent the pruned parent buckets of interest.
</description><key id="33307252">6128</key><summary>Deferred aggregations prevent combinatorial explosion </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T13:28:03Z</created><updated>2015-06-06T18:31:59Z</updated><resolved>2014-06-06T15:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-30T10:58:39Z" id="44638877">I left a few comments, but I like the new per-segment buffering of documents/buckets. I also think we should remove `FilteringSingleBucketCollector`, I don't like having a specialization for something that would be used so rarely: it is only used when `shard_size` is 1.
</comment><comment author="jpountz" created="2014-05-30T21:19:23Z" id="44700847">I quickly looked at the last changes and they look good! Before we pull that in, I think we should make sure users would get a meaningful error if they try to use scores while replaying doc IDs and to take another look at the formatting (some missing spaces around operators/brackets and lines with trailing spaces).
</comment><comment author="markharwood" created="2014-06-02T16:14:21Z" id="44857735">@jpountz Any preferences for an exception type when top_hits requests a Scorer from an aggregation tree with a deferred aggregation? I opted for ElasticsearchParseException - technically parsing is already done and it is triggered at execute time but it does suggest the user is at fault
</comment><comment author="jpountz" created="2014-06-02T16:18:30Z" id="44858251">Good question. I think `ElasticsearchParseException` is fine. On a side note, this doesn't only apply to the `top_hits` agg, eg. the `max` aggregation can be applied to a script that would compute the max of `log(_doc.score)`, which would call `Scorer.score` under the hood.
</comment><comment author="markharwood" created="2014-06-02T16:23:23Z" id="44858840">Sure - it calls AggregationContext.setScorer(unavailableScorer) when replaying - effectively ruining the fun for all concerned.
</comment><comment author="markharwood" created="2014-06-03T07:51:42Z" id="44929694">@jpountz before we wrap this up I had another use case to consider which may require some small changes. It's the idea of using a deferred agg as a quality filter e.g. so you could analyse the top 1000 docs only. This goes some way to addressing the fuzzy-set-non-fuzzy-counts problem I have been concerned about for a while (see https://gigaom.com/2013/03/17/can-we-please-stop-saying-unstructured-data/#comment-1321209 ). 
Even if we don't choose to expose it directly as a parent node in the query DSL I can see that there are certain situations where a child agg may choose to invoke this quality filtering. Top_hits is one example but I also see a mode of significant_terms where it re-analyzes the text of the top scoring docs only. This overcomes two issues with significant_terms: 1) It removes the need to load FieldData for free text fields and 2) it trims the long-tail of low-scoring results and can improve the quality of significant terms suggested.

With these potential new use cases we may need to revisit how the Aggregator base class decides to invoke deferred collection. 
</comment><comment author="markharwood" created="2014-06-04T06:10:03Z" id="45054667">Another "deferred" use case to consider? https://groups.google.com/forum/#!topic/elasticsearch/CtDhs0HDK2Q
</comment><comment author="jpountz" created="2014-06-04T07:49:44Z" id="45060810">I don't think it could help: building buckets based on counts is not practical as you would need the global counts to make a decision while a shard would only have shard-local knowledge.
</comment><comment author="jpountz" created="2014-06-04T08:44:54Z" id="45066144">LGTM, I just left comments about formatting. Can you fix these before pushing?
</comment><comment author="jpountz" created="2014-06-06T15:14:20Z" id="45347964">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Byte and BytesRef to the XContentBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6127</link><project id="" key="" /><description>Hi all,

currently the XContentBuilder will call .ToString()  on the object as fallback if it isn't known.
This will cause Byte instances to become a String instead of staying a number and BytesRef have some kind of hex representation which is wrong.

We could work around this if course by converting the values beforehand, but for performance reasons it would be nice to have the XContentBuilder handling the cases correctly.
</description><key id="33303384">6127</key><summary>Add support for Byte and BytesRef to the XContentBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T12:31:58Z</created><updated>2015-06-07T13:31:00Z</updated><resolved>2014-05-28T10:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-12T12:55:13Z" id="42827957">Although the change with `Byte` looks good to me, I don't think `XContentBuilder` should expect all `BytesRef`'s bytes to be UTF-8 bytes?
</comment><comment author="mfussenegger" created="2014-05-12T15:38:20Z" id="42848058">Afaik BytesRef are only used inside Lucene and there BytesRef are UTF-8 encoded. 
Our use case is that we get these values directly from lucene and we want to keep working with them without doing the utf16/utf8 encoding step again until really needed.

I don't think that there is a use case for BytesRef with bytes that aren't UTF-8 encoded. 
</comment><comment author="nik9000" created="2014-05-12T15:42:34Z" id="42848616">On Mon, May 12, 2014 at 11:38 AM, Mathias Fu&#223;enegger &lt;
notifications@github.com&gt; wrote:

&gt; Afaik BytesRef are only used inside Lucene and there BytesRef are UTF-8
&gt; encoded.
&gt; Our use case is that we get these values directly from lucene and we want
&gt; to keep working with them without doing the utf16/utf8 encoding step again
&gt; until really needed.
&gt; 
&gt; I don't think that there is a use case for BytesRef with bytes that aren't
&gt; UTF-8 encoded.
&gt; 
&gt; The BytesRefs used by the fuzzy terms enum are utf-32 I believe.  Its
&gt; reasonably rare that you'd encounter one but it happens.

Nik
</comment><comment author="jpountz" created="2014-05-13T09:50:47Z" id="42936365">&gt; Afaik BytesRef are only used inside Lucene and there BytesRef are UTF-8 encoded.

When indexing text with Lucene, terms will indeed be indexed as UTF-8, but you could as well index something that is not text, and in that case it could be anything. For example, binary fields can have doc values today, and the terms are binary. Similarly, the index terms for numeric fields are not UTF-8.

&gt; Our use case is that we get these values directly from lucene and we want to keep working with them without doing the utf16/utf8 encoding step again until really needed.

If you know that the `BytesRef` contains UTF-8 bytes, something that you can do would be to wrap the term bytes into an `org.elasticsearch.common.Text` instance. Elasticsearch will then know that it contains text and that the bytes it gets are encoded in UTF-8.
</comment><comment author="mfussenegger" created="2014-05-13T13:24:10Z" id="42953595">thanks for the feedback.

I've updated the PR to just add the Byte type handling to the XContentBuilder. If the PR is otherwise okay I'll squash the commits and reword the commit message.

And I'll try to see if the `org.elasticsearch.common.Text` class is usable for us.
</comment><comment author="mfussenegger" created="2014-05-13T13:31:17Z" id="42954431">One more thing. The XContentBuilder already has a method 

```
public XContentBuilder field(XContentBuilderString name, BytesRef value) throws IOException {
    field(name);
    generator.writeUTF8String(value.bytes, value.offset, value.length);
    return this;
}
```

Isn't that also wrong?
</comment><comment author="jpountz" created="2014-05-13T14:06:03Z" id="42959014">This might be dangerous indeed if not called with bytes which are encoded in UTF-8.
</comment><comment author="jpountz" created="2014-05-13T14:12:15Z" id="42959793">Maybe one way to make this less trappy and to address your needs would be to make the method name explicit about the fact that it expects the argument to be UTF-8 bytes. For example, the method could be called `utf8Field` (in the same way that there are some `rawField` methods)?
</comment><comment author="s1monw" created="2014-05-13T14:22:49Z" id="42961171">@nik9000 

&gt; The BytesRefs used by the fuzzy terms enum are utf-32 I believe.  Its reasonably rare that you'd encounter one but it happens.

They are UTF-8 but we convert them to UTF-32 but that is then and IntsRef I think that is what you are referring to. 

&gt; Maybe one way to make this less trappy and to address your needs would be to make the method name explicit about the fact that it expects the argument to be UTF-8 bytes. For example, the method could be called utf8Field (in the same way that there are some rawField methods)?

I agree this is dangerous and I think we should just name the method accordingly as @jpountz suggested. But I guess we tread a lot of stuff in Elasticsearch as UTF-8 so I wonder if there are more places?
</comment><comment author="jpountz" created="2014-05-13T14:28:38Z" id="42961966">I think it's not too bad in general. For example the method that @mfussenegger pointed out is only used by aggregations on string terms. There is at least one issue open related to the serialization of search responses: https://github.com/elasticsearch/elasticsearch/issues/6077: when sorting everything works fine even if you're not working with UTF-8 bytes until the serialization of the final response where it assumes that bytes are UTF-8 encoded.
</comment><comment author="mfussenegger" created="2014-05-16T08:41:03Z" id="43309379">What does this now mean for this PR? Should I also change the `field(XContentBuilderString name, BytesRef value)` method name to something else or can the Byte change already get merged?
</comment><comment author="jpountz" created="2014-05-20T07:33:58Z" id="43594660">@mfussenegger In my opinion, `field(XContentBuilderString name, BytesRef value)` should be marked with `@Deprecated` and replaced with `utf8Field(XContentBuilderString name, BytesRef utf8Bytes)` that does exactly the same thing but makes it explicit that the bytes of the input will be interpreted as utf-8 bytes. Regarding the change that you initially proposed, the `Byte` change is good and I think we could have `utf8Field(String name, BytesRef utf8Bytes)` to address your concern to not keep converting back and forth between utf-8 and utf-16?
</comment><comment author="mfussenegger" created="2014-05-20T19:15:50Z" id="43670457">I've added the utf8Field method and deprecated the other one. I also rebased it against current master.

Unfortunately my use case isn't quite solved with this as I use  field(Map..) with BytesRef inside the Map. But for the moment that is okay I can work around that for now.

Is there anything else stopping this PR from getting merged?
</comment><comment author="jpountz" created="2014-05-22T00:00:37Z" id="43832040">The commit looks great. Could you please just move the `Byte` check above with the other numeric wrapper classes (`Double`, `Short`, etc.) and use the same way to check the class of the value (`type == Byte.class`)?
</comment><comment author="mfussenegger" created="2014-05-25T18:55:38Z" id="44142520">Of course.. I've updated the PR as suggested.
</comment><comment author="jpountz" created="2014-05-28T10:21:58Z" id="44387887">@mfussenegger Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Faraday::Error::ConnectionFailed error in elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6126</link><project id="" key="" /><description>I have installed the elastic search and jdk 1.7 in windows.
When I start elastic search , Its gets started.

```
C:\elasticsearch-1.1.1\bin&gt;service.bat start
The service 'elasticsearch-service-x64' has been started

http://localhost:9200/ returns proper json in browser
{
  "status" : 200,
  "name" : "Ever",
  "version" : {
    "number" : "1.1.1",
    "build_hash" : "f1585f096d3f3985e73456debdc1a0745f512bbc",
    "build_timestamp" : "2014-04-16T14:27:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.7"
  },
  "tagline" : "You Know, for Search"
}
```

I have set my JAVA_HOME to 

```
'C:\Program Files\Java\jdk1.7.0_55\jre'
```

And my java version
java -version

```
java version "1.7.0_55"
Java(TM) SE Runtime Environment (build 1.7.0_55-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)
```

But when I am trying to do search or import the database to elastic search

```
FundraiserCamapignsIndex.import
```

I gives this error.

```
Faraday::Error::ConnectionFailed: getaddrinfo: No such host is known.
from c:/RailsInstaller/Ruby1.9.3/lib/ruby/1.9.1/net/http.rb:762:in `initialize'
```

Can you tell what I am missing here.
</description><key id="33300458">6126</key><summary>Faraday::Error::ConnectionFailed error in elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rocksjce123</reporter><labels /><created>2014-05-12T11:42:17Z</created><updated>2014-05-19T06:24:38Z</updated><resolved>2014-05-19T06:24:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-19T06:24:38Z" id="43469433">it looks as if your ruby script cannot resolve the host name you specified to connect. Please ask questions like this on the google group at https://groups.google.com/forum/#!forum/elasticsearch, we try to use this for elasticsearch issues and bugs only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>publish src for elasticsearch-xxx-tests.jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6125</link><project id="" key="" /><description>We are using the tests jar for elastic intergration tests - but we dont have access to the source in the ide because there is no sources jar published to the maven repo.
</description><key id="33299741">6125</key><summary>publish src for elasticsearch-xxx-tests.jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>enhancement</label></labels><created>2014-05-12T11:29:38Z</created><updated>2014-06-17T09:10:36Z</updated><resolved>2014-06-12T10:06:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickminutello" created="2014-06-09T15:29:44Z" id="45503885">Please? :)
</comment><comment author="nik9000" created="2014-06-10T00:00:06Z" id="45559385">coming.....
</comment><comment author="nickminutello" created="2014-06-17T09:10:36Z" id="46284075">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add top_hits aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6124</link><project id="" key="" /><description>The `top_hits` aggregator keeps track of the most relevant document being aggregated. This aggregator should be used as a sub aggregator of a bucket based aggregator, so that the top documents per bucket are computed.

Via this aggregator [grouping / field collapsing](https://github.com/elasticsearch/elasticsearch/issues/256) can be achieved and is very versatile. Someone can group by a field (using a terms aggregator as parent) or by time (using a histogram aggregator as parent), in any case the parent bucket aggregator determines how to group. How correct the top hits will depend on the parent aggregator. For example when using the `terms` aggregator and the `top_hits` aggregator some document may not end up in the response, because the `shard_size` on the `terms` aggregator is less then the field's cardinality.

The `top_hits` aggregator should have the following options:
- `size` - The amount of hits to collect.
- `sort` - Defines how the top hits should be sorted.
- and any other fetch phase options. Like source filtering and highlighting.

The prototype that is attached right now to this PR integrates nicely with the fetch phase, which allows all fetch like features to be implemented easily. Also it executes as if the `search_type` is set to `query_and_fetch`, this way aggregations don't need to execute extra round trips. 

Example usage of the current prototype:

``` json
GET /stack/question/_search?search_type=count
{
  "aggs": {
    "terms": {
      "terms": {
        "field": "tags",
        "size": 10
      },
      "aggs": {
        "top_tag_hits": {
          "top_hits": {
            "_source": {
              "include": [
                "title"
              ]
            },
            "sort": [
              {
                "last_activity_date": {
                  "order": "desc"
                }
              }
            ],
            "size" : 3
          }
        }
      }
    }
  }
}
```

In this example the hits are sorted by the field `last_activity_date` and only the top 3 hits are returned. Also per hit only the `title` field is included.

Response:

``` json
{
   "took": 151,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 175275,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "terms": {
         "buckets": [
            {
               "key": "windows-7",
               "doc_count": 25365,
               "top_tag_hits": {
                  "hits": {
                     "total": 25365,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602679",
                           "_score": 1,
                           "_source": {
                              "title": "Windows port opening"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602570",
                           "_score": 1,
                           "_source": {
                              "title": "Counter Strike Screen Resolution"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602249",
                           "_score": 1,
                           "_source": {
                              "title": "Hardware error while burning DVD+"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "linux",
               "doc_count": 18342,
               "top_tag_hits": {
                  "hits": {
                     "total": 18342,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602672",
                           "_score": 1,
                           "_source": {
                              "title": "Ubuntu RFID Screensaver lock-unlock"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "543625",
                           "_score": 1,
                           "_source": {
                              "title": "Linux Mint doesn't boot after creating a swap partition"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602434",
                           "_score": 1,
                           "_source": {
                              "title": "Is desktop pc support ssd and sata hard disk in one machine?"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows",
               "doc_count": 18119,
               "top_tag_hits": {
                  "hits": {
                     "total": 18119,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602678",
                           "_score": 1,
                           "_source": {
                              "title": "If I change my computers date / time, what could be affected?"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "472446",
                           "_score": 1,
                           "_source": {
                              "title": "Remove the Browser ballot app from Windows 8"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "321988",
                           "_score": 1,
                           "_source": {
                              "title": "How do I determine if my Windows is 32-bit or 64-bit using a command?"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "osx",
               "doc_count": 10971,
               "top_tag_hits": {
                  "hits": {
                     "total": 10971,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602680",
                           "_score": 1,
                           "_source": {
                              "title": "How to Install Google Chrome from the command line"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602482",
                           "_score": 1,
                           "_source": {
                              "title": "All Mac OS X apps crash as opened"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "517263",
                           "_score": 1,
                           "_source": {
                              "title": "Create a shortcut for application on Google Chrome for MacOSX"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "ubuntu",
               "doc_count": 8743,
               "top_tag_hits": {
                  "hits": {
                     "total": 8743,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602665",
                           "_score": 1,
                           "_source": {
                              "title": "Add more partitions to Grub2 - Ubuntu"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "190759",
                           "_score": 1,
                           "_source": {
                              "title": "Ubuntu 10.04 Keyboard and Mouse Freezing Problem"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "597297",
                           "_score": 1,
                           "_source": {
                              "title": "Curly braces with LCtrl+LShift+LAlt+&#232; - how?"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows-xp",
               "doc_count": 7517,
               "top_tag_hits": {
                  "hits": {
                     "total": 7517,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602679",
                           "_score": 1,
                           "_source": {
                              "title": "Windows port opening"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "510161",
                           "_score": 1,
                           "_source": {
                              "title": "Windows 8 Hyper-V fails to boot Windows XP ISO"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "180565",
                           "_score": 1,
                           "_source": {
                              "title": "Logitech Optical Mouse Frozen In Middle of Windows XP Pro Screen"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "networking",
               "doc_count": 6739,
               "top_tag_hits": {
                  "hits": {
                     "total": 6739,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602679",
                           "_score": 1,
                           "_source": {
                              "title": "Windows port opening"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602645",
                           "_score": 1,
                           "_source": {
                              "title": "are there any requirements for the sequence number on CP RST packets?"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602449",
                           "_score": 1,
                           "_source": {
                              "title": "Vmware Dev Server not allowing HTTP traffic"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "mac",
               "doc_count": 5590,
               "top_tag_hits": {
                  "hits": {
                     "total": 5590,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602482",
                           "_score": 1,
                           "_source": {
                              "title": "All Mac OS X apps crash as opened"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602553",
                           "_score": 1,
                           "_source": {
                              "title": "How can I load VLC instead of iTunes on my Mac when I press the player buttons on my Mac keyboard?"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602460",
                           "_score": 1,
                           "_source": {
                              "title": "startup mac mini using boot usb with MASTER BOOT RECORD scheme but failed"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "wireless-networking",
               "doc_count": 4409,
               "top_tag_hits": {
                  "hits": {
                     "total": 4409,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "265142",
                           "_score": 1,
                           "_source": {
                              "title": "Connect to Wi-Fi access point with specific MAC address"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "602586",
                           "_score": 1,
                           "_source": {
                              "title": "How to adjust Tx Power for Macbook Air mid-2012 Wi-Fi card"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "435290",
                           "_score": 1,
                           "_source": {
                              "title": "Use wifi and ethernet simultaneously?"
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows-8",
               "doc_count": 3601,
               "top_tag_hits": {
                  "hits": {
                     "total": 3601,
                     "max_score": 1,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "510161",
                           "_score": 1,
                           "_source": {
                              "title": "Windows 8 Hyper-V fails to boot Windows XP ISO"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "591388",
                           "_score": 1,
                           "_source": {
                              "title": "Android USB Driver on Windows 8"
                           }
                        },
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "553339",
                           "_score": 1,
                           "_source": {
                              "title": "Can I have 2 PDF documents open in Windows 8?"
                           }
                        }
                     ]
                  }
               }
            }
         ]
      }
   }
}
```
</description><key id="33297086">6124</key><summary>Add top_hits aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T10:44:11Z</created><updated>2017-03-31T10:06:19Z</updated><resolved>2014-05-23T14:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-13T10:28:48Z" id="42939308">This looks good to me. Let's add some documentation and tests and I think that will be it.
</comment><comment author="yao23" created="2014-05-13T16:41:17Z" id="42980004">It's so great to have this aggregation feature with top_hits, but how to use it? Wait for elasticsearch:master accept the merge request and update? Or clone your branch and compile, import in Maven? Thanks in advance for your future response. 
</comment><comment author="yao23" created="2014-05-13T21:13:37Z" id="43013779">Beside using JSON api to parse, do you have any Java or Scala APIs to retrieve or iterate bucket aggregation results, for example, in "key": "osx", how to get "title": "How to Install Google Chrome from the command line", "title": "All Mac OS X apps crash as opened" and "title": "Create a shortcut for application on Google Chrome for MacOSX"? Thanks!
</comment><comment author="kimchy" created="2014-05-14T00:05:23Z" id="43028315">@yao23 the feature, when it gets in, will be on the 1.3 release (we still have a 1.2 release that will happen hopefully soonish). I would not build this now, wait till it gets into master + 1.x branch, and then if you are eager to try it out, you can build the 1.x branch release once its in.

Regarding the API, there is a full Java client API as part of Elasticsearch, how to access aggregations using it is best asked on the mailing list.
</comment><comment author="yao23" created="2014-05-14T03:48:37Z" id="43039543">@kimchy Appreciate for your immediate response, I will try to build it and use Java APIs to access aggregations buckets content and post results here after experiment. Reference for other guys, link about Java APIs: http://stackoverflow.com/questions/21018493/how-to-access-aggregations-result-with-elasticsearch-java-api-in-searchresponse
</comment><comment author="martijnvg" created="2014-05-22T20:43:00Z" id="43941486">@jpountz I added tests and documentation.
</comment><comment author="jpountz" created="2014-05-22T22:15:29Z" id="43950808">@martijnvg this looks great. I left some minor comments about the documentation but other than that I'm good with pushing this change!
</comment><comment author="martijnvg" created="2014-05-23T12:07:45Z" id="44001452">@s1monw @jpountz I updated the PR: added more tests and updated the docs and disallow sub aggs in the top_hits agg.
</comment><comment author="s1monw" created="2014-05-23T12:09:33Z" id="44001580">thanks @martijnvg  LGTM 
</comment><comment author="jpountz" created="2014-05-23T12:40:02Z" id="44003821">LGTM
</comment><comment author="vvaradhan" created="2014-06-26T18:24:52Z" id="47261086">Is there a master-snapshot version available through maven?  I can start on my development till 1.3.0 gets officially released.

Also, what would be a likely release date of 1.3.0?
</comment><comment author="clintongormley" created="2014-07-01T13:35:57Z" id="47656307">&gt; Is there a master-snapshot version available through maven? I can start on my development till 1.3.0 gets officially released.

You can always compile from source. See https://github.com/elasticsearch/elasticsearch/blob/master/README.textile

&gt; Also, what would be a likely release date of 1.3.0?

Shortly before 1.4.0 ;)  

It'll be released when it is ready.
</comment><comment author="murugan-sundararaj" created="2014-07-24T06:38:19Z" id="49972524">Thanks for the top_hits feature in 1.3
Can I use top_hits to get ALL the documents present under a terms bucket. I understand the top_hits "size" accepts only numbers greater than zero. But how to get all the documents?. Thanks
</comment><comment author="jpountz" created="2014-07-24T06:41:15Z" id="49972706">You should not try to get all documents, this would blow up CPU and memory on your cluster.
</comment><comment author="murugan-sundararaj" created="2014-07-24T07:20:40Z" id="49975217">My use case is like this. I store product data

``` javascript
{
"_id": "product_id",
 "group": "A",
 "page_views": 1000,
 "field_x": "123abc",
 "field_y": "1010zzz"
}
```

I want to do terms bucket on "group" and get ALL the products, present under each bucket, in the descending order of their "page_views". I would use this result for further calculation. 

###### Query:

``` javascript
GET /my_idx/my_type/_search
{
    "size": 0, 
    "aggs": {
     "product_group": {
      "terms": {
        "field": "group",
        "size": 0
      },
      "aggs": {
        "top_products": {
          "top_hits": {
            "sort": [
              {
                "page_views": {
                  "order": "desc"
                }
              }
            ],
            "_source": {
              "include": [
                "page_views", "field_x", "field_y"
              ]
            },
            "size": 1000000 //this size is not known
          }
        }
      }
    }
  }
}
```

Please let me know if there is an alternate way to accomplish this.
</comment><comment author="jpountz" created="2014-07-24T07:54:12Z" id="49977691">The only reasonable way to do it would be to first start a request to compute the top groups and then one request per group (with a filter) using scroll for pagination.
</comment><comment author="murugan-sundararaj" created="2014-07-24T08:17:21Z" id="49979543">@jpountz thank you. Will try your approach.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add num_of_shards statistic to percolate context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6123</link><project id="" key="" /><description>PR for #6037
</description><key id="33296880">6123</key><summary>Add num_of_shards statistic to percolate context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-12T10:40:32Z</created><updated>2015-06-07T20:10:14Z</updated><resolved>2014-05-22T08:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-20T18:41:50Z" id="43666489">@jpountz Updated the PR to pass shardIt.size() instead of shardIt.
</comment><comment author="jpountz" created="2014-05-21T10:29:59Z" id="43738511">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update object-type.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6122</link><project id="" key="" /><description>Fix small typo.
</description><key id="33261994">6122</key><summary>Update object-type.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">gseng</reporter><labels><label>docs</label></labels><created>2014-05-11T13:41:42Z</created><updated>2014-07-16T21:45:22Z</updated><resolved>2014-06-05T17:35:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:52:49Z" id="43062027">Hi @gseng 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="gseng" created="2014-05-15T15:25:59Z" id="43224143">Done!
</comment><comment author="javanna" created="2014-06-05T17:35:11Z" id="45249919">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix wrong javadoc in IdentityHashSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6121</link><project id="" key="" /><description /><key id="33258496">6121</key><summary>Fix wrong javadoc in IdentityHashSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2014-05-11T09:42:39Z</created><updated>2014-09-08T09:27:17Z</updated><resolved>2014-05-13T09:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-13T09:58:52Z" id="42937045">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove SerialMergeScheduler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6120</link><project id="" key="" /><description>In master we can just remove it, and for 1.2 I think we should "translate" it into the equivalent ConcurrentMergeScheduler (max_thread_count=1, max_merge_count=1).

This merge scheduler is not practical for real usage: it only allows one merge to run at a time, and it's scync'd so it blocks any other incoming threads.

It also doesn't work with the upcoming index throttling (https://github.com/elasticsearch/elasticsearch/tree/enhancement/throttle_engine ), and while we could "solve" this by just documenting that apps shouldn't use SMS, I'd prefer to remove/remap it instead.
</description><key id="33257925">6120</key><summary>Remove SerialMergeScheduler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-11T08:57:22Z</created><updated>2015-06-08T14:37:22Z</updated><resolved>2014-05-12T18:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-11T10:40:52Z" id="42767391">++
</comment><comment author="mikemccand" created="2014-05-11T13:24:38Z" id="42770465">OK I pushed a fix for 1.x to this branch: https://github.com/elasticsearch/elasticsearch/tree/fix/6120

I would appreciate it if someone could review!  This code is all new to me.

I just changed SMSProvider to secretly return its own CMS instance and set max_merge_threads and max_merge_count to 1.  And I removed TrackingCMS, and "fixed" the docs to not mention SMS anymore.  These changes are for 1.x ... for master I'll remove SMS entirely.
</comment><comment author="jpountz" created="2014-05-12T11:47:39Z" id="42822371">The change looks good to me, I just left minor comments. I think it's nice to remap the merge scheduler this way, the good news being that since merge schedulers are randomized in the integration tests, this compatibility layer should get pretty good test coverage.
</comment><comment author="mikemccand" created="2014-05-12T13:55:46Z" id="42834511">Thanks Adrien, I committed a fix for your feedback.  I think it's ready!  I'll commit soon, and then merge to master by removing SMS entirely.
</comment><comment author="jpountz" created="2014-05-12T14:15:47Z" id="42836899">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added `filters` aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6119</link><project id="" key="" /><description>A multi-bucket aggregation where multiple filters can be defined (each filter defines a bucket). The buckets will collect all the documents that match their associated filter.

This aggregation can be very useful when one wants to compare analytics between different criterias. It can also be accomplished using multiple definitions of the single filter aggregation, but here, the user will only need to define the sub-aggregations only once.

Closes #6118
</description><key id="33256124">6119</key><summary>Added `filters` aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">uboness</reporter><labels /><created>2014-05-11T06:21:27Z</created><updated>2014-08-01T15:05:04Z</updated><resolved>2014-08-01T15:05:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-11T06:22:05Z" id="42763261">@rashidkpc @jpountz fyi
</comment><comment author="jpountz" created="2014-05-12T09:06:43Z" id="42810187">I like the way that this aggregation manages buckets (like the range agg). I left a couple of questions inlined.
</comment><comment author="rashidkpc" created="2014-05-12T19:50:34Z" id="42879313">Love it, exactly what we needed. 
</comment><comment author="s1monw" created="2014-06-18T19:03:13Z" id="46479075">@uboness what's the status here? I remove the review tag for now...
</comment><comment author="uboness" created="2014-06-18T19:08:38Z" id="46479758">I think it's pretty much ready... there's only a debate about the json structur... maybe @clintongormley can have a look
</comment><comment author="s1monw" created="2014-06-18T19:11:41Z" id="46480106">+1 thanks @uboness 
</comment><comment author="clintongormley" created="2014-06-19T13:51:44Z" id="46562440">@uboness I think it should be:

```
{
     "aggs" : {
         "messages" : {
             "filters" : {
                 "errors" : { "query" : { "match" : { "body" : "error" } } },
                 "warnings" : { "query" : { "match" : { "body" : "error" } } }
             },
             "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
         }
     }
 }
```

This fits nicely with the `filter` agg.  It doesn't make sense to allow `keyed` because there is no intrinsic order to the keys of a hash in JSON.  If you want the equivalent of `keyed: false` you could potentially support this format too:

```
{
     "aggs" : {
         "messages" : {
             "filters" : [
                 { "query" : { "match" : { "body" : "error" } } },
                 { "query" : { "match" : { "body" : "error" } } }
             ],
             "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
         }
     }
 }
```

... but I think that's just providing options for the sake of it.  
</comment><comment author="uboness" created="2014-06-19T13:57:58Z" id="46563193">@clintongormley the idea behind the "keyed" version was to be future proof... it's consistent with other aggs, in the future if we'll need to add meta data to the application (which we actually working on doing in another PR https://github.com/elasticsearch/elasticsearch/pull/6465) it'll be mixed with the aggs data... and that's no good
</comment><comment author="clintongormley" created="2014-06-19T15:22:47Z" id="46574709">@uboness PR #6465 will work happily with my example structure. The `meta` element isn't part of the agg body:

```
{
     "aggs" : {
         "messages" : {
             "filters" : {
                 "errors" : { "query" : { "match" : { "body" : "error" } } },
                 "warnings" : { "query" : { "match" : { "body" : "error" } } }
             },
             "meta": { "foo": "bar" },
             "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
         }
     }
 }
```

The `filter` agg doesn't have space for any extra params in the body either:

```
{
     "aggs" : {
         "messages" : {
             "filter" : {"query" : { "match" : { "body" : "error" } } }
             "meta": { "foo": "bar" }
         }
     }
 }
```

So the only distinction between these `filter` and `filters` is the existence of a key, which predicates returning the result as `{keyed: true}` (as `{keyed: false}` would be meaningless here.
</comment><comment author="uboness" created="2014-06-19T16:01:38Z" id="46580221">@clintongormley actually, you're right... my bad.. will work to see how it works out, thx
</comment><comment author="colings86" created="2014-08-01T15:05:04Z" id="50894879">Closing as PR 6974 supersedes this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Add `filters` aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6118</link><project id="" key="" /><description>Add a `filters` (note the `s`) aggregation that takes a list of filters and uses them to construct multiple buckets.

Technically the user could create multiple `filter` aggregations, however this gets messy if they have several sub aggregations as they would need to copy the contents into each of the top level aggregations.
</description><key id="33254593">6118</key><summary>Aggregations: Add `filters` aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">uboness</reporter><labels><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-11T03:57:27Z</created><updated>2014-08-04T09:50:44Z</updated><resolved>2014-08-01T15:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T13:57:14Z" id="48473024">@uboness I pushed this out to 1.4
</comment><comment author="uboness" created="2014-07-09T23:49:08Z" id="48549828">+1
</comment><comment author="rashidkpc" created="2014-07-17T15:52:32Z" id="49325880">Out of curiosity, what is blocking this? Just review?
</comment><comment author="uboness" created="2014-07-17T16:07:53Z" id="49327989">@rashidkpc already talked with @colings86 about taking over and getting it in
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Generalising the suggest API to suggest zero or multiple terms for a given input term</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6117</link><project id="" key="" /><description>Hi, 

I'm using the Phrase Suggest API at the moment to do spelling correction on user's searches, so we do another search for the suggested phrase if their original search yields no results (and we tell them what we searched for instead).

This works great, but there is a common class of typos I'd also like to correct like this which is the user forgetting a space.

Is there any scope for generalising the suggest API to suggest zero or multiple terms for a given input term?

For example, given "omega3", the suggestion is "omega 3".

Thanks
</description><key id="33243780">6117</key><summary>Generalising the suggest API to suggest zero or multiple terms for a given input term</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaunhall</reporter><labels /><created>2014-05-10T17:43:24Z</created><updated>2014-05-11T11:54:47Z</updated><resolved>2014-05-11T11:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-11T10:49:38Z" id="42767529">Are you using shingles on your suggestion field? For instance:

```
DELETE /test

PUT /test
{
  "settings": {
    "number_of_shards": 1,
    "analysis": {
      "filter": {
        "space_collapser": {
          "type": "shingle",
          "min_shingle_size": 2,
          "max_shingle_size": 3
        }
      },
      "analyzer": {
        "suggest": {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "space_collapser"
          ]
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "text": {
          "type": "string",
          "fields": {
            "shingles": {
              "type": "string",
              "analyzer": "suggest"
            }
          }
        }
      }
    }
  }
}

PUT /test/test/1
{
  "text": "Omega 3 is the bomb"
}

PUT /test/test/2
{
  "text": "Omega4 is the bomb"
}

GET /test/_suggest
{
  "text": "omega3",
  "test_suggest": {
    "phrase": {
      "field": "text.shingles",
      "max_errors": 3,
      "highlight": {
        "pre_tag": "&lt;em&gt;",
        "post_tag": "&lt;/em&gt;"
      }
    }
  }
}
```

Outputs:

```
{
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "test_suggest": [
      {
         "text": "omega3",
         "offset": 0,
         "length": 6,
         "options": [
            {
               "text": "omega 3",
               "highlighted": "&lt;em&gt;omega 3&lt;/em&gt;",
               "score": 0.33987367
            },
            {
               "text": "omega4",
               "highlighted": "&lt;em&gt;omega4&lt;/em&gt;",
               "score": 0.33987367
            },
            {
               "text": "omega",
               "highlighted": "&lt;em&gt;omega&lt;/em&gt;",
               "score": 0.33390126
            }
         ]
      }
   ]
}
```
</comment><comment author="shaunhall" created="2014-05-11T11:25:58Z" id="42768144">Brilliant, thanks Clinton, exactly what I wanted!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate query ignores type filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6116</link><project id="" key="" /><description>Steps to reproduce:

```
PUT twitter

GET twitter/tweet/_validate/query?explain
{
  "query" : {
    "term" : {"field":"value"}        
  }
}

{
   "valid": true,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "twitter",
         "valid": true,
         "explanation": "field:value"
      }
   ]
}
```

The resulting above explanation should be wrapped into a filtered query `_type:tweet` but it isn't.
</description><key id="33221129">6116</key><summary>Validate query ignores type filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-10T00:26:20Z</created><updated>2014-05-19T08:23:35Z</updated><resolved>2014-05-12T10:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Minor: update aggregations.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6115</link><project id="" key="" /><description>Fix plural/singular forms.
</description><key id="33220220">6115</key><summary>Minor: update aggregations.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jnaous</reporter><labels><label>docs</label></labels><created>2014-05-10T00:04:30Z</created><updated>2014-07-16T21:45:23Z</updated><resolved>2014-06-05T17:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:53:43Z" id="43062110">Hi @jnaous 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="jnaous" created="2014-05-14T18:53:40Z" id="43121986">Done.
</comment><comment author="javanna" created="2014-06-05T17:48:15Z" id="45251530">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed validate query parsing issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6114</link><project id="" key="" /><description>Made sure that a match_all query is used when no query is specified and ensure no NPE is thrown either.
Also used the same code path as the search api to ensure that alias filters are taken into account, same for type filters.

Closes #6111 Closes #6112 Closes #6116
</description><key id="33218308">6114</key><summary>Fixed validate query parsing issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T23:26:38Z</created><updated>2015-06-07T20:08:31Z</updated><resolved>2014-05-12T10:53:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-12T10:13:48Z" id="42815858">LGTM
</comment><comment author="javanna" created="2014-05-12T10:53:21Z" id="42818650">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percent bytes recovered greater than 100%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6113</link><project id="" key="" /><description>The recovery API sometimes reports percentages &gt; 100 for bytes recovered. This is confusing and makes no sense. 
</description><key id="33209917">6113</key><summary>Percent bytes recovered greater than 100%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T21:15:16Z</created><updated>2014-06-18T19:25:08Z</updated><resolved>2014-05-13T16:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-09T21:34:21Z" id="42716512">http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-recovery.html#big-percent
</comment><comment author="aleph-zero" created="2014-05-09T21:41:39Z" id="42717079">Hi @nik9000. Indeed the docs explain this behavior, so technically perhaps it is not a bug. That being said, it has been confusing a lot of people. I think reporting non-intuitive numbers like this detracts from confidence in the correctness of all the other measurements. 

Do you have a use-case that relies on the current behavior?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate query ignores alias filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6112</link><project id="" key="" /><description>```
PUT /foo/t/1
{"foo": "bar"}

PUT /foo/_alias/bar
{
  "filter": {"term": { "foo": "bar"}}
}

GET /foo/_validate/query?explain
{"query": { "match_all": {}}}
```

Returns:

```
  {
     "index": "foo",
     "valid": true,
     "explanation": "ConstantScore(*:*)"
  }
```

It should include the filter from the alias.
</description><key id="33194296">6112</key><summary>Validate query ignores alias filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T18:01:35Z</created><updated>2014-05-19T08:23:19Z</updated><resolved>2014-05-12T10:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-18T10:05:23Z" id="43435955">should we get this into `1.1.2` as well?
</comment><comment author="javanna" created="2014-05-19T07:55:05Z" id="43474655">Makes sense to me, will do
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate query without a body throws an NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6111</link><project id="" key="" /><description>```
GET /_validate/query

failed to executed [[[]][], source[_na_], explain:false]
java.lang.NullPointerException
    at org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction.shardOperation(TransportValidateQueryAction.java:185)
    at org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction.shardOperation(TransportValidateQueryAction.java:63)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

Should just default to a `match_all` query
</description><key id="33193996">6111</key><summary>Validate query without a body throws an NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T17:57:26Z</created><updated>2014-05-19T08:23:28Z</updated><resolved>2014-05-12T10:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Inconsistent results between template-applied Index Alias and manually-applied Index Alias filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6110</link><project id="" key="" /><description>I am using **Logstash**, **ElasticSearch 1.1.1** and **Kibana** to allow groups of users to view logs for their machines. I'd like to use an Index Template to automatically add the Index Alias Filter to the Logstash indices when they are created.

However, the template-added alias filters return inconsistent results, as you can see by running my Gist recreation https://gist.github.com/cvializ/0a494579e19e645ecd4a

A manually created index alias filter returns the correct results as expected. I believe this to be a bug in Elasticsearch.
</description><key id="33188146">6110</key><summary>Inconsistent results between template-applied Index Alias and manually-applied Index Alias filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cvializ</reporter><labels><label>bug</label><label>discuss</label></labels><created>2014-05-09T16:43:02Z</created><updated>2014-11-12T14:58:35Z</updated><resolved>2014-10-17T08:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-09T17:49:29Z" id="42694087">Replicated in master.  It looks like the filter is created before the mapping, and isn't updated after the mapping is changed.

The `explain` API output for a query via alias created via the template is: 

```
ConstantScore(+cache(_type:example) +MatchNoDocsFilter) doesn't match id 0
```

While for the alias created after the mapping is in place is:

```
ConstantScore(+cache(_type:example) +cache(host:computer1 host:computer2 host:computer3)), 
```

This has nothing to do with the filter caching.  Slightly simplified recreation:

```
PUT /accesscontrol/group/admin
{
    "name" : "admin",
    "hosts" : ["computer1","computer2","computer3"]
}

PUT /_template/admin_group
{
    "template" : "logstash-*",
    "aliases" : {        
        "template-admin-{index}" : {
            "filter" : {
                "terms" : {
                    "host" : {
                        "index" : "accesscontrol",
                        "type" : "group",
                        "id" : "admin",
                        "path" : "hosts"
                    }
                }
            }
        }
    }
}

POST /logstash-2014.05.09/example/1
{
    "message":"my sample data",
    "@version":"1",
    "@timestamp":"2014-05-09T16:25:45.613Z",
    "type":"example",
    "host":"computer1"
}

POST /_aliases
{
    "actions" : [
        { 
            "add" : { 
                "index" : "logstash-2014.05.09",
                "alias" : "admin-logstash-2014.05.09",
                "filter": {
                    "terms" : {
                        "host" : {
                            "index" : "accesscontrol",
                            "type" : "group",
                            "id" : "admin",
                            "path" : "hosts"
                        }
                    }
                }
            }
        }
    ]
}

# returns the document
GET /admin-logstash-2014.05.09/_search

# matches no docs
GET /template-admin-logstash-2014.05.09/_search


GET /admin-logstash-2014.05.09/example/1/_explain
{"query": {"match_all": {}}}

GET /template-admin-logstash-2014.05.09/example/1/_explain
{"query": {"match_all": {}}}
```
</comment><comment author="javanna" created="2014-05-09T18:36:53Z" id="42699046">I confirm @clintongormley's diagnosis. This happens specifically with terms filter lookup since the filter gets parsed upon index creation, before the dynamic mapping gets created.

Same reproduces with ordinary aliases too if created before the first document gets indexed (just creating an empty index before creating the alias):

```
PUT /accesscontrol/group/admin
{
    "name" : "admin",
    "hosts" : ["computer1","computer2","computer3"]
}

PUT /logstash-2014.05.09

POST /_aliases
{
    "actions" : [
        { 
            "add" : { 
                "index" : "logstash-2014.05.09",
                "alias" : "admin-logstash-2014.05.09",
                "filter": {
                    "terms" : {
                        "host" : {
                            "index" : "accesscontrol",
                            "type" : "group",
                            "id" : "admin",
                            "path" : "hosts"
                        }
                    }
                }
            }
        }
    ]
}

POST /logstash-2014.05.09/example/1
{
    "message":"my sample data",
    "@version":"1",
    "@timestamp":"2014-05-09T16:25:45.613Z",
    "type":"example",
    "host":"computer1"
}

# matches no docs
GET /admin-logstash-2014.05.09/_search
```

A work around till we find a proper way to solve it is to manually create the index specifying its mappings before indexing the first document. This way the filter gets created after the mapping is parsed:

```
PUT /accesscontrol/group/admin
{
    "name" : "admin",
    "hosts" : ["computer1","computer2","computer3"]
}

PUT /_template/admin_group
{
    "template" : "logstash-*",
    "aliases" : {        
        "template-admin-{index}" : {
            "filter" : {
                "terms" : {
                    "host" : {
                        "index" : "accesscontrol",
                        "type" : "group",
                        "id" : "admin",
                        "path" : "hosts"
                    }
                }
            }
        }
    }
}


PUT /logstash-2014.05.09
{
  "mappings": {
    "example" : {
      "properties": {
        "host" : {
          "type" : "string"
        }  
      } 
    }
  }
}

POST /logstash-2014.05.09/example/1
{
    "message":"my sample data",
    "@version":"1",
    "@timestamp":"2014-05-09T16:25:45.613Z",
    "type":"example",
    "host":"computer1"
}

GET /template-admin-logstash-2014.05.09/_search
```
</comment><comment author="cvializ" created="2014-05-09T18:40:40Z" id="42699423">Thank you, that helps a lot!
</comment><comment author="javanna" created="2014-05-12T13:22:54Z" id="42830727">Glad to hear @cvializ !

One thing I missed is that you can also add the mappings to your template, that would make things nicely work without needing to manually create the index upfront:

```
PUT /accesscontrol/group/admin
{
    "name" : "admin",
    "hosts" : ["computer1","computer2","computer3"]
}


PUT /_template/admin_group
{
    "template" : "logstash-*",
    "aliases" : {        
        "template-admin-{index}" : {
            "filter" : {
                "terms" : {
                    "host" : {
                        "index" : "accesscontrol",
                        "type" : "group",
                        "id" : "admin",
                        "path" : "hosts"
                    }
                }
            }
        }
    },
    "mappings": {
      "example" : {
        "properties": {
          "host" : {
            "type" : "string"
          }  
        } 
      }
    }
}


POST /logstash-2014.05.09/example/1
{
    "message":"my sample data",
    "@version":"1",
    "@timestamp":"2014-05-09T16:25:45.613Z",
    "type":"example",
    "host":"computer1"
}


GET /template-admin-logstash-2014.05.09/_search
```
</comment><comment author="clintongormley" created="2014-10-17T08:03:05Z" id="59479227">Fixed by #6664
</comment><comment author="loren" created="2014-11-12T14:47:37Z" id="62728624">I'm confused how this got fixed with #6664 and marked closed. Without @javanna 's workaround, @clintongormley 's slightly simplified recreation fails in 1.4.0 with 

``` json
{
   "error": "ElasticsearchIllegalArgumentException[failed to parse filter for alias [template-admin-logstash-2014.05.09]]; nested: QueryParsingException[[logstash-2014.05.09] Strict field resolution and no field mapping can be found for the field with name [host]]; ",
   "status": 400
}
```

If `host` is mapped explicitly in the logstash mapping, why can't the `admin_group` template make use of it?
</comment><comment author="clintongormley" created="2014-11-12T14:58:35Z" id="62730554">@loren not following exactly.  In my example, the `host` field is **not** mapped explicitly in the logstash mapping.  The way to fix this is to add the `host` field into the template, eg:

```
DELETE _all

PUT /accesscontrol/group/admin
{
    "name" : "admin",
    "hosts" : ["computer1","computer2","computer3"]
}

PUT /_template/admin_group
{
    "template" : "logstash-*",
    "mappings": {
      "example": {
        "properties": {
          "host": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      }
    }, 
    "aliases" : {        
        "template-admin-{index}" : {
            "filter" : {
                "terms" : {
                    "host" : {
                        "index" : "accesscontrol",
                        "type" : "group",
                        "id" : "admin",
                        "path" : "hosts"
                    }
                }
            }
        }
    }
}

POST /logstash-2014.05.09/example/1
{
    "message":"my sample data",
    "@version":"1",
    "@timestamp":"2014-05-09T16:25:45.613Z",
    "type":"example",
    "host":"computer1"
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms facet results retrieve and pick certain number products for top users</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6109</link><project id="" key="" /><description>I have a collection of products which belong to few users,  like 

[ 
  { id: 1, user_id: 1, description: "blabla...", ... }, 
  { id: 2, user_id: 2, description: "blabla...", ... }, 
  { id: 3, user_id: 2, description: "blabla...", ... }, 
  { id: 4, user_id: 3, description: "blabla...", ... }, 
  { id: 5, user_id: 4, description: "blabla...", ... }, 
  { id: 6, user_id: 2, description: "blabla...", ... }, 
  { id: 7, user_id: 3, description: "blabla...", ... }, 
  { id: 8, user_id: 4, description: "blabla...", ... }, 
  { id: 9, user_id: 2, description: "blabla...", ... }, 
  { id: 10, user_id: 3, description: "blabla...", ... }, 
  { id: 11, user_id: 4, description: "blabla...", ... }, 
  ... 
] 

(the real data has more fields, but most important ones like 1st for product id, 2nd for user id, 3rd for product description.) 

I'd like to retrieve 2 products for top 3 users whose products have highest matching score (matching condition is description includes "fashion" and some other keywords, in this case just use "fashion" as example) : 

[ 
  { id: 2, user_id: '2', description: "blabla...", ..., _score: 100}, 
  { id: 3, user_id: '2', description: "blabla...", ..., _score: 95}, 
  { id: 4, user_id: '3', description: "blabla...", ..., _score: 90}, 
  { id: 5, user_id: '4', description: "blabla...", ..., _score: 80}, 
  { id: 7, user_id: '3', description: "blabla...", ..., _score: 70}, 
  { id: 8, user_id: '4', description: "blabla...", ..., _score: 65}, 
  ... 
] 

I have 3 possible ways to try: 

(1) use term facet to get unique user_id in nested query, then use them for the user id range of outside query which focus on match description with keywords like "fashion". 

I don't know how to implement it in ES (stuck in facet terms iteration and construct user_id range with subquery with facet), try in sql like: 

select id, user_id, description 
from product 
where user_id in ( 
  select distinct user_id 
  from product 
  limit 3) 
order by _score 
limit 6 
/\* 6  = 2 \* 3 */ 

But it cannot guarantee top 6 products coming from 3 different user. 

Also, according to the following two links, it seems facet terms specific information iteration feature has not been implemented in ES so far. 
http://elasticsearch-users.115913.n3.nabble.com/Terms-stats-facet-Additional-information-td4035199.html

https://github.com/elasticsearch/elasticsearch/issues/256

(2)  query with term filed in description matched with keywords like "fashion", at same time do statistics for each user_id with aggregation and limit the count to 2, then pick top 6 products with highest matching score. 

I still don't know how to implement in ES. 

(3) use brute force with multiple queries until find top 3 users, each one has 2 products with highest matching scores. 

I mean use a hash map, key is user_id, value is how many times it appears. Query with matching keywords first, then iterate immediate results and check hash map, if value is less than 2, add to final result product list, otherwise skip it. 

Please let me know if you can figure it out in the above 1st or 2nd way. 

Appreciate in advance. 
Yao
</description><key id="33186375">6109</key><summary>Terms facet results retrieve and pick certain number products for top users</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yao23</reporter><labels /><created>2014-05-09T16:21:53Z</created><updated>2014-05-12T12:00:43Z</updated><resolved>2014-05-12T12:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-12T11:10:44Z" id="42819854">I think the best way to achieve this is via executing subsequent requests after the main request with an aggregation definition. For each bucket you need the top documents for, you'll need to execute a search request for it (using the bucket as a filter). You can bundle the search requests in a multi search request in order to optimize this a bit.

There is a new `top_hits` aggregation (#6124) on its way, that can group the top hits under each bucket being returned.
</comment><comment author="clintongormley" created="2014-05-12T12:00:43Z" id="42823417">Closed in favour of #6124 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner object with same name as type causes query issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6108</link><project id="" key="" /><description>If I have an inner object that has the same name as a type, then queries for that inner object will fail.

The workaround is to rename the type or the inner object, but the behaviour is confusing and misleading (and wrong).

To reproduce, create a new index, type and document:
curl -XPOST 'localhost:9200/test/flow/' -d '{
  "device": {
    "id": "dummy"
  }
}'

Querying for that document by device.id will work:
curl 'localhost:9200/test/_search?pretty' -d '{
  "query":{
    "filtered": {
      "filter": {
        "term": {
          "device.id": "dummy"
        }
      }
    }
  }
}'

Add a type with the same name as the inner object, AND containing an inner object with that name as well:
curl -XPOST 'localhost:9200/test/device/' -d '{
  "device": {
    "id": "foo"
  }
}'

Now querying for the original document by device.id will fail:
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}

Removing the 'device' type will allow the original document to be queryable again:
curl -XDELETE 'localhost:9200/test/device'
</description><key id="33170681">6108</key><summary>Inner object with same name as type causes query issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rtkbkish</reporter><labels /><created>2014-05-09T13:12:29Z</created><updated>2014-05-09T13:14:46Z</updated><resolved>2014-05-09T13:14:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-09T13:14:46Z" id="42664154">Closed as duplicate of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add doc values support to _parent field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6107</link><project id="" key="" /><description>The `_parent` field can easily have a high cardinality, as a consequence field data for this field can take a lot of memory. It would be useful to have the ability to store this mapping on disk using Lucene doc values.

Doc values proved to perform very well for aggregations in combination with global ordinals (https://github.com/elasticsearch/elasticsearch/pull/5672). So now that parent/child queries use global ordinals as well (https://github.com/elasticsearch/elasticsearch/pull/5846) I think doc values could even be the default for the `_parent` field?
</description><key id="33168916">6107</key><summary>Add doc values support to _parent field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T12:46:22Z</created><updated>2015-09-03T09:54:46Z</updated><resolved>2015-05-29T19:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-09T12:52:57Z" id="42662286">+1! The tricky bit here is that the `ParentChildIndexFieldData` is a combination between the `_uid` and the `_parent` field, so I think at index time the doc values field for _parent field data needs to be based on both of these fields.
</comment><comment author="martijnvg" created="2014-05-09T13:57:18Z" id="42668541">Doc values for parent child isn't so tricky. The parent child doc values field should just contain the values of both the `_uid` and `_parent` field. Each parent type should have its own parent/child doc values field (type type name can be used as suffix for the doc values field). This logic can be implemented in ParentFieldMapper#parseCreateField

The `ParentChildIndexFieldData` should just check if a parent/child doc values field exists for a type. If that isn't the case it should just load the `ParentChildAtomicFieldData` based on the _uid and _parent field what it does today. 
</comment><comment author="jpountz" created="2014-05-09T13:58:16Z" id="42668647">This sounds good to me!
</comment><comment author="ostersc" created="2014-08-05T19:09:34Z" id="51244615">Will this actually address the problem raised in #3516 (as it was closed in favor of this one)?  This seems to imply a reduction in the space needed for _parent, but our problem is that the id_cache is growing linearly with the number of parents, even if there are no children.
</comment><comment author="clintongormley" created="2014-08-05T19:26:10Z" id="51246782">@ostersc the id_cache has been removed and instead the p/c data is stored in the fielddata cache.  it still uses up memory. this issue is about moving the p/c data to disk, which will save a lot of memory.
</comment><comment author="ostersc" created="2014-08-05T19:30:23Z" id="51247313">@clintongormley thanks for clarifying.  yes, we are seeing this issue manifest in the fielddata_breaker, but in verifying it was related to parent/child issues, we ran
 GET /_nodes/stats/indices/id_cache?human
and found all the memory residing in:
"indices": {
            "id_cache": {
               "memory_size": 
</comment><comment author="clintongormley" created="2014-08-05T19:32:37Z" id="51247637">@ostersc yes, that's just for bwc.  the actual store is in the fielddata.
</comment><comment author="ostersc" created="2014-08-05T19:36:08Z" id="51248086">@clintongormley gotcha.  Is there any known work around for this?  I was quite surprised to find the field cache grow for each parent doc (even if there are no children), so we are looking needing to move away from using parent-child as we need to support hundreds of millions of parent docs with sparsely populated children.
</comment><comment author="clintongormley" created="2014-08-06T08:57:49Z" id="51309507">@ostersc Have a read of this chapter: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/relations.html
</comment><comment author="gmenegatti" created="2014-10-22T21:25:41Z" id="60158246">+1
</comment><comment author="tmcerwin" created="2015-06-04T16:40:49Z" id="108962813">Is there any estimate on when the 2.0 version will be released?  We will likely have to remove usage of parent / child relationships, until they are moved to doc-values.  The in memory relationships are not scaling well with our application. 
</comment><comment author="jpountz" created="2015-06-04T16:45:58Z" id="108965450">As you can see from https://github.com/elastic/elasticsearch/issues/9970 there are 3 remaining boxes to tick and good progress is being made these days, so the first release candidate should happen pretty soon. However it might still take time between the release candidate and the GA depending on feedback.
</comment><comment author="tmcerwin" created="2015-06-04T16:49:51Z" id="108966874">Thanks for the quick response!  I will run some testing on the master branch.
</comment><comment author="alexkavon" created="2015-08-19T22:56:44Z" id="132817287">Is this still happening for 2.0?
</comment><comment author="martijnvg" created="2015-08-23T19:10:57Z" id="133899559">@alexkavon yes, doc values support for parent/child will be included in the first 2.0 release. If you want to try it out, just make sure that you create a new index once upgraded to 2.0. The doc values support is only enabled on indices created on or after version 2.0. Indices that existed before the upgrade to 2.0 will remain to work and perform in the same way they did on previous 1.x releases.
</comment><comment author="alexkavon" created="2015-09-02T19:33:02Z" id="137221802">So a reindexing should take care of this then?
</comment><comment author="martijnvg" created="2015-09-03T09:54:46Z" id="137395794">@alexkavon Yes, once upgraded to 2.0.x a reindex would use the new p/c implementation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Groovy as a scripting language, switching default from Mvel -&gt; Groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6106</link><project id="" key="" /><description>I'm currently working on sandboxing Groovy, but I wanted to get the discussion started/confirmed for where this is landing, 1.2+, or only the master (2.0) branch.
</description><key id="33166692">6106</key><summary>Add Groovy as a scripting language, switching default from Mvel -&gt; Groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T12:07:34Z</created><updated>2015-06-08T14:38:12Z</updated><resolved>2014-05-13T19:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-09T14:33:43Z" id="42672533">in terms of packing, I made a note that it should be optional in the pom file, so people can run it without (and handle the case where its not available in the classpath).

Also, I believe we will need to add an include in the main/assembly/common-bin.xml and also in the dev/rpm packages to make sure the groovy lib is part of all the distributions we have.
</comment><comment author="hbs" created="2014-05-09T14:52:25Z" id="42674686">How do you intend to prevent rogue scripts?
</comment><comment author="kimchy" created="2014-05-09T15:02:48Z" id="42675899">@hbs the safest way is to disable dynamic scripting (default in 1.2), and rely on pre defined set of scripts. As for sandboxing, this is still in the explorative mode on how well we can sandbox groovy for cases where dynamic scripts are allowed.
</comment><comment author="nik9000" created="2014-05-09T15:08:27Z" id="42676517">&gt; @hbs the safest way is to disable dynamic scripting (default in 1.2), and rely on pre defined set of scripts. As for sandboxing, this is still in the explorative mode on how well we can sandbox groovy for cases where dynamic scripts are allowed.

If it turns out Groovy can't be sandboxed is it still worth switching to?
</comment><comment author="nik9000" created="2014-05-09T15:09:40Z" id="42676660">&gt; If it turns out Groovy can't be sandboxed is it still worth switching to?

Though to make my position clear: if it can be sandboxed it is most definitely worth switching to the default.
</comment><comment author="kimchy" created="2014-05-09T15:12:32Z" id="42676996">@nik9000 this is what we are exploring now, if it can be properly sandboxed, thats why I expect this to be delayed to 1.3 to properly make a decision with a decent time frame. Even if it can't be sandboxed, it does has the benefit of (1) a properly maintained language compared to mvel (2) speed, based on our tests, its faster than mvel and different javascript implementations (3) great story around concurrent execution and clean integration when running on the JVM
</comment><comment author="hbs" created="2014-05-11T18:30:54Z" id="42778824">My experience with previous versions of Groovy is that it cannot be sandboxed easily, if at all. The use of SecureASTCustomizer proves difficult, especially if you want to allow anything else than basic arithmetic functions, and the use of a JVM wide SecurityManager is just infeasible.
</comment><comment author="dakrone" created="2014-05-12T12:57:54Z" id="42828201">@hbs yes, the SecureASTCustomizer is not really nice for security, more for locking down the available language features. I am looking into using a separate classloader with sandboxing as an alternative, but still researching this subject currently.

@nik9000 it's definitely worth switching, even if we can't sandbox groovy, because mvel has some serious compilation issues (see: https://gist.githubusercontent.com/dakrone/b193c3510073e6a22f42/raw/83a9f7614821a6fb917356324d2a788d33b981e1/gistfile1.txt for an example). Additionally, groovy is much faster than mvel, so even if we keep the same amount of functionality, the speed improvement would be nice.
</comment><comment author="dakrone" created="2014-05-13T19:06:30Z" id="42998200">Closing this for now, as we've decided to postpone this until 1.3 to get sandboxing in. I'll re-open when I get the sandboxing in a testable state.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Partial network partition and retries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6105</link><project id="" key="" /><description>Given the following scenario:
- 3 server nodes (`minimum_master_nodes=2`)
- `node 1` detaches from `node 2` and `node 3`
- The `client` (java) can still reach all nodes.

If the `client` chooses to connect to `node 1`, it will return an `EsRejectedExecutionException` that reports too few master nodes (since `node 1` only sees itself).
Then, if the client did retry `node 2` or `node 3`, as if this was a connection-error, it would return a success.
We would prefer if the java-client could handle this type of error, just like it handles connection errors (or at least retry up to `n-q` nodes).

This could happen in a variety of ways, i.e. a 2-rack setup with a different network interface for inter-rack-communication and external communication with clients.

We've seen this scenario in our live systems, with a much simpler setup.
</description><key id="33161861">6105</key><summary>Partial network partition and retries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">magnhaug</reporter><labels><label>:Java API</label><label>discuss</label><label>enhancement</label></labels><created>2014-05-09T10:38:34Z</created><updated>2015-10-14T16:25:22Z</updated><resolved>2015-10-14T16:25:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-09T10:56:54Z" id="42654452">double checking, I assume you are talking about the `TransportClient`, right?
</comment><comment author="magnhaug" created="2014-05-09T12:02:21Z" id="42658662">Yes, I am talking about the `TransportClient` primarly. Searching/indexing/etc.
</comment><comment author="kimchy" created="2014-05-09T12:46:13Z" id="42661787">yea, the `TransportClient`, different form the `NodeClient`, has the retry logic on another endpoint, today, we retry on connection problems, but we should retry on other cases as well, specifically, the one mentioned above. The `NodeClient` doesn't suffer from this problem, because it automatically handles it already.
</comment><comment author="dakrone" created="2015-04-10T17:29:50Z" id="91628767">@spinscale is this something that is fixed by using sniffing on the transport client?
</comment><comment author="spinscale" created="2015-05-29T16:36:46Z" id="106866333">@dakrone we dont seem to have this fixed completely in all setups (sniffing seems not to be affected though, but adding all of the nodes manually seems to have a different behaviour)... I created a test that fakes long gc runs, network interruptions or a down node and that one fails in about 20% of the time it runs... this is the test, maybe there is a flaw in it, but so far it looks ok to me

``` java
    /**
     * issue: https://github.com/elastic/elasticsearch/issues/6105
     *
     * 1. Start a cluster with three nodes
     *
     * 2. Start threee different transport clients
     *  - First hast only nodeX confgiured to connect to
     *  - Second has all nodes configured
     *  - Third has only nodeX configured but sniffing enabled
     *
     * 3. Split nodeX away from cluster
     *
     * 4. Check TransportClient expectations
     *  - First should fail
     *  - Second should keep working
     *  - Third should keep working
     */
    @Test
    @TestLogging("discovery:DEBUG")
    public void testThatTransportClientCanHandleNodeOutage() throws Exception {
        final List&lt;String&gt; nodes = startUnicastCluster(3, null, 2);
        final String masterNode = internalCluster().getMasterName();

        // kill the first non master node
        String failingNode = null;
        for (String node : nodes) {
            if (!node.equals(masterNode)) {
                failingNode = node;
                break;
            }
        }

        Map&lt;String, TransportAddress&gt; nodeAddresses = new HashMap&lt;&gt;();
        for (TransportService transportService : internalCluster().getDataNodeInstances(TransportService.class)) {
            nodeAddresses.put(transportService.nodeName(), transportService.boundAddress().publishAddress());
        }

        SingleNodeDisruption nodeDisruption = new LongGCDisruption(getRandom(), failingNode);

        TransportClient tc1 = null, tc2 = null, tc3 = null;
        Settings allTransportClientSettings = settingsBuilder().put("cluster.name", internalCluster().getClusterName())
                .put("client.transport.nodes_sampler_interval", "1s")
                .put("path.home", createTempDir())
                .put("node.mode", InternalTestCluster.nodeMode())
                .put("name", "transport_client_" )
                .put("config.ignore_system_properties", true)
                .put("plugins." + PluginsService.LOAD_PLUGIN_FROM_CLASSPATH, false)
                .put("client.transport.sniff", false)
                .build();
        try {
            tc1 = TransportClient.builder().settings(settingsBuilder().put(allTransportClientSettings).put("name", "tc1")).build();
            tc1.addTransportAddress(nodeAddresses.get(failingNode));

            tc2 = TransportClient.builder().settings(settingsBuilder().put(allTransportClientSettings).put("name", "tc2")).build();
            tc2.addTransportAddresses(nodeAddresses.values().toArray(new TransportAddress[]{}));

            tc3 = TransportClient.builder().settings(settingsBuilder().put(allTransportClientSettings).put("name", "tc3").put("client.transport.sniff", true)).build();
            tc3.addTransportAddress(nodeAddresses.get(failingNode));

            if (randomBoolean()) {
                nodeDisruption.startDisrupting();
            } else {
                addRandomIsolation(failingNode).startDisrupting();
            }

            logger.info("waiting for [{}] to be removed from cluster", failingNode);
            ensureStableCluster(2, masterNode);

            TimeValue timeout = timeValueSeconds(10);

            try {
                tc1.admin().cluster().prepareHealth().get(timeout);
            } catch (ElasticsearchTimeoutException e) {
                // all good, no sniffing enabled, no chance to connect to other nodes
            }

            // TODO this fails in case of a network disruption, transport client should be smarter
            assertThat(tc2.admin().cluster().prepareHealth().get(timeout).getStatus(), is(ClusterHealthStatus.GREEN));
            assertThat(tc3.admin().cluster().prepareHealth().get(timeout).getStatus(), is(ClusterHealthStatus.GREEN));

            // lets resolve the disruption and check that all three clients can execute requests as expected
            nodeDisruption.stopDisrupting();
            internalCluster().clearDisruptionScheme();
            logger.info("waiting for [{}] to be re-added to cluster", failingNode);
            ensureStableCluster(3, masterNode);

            assertThat(tc1.admin().cluster().prepareHealth().get(timeout).getStatus(), is(ClusterHealthStatus.GREEN));
            assertThat(tc2.admin().cluster().prepareHealth().get(timeout).getStatus(), is(ClusterHealthStatus.GREEN));
            assertThat(tc3.admin().cluster().prepareHealth().get(timeout).getStatus(), is(ClusterHealthStatus.GREEN));

        } finally {
            nodeDisruption.stopDisrupting();
            internalCluster().clearDisruptionScheme();
            if (tc1 != null) tc1.close();
            if (tc2 != null) tc2.close();
            if (tc3 != null) tc3.close();
        }
    }

```
</comment><comment author="clintongormley" created="2015-10-14T16:25:22Z" id="148106099">This appears to be fixed.  Please reopen if you see the same issues with 2.0 or later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log script change/add and removal at INFO level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6104</link><project id="" key="" /><description>Now that dynamic scripting is disabled, it's extremely helpful to know if/when a script is added or removed via the `ScriptChangesListener`, previously these were only TRACE level messages, they now look like:

```
[2014-05-09 12:01:25,393][INFO ][script                   ] [Plunderer] compiling script file [/Users/hinmanm/src/elasticsearch/config/scripts/calculate-score.mvel]
[2014-05-09 12:02:25,397][INFO ][script                   ] [Plunderer] compiling script file [/Users/hinmanm/src/elasticsearch/config/scripts/test.mvel]
[2014-05-09 12:03:25,399][INFO ][script                   ] [Plunderer] removing script file [/Users/hinmanm/src/elasticsearch/config/scripts/test.mvel]
```

To give an indication as to when the script changes have noticed and the scripts added/reloaded/removed.
</description><key id="33159881">6104</key><summary>Log script change/add and removal at INFO level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-09T10:06:07Z</created><updated>2015-06-08T14:38:26Z</updated><resolved>2014-05-13T07:46:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-05-09T15:30:57Z" id="42679095">LGTM. Certainly an improvement to have it pass in the argument as opposed to always concatenating it anyway.
</comment><comment author="jpountz" created="2014-05-12T10:42:57Z" id="42817913">It looks good to me as well!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why AnalyzerProvider is initialized twice When create index </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6103</link><project id="" key="" /><description>Hi:
Java code:
Part 1:
    client.admin().indices().prepareExists(index).execute().actionGet();
Part 2:
    BulkRequestBuilder builder = client.prepareBulk();
    for(AbsBean bean : e){
        builder.add(client.prepareIndex(index, type).setSource(bean.toRequest()));
    }
    BulkResponse resp = builder.execute().actionGet();

AnalyzerProvider is initialized twice when Part1 and Part2 run together in one process, but AnalyzerProvider is initialized once when Part1 and Part2 run in two processes.

thanks 
</description><key id="33146733">6103</key><summary>Why AnalyzerProvider is initialized twice When create index </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xushaonan</reporter><labels><label>:Internal</label></labels><created>2014-05-09T05:21:00Z</created><updated>2016-11-27T14:38:13Z</updated><resolved>2016-11-27T14:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-09T09:15:44Z" id="42647340">Hey,

can you be somewhat more specific, what you mean with initializing `AnalyzerProvider`, which is an interface. What exactly are you trying to do and where exactly is the problem you are having?
</comment><comment author="xushaonan" created="2014-05-10T01:26:29Z" id="42727854">the code
public class XXXAnalyzerProvider extends AbstractIndexAnalyzerProvider&lt;XXXAnalyzer&gt;  {
@Inject
    public XXXAnalyzerProvider(Index index, ThreadPool pool,
            @IndexSettings Settings indexSettings, Environment env, @Assisted String name, @Assisted Settings settings) {
        super(index, indexSettings, name, settings);
        this.analyzer = new XXXAnalyzer(env.configFile());
        System.out.println("======Analyzer Init=======" + System.currentTimeMillis());
    }

Analyzer Init print twice, when create index.
</comment><comment author="s1monw" created="2016-11-27T14:38:06Z" id="263125755">we don't use guice to create this stuff anymore I will close this for now</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add link to staltstack module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6102</link><project id="" key="" /><description /><key id="33142259">6102</key><summary>add link to staltstack module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">medcl</reporter><labels /><created>2014-05-09T03:00:44Z</created><updated>2014-07-18T10:47:52Z</updated><resolved>2014-07-18T10:47:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-09T09:13:48Z" id="42647190">Hey, 

nice, thanks! Please sign the CLA at http://www.elasticsearch.org/contributor-agreement/ so I can get it the PR in.
</comment><comment author="medcl" created="2014-05-09T10:01:45Z" id="42650755">i just signed the CLA.
</comment><comment author="spinscale" created="2014-07-18T10:47:52Z" id="49418269">closed by https://github.com/elasticsearch/elasticsearch/commit/792d8970e8168fc7f1a6997430db9b8073df13b1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a typo. Changed "should" to "optional" to match the docs for minimum_should_match.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6101</link><project id="" key="" /><description>It also might be clearer to change it to 'or' to keep consistent with the language of the section.
</description><key id="33130962">6101</key><summary>Fixed a typo. Changed "should" to "optional" to match the docs for minimum_should_match.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ericheiker</reporter><labels /><created>2014-05-08T22:46:25Z</created><updated>2014-07-16T21:45:24Z</updated><resolved>2014-05-14T10:00:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-09T10:33:00Z" id="42652855">Hi @ericheiker 

Hmm, actually, I'm thinking it should be "`should` clauses" or "optional `should` clauses". I think we definitely need `should` in there, otherwise people will wonder which clauses are optional.

Could you make that change and sign the CLA, so that we can get your commit merged in?
http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="ericheiker" created="2014-05-09T17:34:11Z" id="42692510">Well how about changing it to "`or` clauses"? `should` is used in the actual "bool query" documentation, but in this "match query" section the `or` terminology seems to be preferred. As a newbie reading this doc the shift in terminology was a bit confusing, especially considering `should` isn't mentioned or explained anywhere before.

CLA signed!
</comment><comment author="clintongormley" created="2014-05-14T10:00:17Z" id="43062631">Thanks, I changed it to "optional `should` clauses" as I think that is more accurate.

merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move logger.trace() outside of try statement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6100</link><project id="" key="" /><description>While using  TransportClient in application with older log4j version 1.2.8 it is possible to get very misleading NoNodeAvailableException (see at the very bottom of this message):

The reason is that log4j doesn't have method trace() and call to logger.trace() is placed inside try/catch(Throwable) below. It results in exception being thrown during execution of logger.trace()  which later lead to NoNodeAvailable exception down the road.

Thanks,
Igor.

protected ImmutableList&lt;DiscoveryNode&gt; validateNewNodes(Set&lt;DiscoveryNode&gt; nodes) {
            for (Iterator&lt;DiscoveryNode&gt; it = nodes.iterator(); it.hasNext(); ) {
                DiscoveryNode node = it.next();
                if (!transportService.nodeConnected(node)) {
                    try {
                        logger.trace("connecting to node [{}]", node);
                        transportService.connectToNode(node);
                    } catch (Throwable e) {
                        it.remove();
                        logger.debug("failed to connect to discovered node [" + node + "]", e);
                    }

May  8 11:15:28 9.21.119.107 [dataNode.dataNode] [Ariel Writer#events] org.elasticsearch.client.transport.NoNodeAvailableException: No node available
May  8 11:15:28 9.21.119.107 [dataNode.dataNode] [Ariel Writer#events]    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:219)
</description><key id="33109444">6100</key><summary>move logger.trace() outside of try statement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zztv</reporter><labels><label>:Logging</label><label>enhancement</label></labels><created>2014-05-08T18:23:47Z</created><updated>2016-05-20T20:44:18Z</updated><resolved>2016-05-20T20:38:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-08T18:30:34Z" id="42587062">I think that maybe we should validate that trace is actually there when the log4j logger is configured, and fail the node from starting..., this is probably bad on many other places...
</comment><comment author="zztv" created="2014-05-08T18:53:23Z" id="42590168">This is not real node. It is transport client embedded into our application. TransportClient connects to the real ES nodes. The only place where you can do this kind of checking is constructor of TransportClient by throwing an exception ( which I don't like):

Client client = new TransportClient(settings);

My point is - It is our application fault to use old log4j library, but your diagnostic exception should be clearer. After I replace log4j to newer version everything was fine...
</comment><comment author="pickypg" created="2014-05-09T15:12:06Z" id="42676934">Related [slf4j.org](http://www.slf4j.org/codes.html#log4j_version). log4j added trace on August 29, 2005 in `1.2.12`. Elasticsearch itself bundles log4j `1.2.17` as an optional dependency.

Any environment that is producing this issue really has a [_very_ outdated version of log4j](http://logging.apache.org/log4j/1.2/changes-report.html) on the classpath (e.g., `1.2.8` was released in early 2003). It certainly is annoying that `NoNodeAvailableException` occurs because of this, but it's so unexpected that perhaps something else can be done.

What may be useful is to do what @kimchy is getting at, but in a grander way on startup of the client. In essence, create a short integration test that kicks off at startup that tests known potential issues that are deemed worthwhile, and rethrows them as an `ElasticsearchIllegalStateException` (or similar). Then this could just be a quick one-liner there if it made the cut.

Personally, if we wanted to fail/test for _this_ behavior, then I would actually just add a logger.trace to the top of the function that did something like `logger.trace("validating new nodes");` and then change the current trace `logger.debug("connecting to node [{}]", node);` (or leave as-is, but personally see it as debug anyway). This way, it will fail outside of the `try` and give a pretty clear `Error`.
</comment><comment author="nikolavp" created="2014-10-03T10:58:19Z" id="57780556">We also bumped on this and it isn't cool. Required a remote debugging in a comlpex environment... Can't you just do one of the following:
- Don't log the trace but make it a debug
- Log in the catch Throwable blog with info or warning - this way I will be able to at least see the exception(in this case NoSuchMethodError)

i can understand that this is partly our fault because as @pickypg said, we had an old version of the log4j jar in the classpath.

One more thing would be to throw an IllegalStateException when you do the logger detection in the ESLogger factory if there is no trace level.
</comment><comment author="clintongormley" created="2014-10-24T09:01:56Z" id="60361917">We should add a check for the ability to log at trace level in the transport client.
</comment><comment author="colings86" created="2014-10-24T09:02:55Z" id="60361997">We could do this as a very lightweight 'LoggingCheckerModule' started by the transport client which checks it can log at each level
</comment><comment author="jasontedor" created="2016-05-20T20:38:51Z" id="220712801">As of #16703, we only support log4j on the backend and with #17697 we will support log4j 2 only (which of course, supports trace). Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>APT repo for 0.90 has wrong architecture</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6099</link><project id="" key="" /><description>I'm unable to mirror the 0.90 APT repository with the tool [aptly](www.aptly.info).

This is because the `Architectures: all` listed in the main index doesn't match the package indexes/structure of `all`, `amd64` and `i386`. Are you able to update this?

Here's the relevant parts of the 0.90 repo:

```
&#10140;  ~  curl http://packages.elasticsearch.org/elasticsearch/0.90/debian/dists/stable/Release
Architectures: noarch
&#8230;
MD5Sum:
 d41d8cd98f00b204e9800998ecf8427e                0 Release
 473b769975729a4d7fd2e9a85eb72b97            26366 main/binary-all/Packages
 0ecf4ce88c1b561295f31f0fb19ea92a             3745 main/binary-all/Packages.gz
 473b769975729a4d7fd2e9a85eb72b97            26366 main/binary-amd64/Packages
 0ecf4ce88c1b561295f31f0fb19ea92a             3745 main/binary-amd64/Packages.gz
 473b769975729a4d7fd2e9a85eb72b97            26366 main/binary-i386/Packages
 0ecf4ce88c1b561295f31f0fb19ea92a             3745 main/binary-i386/Packages.gz
```

The 1.0 repo doesn't have the same problem:

```
&#10140;  ~  curl http://packages.elasticsearch.org/elasticsearch/1.0/debian/dists/stable/Release
Architectures: i386 amd64
&#8230;
MD5Sum:
 d41d8cd98f00b204e9800998ecf8427e                0 Release
 d5fdde89ae778da38f0782f5aad15930            15036 main/binary-all/Packages
 b97a9a5bb1352d3f3aa7b643e5994807             2508 main/binary-all/Packages.gz
 d5fdde89ae778da38f0782f5aad15930            15036 main/binary-amd64/Packages
 b97a9a5bb1352d3f3aa7b643e5994807             2508 main/binary-amd64/Packages.gz
 d5fdde89ae778da38f0782f5aad15930            15036 main/binary-i386/Packages
 b97a9a5bb1352d3f3aa7b643e5994807             2508 main/binary-i386/Packages.gz
```
</description><key id="33105760">6099</key><summary>APT repo for 0.90 has wrong architecture</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dcarley</reporter><labels /><created>2014-05-08T17:40:06Z</created><updated>2014-05-09T11:05:03Z</updated><resolved>2014-05-09T10:44:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-05-09T09:04:18Z" id="42646487">Hi,

Sorry for the difference in the repo's.
I've made the required changes and updated the repo.
Can you verify if it works now for you?

Cheers.
</comment><comment author="dcarley" created="2014-05-09T10:44:42Z" id="42653601">Excellent, that works now. Thanks for the quick response! :beers: 
</comment><comment author="electrical" created="2014-05-09T11:05:03Z" id="42654947">Great to hear its working now :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to change concurrent merge scheduling setting dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6098</link><project id="" key="" /><description>Allow to change the concurrent merge scheduler settings dynamically using the update settings API
</description><key id="33104860">6098</key><summary>Allow to change concurrent merge scheduling setting dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-08T17:28:30Z</created><updated>2015-06-07T13:31:07Z</updated><resolved>2014-05-12T14:34:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-05-08T17:45:06Z" id="42581457">The changes look fine, except we have no test coverage in Lucene verifying these live settings changes take effect / don't cause deadlock / whatnot.  I know Rob is looking at randomizing this in the test infra, so we'll get some limited "first do no harm" coverage with that.
</comment><comment author="mikemccand" created="2014-05-08T19:13:50Z" id="42592945">I committed a test case to Lucene to confirm live changes to maxMergeCount take effect: http://svn.apache.org/r1593380 ... seems to be passing!  Testing maxThreadCount is trickier ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>snapshot/restore expand_wildcards option not consistent with other apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6097</link><project id="" key="" /><description>With #4453 we introduced new indices options to be used across all apis that support multiple indices. Those options are listed here in our docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/multi-index.html .

All apis support the `expand_wildcards` option that accepts `open`, `closed` or both as values, while create and restore snapshot apis accept `expand_wildcards_open` and `expand_wildcards_closed` as booleans instead. Looks like we should make them consistent with other apis.
</description><key id="33102167">6097</key><summary>snapshot/restore expand_wildcards option not consistent with other apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label></labels><created>2014-05-08T16:59:36Z</created><updated>2015-04-23T01:27:46Z</updated><resolved>2015-04-23T01:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-02-26T10:19:57Z" id="76153895">I think this got fixed right @imotov ? can we close?
</comment><comment author="imotov" created="2015-02-26T13:01:54Z" id="76174089">@javanna no, it looks like it's still an issue https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java#L392
</comment><comment author="javanna" created="2015-02-26T13:02:50Z" id="76174194">doh! I was almost sure we were using indices options there like in all other apis. thanks for checking.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OSGI ClassLoader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6096</link><project id="" key="" /><description>&gt; public static ClassLoader getDefaultClassLoader() {
&gt;         ClassLoader cl = null;
&gt;         try {
&gt;             cl = Thread.currentThread().getContextClassLoader();
&gt;         } catch (Throwable ex) {
&gt;             // Cannot access thread context ClassLoader - falling back to system class loader...
&gt;         }
&gt;         if (cl == null) {
&gt;             // No thread context class loader -&gt; use class loader of this class.
&gt;             cl = Classes.class.getClassLoader();
&gt;         }
&gt;         return cl;
&gt;     }

in osgi the ClassLoader is incorrect&#12290;

for example I usring titan0.4.4 with elasticsearch 0.9.5&#65292;when elasticsearch throw exception ,sometimes can't load the class Remotetransportexception.

So can provide an interface to set it
</description><key id="33079720">6096</key><summary>OSGI ClassLoader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">boliza</reporter><labels><label>feedback_needed</label></labels><created>2014-05-08T12:41:03Z</created><updated>2015-02-28T05:14:49Z</updated><resolved>2015-02-28T05:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:21:48Z" id="68381767">Hi @boliza 

Sorry it has taken a long time to get to this.  I don't know anything about OSGI. Could you explain in more detail what the problem is (if it is still a problem) and what you would like us to change?
</comment><comment author="clintongormley" created="2015-02-28T05:14:49Z" id="76511316">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pc min should match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6095</link><project id="" key="" /><description>Adds a `minimum_children` parameter to the `has_child` query and filter.  
- For the query with `score_mode: sum|avg|max`, it was possible to support this directly in ChildrenQuery.
- For the filter and the query with `score_mode: none`, I added CountChildrenConstantScoreQuery as it seemed a cleaner solution than trying to make either ChildrenQuery or ChildrenConstantScoreQuery fit

YAML tests have been added, but not sure what to do the Java tests.  Also still needs docs.

Closes #6019
</description><key id="33076793">6095</key><summary>Pc min should match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-05-08T11:50:26Z</created><updated>2014-06-27T23:36:26Z</updated><resolved>2014-06-04T09:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-19T15:31:33Z" id="43519063">@clintongormley can we add a `maximum_children` parameter as well?
</comment><comment author="clintongormley" created="2014-05-19T15:33:19Z" id="43519272">@dakrone Yeah, I was thinking the same thing. 

Btw, this PR hasn't stalled - I want to sit down with @martijnvg after 1.2 and see what to do about refactoring.
</comment><comment author="clintongormley" created="2014-05-29T14:40:39Z" id="44538951">This PR now includes `max_children`, tests and docs and is ready for review.  
</comment><comment author="martijnvg" created="2014-05-30T17:13:15Z" id="44675411">Lets get this in! LGTM!
</comment><comment author="javanna" created="2014-06-03T11:27:50Z" id="44952267">I think we can close this one right @clintongormley ? Also the labels are already on the related issue, we might want to remove them from the PR.
</comment><comment author="clintongormley" created="2014-06-04T09:33:40Z" id="45070336">Closed by #6019
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: BenchmarkIntegrationTest failure fixed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6094</link><project id="" key="" /><description>Following tests are failing:
BenchmarkIntegrationTest#testSubmitBenchmark
BenchmarkIntegrationTest#testAbortBenchmark
</description><key id="33074842">6094</key><summary>Benchmark: BenchmarkIntegrationTest failure fixed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>blocker</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-08T11:15:35Z</created><updated>2014-09-27T08:38:05Z</updated><resolved>2014-05-14T23:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-05-14T23:58:24Z" id="43154841">@s1monw Fixed this in fc2ab0909ee2bd074de9a380405aec00c7b1fde0 and 2c1c5c163f24183ce4275c0190f11f85f6ee8311 so I'm closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check if root mapping is actually valid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6093</link><project id="" key="" /><description>When a mapping is declared and the type is known from the uri
then the type can be skipped in the body (see #4483). However,
there was no check if the given keys actually make a valid mapping.

closes #5864
</description><key id="33073822">6093</key><summary>Check if root mapping is actually valid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-08T10:59:17Z</created><updated>2015-06-08T14:38:34Z</updated><resolved>2014-05-12T16:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-05-09T10:23:55Z" id="42652256">I added some commits to implement the comments. I am unsure about the RootObjectMapper properties ("dynamic_templates", ...). I added two commits: One just checks for all the field names that I found in the RootObjectMapper code ("check rootObjectMapper keys also") and another that converts the Strings that are checked in there to ParseFields ("make RootObjectMapper keys ParseFields") but I am unsure if this makes the code much better. 
</comment><comment author="brwe" created="2014-05-09T18:45:34Z" id="42699951">There are still options missing, like "analyzer", "index_analyzer",...
I'll update the pull request.
</comment><comment author="brwe" created="2014-05-10T17:01:37Z" id="42747601"> I pushed a new version, the old one can still be found at: https://github.com/brwe/elasticsearch/tree/issue-5864-v1

  Instead of collecting all keys that can be in a root object mapping I split the root object  and object parsing into properties that can be in the root object, those that can't and those that can be in both. In the root object parsing, I remove each found key from the mapping and then check if the mapping is empty after parsing. This has the nice side effect that now also mappings of the form `{ "TYPME_NAME": {` are parsed and therefore there is a pretty high test coverage I think. This also revealed that I missed many of the root object properties in my first draft. Thanks for the tip, @jpountz!
</comment><comment author="jpountz" created="2014-05-12T09:47:37Z" id="42813735">Apart from minor style issues this looks good to me.
</comment><comment author="brwe" created="2014-05-12T10:41:57Z" id="42817837">thanks for the review, I added some commits
</comment><comment author="jpountz" created="2014-05-12T10:46:28Z" id="42818149">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provided some insights as to how More Like This works internally.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6092</link><project id="" key="" /><description>In the Google Groups forum there appears to be some confusion as to what mlt
does. This documentation update should hopefully help demystifying this
feature, and provide some understanding as to how to use its parameters.
</description><key id="33072306">6092</key><summary>Provided some insights as to how More Like This works internally.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-08T10:33:45Z</created><updated>2014-07-16T21:45:26Z</updated><resolved>2014-05-09T10:15:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Fix abort benchmark test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6091</link><project id="" key="" /><description>In some cases a benchmark may complete before we have a chance to abort
it. Just log these cases and continue.
</description><key id="33051830">6091</key><summary>Benchmark: Fix abort benchmark test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>test</label></labels><created>2014-05-08T04:08:31Z</created><updated>2015-03-19T20:05:53Z</updated><resolved>2014-05-10T00:18:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-09T06:36:43Z" id="42637613">This looks good for now but in the long term I think we should try to make it more robust. Otherwise `abort` would never get tested on machines that are fast enough, so if there is an actual bug that also happens on very fast machines, we wouldn't catch it.
</comment><comment author="kimchy" created="2014-05-09T14:26:14Z" id="42671711">I talked to @aleph-zero, I suggested another way to solve the "sleep" problem, something along the lines of:

```
@Test
    @Timeout(millis = 10000)
    public void testAbortBenchmark() throws Exception {

        final BenchmarkRequest request =
                BenchmarkTestUtil.randomRequest(client(), indices, numExecutorNodes, competitionSettingsMap, crazyNumberOfExecutions);
        http://logger.info("--&gt; Submitting benchmark - competitors [{}] iterations [{}]", request.competitors().size(),
                request.settings().iterations());
        ActionFuture&lt;BenchmarkResponse&gt; bench = client().bench(request);
        final AbortBenchmarkResponse response = client().prepareAbortBench(BENCHMARK_NAME).execute().actionGet();
        assertThat(response.getNodeResponses().size(), lessThanOrEqualTo(numExecutorNodes));
        assertThat(response.getBenchmarkName(), equalTo(BENCHMARK_NAME));
        for (AbortBenchmarkNodeResponse nodeResponse : response.getNodeResponses()) {
            assertThat(nodeResponse.benchmarkName(), equalTo(BENCHMARK_NAME));
            assertThat(nodeResponse.errorMessage(), nullValue());
            assertThat(nodeResponse.nodeName(), notNullValue());
        }
        // Send a list request to make sure there are no active benchmarks
        final BenchmarkStatusResponse statusResponse = client().prepareBenchStatus().execute().actionGet();
        assertThat(statusResponse.benchmarkResponses().size(), equalTo(0));

        assertThat(bench.get().state(), equalTo(BenchmarkResponse.State.ABORTED));
    }
```
</comment><comment author="jpountz" created="2014-05-09T15:09:46Z" id="42676667">Agreed it would be better without sleeping! Maybe there should just be an `awaitBusy` on the benchmark status API between `client().bench(request)` and `client().prepareAbortBench` in order to make sure that nodes won't try to abort the benchmark before it is actually started?
</comment><comment author="kimchy" created="2014-05-09T15:10:39Z" id="42676770">@jpountz ++
</comment><comment author="aleph-zero" created="2014-05-09T16:42:43Z" id="42687055">Interesting idea @jpountz. I like it's simplicity. Should have something for review shortly.
</comment><comment author="aleph-zero" created="2014-05-09T20:52:35Z" id="42712485">@kimchy @jpountz Busy waiting has been implemented and tests are running smoothly. Would you mind taking a look?
</comment><comment author="jpountz" created="2014-05-09T22:25:08Z" id="42720184">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin should allow plugin files to be maintained for debugging purposes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6090</link><project id="" key="" /><description>Currently, when unpacking plugin files, the files will be deleted in multiple scenarios (e.g., if it detects any `*.java` files) without a way to prevent it.

It would be ideal if there were a flag like `--retain` that could accompany `--install` (and the future `--update`) that would prevent the plugin manager from deleting the perceived-to-be extra--or particularly invalid--files.  That way, plugin developers could themselves debug situations more easily and end users could see what the actual cause for the failure was.

The files should be retained in a special directory rather than in-place to avoid leaving files where they should not exist in the event that users do nothing with the retained files. It's worth considering whether to wholesale copy the original directory (`extractLocation`) into the special directory and then delete the actual files versus selectively moving the content that would otherwise be deleted.

As it currently exists, this pertains to the `unpack` function as it is currently implemented and modified in #5977.
</description><key id="33047998">6090</key><summary>bin/plugin should allow plugin files to be maintained for debugging purposes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>feedback_needed</label></labels><created>2014-05-08T02:46:19Z</created><updated>2014-12-31T17:36:29Z</updated><resolved>2014-12-31T17:36:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:20:19Z" id="68381618">@pickypg not quite understanding this issue... is this still something that you want?  if so, could you explain more?
</comment><comment author="pickypg" created="2014-12-30T18:30:38Z" id="68382574">https://github.com/elasticsearch/elasticsearch/pull/5977#discussion-diff-12103064

It related to how the `PluginManager` deletes everything, which seems pretty strict for any plugin developer that may be hitting issues with integration. Every step of [unpacking the download results in deletions](https://github.com/dadoonet/elasticsearch/blob/pr/5566-plugin-isinstalled/src/main/java/org/elasticsearch/plugins/PluginManager.java#L485) that may be hiding an issue.

It's certainly not a must-have flag.
</comment><comment author="clintongormley" created="2014-12-31T10:59:11Z" id="68435633">@pickypg The part I don't understand is why the unpacking step requires so much debugging?  Surely that's the easy bit.  A developer will typically work with an unpacked plugin, no?
</comment><comment author="pickypg" created="2014-12-31T17:36:29Z" id="68456693">True, I guess when I wrote the comment that I was thinking of autogenerated bundles failing. Since then, I think I've realized how unlikely that is. I will just got ahead and close this one. We can open it if it is ever an actual issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Fix for benchmark test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6089</link><project id="" key="" /><description>- Fix bug where repeatedly calling computeSummaryStatistics() could
  accumulate some values incorrectly.
- Fix check for number of responsive nodes on list is &lt;= number of
  candidate benchmark nodes.
</description><key id="33040052">6089</key><summary>Benchmark: Fix for benchmark test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>blocker</label><label>test</label></labels><created>2014-05-08T00:35:13Z</created><updated>2015-03-19T20:06:02Z</updated><resolved>2014-05-08T01:49:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-08T00:46:49Z" id="42502232">LGTM, looking at the `CompetitionSummary`, I think we should go with getters for the relevant variables (and move them back to private).
</comment><comment author="kimchy" created="2014-05-08T01:39:50Z" id="42505095">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The plugin command should reveal plugin versions when listing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6088</link><project id="" key="" /><description>I want to see which versions of plugins are installed not just which plugins are installed.

Is there a way to have an internal version number provided to elasticsearch by the plugin, exposed to the plugin command?

Would be handy for installing a particular version of a plugin rather than having to specify different plugin install location urls for different versions as is the case now.

This would also need some central plugin repository system I guess, which is currently just github?
</description><key id="33035192">6088</key><summary>The plugin command should reveal plugin versions when listing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2014-05-07T23:37:52Z</created><updated>2014-05-20T23:10:24Z</updated><resolved>2014-05-20T23:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-08T12:53:28Z" id="42545998">You can do it on a running cluster with [cat plugin](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-plugins.html) feature.

&gt; Would be handy for installing a particular version of a plugin rather than having to specify different plugin install location urls for different versions as is the case now.

What do you mean? You can by now install plugins using for example:

```
bin/plugin -install fr.pilato.elasticsearch.river/fsriver/1.0.0
```

Just change version number if you need another version. Are you looking for something else?
</comment><comment author="geekpete" created="2014-05-20T23:10:24Z" id="43694905">Yeah I guess that works fine. Just means you need a http hit to reveal version then plugin command to install/upgrade a plugin to a certain version.

I was just comparing it to (slightly) more mature package/installers like python's pip tool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs suggest downloading GPG key over HTTP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6087</link><project id="" key="" /><description>tl;dr: please check the GPG key for downloading apt packages into the main elasticsearch repo, alongside the documentation on repositories.

The docs (at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html ) suggest the following command to install the public GPG key for elasticsearch .deb packages:

```
wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
```

This is insecure; the http transport is not valiated in any way, and the output from it is automatically trusted.  A man in the middle could use this to add his own GPG key instead of the official one, and would then be able to add arbitrary code to the packages subsequently downloaded, and have it trusted by the user's machine.  You shouldn't be suggesting that users run this command.

In the absence of moving to a key signing web-of-trust or similar, please make there be some way of obtaining the GPG key over HTTPS or some other relatively secure pathway.

Ideally, you'd do this by making both the location the GPG key is downloaded from, and the documentation saying how to download it, be served over HTTPS (if you don't make the page with the instructions be served over HTTPS, an attacker can just change the instructions).  I suspect this is awkward to do, because you're serving the documentation directly from S3.  Therefore, a simpler solution would be to check the GPG key into the main repository: that way, users can browse the repository over github's HTTPS connections, and download the key over the same HTTPS connections.
</description><key id="33033749">6087</key><summary>Docs suggest downloading GPG key over HTTP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels><label>:Packaging</label><label>adoptme</label></labels><created>2014-05-07T23:15:18Z</created><updated>2014-12-30T18:18:29Z</updated><resolved>2014-12-30T18:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rcsheets" created="2014-10-27T08:38:58Z" id="60562424">The source for the documentation seems to be https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/setup/repositories.asciidoc

The documentation fix is trivial, but someone with knowledge of the valid GPG key would first need to arrange for the key to be added to the repo on GitHub, or make it accessible from some other verifiable location as suggested by @rboulton.
</comment><comment author="clintongormley" created="2014-10-27T09:37:45Z" id="60568030">@jeff4 @drewr Do we have plans to add https support to our download service?
</comment><comment author="rcsheets" created="2014-11-24T18:45:18Z" id="64242308">Even just getting the proper GPG public key into the git repo would provide a reasonable workaround. Surely that's an easy thing for someone with commit access and a verified copy of the public key.
</comment><comment author="grue" created="2014-11-29T02:45:03Z" id="64939362">+1 for this.
</comment><comment author="laughingdog" created="2014-11-29T18:49:57Z" id="64961373">+1 here as well
</comment><comment author="nik9000" created="2014-11-29T19:34:43Z" id="64962637">+1
</comment><comment author="clintongormley" created="2014-12-30T18:18:29Z" id="68381481">This has been fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add /_cat/fielddata to display fielddata usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6086</link><project id="" key="" /><description>Currently has a disabled REST test, I'm not yet sure why the regular expression doesn't match.

The output is a little different because it has a dynamic number of columns depending on the number of fields, so if anyone has an alternative output format suggestion that would be great.

Closes #4593
</description><key id="33027203">6086</key><summary>Add /_cat/fielddata to display fielddata usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:CAT API</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-07T21:36:52Z</created><updated>2015-06-06T18:32:16Z</updated><resolved>2014-05-09T11:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-07T23:27:43Z" id="42495903">the code looks good, yea, this is the first time we have dynamic headers, but I think it makes sense in this context, maybe @clintongormley can chip in on the test / the fact that the headers are dynamic.
</comment><comment author="javanna" created="2014-05-08T15:00:50Z" id="42560487">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats API should report on index throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6085</link><project id="" key="" /><description>In #6066 we are adding index throttling when merges are falling behind, and we log.info when that happens, but we should also gather the stats showing how many times index throttling kicked in.
</description><key id="33024581">6085</key><summary>Stats API should report on index throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-05-07T21:05:07Z</created><updated>2015-03-19T15:24:04Z</updated><resolved>2014-10-02T10:23:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-10-02T10:23:20Z" id="57609642">Dup of #7861
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query string query with wildcards fails to return any results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6084</link><project id="" key="" /><description>I believe there may be an issue in handling query string queries with wildcards.  Or maybe I am misunderstanding query syntax.  Consider this test case (v1.1.1, clean install, no modifications):

First, let's create an empty index as usual:

```
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test
```

In my case, I am interesting in searching sequences of arbitrary text characters by using trailing wildcards.  I don't want any analysis, I just want to be able to query for an arbitrary sequence of characters and have the engine return docs that start with that sequence.  So I am using a "not_analyzed" field, and I want to disable all tokenization at search time by using "keyword analyzer":

```
curl -XPUT localhost:9200/test/doc/_mapping -d '{
  "doc" : {
    "properties" : {
      "token" : {
        "type" : "string",
        "index" : "not_analyzed",
        "search_analyzer" : "keyword"
      }
    }
  }
}'
```

Let's now insert a test doc:

```
curl -XPOST localhost:9200/test/doc -d '{
  "token": "/abc.123.SomeMoreText"
}'
```

Trying a query string with a trailing wildcard, everything seems to be working as expected:

```
curl -XGET localhost:9200/test/_search -d '{
  "query": {
    "query_string": {
      "default_field": "doc.token",
      "query": "token:\\/abc.123.*"
    }
  }
}'

{"took":4,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test","_type":"doc","_id":"onIsyEjiTRWkudeCMHgC0w","_score":1.0, "_source" : {
  "token": "/abc.123.SomeMoreText"
}}]}}
```

But if I add even one extra character, the query will not return anything (POSSIBLE BUG):

```
curl -XGET localhost:9200/test/_search -d '{
  "query": {
    "query_string": {
      "default_field": "doc.token",
      "query": "token:\\/abc.123.S*"
    }
  }
}'

{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

I cannot explain this behavior.  I was supposed to escape any "regex" mode by putting backslashes in front of /; there are no exceptions thrown either, just no results returned.  There are no other special characters in play, according to the list from http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html .  So this looks like a bug.
</description><key id="33014021">6084</key><summary>query string query with wildcards fails to return any results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dshelepov</reporter><labels /><created>2014-05-07T18:58:33Z</created><updated>2014-05-08T19:01:49Z</updated><resolved>2014-05-08T09:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-08T09:40:10Z" id="42530429">Hi @dshelepov 

No, this isn't a bug.  The `query_string` query is a complex beast which I would avoid if at all possible.  You can see the problem if you run your query through the `validate-query` API:

```
GET /test/_validate/query?explain
{
  "query": {
    "query_string": {
      "default_field": "doc.token",
      "query": "token:\\/abc.123.S*"
    }
  }
}
```

This returns:

```
"explanation": "token:/abc.123.s*"
```

You see how the `S` has been lowercased? If you change the query as follows, it works:

```
GET /test/_search
{
  "query": {
    "query_string": {
      "default_field": "doc.token",
      "query": "token:\\/abc.123.S*",
      "lowercase_expanded_terms": false
    }
  }
}
```

But if you don't want analysis, why not just use the term-level `wildcard` query instead:

```
GET /test/_search
{
  "query": {
    "wildcard": {
      "token": "/abc.123.S*"
    }
  }
}
```

Also, wildcard queries are not terribly efficient.  Have a read of this chapter for other approaches: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/partial-matching.html
</comment><comment author="dshelepov" created="2014-05-08T18:04:26Z" id="42583826">Ok, that makes sense, thanks.

I cannot switch to a different query type easily, because I use Kibana as a frontend, but your suggestions gave me a few extra options to consider. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange appearance of dynamic field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6083</link><project id="" key="" /><description>Last month in my elasticsearch index appeared weird dynamic mapping http://pastebin.com/dykbXEJy. Is seems as if this mapping would create itself...

What might be the reason for this? Can I remove those dynamic mappings? 

I use version: 0.20.4
</description><key id="33013803">6083</key><summary>Strange appearance of dynamic field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2014-05-07T18:55:55Z</created><updated>2014-05-08T09:26:21Z</updated><resolved>2014-05-08T09:26:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-08T09:26:21Z" id="42529328">Hi @Rzulf 

Please ask these questions in the forum.  The issues list is for bugs or feature requests in Elasticsearch, rather than to answer questions about how to use it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update fielddata.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6082</link><project id="" key="" /><description>Spelling correction and corrected breaker limit default value.
</description><key id="33012934">6082</key><summary>[DOCS] Update fielddata.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">brusic</reporter><labels><label>docs</label></labels><created>2014-05-07T18:45:36Z</created><updated>2014-07-16T21:45:28Z</updated><resolved>2014-05-08T08:59:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-08T08:59:41Z" id="42527138">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should we lower the default merge IO throttle rate?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6081</link><project id="" key="" /><description>In #6018 we are fixing a serious bug in the IO RateLimiting API we use from Lucene ... causing IO throttling to be effectively 8 MB/sec (way too low).

So, should we now bring the merge throttling back down from the current 50 MB/sec given that we previously increased it from 20 MB/sec not knowing about this issue?

The problem is, even at 30 MB/sec rate limiting, with #6018 fixed, I can quite easily get merges to fall behind with 4 indexing threads.

But then in that same case, when testing with #6066 the merges are able to keep up, but at the cost of periodically making indexing single threaded.
</description><key id="33011219">6081</key><summary>Should we lower the default merge IO throttle rate?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label></labels><created>2014-05-07T18:25:38Z</created><updated>2015-06-08T15:03:16Z</updated><resolved>2014-05-12T18:50:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-07T19:14:52Z" id="42470470">Right now it'd be a 625% increase instead of the planned 250% increase.  I think that big of a bump might be too much.  I guess it depends who the defaults are targeted for.  I imagine folks with lots of spindles and/or ssds care enough to tweak the defaults.  That leaves cloud folks and those of us with physical hardware but few spindles.

For what it is worth I use two standard small hard drives on each of my nodes with software striping raid and iostat reports 10% utilization when they write 10MB/s each.
</comment><comment author="rmuir" created="2014-05-07T21:15:07Z" id="42484078">I don't understand the logic at this point behind throttling merges at all.

With #6066 the term 'Throttling merges' really just implies 'Throttling indexing'.

So why rate-limit indexing to some arbitrary bytes/second? Surely not all users want to make this throughput/latency tradeoff just to tune Pnn search metrics, e.g. some may be doing high ingest and things like analytics instead that are less sensitive to small latency changes in search.

Why rate-limit indexing more than its already been rate-limited? We are discussing adjusting the dosage for a patient that may no longer need the medicine: perhaps #5882 was the cure.

Why rate-limit indexing via this indirect, confusing path (via merging). Why not just have the option to rate-limit indexing directly for users that want this?

Why rate-limit to bytes/second anyway? This is a bad measure, it doesn't adapt to different use cases or hardware.

It seems pretty expert to make this tradeoff, so maybe it should just be disabled by default?
</comment><comment author="clintongormley" created="2014-05-08T09:45:40Z" id="42530854">@rmuir The original idea behind merge throttling was to prevent a 5GB merge (which could be sparked off by a single indexing request) from consuming all resources and killing search, and perhaps even making the box unresponsive.  This was a scenario I saw regularly in production before merge throttling was introduced.
</comment><comment author="rmuir" created="2014-05-08T09:49:27Z" id="42531136">Then maybe searches should be throttled too? After all a badly formed wildcard or rangequery can easily do the same thing.
</comment><comment author="clintongormley" created="2014-05-08T09:54:09Z" id="42531611">Searches are something that the user has control over. Merges are not. Also, there's PR #4586 which wouldn't throttle searches, but would allow the user to timeout long running searches
</comment><comment author="rmuir" created="2014-05-08T09:57:58Z" id="42532238">The merge policy can be controlled too. Like i said, the math here behind throttling by default to an arbitrary bytes/second makes little sense. 

Sure for some specific hardware or specific use case, maybe it can make things better, but by default for everyone?

How can you sure it was a single merge causing the issue for you and not #5882 ? Maybe your hardware just did not have enough i/o concurrency.
</comment><comment author="clintongormley" created="2014-05-08T10:06:40Z" id="42533036">&gt; How can you sure it was a single merge causing the issue for you and not #5882 ? 

I had this problem several times and usually the stats output showed only one merge, but this was a long time ago, so no I can't be sure.

&gt; Maybe your hardware just did not have enough i/o concurrency.

Oh for sure it didn't!  But I wasn't running on terrible boxes, in fact they were the biggest boxes I had, and certainly bigger than lots of our users are using now.

&gt; Sure for some specific hardware or specific use case, maybe it can make things better, but by default for everyone?

Trade offs.  For people with fast disks, setting merge throttling too low gives them slow performance.  For people with slow disks, setting merge throttling too high can cause nodes to fall out of the cluster.  I'd prefer the defaults to minimise cluster instability.  If you see slow throughput, it's easy enough to change the throttling. If your node is unstable, it can be more difficult to diagnose.

Clearly there is no single default that will work for everybody.  We need to provide good example settings for different hardware setups.  Even better would be some stress test tool which would make recommendations based on the user's actual hardware.
</comment><comment author="rmuir" created="2014-05-08T10:12:51Z" id="42533484">&gt; Clearly there is no single default that will work for everybody. We need to provide good example settings for different hardware setups. Even better would be some stress test tool which would make recommendations based on the user's actual hardware.

Sure, but why do it in an indirect way (via merging), instead of directly rate-limiting all writes? This is easier for the user to understand, and its whats implied by #6066
</comment><comment author="clintongormley" created="2014-05-08T10:17:02Z" id="42534064">Fair enough
</comment><comment author="kimchy" created="2014-05-08T11:54:35Z" id="42541181">remember also that merge scheduler and policy are _per_ Lucene index, and in ES, we can have several shards on a node, and merge throttling is the only way we have today to control it across all those shards.

Writes are "rate" limited already, btw, because we have a bounded thread pool that defaults to the number of cores, you can't have more threads than that. Rate limiting on number of operations feels weird, it needs to happen due to backpressure building.
</comment><comment author="rmuir" created="2014-05-08T12:23:06Z" id="42543370">Search performance is also impacted by the number of segments... today this throttling (combined with disabling lucene's safety) causes segment explosion, so its probably having the exact opposite impact.

Keeping the index in a healthy state will also naturally improve search performance! These are the reasons why I question if anything should happen here by default.
</comment><comment author="rmuir" created="2014-05-08T12:36:00Z" id="42544428">&gt; Writes are "rate" limited already, btw, because we have a bounded thread pool that defaults to the number of cores

Yes, thats nice because it adapts to the concurrency of the hardware of the machine and how fast it can process requests. MB/S does not.
</comment><comment author="s1monw" created="2014-05-08T12:42:26Z" id="42544997">&gt; So why rate-limit indexing to some arbitrary bytes/second? Surely not all users want to make this throughput/latency tradeoff just to tune Pnn search metrics, e.g. some may be doing high ingest and things like analytics instead that are less sensitive to small latency changes in search.

the numbers were not set arbitrarily - I did some measurements in a incremental indexing scenario on a real world search index (~1k docs / min update rate) and the 20mb at that point were the setting  that removed the spikes from the response rate. It was targeted to let housekeeping do it's work but don't go crazy with merging even not for a short time at the cost of having more segments. Given that the intention was to let searches go wild and reduce IO consumption for "segment housekeeping" I think it's a valid default? I mean I'd not recommend it if you do bulk indexing while not searching at all which is IMO not the main usecase of ES? 
</comment><comment author="rmuir" created="2014-05-08T12:57:55Z" id="42546398">&gt; the numbers were not set arbitrarily - I did some measurements in a incremental indexing scenario on a real world search index (~1k docs / min update rate) and the 20mb at that point were the setting that removed the spikes from the response rate. It was targeted to let housekeeping do it's work but don't go crazy with merging even not for a short time at the cost of having more segments. Given that the intention was to let searches go wild and reduce IO consumption for "segment housekeeping" I think it's a valid default?

If its a valid default then what was #5902 :)

I think having an arbitrary number will be tricky over time and hard to tune. It would be great if the default was a little smarter and worked better out of box for different configurations.
</comment><comment author="mikemccand" created="2014-05-12T17:07:34Z" id="42859405">We agreed to just keep this at 20 MB/sec default for now.

Longer term we can explore other ideas, e.g. if we can somehow ask the OS to do "ionice" for us that would really be best.  Failing that, maybe we tie the merge throttle rate to the recent indexing rate, so that if you are indexing at 20 MB/sec maybe you get allowance for 40 MB/sec merges or something.  This way apps doing bulk indexing will let their merges keep up, but apps doing "trickle" indexing (just to push updates into the index) and more searching, will lower the merge throttle.  But we can explore this later.
</comment><comment author="otisg" created="2014-05-12T17:19:40Z" id="42860932">@mikemccand - something from the past: http://search-lucene.com/m/D7ypz1gT2H91
</comment><comment author="mikemccand" created="2014-05-12T18:18:34Z" id="42868084">@otisg You should go build those changes :)
</comment><comment author="mikemccand" created="2014-05-12T18:20:23Z" id="42868303">Er, not closed yet ... gotta actually put the default back to 20 MB/sec.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documenting the illusion of choice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6080</link><project id="" key="" /><description>The user only has the illusion of choice on long vs. int when using sort. (They must use a long)
</description><key id="33000811">6080</key><summary>Documenting the illusion of choice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brianhorakh</reporter><labels /><created>2014-05-07T16:20:35Z</created><updated>2014-06-18T04:36:32Z</updated><resolved>2014-05-08T09:18:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-08T09:18:26Z" id="42528683">Sorry @brianhorakh but this is incorrect.

It is perfectly OK to sort on integers.  I think the problem that you have run into is having a field with the same name in two different types, with two different mappings.  When you try to sort on the integer, it loads all values for that field, across both types, which causes the conflict.
</comment><comment author="benjismith" created="2014-05-08T17:39:00Z" id="42580761">FYI, @brianhorakh , take a look at [Issue #5851, Prepend the type name to the index_name automatically](https://github.com/elasticsearch/elasticsearch/issues/5851) and add your vote for fixing this issue.

In my experience, most new ES users think of the Elasticsearch INDEX &amp; TYPE system as being analogous to the DATABASE &amp; TABLE system in a RDBMS, where each table has its own independent storage, with support for independent sorting, filtering, and faceting.

But it's not quite like that. In Elasticsearch, all of the types in an index are stored together, using a consolidated set of fields (defined by their local names, not their fully-qualified names). The type-mapping is basically a validator to help Elasticsearch enforce type-conformance before storing the document into a (basically) schemaless storage engine.

As you discovered, you can't use that consolidated storage to index heterogeneous data types (INT vs LONG), but you also can't independently filter or facet upon those field values. The mechanism makes sense, given the history of Elasticsearch, but it's considerably different than the RDBMS way of doing things, so it can be a source of hidden bugs for people who don't understand the different mechanism.

Anyhow, to eliminate that common source of confusion, I'd like to see the default behavior changed so that ES always uses fully-qualified-names to disambiguate between different fields at the storage layer. Add your comments to #5851, and hopefully we can help iron out a good solution to this problem :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed the respnose structure of the percentiles aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6079</link><project id="" key="" /><description>now all the percentiles are placed under a `values` object (or `values` array in case the `keyed` flag is set to `false`)

Closes #5870
</description><key id="32997553">6079</key><summary>Changed the respnose structure of the percentiles aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-07T15:45:23Z</created><updated>2015-08-13T15:10:23Z</updated><resolved>2014-05-09T06:37:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-07T15:51:18Z" id="42444941">LGTM, can we mark the issue with breaking and have good description for it?
</comment><comment author="jpountz" created="2014-05-07T15:55:01Z" id="42445454">LGTM

Good that this agg was marked as experimental.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Fix the other default values for filter cache size and field data ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6078</link><project id="" key="" /><description>...circuit

Relates to #5990
</description><key id="32997244">6078</key><summary>[DOC] Fix the other default values for filter cache size and field data ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">neilschelly</reporter><labels /><created>2014-05-07T15:42:05Z</created><updated>2014-06-18T19:21:55Z</updated><resolved>2014-05-08T09:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-08T09:01:05Z" id="42527242">Thanks for your PR, same change was just merged as part of #6082 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow binary sort values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6077</link><project id="" key="" /><description>In order to support the new ICUCollationKeyAnalyzer https://github.com/elasticsearch/elasticsearch-analysis-icu/issues/28 we need to be able to handle binary values as sort keys.  The problem comes when serializing the binary keys into JSON.

Possibly we can detect whether the analyzer creates binary token streams.

An alternative would be to add a dedicated `icu_collation_key` field type: https://github.com/elasticsearch/elasticsearch-analysis-icu/issues/29
</description><key id="32997213">6077</key><summary>Allow binary sort values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2014-05-07T15:41:50Z</created><updated>2016-08-26T13:17:21Z</updated><resolved>2016-05-06T07:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-08-27T15:37:34Z" id="135471648">See also #13148
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>normalize/unit on geodistance filter are undocumented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6076</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-geo-distance-filter.html#query-dsl-geo-distance-filter

Makes no mention of these parameters (`normalize` and `unit`) but the geo distance parser still checks for them:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java#L113

NEST relies on `unit` existing and i noticed [es-js also has these properties mapped](http://docs.fullscale.co/elasticjs/ejs.GeoDistanceFilter.html#).
</description><key id="32983628">6076</key><summary>normalize/unit on geodistance filter are undocumented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Geo</label></labels><created>2014-05-07T13:13:55Z</created><updated>2014-12-30T18:16:52Z</updated><resolved>2014-12-30T18:16:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-08T09:08:16Z" id="42527836">Hmm - geopoints should always be normalized.  I think the option was added for bwc when geopoint normalization was added, but is no longer required.

Also, the units are typically specified as part of the `distance`, eg `5mi`.  Not sure we should complicate things by documenting all possible ways of specifying this?
</comment><comment author="Mpdreamz" created="2014-05-08T12:24:46Z" id="42543520">From a typed standpoint I would love it if `distance` would be just a string. 

Right now both of these are valid according to the parser 

```
distance: "5mi"
```

as well as 

```
distance: 5,
unit: "km"
```

Right now to support both I have to box `distance` as `object`. 
</comment><comment author="clintongormley" created="2014-05-08T12:29:44Z" id="42543907">well that ain't gonna change ;)

of course, you could just support the second version in Nest if you wanted to
</comment><comment author="clintongormley" created="2014-12-30T18:16:52Z" id="68381335">Not an issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update keyword-tokenizer.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6075</link><project id="" key="" /><description /><key id="32982648">6075</key><summary>Update keyword-tokenizer.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2014-05-07T13:02:17Z</created><updated>2014-07-16T21:45:30Z</updated><resolved>2014-05-07T13:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-07T13:04:39Z" id="42423797">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add field data for boolean fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6074</link><project id="" key="" /><description>This will ensure that facets and aggregations display them as true/false
instead of T/F.

Close #4678
</description><key id="32980350">6074</key><summary>Add field data for boolean fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-07T12:29:42Z</created><updated>2014-06-26T19:07:48Z</updated><resolved>2014-05-07T13:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T13:32:49Z" id="42426820">Closed: It would be better to have field data for native booleans.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a dedicated field data type for the _index field mapper.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6073</link><project id="" key="" /><description>This makes aggregations work on the _index field, and also allows to remove the
special facet aggregator for the _index field.

Close #5848
</description><key id="32978899">6073</key><summary>Add a dedicated field data type for the _index field mapper.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-07T12:07:11Z</created><updated>2015-06-07T20:46:30Z</updated><resolved>2014-05-07T12:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-07T12:24:34Z" id="42420166">LGTM, nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force awarness shards wrong assignment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6072</link><project id="" key="" /><description>We try to index large database data in our company and we encouter some reliability problems with elasticsearch 1.1.0 and 1.1.1. When we use force awarness option elasticsearch has problems with proper shards allocation and we loose access to some or whole index data.

Below steps to reproduce bug:
1. Prepare two separate linux machines called DC2 and DC4 connected via LAN.
2. On machine DC2 unzip 4 separate instances of elasticsearch 1.1.1 with following configuration:
   
   elastic 1:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC2
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC2_1
   
   elastic 2:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC2
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC2_2
   
   elastic 3:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC2
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC2_3
   
   elastic 4:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC2
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC2_4
3. On machine DC4 unzip 4 separate instances of elasticsearch 1.1.1 with following configuration:
   
   elastic 1:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC4
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC4_1
   
   elastic 2:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC4
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC4_2
   
   elastic 3:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC4
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC4_3
   
   elastic 4:
   cluster.routing.allocation.awareness.force.zone.values: DC2,DC4
   node.zone: DC4
   cluster.routing.allocation.awareness.attributes: zone
   node.name: DC4_4
4. Start all elastics on DC2. 4-node cluster forms properly.
   ![01_empty_dc2_start](https://cloud.githubusercontent.com/assets/1789867/2899770/59ce842c-d5bc-11e3-8ac2-94666fcdd1c5.png)
5. Start all elastics on DC4. 8-node cluster forms properly.
   ![02_empty_dc2_start](https://cloud.githubusercontent.com/assets/1789867/2899787/ac919bae-d5bc-11e3-8d57-970e4149e067.png)
6. Create new empty index (5 shards, 3 replicas). Shards are spread properly across nodes.
   ![03_dc2_up_dc4_up_create_index](https://cloud.githubusercontent.com/assets/1789867/2899788/b2dc9982-d5bc-11e3-87cf-ce19791a2a14.png)
7. Shutdown all elastics on DC4.
   PROBLEM: All shards are assigned to all elastics on DC2. According to force awarness configuration only half of them should be assigned. Shards from DC4 should be unassigned.
   ![04_shutdown_dc4_wrong_assignment](https://cloud.githubusercontent.com/assets/1789867/2899793/c49d0044-d5bc-11e3-8396-ac09a89451c7.png)
8. Shutdown all elastics on DC2.
9. Start all elastics on DC2.
   PROBLEM: All shards are assigned to all elastics on DC2. According to force awarness configuration only half of them should be assigned. Shards from DC4 should be unassigned.
   ![05_start_only_dc2_wrong_assignment](https://cloud.githubusercontent.com/assets/1789867/2899802/d9a37bda-d5bc-11e3-98e7-fb53e22ff9ad.png)
10. Shutdown all elastics on DC2.
11. Start all elastics on DC4.
    PROBLEM: All shards are unassigned. According to force awarness configuration half of them should be assigned. Shards from DC2 should be unassigned.
    ![06_start_only_dc4_wrong_assignment](https://cloud.githubusercontent.com/assets/1789867/2899804/e64738d6-d5bc-11e3-8031-e5b2f2f0a8e7.png)
12. Start all elastics on DC2 - all shards are assigned properly.

Problems in steps 7-11 occure all the time in this index. Sometimes (other but similar index) all shards are unassgined in steps 7 and 9 (just like in step 11).

When shards cannot be assign following lines in log occures:
[2014-05-07 08:21:23,787][DEBUG][gateway.local            ] [DC4_1] [some_index][3]: not allocating, number_of_allocated_shards_found [1], required_number [3]
[2014-05-07 08:21:23,787][DEBUG][gateway.local            ] [DC4_1] [some_index][0]: not allocating, number_of_allocated_shards_found [1], required_number [3]
[2014-05-07 08:21:23,787][DEBUG][gateway.local            ] [DC4_1] [some_index][1]: not allocating, number_of_allocated_shards_found [0], required_number [3]
[2014-05-07 08:21:23,787][DEBUG][gateway.local            ] [DC4_1] [some_index][4]: not allocating, number_of_allocated_shards_found [0], required_number [3]
[2014-05-07 08:21:23,787][DEBUG][gateway.local            ] [DC4_1] [some_index][2]: not allocating, number_of_allocated_shards_found [1], required_number [3]
</description><key id="32963074">6072</key><summary>Force awarness shards wrong assignment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yu55</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label></labels><created>2014-05-07T07:58:59Z</created><updated>2016-11-25T17:48:17Z</updated><resolved>2016-11-25T17:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:15:23Z" id="68381170">Hi @yu55 

Sorry it has taken so long to look at this.  In recent versions, shutting down all nodes in DC4 only assigns half the replicas to DC2, so this is now correct.

However, shutting down DC2 and DC4, then starting up DC4 still results in no shards being assigned...
</comment><comment author="clintongormley" created="2016-11-25T17:47:55Z" id="263004169">As of 5.0, all of these scenarios now work correctly</comment><comment author="clintongormley" created="2016-11-25T17:48:17Z" id="263004209">thanks @yu55 for the very thorough recreation</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix _cat/allocation rest test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6071</link><project id="" key="" /><description>The rest test for _cat/allocation was failing due to a regular
expression not accounting for space-padded right-justified text.
</description><key id="32954770">6071</key><summary>Fix _cat/allocation rest test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-07T04:31:21Z</created><updated>2015-06-07T11:46:16Z</updated><resolved>2014-05-07T20:17:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T05:57:02Z" id="42392172">LGTM
</comment><comment author="clintongormley" created="2014-05-07T09:14:50Z" id="42405317">Hi @aleph-zero 

The only time the first number is right justified is when there is a column header, no?  Otherwise it is left justified I think?

Btw, the reason the JVM is running out of stack space is because this is a pathological regex.  You have:

```
             (\d+(\.\d+)?[kmgt]b)?  \s+  #no value from client nodes
             (\d+(\.\d+)?[kmgt]b)?  \s+  #no value from client nodes
```

In other words, there may be a number,  followed by 1 or more spaces, maybe followed by a number, followed by 1 or more spaces.  But if there are no numbers, then the `\s+` eats up all the spaces, then it tries to match on `\s+` again.  That means it need to backtrack into the previous `\s+` again, and it all start going wrong...

Actually, you're only looking for trailing spaces IF THERE IS A NUMBER, so put the `\s+` into the conditional group `(... \s)?`, as follows:

```
        /^
          ( 0                      \s+
            \d+(\.\d+)?[kmgt]?b    \s+
            (\d+(\.\d+)?[kmgt]b    \s+)?  #no value from client nodes
            (\d+(\.\d+)?[kmgt]b    \s+)?  #no value from client nodes
            (\d+                   \s+)?  #no value from client nodes
            [-\w.]+                \s+
            \d+(\.\d+){3}          \s+
            \w.*
            \n
          )+
        $/
```
</comment><comment author="aleph-zero" created="2014-05-07T17:06:32Z" id="42454484">Hey @clintongormley 

I incorporated your suggestions into the PR. Would you take a look?  
</comment><comment author="clintongormley" created="2014-05-07T19:04:30Z" id="42469167">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for build failure #847</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6070</link><project id="" key="" /><description>This fixes a stack overflow in the test for the _cat/recovery API.
The regular expression that tests the response body was modified to
handle large responses properly.
</description><key id="32946112">6070</key><summary>Fix for build failure #847</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-07T00:34:27Z</created><updated>2014-10-21T23:27:22Z</updated><resolved>2014-05-07T21:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T06:30:08Z" id="42393743">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make cluster recovery near instantaneous if all shards are present and accounted for</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6069</link><project id="" key="" /><description>When restarting a cluster from green state, each shard appears to undergo some form of checksum to verify it before bringing it online.

Is there a way to journal writes so that recovery is much much faster, in the way that the xfs filesystem does it.

Only review the data that was being written to at the time of the outage or shutdown so that only the in-progress write data needs to be checked.

For a clean shutdown, maybe a complete cluster restart command could tell all nodes to shutdown in a clean state then turn off, allowing a near instantaneous recovery on startup. Like stop allocation then flush all translogs, then shut down,etc.

Would just take lots of the pain out of cluster restarts.

Just an idea
</description><key id="32944579">6069</key><summary>Make cluster recovery near instantaneous if all shards are present and accounted for</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">geekpete</reporter><labels /><created>2014-05-06T23:58:36Z</created><updated>2015-05-31T11:08:20Z</updated><resolved>2015-05-31T11:08:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-17T18:38:10Z" id="49346459">@javanna, when I met you in Germany we talked about doing _something_ about the slow recovery times.  I'm wondering if there is anything I can do to help with that.
</comment><comment author="s1monw" created="2014-07-17T19:12:07Z" id="49350687">@nik9000 we have improvements in the pipeline for this. I can't promise when we will start working on them or when they will land but what we essentially plan is to work out algorithmic parts to reduce the risk of full recovery from the primary shard even if they out of sync just a hand full of documents. I will try to update this issue once I have news about it. Thanks for pinging
</comment><comment author="nik9000" created="2014-07-17T19:26:55Z" id="49352435">Thanks for the reply!  I'm happy to help work on it but I imagine it'd be faster to just have someone familiar with your ideas do it.
</comment><comment author="nik9000" created="2014-07-29T20:25:43Z" id="50533219">I'm feeling this pain today again while I do a rolling restart to pick up a plugin.  And in two weeks when I'll be ready to upgrade to 1.3.1.  Because the restart process is so slow I try to batch things that get picked up by the restart but that isn't really good from a "change one thing at a time" perspective.....
</comment><comment author="nik9000" created="2014-08-23T00:33:06Z" id="53137263">I figure I should poke this issue every time I do a full day cluster restart.  Poke.  I'm happy to work on this if someone who has thought more about this can share.  At this point I figure sinking a couple weeks into speeding up cluster restarts will save me time in the long run.
</comment><comment author="clintongormley" created="2014-08-23T11:49:33Z" id="53150664">Hi @nik9000. This improvement depends on the addition of "sequence numbers" (a feature that will enable a number of other improvements).  We are currently experimenting with various approaches but rest assured, this issue is not being ignored.
</comment><comment author="nik9000" created="2014-11-12T14:30:15Z" id="62725376">@clintongormley since you poked me last night about outstanding work I planned to do, can I poke this one?  I'd live to have this.  In the middle of a cluster rolling restart that is taking two days.....  Its thankfully quite boring but still requires some degree of babysitting.
</comment><comment author="clintongormley" created="2014-11-12T14:33:53Z" id="62725909">@nik9000 we are working on the design for this one. it is in the top 3 on our list, but obviously complicated and not guaranteed.  We'll update the issue as soon as we have more news.
</comment><comment author="geekpete" created="2014-11-20T01:02:23Z" id="63744401">This near instantaneous recovery idea might also be applied to when a node drops out of the cluster but rejoins with all its data on disk still intact. 

Instead of throwing all that data away, if it could be salvaged in some efficient manner so that only the outdated differences need to be transmitted for storage, this would save quite a lot of data transfer on large clusters. Would make failures recoverable in much shorter time periods.
</comment><comment author="bleskes" created="2014-11-20T08:59:06Z" id="63778292">@geekpete yeah - the plan is to help with that as well - at least when the down time is planned. When it isn't planned things get slightly trickier as ES will start replicating as a soon as a node goes down - this is no way for it to know how long the down time will take.
</comment><comment author="bobrik" created="2014-12-01T15:20:22Z" id="65079197">This is probably related to #8725
</comment><comment author="connieyang" created="2015-01-20T18:49:18Z" id="70709111">We have an Elasticsearch cluster (as part of our ELK stack) in production and have experienced (6+ hours) for the cluster to turn green during a rolling restart.  When will a fix for this be ready?
</comment><comment author="bleskes" created="2015-01-20T22:02:15Z" id="70743236">@connieyang sorry to hear the pain. We are actively working on it. Sadly I can't promise any ETA at the moment.   
</comment><comment author="cfeio" created="2015-02-17T20:58:36Z" id="74751829">+1, We are also experiencing pains with cluster recover due to a large cluster size (6 TB cluster). It is taking hours to recover even after a planned maintenance restart. This feature would be a huge improvement!
</comment><comment author="shyem" created="2015-02-19T18:04:47Z" id="75103440">+1, same here. With daily index, most of them are unchanged, yet it takes for ever to recover.
</comment><comment author="clintongormley" created="2015-05-31T11:08:18Z" id="107156515">Closed by #11336
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify IndicesOptions constants</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6068</link><project id="" key="" /><description>Renamed IndicesOptions#strict and IndicesOptions#lenient to make it clearer what they actually return, reused methods and introduced new one

Relates to #6059, where two new constants were introduced in IndicesOptions. There were already two constants there though, one of which we could have reused. This commit tries to unify them.
</description><key id="32910224">6068</key><summary>Unify IndicesOptions constants</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T16:42:03Z</created><updated>2015-06-06T16:58:44Z</updated><resolved>2014-05-07T15:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-07T15:41:50Z" id="42443656">Merged to master and 1.x
</comment><comment author="javanna" created="2014-05-12T10:07:59Z" id="42815418">This change is marked as breaking in master only for Java API users as it removes two public methods (`strict` and `lenient`) from `IndicesOptions`. Restored the methods as deprecated in 1.x: https://github.com/elasticsearch/elasticsearch/commit/c1a7f3cb7f444e3f51edf94407e88f546e12e900
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the ability to include the queried document for More Like This API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6067</link><project id="" key="" /><description>By default More Like This API excludes the queried document from the response.
However, when debugging or when comparing scores across different queries, it
could be useful to have the best possible matched hit. So this option lets users
explicitly specify the desired behavior.
</description><key id="32901082">6067</key><summary>Added the ability to include the queried document for More Like This API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T15:04:16Z</created><updated>2015-06-07T13:31:14Z</updated><resolved>2014-05-09T11:11:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T06:11:42Z" id="42392885">This looks good, I just left one comment about stream versioning.
</comment><comment author="jpountz" created="2014-05-09T06:39:20Z" id="42637702">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throttling incoming indexing when Lucene merges fall behind</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6066</link><project id="" key="" /><description>Lucene has low-level protection that blocks incoming segment-producing threads (indexing threads, NRT reopen threads, commit, etc.) when there are too many merges running.

But this is too harsh for Elasticsearch, so it's entirely disabled, but this means merges can fall far behind under heavy indexing, and this results in too many segments in the index, which causes all sorts of problems (slow version lookups, too much RAM, etc.).

So we need to do something "softer"; Simon has a good starting patch, which I tested and confirmed (after https://issues.apache.org/jira/browse/LUCENE-5644 is fixed) at least in one use-case that it prevents too many segments in the index:

Before Simon's + Lucene's fix: http://people.apache.org/~mikemccand/lucenebench/base.html

Same test with the fix: http://people.apache.org/~mikemccand/lucenebench/throttled.html

Segment counts stay essentially flat.

Here's Simon's prototype patch: https://github.com/s1monw/elasticsearch/commit/2de96f9176ee471d30fab7ab7ff40348b9a5bf57
</description><key id="32900627">6066</key><summary>Throttling incoming indexing when Lucene merges fall behind</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T14:59:31Z</created><updated>2015-10-26T21:10:50Z</updated><resolved>2014-05-19T18:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-06T15:10:50Z" id="42314954">It looks like Simon's prototype pauses the indexing thread if too many merges are in flight.  I'm not 100% clear on the code path that gets here.  Will that pause indexing or pause refreshing or both?  It'd be neat to slow down just the refreshing and let indexing be slowed down by the refresh backlog logic.  Or am I crazy?
</comment><comment author="s1monw" created="2014-05-06T19:25:30Z" id="42347228">@nik9000 internally the IndexWriter has several threads states (8 by default) that we index into. If we limit to a single threads we only use on of the states and make sure we max out the RAM buffer and write the least amount of segments. This means we 1. reduce the number of segments to merge and 2. make sure flushes are only done if really needed. I think we can't slow down refreshes otherwise folks will see odd results since you don't get new documents. You also want to refresh to publish merged segments to further reduce the number of segments. We will do the right thing and provide backpressure on indexing not on refresh. Hope that makes sense?
</comment><comment author="nik9000" created="2014-05-06T19:42:31Z" id="42349244">&gt; We will do the right thing and provide backpressure on indexing not on refresh. Hope that makes sense?

I'd honestly forgotten about flushes.  Its what I get for only playing on the other side.  Anyway, I'm happy so long as back pressure is provided on indexing.
</comment><comment author="mikemccand" created="2014-05-09T11:43:56Z" id="42657513">I tested the current throttling branch with the refresh=-1 case, and we have problems because the "abandoned" thread states will never flush until a full flush ... workaround is you must use a refresh to get them flushed.
</comment><comment author="mikemccand" created="2014-05-09T15:17:14Z" id="42677542">I'm inclined to simply document that index throttling won't kick in if you use SerialMergeScheduler.

SMS only allows one merge to run at a time, so apps that are doing heavy bulk indexing really should not be using it.
</comment><comment author="mikemccand" created="2014-05-13T10:32:48Z" id="42939629">OK I reviewed these changes with Simon.  We decided we don't need to add a separate "kill switch" for this because you can just set max_merge_count higher to avoid throttling.  But we also decided not to document this new setting on the index-modules-merges docs: it's a very advanced setting, and playing with it could easily mess up merges.
</comment><comment author="l15k4" created="2015-10-21T22:18:26Z" id="150040672">Guys I don't think this works as expected, I'm getting : 

```
now throttling indexing: numMergesInFlight=4, maxNumMerges=3
stop throttling indexing: numMergesInFlight=2, maxNumMerges=3
```

**5 times a second** right at the beginning of bulk indexing. I'm disabling throttling and refresh interval, I start with `optimized` index, waiting until segment merging finishes, but segment merging is still falling behind... Why is the indexing throttling starting and stopping so frequently?
</comment><comment author="clintongormley" created="2015-10-23T18:08:22Z" id="150650731">@l15k4 your merges are not keeping up.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-merge.html#scheduling
</comment><comment author="l15k4" created="2015-10-23T23:00:10Z" id="150711740">@clintongormley but they are not not keeping up right at the moment of starting indexing intto a a small (1M records) optimized index... increasing merge thread pool doesn't help...

I have 4 `ec2.xlarge` instances clustered with 1B records in 30 indices (5 shards each). And if I create a new index and start bulk index into it then throttling happens right away. All fields are doc_values and  I think that it happened right I after I reindexed everything to doc_values around 1.6.0 .... I cannot shake it off since then... I tried everything...

Imho I need to scale it up just because of segment merging, but there will be plenty of unused resources ...

I'm trying to solve this issue for months now...
</comment><comment author="mikemccand" created="2015-10-24T10:48:39Z" id="150783880">@l15k4 did you disable store IO throttling (defaults to 20 MB/sec, which is too low for heavy indexing cases).

Where are you storing the shards (what IO devices), EBS or local instance storage?

Also try the ideas here: https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing
</comment><comment author="l15k4" created="2015-10-25T14:12:10Z" id="150926771">@mikemccand I set it up to 30, 40, 80, 100 MB/s ... it had no effect. I also tried to set `index.merge.scheduler.max_thread_count: 6` but it lead to throttling `now throttling indexing: numMergesInFlight=9` so it didn't help either... 

We use EBS (General Purpose (SSD)) on `c4.xlarge` instances. 2 volumes, one for system and one dedicated for ES...

It seems that if you are doing bulk indexing and have all fields `doc_values` then you need either quad core machine or physically attached SSD or both, otherwise segment merging will always fall behind no matter what optimizations one does...

It always looks this way, it is throttling for some period of time like 15-20 minutes and then it stops  http://i.imgur.com/UyDTlHi.png 

I also tried to shrink `index` and `bulk` threadpools for segment merging to keep up with bulk indexing, but it didn't help either ... it doesn't keep up right when the first few bulk index requests come...
</comment><comment author="l15k4" created="2015-10-25T20:23:41Z" id="150965755">The best bulk indexing performance I can get on a machine with 4 hyper threads and EBS (750 Mbps) with all fields being doc_values is by increasing `index.merge.scheduler.max_thread_count` to 6 and decreasing `threadpool.bulk.size: 2`, this way it is throttling `now throttling indexing` like every 6th second but it is still throttling so the throughput is now http://i.imgur.com/wXCNZh7.png

I think that after doc_values people don't have much of a choice, they'll need physically attached SSD...
</comment><comment author="mikemccand" created="2015-10-26T01:07:06Z" id="150995627">Hmm enabling doc values is typically a minor indexing performance hit in my experience, e.g. see the nightly benchmarks at https://benchmarks.elastic.co (annotation R on the first chart).

Do you have provisioned IOPs for your EBS mounts?  Are you sure you're not running into that limit?

Can you try the local instance SSD, just for comparison?  Your EBS is backed by SSD as well, so this would let us remove EBS from the equation.  (You'd need to switch to an i2.4xlarge instance for this test).
</comment><comment author="l15k4" created="2015-10-26T12:53:25Z" id="151123698">General Purpose unfortunately,  the price of IO Provisioned SSDs surprised us. If you want to go beyond 160 MiB/s to 320 MiB/s it costs double than the volume itself.

I guess it wouldn't throttle with IO Provisioned SSD with 9000 IOPS to reach those 320MiB/s ... but these machines cost fortune :-)
</comment><comment author="mikemccand" created="2015-10-26T21:10:50Z" id="151285903">&gt; I guess it wouldn't throttle with IO Provisioned SSD with 9000 IOPS to reach those 320MiB/s ... but these machines cost fortune :-)

Or just use the local instance attached SSDs on the i2.\* instance types ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use GeoUtils.earthDiameter in the bounding box optimization.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6065</link><project id="" key="" /><description>We switched to Lucene's SloppyMath way of computing an approximate value of
the eath diameter given a latitude in order to compute distances, yet the
bounding box optimization of the geo distance filter still assumed a constant
earth diameter, equal to the average.

Close #6008
</description><key id="32894771">6065</key><summary>Use GeoUtils.earthDiameter in the bounding box optimization.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-06T14:00:54Z</created><updated>2014-07-01T18:03:28Z</updated><resolved>2014-05-06T14:21:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-06T14:06:04Z" id="42305969">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6064</link><project id="" key="" /><description /><key id="32890881">6064</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">eeishaan</reporter><labels /><created>2014-05-06T13:13:36Z</created><updated>2014-07-16T21:45:33Z</updated><resolved>2014-05-14T10:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:22:26Z" id="42308058">HI @eeishaan 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-05-14T10:03:35Z" id="43062907">CLA not signed.  Treating as bug report.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve 404 on missing scroll id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6063</link><project id="" key="" /><description>This relates to #6040, the fix is twofold, first, not handling missing context specifically in the search code, but behave the same as we do in non scroll search, where if all the shards failed, raise an exception. The second is to apply this logic in both scroll cases.
</description><key id="32889896">6063</key><summary>Improve 404 on missing scroll id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T13:00:25Z</created><updated>2015-06-08T15:03:45Z</updated><resolved>2014-05-06T13:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-06T13:08:18Z" id="42299383">LGTM
</comment><comment author="kimchy" created="2014-05-06T13:57:14Z" id="42304785">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed Index Status API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6062</link><project id="" key="" /><description>The functionality of the index status API has been replaced by the recovery API.

Relates #4854
</description><key id="32889454">6062</key><summary>Removed Index Status API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T12:54:35Z</created><updated>2015-06-06T16:58:53Z</updated><resolved>2014-05-07T15:32:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T06:16:17Z" id="42393108">LGTM
</comment><comment author="dadoonet" created="2014-08-01T08:12:40Z" id="50859958">I think we should add a breaking in 2.0 page in master with that change in.
`docs/migration/migrate_2_0.asciidoc`

@clintongormley WDYT?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: `size` property parsing inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6061</link><project id="" key="" /><description>In aggregations, the size property must be unquoted while in other places it can be quoted (tested in 1.0.0, 1.1.0 and 1.1.1)

The following work:

``` javascript
{
  "size": "1",
  "aggs": {
    "aggtest": {
      "terms": {
        "field": "somefield",
        "size": 1
      }
    }
  }
}
```

But this doesn't:

``` javascript
{
  "size": "1",
  "aggs": {
    "aggtest": {
      "terms": {
        "field": "somefield",
        "size": "1"
      }
    }
  }
}
```

Resulting in a 400 error ending in "Parse Failure [Unknown key for a VALUE_STRING in [aggtest]: [size].]]; }]"

This inconsistency is not mentioned in documentation and the error message is not very helpful, so I could only spot the problem after @HonzaKral pasted a working example on irc.

I believe at least a fix should be done so all fields behave the same, or with mandatory or with optional quotes, but no two fields with same name but different behaviour.
</description><key id="32886573">6061</key><summary>Aggregations: `size` property parsing inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oswaldo</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T12:10:57Z</created><updated>2014-11-25T19:35:01Z</updated><resolved>2014-11-25T07:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove unused dump infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6060</link><project id="" key="" /><description>Way back when, when ES started, there was an idea for a dump infrastructure, but it ended up supporting its serviceability aspects through APIs, remove the unused code
</description><key id="32878541">6060</key><summary>Remove unused dump infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T09:50:40Z</created><updated>2015-06-08T15:03:56Z</updated><resolved>2014-05-06T12:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-06T09:58:48Z" id="42284616">LGTM, great diff stats :)
</comment><comment author="kimchy" created="2014-05-06T12:03:09Z" id="42294083">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made it mandatory to specify IndicesOptions when calling MetaData#concreteIndices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6059</link><project id="" key="" /><description>Removed MetaData#concreteIndices variations that didn't require an IndicesOptions argument. Every caller should use a single method and specify how indices should be resolved to concrete indices based on the indices options argument.
</description><key id="32876681">6059</key><summary>Made it mandatory to specify IndicesOptions when calling MetaData#concreteIndices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T09:18:35Z</created><updated>2015-06-07T13:31:27Z</updated><resolved>2014-05-06T10:46:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-06T10:02:58Z" id="42284925">LGTM, added a small comment about documentation.
</comment><comment author="javanna" created="2014-05-06T10:10:15Z" id="42285394">I added some javadoc, can you have a look again @dakrone ? 
</comment><comment author="dakrone" created="2014-05-06T10:14:14Z" id="42285665">Looks good, I agree with @uboness' comment also.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update repositories.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6058</link><project id="" key="" /><description /><key id="32874069">6058</key><summary>Update repositories.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mathieu-chauvet</reporter><labels /><created>2014-05-06T08:33:19Z</created><updated>2014-07-08T06:45:06Z</updated><resolved>2014-05-13T13:57:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:21:25Z" id="42307924">Hi @matdere 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

Also, you have misspelled `example`

thanks
</comment><comment author="clintongormley" created="2014-05-13T13:57:25Z" id="42957858">Merged, many thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Outdated documentation on mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6057</link><project id="" key="" /><description>The guide about [breaking changes in 1.0](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_indices_apis.html) describes the new format for the mapping API as `GET /{indices}/_mapping/{types}`.

However the following docs show different format:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html
 http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-mapping.html

Is this a bug in the docs or is it okay to use both?
</description><key id="32873816">6057</key><summary>Outdated documentation on mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cebe</reporter><labels /><created>2014-05-06T08:28:42Z</created><updated>2014-05-06T16:16:36Z</updated><resolved>2014-05-06T15:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T15:21:51Z" id="42316551">Hi @cebe 

Bug in the docs - now fixed

thanks
</comment><comment author="cebe" created="2014-05-06T16:16:36Z" id="42323948">cool, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Random dynamic templates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6056</link><project id="" key="" /><description>This change randomly indexes the _id field and randomizes field data formats
and loading.

Close #5834
</description><key id="32872876">6056</key><summary>[TEST] Random dynamic templates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-06T08:09:41Z</created><updated>2014-07-16T21:45:36Z</updated><resolved>2014-05-06T13:24:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-06T08:25:10Z" id="42277626">looks great, left a few minor comments
</comment><comment author="jpountz" created="2014-05-06T08:34:55Z" id="42278339">@javanna pushed a new commit to remove all `"match": "*"` in the mapping templates.
</comment><comment author="javanna" created="2014-05-06T08:39:55Z" id="42278675">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>boost query in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6055</link><project id="" key="" /><description>Hello everybody,
i would like to use boost in the case : 

I have a personal index that contains the following properties: 
- sname
- fname 
- login 

When I do research on fname= 'toto' and sname = 't' I would like my results are sorted by relevance. For example : 
- Toto tata
- Toto tutu
- Toto tttttt
- Totoaaaa Tutfut
- etc...

But I do not know how to deal with boost. Can you help me please ? 
</description><key id="32871413">6055</key><summary>boost query in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">20pro</reporter><labels /><created>2014-05-06T07:35:50Z</created><updated>2014-05-06T10:42:49Z</updated><resolved>2014-05-06T10:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-06T10:42:49Z" id="42287595">@20pro please use the mailing list (https://groups.google.com/forum/?fromgroups#!forum/elasticsearch) or IRC ( #elasticsearch on webchat.freenode). We keep github for bug reporting &amp; feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove field names in stats url</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6054</link><project id="" key="" /><description>#5671
</description><key id="32865956">6054</key><summary>Remove field names in stats url</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-06T04:49:55Z</created><updated>2015-06-07T13:32:04Z</updated><resolved>2014-06-03T12:04:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T12:04:01Z" id="44955784">Thanks @kzwang merged, and sorry for the delay ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bigdesk Head: issue when setting threadpool to -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6051</link><project id="" key="" /><description>I see this failure when loading the bigdesk or head UI after setting the following values to -1 (unbounded) in elasticsearch.yml.  
threadpool.index.queue_size: -1
threadpool.bulk.queue_size: -1
threadpool.search.queue_size: -1

If I set these to a fixed positive number everything works fine.  I'm running Elasticsearch 1.1.1 and the latest version of bigdesk and head. I've also observed this issue with Elasitcearch 0.90.x

[2014-05-05 12:33:00,046][DEBUG][action.admin.cluster.node.info] [ESVM1] failed to execute on node [iQfORVfLQJGavRUw1mGE1w]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 8428
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:130)
    at org.elasticsearch.common.io.stream.AdapterStreamInput.readInt(AdapterStreamInput.java:102)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:598)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
</description><key id="32841319">6051</key><summary>Bigdesk Head: issue when setting threadpool to -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">loganbhardy</reporter><labels /><created>2014-05-05T20:15:14Z</created><updated>2014-12-30T18:00:18Z</updated><resolved>2014-12-30T18:00:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="loganbhardy" created="2014-05-06T15:41:35Z" id="42319276">I should also note that to trigger the error you have to browse to the Head UI or go to bigdesk and try to hit a different node other than the one you directly connected to. You can see graphs for the endpoint you connected to directly but nothing is displayed on other nodes in the cluster. 
</comment><comment author="gpstathis" created="2014-05-22T16:59:31Z" id="43915396">@loganbhardy we were seeing the same stack on AWS since our 1.0 upgrade and not only with BigDesk or ES Head, but with the Java Client as well. As soon as I bound the queue_size for our bulk operations, the stacks stopped. Thanks for the tip, it fixed the symptom. Now curious to see what the root cause is.
</comment><comment author="clintongormley" created="2014-12-30T18:00:18Z" id="68379756">Hi @loganbhardy 

Sorry it has taken a while to get to this.  It looks like this has been fixed in a more recent version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit the number of bytes that can be allocated to process requests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6050</link><project id="" key="" /><description>This should prevent costly requests from killing the whole cluster.
</description><key id="32825811">6050</key><summary>Limit the number of bytes that can be allocated to process requests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T17:03:42Z</created><updated>2015-06-07T13:32:49Z</updated><resolved>2014-05-07T10:57:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clarify `missing` behavior.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6049</link><project id="" key="" /><description /><key id="32810395">6049</key><summary>Clarify `missing` behavior.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">tmacam</reporter><labels /><created>2014-05-05T13:53:55Z</created><updated>2014-07-16T21:45:37Z</updated><resolved>2014-05-13T13:50:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:20:10Z" id="42307756">HI @tmacam 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="tmacam" created="2014-05-09T19:51:15Z" id="42706581">Done, I guess.

My CTO/Manager signed as "A company wanting its employees to contribute"
and listed me as a employee allowed to to contribute. Do I have to sign as
a "An individual contributing under an existing company contributor
agreement" as well?
</comment><comment author="clintongormley" created="2014-05-13T13:50:15Z" id="42956842">That's perfect. Many thanks, merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update asciifolding-tokenfilter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6048</link><project id="" key="" /><description>Typo
</description><key id="32809585">6048</key><summary>Update asciifolding-tokenfilter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Neamar</reporter><labels /><created>2014-05-05T13:42:59Z</created><updated>2014-07-16T21:45:38Z</updated><resolved>2014-05-06T14:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-05T18:41:07Z" id="42222075">LGTM
</comment><comment author="clintongormley" created="2014-05-06T14:19:15Z" id="42307634">Hi @Neamar 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="Neamar" created="2014-05-06T14:28:23Z" id="42308881">Just signed the document
</comment><comment author="clintongormley" created="2014-05-06T14:30:27Z" id="42309167">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reset locale to C in bin/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6047</link><project id="" key="" /><description>Because the NetworkExceptionHelper class relies on the english language in
order to extract information and decide whether a certain exception is a
network problem, we need to set the english locale on startup in order
to prevent other locales to circumvent this check.
</description><key id="32804926">6047</key><summary>Reset locale to C in bin/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T12:23:11Z</created><updated>2015-06-07T13:33:02Z</updated><resolved>2014-05-27T15:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-09T06:52:30Z" id="42638328">I'm surprised the exception messages are localized, I thought it was only the case for the `getLocalizedMessage` method (which is used in `toString()`), not `getMessage()`?
</comment><comment author="spinscale" created="2014-05-09T08:30:38Z" id="42644037">Hey,

thats true. I dont know why it happens, but I know it happens, see https://github.com/elasticsearch/elasticsearch/issues/5957

Maybe some user/library code does this by accident?
</comment><comment author="jpountz" created="2014-05-09T09:05:25Z" id="42646575">I suspect this error message is generated at the OS level and is just reused by the JVM so I think your PR would fix it!
</comment><comment author="dakrone" created="2014-05-13T11:47:40Z" id="42945007">This is the only `export` statement in our startup files, should we do:

```
LANG=C
LC_ALL=C
```

instead to follow the pattern from `bin/elasticsearch.in.sh`? (ie, no export, just set it locally)
</comment><comment author="s1monw" created="2014-05-15T11:41:43Z" id="43199492">@spinscale what's the status of this?
</comment><comment author="spinscale" created="2014-05-15T12:27:57Z" id="43202857">@s1monw I just need to test, if Lee's suggestion works flawless under ubuntu (as they use dash and not bash for `/bin/sh`) and then its ready to push from my side
</comment><comment author="spinscale" created="2014-05-15T12:48:20Z" id="43204637">removed the export and added a bit more explanation
</comment><comment author="dakrone" created="2014-05-16T08:59:17Z" id="43310897">Hardcoding the locale to C causes indices with non-ascii names to have some issues, for the index "test-weird-index-&#20013;&#25991;", I get this exception:

```
[2014-05-16 10:42:07,288][WARN ][cluster.action.shard     ] [Quentin Beck] [test-weird-index-??][0] sending failed shard for [test-weird-index-??][0], node[R7pUZLpKQWmwfjq65z-M-A], [P], s[INITIALIZING], indexUUID [LY4SJInhT3CqBeMZuscziw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed recovery]; nested: InvalidPathException[Malformed input or input contains unmappable chacraters: /Users/hinmanm/src/elasticsearch/tmp/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index/write.lock]; ]]
[2014-05-16 10:42:07,288][WARN ][cluster.action.shard     ] [Quentin Beck] [test-weird-index-??][0] received shard failed for [test-weird-index-??][0], node[R7pUZLpKQWmwfjq65z-M-A], [P], s[INITIALIZING], indexUUID [LY4SJInhT3CqBeMZuscziw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed recovery]; nested: InvalidPathException[Malformed input or input contains unmappable chacraters: /Users/hinmanm/src/elasticsearch/tmp/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index/write.lock]; ]]
[2014-05-16 10:42:07,291][WARN ][indices.cluster          ] [Quentin Beck] [test-weird-index-??][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.InvalidPathException: Malformed input or input contains unmappable chacraters: /Users/hinmanm/src/elasticsearch/tmp/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index/write.lock
        at sun.nio.fs.UnixPath.encode(UnixPath.java:147)
        at sun.nio.fs.UnixPath.&lt;init&gt;(UnixPath.java:71)
        at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
        at java.io.File.toPath(File.java:2186)
        at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:145)
        at org.apache.lucene.store.Lock.obtain(Lock.java:77)
        at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:710)
        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1279)
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:259)
        at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:687)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:161)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
        ... 3 more
```

And then a ton of these errors repeating. To reproduce, package up this branch, run `bin/elasticsearch`, then from a checkout of Elasticsearch: `LANG=C LC_ALL=C ES_WAIT_ON_MAPPING_CHANGE=true mvn -Dtests.jvms=2 -Dtests.class=\*ElasticsearchRestTests -Dtests.cluster=localhost:9300 clean compile test`
</comment><comment author="dakrone" created="2014-05-16T09:08:41Z" id="43311618">You should also be able to see it by running ES from this branch and doing:

```
&#187; curl -XPOST 'localhost:9200/test-weird-index-&#20013;&#25991;/doc/1' -d'{"foo": "bar"}'
{"_index":"test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;","_type":"doc","_id":"1","_version":1,"created":true}%                                                                                                

&#187; indices
health index                   pri rep docs.count docs.deleted store.size pri.store.size
yellow test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;   5   1          1            0      2.6kb          2.6kb
```

And in the logs:

```
[2014-05-16 11:07:08,699][INFO ][cluster.metadata         ] [Anomaly] [test-weird-index-??????] creating index, cause [auto(index api)], shards [5]/[1], mappings []
[2014-05-16 11:07:09,184][INFO ][cluster.metadata         ] [Anomaly] [test-weird-index-??????] update_mapping [doc] (dynamic)
```
</comment><comment author="spinscale" created="2014-05-19T07:50:34Z" id="43474348">as suggested by robert, I set the locale to `en_EN.UTF-8` - @dakrone maybe you can rerun your tests?
</comment><comment author="dakrone" created="2014-05-19T08:34:41Z" id="43477559">Tests are much happier now, +1
</comment><comment author="s1monw" created="2014-05-19T09:43:28Z" id="43483109">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Data mismatch in terms_stats facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6046</link><project id="" key="" /><description>My test data and mapping are listed here: 

https://gist.github.com/rbnacharya/7acc70b99f758da2162e

Apply the mapping, and add the data... 

After that: 

I've used a simple facet: 

```
POST http://localhost:9200/testindex/Medical/_search
{
   "size": 0,
   "facets": {
      "totalPaidAmount:top20": {
         "terms_stats": {
            "key_field": "udf21Id",
            "value_field": "paidAmount",
            "size": 20,"order":"total"
         }
      }
   }
}
```

And,

```
POST http://localhost:9200/testindex/Medical/_search
{
   "size": 0,
   "facets": {
      "totalPaidAmount:top500": {
         "terms_stats": {
            "key_field": "udf21Id",
            "value_field": "paidAmount",
            "size": 500,"order":"total"

         }
      }
   }
}
```

The response is not same for some records [count and total] . As you can match results yourself. 

In facet top20, There are less documents, but in facet with name containing top500, there are more document counts. 

Am I doing wrong? or is it a elasticsearch bug?? 
</description><key id="32803236">6046</key><summary>Data mismatch in terms_stats facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbnacharya</reporter><labels /><created>2014-05-05T11:51:48Z</created><updated>2014-05-05T12:06:20Z</updated><resolved>2014-05-05T12:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T12:06:20Z" id="42180911">This is indeed a known limitation of the terms and terms stats facets, see https://github.com/elasticsearch/elasticsearch/issues/1305 for more information. You can improve accuracy by increasing the value of the  `shard_size` parameter at the cost of more memory usage and network traffic.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search 1.1.0 Windows Inconstent Results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6045</link><project id="" key="" /><description>Hi,

I have ES setup on windows server 2003 with no changes to the default settings and I'm having an issue wherey the same query will consistenly give me different results on even and odd query cycles.

i.e.
Run Fixed Query: ES gives me resultsset1
Run Fixed Query: ES gives me resultsset2
Run Fixed Query: ES gives me resultsset1
Run Fixed Query: ES gives me resultsset2
Run Fixed Query: ES gives me resultsset1
Run Fixed Query: ES gives me resultsset2
Run Fixed Query: ES gives me resultsset1
Run Fixed Query: ES gives me resultsset2

It looks like replicas are not in sync but not sure how I should go about testing or verifying this.

Have tried clearing index caches but makes no difference.
Have tried dropping and re-indexing multiple times but makes no difference.

I downgraded to ES 1.0.0, re-indexed and all works as expected.

This is the query that was being run:

{query: {bool: {minimum_should_match: 1, should: [{fuzzy_like_this: {
like_text: 2541, fuzziness: 0.5}}, {match: {message: {operator: and
, query: 2541, boost: 100}}}, {ids: {values: [2541], boost: 100}}]
, must: [{filtered: {filter: {terms: {project_id: [1, 2, 3, 4, 5, ]}}}}]}}}

I looked through the list of existing issues and couldn't find anything that would be the cause of this. Apologies if by chance I have missed something and this issue has already been resolved.

Thanks,
Vackar
</description><key id="32802627">6045</key><summary>Elastic Search 1.1.0 Windows Inconstent Results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">VackarAfzal</reporter><labels /><created>2014-05-05T11:38:17Z</created><updated>2014-05-06T15:52:49Z</updated><resolved>2014-05-05T12:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-05T12:46:06Z" id="42183685">Hi Vackar

It looks like you're running into the "bouncing results" phenomenon.  See http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_search_options.html#_literal_preference_literal for an explanation
</comment><comment author="VackarAfzal" created="2014-05-05T15:15:40Z" id="42198652">Apologies, should have been clearer in my initial post.
While I am getting the same results (consistantly bouncing), one is a subset of the other. 
i.e. it's not simply a case that one is ordered differently from the other.
Resultset one always returns 37 hits, whereas resultset 2 always returns 60 hits.
Any ideas?
</comment><comment author="clintongormley" created="2014-05-06T15:05:25Z" id="42314215">It does look like your replicas are out of sync. Not sure what would have caused that.  Unfortunately, it's a bit late to figure out as I'm guessing you no longer have the old logs.
</comment><comment author="VackarAfzal" created="2014-05-06T15:52:49Z" id="42320847">I did keep a zip backup of the logs, is there any way that I could upload them? I've tried using the attach option but it doesn't seem to like zip files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove operation threading from broadcast actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6044</link><project id="" key="" /><description>Similar to search removal, the operation threading options are not really ued, and the default should always be used. This also considerably simplifies the code.
A side affect is that we can now remove the ShardIterator#firstOrNull method, which can cause for sneaky bugs to occur.
</description><key id="32798928">6044</key><summary>Remove operation threading from broadcast actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Java API</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T10:13:29Z</created><updated>2015-06-06T16:59:01Z</updated><resolved>2014-05-05T15:10:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T14:49:56Z" id="42195693">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API: Default analyzer accidentally removed stopwords</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6043</link><project id="" key="" /><description>The analyze API used the standard analyzer from lucene and therefore removed
stopwords instead of using the elasticsearch default analyzer.

Closes #5974
</description><key id="32796744">6043</key><summary>Analyze API: Default analyzer accidentally removed stopwords</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Analysis</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T09:30:03Z</created><updated>2015-06-07T20:32:32Z</updated><resolved>2014-05-05T14:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-05T12:23:20Z" id="42182097">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove search operation threading option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6042</link><project id="" key="" /><description>Search operation threading is an option that is not really used, and current non default implementations are flawed. Handling it also creates quite the complexity in the search handling codebase...
This is a breaking change, but one that is actually a good one, since I haven't seen/heard anybody use it, and if its used, its problematic...
</description><key id="32795525">6042</key><summary>Remove search operation threading option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Java API</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T09:05:10Z</created><updated>2015-06-06T16:59:17Z</updated><resolved>2014-05-05T09:39:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T09:36:57Z" id="42172314">LGTM
</comment><comment author="javanna" created="2014-05-05T09:38:46Z" id="42172446">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `shard_min_doc_count` parameter for significant terms similar to `shard_size`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6041</link><project id="" key="" /><description>Significant terms internally maintain a priority queue per shard with a size potentially
lower than the number of terms. This queue uses the score as criterion to determine if
a bucket is kept or not. If many terms with low subsetDF score very high
but the `min_doc_count` is set high, this might result in no terms being
returned because the pq is filled with low frequent terms which are all sorted
out in the end.

This can be avoided by increasing the `shard_size` parameter to a higher value.
However, it is not immediately clear to which value this parameter must be set
because we can not know how many terms with low frequency are scored higher that
the high frequent terms that we are actually interested in.

On the other hand, if there is no routing of docs to shards involved, we can maybe
assume that the documents of classes and also the terms therein are distributed evenly
across shards. In that case it might be easier to not add documents to the pq that have
subsetDF &lt;= `shard_min_doc_count` which can be set to something like
`min_doc_count`/number of shards  because we would assume that even when summing up
the subsetDF across shards `min_doc_count` will not be reached.
</description><key id="32795217">6041</key><summary>Add `shard_min_doc_count` parameter for significant terms similar to `shard_size`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T08:58:43Z</created><updated>2015-06-08T15:04:08Z</updated><resolved>2014-05-07T16:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T09:49:47Z" id="42173167">I think this parameter would be useful to the terms aggregation as well (in a separate change).
</comment><comment author="brwe" created="2014-05-05T13:15:20Z" id="42186068">Thanks for the review! I implemented the changes.
I will make a separate pull request for the terms aggregation once this is pushed. Or do we need a separate issue for that also?
</comment><comment author="jpountz" created="2014-05-05T16:48:13Z" id="42209120">LGTM!

&gt; I will make a separate pull request for the terms aggregation once this is pushed. Or do we need a separate issue for that also?

A pull request is fine, thanks!
</comment><comment author="markharwood" created="2014-05-06T09:36:27Z" id="42282975">LGTM - just added a comment on the docs re documenting side-effects of shard_min_doc_count settings.
Part of me wonders if some of the low-level settings we offer (shard/reducer PQ sizes, shard/reducer frequency filters) are a bit techy and could be abstracted to more user-friendly policy choices e.g. "find rare things". 
</comment><comment author="brwe" created="2014-05-06T10:25:45Z" id="42286429">Thanks! I added a new commit to enhance the documentation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing scroll id now returns 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6040</link><project id="" key="" /><description>A bad/non-existing scroll ID used to return a 200, however a 404 might be more useful.
Also, this PR returns the right Exception (SearchContextMissingException) in the Java API.

TODO: I need some more feedback for this PR. I think this solution is not optimal and there might be a better option (checking the exception type twice is not what I want to do, I do this in order to return a real exception instead of a shard failure, but it is not a good solution codewise and I think there are better solutions which I do not see at the moment).

Closes #5729
</description><key id="32794657">6040</key><summary>Missing scroll id now returns 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T08:46:58Z</created><updated>2015-06-07T21:07:41Z</updated><resolved>2014-05-05T15:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-05T12:20:41Z" id="42181892">looks good, one comment:

Won't calling `SearchResponse#status` will just return the correct HTTP code in this case? No need for specific handling of the status code?

Saying that, we have a bug in RestSearchAction where we don't use `SearchResponse#status` to use as the status code we return now (this is because of the REST refactoring that was done), can you fix that as well since its the same listener logic and can be shared.

Wondering if we can add a `ToXContent` extension interface, that can also return the relevant status code, and then the `SearchResponse` can implement it, and in `RestToXContentListener` we can the status from the response if its an instance of that "StatusToXContent" interface.
</comment><comment author="kimchy" created="2014-05-05T15:33:45Z" id="42200794">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore read/write visibility in PlainShardsIterator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6039</link><project id="" key="" /><description>Change #5561 introduced a potential bug in that iterations that are performed
on a thread are might not be visible to other threads due to the removal of the
`volatile` keyword.
</description><key id="32792655">6039</key><summary>Restore read/write visibility in PlainShardsIterator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T08:01:49Z</created><updated>2015-06-07T20:09:48Z</updated><resolved>2014-05-05T08:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-05T08:04:18Z" id="42166473">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make _search on all indices switchable?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6038</link><project id="" key="" /><description>Hi,

A real story.

First off: my code was flawed. It triggered a "POST /_search" (query + timerange filter + sort on timestamp) resulting in a search on all shards (4000 pri shards) filling fielddata cache to its maximum causing my nodes to dive into full gc and become unresponsive.

Maybe its an idea to disallow search on all indices? To act as safeguard for bugs like mine?

Regards,
Renzo
</description><key id="32792531">6038</key><summary>Make _search on all indices switchable?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rtoma</reporter><labels><label>discuss</label></labels><created>2014-05-05T07:59:08Z</created><updated>2014-07-28T10:07:05Z</updated><resolved>2014-07-23T08:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-10T16:09:54Z" id="48626389">@rtoma what version was this on?  In recent versions the fielddata circuit breaker should kick in and halt the request, preventing the situation you described.
</comment><comment author="clintongormley" created="2014-07-23T08:52:05Z" id="49848330">No further info. Closing
</comment><comment author="rtoma" created="2014-07-28T09:29:14Z" id="50316765">hi @clintongormley - this issue was encountered with:

```
{
  "status" : 200,
  "name" : "xxxxxxxxxx",
  "version" : {
    "number" : "1.0.1",
    "build_hash" : "5c03844e1978e5cc924dab2a423dc63ce881c42b",
    "build_timestamp" : "2014-02-25T15:52:53Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="clintongormley" created="2014-07-28T10:07:05Z" id="50320951">@rtoma recent versions should handle this much more gracefully
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator: Allow significant terms and geo hash grid aggregations in the percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6037</link><project id="" key="" /><description>The problem is that these two aggregations use an heuristic to compute the default value of `shard_size` that depends on the number of shards that the request targets, yet `PercolateContext.numberOfShards` throws an `UnsupportedOperationException`.
</description><key id="32791532">6037</key><summary>Percolator: Allow significant terms and geo hash grid aggregations in the percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-05T07:33:50Z</created><updated>2014-07-16T11:51:08Z</updated><resolved>2014-05-22T08:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update index.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6036</link><project id="" key="" /><description>Because this client doesn't apparently work with the 1.x elasticsearch (see https://github.com/elasticsearch/elasticsearch-lang-groovy/issues/20 ) and you can't find the source code either, we should tell people so they don't waste a lot of time :-)
</description><key id="32783312">6036</key><summary>Update index.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pmcneil</reporter><labels><label>docs</label></labels><created>2014-05-05T02:05:33Z</created><updated>2015-03-21T09:08:17Z</updated><resolved>2015-03-21T09:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-05T08:04:30Z" id="42166485">@pmcneil You're right. 
We are currently moving groovy client from the "lang" repo to a "client" repo. So we will be able to update this page with the right link!

Thanks for raising it.
</comment><comment author="pmcneil" created="2014-05-05T10:23:04Z" id="42175233">More than that. The Groovy client doesn't work with es 1.x :-(&#160;

Sent from Samsung Mobile

&lt;div&gt;-------- Original message --------&lt;/div&gt;&lt;div&gt;From: David Pilato notifications@github.com &lt;/div&gt;&lt;div&gt;Date:05/05/2014  6:04 PM  (GMT+10:00) &lt;/div&gt;&lt;div&gt;To: elasticsearch/elasticsearch elasticsearch@noreply.github.com &lt;/div&gt;&lt;div&gt;Cc: Peter McNeil peter@mcneils.net &lt;/div&gt;&lt;div&gt;Subject: Re: [elasticsearch] Update index.asciidoc (#6036) &lt;/div&gt;&lt;div&gt;
&lt;/div&gt;@pmcneil You're right. 
We are currently moving groovy client from the "lang" repo to a "client" repo. So we will be able to update this page with the right link!

Thanks for raising it.

&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="dadoonet" created="2014-05-05T16:14:55Z" id="42205405">I should have written: we are moving it to another repo **AND** upgrading it to elasticsearch 1.x! :)

Stay tuned...
</comment><comment author="clintongormley" created="2014-07-11T09:39:03Z" id="48711904">@dadoonet any news on this one?
</comment><comment author="KevinGreene" created="2014-09-08T04:36:08Z" id="54776030">@dadoonet It's been many months, and as evidenced here: https://github.com/elasticsearch/elasticsearch-lang-groovy/issues/20#issuecomment-37762347, people are still confused about this based on the docs. Can this be merged in?
</comment><comment author="pmcneil" created="2014-09-10T01:22:10Z" id="55059650">Bump. _Please_ at least merge this pull request and push to prevent people having to find out the hard way and wasting their time. Elasticsearch is a great project, shame to see it let down by something that can be fixed simply by adding a line of doco.

Do you need help to get the Groovy driver going?
</comment><comment author="dadoonet" created="2014-10-03T09:26:12Z" id="57773137">@pickypg I'm assigning you to this one as it's relative to groovy client.
</comment><comment author="pickypg" created="2014-11-27T03:23:07Z" id="64741689">@pmcneil We were actively working on the Groovy client that is now hosted over at [elasticsearch-groovy](https://github.com/elasticsearch/elasticsearch-groovy).

These documents will be updated relative to the new Groovy client.

https://github.com/elasticsearch/elasticsearch-groovy
</comment><comment author="javanna" created="2015-03-21T09:08:17Z" id="84284046">I think we can close this now that the [elasticsearch-groovy project](https://github.com/elastic/elasticsearch-groovy) is live and updated. Sorry it took a while...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update allocation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6035</link><project id="" key="" /><description>It's minor, but I've added the percentage representations in brackets as it took me a couple of attempts to figure out I wasn't supposed to be using an actual percentage, eg 80%/90%, but a decimal representation of the value, eg 0.80/0.90.

That should make it a lot clearer for others.
</description><key id="32779692">6035</key><summary>Update allocation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-05-04T22:58:43Z</created><updated>2014-07-02T06:32:30Z</updated><resolved>2014-05-05T06:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T06:37:05Z" id="42162333">Both notations are supposed to work. Can you please provide us with the command that didn't work so that we can look into what went wrong?
</comment><comment author="markwalkom" created="2014-05-05T06:45:49Z" id="42162701">Using sense, updating from 0.99 to 99%;
[2014-05-05 16:43:36,286][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:36,287][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:36,287][WARN ][node.settings            ] [na0-esd-a-001.campmon.net] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings@7a4aea84]
org.elasticsearch.ElasticsearchParseException: Unable to parse high watermark: [99%]
    at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings.onRefreshSettings(DiskThresholdDecider.java:96)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:428)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-05-05 16:43:36,287][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:36,287][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:36,287][WARN ][node.settings            ] [na0-esd-a-001.campmon.net] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings@7fd52606]
org.elasticsearch.ElasticsearchParseException: Unable to parse high watermark: [99%]
    at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings.onRefreshSettings(DiskThresholdDecider.java:96)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:428)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-05-05 16:43:36,289][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:36,289][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:36,289][WARN ][node.settings            ] [na0-esd-a-001.campmon.net] failed to refresh settings for [org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings@601a8ef6]
org.elasticsearch.ElasticsearchParseException: Unable to parse high watermark: [99%]
    at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider$ApplySettings.onRefreshSettings(DiskThresholdDecider.java:96)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:428)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

This is the curl;
PUT /_cluster/settings 
{
        "transient" : {
        "cluster.routing.allocation.disk.watermark.high" : "99%"
    }
}

Then setting back to 0.99
[2014-05-05 16:43:56,500][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:56,500][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:56,500][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.high] to [0.99]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.high] to [0.99]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.threshold_enabled] from [true] to [true]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.low] to [0.95]
[2014-05-05 16:43:56,501][INFO ][cluster.routing.allocation.decider] [na0-esd-a-001.campmon.net] updating [cluster.routing.allocation.disk.watermark.high] to [0.99]

Using this curl;
PUT /_cluster/settings 
{
        "transient" : {
        "cluster.routing.allocation.disk.watermark.high" : "0.99"
    }
}

I'm running ES 1.1.1 on Ubuntu 12.04, Oracle JRE 1.7.0_u55.
</comment><comment author="jpountz" created="2014-05-05T06:55:44Z" id="42163083">OK, I just understood why I was confused. There is indeed an issue with Elasticsearch 1.1 but this is fixed in current development code and the fix will be part of the upcoming 1.2.0 release. See https://github.com/elasticsearch/elasticsearch/pull/5690 for more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix bug in PropertyPlaceholder and add unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6034</link><project id="" key="" /><description>This pull request includes the following changes:
- Fix bug in `PropertyPlaceholder` when prefix and suffix have same length or suffix is longer than prefix
- Add unit tests for the `PropertyPlaceholder` class. `testNestedSameLengthPrefixSuffix` and `testNestedShorterPrefix` failed before this bug fix.
</description><key id="32766126">6034</key><summary>Fix bug in PropertyPlaceholder and add unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>:Settings</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-04T10:42:47Z</created><updated>2015-06-07T20:10:47Z</updated><resolved>2014-05-05T08:29:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T08:30:18Z" id="42167909">Merged. Excellent change, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cat API test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6033</link><project id="" key="" /><description>The REST tests for the cat API currently fail with the following error:

```
org.elasticsearch.test.rest.ElasticsearchRestTests.test {yaml=cat.thread_pool/10_basic/Test cat thread_pool output}

java.lang.AssertionError: field [$body] was expected to match the provided regex but didn't
Expected:   #pid       id          host       ip                          port
^  (\d+  \s+  \S{4}  \s+  \S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d{4} \s+ \n)+  $
     got: "23702 HEtV ip-10-224-38-25 10.224.38.25 - \n23702 0UsM ip-10-224-38-25 10.224.38.25 - \n23702 11n9 ip-10-224-38-25 10.224.38.25 - \n23702 pXcq ip-10-224-38-25 10.224.38.25 - \n23702 suvM ip-10-224-38-25 10.224.38.25 - \n23702 Lvf7 ip-10-224-38-25 10.224.38.25 - \n"
```
</description><key id="32765444">6033</key><summary>cat API test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-04T09:57:00Z</created><updated>2014-05-04T20:11:18Z</updated><resolved>2014-05-04T20:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Search might not return on thread pool rejection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6032</link><project id="" key="" /><description>When a thread pool rejects the execution on the local node, the search might not return.
This happens due to the fact that we move to the next shard only _within_ the execution on the thread pool in the start method. If it fails to submit the task to the thread pool, it will go through the fail shard logic, but without "counting" the current shard itself. When this happens, the relevant shard will then execute more times than intended, causing the total opes counter to skew, and for example, if on another shard the search is successful, the total ops will be incremented _beyond_ the expectedTotalOps, causing the check on == as the exit condition to never happen.
The fix here makes sure that the shard iterator properly progresses even in the case of rejections, and also includes improvement to when cleaning a context is sent in case of failures (which were exposed by the test).
Though the change fixes the problem, we should work on simplifying the code path considerably, the first suggestion as a followup is to remove the support for operation threading (also in broadcast), and move the local optimization execution to SearchService, this will simplify the code in different search action considerably, and will allow to remove the problematic #firstOrNull method on the shard iterator.
The second suggestion is to move the optimization of local execution to the TransportService, so all actions will not have to explicitly do the mentioned optimization.
fixes #4887
</description><key id="32758849">6032</key><summary>Search might not return on thread pool rejection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Search</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-04T01:09:20Z</created><updated>2015-06-07T20:10:53Z</updated><resolved>2014-05-05T07:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-04T08:24:59Z" id="42127504">@kimchy Could #5997 be related to this?
</comment><comment author="kimchy" created="2014-05-04T16:43:14Z" id="42137471">@clintongormley doesn't look like it at first glance...
</comment><comment author="martijnvg" created="2014-05-04T17:29:21Z" id="42138751">Sneaky bug! Left one minor comment, but other than that LGTM.
</comment><comment author="megastef" created="2014-05-31T22:24:38Z" id="44761362">In which version is this fixed? I've got a 1.1.2 and a single node.js process with async queries is  running into the issue. Could you pls. recommend me quickly an alternative (1.2 got again breaking changes ...)
</comment><comment author="kimchy" created="2014-06-19T11:46:26Z" id="46550721">this is fixed in 1.1.2, are you sure you are running into this problem and not something else maybe? Can you open a new issue so we can track it, with a recreation if you can which would help tremendously
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added Bloodhound Haskell client/DSL to docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6031</link><project id="" key="" /><description /><key id="32758598">6031</key><summary>Added Bloodhound Haskell client/DSL to docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">bitemyapp</reporter><labels><label>docs</label></labels><created>2014-05-04T00:46:08Z</created><updated>2014-08-18T10:11:22Z</updated><resolved>2014-08-18T10:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-05T08:03:24Z" id="42166424">Hey,

cool stuff! Can you sign the CLA at http://www.elasticsearch.org/contributor-agreement/ so I can get the PR in?

Thanks a lot!
</comment><comment author="spinscale" created="2014-07-18T10:42:33Z" id="49417947">@bitemyapp did you sign the CLA, would like to get it in
</comment><comment author="clintongormley" created="2014-08-07T18:35:25Z" id="51512986">CLA not signed. Closing
</comment><comment author="bitemyapp" created="2014-08-07T18:39:10Z" id="51513553">@clintongormley re-open/merge? I signed and confirmed. This fell through the cracks for me, sorry.
</comment><comment author="clintongormley" created="2014-08-07T18:40:29Z" id="51513749">OK - it takes some time before we can see the signature, but i've reopened and will check again soon

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6030</link><project id="" key="" /><description>Fixed "Jone Done" to "Jone Doe"
</description><key id="32758553">6030</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">yosssi</reporter><labels /><created>2014-05-04T00:42:24Z</created><updated>2014-07-16T21:45:43Z</updated><resolved>2014-05-06T14:32:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:18:14Z" id="42307500">Hi @yosssi 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="yosssi" created="2014-05-06T14:30:02Z" id="42309119">@clintongormley Thanks for your comment. I signed the CLA. Thanks!
</comment><comment author="clintongormley" created="2014-05-06T14:32:55Z" id="42309550">Merged, thanks!
</comment><comment author="yosssi" created="2014-05-06T14:40:08Z" id="42310616">You're welcome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correcting gramma</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6029</link><project id="" key="" /><description /><key id="32755372">6029</key><summary>Correcting gramma</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">violuke</reporter><labels /><created>2014-05-03T21:22:22Z</created><updated>2014-07-16T21:45:43Z</updated><resolved>2014-05-06T16:01:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:17:11Z" id="42307372">Hi @violuke 

Thanks for the PR. Sorry it has taken a while to get to it.  Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="violuke" created="2014-05-06T14:21:30Z" id="42307933">Thanks, I've done this now.

Kind Regards,

Luke Cousins

Technical Director

_VioVet Ltd_
_53 Bilton Way_
_Luton_
_LU1 1UU_
_United Kingdom_
_Mobile: 07881916567_
_Direct Dial: 01582 843329_
_Office: 01582 842096_
_www.viovet.co.uk_ http://www.viovet.co.uk

On 6 May 2014 15:17, Clinton Gormley notifications@github.com wrote:

&gt; Hi @violuke https://github.com/violuke
&gt; 
&gt; Thanks for the PR. Sorry it has taken a while to get to it. Please could I
&gt; ask you to sign our CLA so that I can get your commits merged in:
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/6029#issuecomment-42307372
&gt; .
</comment><comment author="clintongormley" created="2014-05-06T16:01:20Z" id="42321971">Thanks, merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error on update a field called "time" with type long</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6028</link><project id="" key="" /><description>Hi,
I have a mapping as follows:
`"\"task\": { " +
                            "\"properties\": { " +
                                "\"id\" : { \"type\": \"string\", \"index\": \"not_analyzed\" }," +
                                "\"title\": { \"type\": \"string\" }," +
                                "\"description\": { \"type\": \"string\", \"index\": \"not_analyzed\" }," +
                                "\"address\": { \"type\": \"string\" }," +
                                "\"time\": { \"type\": \"long\" }," +
                                "\"location\" : {" +
                                    "\"type\" : \"geo_point\", " +
                                    "\"lat_lon\" : \"true\", " +
                                    "\"normalize\" : \"false\"," +
                                    "\"validate\" : \"true\"" +
                                    "}"+
                                " }" +
                            " } " +
                      "}"`
When I try to update the "time" field with another long value with the following script:
`{
    "script": "ctx._source.time += time",
    "params": {
        "time": 4
    }
}`
 i get this error:
`{
"error" : "MapperParsingException[failed to parse [time]]; nested: NumberFormatException[For input string: "1396457400org.elasticsearch.common.mvel2.util.MethodStub@7f63eccb"]; ",
"status" : 400
}`
Now If i rename "time" to "timex" and recreate the index. my update works fine! why? is "time" a reserved word or something?
Using the JAVA API and just a simple RESTful call give me the same behaviour
</description><key id="32748765">6028</key><summary>Error on update a field called "time" with type long</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hossein761</reporter><labels /><created>2014-05-03T15:21:55Z</created><updated>2014-05-03T15:26:41Z</updated><resolved>2014-05-03T15:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-03T15:26:41Z" id="42107711">Duplicate of #6027
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error on update a field called "time" with type long</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6027</link><project id="" key="" /><description>Hi,
I have a mapping as follows:
`"\"task\": { " +
                            "\"properties\": { " +
                                "\"id\" : { \"type\": \"string\", \"index\": \"not_analyzed\" }," +
                                "\"title\": { \"type\": \"string\" }," +
                                "\"description\": { \"type\": \"string\", \"index\": \"not_analyzed\" }," +
                                "\"address\": { \"type\": \"string\" }," +
                                "\"time\": { \"type\": \"long\" }," +
                                "\"location\" : {" +
                                    "\"type\" : \"geo_point\", " +
                                    "\"lat_lon\" : \"true\", " +
                                    "\"normalize\" : \"false\"," +
                                    "\"validate\" : \"true\"" +
                                    "}"+
                                " }" +
                            " } " +
                      "}"`
When I try to update the "time" field with another long value with the following script:
`{
    "script": "ctx._source.time += time",
    "params": {
        "time": 4
    }
}`
 i get this error:
`{
"error" : "MapperParsingException[failed to parse [time]]; nested: NumberFormatException[For input string: "1396457400org.elasticsearch.common.mvel2.util.MethodStub@7f63eccb"]; ",
"status" : 400
}`
Now If i rename "time" to "timex" and recreate the index. my update works fine! why? is "time" a reserved word or something?
Using the JAVA API and just a simple RESTful call give me the same behaviour
</description><key id="32748744">6027</key><summary>Error on update a field called "time" with type long</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hossein761</reporter><labels /><created>2014-05-03T15:21:02Z</created><updated>2014-07-23T14:47:19Z</updated><resolved>2014-07-23T14:47:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T14:47:19Z" id="49883809">Hi @hossein761 

`time` is a reserved word in MVEL, but MVEL is now deprecated. This script works in `groovy` which will be the new default language from 1.4 onwards (already available in 1.3)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fields="*" doesn't work for search request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6026</link><project id="" key="" /><description>I am trying to use something like this {
  "query": {
    "match_all": {}
  },
  "fields": "*"
}

but it doesn't return any of the fields in the search result.
reference is here http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html
</description><key id="32728840">6026</key><summary>Fields="*" doesn't work for search request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kajal23</reporter><labels /><created>2014-05-02T21:48:44Z</created><updated>2014-05-06T15:08:20Z</updated><resolved>2014-05-06T15:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-02T22:44:23Z" id="42086453">Do you have stored fields?
What does your mapping look like?
</comment><comment author="kajal23" created="2014-05-05T17:41:19Z" id="42215039">this is what my mapping looks like.    curl -s -XPUT "http://localhost:9200/test/" -d '{
         "settings": {
            "index.number_of_shards": 1,
            "index.number_of_replicas": 0
         },
         "mappings": {
                 "city": {
                     "properties": {
                            "city": {"type": "string"},
                            "state": {"type": "string","index": "not_analyzed"},
                            "population": {"type": "integer"}
                      }
                  }
        }
   }'
My scenario is, i have a custom plugin that client has to call as a script field. and as soon as we specify script fields, we don't get all the regular fields document by default in search response. I have so many fields in the index (above is just the test), I don't want them to specify each field. So looking for something like all fields and I saw \* but it doesn't seems to be working for me.
</comment><comment author="clintongormley" created="2014-05-06T15:08:20Z" id="42314601">`fields` is for stored fields.  It sounds like you want:

```
GET /_search?_source=true
{ "query": {....}}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete By Query under heavy indexing load causes OOM errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6025</link><project id="" key="" /><description>Hello,

We are running an ES 1.1.1 cluster and when we bulk index into it with a high write load we have found that it can trigger OOM errors if we run deletebyquery calls. The symptoms of the problems appears to be the following:
1. Index into the cluster with a heavy write load, this will cause merges to pile up. During the pileup the merge thread pool queue goes up and stays at ~120 per node. (Maybe related to #5779?)
2. Merges are no longer able to keep up with new segments created from indexing operations and segments per node shoots up. (We have seen over 30k segments per node.)
3. Long GC cycles are triggered which may or may not cause the node to drop out of the cluster.
4. When a Delete By Query call is issued an internal refresh is triggered (#3593) which fails due to an out of memory error and causes the node to be removed from the cluster for not responding. (See [gist](https://gist.github.com/xyu/4ca1ddb56200479a07d7) for logs. In this example es13.iad sends a call to es13.sat which experiences OOM causing es13.iad to remove it from the cluster.)

Please let me know if there are any other info I can provide that may be helpful.
</description><key id="32714004">6025</key><summary>Delete By Query under heavy indexing load causes OOM errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">xyu</reporter><labels><label>bug</label></labels><created>2014-05-02T18:26:10Z</created><updated>2015-05-29T17:24:14Z</updated><resolved>2015-05-28T16:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xyu" created="2014-05-12T15:37:51Z" id="42847995">It appears that we are experiencing segment explosion issues during these times and are seeing queues in the merge thread pool. For now we are working around this issue by backing off on bulk index when merges fall behind in our indexing jobs. It looks like this issue is also being worked on in #6066 
</comment><comment author="clintongormley" created="2014-07-28T09:27:19Z" id="50316594">Fixed by #6066 
</comment><comment author="mikemccand" created="2015-03-04T19:25:22Z" id="77227288">Alas, deleteByQuery is not throttled when merges are falling behind.
</comment><comment author="mikemccand" created="2015-03-04T19:38:11Z" id="77229916">I'll make this operation throttled like we do for index/create ops.

However I'm doubtful this is "enough" to always prevent segment explosion, i.e. https://github.com/elasticsearch/elasticsearch/issues/7052 is a better (trickier) long term solution.
</comment><comment author="s1monw" created="2015-05-28T16:01:21Z" id="106444330">@mikemccand can we close this?
</comment><comment author="mikemccand" created="2015-05-28T16:59:42Z" id="106489214">Yeah, I'll close it ... I think you can still easily provoke OOME on 1.6, but in 2.0 we are switching to a plugin that runs scan/scroll query and then bulk delete the resulting IDs, which does fix it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not start a recovery process if the primary shard is currently allocated on a node which is not part of the cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6024</link><project id="" key="" /><description>If a source node disconnect during recover, the target node will respond by cancelling the recovery. Typically the master will respond by removing the disconnected node from the cluster state, promoting another shard to become primary. This is sent it to all nodes and the target node will start recovering from the new primary. However, if the drop of a node caused the node count to go bellow min_master_node, the master will step down and will not promote shard immediately. When a new master is elected we may publish a new cluster state (who's point is to notify of a new master) which is not yet updated. This caused the node to start a recovery to a non existent node. Before we aborted the recovery without cleaning up the shard, causing subsequent correct cluster states to be ignored. We should not start the recovery process but wait for another cluster state to come in.
</description><key id="32713465">6024</key><summary>Do not start a recovery process if the primary shard is currently allocated on a node which is not part of the cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T18:19:14Z</created><updated>2015-06-07T20:12:09Z</updated><resolved>2014-05-02T21:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-02T21:19:42Z" id="42080157">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Integration tests for benchmark API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6023</link><project id="" key="" /><description>Adds randomized integration tests for the benchmark API.
Adds negative tests for cases where the cluster cannot run benchmarks.
Small fixes for NPE's exposed during testing.

Closes #6003
</description><key id="32711735">6023</key><summary>Test: Integration tests for benchmark API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T17:55:13Z</created><updated>2014-10-21T23:26:55Z</updated><resolved>2014-05-07T21:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-07T20:10:35Z" id="42476936">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scroll api reduce phase fails if shard failures occur</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6022</link><project id="" key="" /><description>Only occurs in 1.x and master branches and not in any released versions.
</description><key id="32711567">6022</key><summary>Scroll api reduce phase fails if shard failures occur</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>blocker</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T17:52:50Z</created><updated>2015-06-07T20:12:21Z</updated><resolved>2014-05-08T08:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T20:15:01Z" id="42074374">Very nice catch!
</comment><comment author="martijnvg" created="2014-05-03T10:54:26Z" id="42102068">Updated the PR and addressed the feedback.
</comment><comment author="jpountz" created="2014-05-05T16:50:38Z" id="42209349">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix behavior on default boost factor for More Like This.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6021</link><project id="" key="" /><description>A boost terms factor of 1.0 is not the same as no boosting of terms.
The desired behavior is to deactivate boosting by default. If the user
specifies any value other than 0, then boosting is activated.
</description><key id="32694601">6021</key><summary>Fix behavior on default boost factor for More Like This.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T14:01:32Z</created><updated>2015-08-13T15:31:20Z</updated><resolved>2014-05-02T15:04:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T14:05:01Z" id="42035453">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster State API: Remove index template filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6020</link><project id="" key="" /><description>The possibility of filtering for index templates in the cluster state API
had been introduced before there was a dedicated index templates API. This
commit removes this support from the cluster state API, as it was not really
clean, requiring you to specify the metadata and the index templates.

Closes #4954
</description><key id="32684178">6020</key><summary>Cluster State API: Remove index template filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Cluster</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T11:09:24Z</created><updated>2015-06-06T16:59:41Z</updated><resolved>2014-05-05T13:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-05T12:22:40Z" id="42182059">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support `min_children` and `max_children` on `has_child` query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6019</link><project id="" key="" /><description>Currently it is very difficult and inefficient to write a query like:

```
where parent has at least 5 matching children
```

The only way to do it is to:
- use a `has_child` query with an inner `constant_score` and `score_mode` of `sum`
- wrap in a `function_score` that resets the `_score` to zero if the min count is not achieved
- do similar score calculations for all other clauses in the query
- apply a `min_score` parameter to the overall query

It would be much more efficient to be able to say:

```
{
  "has_child": {
    "type": "foo",
    "minimum_should_match": 5,
    "query": {...some query...}
  }
}
```

Hopefully this could be applied to filters as well as queries?

/cc @martijnvg 
</description><key id="32683430">6019</key><summary>Support `min_children` and `max_children` on `has_child` query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T10:54:15Z</created><updated>2015-06-07T13:33:42Z</updated><resolved>2014-05-30T17:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-02T13:42:54Z" id="42033034">+1 I like it
</comment><comment author="jsuchal" created="2015-03-05T17:14:21Z" id="77405999">@clintongormley sorry for bumping this, but is this possible with nested docs?
</comment><comment author="clintongormley" created="2015-03-09T00:39:41Z" id="77785163">@jsuchal currently not - this is a change that would have to go into Lucene, I believe.  But you could open a separate issue requesting it if you like
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store IO throttling throttles far more than asked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6018</link><project id="" key="" /><description>I've been digging into the "merges can fall behind" at high indexing
rates, and I discovered some serious issues with the IO throttling,
which we recently (#5902) up'd from 20 MB/sec to 50 MB/sec by default.

Net/net I think when we ask for 50 MB/sec today we are really
throttling at something like 8 MB/sec!

Details:

I indexed a bunch of small log-file type docs into 1 shard, 0
replicas, using 1 sync _bulk client, to the point where it did it's
first big-ish merge (611 MB, 440K docs); the merge does not use CFS so
it's really writing 611 MB.  I'm using a fast SSD.

With no throttling (index.store.throttle.type=none), the merge takes
20.8 seconds.

With the default 50 MB/sec merge throttling, it takes 72.1 sec, which
far too long (611 MB / 50 = 12.2 sec).  The rate limiter enforces the
instantaneous rate, so at worse the merge time should have been 20.8 +
12.2 = 33 sec but likely much less than that because merging takes
CPU time.

So I dug in and discovered one problem, I think caused by the
super.flush and then delegate.flush in BufferedChecksumIndexOutput,
where the RateLimiter is always alternately called first on 8192 bytes
then on 0 bytes.  If I fix RateLimiter to just ignore those 0 bytes,
the merge time with 50 MB/sec throttle drops to 49.9 sec: better, but
still too long.  (I think once we cutover to Lucene's checksums this 0
byte issue will be fixed?)

System.nanoTime is actually quite costly, so I suspect the overhead of
just checking whether to pause, and of calling Thread.sleep, is way
too much when the pause time is small.  So I change SimpleRateLimiter to
just accumulate the incoming bytes and then once it crosses 1 msec
worth at the specified rate, invoke the pause logic.

This really improved it: now the merge takes 25.7 sec at 50 MB/sec
throttle, and 64.9 sec at 10 MB/sec throttle.  These times seem correct.

I'll also open a Lucene issue to fix this, and make an XRateLimiter
for ES in the meantime.
</description><key id="32683081">6018</key><summary>Store IO throttling throttles far more than asked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T10:47:34Z</created><updated>2014-07-25T15:00:29Z</updated><resolved>2014-05-19T18:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-05-02T12:16:23Z" id="42024256">&gt; So I dug in and discovered one problem, I think caused by the
&gt; super.flush and then delegate.flush in BufferedChecksumIndexOutput,
&gt; where the RateLimiter is always alternately called first on 8192 bytes
&gt; then on 0 bytes. If I fix RateLimiter to just ignore those 0 bytes,
&gt; the merge time with 50 MB/sec throttle drops to 49.9 sec: better, but
&gt; still too long. (I think once we cutover to Lucene's checksums this 0
&gt; byte issue will be fixed?)

It will solve it in that place: the BufferedChecksumIndexOutput. Note that RateLimitedIndexOutput itself has a similar construction, we should ensure its not doing anything stupid too.

Another similar expensive call is any digester's updateBytes(). We want to avoid calling this native method frequently (e.g. for zero bytes) for the same reasons.
</comment><comment author="mikemccand" created="2014-05-05T10:17:47Z" id="42174916">I opened https://issues.apache.org/jira/browse/LUCENE-5641 to fix this in Lucene; I'll carry the fix over to ES once that's in ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reworded Note about shorthand suggest syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6017</link><project id="" key="" /><description>The existing Note about the shorthand suggest syntax was poorly worded and confusing. Please check whether the way I've phrased it now is still correct as to what the shorthand form actually does and doesn't do: the original wording did not provide me enough information to be sure.
Thanks!
</description><key id="32680760">6017</key><summary>Reworded Note about shorthand suggest syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">fransflippo</reporter><labels><label>docs</label></labels><created>2014-05-02T10:01:44Z</created><updated>2014-07-16T21:45:46Z</updated><resolved>2014-06-06T08:21:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-05T07:50:41Z" id="42165697">Hey,

makes sense. Can you sign the CLA at http://www.elasticsearch.org/contributor-agreement/ so I can get the PR in?

Thanks a lot!
</comment><comment author="clintongormley" created="2014-05-14T10:05:32Z" id="43063052">CLA not signed. Closing
</comment><comment author="fransflippo" created="2014-05-15T05:21:07Z" id="43169568">Huh? I did sign it. I figured it would notify you..?
</comment><comment author="clintongormley" created="2014-05-15T09:11:45Z" id="43185837">hi @fransflippo 

Sorry about that. Our CLA process is pretty manual right now, but you don't appear in the list, at least not with your github account.  Not sure what happened here.  Would you mind resigning it if you send another PR?

thanks
</comment><comment author="fransflippo" created="2014-05-15T09:13:26Z" id="43186021">Sure, I'll give it another shot :) I'll comment here when I have so you can check it right away.
</comment><comment author="fransflippo" created="2014-06-06T08:08:44Z" id="45312682">OK, just signed the CLA. Sorry for the delay, I got married in the meantime :)
</comment><comment author="javanna" created="2014-06-06T08:21:30Z" id="45313544">Merged, thanks @fransflippo and congrats!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Track per-request allocated bytes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6016</link><project id="" key="" /><description>As more and more data-structures are implemented on top of BigArrays, we can
add byte tracking on top of BigArrays in order to get a picture of how many
bytes have been allocated by the current request.

A potential follow-up to this change would be the ability to fail requests that
allocate too much memory instead of letting them cause out-of-memory errors on
the cluster.
</description><key id="32677892">6016</key><summary>Track per-request allocated bytes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-02T09:08:32Z</created><updated>2014-06-23T19:05:44Z</updated><resolved>2014-05-05T17:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-05T17:05:28Z" id="42210994">Closed in favor of #6050  that tracks memory across requests as well as releases.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update shade-plugin to 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6015</link><project id="" key="" /><description>Shade-plugin 2.2 does not work with JDK8 (see http://jira.codehaus.org/browse/MSHADE/fixforversion/19828)

Not sure if we need to apply that patch in 1.2 branch as well?
</description><key id="32672369">6015</key><summary>Update shade-plugin to 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>non-issue</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T07:04:17Z</created><updated>2015-06-10T09:34:51Z</updated><resolved>2014-05-12T08:25:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-09T06:56:22Z" id="42638516">LGTM

&gt; Not sure if we need to apply that patch in 1.2 branch as well?

I think we should.
</comment><comment author="dadoonet" created="2014-05-12T08:25:34Z" id="42806859">Closed with 645efa05df9880a78f5550c09619c15ebf5e915f in master (2.0) and 86f30585cf5dc3fb799d5582e4077bebddad5f11 in 1.x (1.2)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch date range query ignores month field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6014</link><project id="" key="" /><description>Hi guys,

I've been using Elasticsearch as my data store and I got  lots of documents in it. My problem is, I figured it out that Elasticsearch does ignore month field regarding mapping and I can not get real search response. 

Here is what I have in my index and my query, please tell me if I'm wrong:

```
curl -XPUT 'http://localhost:9200/tt6/' -d '{}'
curl -XPUT 'http://localhost:9200/tt6/tweet/_mapping' -d '{"tweet" : {"properties" : {"date" : {"type" : "date", "format": "YYYY-MM-DD HH:mm:ss" }}}}'
curl -XPUT 'http://localhost:9200/tt6/tweet/1' -d '{"date": "2014-02-14 04:00:45"}'

curl -XGET 'http://localhost:9200/tt6/_search' -d '
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "tweet.date": {
              "from": "2014-12-01 00:00:00",
              "to": "2014-12-30 00:00:00"
            }
          }
        }
      ],
      "must_not": [],
      "should": []
    }
  },
  "from": 0,
  "size": 10,
  "sort": [],
  "facets": {}
}
```

And my response is

```
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "tt6",
        "_type": "tweet",
        "_id": "1",
        "_score": 1,
        "_source": {
          "date": "2014-02-14 04:00:45",
          "name": "test"
        }
      }
    ]
  }
}
```

By given date range it must has no response beet 1st of December 2014 and 30th of December 2014, but it returns.

Any help will be appreciated.

Regards.

Fatih.
</description><key id="32671741">6014</key><summary>Elasticsearch date range query ignores month field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fatihzkaratana</reporter><labels /><created>2014-05-02T06:45:40Z</created><updated>2014-05-02T18:17:26Z</updated><resolved>2014-05-02T18:17:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-05-02T18:17:26Z" id="42061594">The `DD` portion of the [date format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html) specifies day of _year_ not day of _month_, as a result the month section is ignored. The correct format in your case would be `YYYY-MM-dd HH:mm:ss`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`bin/plugin` tests for missing plugin name when passing `--url`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6013</link><project id="" key="" /><description>When setting the `--url`, it now sets the implied action--`INSTALL`. Therefore, if the user fails to supply the install flag themselves, then the plugin name will be caught as missing (also added to remove incase a future scenario allows that) and fail immediately.

Adding code to test for unset plugin names to fail fast with descriptive error messages (for all flags that require a value). Also simplified the series of `if` statements checking for the commands by using a `switch` (now that it's using Java 7), added tests, and updated random exceptions with the up-to-date flag names (e.g., "--verbose" instead of "-verbose").

@dadoonet Note: This will cause or have merge conflicts with #5977. I have no problem with my changes just being incorporated into that PR or doing the merge here.

Also, while messing with the unit tests, I noticed that the package for the tests is `org.elasticsearch.plugin` while the non-test code is `org.elasticsearch.plugins`. That's probably worth a cleanup by whoever loses the merge; I avoided doing it upfront to simplify any potential merge.

Closes #5976
</description><key id="32668693">6013</key><summary>`bin/plugin` tests for missing plugin name when passing `--url`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-02T05:18:01Z</created><updated>2015-06-07T13:37:16Z</updated><resolved>2014-07-04T14:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-04T14:22:28Z" id="48049638">Thanks! Pushed in 1.x and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6012</link><project id="" key="" /><description /><key id="32666535">6012</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gabriel-tessier</reporter><labels /><created>2014-05-02T04:06:19Z</created><updated>2014-07-16T21:45:48Z</updated><resolved>2014-05-02T21:45:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T21:45:36Z" id="42082321">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[elasticsearch 1.1] Marvel preventing cluster restart.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6011</link><project id="" key="" /><description>We have a 3 node cluster running on i2.xlarge instances.

Tried to reindexing a 10Gb dataset, 30M docs using the scroll API, with scroll size to 1M. This was causing elasticsearch to be unresponsive. 

I then tried a rolling restart of elasticsearch nodes which failed. Shutting down all the nodes and restarting also failed. We saw a lof of the following exceptions, which went away after deleting marvel.

[2014-05-01 12:11:44,329][WARN ][search.action            ] [elasticsearch-i2-1] Failed to send release search context
org.elasticsearch.transport.SendRequestTransportException: [es3][inet[/10.0.0.150:9300]][search/freeContext]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:202)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:173)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendFreeContext(SearchServiceTransportAction.java:103)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.releaseIrrelevantSearchContexts(TransportSearchTypeAction.java:381)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:188)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [es3][inet[/10.0.0.150:9300]] Node not connected
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:859)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:540)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:189)
    ... 12 more
[2014-05-01 12:11:44,329][WARN ][search.action            ] [elasticsearch-i2-1] Failed to send release search context
org.elasticsearch.transport.SendRequestTransportException: [es3][inet[/10.0.0.150:9300]][search/freeContext]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:202)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:173)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendFreeContext(SearchServiceTransportAction.java:103)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.releaseIrrelevantSearchContexts(TransportSearchTypeAction.java:381)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:188)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [es3][inet[/10.0.0.150:9300]] Node not connected
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:859)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:540)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:189)
    ... 12 more
[2014-05-01 12:11:44,354][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] error connecting to [localhost:9200]
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:316)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:181)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:245)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:129)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:331)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:229)
    at java.lang.Thread.run(Thread.java:744)
[2014-05-01 12:11:44,378][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] could not connect to any configured elasticsearch instances: [localhost:9200]
[2014-05-01 12:11:44,379][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] error connecting to [localhost:9200]
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:316)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:181)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:245)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportEvents(ESExporter.java:160)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportEvents(AgentService.java:287)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:233)
    at java.lang.Thread.run(Thread.java:744)
[2014-05-01 12:11:44,379][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] could not connect to any configured elasticsearch instances: [localhost:9200]
[2014-05-01 12:11:44,432][WARN ][action.bulk              ] [elasticsearch-i2-1] failed to update master on updated mapping for index [prod-project-9-5-1], type [emailSend]
org.elasticsearch.indices.IndexMissingException: [prod-project-9-5-1] missing
    at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:254)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.updateMappingOnMaster(TransportShardBulkAction.java:605)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:188)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-05-01 12:11:44,432][WARN ][monitor.jvm              ] [elasticsearch-i2-1] [gc][old][614438][125] duration [14.8m], collections [49]/[14.9m], total [14.8m]/[19m], memory [17.7gb]-&gt;[10.6gb]/[17.7gb], all_pools {[young] [254.8mb]-&gt;[162mb]/[266.2mb]}{[survivor] [0b]-&gt;[0b]/[33.2mb]}{[old] [17.5gb]-&gt;[10.5gb]/[17.5gb]}
[2014-05-01 12:11:45,328][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] error connecting to [localhost:9200]
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:316)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:292)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.access$300(ESExporter.java:58)
    at org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run(ESExporter.java:696)
    at java.lang.Thread.run(Thread.java:744)
[2014-05-01 12:11:46,329][ERROR][marvel.agent.exporter    ] [elasticsearch-i2-1] error connecting to [localhost:9200]
java.net.ConnectException: Connection refused
</description><key id="32662059">6011</key><summary>[elasticsearch 1.1] Marvel preventing cluster restart.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hjz</reporter><labels /><created>2014-05-02T01:39:10Z</created><updated>2014-08-02T07:18:48Z</updated><resolved>2014-08-02T07:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-05-02T07:23:11Z" id="41998245">Hi, 

I'm not sure I understand why you say Marvel prevents you from restarting the cluster. It seems your problem lie in the fact that you are issues a scroll request with size=1m. This means ES has to load 1M docs per shard, which is very memory intensive. My guess is that your nodes run into memory pressure and become unresponsive. 

Marvel sends periodic information about the node. By default store that information locally using localhost:9200. My guess is that your node takes a long time to shut down due to the huge memory pressure and thus you see a couple of errors from Marvel as it retries to send the information. 

To solve all of this I suggest you use a much smaller size in your scroll (typically in the order of hundreds, depending on the number of shards, disks and network you have). 1M is a extreme.

As a side note- we recommend sending the marvel data to another monitoring cluster rather then storing it locally. This would allow you to use it properly when you have problems with your main cluster - exactly when you need it most.
</comment><comment author="hjz" created="2014-05-02T23:44:34Z" id="42089548">The scroll query provided was for context. Even when we shut down all the nodes and tried a restart, the nodes weren't starting.

The behavior here seems to be an exception in Marvel is preventing the node from starting, so this can happen even if Marvel is connected to an external cluster (good suggestion), if the external cluster is down. 

I'm unfamiliar with how Marvel is integrated with elasticsearch, is there a chance for marvel failures the node its installed on?
</comment><comment author="clintongormley" created="2014-08-02T07:18:48Z" id="50956334">The only Marvel related messages in these logs indicates that the node it was trying to contact was unresponsive, which is not a surprise given that it spent nearly 15 minutes doing a garbage collection :)

Please reduce your scroll size (and bulk size to a much smaller number, eg 1000) and check on this page that you have configured ES correctly for your system: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html

If, after that, you are still seeing issues, then please open a new ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>for nested queries, field mapper doesn't take path into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6010</link><project id="" key="" /><description>The field-mapping behavior around nested queries is somewhat unintuitive. Say a document has two nested documents in its mapping, 'one' and 'two', and each contains a field called 'name'. Then doing a nested query for 'name' with path 'two' may not return any results, because the field resolves to 'one.name':

https://gist.github.com/jtibshirani/f63b92ec91e71b35bd4c

Using the full path 'two.name' fixes the issue, but it'd be awesome if the field mapper could take the path from the query into account when resolving fields, and only consider fields in that nested document.

This reproduced on ES 1.1.1, and seems related to #4644.
</description><key id="32649011">6010</key><summary>for nested queries, field mapper doesn't take path into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jtibshirani</reporter><labels /><created>2014-05-01T21:21:11Z</created><updated>2014-07-23T12:27:06Z</updated><resolved>2014-07-23T12:27:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T12:27:06Z" id="49866571">Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_timestamp: enabled=true doesn't work when applied via index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6009</link><project id="" key="" /><description>I have the following index template set:

``` json
{
    "template" : "foo-*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "_default_" : {
            "_timestamp" : { "enabled" : true },
            "dynamic_templates": [
                { "standard_string": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "index":          "not_analyzed"
                      }
                }},
                { "analyzed_string": {
                      "match":              "*_analyzed", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
        }
    }
}
```

When creating a new index `foo-bar` I can verify the mapping is applied correctly, and the `_timestamp` setting is indeed persisted in the index mapping for each and every type in it, but no timestamps are recorded for any document that's being added to that index.
</description><key id="32645462">6009</key><summary>_timestamp: enabled=true doesn't work when applied via index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2014-05-01T20:35:31Z</created><updated>2014-05-09T13:17:16Z</updated><resolved>2014-05-08T09:24:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-05T09:42:15Z" id="42172677">Hey,

can you provide some more information what exactly is not working? I just tried this snippet which includes searches and aggregations based on the timestamp field and it worked as expected with es 1.1.1

``` json
DELETE foo-bar

PUT /_template/foo
{
    "template" : "foo-*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "_default_" : {
            "_timestamp" : { "enabled" : true },
            "dynamic_templates": [
                { "standard_string": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "index":          "not_analyzed"
                      }
                }},
                { "analyzed_string": {
                      "match":              "*_analyzed", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
        }
    }
}

PUT foo-bar

GET foo-bar/_mapping

PUT /foo-bar/foo/1
{
  "key" : "value"
}

GET /foo-bar/_search
{
  "post_filter" : {
    "range" : {
      "_timestamp" : {
        "gte" : "2014-05-01"
      }
    }
  }
}

GET /foo-bar/_search
{
  "post_filter" : {
    "range" : {
      "_timestamp" : {
        "gte" : "2014-06-01"
      }
    }
  }
}

GET /foo-bar/_search?size=0
{
  "aggs": {
    "keys": {
      "terms": {
        "field": "_timestamp",
        "size": 10
      }
    }
  }
}
```
</comment><comment author="synhershko" created="2014-05-07T15:57:37Z" id="42445817">Just to confirm, issuing `GET /foo-bar/_search` should return results with `_timestamp` next to the `_source` and `_score`, right...?
</comment><comment author="synhershko" created="2014-05-07T16:34:13Z" id="42450408">@spinscale I can confirm that using the following actions Kibana shows 0 results once Time Filter is enabled on `/index.html#/dashboard/file/guided.json` (the Sample Dashboard thing)

Here's my exact scenario (modified a bit from yours):

``` json
DELETE foo-bar

PUT /_template/foo
{
    "template" : "foo-*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "_default_" : {
            "_timestamp" : { "enabled" : true },
            "dynamic_templates": [
                { "standard_string": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "index":          "not_analyzed"
                      }
                }},
                { "analyzed_string": {
                      "match":              "*_analyzed", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
        }
    }
}

POST /foo-bar/foo
{
  "key" : "value"
}

GET /foo-bar/_search

POST /foo-bar/_search
{
  "post_filter" : {
    "range" : {
      "_timestamp" : {
        "gte" : "2014-05-01"
      }
    }
  }
}

POST /foo-bar/_search
{
  "post_filter" : {
    "range" : {
      "_timestamp" : {
        "gte" : "2014-06-01"
      }
    }
  }
}

POST /foo-bar/_search?size=0
{
  "aggs": {
    "keys": {
      "terms": {
        "field": "_timestamp",
        "size": 10
      }
    }
  }
}

GET foo-bar/_mapping
```
</comment><comment author="clintongormley" created="2014-05-08T09:24:10Z" id="42529167">&gt; Just to confirm, issuing GET /foo-bar/_search should return results with _timestamp next to the _source and _score, right...?

No, enabling the timestamp makes it indexed, not stored.  So you can use it in `range` clauses, and you can use it in aggs, but you can't retrieve it unless you change the mapping to:

```
"_timestamp" : { "enabled" : true, "store": true }
```

And you'd need to request the field in the search request, eg:

```
GET /_search?fields=_timestamp
```

or to get all stored fields:

```
GET /_search?fields=*
```
</comment><comment author="synhershko" created="2014-05-09T11:21:46Z" id="42656020">@clintongormley @spinscale can you please help me understand why Kibana won't recognize this then? When viewing this in Kibana I get 0 results once I enable the Time Filter, which uses _timestamp by default.

Steps to reproduce:

On a blank new ES 1.1 installation do:

``` json
DELETE foo-bar

PUT /_template/foo
{
    "template" : "foo-*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "_default_" : {
            "_timestamp" : { "enabled" : true },
            "dynamic_templates": [
                { "standard_string": {
                      "match":              "*", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "index":          "not_analyzed"
                      }
                }},
                { "analyzed_string": {
                      "match":              "*_analyzed", 
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
        }
    }
}

POST /foo-bar/foo
{
  "key" : "value"
}
```

Use the last one to post multiple documents for funs

Go to the Kibana Sample dashboard (e.g. http://localhost:3579/index.html#/dashboard/file/guided.json), you will see it shows some data.

Click the Time Filter and set it to `15 minutes ago to a few seconds ago`. The dashboard will now show no data at all.

If the `_timestamp` field is enabled and indexed, how come Kibana shows no data?
</comment><comment author="spinscale" created="2014-05-09T12:14:22Z" id="42659450">Kibana uses the `@timestamp` field by default for timestamps not the built-in timestamp functionality, I guess you changed that? (wondering why you are referring to the guided dashboard then).
</comment><comment author="clintongormley" created="2014-05-09T12:26:41Z" id="42660293"> You can change the timestamp field that is used by going to: Configure dashboard (top right icon) &gt; Timepicker
</comment><comment author="synhershko" created="2014-05-09T12:53:29Z" id="42662343">Hmm I was over thinking this, thought `@timestamp` is some sort of an alias to `_timestamp`. Just checked and it works with `_timestamp`, thanks!

@spinscale I'm building a demo with a logstash-like functionality for some database system, and I want to provide an easy Kibana experience once data is replicated to ES.

I assume there is good reasoning behind using `@timestamp` and not `_timestamp`? probably because the timestamp field set by logstash might refer to a non-current time (backlog indexing for example)?
</comment><comment author="clintongormley" created="2014-05-09T13:17:16Z" id="42664370">&gt; I assume there is good reasoning behind using @timestamp and not _timestamp? probably because the timestamp field set by logstash might refer to a non-current time (backlog indexing for example)?

Correct - kibana was originally built as a better GUI for Logstash, which used `@timestamp`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`optimize_bbox` for geo_distance filters can cause missing results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6008</link><project id="" key="" /><description>When optimize_bbox is enabled for geo_distance filters, it can cause missing results:

https://gist.github.com/jtibshirani/1e42809a52be9ac651fc

This issue occurs on ES 1.1.1, and also in ES 1.0.3 and below, before the upgrade to Lucene 4.7. It seems the distance calculation for the bounding box uses DistanceUnit#getEarthRadius(), which is the radius at the semi-major axis, whereas the actual geo_distance filter uses SloppyMath to do the calculation. In Lucene 4.6 SloppyMath uses the average earth radius, and in 4.7 it averages the radii at the two points.

The same problem exists for both 'memory' and 'indexed' bounding boxes.
</description><key id="32644335">6008</key><summary>`optimize_bbox` for geo_distance filters can cause missing results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jtibshirani</reporter><labels><label>:Geo</label><label>blocker</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-01T20:19:54Z</created><updated>2015-06-07T20:12:35Z</updated><resolved>2014-05-06T14:20:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-06T13:19:25Z" id="42300518">Thanks for the detailed report, I could reproduce the issue and will work on a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DocAsUpsert Not Available on BulkUpdateDescriptor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6007</link><project id="" key="" /><description>Only on UpdateDescriptor... I'm sure this is an oversight?
</description><key id="32625835">6007</key><summary>DocAsUpsert Not Available on BulkUpdateDescriptor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nariman-haghighi</reporter><labels /><created>2014-05-01T16:19:48Z</created><updated>2014-05-02T07:31:34Z</updated><resolved>2014-05-02T07:31:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nariman-haghighi" created="2014-05-01T16:26:57Z" id="41926688">It also generates an error on UpdateDescriptor too:

```
.Update&lt;MediaStream&gt;(u =&gt; u.Index("Stream").Document(stream).DocAsUpsert(true))
```

Exception:

```
Could not dispatch IElasticClient.Update() into any of the following paths: 
```
</comment><comment author="Mpdreamz" created="2014-05-02T07:31:34Z" id="41998914">Hi @nariman-haghighi 

Nice catch this is indeed an oversight :exclamation: . Mind reopening this ticket inside Elasticsearch.Net/NEST own repository?

https://github.com/elasticsearch/elasticsearch-net/issues

This is elasticsearch's main repository.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support empty properties array in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6006</link><project id="" key="" /><description>closes #5887
</description><key id="32625822">6006</key><summary>Support empty properties array in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-01T16:19:36Z</created><updated>2015-06-07T14:26:19Z</updated><resolved>2014-05-01T16:41:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-05-01T16:41:06Z" id="41928177">LGTM
</comment><comment author="evillemez" created="2014-05-01T16:51:00Z" id="41929223">Awesome!  That was quick, thanks a bunch!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to obtain NativeFSLock - corrupt index stays corrupt after delete + recreate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6005</link><project id="" key="" /><description>Hi,

On ES 1.0.1 replica shard 0 of an old index (no updates/inserts anymore) I get errors like below.

I tried dropping the replica via number_of_replicas=0. Dropping works and data is gone from disk. But after initializing (number_of_replicas=1) the replica, the errors like below start again. After an error it restarts recovery, fails and restarts again. This is consuming 1Gbit/s.

Is there a solution except running on no replicas?

```
[2014-05-01 16:16:05,868][WARN ][indices.cluster          ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [logstash-adm-syslog-2014.04.07][0]: Recovery failed from [adm-logsearch-db-001.ams5][oBeYGkQOR5SfVhpu5GTDlA][adm-logsearch-db-001.bolcom.net][inet[/10.98.252.21:9300]]{datacenter=ams5} into [adm-logsearch-db-005.ams5][unuBb8hOQZWFT8cnQ8QcGw][adm-logsearch-db-005.bolcom.net][inet[/10.98.252.85:9300]]{datacenter=ams5}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:303)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$300(RecoveryTarget.java:65)
    at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:171)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.transport.RemoteTransportException: [adm-logsearch-db-001.ams5][inet[/10.98.252.21:9300]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [logstash-adm-syslog-2014.04.07][0] Phase[2] Execution failed
    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1102)
    at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:634)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
    at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:61)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:323)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.transport.RemoteTransportException: [adm-logsearch-db-005.ams5][inet[/10.98.252.85:9300]][index/shard/recovery/prepareTranslog]
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-adm-syslog-2014.04.07][0] failed to create engine
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:260)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:706)
    at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:389)
    at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:363)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:84)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:702)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1399)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)
    ... 7 more
```
</description><key id="32617624">6005</key><summary>Failed to obtain NativeFSLock - corrupt index stays corrupt after delete + recreate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rtoma</reporter><labels><label>bug</label></labels><created>2014-05-01T14:20:58Z</created><updated>2015-06-07T20:13:00Z</updated><resolved>2014-07-23T15:02:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rtoma" created="2014-05-01T14:25:32Z" id="41914191">When setting number_of_replicas=2 (I have 3 nodes) the recovery failes on both replica hosts. So I guess the primary is broken?
</comment><comment author="kimchy" created="2014-05-01T14:37:05Z" id="41915262">if you restart the node that will eventually hold the replicas, does it still happen? This might the caused because of this bug that is fixed in Lucene 4.8: https://issues.apache.org/jira/browse/LUCENE-5612
</comment><comment author="rtoma" created="2014-05-01T14:58:36Z" id="41917421">Hi Kimchy,

We will try to restart the node next week.

We were testdriving curator yesterday during the same timerange to optimize the index to 2 max_num_segments. Maybe this caused the index to get broken. Strange fact: searching it does not return errors (yet).

I have looked for the 1st error regarding this index:

```
[2014-04-30 13:55:37,009][WARN ][index.merge.scheduler    ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] failed to merge
java.io.EOFException: read past EOF: MMapIndexInput(path="/srv/elastic5/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/_n
08.fdt")
        at org.apache.lucene.store.ByteBufferIndexInput.readBytes(ByteBufferIndexInput.java:101)
        at org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:254)
        at org.elasticsearch.index.store.Store$StoreIndexOutput.copyBytes(Store.java:619)
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$ChunkIterator.copyCompressedData(CompressingStoredFieldsReader.java:499)
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.merge(CompressingStoredFieldsWriter.java:376)
        at org.apache.lucene.index.SegmentMerger.mergeFields(SegmentMerger.java:316)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4071)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3668)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
```
</comment><comment author="rtoma" created="2014-05-07T12:38:28Z" id="42421313">@kimchy: I finished a rolling restart of my cluster holding that 1 bad index. After restart I increased replica count from 0 to 1 and now the replica is good.
</comment><comment author="rtoma" created="2014-05-09T11:38:33Z" id="42657161">It gets stranger. Throwing away the bad index and recreating it, triggers the same issue. Somewhere state is maintained outside the index causing the corruption?

Details:

All of a sudden 1 shard of my 0-replica index got unassigned. Recovery is never ending.

```
index                          shard prirep state          docs store ip           node                      
logstash-adm-syslog-2014.04.07 2     p      STARTED    11337750 4.2gb 10.98.252.85 adm-logsearch-db-005.ams5 
logstash-adm-syslog-2014.04.07 0     p      UNASSIGNED                                                       
logstash-adm-syslog-2014.04.07 3     p      STARTED    11324303 4.2gb 10.98.252.22 adm-logsearch-db-002.ams5 
logstash-adm-syslog-2014.04.07 1     p      STARTED    11348976 4.2gb 10.98.252.21 adm-logsearch-db-001.ams5 
logstash-adm-syslog-2014.04.07 4     p      STARTED    11326723 4.2gb 10.98.252.22 adm-logsearch-db-002.ams5 
```

So we decided to deleted and recreate the whole index. The index was really gone (checked on disk), but recreation triggers a bad shard + replica again:

```
index                          shard prirep state        docs store ip           node                      
logstash-adm-syslog-2014.04.07 0     p      INITIALIZING            10.98.252.85 adm-logsearch-db-005.ams5 
logstash-adm-syslog-2014.04.07 0     r      UNASSIGNED                                                     
logstash-adm-syslog-2014.04.07 1     r      STARTED         0   79b 10.98.252.21 adm-logsearch-db-001.ams5 
logstash-adm-syslog-2014.04.07 1     p      STARTED         0   99b 10.98.252.22 adm-logsearch-db-002.ams5 
logstash-adm-syslog-2014.04.07 2     r      STARTED         0   79b 10.98.252.21 adm-logsearch-db-001.ams5 
logstash-adm-syslog-2014.04.07 2     p      STARTED         0   99b 10.98.252.22 adm-logsearch-db-002.ams5 
```

Logging says:

```
[2014-05-09 13:32:34,746][WARN ][index.engine.internal    ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] Could not lock IndexWriter isLocked [false]
org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock
        at org.apache.lucene.store.Lock.obtain(Lock.java:84)
        at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:702)
        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1399)
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)
        at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:684)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2014-05-09 13:32:34,746][WARN ][indices.cluster          ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-adm-syslog-2014.04.07][0] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-adm-syslog-2014.04.07][0] failed to create engine
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:260)
        at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:684)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
        ... 3 more
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock
        at org.apache.lucene.store.Lock.obtain(Lock.java:84)
        at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:702)
        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1399)
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)
        ... 6 more
[2014-05-09 13:32:34,764][WARN ][cluster.action.shard     ] [adm-logsearch-db-005.ams5] [logstash-adm-syslog-2014.04.07][0] sending failed shard for [logstash-adm-syslog-2014.04.07][0], node[l_U2IDMwQwSNYzpsyGuWaQ], [P], s[INITIALIZING], indexUUID [p0dmJBQiROClRk3cIXZOkA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-adm-syslog-2014.04.07][0] failed recovery]; nested: EngineCreationFailureException[[logstash-adm-syslog-2014.04.07][0] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/srv/elastic1/logsearch-pro-cluster/nodes/0/indices/logstash-adm-syslog-2014.04.07/0/index/write.lock]; ]]
```
</comment><comment author="s1monw" created="2014-07-23T15:02:57Z" id="49886222">this has been fixed with Lucene 4.8 upgrade
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add time based UUIDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6004</link><project id="" key="" /><description>See #5941.

Using SecureRandom as a UUID generator is slow and doesn't allow us
to take adavantage of some lucene optimizations around ids with common
prefixes. This commit will allow us to use a
timestamp64bit-macAddr-counter UUID. Since the macAddr may be shared
among several nodes running on the same hardware we use an xor of the
macaddr with a SecureRandom number generated on startup.
</description><key id="32614540">6004</key><summary>Add time based UUIDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-05-01T13:27:28Z</created><updated>2014-09-02T13:13:35Z</updated><resolved>2014-09-02T13:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-01T13:38:33Z" id="41910313">can we add a unit test that verifies the shared prefix behavior of this new UUID?
</comment><comment author="kimchy" created="2014-05-01T14:42:55Z" id="41915840">LAST_TIMESTAMP handling is not thread safe, but at the end, an 8 bytes incremental sequence number with salted mac address should protect from time shifts and restarts. I wonder if we can remove it completely? Need to think about this. 

Also, now we are moving to 22 byte UID, compared to previously having 16 byte UID, potentially, the sequence number can be smaller, but then, we might need to make sure we generate / make it incremental within the same timestamp as the seq generation scope is smaller which entails synchronization. 
</comment><comment author="kimchy" created="2014-05-01T14:51:41Z" id="41916745">also, the mac address code is flaky, its not only on OSX that we can potentially fail, but also on systems that are virtualized, ... . Check https://github.com/cowtowncoder/java-uuid-generator/blob/3.0/src/main/java/com/fasterxml/uuid/EthernetAddress.java for inspiration.
</comment><comment author="GaelTadh" created="2014-05-02T14:19:20Z" id="42036801">I've pushed some new changes including the unit tests. @s1monw I can remove the ByteBuffer stuff if we really want to and add our own. I'm pulling things into a singleton now. 
</comment><comment author="mikemccand" created="2014-08-31T10:18:27Z" id="53983638">Thanks @pickypg, I started from @GaelTadh patch and then folded in
most of your &amp; my feedback into a new branch here:

  https://github.com/mikemccand/elasticsearch/commit/e5e21b752d25cd6253d47c8e46030e84362ae34f

The one thing I didn't do is move the singleton to UUIDGenerator
interface; I think Strings.\* APIs is good place to expose access to
these different generators?

I removed the PaddedAtomicLong: I think it's overkill here, and a
distraction.

I left the CAS loop to ensure lastTimestamp grows monotonically, but
tried to simplify it.

I removed APIs to parse out the time-based UUID: it's supposed to be
opaque to callers.

I broke out SecureRandomHolder as its own package private class.

I dropped sequence number from 8 bytes to 3, and now increment the
timestamp when the bottom two bytes are 0, to deal with a biggish
clock slip backwards while JVM is up.  I also randomized the sequence
id on init for best effort protection of backwards clock-slip while
JVM is down.
</comment><comment author="pickypg" created="2014-08-31T15:25:11Z" id="53991148">@mikemccand LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Benchmark API needs integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6003</link><project id="" key="" /><description>The benchmark API needs thorough tests for submitting, listing, and aborting benchmarks.
</description><key id="32594260">6003</key><summary>Test: Benchmark API needs integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-01T04:45:41Z</created><updated>2014-09-27T08:38:05Z</updated><resolved>2014-05-07T21:18:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Document how to detect and avoid duplicate data?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6002</link><project id="" key="" /><description>Hi,

This is about documentation probably in tips and tricks section. Sometimes there could be duplicates in documents. Documentation about
1. How do we detect duplicates (pre and post index)
2. How to avoid/remove them?
3. Is it possible to have a document in more than one type? {"title": "Doc title", "_type": ["type1", "type2"]}

I used document content (fields) to generate a Hash and stored it with document. Used the computed hash to detect dupplicates and avoided adding them again.
</description><key id="32588643">6002</key><summary>Document how to detect and avoid duplicate data?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abibell</reporter><labels /><created>2014-05-01T01:28:59Z</created><updated>2015-09-07T19:53:02Z</updated><resolved>2014-12-30T17:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T17:54:26Z" id="68379199">Hi @abibell 

Sorry it has taken a while to get to this.  It sounds like you're using a good approach.  At the end of the day, there is only one unique field in Elasticsearch: the `_id`.  Content deduplication is a big and complex subject, and probably worthy of several blog posts.
</comment><comment author="ChrisMagnuson" created="2015-09-07T19:53:02Z" id="138368335">I would also like to see a documented strategy recommending what to do to avoid duplicate data. Is there somewhere else we should be submitting a request to create documentation on this topic?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use non analyzed token stream optimization everywhere</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6001</link><project id="" key="" /><description>In the string type, we have an optimization to reuse the StringTokenStream on a thread local when a non analyzed field is used (instead of creating it each time). We should use this across the board on all places where we create a field with a String.
Also, move to a specific XStringField, that we can reuse StringTokenStream instead of copying it.
</description><key id="32562970">6001</key><summary>Use non analyzed token stream optimization everywhere</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-30T18:52:52Z</created><updated>2015-06-07T13:37:27Z</updated><resolved>2014-04-30T21:19:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-04-30T19:26:36Z" id="41838907">I don't know much about Elasticsearch's thread churn, but maybe CloseableThreadLocal is interesting here? It is annoying as it seems to produce ghosts in profilers (https://issues.apache.org/jira/browse/LUCENE-4474) as it calls a native method when it purges, but i played some more and this does seem to be a ghost.
</comment><comment author="mikemccand" created="2014-04-30T19:31:47Z" id="41839543">Patch looks good!  Maybe add a comment that StringTokenStream comes from Lucene?

We are not nervous about StringTokenStream always hanging onto the last value it indexed?  I imagine these values are typically small ...
</comment><comment author="kimchy" created="2014-04-30T20:56:13Z" id="41849331">@rmuir the problem with `CloseableThreadLocal` is that we use a static thread local, and there isn't a good place to close it. Even when closing a node, there might be other nodes running in the JVM (like in our test infra). We could go with non static thread local, but then there will be just too many of those... . I think its ok for this case. Also, because we have a fully defined thread pool model for all the places that might use it (index/bulk thread pool), I am good with it.

@mikemccand I think its evident that `StringTokenStream`, if you think docs are needed, I will add it. Regarding the value, yea, I don't think it will be a big problem, but it would be nice if in Lucene, maybe the value can be explicitly nullified post usage? We can create a temp class that mirrors `StringTokenStream` on ES side till its done, not sure its needed though.
</comment><comment author="mikemccand" created="2014-04-30T20:59:17Z" id="41849758">@kimchy OK it's fine with me to skip the comment ...
</comment><comment author="rmuir" created="2014-04-30T21:01:40Z" id="41850059">@kimchy I think CloseableThreadLocal has a confusing name actually (and documentation). 

I am not speaking of its close() properties here, instead the fact that it has a built-in garbage collection mechanism: it periodically purges (this is amortized over the get()s, as a multiplier \* number of threads enrolled) 
</comment><comment author="kimchy" created="2014-04-30T21:05:16Z" id="41850509">@rmuir ahh, yea, agreed, it is still not requires in ES because of the bounded threads that will be allowed to use it? I can easily change it, but the main merit of using it in the context of ES is actually using close where we can (because threads don't come and go)
</comment><comment author="rmuir" created="2014-04-30T21:09:02Z" id="41850925">Yeah i am unsure if its appropriate here or not. The only benefit really would be that the "GC" of threadlocal here would be consistent with what is happening with Analyzed fields too (since it uses CTL)
</comment><comment author="kimchy" created="2014-04-30T21:09:35Z" id="41850987">@rmuir kk, will add it as an additional safety measure
</comment><comment author="kimchy" created="2014-04-30T21:10:38Z" id="41851120">added, if all is good, will push it soonish
</comment><comment author="rmuir" created="2014-04-30T21:14:51Z" id="41851604">+1 

I will work with mike to fix this guy in lucene too.
</comment><comment author="mikemccand" created="2014-04-30T21:52:32Z" id="41855798">@kimchy Turns out StringTokenStream already sets value=null in its close method ...
</comment><comment author="kimchy" created="2014-04-30T21:56:50Z" id="41856270">ahh, cool!
</comment><comment author="mikemccand" created="2014-04-30T22:04:47Z" id="41857101">I opened https://issues.apache.org/jira/browse/LUCENE-5634 to get this into Lucene.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `include`/`exclude` support to global ordinals based terms and significant terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6000</link><project id="" key="" /><description /><key id="32554492">6000</key><summary>Add `include`/`exclude` support to global ordinals based terms and significant terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-30T17:06:31Z</created><updated>2015-06-07T13:37:53Z</updated><resolved>2014-05-12T11:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T14:13:00Z" id="42036215">I like the approach in general but I think it would be more efficient to compute the ordinals that match the include/exclude in the factory? This way, they would be computed only once for all the aggregators that are created by this factory (and there can be a lot of them for a 2nd level terms aggregation).
</comment><comment author="martijnvg" created="2014-05-02T15:01:53Z" id="42041086">You're absolutely right, the ordinals that match with include/exclude should be computed in the factories. I'll change this.
</comment><comment author="martijnvg" created="2014-05-08T09:37:20Z" id="42530212">@jpountz I moved the computation of the included global ordinals to the factory.
</comment><comment author="jpountz" created="2014-05-09T07:41:58Z" id="42640923">I left a few comments but it's very close IMO.
</comment><comment author="martijnvg" created="2014-05-09T12:44:37Z" id="42661638">@jpountz I updated the PR, the includeExclude logic for global ordinals based terms and significant_terms are now an implementation detail of Ordinals.Docs.
</comment><comment author="jpountz" created="2014-05-09T13:11:37Z" id="42663888">I just figured out there is an issue if you mutate the `ValuesSource` in place: values sources are cached at the `AggregationContext` level so that aggregators that are created on the same level would share the same ValuesSource object. So for example if you have two terms aggregations on the same level with different values for `include` or `exclude`, the current PR would make both of them use the same.

This design around caching is not so nice but would be a big change, so for now maybe we should just keep constructing the bit set of matched ords on each aggregator creation (it's still much better than not using ordinals at all) and open a new issue to refactor aggregator factories so that we could make things cleaner?
</comment><comment author="martijnvg" created="2014-05-09T13:16:08Z" id="42664270">Ouch... ok then for now lets then not cache the bitset. I'll update the PR.

On 9 May 2014 20:11, Adrien Grand notifications@github.com wrote:

&gt; I just figured out there is an issue if you mutate the ValuesSource in
&gt; place: values sources are cached at the AggregationContext level so that
&gt; aggregators that are created on the same level would share the same
&gt; ValuesSource object. So for example if you have two terms aggregations on
&gt; the same level with different values for include or exclude, the current
&gt; PR would make both of them use the same.
&gt; 
&gt; This design around caching is not so nice but would be a big change, so
&gt; for now maybe we should just keep constructing the bit set of matched ords
&gt; on each aggregator creation (it's still much better than not using ordinals
&gt; at all) and open a new issue to refactor aggregator factories so that we
&gt; could make things cleaner?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/6000#issuecomment-42663888
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="martijnvg" created="2014-05-09T18:05:57Z" id="42695864">@jpountz I moved the global ordinal based include exclude logic to IncludeExclude class. Also by putting it there caching seems to be possible?
</comment><comment author="jpountz" created="2014-05-09T19:53:53Z" id="42706798">It doesn't feel very clean but it should work indeed.
</comment><comment author="jpountz" created="2014-05-12T11:01:36Z" id="42819222">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to write a custom river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5999</link><project id="" key="" /><description>I'm making my first go at writing a river. (Here's the source code: https://bitbucket.org/futurechan/example-river/src)

I followed this tutorial 
http://blog.trifork.com/2013/01/10/how-to-write-an-elasticsearch-river-plugin/

and compared it to this existing river
https://github.com/jprante/elasticsearch-river-jdbc

but I haven't had much luck.

To deploy the river, I created a folder called example-river under plugins, dropped my jar in that folder, and restarted the node. Everything starts up fine.

I have also tried `bin/plugin --url file:///path/to/plugin --install example-river`, which seems to work, but it unpacks my jar. So, I tried zipping it first and then installing, which works and does not unpack my jar, but it didn't help.

When I issue this PUT request:

```
http://localhost:9200/_river/example_river/_meta
{
    "type": "example_river",
  "example_river":{
    "blah":"blah"
  }
}
```

I get this exception:

```
[2014-04-20 22:28:46,538][DEBUG][river ] [Gloom] creating river [example_river][example_river] 
[2014-04-20 22:28:46,543][WARN ][river ] [Gloom] failed to create river [example_river][example_river] org.elasticsearch.common.settings.NoClassSettingsException: Failed to load class with value [example_river] at 
org.elasticsearch.river.RiverModule.loadTypeModule(RiverModule.java:87) at 
org.elasticsearch.river.RiverModule.spawnModules(RiverModule.java:58) at 
org.elasticsearch.common.inject.ModulesBuilder.add(ModulesBuilder.java:44) at 
org.elasticsearch.river.RiversService.createRiver(RiversService.java:137) at 
org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:275) at 
org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:269) at 
org.elasticsearch.action.support.TransportAction$ThreadedActionListener$1.run(TransportAction.jav
a:93) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at 
java.lang.Thread.run(Unknown Source) Caused by: java.lang.ClassNotFoundException: example_river at java.net.URLClassLoader$1.run(Unknown Source) at 
java.net.URLClassLoader$1.run(Unknown Source) at 
java.security.AccessController.doPrivileged(Native Method) at 
java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at 
java.lang.ClassLoader.loadClass(Unknown Source) at 
org.elasticsearch.river.RiverModule.loadTypeModule(RiverModule.java:73) ... 9 more
```

Can someone point out what I am missing?
</description><key id="32532279">5999</key><summary>How to write a custom river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">futurechan</reporter><labels /><created>2014-04-30T12:59:46Z</created><updated>2014-05-01T18:39:39Z</updated><resolved>2014-04-30T18:04:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-30T18:04:13Z" id="41828741">First thing I noticed is that your `es-plugin.properties` contains the class name only, while it should contain the fully qualified with the package too. I think your classes don't get loaded. May I ask you to post these types of question on our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch)? It seems more appropriate unless you find issues in elasticsearch itself that we need to fix.
</comment><comment author="futurechan" created="2014-05-01T18:39:39Z" id="41941698">@javanna Will do. My class is in the default package. Would that be the problem?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant terms misses some terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5998</link><project id="" key="" /><description>When defining a "min_doc_count" and a "size", significant terms might fail to return the right significant terms. The scenario is as follows:
1. There are at least size \* 2 terms that have a subsetDF &lt; min_doc_count 
2. There are terms that have a subsetDF &gt; min_doc_count 
3. Terms in 1. score higher than terms in 2.

Under these circumstances no significant term is returned at all (test is in the branches below). This makes it hard to use significant terms for processing natural text which contains many low frequent terms.

I think the reason for this behavior is that internally a priority queue is maintained that has a maximum of 2*size entries. This queue uses the score as criterion to determine if a bucket is kept or not. Because terms in 1. score higher than terms in 2., only documents with subsetDF &lt; min_doc_count  will be kept and therefore finally no terms are returned at all.

It is unclear to me how to fix this. I prepared two potential ways to fix this (not nearly ready for a pr, more proof of concept):

A. assign score -MAX_FLOAT if subsetDf &lt; minDocCount
This would cause terms with a too low subsetDF to be added with a very low score. These terms still would get a chance to be returned if by [merging](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java#L151) their subsetDf increases. However, they might be thrown out of the priority queue before that by terms with a higher score and never be merged at all.

Branch: https://github.com/brwe/elasticsearch/tree/missing-sig-terms-1

B. Do not add terms to priority queue if subsetDf &lt; minDocCount
Easier to code but unfortunately this means that terms that might match the minDocCount criterion after merging would not be added in the first place.

Branch: https://github.com/brwe/elasticsearch/tree/missing-sig-terms-2

Both ways do not guarantee that the "correct" significant terms are returned. 
I would prefer B.
</description><key id="32531001">5998</key><summary>Significant terms misses some terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-04-30T12:41:48Z</created><updated>2014-05-13T08:01:54Z</updated><resolved>2014-05-07T16:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-05-02T08:30:08Z" id="42002984">Thanks for looking at this, Britta. The `shard_size` setting controls the size of the shard-local PriorityQueue and defaults to a multiple of the final `size` requested by the end user to help improve accuracy. 
The `terms` aggregation employs a similar strategy but for the reasons you point out I use a larger multiplier for the default `shard_size` in `significant_terms` because that algo is not as certain as the terms aggregation in the selections it gathers from each shard.
Have you tried experimenting with a bigger `shard_size` to fix the accuracy issues? The best choice of shard_size will depend on your index sharding policy, the data you have stored, the chosen query/agg and the amount of RAM/network traffic you want to throw at the problem.
Before applying the tweaks you suggest I think it would make sense to try understand the problem better (although for the reasons I outlined above "the problem" can vary wildly). I think it would be useful to measure how many of "the right" terms found in a single-shard scenario were not found in a multi-shard scenario where we perhaps vary the number of shards and the shard_size setting. If we find we have to increase shard_size to unreasonably large settings we could employ some of the techniques you suggest and again measure the improvement based on how many of the correct results it returns.
</comment><comment author="brwe" created="2014-05-02T11:52:34Z" id="42022065">My bad, did not try the `shard_size` parameter  - I can solve this problem for my case it I set `shard_size` to something like 30X the size parameter which is 100. 

A little more explanation: In my particular case I was trying to get the significant terms for wikipedia articles (only partially indexed) that contain a particular term in the title, in this case "shoe". I have only one shard.
If I try getting the top 100 terms (`min_doc_count` is 10) then with the default settings only one term is returned. I actually get 100 terms returned if I set the `shard_size` to 3k. 

Not sure if 3k is unreasonable and I also only tested tis one case. In addition, I do not know the true significance of the terms that are returned. The documents (only 254 contained "shoe" in the title) did not contain many significant terms anyway, so most of the 100 terms did not make sense and also scored low.

I am still wondering if it makes sense use the `min_doc_count` parameter for determining if a candidate should be added to the pq or not. The reason is this:
I cannot know in advance how many terms with a low `min_doc_count` will score higher than the one I with a high `min_doc_count`. Therefore this parameter must be tuned which might be tricky, since it can also depend on `size` and `min_doc_count`.

On the other hand, if there is no routing of docs to shards involved, I can maybe assume that the documents of classes and also the terms therein are distributed evenly across shards. So in that case it might be easier to not add documents to the pq that have `subsetDF &lt;= min_doc_count/number_of_shards` (maybe not exactly but something similar) because I would assume that even when summing up the subsetDF across shards `min_doc_count` will not be reached. However, I am yet unsure if this would be more easy than setting the `shard_size` parameter properly - might depend on how rare the sought terms are and how random distribution of docs across shards really is.

Anyway, this is just and idea. We could proceed with formal measurements but this might be tricky and time consuming. If 3k is reasonable for the pq size we can also close this issue for now.
</comment><comment author="markharwood" created="2014-05-02T13:01:53Z" id="42028874">So if `shard_size` is a shard-local equivalent for the final `size` then maybe we can also introduce `shard_min_doc_count`  as an optional shard-level policy for controlling quality?

One thing to watch with Wikipedia - there tend to be a lot of very short docs that have no real wordy content on a theme but instead serve as disambiguation or synonym type entries. It can be important to tune these out from these sorts of analysis.
</comment><comment author="brwe" created="2014-05-05T07:30:44Z" id="42164644">I like the `shard_min_doc_count` parameter. I'll prepare a pull request for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete mapping sometimes hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5997</link><project id="" key="" /><description>This test fails occasionally when running the Perl test suite:

https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/indices.delete_mapping/10_basic.yaml

Essentially, the test creates three indices, doesn't wait for status yellow, then runs a delete mapping request.  Normally this succeeds but sometimes it just hangs. After 30s my Perl client times out, and shortly thereafter I see a null pointer exception in Elasticsearch. 

Logs of requests from the Perl client, and trace logs from Elasticsearch are available here: https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9

I think the problem is that not all shards are live when the delete mapping request starts, so the delete-by-query request never runs on the unstarted shards.  See https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9#file-eslogs-L1353 Elasticsearch waits for successful responses from all delete-by-query requests, and so just hangs until the requests time out, and we get the NPE: https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9#file-eslogs-L1642
</description><key id="32524967">5997</key><summary>Delete mapping sometimes hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-04-30T10:55:28Z</created><updated>2015-03-19T15:43:48Z</updated><resolved>2014-09-25T11:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-09T10:08:01Z" id="42651185">Temporarily added a wait-for-yellow to the delete_mapping YAML tests. Once this issue has been fixed, can revert:
- branch 1.0 3a4fe61e8e911b1b63a53489297c1832dc99a922
- branch 1.1 8b4ad897c12a0ace8f45dcd05c3e61da98a9b00b
- branch 1.x 70662004e003932902e99c143f2fc3da744d98f1
- master a972aaa7aec7419197d08dce6eaea1d87422846e
</comment><comment author="GaelTadh" created="2014-09-25T11:12:18Z" id="56804285">I think this was fixed by #7744. If we see it again reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>s/boost_factor/boost in custom_filters_score doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5996</link><project id="" key="" /><description>I may be wrong but I think `custom_filters_score` used `boost` rather than `boost_factor`?
</description><key id="32523475">5996</key><summary>s/boost_factor/boost in custom_filters_score doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2014-04-30T10:28:46Z</created><updated>2014-09-08T09:27:19Z</updated><resolved>2014-05-06T14:16:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:16:03Z" id="42307235">You are correct - merged thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score parser should throw exception if both `functions:[]` and single `function` given</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5995</link><project id="" key="" /><description /><key id="32519680">5995</key><summary>Function score parser should throw exception if both `functions:[]` and single `function` given</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-30T09:29:18Z</created><updated>2015-06-07T13:38:27Z</updated><resolved>2014-05-02T09:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-04-30T10:01:22Z" id="41779833">We might want to consider treating the following case as a special case

```
...
"functions": [
...
], 
"boost_factor": some number
...
```

because the name "boost" and "boost_factor" are pretty similar. The error message could be the same with the addition "... Did you mean "boost" instead?"
</comment><comment author="jpountz" created="2014-04-30T12:25:04Z" id="41790088">LGTM

&gt; The error message could be the same with the addition "... Did you mean "boost" instead?"

This would be nice indeed. I can easily see some confusion here.
</comment><comment author="brwe" created="2014-04-30T16:47:19Z" id="41820058">Thanks for the review! Iadded two commits implementing the suggestions.
</comment><comment author="brwe" created="2014-05-02T08:25:01Z" id="42002587">OK, added new commit to remove the duplicate code and make the strings constant 
</comment><comment author="jpountz" created="2014-05-02T08:39:27Z" id="42003803">Apart from a minor formatting issue, this looks good to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lower the initial sizing of sub aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5994</link><project id="" key="" /><description>We currently compute initial sizings based on the cardinality of our fields.
This can be highly exagerated for sub aggregations, for example if there is a
parent terms aggregation that is executed over a field that has a very long
tail: most buckets will only collect a couple of documents.
</description><key id="32517802">5994</key><summary>Lower the initial sizing of sub aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-30T09:00:31Z</created><updated>2015-06-08T15:04:26Z</updated><resolved>2014-05-06T15:37:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-06T15:19:15Z" id="42316160">@uboness I just pushed a new commit to address your comments
</comment><comment author="uboness" created="2014-05-06T15:20:27Z" id="42316330">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update phrase-suggest.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5993</link><project id="" key="" /><description>Grammatical error
</description><key id="32516100">5993</key><summary>Update phrase-suggest.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">audreyrsc</reporter><labels /><created>2014-04-30T08:34:16Z</created><updated>2014-07-16T21:45:50Z</updated><resolved>2014-05-06T08:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-30T12:37:18Z" id="41791187">+1
</comment><comment author="jpountz" created="2014-05-02T21:50:02Z" id="42082635">@audreyrsc Would you mind signing our [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can merge this change in?
</comment><comment author="audreyrsc" created="2014-05-03T12:05:27Z" id="42103343">Hi Adrien !

I just sign it.

De : Adrien Grand [mailto:notifications@github.com] 
Envoy&#233; : vendredi 2 mai 2014 23:50
&#192; : elasticsearch/elasticsearch
Cc : Audrey
Objet : Re: [elasticsearch] Update phrase-suggest.asciidoc (#5993)

@audreyrsc https://github.com/audreyrsc  Would you mind signing our contributor license agreement http://www.elasticsearch.org/contributor-agreement/  so that I can merge this change in?

&#8212;
Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/pull/5993#issuecomment-42082635 .  https://github.com/notifications/beacon/2867271__eyJzY29wZSI6Ik5ld3NpZXM6QmVhY29uIiwiZXhwaXJlcyI6MTcxNDY4NjYyNywiZGF0YSI6eyJpZCI6MzExMTg4NzF9fQ==--8181e143ec8c33f868df368058d8f541701e7cc9.gif 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't start the elasticsearch server after upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5992</link><project id="" key="" /><description>When i upgraded the elasticsearch from 1.1.0 to 1.1.1, some nodes can't start at all.
I found some error messages in node logs,such as:

```
[2014-04-30 14:49:18,134][ERROR][bootstrap                ]
 {1.1.1}: Initialization Failed ...
1) A binding to org.elasticsearch.indices.analysis.smartcn.SmartChineseIndicesAnalysis was already configured at _unknown_.
ubuntu@np8:/mnt/avos/logs/elasticsearch$ /usr/share/elasticsearch/bin/elasticsearch -v
```

I don't know the meaning of `SmartChineseIndicesAnalysis was already configured at _unknown_`, any help? Thanks.
</description><key id="32510877">5992</key><summary>Can't start the elasticsearch server after upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">killme2008</reporter><labels /><created>2014-04-30T06:57:00Z</created><updated>2015-08-27T11:02:24Z</updated><resolved>2014-04-30T08:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-30T07:36:19Z" id="41768527">Thanks for reporting it.

Which plugins and versions are you using?
</comment><comment author="killme2008" created="2014-04-30T07:46:54Z" id="41769205">I am using `elasticsearch-analysis-smartcn-2.1.0.jar`  

https://github.com/elasticsearch/elasticsearch-analysis-smartcn
</comment><comment author="killme2008" created="2014-04-30T08:16:51Z" id="41771277">I found the problem.Because we have  two different versions of smartcn in the classpath,and the elasticsearch used the old version in some nodes.After deleted the old version jar file, it's ok right now.

But thanks a lot for your reply.
</comment><comment author="dadoonet" created="2014-04-30T08:35:24Z" id="41772685">Ha! Cool! Thanks for the follow up !
</comment><comment author="AdeMiller" created="2015-08-26T21:32:35Z" id="135176694">FWIW I'm seeing a similar issue with my upgrade from 1.4.5 to 1.7.1 

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) A binding to org.elasticsearch.license.core.LicenseVerifier was already configured at _unknown_.
  at _unknown_

2) A binding to org.elasticsearch.license.plugin.core.LicensesService was already configured at _unknown_.
  at _unknown_

2 errors
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:151)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:210)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:77)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:245)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```

This was because the `plugins` directory contained a `license` and `licence` plugin. My guess is that at some point a plugin got renamed and then I installed the new one but failed to remove the old one.
</comment><comment author="clintongormley" created="2015-08-27T11:02:24Z" id="135384553">thanks @AdeMiller - in 2.0, you'll get a JarHell exception which will make this problem a lot easier to detect
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No function with the name [DECAY_FUNCTION] is registered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5991</link><project id="" key="" /><description>My es version is 1.0.0, I just try the code in http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/query-dsl-function-score-query.html 
curl 'localhost:9200/hotels/_search/' -d '{
"query": {
    "function_score": {
        "functions": [
            {
                "DECAY_FUNCTION": {
                    "price": {
                        "origin": "0",
                        "scale": "20"
                    }
                }
            },
            {
                "DECAY_FUNCTION": {
                    "location": {
                        "origin": "11, 12",
                        "scale": "2km"
                    }
                }
            }
        ],
        "query": {
            "match": {
                "properties": "balcony"
            }
        },
        "score_mode": "multiply"
    }
}
}'

and got error below:
{[VSYPe_leQOy4iIxVwCJt2w][xmu_search][3]: SearchParseException[[xmu_search][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"function_score":{"functions":[{"DECAY_FUNCTION":{"price":{"origin":"0","scale":"20"}}},{"DECAY_FUNCTION":{"location":{"origin":"11, 12","scale":"2km"}}}],"query":{"match":{"properties":"balcony"}},"score_mode":"multiply"}}}]]]; nested: QueryParsingException[[xmu_search] No function with the name [DECAY_FUNCTION] is registered.]; }]
</description><key id="32501260">5991</key><summary>No function with the name [DECAY_FUNCTION] is registered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">neversion</reporter><labels /><created>2014-04-30T02:10:35Z</created><updated>2015-06-26T03:48:26Z</updated><resolved>2014-04-30T18:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-30T02:14:54Z" id="41754028">the docs state `DECAY_FUNCTION can be "linear", "exp" and "gauss"`, you need to replace it with either `linear`, `exp`, or `gauss`. Btw, best place to ask questions is the mailing list, and if its an actual issue, it can be opened here.
</comment><comment author="javanna" created="2014-04-30T18:06:47Z" id="41829100">Closing this issue as replacing `DECAY_FUNCTION` with one of the actual functions should fix it.
</comment><comment author="dwenaus" created="2014-06-19T18:24:56Z" id="46597989">I had the same problem. It would be helpful if the examples were complete examples. for example:
`"gauss": {
"location": {
"origin": "11, 12",
"scale": "2km"
}}`
</comment><comment author="jbardu" created="2015-06-26T03:48:26Z" id="115492437">I had the same issue. I hesitated as DECAY_FUNCTION is not a normal lower case command but forged on anyway because all the "examples" used it. I feel dumb now.

The docs should be changed to make examples real examples, and also I've noticed the tendency of an "example" to be used out of the context of the container it should be in. Preferably at least once in each page for a command there should be a full working example pasted in? that includes any outer levels, even if it is just a match_all ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default filter cache to 10% and circuit breaker to 60%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5990</link><project id="" key="" /><description>The defaults we have today in our data intensive memory structures don't properly add up to properly protected from potential OOM.
The circuit breaker, today at 80%, aims at protecting from extensive field data loading. The default threshold today is too permissive and can still cause OOMs.
 The filter cache today is at 20%, and its too high when adding it to other limits we have, reduce it to 10%, which is still a big enough portion of the heap, yet provides improved safety measure.
</description><key id="32488167">5990</key><summary>Change default filter cache to 10% and circuit breaker to 60%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T21:44:53Z</created><updated>2015-06-06T16:59:54Z</updated><resolved>2014-05-04T13:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-04-29T23:13:17Z" id="41743754">LGTM. Are there perhaps any functional tests that exist to hammer these kind of settings?
</comment><comment author="kimchy" created="2014-04-29T23:14:36Z" id="41743838">there aren't, but we discussed potentially doing static check to verify that out of the current limits we have, the sum of the defaults of them does not exceed a certain threshold. So if we change one, we don't go overboard with it.
</comment><comment author="jpountz" created="2014-04-30T06:19:06Z" id="41764277">LGTM
</comment><comment author="pickypg" created="2014-04-30T14:23:24Z" id="41802098">@kimchy What kind of loads do we expect this behavior to occur at (amounts of data and number/size of concurrent queries)? I might have some time this weekend to try to make a CI-style test for this behavior moving forward that we can use to test memory and such.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_histogram aggregation's extended_bounds doesn't allow Date Math</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5989</link><project id="" key="" /><description>When using a date_histogram aggregation, the interval is specified using elasticsearch Date Math. extended_bounds is however still specified using longs for min and max. This makes extended_bounds not practical as the exact start and stop aren't necessarily known ahead of time (e.g. interval: {from: "now-15m"}). 
</description><key id="32484582">5989</key><summary>date_histogram aggregation's extended_bounds doesn't allow Date Math</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emidln</reporter><labels /><created>2014-04-29T20:58:30Z</created><updated>2014-05-12T17:02:36Z</updated><resolved>2014-05-12T17:02:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-30T13:36:43Z" id="41796713">the `extended_bounds.min` &amp; `extended_bounds.max` do accept date math string expressions as well. eg:

``` json
"aggs" : {
    "trend" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "1d",
            "extended_bounds" : {
                "min" : "now-3M",
                "max" : "now"
            }
        }
    }
}
```
</comment><comment author="emidln" created="2014-05-12T17:02:36Z" id="42858819">Closing as I was in error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a sandboxed scripting language</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5988</link><project id="" key="" /><description>It'd be nice to be able to have dynamic scripting enabled by default again, but only for well sandboxed languages.  Are there any scripting languages with good sandboxing and good enough performance to run in function score queries?

I've seen some systems use Lua for its good sandbox and speed but I don't know how good any of the JVM implementation's are in those respects.  Javascript also sounds like it could be a natural choice with all the JSON floating around.  I wouldn't be surprised if there are several languages that fit the bill.

Sorry if this is a dupe, but I hadn't seen much conversation on this and figured it'd be useful.
</description><key id="32480735">5988</key><summary>Add a sandboxed scripting language</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-04-29T20:16:42Z</created><updated>2014-07-16T11:55:10Z</updated><resolved>2014-07-10T18:40:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-05T14:17:55Z" id="42192295">+1, this would be great since #5853 has landed; adding an option for allowing either no dynamic scripting, sandboxed scripting, or all scripting.
</comment><comment author="javanna" created="2014-06-03T11:31:16Z" id="44952594">That's what we are trying to address in #6233, using sandboxed groovy as a scripting language.
</comment><comment author="nik9000" created="2014-07-10T18:37:31Z" id="48645683">I believe this can be closed now that #6233 is merged?
</comment><comment author="dakrone" created="2014-07-10T18:40:11Z" id="48646023">Yep definitely, thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not possible to disable bulk size in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5987</link><project id="" key="" /><description>bulkSize will never equal -1, so we need to check for bulkSize.getBytes() instead.
</description><key id="32480400">5987</key><summary>Not possible to disable bulk size in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jplock</reporter><labels /><created>2014-04-29T20:12:45Z</created><updated>2014-07-16T21:45:51Z</updated><resolved>2014-04-30T18:31:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jplock" created="2014-04-29T20:21:48Z" id="41727040">I've signed the CLA
</comment><comment author="jplock" created="2014-04-30T17:48:10Z" id="41826864">Ahh you're right.  So if we set the bulkSize to -1, and the bulkActions to -1 and use a flushInterval to 1, then we should have 1 second asynchronous flushes, correct?
</comment><comment author="pickypg" created="2014-04-30T21:02:46Z" id="41850207">@jplock Yeah. Also, if you were to use a `null` `flushInterval`, then you would have to manually invoke `flush()`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix setting of readerGen in BytesRefOrdValComparator on nested documents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5986</link><project id="" key="" /><description>Sorting was broken on nested documents because the `missing(slot)` method
didn't correctly set the segment ordinal (readerGen), causing term ordinals to
be compared across segments.
</description><key id="32471331">5986</key><summary>Fix setting of readerGen in BytesRefOrdValComparator on nested documents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>blocker</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T18:20:32Z</created><updated>2015-06-07T20:13:14Z</updated><resolved>2014-04-30T06:46:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update index.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5985</link><project id="" key="" /><description>Change to use elasticsearch-lang-groovy
</description><key id="32461932">5985</key><summary>Update index.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YannBrrd</reporter><labels /><created>2014-04-29T16:32:47Z</created><updated>2014-06-25T03:25:09Z</updated><resolved>2014-05-14T10:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:12:15Z" id="42306785">@dakrone is this correct?
</comment><comment author="clintongormley" created="2014-05-14T10:09:39Z" id="43063361">Groovy docs fixed.  Thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch groovy client for ES 0.90.X</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5984</link><project id="" key="" /><description>Hi

elasticsearch-client-groovy seems not te be ok with 0.90.X branch. Whatever the [version](http://search.maven.org/#search|gav|1|g%3A%22org.elasticsearch%22%20AND%20a%3A%22elasticsearch-client-groovy%22) I tired, I got NoClassDefFoundErrors

Output for 0.10.0 :

``` ruby
avr. 29, 2014 6:12:13 PM org.elasticsearch.plugins
Infos: [Deadhead] loaded [], sites []

java.lang.NoClassDefFoundError: org/elasticsearch/client/action/deletebyquery/DeleteByQueryRequestBuilder
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
    at java.lang.Class.getDeclaredMethods(Class.java:1855)
    at org.codehaus.groovy.reflection.CachedClass$3$1.run(CachedClass.java:84)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:81)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:79)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at org.codehaus.groovy.reflection.CachedClass.getMethods(CachedClass.java:250)
    at groovy.lang.MetaClassImpl.populateMethods(MetaClassImpl.java:342)
    at groovy.lang.MetaClassImpl.fillMethodIndex(MetaClassImpl.java:292)
    at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:3045)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:178)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:194)
    at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:255)
    at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:859)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallConstructorSite(CallSiteArray.java:84)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:57)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
    at com.axa.elasticsearch.groovy.Monitor.init(Monitor.groovy:24)
    at com.axa.elasticsearch.groovy.Monitor$init.call(Unknown Source)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
    at com.axa.elasticsearch.groovy.MonitorTest.testStart(MonitorTest.groovy:10)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.client.action.deletebyquery.DeleteByQueryRequestBuilder
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 49 more
```

Also, it would be kind to update [online doc](http://www.elasticsearch.org/guide/en/elasticsearch/client/groovy-api/0.90/_preface.html) to explain which client version to use, for `${es.version}` is just wrong at the moment.

Thanks,
Yann
</description><key id="32460991">5984</key><summary>Elasticsearch groovy client for ES 0.90.X</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YannBrrd</reporter><labels /><created>2014-04-29T16:22:08Z</created><updated>2014-04-29T16:33:04Z</updated><resolved>2014-04-29T16:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-29T16:23:03Z" id="41698053">Could you please open it in https://github.com/elasticsearch/elasticsearch-lang-groovy project?
</comment><comment author="YannBrrd" created="2014-04-29T16:27:34Z" id="41698662">Sorry David I don't get you. elasticsearch-lang-groovy is a plugin for ES to have scripts running onver ES, no ? I just want a [working Groovy API](http://www.elasticsearch.org/guide/en/elasticsearch/client/groovy-api/0.90/_preface.html) similar to Java API... 
It's different no ? 
</comment><comment author="dadoonet" created="2014-04-29T16:29:19Z" id="41698904">Actually, until elasticsearch 1.0, groovy client and groovy lang plugin were sharing the same repo.

See https://github.com/elasticsearch/elasticsearch-lang-groovy/tree/es-0.90/src/main/groovy

Starting 1.0, we are creating a groovy client repo. It's not open yet.
</comment><comment author="YannBrrd" created="2014-04-29T16:31:36Z" id="41699186">Ok, right. Then I'll test with this one. doc should be fixed then I guess.
</comment><comment author="YannBrrd" created="2014-04-29T16:33:04Z" id="41699380">https://github.com/elasticsearch/elasticsearch/pull/5985
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy client for 0.90.X</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5983</link><project id="" key="" /><description>Hi

elasticsearch-client-groovy seems not te be ok with 0.90.X branch. Whatever the [version](http://search.maven.org/#search|gav|1|g%3A%22org.elasticsearch%22%20AND%20a%3A%22elasticsearch-client-groovy%22) I tired, I got NoClassDefFoundErrors

Ooutput for 0.10.0 : 

``` ruby
avr. 29, 2014 6:12:13 PM org.elasticsearch.plugins
Infos: [Deadhead] loaded [], sites []

java.lang.NoClassDefFoundError: org/elasticsearch/client/action/deletebyquery/DeleteByQueryRequestBuilder
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
    at java.lang.Class.getDeclaredMethods(Class.java:1855)
    at org.codehaus.groovy.reflection.CachedClass$3$1.run(CachedClass.java:84)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:81)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:79)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at org.codehaus.groovy.reflection.CachedClass.getMethods(CachedClass.java:250)
    at groovy.lang.MetaClassImpl.populateMethods(MetaClassImpl.java:342)
    at groovy.lang.MetaClassImpl.fillMethodIndex(MetaClassImpl.java:292)
    at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:3045)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:178)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:194)
    at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:255)
    at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:859)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallConstructorSite(CallSiteArray.java:84)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:57)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
    at com.axa.elasticsearch.groovy.Monitor.init(Monitor.groovy:24)
    at com.axa.elasticsearch.groovy.Monitor$init.call(Unknown Source)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
    at com.axa.elasticsearch.groovy.MonitorTest.testStart(MonitorTest.groovy:10)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.client.action.deletebyquery.DeleteByQueryRequestBuilder
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 49 more
```

Also, it would be kind to update [online doc](http://www.elasticsearch.org/guide/en/elasticsearch/client/groovy-api/0.90/_preface.html) to explain which client version to use, for `${es.version}` is just wrong at the moment.

Thanks,
Yann
</description><key id="32460699">5983</key><summary>Groovy client for 0.90.X</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YannBrrd</reporter><labels /><created>2014-04-29T16:18:22Z</created><updated>2014-06-25T05:47:41Z</updated><resolved>2014-04-29T16:19:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolation request parameters order dependent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5982</link><project id="" key="" /><description>This request returns aggregations:

```
GET /t/mytype/_percolate
{
  "aggs": {
    "colors": {
      "terms": {
        "field": "color"
      }
    }
  },
  "doc": { "foo": "bar baz"}
}
```

This version doesn't:

```
GET /t/mytype/_percolate
{
  "doc": { "foo": "bar baz"},
  "aggs": {
    "colors": {
      "terms": {
        "field": "color"
      }
    }
  }
}
```

/cc @martijnvg 
</description><key id="32456291">5982</key><summary>Percolation request parameters order dependent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-04-29T15:32:11Z</created><updated>2014-04-29T17:35:13Z</updated><resolved>2014-04-29T17:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-29T17:03:36Z" id="41703151">This appears to have already been fixed in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Like This API would not take into account size and from in request body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5981</link><project id="" key="" /><description>Instead these values would always be overridden by the default values of REST parameters search_size and search_from.
</description><key id="32451663">5981</key><summary>More Like This API would not take into account size and from in request body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T14:46:22Z</created><updated>2015-06-07T20:13:43Z</updated><resolved>2014-05-12T10:35:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T21:51:29Z" id="42082742">The fix looks good. Can we also have a test for it?
</comment><comment author="jpountz" created="2014-05-12T10:15:46Z" id="42816006">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fielddata: Switch to the Lucene comparators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5980</link><project id="" key="" /><description>Now that ordinals have been made consistent with Lucene, we should remove our custom comparators and use the Lucene ones.
</description><key id="32448719">5980</key><summary>Fielddata: Switch to the Lucene comparators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T14:15:32Z</created><updated>2014-07-23T18:11:17Z</updated><resolved>2014-07-23T18:10:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-04-29T14:22:53Z" id="41681782">+1
</comment><comment author="clintongormley" created="2014-07-11T08:56:58Z" id="48708525">Bumping to 1.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide meaningful error message if field has no fielddata type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5979</link><project id="" key="" /><description>closes #5930
</description><key id="32443576">5979</key><summary>Provide meaningful error message if field has no fielddata type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T13:12:33Z</created><updated>2015-06-08T15:04:37Z</updated><resolved>2014-04-30T09:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-29T13:17:29Z" id="41674081">LGTM
</comment><comment author="brwe" created="2014-04-30T09:37:36Z" id="41777919">closed with  9d214d1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tie-break suggestions by term</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5978</link><project id="" key="" /><description>OK, DirectCandidateGenerator.CandidateSet.candidates also required
tie-breaking because CandidateScorer.findBestCandidates would change
its results depending on the sort order of the candidates in each
CandidateSet.  And it was tricky because the tests would
intermittently fail since BytesRef.hashCode is different for
every JVM instance...

So I added an Arrays.sort in addCandidates, and fixed Correction and
Candidate to implement Comparable, and fixed CandidateScorer.updateTop
to use .compareTo when checking if the new correction competes with
the top of the PQ.

I folded in Simon's comment as-is into SuggestSearchTests, but I still
don't fully understand the test, e.g. why after
"phraseSuggestion.analyzer(null);" would this get bigram to
participate in scoring again?  (The previous test did
.forceUnigrams(true) which I think is why xorn/xorr were tied for score).
</description><key id="32438711">5978</key><summary>Tie-break suggestions by term</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T11:55:47Z</created><updated>2015-06-07T20:31:20Z</updated><resolved>2014-05-18T20:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-05-18T18:51:44Z" id="43447916">Talked to Simon ... he feels we should fix this for 1.2.
</comment><comment author="s1monw" created="2014-05-18T19:45:45Z" id="43449459">LGTM - can you squash and push?
</comment><comment author="mikemccand" created="2014-05-18T20:30:06Z" id="43450641">OK will do ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose a PluginManager.isPluginInstalled(String name) method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5977</link><project id="" key="" /><description>@nickminutello did a nice work to expose `isInstalled(String)` method in PR #5566.

After reviewing it, I made the `Plugin` class static.
This changes needs also to be reviewed :)
</description><key id="32435196">5977</key><summary>Expose a PluginManager.isPluginInstalled(String name) method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2014-04-29T10:51:51Z</created><updated>2015-08-18T11:27:33Z</updated><resolved>2015-06-05T12:24:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-07T12:25:55Z" id="42420262">Hey @pickypg!

Thanks for the review. I applied most of proposed changes. Looks better now.
Some changes are not applied here as they concern IMHO another issue.

Let me know what you think.
</comment><comment author="pickypg" created="2014-05-08T02:55:17Z" id="42508693">@dadoonet LGTM. It makes sense that those comments will not be addressed here. I have some other really minor comments that should just help you drop a few lines of code and keep it tidier.
</comment><comment author="s1monw" created="2014-06-12T13:02:28Z" id="45888003">@dadoonet can you update the PR title it's very hard to tell what this is about?

@spinscale are you going to review this, can I remove the review tag?
</comment><comment author="s1monw" created="2014-06-18T19:00:49Z" id="46478741">ping?
</comment><comment author="spinscale" created="2014-06-19T06:10:04Z" id="46526716">@s1monw will do it today
</comment><comment author="spinscale" created="2014-06-19T10:23:06Z" id="46544698">First: Please adapt the forbidden API config, so this compiles with maven again.

The change itself LGTM, but I am still in favor of a bigger refactorting of the PluginManager, which IMO needs some cleanup (can be done after this got in of course):
- No need for several `DownloadProgress` impls, if you hand over the logger appropriately (also no need for the `outputMode` then, when the logger is created on startup
- Unify the usage of the plugin class compared with a lot of `File` s being passed around.
- Make the `HttpDownloadHelper` a bit more self-contained (quite a few arguments for downloading)

@dadoonet Is there a differente between `/bin/plugin` and `/bin/plugin -v`? Just setting `-s` seems to have an affect? Or is that some BWC option, we could drop in a major release?
</comment><comment author="clintongormley" created="2014-07-11T09:52:44Z" id="48712986">@dadoonet ping?
</comment><comment author="dadoonet" created="2014-07-23T15:49:26Z" id="49893208">@spinscale `-v` is a DEBUG mode. So it adds lot of information.
`-s` is the exact opposite. It does not output anything. In that mode you could only rely on exit code.

If you don't set `-v` or `-s` you will get back some information while the plugin is installing, like progress bar and success message.

BTW, I'll look at your comments, update the PR and ping you again when ready. Thanks for the review so far! 
</comment><comment author="clintongormley" created="2014-10-20T13:10:24Z" id="59748527">@dadoonet any progress on this?
</comment><comment author="clintongormley" created="2015-06-04T19:15:48Z" id="109016608">@tlrx this PR is over a year old.  Does your plugin work already cover a check for whether a plugin is installed or not? Can we close this PR?
</comment><comment author="clintongormley" created="2015-06-05T12:24:35Z" id="109277639">Chatted to @tlrx - this functionality is included in #9998, so closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin -u url fails silently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5976</link><project id="" key="" /><description>When installing a plugin using `-u` ( `--url`) option, it fails silently when you don't provide `-i` (`--install`)  option:

``` sh
$ bin/plugin -u http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip
$ bin/plugin -l
Installed plugins:
    - No plugin detected in elasticsearch/plugins
```

With `-i`:

``` sh
$ bin/plugin -u http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip -i rssriver
-&gt; Installing rssriver...
Trying http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip...
Downloading ......................................DONE
Installed rssriver into /Users/dpilato/Documents/Elasticsearch/apps/elasticsearch/elasticsearch-2.0.0-SNAPSHOT/plugins/rssriver
$ bin/plugin -l
Installed plugins:
    - rssriver
```
</description><key id="32433121">5976</key><summary>bin/plugin -u url fails silently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label></labels><created>2014-04-29T10:14:22Z</created><updated>2014-07-04T14:22:00Z</updated><resolved>2014-07-04T14:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve the way sub-aggregations are collected.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5975</link><project id="" key="" /><description>Sub-aggregations are currently collected directly, by just forwarding the
doc ID and bucket ordinal to them. This change adds the new BucketCollector
abstract class that Aggregator extends, so that we have more flexibility to
add implicit filters or buffering between an aggregator and its sub
aggregators.
</description><key id="32432978">5975</key><summary>Improve the way sub-aggregations are collected.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T10:11:59Z</created><updated>2015-06-07T13:38:34Z</updated><resolved>2014-04-30T06:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-29T14:03:49Z" id="41679401">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Default analyzer includes stopwords</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5974</link><project id="" key="" /><description>Using the default analyzer:

```
GET /_analyze?text=The fox
```

Removes stopwords:

```
{
   "tokens": [
      {
         "token": "fox",
         "start_offset": 4,
         "end_offset": 7,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      }
   ]
}
```

Using the `standard` analyzer:

```
GET /_analyze?text=The fox&amp;analyzer=standard
```

Keeps stopwords:

```
{
   "tokens": [
      {
         "token": "the",
         "start_offset": 0,
         "end_offset": 3,
         "type": "&lt;ALPHANUM&gt;",
         "position": 1
      },
      {
         "token": "fox",
         "start_offset": 4,
         "end_offset": 7,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      }
   ]
}
```
</description><key id="32432520">5974</key><summary>Analysis: Default analyzer includes stopwords</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T10:04:23Z</created><updated>2014-07-16T11:55:19Z</updated><resolved>2014-05-05T14:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2014-05-04T19:14:03Z" id="42141778">@clintongormley 

Based on #4092 and the below snippet, it appears that the correct behavior is to not remove stop words for versions on or after 1.0.0.Beta1.

```
STANDARD(CachingStrategy.ELASTICSEARCH) { // we don't do stopwords anymore from 1.0Beta on
    @Override
    protected Analyzer create(Version version) {
        if (version.onOrAfter(Version.V_1_0_0_Beta1)) {
            return new StandardAnalyzer(version.luceneVersion, CharArraySet.EMPTY_SET);
        }
        return new StandardAnalyzer(version.luceneVersion);
    }
}
```

The inconsistency here is due to the fact that when an analyzer name isn't specified in the query params, the analyzer isn't being resolved from the pre-built analyzers, but instead from Lucene.STANDARD_ANALYZER which is configured to use stop words.

Perhaps it should be resolved from the pre-built analyzers instead.
</comment><comment author="s1monw" created="2014-05-18T10:06:09Z" id="43435971">I guess we should port this to `1.1.2` as well @spinscale 
</comment><comment author="spinscale" created="2014-05-18T15:58:02Z" id="43443525">done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include thread name when logging IndexWriter's infoStream messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5973</link><project id="" key="" /><description>When I enabled TRACE logging to see IndexWriter's infoStream messages, I noticed the message doesn't show the thread name, which is useful when diagnosing indexing issues ... so I just added it to the message passed to logger.trace.  This seems to work but maybe there's a better way to just ask the logger to include it?
</description><key id="32431818">5973</key><summary>Include thread name when logging IndexWriter's infoStream messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T09:52:27Z</created><updated>2015-06-08T15:04:49Z</updated><resolved>2014-04-29T14:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-29T09:53:00Z" id="41658615">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent results when mixing `fieldname: { fieldname: {` and `fieldname.fieldname` for mapping and indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5972</link><project id="" key="" /><description>The handling of objects in a mapping seems inconsistent. When adding fields with dots, for example `afield.number`, then, when indexing a document 

```
"afield": {
    "number": 4
}
```

the type of `"afield": { "number":` is found correctly, but a second mapping for the object is added. This happens even if the indexing operation fails in case the type of `"afield": { "number":` does not correspond with `"afield.number"` as defined in the mapping.

An example is here: https://gist.github.com/brwe/11394327

There are two issue with this:
1. In STEP 2 (indexing the object) the indexing operation is rejected but the mapping for the object is still added. I do not think this should be so (This might be related to [#5798](https://github.com/elasticsearch/elasticsearch/issues/5798))
2. It seems to me that when creating or merging mappings, it should be checked if pathes exist twice. For example, the following yields no error:

```
PUT /testindex/sometype/_mapping
{
   "sometype": {
      "properties": {
         "afield.number": {
            "type": "integer"
         },
         "afield.text": {
            "type": "string"
         },
         "afield": {
            "properties": {
               "number": {
                  "type": "string"
               }
            }
         }
      }
   }
}
```
</description><key id="32430381">5972</key><summary>Inconsistent results when mixing `fieldname: { fieldname: {` and `fieldname.fieldname` for mapping and indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label></labels><created>2014-04-29T09:28:47Z</created><updated>2014-07-09T10:07:31Z</updated><resolved>2014-07-09T10:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-30T00:35:14Z" id="41748883">It seems that during mapper lookup elasticsearch recognizes that fields with "." in them represent inner object fields, but during mapper creation it treats them as root level fields. Here is another example of this strange behavior that shows how elasticsearch treats such fields in case of dynamic mapping:

```
$ curl -XPUT localhost:9200/test/doc/1 -d '{"foo.bar": 10}'
{"_index":"test","_type":"doc","_id":"1","_version":1,"created":true}

$ curl -XPUT localhost:9200/test/doc/2 -d '{"foo": {"baz": "test"}}'
{"_index":"test","_type":"doc","_id":"2","_version":1,"created":true}

$ curl "localhost:9200/test/_mapping?pretty"
{
  "test" : {
    "mappings" : {
      "doc" : {
        "properties" : {
          "foo" : {
            "properties" : {
              "baz" : {
                "type" : "string"
              }
            }
          },
          "foo.bar" : {
            "type" : "long"
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-07-09T10:07:31Z" id="48451874">Closing in favour of #6736
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation wrong for store throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5971</link><project id="" key="" /><description>The documentation for indices.store.throttle.type says that it can be all, merge or not (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-translog.html) but it is all, merge or none.
</description><key id="32427800">5971</key><summary>Documentation wrong for store throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">michaelsalmon</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T08:46:03Z</created><updated>2014-05-06T00:13:35Z</updated><resolved>2014-05-06T00:13:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinkluge" created="2014-05-05T19:54:14Z" id="42230850">Michael, thanks for the report.

Clint, can you make this simple fix?  I think Michael pasted the wrong our out of date URL.  I see this problem at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-indices.html
</comment><comment author="mikemccand" created="2014-05-05T20:09:05Z" id="42232747">I'll fix this.

Mike

http://blog.mikemccandless.com

On Mon, May 5, 2014 at 3:54 PM, Kevin Kluge notifications@github.comwrote:

&gt; Michael, thanks for the report.
&gt; 
&gt; Clint, can you make this simple fix? I think Michael pasted the wrong our
&gt; out of date URL. I see this problem at
&gt; http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-indices.html
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5971#issuecomment-42230850
&gt; .
</comment><comment author="mikemccand" created="2014-05-06T00:13:35Z" id="42255778">I committed a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add global ordinal based implementations for significant terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5970</link><project id="" key="" /><description>Add global ordinals based significant terms aggregation.
Just as in terms aggregation the significant terms aggregation has two new execution hints: `global_ordinals` and `global_ordinals_hash`.

For top / single significant terms `global_ordinals` is the default execution hint and for sub significant terms `global_ordinals_hash` is the default execution hint. 
</description><key id="32427600">5970</key><summary>Add global ordinal based implementations for significant terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T08:42:25Z</created><updated>2015-06-07T13:38:40Z</updated><resolved>2014-04-29T18:37:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-29T08:54:14Z" id="41654224">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range/Term query/filter on dates fail to handle numbers properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5969</link><project id="" key="" /><description>When providing a number (milliseconds since epoch, UTC), range and term query/filter don't handle it correctly and convert it to a string, that is then first tried to parse as a date
</description><key id="32422134">5969</key><summary>Range/Term query/filter on dates fail to handle numbers properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T06:43:08Z</created><updated>2015-06-07T20:34:36Z</updated><resolved>2014-04-29T18:25:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-29T18:24:07Z" id="41713762">LGTM
</comment><comment author="javanna" created="2014-04-30T09:39:50Z" id="41778099">Hey @kimchy shouldn't this go in 1.1 too?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_polygon not handling polygons that cross the date line properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5968</link><project id="" key="" /><description>Using Elasticsearch 1.1.1. I'm seeing geo_polygon behave oddly when the input polygon cross the date line. From a quick look at GeoPolygonFilter.GeoPolygonDocSet.pointInPolygon it doesn't seem that this is explicitly handled. 

The reproduce this create an index/mapping as:

POST /geo

``` json
{ "mappings": { "docs": { "properties": { "p": { "type": "geo_point" } } } } }
```

Upload a document:

PUT /geo/docs/1

``` json
{ "p": { "lat": 40, "lon": 179 } }
```

Search with a polygon that's a box around the uploaded point and that crosses the date line:

POST /geo/docs/_search

``` json
{
  "filter": { "geo_polygon": { "p": { "points": [
      { "lat": 42, "lon": 178 },
      { "lat": 39, "lon": 178 },
      { "lat": 39, "lon": -179 },
      { "lat": 42, "lon": -179 },
      { "lat": 42, "lon": 178 }
  ] } } }
}
```

ES returns 0 results. If I use a polygon that stays to the west of the date line I do get results:

``` json
{
  "filter": { "geo_polygon": { "p": { "points": [
      { "lat": 42, "lon": 178 },
      { "lat": 39, "lon": 178 },
      { "lat": 39, "lon": 179.5 },
      { "lat": 42, "lon": 179.5 },
      { "lat": 42, "lon": 178 }
  ] } } }
}
```

Also, if I use a bounding box query with the same coordinates as the initial polygon, it does work:

``` json
{
  "filter": { "geo_bounding_box": { "p": 
    { "top_left": { "lat": 42, "lon": 178 },
       "bottom_right": { "lat": 39, "lon": -179 }
     }
  } }
}
```

It seems that this code needs to either split the check into east and west checks or normalize the input values. Am I missing something?
</description><key id="32418613">5968</key><summary>geo_polygon not handling polygons that cross the date line properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">pablocastro</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2014-04-29T04:41:30Z</created><updated>2016-05-27T16:21:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-05-27T15:58:46Z" id="44296053">This is actually working as expected.  Your first query will resolve as shown in the following GeoJSON gist which will not contain your document:
https://gist.github.com/anonymous/82b50b74a7b6d170bfc6

To create the desired results you specified you would need to split the polygon in to two polygons, one to the left of the date line and the other to the right.  This can be done with the following query:

```
curl -XPOST 'localhost:9200/geo/_search?pretty' -d '{
    "query" : {
        "filtered" : {
            "query" : {
                "match_all" : {}
            },
            "filter" : {
                "or" : [
                    {
                        "geo_polygon" : {
                            "p" : {
                                "points" : [
                                     { "lat": 42, "lon": 178 },
                                     { "lat": 39, "lon": 178 },
                                     { "lat": 39, "lon": 180 },
                                     { "lat": 42, "lon": 180 },
                                     { "lat": 42, "lon": 178 }
                                ]
                            }
                        }
                    },
                    {
                        "geo_polygon" : {
                            "p" : {
                                "points" : [
                                     { "lat": 42, "lon": -180 },
                                     { "lat": 39, "lon": -180 },
                                     { "lat": 39, "lon": -179 },
                                     { "lat": 42, "lon": -179 },
                                     { "lat": 42, "lon": -180 }
                                ]
                            }
                        }
                    }
                ]
            }
        }
    }
}'
```

The bounding box is a little different since by specifying which coordinate is top_left and which is top_right you are fixing the box to overlap the date line.
</comment><comment author="jtibshirani" created="2014-09-17T21:44:43Z" id="55965091">For me, I'd expect the line segment between two points to lie in the same direction as the great circle arc. Splitting an arbitrary query into sub-polygons isn't entirely straightforward, because it could intersect longitude 180 multiple times.

I have a rough draft of a commit that fixes this issue by shifting the polygon in GeoPolygonFilter (and all points passed in to pointInPolygon) so that it lies completely on one side of longitude 180. It is low-overhead and has basically no effect on the normal case. The only constraint is that the polygon can't span more than 360 degrees in longitude.

Does this sound reasonable, and is it worth submitting a PR?
</comment><comment author="clintongormley" created="2014-09-25T18:21:19Z" id="56861275">@colings86 could this be solved with a `left/right` parameter or something similar?
</comment><comment author="colings86" created="2014-11-28T09:49:36Z" id="64874846">@nknize is this fixed by https://github.com/elasticsearch/elasticsearch/pull/8521 ?
</comment><comment author="nknize" created="2014-12-01T14:32:21Z" id="65071975">It does not.  This is the infamous "ambiguous polygon" problem that occurs when treating a spherical coordinate system as a cartesian plane.  I opened a discussion and feature branch to address this in #8672 

tldr: GeoJSON doesn't specify order, but OGC does.

Feature fix:  Default behavior = For GeoJSON poly's specified in OGC order (shell: ccw, holes: cw) ES Ring logic will correctly transform and split polys across the dateline (e.g., see https://gist.github.com/nknize/d122b243dc63dcba8474).  For GeoJSON poly's provided in the opposite order original behavior will occur (e.g., @colings86 example https://gist.github.com/anonymous/82b50b74a7b6d170bfc6).   

Additionally, I like @clintongormley suggestion of adding an optional left/right parameter.  Its an easy fix letting user's clearly specify intent.
</comment><comment author="nknize" created="2014-12-03T14:00:29Z" id="65410335">This is now addressed in PR #8762
</comment><comment author="nknize" created="2014-12-16T18:41:49Z" id="67209245">Optional left/right parameter added in PR #8978 
</comment><comment author="nknize" created="2014-12-29T22:11:30Z" id="68308995">Merged in edd33c0
</comment><comment author="jtibshirani" created="2015-01-03T22:17:20Z" id="68611820">I tried @pablocastro's example on trunk, and unfortunately the issue is still there. There might've been some confusion -- the original example refers to the geo_polygon filter, whereas @nknize's fix is for the polygon geo_shape type.

Should I create a new ticket for the geo_polygon filter, or should we re-open this one?
</comment><comment author="nknize" created="2015-01-04T04:41:55Z" id="68620898">Good catch @jtibshirani!  We'll go ahead and reopen this ticket since its a separate issue.
</comment><comment author="nknize" created="2015-01-28T15:06:48Z" id="71849191">Reopening due to #9462 
</comment><comment author="ogerman" created="2015-02-12T17:36:56Z" id="74115212">The search hits of really huge polygon (elasticsearch 1.4.3)

``` javascript
{
  "query": {
    "filtered": {
      "filter": {
        "geo_shape": {
          "location": {
            "shape": {
              "type": "Polygon",
              "coordinates": [
                [
                  [
                    -70.4873046875,
                    79.9818262344106
                  ],
                  [
                    -70.4873046875,
                    -28.07230647927298
                  ],
                  [
                    -103.3583984375,
                    -28.07230647927298
                  ],
                  [
                    -103.3583984375,
                    79.9818262344106
                  ],
                  [
                    -70.4873046875,
                    79.9818262344106
                  ]
                ]
              ],
              "orientation": "ccw"
            }
          }
        }
      }
    }
  }
}
```

doesn't include any points inside that polygon even with orientation option.
Is it related with this issue?
</comment><comment author="nknize" created="2015-02-12T22:49:53Z" id="74170695">Its related.  For now, if you have an ambiguous poly that crosses the pole you'll need to manually split it into 2 separate explicit polys and put inside a `MultiPolygon`  Depending on the complexity of the poly computing the pole intersections can be non-trivial.  The in-work patch will do this for you.

A separate issue is related to the distance_error_pct parameter.  If not specified, larger filters will have reduced accuracy.  Though this seems unrelated to your GeoJSON
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug of Term Suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5967</link><project id="" key="" /><description>Suggester wills return fail results if suggest mode is missing. 

I have example: 

 #analyzer
        index:
        analysis:
        analyzer:
        vi_analyzer:
        type: custom
        tokenizer: whitespace
        filter: [trim, lowercase, hunspell_vi]
        filter:
        hunspell_vi:
        type: hunspell
        locale: vi_VN

 # delete the index
 curl -XDELETE localhost:9200/test

  # insert a document
  curl -XPUT localhost:9200/test/test/1?refresh=true -d '{
   "title": "bong bay"
  }'

  curl -XPUT localhost:9200/test/test/2?refresh=true -d '{
   "title": "ba bong bong"
  }'

  curl -XPUT localhost:9200/test/test/3?refresh=true -d '{
   "title": "ba bong bay"
  }'
# Refresh

 curl -XPOST 'http://localhost:9200/test/_refresh'

 # Suggest
 curl -XPOST 'localhost:9200/_suggest' -d '{
  "_suggestion" : {
    "text" : "ba",
    "term" : {
      "field" : "title",
      "suggest_mode": "missing",
      "sort" : "score",
      "min_word_length" : 1 
    }
  }
 }'

 # Results
{"_shards":{"total":10,"successful":10,"failed":0},"_suggestion":[{"text":"ba","offset":0,"length":2,"options":[{"text":"bay","score":0.5,"freq":1}]}]}
</description><key id="32416430">5967</key><summary>Bug of Term Suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">letrungtrung</reporter><labels><label>discuss</label></labels><created>2014-04-29T03:20:41Z</created><updated>2015-04-10T22:25:36Z</updated><resolved>2015-04-10T22:25:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-30T14:49:38Z" id="41805476">What happens if you call the analyze API like this? (wondering why the frequency is 1 as well)

```
curl -X PUT localhost:9200/foo
curl -XGET 'localhost:9200/foo/_analyze?analyzer= vi_analyzer&amp;pretty' -d 'bong bay'
curl -XGET 'localhost:9200/foo/_analyze?analyzer= vi_analyzer&amp;pretty' -d 'ba bong bong'
curl -XGET 'localhost:9200/foo/_analyze?analyzer= vi_analyzer&amp;pretty' -d 'ba bong bay'
```

Also, can you call a refresh after the index operations in your example? Or index with the refresh flag set to true in this test?
</comment><comment author="letrungtrung" created="2014-04-30T15:35:18Z" id="41811713">- I've just updated my debug with your requirement ( calling refresh after indexing) and you can see results above.
- This's my logs of analyzer:

$ curl -XGET 'localhost:9200/foo/_analyze?analyzer=vi_analyzer&amp;pretty' -d 'bong                                                                                                                 bay'
{
  "tokens" : [ {
    "token" : "bong",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "bay",
    "start_offset" : 5,
    "end_offset" : 8,
    "type" : "word",
    "position" : 2
  } ]
}

$ curl -XGET 'localhost:9200/foo/_analyze?analyzer=vi_analyzer&amp;pretty' -d 'ba bong bong'
{
  "tokens" : [ {
    "token" : "ba",
    "start_offset" : 0,
    "end_offset" : 2,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "bong",
    "start_offset" : 3,
    "end_offset" : 7,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "bong",
    "start_offset" : 8,
    "end_offset" : 12,
    "type" : "word",
    "position" : 3
  } ]
}

$ curl -XGET 'localhost:9200/foo/_analyze?analyzer=vi_analyzer&amp;pretty' -d 'ba bong bay'
{
  "tokens" : [ {
    "token" : "ba",
    "start_offset" : 0,
    "end_offset" : 2,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "bong",
    "start_offset" : 3,
    "end_offset" : 7,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "bay",
    "start_offset" : 8,
    "end_offset" : 11,
    "type" : "word",
    "position" : 3
  } ]
}

# Note

also, you can remove 'hunspell_vi' filter to debug because it's not effect to the results.
</comment><comment author="letrungtrung" created="2014-05-04T02:34:15Z" id="42122212">hey @spinscale !
did my isssue record? That's problem or not, please explain for me. I want to end it to close this topic and get back my job! Please feedback early, thanks a lot. 
</comment><comment author="clintongormley" created="2014-12-30T16:58:20Z" id="68373898">It appears that the `term` suggester only takes local shard statistics into account, which is why the same term `ba` is returned with suggest_mode `missing`
</comment><comment author="dakrone" created="2015-04-10T17:24:48Z" id="91627739">@areek can you follow up with this?
</comment><comment author="areek" created="2015-04-10T22:25:36Z" id="91708113">In `missing` mode, the suggested term is only checked for existence at the local shard. The document(s) with the token `bay` are present only in certain shards.
In order to use the `missing` mode correctly in this situation, all the relevant document(s) should reside in the same shard (e.g. adding `routing=xyz` to all the index requests will do the right thing) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for hanging aborted snapshot during node shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5966</link><project id="" key="" /><description>If a node is shutdown while a snapshot that runs on this node is aborted, it might cause the snapshot process to hang.

Closes #5958
</description><key id="32413527">5966</key><summary>Fix for hanging aborted snapshot during node shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-29T01:42:25Z</created><updated>2015-06-07T20:34:48Z</updated><resolved>2014-05-12T22:43:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-08T01:18:59Z" id="42504015">minor comments, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update client.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5965</link><project id="" key="" /><description>should be classpath rather than classloader, no?
</description><key id="32411654">5965</key><summary>Update client.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">igal-getrailo</reporter><labels /><created>2014-04-29T00:46:30Z</created><updated>2014-07-16T21:45:56Z</updated><resolved>2014-05-06T08:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T21:56:03Z" id="42083120">@igal-getrailo Would you mind signing the [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can merge this change?
</comment><comment author="igal-getrailo" created="2014-05-03T00:39:48Z" id="42091600">done
</comment><comment author="igal-getrailo" created="2014-05-06T14:47:03Z" id="42311686">So the request were closed without being merged?
</comment><comment author="jpountz" created="2014-05-06T14:50:04Z" id="42312103">@igal-getrailo it has been merged, see https://github.com/elasticsearch/elasticsearch/commit/20b05b56c42a212a14a30723690d7ffba4b6867e. It's just that I merged it manually instead of using the Github button.
</comment><comment author="igal-getrailo" created="2014-05-06T14:52:43Z" id="42312479">OK, cool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting w/ multiple fields only sorts with first</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5964</link><project id="" key="" /><description>We've written some pretty complex queries and have had success with sort, until we recently attempted to sort by multiple fields on the same results.  The following is only sorting on Field1.  The data has several hundred records return with unique numbers on each of these two fields. No matter how we sort Field2, the records return in the same order (sorted only by Field1) every time.

``` json
    "sort": [
        {
            "Field1": {
                "order": "desc"
            }
        },
        {
            "Field2": {
                "order": "asc"
            }
        }
    ]
```
</description><key id="32400070">5964</key><summary>Sorting w/ multiple fields only sorts with first</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sallgeud</reporter><labels /><created>2014-04-28T21:22:56Z</created><updated>2014-05-02T15:21:40Z</updated><resolved>2014-05-02T15:20:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-30T00:10:15Z" id="41747349">When you sort on multiple fields the first field is used to sort on first. If two or more records have the same value of the first field, the second field is used to sort such records and os on. If all records have unique values of `Field1` you will not see any effect of `Field2`. What type of sorting did you expect?
</comment><comment author="sallgeud" created="2014-04-30T16:08:37Z" id="41815680">Exactly as you described. Field1, Field2 is unique, but just Field1 is not. We may have discovered that it is only a Field2 issue
</comment><comment author="imotov" created="2014-05-01T15:27:44Z" id="41920353">Can you post here the search response that demonstrates the issue?
</comment><comment author="sallgeud" created="2014-05-01T16:49:34Z" id="41929075">Email me, I'll reply with some examples:  first name [at] onspring dot com
</comment><comment author="imotov" created="2014-05-02T00:49:37Z" id="41973203">Sorry, I don't think I know your first name. Please send examples to igor.motov@elasticsearch.com
</comment><comment author="sallgeud" created="2014-05-02T15:21:40Z" id="42043250">Turned out to be an issue on our end with some duplicate gets post-query.  Made it seem out of order, though the first query was in order.  Closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5963</link><project id="" key="" /><description /><key id="32399628">5963</key><summary>Fixed a typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">alibozorgkhan</reporter><labels /><created>2014-04-28T21:17:27Z</created><updated>2014-06-18T21:47:34Z</updated><resolved>2014-05-06T08:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T21:58:24Z" id="42083314">@alibozorgkhan  Would you mind signing the [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can merge this change?
</comment><comment author="alibozorgkhan" created="2014-05-02T22:01:42Z" id="42083574">@jpountz Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add tie-breaker by suggestion string when sorting candidates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5962</link><project id="" key="" /><description>After upgrading to Lucene 4.8, when BytesRef.hashCode changed to MurmurHash3, we see test failures in SuggestSearchTests because the phrase suggester was relying on HashSet sort order.  I think this change should fix it but I'm not very familiar with this code ....
</description><key id="32394252">5962</key><summary>add tie-breaker by suggestion string when sorting candidates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-04-28T20:13:14Z</created><updated>2014-07-16T21:45:57Z</updated><resolved>2014-04-29T11:32:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-28T20:18:27Z" id="41607877">Looks good to me.  I can't any better tie breaker.  This is the most intuitive, at least.
</comment><comment author="mikemccand" created="2014-04-28T20:20:15Z" id="41608102">Hmm this is causing other test failures, e.g. NoisyChannelSpellCheckerTests ... I'm digging.
</comment><comment author="mikemccand" created="2014-04-28T20:41:45Z" id="41610677">OK I fixed the test failure (put the "dedup" back), and left the tie-breaking change in.
</comment><comment author="nik9000" created="2014-04-28T20:44:08Z" id="41610937">I bet dupes are possible if you have multiple generators.  Or broken ones, I guess.
</comment><comment author="mikemccand" created="2014-04-28T20:53:34Z" id="41612122">Hmm another failure ... digging again ...
</comment><comment author="mikemccand" created="2014-04-29T11:32:21Z" id="41665315">Closing this; I think I screwed up rebase somehow; I'll open a new one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange Results searching postal addresses with state/province of "IN"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5961</link><project id="" key="" /><description>I was not sure how to search for this issue, "IN" being such a common word, but I ran across an interesting issue when searching for customers in Indiana. I hope this is the correct forum. This is in 0.90.x

I created two new people in a brand new index

``` bash
curl -XPOST 'http://localhost:9200/in_wtf/person' -d '{"name": "weirdness", "postal_addresses": [{"add1": "123 Weird St.", "city": "Muncie", state: "IN", zip: "55555"}]}'
curl -XPOST 'http://localhost:9200/in_wtf/person' -d '{"name": "weirdness", "postal_addresses": [{"add1": "123 Weird St.", "city": "Beloit", state: "WI", zip: "66666"}]}'
```

I can query to make sure they're there

``` bash
curl -XPOST 'http://localhost:9200/in_wtf/person/_search?*.*&amp;pretty=true'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "in_wtf",
      "_type" : "person",
      "_id" : "tJCKYCkLRhiYUdk4wa3ZkQ",
      "_score" : 1.0, "_source" : {"name": "weirdness", "postal_addresses": [{"add1": "123 Weird St.", "city": "Muncie", state: "IN", zip: "55555"}]}
    }, {
      "_index" : "in_wtf",
      "_type" : "person",
      "_id" : "o4w0AnZhSJii8maZkkyQKQ",
      "_score" : 1.0, "_source" : {"name": "weirdness", "postal_addresses": [{"add1": "123 Weird St.", "city": "Beloit", state: "WI", zip: "66666"}]}
    } ]
  }
```

If I query for "IN", I do not get what I expect

```
curl -XGET 'http://localhost:9200/in_wtf/person/_search' -d '{"query":{"query_string":{"query":"IN","fields":["postal_addresses.state"]}}}'
```

result

```
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

if I query for "WI" I get what I expect

```
curl -XGET 'http://localhost:9200/in_wtf/person/_search' -d '{"query":{"query_string":{"query":"WI","fields":["postal_addresses.state"]}}}'
```

result

```
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"in_wtf","_type":"person","_id":"o4w0AnZhSJii8maZkkyQKQ","_score":1.0, "_source" : {"name": "weirdness", "postal_addresses": [{"add1": "123 Weird St.", "city": "Beloit", state: "WI", zip: "66666"}]}}]}}
```

This seems like a bug, but I'm not sure if it's how I'm querying that is the problem.
</description><key id="32386788">5961</key><summary>Strange Results searching postal addresses with state/province of "IN"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scotthelm</reporter><labels /><created>2014-04-28T18:40:22Z</created><updated>2014-04-28T19:13:36Z</updated><resolved>2014-04-28T18:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-28T18:57:14Z" id="41598685">Yes. It's because of analysis process. In 0.90, IN is a stopword and is not indexed.
You need to change your mapping for this field and not analyze your text.

This space is an issue tracker. You should ask your question on the mailing list as explained here: http://www.elasticsearch.org/help

Closing.
</comment><comment author="scotthelm" created="2014-04-28T19:13:36Z" id="41600604">Apologies and thanks. I will do that in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove DELETE /index/type endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5960</link><project id="" key="" /><description>Heya,

As we did it for #4481, it could be safe not to allow anymore removing a type using:

```
DELETE /index/type
```

But only allow:

```
DELETE /index/type/_mapping
DELETE /index/_mapping/type
```

It could happen that a user which have previously posted a new document deletes all documents by forgetting to provide the `_id`.

```
POST /index/type
{
 "foo" : "bar"
}

# Just change the VERB and you wipe every documents! 
DELETE /index/type
{
 "foo" : "bar"
}
```

Thoughts?
</description><key id="32377658">5960</key><summary>Remove DELETE /index/type endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>discuss</label></labels><created>2014-04-28T16:59:01Z</created><updated>2014-10-24T08:55:30Z</updated><resolved>2014-10-24T08:55:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-29T06:18:08Z" id="41644787">I am conflicted, if we remove this, and following the same logic, we should remove `DELETE /_index_`. On the other hand, can easily see how maybe someone can make a mistake here...
</comment><comment author="dadoonet" created="2014-04-29T08:28:10Z" id="41652268">So may be we should introduce a new setting like `action.destructive_requires_confirmation` (default to `false`) and if set only allow operations like:

```
# Scenario 1: index level
PUT /{index}

# Will be rejected
DELETE /{index}

# will work
DELETE /{index}/_confirm

# Scenario 2: type level
POST /{index}/{type}
{ "foo" : "bar" }

# Will be rejected
DELETE /{index}/{type}

# Will work
DELETE /{index}/{type}/_confirm
```

So, we don't break anything with that change which is even better but we let OPS decide if we want to activate that feature or not.

May be there are some cleaner way to do this?
I mean like we do with existing on disk indices which are not in cluster state. We can DELETE an index only after some minutes by setting `indices.delete.interval: 5m` (default to 0 - means immediate removal).

In that case, the index should not be available anymore for search but we could fire a command like:

```
DELETE /{index}/_cancel
```

Any other ideas? Or should we close this "issue"? 
</comment><comment author="ghoumard" created="2014-05-01T13:41:00Z" id="41910512">We make this mistake, so we definitively love to see a circuit-breaker here. 
I'm not sure about the indices.delete.interval: 5m  , but the action.destructive_requires_confirmation settings looks a good start.
</comment><comment author="clintongormley" created="2014-10-24T08:53:36Z" id="60361034">We talked about this today and feel that we should keep the existing API, and not try to provide too much protection as we will end up hurting existing users.

What we can do is to reject DELETE requests which come with a body, when a body is not expected.
</comment><comment author="clintongormley" created="2014-10-24T08:55:30Z" id="60361243">Closed in favour of #8217
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>time_zone should set pre AND post _zone in aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5959</link><project id="" key="" /><description>Until now, with aggregations and facets, when users use `time_zone` we get back some inaccurate results.

The cause is that we set only `pre_zone` when `time_zone` is set. So after computing values, we don't apply `post_zone` to results, which gives "strange" results.

Here is a full recreation:
## Create the test

```
DELETE /test

PUT /test/doc/1
{
  "date" : "2014-04-25T21:05:00"
}

PUT /test/doc/2
{
  "date" : "2014-04-25T22:06:00"
}

PUT /test/doc/3
{
  "date" : "2014-04-25T23:07:00"
}

PUT /test/doc/4
{
  "date" : "2014-04-26T00:08:00"
}
```
## Group by date

```
GET /test/doc/_search
{
  "size": 0,
  "aggs": {
    "bydate": {
      "date_histogram": {
        "field": "date",
        "interval": "day",
        "time_zone": "Europe/Paris"
      }
    }
  }
}
```

This gives

```
   "aggregations": {
      "bydate": {
         "buckets": [
            {
               "key_as_string": "2014-04-25T00:00:00.000Z",
               "key": 1398384000000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T00:00:00.000Z",
               "key": 1398470400000,
               "doc_count": 3
            }
         ]
      }
   }
```

When `pre_zone` and `post_zone` are used:

```
GET /test/doc/_search
{
  "size": 0,
  "aggs": {
    "bydate": {
      "date_histogram": {
        "field": "date",
        "interval": "day",
        "pre_zone": "Europe/Paris",
        "post_zone": "Europe/Paris"
      }
    }
  }
}
```

We get:

```
   "aggregations": {
      "bydate": {
         "buckets": [
            {
               "key_as_string": "2014-04-25T02:00:00.000Z",
               "key": 1398391200000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T02:00:00.000Z",
               "key": 1398477600000,
               "doc_count": 3
            }
         ]
      }
   }
```
## Grouping by hours

It's even more obvious when grouping by hours:

```
GET /test/doc/_search
{
  "size": 0,
  "aggs": {
    "bydate": {
      "date_histogram": {
        "field": "date",
        "interval": "hour",
        "time_zone": "Europe/Paris"
      }
    }
  }
}
```

gives

```
   "aggregations": {
      "bydate": {
         "buckets": [
            {
               "key_as_string": "2014-04-25T21:00:00.000Z",
               "key": 1398459600000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-25T22:00:00.000Z",
               "key": 1398463200000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-25T23:00:00.000Z",
               "key": 1398466800000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T00:00:00.000Z",
               "key": 1398470400000,
               "doc_count": 1
            }
         ]
      }
   }
```

and

```
GET /test/doc/_search
{
  "size": 0,
  "aggs": {
    "bydate": {
      "date_histogram": {
        "field": "date",
        "interval": "hour",
        "pre_zone": "Europe/Paris",
        "post_zone": "Europe/Paris"
      }
    }
  }
}

```

gives

```
   "aggregations": {
      "bydate": {
         "buckets": [
            {
               "key_as_string": "2014-04-25T23:00:00.000Z",
               "key": 1398466800000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T00:00:00.000Z",
               "key": 1398470400000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T01:00:00.000Z",
               "key": 1398474000000,
               "doc_count": 1
            },
            {
               "key_as_string": "2014-04-26T02:00:00.000Z",
               "key": 1398477600000,
               "doc_count": 1
            }
         ]
      }
   }
```

This patch fix this.
</description><key id="32362568">5959</key><summary>time_zone should set pre AND post _zone in aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-04-28T14:13:50Z</created><updated>2015-08-18T11:36:31Z</updated><resolved>2014-05-16T13:48:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-28T14:39:44Z" id="41565939">It looks consistent to me but I'd like to get @uboness ' opinion.
</comment><comment author="uboness" created="2014-04-30T09:28:19Z" id="41777178">@dadoonet `time_zone` should only apply a `pre_offset` not both. I think you're confused because you're looking at the formatted time as an indication. The truth is, that the time format always assume UTC and that's why the formatted times are wrong... not be cause of not applying `post_offset`... this is a bug in the formatting, not in the execution mode... execution should only apply `pre_offset` not both (otherwise the millis will be wrong)
</comment><comment author="uboness" created="2014-04-30T14:14:25Z" id="41801014">thinking about it more... the `time_zone` was originally set support to work the way it does right now for bwc reasons (see https://github.com/elasticsearch/elasticsearch/issues/1580).... but that's relates to pre 0.19.0 releases. We could redefine it's purpose now and say that the `time_zone` parameter is basically a syntactic sugar for `pre_offset` &amp; `post_offset` (where both have the same value)... 

The nice thing here is that we stick to the contract of always returning the UTC "view" of the millis

if we go this direction we should at least treat it as semi-breaking change.
</comment><comment author="dadoonet" created="2014-04-30T14:17:19Z" id="41801334">@uboness While we are at it, should we also expose `timezone()` in `DateHistogramBuilder` class? 
</comment><comment author="uboness" created="2014-04-30T14:26:16Z" id="41802429">@dadoonet yes... expose it in the builder
</comment><comment author="dadoonet" created="2014-05-12T16:45:41Z" id="42856766">@uboness PR updated. WDYT?
</comment><comment author="uboness" created="2014-05-12T19:56:58Z" id="42880012">Left a couple of comments, LGTM oterwise. @jpountz can you also have a look?
</comment><comment author="dadoonet" created="2014-05-13T08:20:37Z" id="42928928">Hmmm... I forget to update documentation as well: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_time_zone_2.

Will push a new commit for docs
</comment><comment author="dadoonet" created="2014-05-13T10:51:12Z" id="42940967">@jpountz Documentation updated as well. Note that I did not change anything in facets.
Do you want me to first squash and rebase my changes before reviewing it?
</comment><comment author="jpountz" created="2014-05-13T21:57:38Z" id="43018550">I think this fix is not enough. For example, I ran the following aggregation with your patch:

``` json
DELETE /test

PUT /test
{
    "mappings": {
        "test": {
            "properties": {
                "d": {
                    "type": "date",
                    "fields": {
                        "string": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            }
        }
    }
}

PUT /test/test/1
{
    "d": "2013-12-18T23:53:14+02"
}

PUT /test/test/2
{
    "d": "2013-12-18T00:12:11.785+02"
}

PUT /test/test/3
{
    "d": "2013-12-23T01:10:12.755+02"
}

GET /test/_search
{
    "aggs": {
        "histo": {
            "date_histogram": {
                "field": "d",
                "interval": "day",
                "time_zone": "+02:00"
            },
            "aggs": {
                "dates": {
                    "terms": {
                        "field": "d.string"
                    }
                }
            }
        }
    }
}
```

and it gives

``` json
   "aggregations": {
      "histo": {
         "buckets": [
            {
               "key_as_string": "2013-12-18T02:00:00.000Z",
               "key": 1387332000000,
               "doc_count": 2,
               "dates": {
                  "buckets": [
                     {
                        "key": "2013-12-18T00:12:11.785+02",
                        "doc_count": 1
                     },
                     {
                        "key": "2013-12-18T23:53:14+02",
                        "doc_count": 1
                     }
                  ]
               }
            },
            {
               "key_as_string": "2013-12-23T02:00:00.000Z",
               "key": 1387764000000,
               "doc_count": 1,
               "dates": {
                  "buckets": [
                     {
                        "key": "2013-12-23T01:10:12.755+02",
                        "doc_count": 1
                     }
                  ]
               }
            }
         ]
      }
   }
```

Buckets are computed correctly: in spite of the offset, the two extreme times on the 2013-12-18 end up in the same bucket which makes me think that `pre_zone` works correctly. However, I suspect `post_zone` to do the opposite of what it is supposed to do as `2013-12-18T02:00:00.000Z` (the key of the 1st bucket) is actually `2013-12-18T04:00:00.000+02` although it should be `2013-12-18T00:00:00.000+02`. The `post_zone` conversion seems to be performed in the reverse order.
</comment><comment author="dadoonet" created="2014-05-14T07:38:10Z" id="43050929">For information, without my patch, the same script gives:

``` js
   "aggregations": {
      "histo": {
         "buckets": [
            {
               "key_as_string": "2013-12-18T00:00:00.000Z",
               "key": 1387324800000,
               "doc_count": 2,
               "dates": {
                  "buckets": [
                     {
                        "key": "2013-12-18T00:12:11.785+02",
                        "doc_count": 1
                     },
                     {
                        "key": "2013-12-18T23:53:14+02",
                        "doc_count": 1
                     }
                  ]
               }
            },
            {
               "key_as_string": "2013-12-23T00:00:00.000Z",
               "key": 1387756800000,
               "doc_count": 1,
               "dates": {
                  "buckets": [
                     {
                        "key": "2013-12-23T01:10:12.755+02",
                        "doc_count": 1
                     }
                  ]
               }
            }
         ]
      }
   }
```

If I replace `time_zone` with:

``` js
                "pre_zone": "+02:00",
                "post_zone": "+02:00"
```

It gives the exact same output as you mentioned.

So, I'm wondering if we should not fix the `post_zone` "issue" if any in another PR.
</comment><comment author="uboness" created="2014-05-14T09:04:26Z" id="43057934">@dadoonet there are several things broken here. First, when defining a `time_zone` you shouldn't apply the same offset for both `pre` and `post`. The `post` needs to be the reverse offset.

But more importantly... This timezone rounding feels completely broken to me tbh. we inherited it from the faceting and unfortunately didn't take the appropriate time to think things thoroughly. The way I look at it, all timestamps (the bucket keys in the `date_histogram` example) returned from ES should always be UTC. It should be the responsibility of the client to convert the UTC timestamp to whatever timezone representation they want. This effectively means that the timezone rounding should only support two parameters: `time_zone` &amp; `offset` (no post/pre distinction). The way it will work, given a timezone TZ and a timestamp T:
1. extract the offset of TZ, add it to T: `T + offset(TZ) = offset_T`
2. round to find the bucket key: `floor(offset_T)`
3. revert back to UTC by `floor(offset_T) - offset(TZ) = bucket_key`

(similar logic will be applied for the `offset` parameter)

For this reason, I'd park this issue for now, as it seems to be a greater issue than what you describe here. And potentially for `aggs` we'll break the previous contract that we inherited from the facets (it's a very complicated contract that can be greatly simplified).

I don't think there's an appropriate issue for this yet (it's only a PR), so please create one and we'll take it from there?

(cc @jpountz )
</comment><comment author="spinscale" created="2014-05-16T13:48:00Z" id="43332780">Closing this one. See Uris comment above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot aborted but but still in progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5958</link><project id="" key="" /><description>Hi

I'm running elasticsearch 1.1.1 on centos 6 AWS instances with java 7. We have a snapshot that the api call "_snapshot/&lt;repo_name&gt;/_all" is listing as IN_PROGRESS, however when attempting to delete that snapshot, it doesn't delete. In fact the delete call just hangs and never returns. When I check the current status of all the snapshots using "_snapshot/&lt;repo_name&gt;/_status" that to just hangs and does not return. 

I then looked at the cluster state and that tells me that the snapshot is in an "ABORTED" state. I can't actually create a new snapshot right now. Any ideas how I can resolve this and is it a bug?

Thanks
</description><key id="32357413">5958</key><summary>Snapshot aborted but but still in progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dipthegeezer</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-28T13:09:26Z</created><updated>2016-10-25T14:09:09Z</updated><resolved>2014-05-12T22:43:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-28T13:23:33Z" id="41557081">Could you post here the portion of the cluster state with the information about the snapshot?
</comment><comment author="dipthegeezer" created="2014-04-28T14:23:24Z" id="41563909">Hi imotov

It's quite a big bit of json and I had to obscure the index and shard information as it was too long for the comment section in github. If you need it happy to talk offline via email etc. We quite a large number of indices.

```
{ 
        "snapshots": {
            "snapshots": [
                {
                    "include_global_state": true, 
                    "indices": [
                    ], 
                    "repository": "s3_repository", 
                    "shards": [
                    ], 
                    "snapshot": "custom_snapshot_24_04_2014_17:27", 
                    "state": "ABORTED"
                }
            ]
        }
}

```
</comment><comment author="williamsandrew" created="2014-07-28T16:51:21Z" id="50365076">Was the solution to this problem updating to a new version? We were on 1.1.1, upgraded to 1.3.1, and are still having this problem
</comment><comment author="imotov" created="2014-07-28T17:15:05Z" id="50368143">@TheDude05 yes, it should be fixed in 1.1.2 and above. Could you provide more details about your problem? Was the snapshot in the `ABORTED` state and it's still in `ABORTED` state after rolling restart or you got an snapshot stack in an `ABORTED` state while running 1.3.1. Could you describe what happened before the snapshot went into this state?
</comment><comment author="williamsandrew" created="2014-07-28T20:27:47Z" id="50394868">I started a snapshot on my cluster (at the time all nodes were version 1.1.1) and found that it was taking a very long time because every node in my cluster was garbage collecting frequently. I performed a rolling restart of all of the nodes in my cluster, found that the snapshot I started was still in a running state so I aborted it via the HTTP DELETE /_snapshot/&lt;repository name&gt;/&lt;snapshot_name&gt; end point. That snapshot was then showing up as aborted in the cluster status so I attempted to start a new one. I however couldn't start a new snapshot and would instead get an error message indicating a snapshot was already running. Whenever I tried to delete that first snapshot again, the HTTP request would hang indefinitely. The issue did not go away after performing another rolling restart. There is also nothing interesting in the logs.

After reading the release notes and this Github issue it seemed that 1.2+ fixed some snapshot issues so I updated my cluster (one by one) to version 1.3.1. I again tried deleting the snapshot in question but am still having the same issue as before.

```
$ curl -s 'http://localhost:9200/_cluster/state' | jq -M '.metadata.snapshots.snapshots[] | {state: .state, snapshot: .snapshot}'
{
  "snapshot": "snapshot-1406558337",
  "state": "ABORTED"
}
```

The repository type is AWS/S3 and I do see that there are "snapshot" and "metadata" files in our S3 bucket for this particular snapshot. 

Let me know if there is more debugging output that would be useful.

_EDIT:_ Reworded some sentences for clarity
</comment><comment author="imotov" created="2014-07-29T12:17:05Z" id="50468280">Unfortunately, the fix in 1.2 doesn't clean already stuck snapshots, it only prevents new snapshots from being stuck in this state. So, you need to perform a full cluster restart to clean the stuck snapshot or use the [cleanup utility](https://github.com/imotov/elasticsearch-snapshot-cleanup) to remove stuck snapshots. 
</comment><comment author="williamsandrew" created="2014-07-29T20:00:31Z" id="50529992">@imotov Thank you for your help. The cleanup utility worked and removed that old snapshot that was causing problems
</comment><comment author="peillis" created="2014-08-27T09:15:35Z" id="53544914">This issue is closed but I'm using elasticsearch 1.3.2 and we are having problems with this. We have to use the cleanup utility to remove stuck snapshots. Am I missing something?
</comment><comment author="JoeZ99" created="2014-08-29T17:24:22Z" id="53905536">more precisely. snapshots restoring hangs from time to time. No need to try to delete or to shut down the node. Igor's script has to bee run periodically. Version is 1.3.2
</comment><comment author="imotov" created="2014-08-29T17:42:30Z" id="53907690">@JoeZ99 now I am completely confused. Which script are we talking about? If you mean https://github.com/imotov/elasticsearch-snapshot-cleanup it shouldn't do anything for snapshot restore process. The only way to stop restore is by deleting the files being restored. Could you describe in more details which repository you are using and what's going on?
</comment><comment author="JoeZ99" created="2014-08-30T17:50:07Z" id="53965438">we're using a s3 repository.

we're on a 1.3.2 ES version

we make about 400 or so snapshot restores a day.  each snapshot restore
restores two indices, all 400 restoring process are for different indices.
the restoration process of a single snapshot usually takes less than a
minute.

Some times the snapshot restoring process takes forever, like it was hung
or something

we've found that after applying that cleanup script you talk about, the
snapshot restore process is agile again, so we apply it regulartly.

For what i've understood from the ticket, it looks like the bug consist one
some snapshot restore process being "hung" and not able to being aborted
afterwards, using the standard DELETE entrypoint (at least until the 1.2
version). Your script is meant to wipe out any "hung" restoring process,
because the fixing that is since version 1.2 doesn't take care of already
hung restoring process, it just makes sure the restoring process doesn't
get "hung" again.

At that light, looks like from time to time one of our multiple restore
process gets "hung" and so no further restored process can be applied ,
since only one restoration at a time is allowed on the cluster, and that
would be when we see our restoration process gets "hung". After applying
your script, this supposedly "hung" restoration process is wiped out and
the cluster is back in business.

Could it be something like that?

On Fri, Aug 29, 2014 at 1:42 PM, Igor Motov notifications@github.com
wrote:

&gt; @JoeZ99 https://github.com/JoeZ99 now I am completely confused. Which
&gt; script are we talking about? If you mean
&gt; https://github.com/imotov/elasticsearch-snapshot-cleanup it shouldn't do
&gt; anything for snapshot restore process. The only way to stop restore is by
&gt; deleting the files being restored. Could you describe in more details which
&gt; repository you are using and what's going on?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5958#issuecomment-53907690
&gt; .

## 

uh, oh http://www.youtube.com/watch?v=GMD_T7ICL0o.

http://www.defectivebydesign.org/no-drm-in-html5
</comment><comment author="imotov" created="2014-09-02T17:47:25Z" id="54190058">@JoeZ99 can you email me cluster state from the cluster in such stuck state?
</comment><comment author="l0bster" created="2014-09-29T09:28:30Z" id="57136510">Hey there,

we are using ES 1.3.2 and there are several Snapshots in State IN_PROGRESS. Deleting them manually doesn't work and using this script https://github.com/imotov/elasticsearch-snapshot-cleanup told me:

[2014-09-29 11:27:17,376][INFO ][org.elasticsearch.org.motovs.elasticsearch.snapshots.AbortedSnapshotCleaner] No snapshots found

A rolling restart of all nodes didn't help removing these stale snapshots. Any advice on removing them? 

Thanks in advance!

#######

ptlxtme02:/tmp/elasticsearch-snapshot-cleanup-1.0-SNAPSHOT/bin # curl -XGET  "http://ptlxtme02:9200/_snapshot/es_backup_fast/_all?pretty=true"
{
  "snapshots" : [ {
    "snapshot" : "2014-08-11_19:30:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-11T17:30:03.764Z",
    "start_time_in_millis" : 1407778203764,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-12_20:00:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-12T18:00:03.255Z",
    "start_time_in_millis" : 1407866403255,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-12_21:00:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-12T19:00:03.723Z",
    "start_time_in_millis" : 1407870003723,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-13_03:00:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-13T01:00:03.350Z",
    "start_time_in_millis" : 1407891603350,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-13_08:30:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-13T06:30:03.183Z",
    "start_time_in_millis" : 1407911403183,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-13_13:30:02",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-13T11:30:03.009Z",
    "start_time_in_millis" : 1407929403009,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-08-14_19:30:03",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "IN_PROGRESS",
    "start_time" : "2014-08-14T17:30:03.620Z",
    "start_time_in_millis" : 1408037403620,
    "failures" : [ ],
    "shards" : {
      "total" : 0,
      "failed" : 0,
      "successful" : 0
    }
  }, {
    "snapshot" : "2014-09-26_16:09:43",
    "indices" : [ ".ps_config", ".ps_status", "_river", "ps_article_index_01" ],
    "state" : "SUCCESS",
    "start_time" : "2014-09-26T14:09:43.829Z",
    "start_time_in_millis" : 1411740583829,
    "end_time" : "2014-09-26T15:26:43.933Z",
    "end_time_in_millis" : 1411745203933,
    "duration_in_millis" : 4620104,
    "failures" : [ ],
    "shards" : {
      "total" : 6,
      "failed" : 0,
      "successful" : 6
    }
  } ]
}
</comment><comment author="imotov" created="2014-09-29T10:36:10Z" id="57143149">@l0bster what do you get when you try to delete these snapshots?
</comment><comment author="l0bster" created="2014-09-29T11:03:07Z" id="57145553">@imotov i get:
ptlxtme02:/tmp # curl -XDELETE "http://ptlxtme02:9200/_snapshot/es_backup_fast/2014-08-14_19:30:03"
{"error":"SnapshotMissingException[[es_backup_fast:2014-08-14_19:30:03] is missing]; nested: FileNotFoundException[/mnt/es_backup/fast_snapshot/metadata-2014-08-14_19:30:03 (No such file or directory)]; ","status":404

here is an ls output of the snapshot directory: 
ptlxtme02:/tmp # ll /mnt/es_backup/fast_snapshot/
insgesamt 368
-rw-r--r-- 1 elasticsearch elasticsearch  118 29. Sep 11:10 index
drwxr-xr-x 8 elasticsearch elasticsearch 4096  7. Aug 17:00 indices
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 04:00 metadata-2014-08-11_04:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 04:30 metadata-2014-08-11_04:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 05:30 metadata-2014-08-11_05:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 07:00 metadata-2014-08-11_07:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 07:30 metadata-2014-08-11_07:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 08:30 metadata-2014-08-11_08:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 10:30 metadata-2014-08-11_10:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 11:30 metadata-2014-08-11_11:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 12:00 metadata-2014-08-11_12:00:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 13:00 metadata-2014-08-11_13:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 15:00 metadata-2014-08-11_15:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 16:00 metadata-2014-08-11_16:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 16:30 metadata-2014-08-11_16:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 17:00 metadata-2014-08-11_17:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 19:00 metadata-2014-08-11_19:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 20:00 metadata-2014-08-11_20:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 20:30 metadata-2014-08-11_20:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 21:00 metadata-2014-08-11_21:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 21:30 metadata-2014-08-11_21:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 22:00 metadata-2014-08-11_22:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 11. Aug 23:30 metadata-2014-08-11_23:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 01:30 metadata-2014-08-12_01:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 02:00 metadata-2014-08-12_02:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 03:00 metadata-2014-08-12_03:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 03:30 metadata-2014-08-12_03:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 04:00 metadata-2014-08-12_04:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 04:30 metadata-2014-08-12_04:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 06:00 metadata-2014-08-12_06:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 10:00 metadata-2014-08-12_10:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 10:30 metadata-2014-08-12_10:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 11:00 metadata-2014-08-12_11:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 11:30 metadata-2014-08-12_11:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 12:00 metadata-2014-08-12_12:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 12:30 metadata-2014-08-12_12:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 13:00 metadata-2014-08-12_13:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 13:30 metadata-2014-08-12_13:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 16:00 metadata-2014-08-12_16:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 17:00 metadata-2014-08-12_17:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 17:30 metadata-2014-08-12_17:30:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 21:30 metadata-2014-08-12_21:30:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 22:00 metadata-2014-08-12_22:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 23:00 metadata-2014-08-12_23:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 12. Aug 23:30 metadata-2014-08-12_23:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 00:00 metadata-2014-08-13_00:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 01:30 metadata-2014-08-13_01:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 02:30 metadata-2014-08-13_02:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 04:30 metadata-2014-08-13_04:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 05:00 metadata-2014-08-13_05:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 06:00 metadata-2014-08-13_06:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 07:00 metadata-2014-08-13_07:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 08:00 metadata-2014-08-13_08:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 09:00 metadata-2014-08-13_09:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 14:00 metadata-2014-08-13_14:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 14:30 metadata-2014-08-13_14:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 16:00 metadata-2014-08-13_16:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 18:00 metadata-2014-08-13_18:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 19:00 metadata-2014-08-13_19:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 20:30 metadata-2014-08-13_20:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 21:00 metadata-2014-08-13_21:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 21:30 metadata-2014-08-13_21:30:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 13. Aug 23:30 metadata-2014-08-13_23:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 00:00 metadata-2014-08-14_00:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 02:00 metadata-2014-08-14_02:00:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 03:00 metadata-2014-08-14_03:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 04:00 metadata-2014-08-14_04:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 05:30 metadata-2014-08-14_05:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 06:00 metadata-2014-08-14_06:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 06:30 metadata-2014-08-14_06:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 07:30 metadata-2014-08-14_07:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 08:00 metadata-2014-08-14_08:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 09:00 metadata-2014-08-14_09:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 09:30 metadata-2014-08-14_09:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 12:30 metadata-2014-08-14_12:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 14:30 metadata-2014-08-14_14:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 16:00 metadata-2014-08-14_16:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 18:00 metadata-2014-08-14_18:00:02
-rw-r--r-- 1 elasticsearch elasticsearch  253 14. Aug 18:30 metadata-2014-08-14_18:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 15. Aug 05:30 metadata-2014-08-15_05:30:04
-rw-r--r-- 1 elasticsearch elasticsearch  253 15. Aug 07:30 metadata-2014-08-15_07:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 15. Aug 08:00 metadata-2014-08-15_08:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  253 15. Aug 10:00 metadata-2014-08-15_10:00:35
-rw-r--r-- 1 elasticsearch elasticsearch  312 26. Sep 16:09 metadata-2014-09-26_16:09:43
-rw-r--r-- 1 elasticsearch elasticsearch  232 11. Aug 19:30 snapshot-2014-08-11_19:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  232 12. Aug 20:00 snapshot-2014-08-12_20:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  232 12. Aug 21:00 snapshot-2014-08-12_21:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  231 13. Aug 03:00 snapshot-2014-08-13_03:00:03
-rw-r--r-- 1 elasticsearch elasticsearch  232 13. Aug 08:30 snapshot-2014-08-13_08:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  232 13. Aug 13:30 snapshot-2014-08-13_13:30:02
-rw-r--r-- 1 elasticsearch elasticsearch  231 14. Aug 19:30 snapshot-2014-08-14_19:30:03
-rw-r--r-- 1 elasticsearch elasticsearch  237 26. Sep 17:26 snapshot-2014-09-26_16:09:43
</comment><comment author="peillis" created="2014-09-29T15:52:48Z" id="57182430">We also see the "No snapshots found" message from the cleanup script, but the fact is that after that the stuck seems to disappear.
</comment><comment author="l0bster" created="2014-09-30T07:48:23Z" id="57278483">@peillis i gave it a try ... we have 7 stuck snapshots in state "IN_PROGRESS". I ran the utility, it told me, no snapshots found and after that, there were still 7 stuck snapshots :(
</comment><comment author="imotov" created="2014-09-30T08:13:45Z" id="57280775">@l0bster the utility is created to clean up snapshots that are currently running. In you case these snapshots are no longer running. They got stuck in IN_PROGRESS state because cluster was shutdown  or you lost connection to the mounted shared file system while they were running. So elasticsearch didn't have chance to update them and they are still stored in this intermittent state. Theoretically, it should be possible to delete them using snapshot delete command. But since it doesn't work for you, you might be hitting a bug similar to #6383. I will try to reproduce the issue but it would really help me if you could check log file on the current master node to see if there are any errors logged while you are running this snapshot delete command. If you see an error, please post it here with complete stacktrace. 
</comment><comment author="l0bster" created="2014-09-30T11:14:09Z" id="57298335">@imotov Hey, when i try to delete one snapshot via: curl -XDELETE "http://ptlxtme02:9200/_snapshot/es_backup_fast/2014-08-11_19:30:03" there is no error logged. I can increase the log level to debug but that requires a node restart. I will edit my post asap to post eventually logged errors.
</comment><comment author="imotov" created="2014-09-30T12:10:50Z" id="57303866">@l0bster thanks, but it will unlikely result in more logging. The log message that I was looking for should have been logged on the WARN level. 
</comment><comment author="l0bster" created="2014-09-30T14:30:30Z" id="57322412">@imotov Hey, i managed to enable debuging... sry for the delay. But the Only thing es is logging is:

[2014-09-30 16:29:27,443][DEBUG][cluster.service          ] [ptlxtme02] processing [delete snapshot]: execute
[2014-09-30 16:29:27,444][DEBUG][cluster.service          ] [ptlxtme02] processing [delete snapshot]: no change in cluster_state

:(
</comment><comment author="l0bster" created="2014-10-02T07:49:46Z" id="57594756">@imotov did you have time to reproduce the issue alrdy?
</comment><comment author="imotov" created="2014-10-03T17:13:10Z" id="57825230">@l0bster yes, I was able to reproduce it. It's a different bug. So I created a new issue for it - #7980. Thank you for report and providing helpful information. As a workaround you can simply delete the file `snapshot-2014-08-14_19:30:03` from the repository.
</comment><comment author="bruce-lyft" created="2014-12-12T20:03:20Z" id="66826876">I am seeing this error in v1.3.6.   Cluster state shows the snapshot is in ABORTED state on all shards.
New snapshots cannot be started (ConcurrentSnapshotExecutionException), and the current snapshot cannot be deleted - DELETE just hangs.

@imotov 's cleanup tool did not help.

Update:   rolling restart corrected the issue.

cluster state is 
  "snapshots": {
      "snapshots": [
        {
          "repository": "my_backup",
          "snapshot": "snapshot_1",
          "include_global_state": true,
          "state": "ABORTED",
          "indices": [
            "grafana-dash"
          ],
          "shards": [
            {
              "index": "grafana-dash",
              "shard": 0,
              "state": "ABORTED",
              "node": "_mTOiyD_TN2vV2C2A8sNbw"
            },
            {
              "index": "grafana-dash",
              "shard": 1,
              "state": "ABORTED",
              "node": "TYy7OHbXR2q_U-xTG4Xtqg"
            },
            {
              "index": "grafana-dash",
              "shard": 2,
              "state": "ABORTED",
              "node": "TYy7OHbXR2q_U-xTG4Xtqg"
            },
            {
              "index": "grafana-dash",
              "shard": 3,
              "state": "ABORTED",
              "node": "TYy7OHbXR2q_U-xTG4Xtqg"
            },
            {
              "index": "grafana-dash",
              "shard": 4,
              "state": "ABORTED",
              "node": "TYy7OHbXR2q_U-xTG4Xtqg"
            }
          ]
        }
      ]
    }
  },
</comment><comment author="vanga" created="2015-04-06T09:40:19Z" id="89996084">hey @imotov , I have the same problem that my ES node was restarted during a snpashot process, I am trying to run this script https://github.com/imotov/elasticsearch-snapshot-cleanup 
My ES version 1.4.0, I did this 

&gt; For all other versions update pom.xml file and appropriate elasticsearch and lucene version, run mvn clean package and untar the file found in the target/releases directory. copied my cluster config to config/elasticsearch.yml

when I run the bin/cleanup package I get this error 

```
Setting ES_HOME as /root/elasticsearch-snapshot-cleanup/target/releases/elasticsearch-snapshot-cleanup-1.4.4.1
Error: Could not find or load main class org.motovs.elasticsearch.snapshots.AbortedSnapshotCleaner
```

Any idea?
</comment><comment author="MosesMansaray" created="2016-10-25T14:09:09Z" id="256045274">Occurred on ES v1.4.2.  [elasticsearch-snapshot-cleanup](https://github.com/imotov/elasticsearch-snapshot-cleanup) did the trick for me with no fuss. 

Thanks a bunch @imotov 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch randomly not responding to query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5957</link><project id="" key="" /><description>Hi folks,

we use elasticsearch since a couple of weeks and there are over 160 million of entries separated in a couple of indexes splitted by month (e.g. index_2014_01,index_2014_02,....).
Sometimes if we send a bulk query requests to the elasticsearch nodes for some requests there is no responses and no timeouts. There is no chance to reproduce this problem because it happens randomly and also on querys wich worked seconds before with a minimum response time.
We started to debug down to the elasticsearch transport client without success. We noticed that there was a request without response (Breakpoints where hooked in response-listener, request-listener and failure-listener) - even after a long wait time there was no response or failure. There is no exception or log-output which indicates that problem except that after a time there is a "disconnected from node" message. In the logs from the cluster-nodes there is no error either.
The only thing we noticed is that after a few minutes a NodeDisconnectedException is thrown.
There is not much load on the elasticsearch cluster and on our firewall all ports from 9200 to 9400 are open.

Our application structure:
TOMCAT 7 (JAVAEE-7u25) --&gt; spring-data-elasticsearch-1.0.0.M2 --&gt; elasticsearch-1.1.0(1.0.0 with same issue) ----------&gt; elasticsearch (8 nodes) version 1.0.0 (JAVA-7u25)

This issue happens randomly and we have no idea where it comes from.
</description><key id="32355074">5957</key><summary>elasticsearch randomly not responding to query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SomeProgrammer85</reporter><labels><label>feedback_needed</label></labels><created>2014-04-28T12:34:46Z</created><updated>2016-08-11T09:06:18Z</updated><resolved>2014-09-24T09:05:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-29T02:33:21Z" id="41636500">can you add more information to this? bulk requests size for example? Do you have a stacktrace of the disconnecting exception in the log files somewhere? Anything else in the log files which might help? Do you have different versions of elasticsearch running in the cluster 1.1.0 and 1.0.0 or did I misread that?
</comment><comment author="SomeProgrammer85" created="2014-04-29T08:01:07Z" id="41650442">Thank you for your response.

The version of elasticsearch is 1.0.0 all over the cluster.
We have a webapplication running with 1.1.0 and another with 1.0.0 and it happens to both.
The size of the bulk request is variable (1 to X).

A long time after the responseless request there is a Exception log entry, but we don't know if there is a interrelation between this and the responseless request (tomcat/log/catalina.out):

```
2014-04-23 11:43:46,273 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic08][hash name of node][elastic08][inet[/ip of node 8:9300]]], channel closed event
org.elasticsearch.transport.NodeDisconnectedException: [elastic08][inet[/ip of node 8:9300]][search] disconnected
org.elasticsearch.transport.NodeDisconnectedException: [elastic08][inet[/ip of node 8:9300]][search] disconnected
```

The log writes permanently this debug information (tomcat/log/catalina.out):

```
2014-04-29 05:26:55,723 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic06][hash name of node][elastic06][inet[/ip of node 6:9300]]], channel closed event
2014-04-29 05:27:00,400 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic06][hash name of node][elastic06][inet[/ip of node 6:9300]]]
2014-04-29 05:28:01,259 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic01][hash name of node][elastic01][inet[/ip of node 1:9300]]], channel closed event
2014-04-29 05:28:01,260 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic01][hash name of node][elastic01][inet[/ip of node 1:9300]]], channel closed event
2014-04-29 05:28:05,571 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic01][hash name of node][elastic01][inet[/ip of node 1:9300]]]
2014-04-29 05:32:56,170 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic03][hash name of node][elastic03][inet[/ip of node 3:9300]]], channel closed event
2014-04-29 05:32:56,172 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic03][hash name of node][elastic03][inet[/ip of node 3:9300]]], channel closed event
2014-04-29 05:32:56,175 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic03][hash name of node][elastic03][inet[/ip of node 3:9300]]], channel closed event
2014-04-29 05:32:56,354 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic03][hash name of node][elastic03][inet[/ip of node 3:9300]]]
2014-04-29 05:38:23,850 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic07][hash name of node][elastic07][inet[/ip of node 7:9300]]], channel closed event
2014-04-29 05:38:27,226 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic07][hash name of node][elastic07][inet[/ip of node 7:9300]]]
2014-04-29 05:39:29,384 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic05][hash name of node][elastic05][inet[/ip of node 5:9300]]], channel closed event
2014-04-29 05:39:32,412 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic05][hash name of node][elastic05][inet[/ip of node 5:9300]]]
2014-04-29 05:43:18,765 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic02][hash name of node][elastic02][inet[/ip of node 2:9300]]], channel closed event
2014-04-29 05:43:18,767 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic02][hash name of node][elastic02][inet[/ip of node 2:9300]]], channel closed event
2014-04-29 05:43:23,043 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic02][hash name of node][elastic02][inet[/ip of node 2:9300]]]
2014-04-29 05:48:46,440 DEBUG [org.elasticsearch.transport.netty] [Flatman] disconnected from [[elastic08][hash name of node][elastic08][inet[/ip of node 8:9300]]], channel closed event
2014-04-29 05:48:48,895 DEBUG [org.elasticsearch.transport.netty] [Flatman] connected to node [[elastic08][hash name of node][elastic08][inet[/ip of node 8:9300]]]
```
</comment><comment author="spinscale" created="2014-04-29T14:17:11Z" id="41681061">what is X in your notes? how big can it grow?
</comment><comment author="SomeProgrammer85" created="2014-04-29T14:33:30Z" id="41683200">It's just a placeholder for the count of asked items. In time the max is about 30. 
</comment><comment author="spinscale" created="2014-04-30T14:37:03Z" id="41803740">hm, so something is closing your network connection inbetween it seems. I guess there is no stacktrace in your logfiles.Is it possible that you have configured either of the two operating systems to close TCP connections after a certain time? Is there possibly a firewall inbetween which could do that?
</comment><comment author="SomeProgrammer85" created="2014-05-02T14:35:26Z" id="42038295">Thank you for the advise.
The OS does close the TCP connection after a long time, but in this case it's positive, because it forces a reconnect. After this it works again.
We will put the server in the same network as the cluster an see if it makes a difference.
</comment><comment author="SomeProgrammer85" created="2014-05-05T10:01:25Z" id="42173879">Hello again.

We put more logging into the code and we got this:

```
2014-05-02 11:03:39,430 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic06][hash name of node][elastic06][inet[/ip of node 6:9300]]], channel closed event
2014-05-02 11:03:39,430 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic05][hash name of node][elastic05][inet[/ip of node 5:9300]]], channel closed event
2014-05-02 11:03:39,432 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic02][hash name of node][elastic02][inet[/ip of node 2:9300]]], channel closed event
2014-05-02 11:03:39,432 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic03][hash name of node][elastic03][inet[/ip of node 3:9300]]], channel closed event
2014-05-02 11:03:39,434 WARN  [org.elasticsearch.transport.netty] [Mimir] exception caught on transport layer [[id: 0x50f404c3, /ip of webapp:40104 =&gt; /ip of node 4:9300]], closing connection
java.io.IOException: Die Wartezeit f&#252;r die Verbindung ist abgelaufen
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
2014-05-02 11:03:39,434 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic01][hash name of node][elastic01][inet[/ip of node 1:9300]]], channel closed event
2014-05-02 11:03:39,434 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic08][hash name of node][elastic08][inet[/ip of node 8:9300]]], channel closed event
2014-05-02 11:03:39,437 WARN  [org.elasticsearch.transport.netty] [Mimir] exception caught on transport layer [[id: 0xcf068483, /ip of webapp:52499 =&gt; /ip of node 7:9300]], closing connection
java.io.IOException: Die Wartezeit f&#252;r die Verbindung ist abgelaufen
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
2014-05-02 11:03:39,438 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic04][hash name of node][elastic04][inet[/ip of node 4:9300]]], channel closed event
2014-05-02 11:03:39,440 DEBUG [org.elasticsearch.transport.netty] [Mimir] disconnected from [[elastic07][hash name of node][elastic07][inet[/ip of node 7:9300]]], channel
```

and this

![screenshot 007 10 10 2 231 - putty 2](https://cloud.githubusercontent.com/assets/7427819/2876732/e2e60938-d43b-11e3-80a2-a344f46a99a6.png)

The firewall is turned to a min between the cluster and the server.
</comment><comment author="spinscale" created="2014-05-05T11:33:11Z" id="42178967">Hey,

can you set the locale of your application to english/none and retry? The reason is, that deep down internally we need to check the message (not the type of the exception) to differentiate between normal and network problems, and those checks check against english language. I want to make sure, that this is not the reason you are running into timeouts.
</comment><comment author="clintongormley" created="2014-08-02T07:13:14Z" id="50956247">@SomeProgrammer85 Has this issue been resolved?
</comment><comment author="SomeProgrammer85" created="2014-09-24T09:05:52Z" id="56643465">Sorry for the late answer. I was quite busy.
Yes, the issue has been solved. The config of the firewall was the problem.
It's a known problem. It also appears when several nodes are not in the same network.

The standard tcp keepalive from linux is set to 7200 sec and the firewall is waiting for it after 5000 sec.
So for the firewall the connection is dead, therefore it closes it.
The tcp keepalive must be set lower than the value in the firewall.
This can be done with the following command:
sudo sysctl net.ipv4.tcp_keepalive_time=4500

@spinscale sorry for not double checking the firewall. had to fight the IT to get it...
thx for the time and help
</comment><comment author="fransflippo" created="2015-03-24T15:45:05Z" id="85567951">We have a similar issue. Also ES nodes on a different network than our application and connections dying at seemingly random times. What I don't understand is the documentation seems to say the connections are tested with a ping every 5 seconds (http://www.elastic.co/guide/en/elasticsearch/client/java-api/current/client.html#transport-client). How is it then possible that they go idle? Or am I misunderstand what it says there?

Doesn't ES try to reconnect the minute it finds out the ping isn't answered?

Hope you can shed some light on this :)
</comment><comment author="DodiChen" created="2016-08-11T03:33:50Z" id="239064724">Hi, I met the same issue. But i closed the firewall, the issue happened again.
Channel closed event, and node not connected.
I run ES in windows server 2008.
Three servers, six nodes. Sometimes, one or two nodes will disconnect from master .
</comment><comment author="SomeProgrammer85" created="2016-08-11T09:06:18Z" id="239108624">Hi, I never worked with the combination between ES and a windows server, so I can only do a little guessing.
So I guess if this is caused by the same issue it still could be the TCP keepalive.
Maybe this is the reason:

&gt; If the keepalive interval that the upper-layer protocol uses is less than the TCP keepalive interval, TCP keepalive value is never sent.

Got it from here: [Additional Registry Entries](https://technet.microsoft.com/de-de/library/dd349797%28v=ws.10%29.aspx)

Hope it helps
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow elastic search to only read files in template folder. Also if template is considered to be in JSON only, add a check for "*.json" extension. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5956</link><project id="" key="" /><description>```
[2014-04-28 06:56:47,396][WARN ][cluster.metadata         ] [Batman(ES-Master)] [app-logstash-2013.05.01] failed to read template [/u001/logparser/tools/elasticsearch/config/templates/.svn] from config
java.io.FileNotFoundException: /u001/logparser/tools/elasticsearch/config/templates/.svn (Is a directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:137)
        at org.elasticsearch.common.io.Streams.copyToByteArray(Streams.java:84)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.findTemplates(MetaDataCreateIndexService.java:485)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$300(MetaDataCreateIndexService.java:86)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:175)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:300)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
```
</description><key id="32335949">5956</key><summary>Allow elastic search to only read files in template folder. Also if template is considered to be in JSON only, add a check for "*.json" extension. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anandaverma</reporter><labels><label>:Index Templates</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-04-28T07:02:45Z</created><updated>2015-10-14T16:19:29Z</updated><resolved>2015-10-14T16:19:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-28T07:11:45Z" id="41529156">May be should ignore dirs and only take into account files whatever the extension is?
</comment><comment author="spinscale" created="2014-04-29T02:22:37Z" id="41636102">as the templates could also be in any format the `XContentParser` is able to parse, we should check if it is a file and readable I guess...
</comment><comment author="anandaverma" created="2014-05-02T16:36:02Z" id="42051307">+1 spinscale  @spinscale's thought.
</comment><comment author="clintongormley" created="2015-10-14T16:19:29Z" id="148104605">File-based index templates are no longer supported https://github.com/elastic/elasticsearch/pull/11052
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use collectExistingBucket() if a bucket already exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5955</link><project id="" key="" /><description>In the case if a bucket already exists the resize check in collectBucket() is redundant, so instead aggregations should use collectExistingBucket() instead which doesn't check for resize or grows the docCount array. 
</description><key id="32327097">5955</key><summary>Use collectExistingBucket() if a bucket already exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-28T02:13:38Z</created><updated>2015-06-07T13:38:48Z</updated><resolved>2014-04-29T04:08:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-28T17:18:59Z" id="41586369">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run tests through forbidden apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5954</link><project id="" key="" /><description>Check the tests for forbidden api's.  Start with simple URL#getPath and URL#getFile that have issues accessing resources with spaces in the path.

This PR has #5950 merged in.
</description><key id="32320310">5954</key><summary>Run tests through forbidden apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-27T20:49:42Z</created><updated>2014-07-12T16:51:01Z</updated><resolved>2014-04-30T15:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-27T20:52:20Z" id="41508668">Once #5950 gets merged in I can rebase this.  Also, I used a separate `test-signatures.txt` file because there are a lot of tests using the api's inside the `core-signatures.txt` file and I am not sure all of these are issues in tests. 

/cc @s1monw 
</comment><comment author="javanna" created="2014-04-28T16:55:41Z" id="41583538">Hey @mattweber it would be great if you can rebase this so I can look into merging it in.
</comment><comment author="mattweber" created="2014-04-28T18:16:03Z" id="41593542">@javanna Rebased with latest master.
</comment><comment author="jpountz" created="2014-04-29T09:39:08Z" id="41657617">This looks good to me. It would be nice to avoid duplicating signatures between two files though. Maybe we could have eg.:
- `core-signatures` for src/main only
- `tests-signatures` for src/test only
- and `all-signatures` for both src/main and src/test?

This way we could put the URL/URI stuff in a single file?
</comment><comment author="mattweber" created="2014-04-29T16:24:20Z" id="41698214">Thanks @jpountz.  I will see if I can break it up into 3 files like this and update the PR when ready.
</comment><comment author="jpountz" created="2014-04-29T16:49:45Z" id="41701491">For now, I don't think we have test-specific signatures, so maybe we can just rename the file to `all-signatures` and apply it to source code as well?
</comment><comment author="mattweber" created="2014-04-29T18:44:24Z" id="41716161">@jpountz updated PR with your suggested changes.
</comment><comment author="jpountz" created="2014-04-30T15:58:31Z" id="41814548">Merged, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Guava 17</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5953</link><project id="" key="" /><description /><key id="32313424">5953</key><summary>Upgrade to Guava 17</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-27T15:19:56Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-04-28T09:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-28T08:59:56Z" id="41536516">LGTM
</comment><comment author="jpountz" created="2014-04-28T08:59:57Z" id="41536519">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hot_threads stacktraces can come from threads that are no longer hot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5952</link><project id="" key="" /><description>I frequently see hot_threads output like this:

```
   24.6% (122.8ms out of 500ms) cpu usage by thread 'elasticsearch[elastic1012][search][T#13]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:724)   
```

or when the system is under lower load, I see this:

```
    8.2% (41.1ms out of 500ms) cpu usage by thread 'elasticsearch[elastic1016][refresh][T#5]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:702)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(LinkedTransferQueue.java:1117)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:724)
```

It looks like hot_threads figures out what threads are hot and then takes stack trace snapshots.  I imagine this is significantly less load on the system then snapshotting all the threads during the load measurements but it makes the results less useful, I think.  Fine for _really_ long running stuff like merges, but searches tend to be bursty.

Its still super useful but I find myself using jstack and thread dumps now that I've squashed most of the long running problems.  Maybe something with hot_threads' stack trace merging that takes snapshots of all the threads and merges them would be more useful to me at this stage. 
</description><key id="32311471">5952</key><summary>hot_threads stacktraces can come from threads that are no longer hot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2014-04-27T13:33:44Z</created><updated>2014-10-24T08:49:08Z</updated><resolved>2014-10-24T08:49:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-14T06:36:29Z" id="48869430">I wonder if we could build something that snapshots in the background infrequently and could be queried like the hot threads.  You'd want to limit the results to the actual live threads.  At some point, though, you are just building a profiler.
</comment><comment author="clintongormley" created="2014-10-24T08:49:08Z" id="60360526">Yeah, this doesn't feel like something we could do performantly on a live cluster.  Hot threads is pretty configurable today, and at least you're aware of when you run it (rather than forgetting that you have this job running in the background consuming resources all the time).

I think we're going to pass on this one.  

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A couple small typos and one clarification.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5951</link><project id="" key="" /><description>Typos.
</description><key id="32302270">5951</key><summary>A couple small typos and one clarification.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">statwonk</reporter><labels /><created>2014-04-27T01:23:08Z</created><updated>2014-07-16T21:45:58Z</updated><resolved>2014-05-14T10:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-02T22:00:47Z" id="42083494">@statwonk Would you mind signing the [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can merge this change?
</comment><comment author="clintongormley" created="2014-05-14T10:10:41Z" id="43063454">CLA not signed - treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use URI vs URL accessing File from classpath.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5950</link><project id="" key="" /><description>URL escapes special characters such as spaces which
causes the resource to not be found when used to create
a File object.  Use URI.

Closes #5915
</description><key id="32299804">5950</key><summary>Use URI vs URL accessing File from classpath.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>test</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-26T22:22:13Z</created><updated>2014-06-20T10:03:18Z</updated><resolved>2014-04-28T16:51:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-27T20:14:54Z" id="41507542">Removed unused imports and fixed formatting.
</comment><comment author="mattweber" created="2014-04-28T14:34:55Z" id="41565363">Thanks for taking a look @javanna!  I replied to your comments.  If you feel I still need to change things, let me know and I will update the PR.
</comment><comment author="javanna" created="2014-04-28T15:18:55Z" id="41571075">Thank you for sending the PR @mattweber , I replied to your replies ;)
</comment><comment author="mattweber" created="2014-04-28T15:21:08Z" id="41571380">@javanna Thanks I followed up as well.
</comment><comment author="mattweber" created="2014-04-28T16:02:54Z" id="41576950">@javanna updated with your suggestions, how does this look?
</comment><comment author="javanna" created="2014-04-28T16:04:38Z" id="41577176">Looks good @mattweber , thanks a lot, will pull it in!
</comment><comment author="javanna" created="2014-04-28T16:51:47Z" id="41583093">Merged, thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] randomly introduced a client node within test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5949</link><project id="" key="" /><description>The default number of clients nodes is 1, applied to all cluster scopes (global, suite and test). Can be changed though through the newly added `@ClusterScope#numClientNodes`.

In our tests we currently refer to nodes in a generic way. All the tests that either stop or start nodes rely on the fact that those nodes hold data though. Made that clearer as that becomes more important when introducing other types of nodes within the test cluster. Reflected this by adapting and renaming the following methods in `TestCluster`:
- `ensureAtLeastNumNodes` to `ensureAtLeastNumDataNodes`
- `ensureAtMostNumNodes` to `ensureAtMostNumDataNodes`
- `stopRandomNode` to `stopRandomDataNode`

and the following ones in `ElasticsearchIntegrationTest`:
- `dataNodes` to `numDataNodes`.
- `@ClusterScope#numNodes` to `numDataNodes`
- `@ClusterScope#minNumNodes` to `minNumDataNodes`
- `@ClusterScope#maxNumNodes` to `maxNumDataNodes`

Added facilities to be able to deal with data nodes specifically, like for instance retrieve a client to a data node, or retrieve an instance of a class through guice only from data nodes.

Adapted existing tests to successfully run although there's a node client around.

Fixed _cat/allocation REST tests to make disk.total, disk.avail and disk.percent optional as client nodes won't return that info.
</description><key id="32297163">5949</key><summary>[TEST] randomly introduced a client node within test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-26T20:15:37Z</created><updated>2014-06-25T22:06:08Z</updated><resolved>2014-04-28T14:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-28T10:56:07Z" id="41545510">Hey @jpountz I added a couple of commits, the first one addresses your comments, the second one completes the renaming work, since I found some leftover `*nodes` methods that need to become `*dataNodes`.
</comment><comment author="jpountz" created="2014-04-28T11:02:56Z" id="41546001">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/allocation returns -1 as disk.total for clients nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5948</link><project id="" key="" /><description>No data gets allocated on client nodes, and a subest of the nodes stats information is not available for client nodes. That is why some columns in `_cat/allocation` might be empty, but there should be consistency on how null values are returned. The example below contains `0b` for `disk.used`, no value for `disk.available`, and `-1b` for `disk.total`:

```
0     0b             -1b    Lucas-MacBook-Air.local 192.168.0.13 Hazard
6 92.5gb 140.4gb 232.9gb 39 Lucas-MacBook-Air.local 192.168.0.13 Ningal
```

I think it should be as follows instead, with no value instead of `-1`, leaving `0b` for `disk.used` though:

```
0     0b                    Lucas-MacBook-Air.local 192.168.0.13 Hazard
6 92.5gb 140.4gb 232.9gb 39 Lucas-MacBook-Air.local 192.168.0.13 Ningal
```
</description><key id="32290220">5948</key><summary>_cat/allocation returns -1 as disk.total for clients nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-26T14:30:41Z</created><updated>2015-06-07T20:44:28Z</updated><resolved>2014-04-26T14:49:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Postings highlighter breaks on "." Full recreation attached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5947</link><project id="" key="" /><description>Postings highlighter in 1.0.3 only captures part of a field value before or after "." if the "." is present in the searched string

For example John M.Smith when searched on John will produce `"&lt;em&gt;John&lt;/em&gt; M."` highlight while search on Smith `"&lt;em&gt;Smith&lt;/em&gt;"`

Plain highlighter produces `"&lt;em&gt;John&lt;/em&gt; M. Smith"` as expected

https://gist.github.com/roytmana/11308459

cc @javanna 
</description><key id="32277387">5947</key><summary>Postings highlighter breaks on "." Full recreation attached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>non-issue</label></labels><created>2014-04-26T00:55:52Z</created><updated>2014-05-08T08:38:48Z</updated><resolved>2014-05-08T08:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2014-04-28T23:56:32Z" id="41628626">Any chance for a fix in 1.0.4?
</comment><comment author="lih515" created="2014-05-05T14:46:20Z" id="42195301">This issue was also found on our production. A quick fix is highly appreciated.
</comment><comment author="javanna" created="2014-05-07T15:37:45Z" id="42443085">Given that the postings highlighter uses a break iterator to split the text into sentences upfront, it will split fragments when it finds the '.' character. If you are highlighting a field that only contains names though, you'd want to disable the ability to have back multiple fragments and just highlight a single fragment containing the whole text. You can achieve this by using `"number_of_fragments": 0` (supported by all highlighter implementations).
</comment><comment author="roytmana" created="2014-05-08T03:26:55Z" id="42510079">Many thanks @javanna I missed in docs. I tried 1 thinking it will return a single fragment but it did not so I neglected to check the docs. Would you like me to close the ticket?
</comment><comment author="javanna" created="2014-05-08T08:38:48Z" id="42525397">No worries @roytmana! Seems like `"number_of_fragments": 0` is the way to go here, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ordinals start at 0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5946</link><project id="" key="" /><description>Our ordinals currently start at 1, like FieldCache did in older Lucene versions.
However, Lucene 4.2 changed it in order to make ordinals start at 0, using -1
as the ordinal for the missing value. We should switch to the same numbering as
Lucene for consistency. This also allows to remove some abstraction on top of
Lucene doc values.

Close #5871
</description><key id="32268023">5946</key><summary>Make ordinals start at 0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cache</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T21:25:31Z</created><updated>2015-06-06T17:01:08Z</updated><resolved>2014-04-28T08:23:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-26T14:52:45Z" id="41470725">this change LGTM why can't this go into 1.2?
</comment><comment author="martijnvg" created="2014-04-27T17:31:07Z" id="41502922">LGTM and I think this should go in 1.2 as well?
</comment><comment author="jpountz" created="2014-04-27T20:47:43Z" id="41508525">I just updated the tags. :-) Will push soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>InternalIndexShard callback handling of failure is missing/wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5945</link><project id="" key="" /><description>In `InternalIndexShard#index` method calls `failedIndex` callback (to reduce current counters) when a `RuntimeException` happens, it should do it for any `Throwable`. Also, we are missing similar logic in create, and we should properly handle delete as well.

Might make sense to have a single post callback, and not post/failed, with an Throwable passed to indicate if it was a failure or not.
</description><key id="32255038">5945</key><summary>InternalIndexShard callback handling of failure is missing/wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>low hanging fruit</label></labels><created>2014-04-25T18:22:31Z</created><updated>2014-12-05T23:05:57Z</updated><resolved>2014-12-05T23:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Significant_terms agg: added option for a backgroundFilter </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5944</link><project id="" key="" /><description>Allows for a narrowed background context in analysis of term frequencies.
</description><key id="32252565">5944</key><summary>Significant_terms agg: added option for a backgroundFilter </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T17:47:01Z</created><updated>2015-06-07T10:37:09Z</updated><resolved>2014-05-13T08:26:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-30T13:02:56Z" id="41793452">The patch looks good. Can we also have a test that makes sure that this agg finds the right terms when a background filter is set?
</comment><comment author="jpountz" created="2014-05-12T10:48:29Z" id="42818304">@markharwood I just did another review and left a couple of minor comments. Other than that it looks good to me, it's nice that the documentation makes clear that although convenient this option can be very slow.
</comment><comment author="jpountz" created="2014-05-12T16:35:09Z" id="42855444">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable dynamic scripting by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5943</link><project id="" key="" /><description>Also modifies the documentation for this, adding an example of using a preloaded script.

Closes #5853
</description><key id="32244432">5943</key><summary>Disable dynamic scripting by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T16:01:13Z</created><updated>2015-06-06T17:00:01Z</updated><resolved>2014-04-25T21:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-25T17:31:41Z" id="41418642">Added some commits adding a `.templatingOnly()` method to the `ScriptEngineService` interface so Mustache can be enabled regardless of dynamic scripting settings. Also enabled dynamic scripting in tests since it is used in many places.
</comment><comment author="s1monw" created="2014-04-25T20:28:36Z" id="41435977">cool stuff
- I think the docs need to state that this came with ES `1.2.0`
- can we maybe name the getter `sandboxed()` instead of `templateingOnly()` because this is what we essentially checking if the script lang / type is sandboxed?
</comment><comment author="dakrone" created="2014-04-25T20:33:47Z" id="41436414">@s1monw good ideas! I've added both changes. The `sandboxed` name is much better and opens the path for sandboxed groovy scripts in the future.
</comment><comment author="s1monw" created="2014-04-25T21:02:12Z" id="41439049">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Word delimiter creating unexpected tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5942</link><project id="" key="" /><description>I just wanted to share complete config file wherein I am able to see this problem with word delimiter (unless I got the config wrong). My config is below and if I analyze the string "650-454-2343", I get the following tokens:

```
650-454-2343 [expected since we have "preserve_original": true]
650 [unexpected]
454 [unexpected]
2343 [unexpected]
6504542343 [expected since we have "catenate_all": true]
```

This looks like a bug and hence created an entry for the same.

{
  "settings": {
    "analysis": {
      "analyzer": {
        "phoneAnalyzer": {
          "type": "custom",
          "tokenizer": "whitespace",
          "filter": [
            "word_delimiter_for_phone"
          ]
        }
      },
      "filter": {
        "word_delimiter_for_phone": {
          "type": "word_delimiter",
          "catenate_all": true,
          "generate_number_parts ": false,
          "split_on_case_change": false,
          "generate_word_parts": false,
          "split_on_numerics": false,
          "preserve_original": true
        }
      }
    }
  },
  "mappings": {
    "my_type": {
      "properties": {
        "phone": {
          "type": "string",
          "index_analyzer": "phoneAnalyzer",
          "include_in_all": false
        }
      }
    }
  }
}
</description><key id="32243547">5942</key><summary>Word delimiter creating unexpected tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amitsoni13</reporter><labels><label>non-issue</label></labels><created>2014-04-25T15:50:07Z</created><updated>2014-04-28T05:05:19Z</updated><resolved>2014-04-25T15:54:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-25T15:54:55Z" id="41408516">you need to check the `type_table` it will split unless a char is mapped to a char or a digit by default. see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-word-delimiter-tokenfilter.html

please use the mailing list for questions like this!
</comment><comment author="amitsoni13" created="2014-04-28T05:05:19Z" id="41523298">Hello Simon - I had a look and tried a few things however still couldn't out a way to let the analyzer NOT split the number parts. Can you pl point me to any sample code? I tried to look for examples on the internet but couldn't find anything.
Your help is greatly appreciated!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explore using timebased decentralized UUID for autogenerated IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5941</link><project id="" key="" /><description>Today we use UUIDs which are very costly to compute since they use `SecureRandom` and they have some properties that might not play very well / or as well as they could with the Lucene internal datastructures since they don't share a common prefix. With a time based ID we can potentially remove the need for bloom fitlers on the ID fields since the shared prefixes across the IDs might be all in memory and that would speedup updates dramatically. Something along the lines of [flake ids](http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/) would be very interesting since it has this property
</description><key id="32224062">5941</key><summary>Explore using timebased decentralized UUID for autogenerated IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T11:27:15Z</created><updated>2014-09-11T10:15:38Z</updated><resolved>2014-09-02T13:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-04-25T11:50:08Z" id="41384211">What we are proposing is since a machine may have several es instances running in parallel. We will generate a secure xor of the mac address, prefixed by a 64bit timestamp and postfixed by an atomic counter. 

timestamp64bit-macAddr-counter
</comment><comment author="mikemccand" created="2014-04-25T13:29:27Z" id="41392171">+1 for prefixing the ID with timestamp, or anything "predictable": this is better for Lucene's term dictionary since sometimes it will be able to short-circuit a lookup and know the ID cannot reside in a given segment.  It's also important that the IDs are fixed-length, so that all terms happen in the "leaf blocks" of the dictionary.  Finally, something like base36 is best, since we tell the terms dict writing to put 25 - 48 terms per block (so up to base 48 should be good too). 
</comment><comment author="GaelTadh" created="2014-04-25T14:01:51Z" id="41395482">@mikemccand why does the encoding matter ? Or do you mean between 36 and 48 bytes long ?
</comment><comment author="mikemccand" created="2014-04-25T14:58:22Z" id="41401971">@GaelTadh what I mean is each digit should have no more than 48 values (and lower would be better: less scanning within a block).  This is because the terms dictionary assigns terms into blocks of at most 48 terms at once.

However ... thinking about this more, I think this matters less, since the IDs won't be sequential here.
</comment><comment author="GaelTadh" created="2014-04-25T15:14:14Z" id="41403836">Ahh ok that makes sense. 
</comment><comment author="GaelTadh" created="2014-04-28T13:33:52Z" id="41558262">I used jmh (http://openjdk.java.net/projects/code-tools/jmh/) to do some micro benchmarks on the following code : 

,,,,

```
@GenerateMicroBenchmark
public void secureRandomUUID(){
    randomBase64UUID(SecureRandomHolder.INSTANCE);
}

@GenerateMicroBenchmark
public void insecureRandomUUID(){
    randomBase64UUID(new Random());
}


@GenerateMicroBenchmark
public void timestampUUID(){
    timestampBase64UUID();
}
```

,,,

Which gave the following output : 

| Benchmark | Mode | Samples | Mean | Mean error | Units |
| --- | --- | --- | --- | --- | --- |
| o.s.UUIDBenchmark.insecureRandomUUID | thrpt | 200 | 5155.120 | 16.137 | ops/ms |
| o.s.UUIDBenchmark.secureRandomUUID | thrpt | 200 | 469.264 | 3.287 | ops/ms |
| o.s.UUIDBenchmark.timestampUUID | thrpt | 200 | 5703.520 | 13.350 | ops/ms |
</comment><comment author="GaelTadh" created="2014-04-28T14:04:21Z" id="41561547">I'm gonna run these again and remove the new call from insecureRandom.
</comment><comment author="GaelTadh" created="2014-04-28T14:37:05Z" id="41565635">| Benchmark | Mode | Samples | Mean | Mean error | Units |
| --- | --- | --- | --- | --- | --- |
| o.s.UUIDBenchmark.insecureRandomUUID | thrpt | 200 | 7348.913 | 16.807 | ops/ms |
| o.s.UUIDBenchmark.secureRandomUUID | thrpt | 200 | 475.986 | 1.489 | ops/ms |
| o.s.UUIDBenchmark.timestampUUID | thrpt | 200 | 5698.841 | 23.908 | ops/ms |
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score multi values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5940</link><project id="" key="" /><description>Decay functions currently only use the first value in a field that contains
multiple values to compute the distance to the origin. Instead, they should
consider all distances if more values are in the field and then use
one of min/max/sum/avg which is defined by the user.

closes #3960
</description><key id="32217304">5940</key><summary>Function score multi values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T09:31:35Z</created><updated>2015-06-06T18:32:32Z</updated><resolved>2014-04-28T09:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-04-25T13:57:52Z" id="41395078">Thanks for the quick review! I updated the branch, added a commit for each comment.
</comment><comment author="jpountz" created="2014-04-25T15:19:07Z" id="41404398">LGTM.
</comment><comment author="s1monw" created="2014-04-25T15:30:29Z" id="41405705">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Create/Update/Delete classes less mutable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5939</link><project id="" key="" /><description>Today we use a builder pattern / setters to set relevant information
to Engine#Delete|Create|Index. Yet almost all the values are required
but they are not passed via ctor arguments but via an error prone builder
pattern. If we add a required argument we should see compile errors on that
level to make sure we don't miss any place to set them.

Prerequisite for #5917
</description><key id="32214786">5939</key><summary>Make Create/Update/Delete classes less mutable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-25T08:47:48Z</created><updated>2015-06-06T17:00:13Z</updated><resolved>2014-04-25T10:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-25T08:52:02Z" id="41371696">LGTM
</comment><comment author="s1monw" created="2014-04-25T10:41:03Z" id="41379713">pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using Snapshot-Restore API as a smart way to change mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5938</link><project id="" key="" /><description>The idea is for the Restore API to be able to fit the snapshoted data into a new mapping, if the new mapping is compatible. 

For example, one may change the analyzer for a string field, and the Restore API should fill the new index with the same string data but honoring the new mapping.

It seems to me that according to #5210, it was already the case previously. 

Here is what I propose:
1. The Restore API should expose a parameter "new_index"  (or even "new_type" if type level restore is possible) which is the name of the index that use the new mapping.
   If no "new_index" is specified then the fixed in #5210 should be effective.
2. Ultimately an API that hide the complexity of same cluster restore. Hence one can change the mapping of a index on the fly with a single API call.
3. (Related) Maybe the same can be done by "update by query", if there is a way to traverse the whole index/type natively and use the result to fill the new index.
</description><key id="32212490">5938</key><summary>Using Snapshot-Restore API as a smart way to change mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lordaugustus</reporter><labels><label>non-issue</label></labels><created>2014-04-25T08:07:18Z</created><updated>2014-05-10T00:46:18Z</updated><resolved>2014-05-10T00:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-25T14:21:02Z" id="41397506">Hi @lordaugustus I'm afraid what you are asking is not possible through snapshot/restore API as it doesn't actually reindex any data, it just copies around segments from the different lucene indices, without making any change to the underlying data structures. What you are asking is more like a reindex operation, which indeed would require to reindex the documents and possibly allow to change their mappings, more like #492 or #1655.

Note that this can be done by using a scan search and reindexing into a new index all of the returned documents.
</comment><comment author="javanna" created="2014-05-10T00:46:18Z" id="42726799">Closing since what was asked is not viable using snapshot/restore api, as expained above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix code typo in FieldSortBuilder.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5937</link><project id="" key="" /><description>typo
</description><key id="32179160">5937</key><summary>Fix code typo in FieldSortBuilder.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cccabot</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T19:41:42Z</created><updated>2015-06-07T13:39:17Z</updated><resolved>2014-05-10T00:58:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-25T14:22:08Z" id="41397629">Hi @cccabot thanks for the fix, could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can go ahead and merge it?
</comment><comment author="cccabot" created="2014-04-28T19:29:51Z" id="41602385">CLA signed. I believe I have fixed all instances of this typo in this file (total of 4).
</comment><comment author="pickypg" created="2014-04-28T20:26:47Z" id="41608874">LGTM
</comment><comment author="javanna" created="2014-05-10T00:58:41Z" id="42727165">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard allocation filtering: Prefer certain nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5936</link><project id="" key="" /><description>`index.routing.allocation.include._host` allows us to pin indices to certain hosts, but it would be also nice to have the functionality to prefer certain hosts (say `index.routing.allocation.prefer._host`) and fall back to whatever hosts that are available if the preferred hosts are not up (bonus: then later move shards over once they become available). 
</description><key id="32178992">5936</key><summary>Shard allocation filtering: Prefer certain nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppat</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2014-04-24T19:39:42Z</created><updated>2015-11-13T11:01:23Z</updated><resolved>2015-11-13T11:01:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-13T11:01:23Z" id="156398166">No replies for more than one year: closing. We discussed it Fixit Friday and also had some concerns about making the allocation rules more complicated than they are today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add factories for some Tokenizers/TokenFilters in Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5935</link><project id="" key="" /><description>There are a few currently not exposed, see annotated list in https://github.com/elasticsearch/elasticsearch/commit/f836c327683dbfc71d82e9e3cf2ac29303302eb2

It would be good if someone reviewed the list, I may have missed some or made mistakes too.
</description><key id="32175931">5935</key><summary>Analysis: Add factories for some Tokenizers/TokenFilters in Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-04-24T18:59:32Z</created><updated>2014-07-16T11:56:21Z</updated><resolved>2014-07-03T09:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T19:20:00Z" id="41320776">should this be `1.2` as well?
</comment><comment author="nik9000" created="2014-04-24T19:21:02Z" id="41320889">I made an effort to get all the language analyzers that are exposed buildable as custom analyzers and uncovered a few of these: #5814.  It does the exposing for some of them but includes the custom analyzer stuff too....
</comment><comment author="rmuir" created="2014-04-24T19:31:24Z" id="41321933">@s1monw it would probably be good for 1.2. I can help and look at the ones @nik9000 did already. Some of these may not make sense but some of the others are useful.
</comment><comment author="s1monw" created="2014-04-24T19:32:15Z" id="41322008">oh I was mistaken - the commit doesn't fix this issue, nevermind leave it for 2.0.0 for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tie in IndexWriter's infoStream output to "lucene.iw" logger with level=TRACE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5934</link><project id="" key="" /><description>See issue #5891
</description><key id="32175301">5934</key><summary>Tie in IndexWriter's infoStream output to "lucene.iw" logger with level=TRACE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T18:51:21Z</created><updated>2015-06-07T14:25:35Z</updated><resolved>2014-04-24T20:47:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T19:46:47Z" id="41323435">I left a minor comment - I think it's ready. can you squash and push to 1.x and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix TemplateQueryParser swallows additional parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5933</link><project id="" key="" /><description>Request parameters such as "size" and "fields" were ignored when
placed after the template query in the reqest.
</description><key id="32171255">5933</key><summary>Fix TemplateQueryParser swallows additional parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search Templates</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T18:01:47Z</created><updated>2015-06-07T20:44:54Z</updated><resolved>2014-04-25T06:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T20:01:52Z" id="41325088">good catch! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.8.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5932</link><project id="" key="" /><description>Lucene 4.8 included several bugfixes and API Improvements. The most relevant for Elasticsearch are:
- LUCENE-4747, LUCENE-5514: Move to Java 7 as minimum Java version.
- LUCENE-5516: MergeScheduler#merge() now accepts a MergeTrigger as
  well as a boolean that indicates if a new merge was found in the
  caller thread before the scheduler was called.
- LUCENE-5487: Separated bulk scorer (new Weight.bulkScorer method)
  from normal scoring (Weight.scorer) for those queries that can do
  bulk scoring more efficiently, e.g. BooleanQuery in some cases.
  This also simplified the Weight.scorer API by removing the two
  confusing booleans.
- LUCENE-5497: HunspellStemFilter properly handles escaped terms and
  affixes without conditions.
- LUCENE-5468: HunspellStemFilter uses 10 to 100x less RAM. It also
  loads all known openoffice dictionaries without error, and supports
  an additional longestOnly option for a less aggressive approach.
- LUCENE-5505: HunspellStemFilter ignores BOM markers in dictionaries
  and handles varying types of whitespace in SET/FLAG commands.
- LUCENE-5507: Fix HunspellStemFilter loading of dictionaries with
  large amounts of aliases etc before the encoding declaration.
- LUCENE-5111: Fix WordDelimiterFilter to return offsets in correct order.
- LUCENE-4848: Use Java 7 NIO2-FileChannel instead of RandomAccessFile
  for NIOFSDirectory and MMapDirectory. This allows to delete open files
  on Windows if NIOFSDirectory is used, mmapped files are still locked.

The changes to WordDelimiterFilter causes a behavior change in the next Elasticsearch release. The previous version of this tokenfilter will still be available for indices that are created in previous version or with the according lucene version.
</description><key id="32148922">5932</key><summary>Upgrade to Lucene 4.8.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>blocker</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T13:37:56Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-04-28T10:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-04-25T06:33:35Z" id="41362762">@s1monw is there any estimation about when we can expect official Elasticsearch release based on Lucene 4.8.0 supporting all the Hunspell improvements? ATM this PR is assigned to v1.2.0 and above.
</comment><comment author="s1monw" created="2014-04-25T06:59:48Z" id="41364052">@lukas-vlcek it will be release with ES `1.2.0` This will likely happen in the near future but given the fact that we are moving with Lucene to Java 1.7 as a minimum requirement we are stretching the release cycle a little longer as in the past. The Lucene 4.8 release is more about robustness since it now has checksumming on the FS level #5924 which we need / want to integrate before the release is due. 

We can't backport this 4.8 dependency to `1.1.2` since it requires Java 1.7 vs Java 1.6 being the minimum requirement on `1.1.1`. I hope this answers your question.
</comment><comment author="lukas-vlcek" created="2014-04-25T07:26:23Z" id="41365689">@s1monw yes it does, thanks. (Though it does not address the question of time estimate - days, weeks?)
</comment><comment author="s1monw" created="2014-04-25T07:33:51Z" id="41366162">we don't have a fix release date but I am pretty sure it will happen in the next 4 weeks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings lost after cluster rolling restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5931</link><project id="" key="" /><description>Hi,

We have 3-node ELK cluster running 1.0.1. To activate a heapchange we did a node-by-node restart.

Unwanted result is loss of mappings on a small number of indices, causing errors like below when querying with Kibana:

```
Caused by: org.elasticsearch.search.SearchParseException: [logstash-pro-endeca-2014.04.13][3]: query[filtered((@source_host:"pro end app 006"))-&gt;cache(@timestamp:[2014-04-10T14:07:21+02:00 TO 2014-04-24T14:07:21+02:00])],from[0],size[50]: Parse Failure [No mapping fo
und for [@timestamp] in order to sort on]
        at org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:198)
        at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:172)
        at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:90)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:583)
        ... 10 more
```

After copying the mapping from a sibling index everything is OK again.
</description><key id="32142859">5931</key><summary>Mappings lost after cluster rolling restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">rtoma</reporter><labels /><created>2014-04-24T12:11:24Z</created><updated>2014-05-07T12:36:20Z</updated><resolved>2014-05-07T12:36:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-04-24T12:29:18Z" id="41273865">Could be that you ran into this issue: https://github.com/elasticsearch/elasticsearch/pull/5623

Do you still have the mappings of logstash-pro-endeca-2014.04.13 from after the restart before copying the mappings back? 
</comment><comment author="rtoma" created="2014-04-24T14:40:56Z" id="41287974">That issue could be the cause for this issue, as we use the bulk api - in async replication mode - for continuous feeding of log events.

The mappings for this endeca index was "{}".
Another index with missing mappings, had this mapping:

```
{
  "logstash-pro-log4json-2014.04.14" : {
    "mappings" : {
      "log4json" : {
        "properties" : { }
      }
    }
  }
}
```
</comment><comment author="brwe" created="2014-04-25T07:18:57Z" id="41365183">The missing mapping {} is the symptom of #5623 but the empty  type ("log4json") in the other index is not. Also, I did not test with async mode. 
I'll see if I can reproduce that.
</comment><comment author="kimchy" created="2014-04-25T15:03:17Z" id="41402554">both can potentially be caused by #5623, thats not to say we might not have another bug, but I can see how both problems can happen because of it.
</comment><comment author="brwe" created="2014-04-28T09:55:10Z" id="41540926">I have to admit that I do not see it yet. Can you give me a hint?
</comment><comment author="rtoma" created="2014-05-07T12:05:56Z" id="42418709">I scanned the changelogs and found this bug fixed on 1.1.1 (April 16th). I guess its time to upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot aggregate on completion suggester field in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5930</link><project id="" key="" /><description>In the mapping, the field named domain (String value) is a completion suggester as below:

``` JSON
"domain": {
                  "max_input_length": 50,
                  "preserve_separators": true,
                  "payloads": false,
                  "analyzer": "simple",
                  "preserve_position_increments": true,
                  "type": "completion"
               }
```

Currently, I would like to aggregate based on the value in the field "domain". The example of a query in Sense is shown below. 
localhost:9200

``` JSON
POST /ourindex/_search
{
   "query": {
     "match_all": {}
   },
   "aggs": {
      "test": {
         "terms": {
            "field": "domain"
         }
      }
   }
}
```

When the query is run, it returns NullPointerException. The detailed response is following.

``` LOG
{
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[iQfn8u55QoqZSnlXEGGUlw][ourindex][4]: SearchParseException[[ourindex][4]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   \"query\": {\r\n     \"match_all\": {}\r\n   },\r\n   \"aggs\": {\r\n      \"test\": {\r\n         \"terms\": {\r\n            \"field\": \"domain\"\r\n         }\r\n      }\r\n   }\r\n}\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][2]: SearchParseException[[ourindex][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   \"query\": {\r\n     \"match_all\": {}\r\n   },\r\n   \"aggs\": {\r\n      \"test\": {\r\n         \"terms\": {\r\n            \"field\": \"domain\"\r\n         }\r\n      }\r\n   }\r\n}\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][3]: SearchParseException[[ourindex][3]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   \"query\": {\r\n     \"match_all\": {}\r\n   },\r\n   \"aggs\": {\r\n      \"test\": {\r\n         \"terms\": {\r\n            \"field\": \"domain\"\r\n         }\r\n      }\r\n   }\r\n}\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][0]: SearchParseException[[ourindex][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   \"query\": {\r\n     \"match_all\": {}\r\n   },\r\n   \"aggs\": {\r\n      \"test\": {\r\n         \"terms\": {\r\n            \"field\": \"domain\"\r\n         }\r\n      }\r\n   }\r\n}\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][1]: SearchParseException[[ourindex][1]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   \"query\": {\r\n     \"match_all\": {}\r\n   },\r\n   \"aggs\": {\r\n      \"test\": {\r\n         \"terms\": {\r\n            \"field\": \"domain\"\r\n         }\r\n      }\r\n   }\r\n}\n]]]; nested: NullPointerException; }]",
   "status": 400
}
```

Does the type of the suggester matter? How can it work to aggregate by the values of domain field?
</description><key id="32139543">5930</key><summary>Cannot aggregate on completion suggester field in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">moqichenle</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T11:15:06Z</created><updated>2014-05-19T06:42:14Z</updated><resolved>2014-04-29T13:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T14:35:27Z" id="41287267">well I don't think it shoudl throw a NPE but on the other hand why whould you want to aggregate on a suggest field. you should use multi field for this IMO. @jpountz can you take a look at this NPE and provide a better error message?
</comment><comment author="brwe" created="2014-04-29T11:22:18Z" id="41664638">If I understand correctly, currently the completion suggester creates a field (called "domain") in the above example which contains the content of "input" and is searchable but one cannot aggregate, because of the special format of this type ( @jpountz please correct if I am wrong). A better error message would be useful.

&gt; you should use multi field for this IMO.

It is unclear to me how `multi_field` can be used with the suggester here. The [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html) is not too explicit: 

&gt; Even though you are losing most of the features of the completion suggest, you can opt in for the shortest form, which even allows you to use inside of multi fields. But keep in mind, that you will not be able to use several inputs, an output, payloads or weights.

I would have expected `multi_field` to work like this:

```
{
   "domain": {
      "type": "multi_field",
      "fields": {
         "domain": {
            "max_input_length": 50,
            "preserve_separators": true,
            "payloads": false,
            "analyzer": "simple",
            "preserve_position_increments": true,
            "type": "completion"
         },
         "input": {
            "type": "string"
         }
      }
   }
}
```

and then expected that aggregation would work on "domain.input". Is that supposed to work so so? If so, I think it is broken.

In any case, it might be nice to have an option to make all parameter fields searchable and and also allow aggregations on them. Currently, the only way to do this seems to be to add the parameters to a field with a different name than the suggestion field. Instead, suggester could create these fields automatically if the user wants to. We could have something like:

```
{
   "domain": {
      "max_input_length": 50,
      "preserve_separators": true,
      "payloads": false,
      "analyzer": "simple",
      "preserve_position_increments": true,
      "type": "completion",
      "input": {
              # Here be whatever the user likes to configure for indexing
      },
      "output": {
             ...
      },
      ...
   }
}
```

which would cause creation of fields `"domain.input"`, `"domain.output"` and so on with user defined indexing options.

Does that make sense?
</comment><comment author="moqichenle" created="2014-04-29T11:52:32Z" id="41666712">I tried multi_fiield and it works as what I wanted. 
The mapping I use is similar to what @brwe mentioned.

``` JSON
{
   "domain": {
      "type": "multi_field",
      "fields": {
         "domain": {
            "max_input_length": 50,
            "preserve_separators": true,
            "payloads": false,
            "analyzer": "simple",
            "preserve_position_increments": true,
            "type": "completion"
         },
         "input": {
            "type": "string"
         }
      }
   }
}
```

Thank you for the hint @s1monw .
</comment><comment author="jpountz" created="2014-04-29T11:53:27Z" id="41666789">@brwe I just tried a multi-field setup, and this seems to work fine:

``` json
DELETE /test

PUT /test
{
    "mappings": {
        "test": {
            "properties": {
                "my_field": {
                    "type": "completion",
                    "analyzer": "simple",
                    "fields": {
                        "raw": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            }
        }
    }
}

PUT /test/test/1
{
    "my_field": "foo bar"
}

PUT /test/test/2
{
    "my_field": "foo bar baz"
}

POST /test/_refresh

GET /test/_suggest
{
    "my-suggest" : {
        "text" : "foo b",
        "completion" : {
            "field" : "my_field"
        }
    }
}

GET /test/_search
{
    "aggs": {
        "my_field_values": {
            "terms": {
                "field": "my_field.raw"
            }
        }
    }
}
```

Adding capabilities to store `domain.input` and `domain.output` as you describe feels to me like reinventing multi-fields. I think we should rather fix the error message when trying to aggregate on a completion field and recommend on either indexing the field twice (eg. if you need to specify input and output separately) or using a multi-field as above for the simple cases.
</comment><comment author="brwe" created="2014-04-29T12:29:25Z" id="41669479">Ah! Now I get it. I was trying out how this works when you explicitly provide "input" and "output" when indexing. I guess this is meant by "opt in for the shortest form".
Thanks for clarifying that this is not what was meant!

&gt; I think we should rather fix the error message

Ok, I'l just add the more descriptive error message.
</comment><comment author="s1monw" created="2014-05-18T10:06:42Z" id="43435980">@brwe can you port this to `1.1.2`?
</comment><comment author="brwe" created="2014-05-19T06:41:27Z" id="43470259">done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version types `EXTERNAL` &amp; `EXTERNAL_GTE` test for version equality in read operation &amp; disallow them in the Update API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5929</link><project id="" key="" /><description>Separate version check logic for reads and writes for all version types, which allows different behavior in these cases.
Change `VersionType.EXTERNAL` &amp; `VersionType.EXTERNAL_GTE` to behave the same as `VersionType.INTERNAL` for read operations.
The previous behavior was fit for writes but is useless in reads.

This commit also makes the usage of `EXTERNAL` &amp; `EXTERNAL_GTE` in the update api raise a validation error as it make cause data to
be lost.

Closes #5663 , closes #5661
</description><key id="32138822">5929</key><summary>Version types `EXTERNAL` &amp; `EXTERNAL_GTE` test for version equality in read operation &amp; disallow them in the Update API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:CRUD</label><label>blocker</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-24T11:02:12Z</created><updated>2015-06-06T17:00:30Z</updated><resolved>2014-04-25T21:07:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T14:32:20Z" id="41286873">I left some comments, looks good but it seems like it has lots of unrelated changes?
</comment><comment author="s1monw" created="2014-04-25T20:14:56Z" id="41434777">ok good! I think this is good to go! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request : improvement of the perceived relevance of the highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5928</link><project id="" key="" /><description>Hi, my name's Damien, I'm working for a french company, Orange.

We discovered Elasticsearch a few months ago and we are very enthusiasts about it, great tool really, so we decided to make a proof a of concept with it. The aim is to replace our internal (home-made) search engines with ES.

To this point we are really satisfied of the results we get in terms of search relevance thanks to all the possible tunings : mappings, lots of query types, filters, etc. But we are facing a big problem, a blocking one. We are not able to obtain what we want from the highlighting part.

To be precise, there are two types of relevance for us :
- the "real" relevance : Do the documents I obtain from my search are relevant according to my search query (and maybe other criterias) ? Is the result exhaustive ? Are the documents well ordered ? etc. It involves reading all the returned documents in their entirety to answer those questions.
- the "perceived" relevance : When I get the highlighted parts of the returned documents do I feel that the results are relevant ? It involves just reading a small part of the document to answer the relevance question.

Actually we have a good relevance and a poor perceived relevance (according to our criterias of what is a good preceived relevance).
This point is a blocking one and our marketing department will not give its approval for the migration until this problem is solved.

I think that we read the the docs correctly, we tried a lot of things, so we need to submit a feature request to your team. Let me explain what we need with some examples, and if we missed something in the docs please accept our apologies.

&lt;h2&gt;First example&lt;/h2&gt;

What we want : no matter the number of fragments, we want a constant total size (ex: 2x30, 3x20, 4x15, ... total size is 60).

What we have:
We index a sentence 90 chars long (approximately). We make two searches with 2 fragments of size 30 specified. The first time we search with two words, we get 2 fragments, each of size 30, everything fine, total size is 60 (approximately). The second time we search with only one word so obviously we can have only one fragment and the total size is 30, not 60.

``` bash
# Deleting all indexes
curl -XDELETE 'localhost:9200/_all?pretty=yes'

# Putting some data, approximately 70 characters long.
curl -XPUT 'localhost:9200/index/doc/1?pretty=yes' -d '
{
    "text": "My parents had the good idea of offering me a dog for my tenth birthday. I like him a lot."
}
'

# Refreshing the index.
curl -XPOST 'http://localhost:9200/_refresh?pretty=yes'

# Searching with two keywords for two fragments.
curl -XGET 'localhost:9200/index/doc/_search?pretty=yes' -d '
{
    "query": {
        "match": {
            "text": {
                "query": "idea birthday",
                "operator" : "and"
            }
        }
    },
    "highlight": {
        "fields": {
            "text": {
                "number_of_fragments": 2,
                "fragment_size": 20
            }
        }
    }
}
'

# Searching with one keyword for two fragments.
curl -XGET 'localhost:9200/index/doc/_search?pretty=yes' -d '
{
    "query": {
        "match": {
            "text": "idea"
        }
    },
    "highlight": {
        "fields": {
            "text": {
                "number_of_fragments": 2,
                "fragment_size": 20
            }
        }
    }
}
'
```

Here we specify:
"number_of_fragments": 2
"fragment_size": 30

We would prefer:
"number_of_fragments": 2
"preferred_fragment_size": 30
"total_size": 60
or something like that, the parameters are up to you, we are looking for a behaviour not a syntax.

What we really want is to have a constant total size because when showing the results to the users through a web page it's nicer to have regular sizes for text blocks (to the marketing department's point of view...). So according to this, there are cases when the fragments size should be of variable length. Typically when there are less matches than the number of required fragments. The only case for which we accept to have a smaller size is when the size of the searched field is shorter than the size we specified for the highlight, this is obvious but has to be mentionned to be exhaustive.
This is kind of a corner case between using "no_match_size", and "number_of_fragments" coupled with "fragment_size".

&lt;h2&gt;Second example&lt;/h2&gt;

What we want: we compose resumes from fragments, by catenating them with ellipses(...). We want the resumes to start by the beginning of a phrase.

What we have:
We have to choose between the postings highlighter and the fast vector highlighter. With the first one we have all the fragments beginning by the beginning of a sentence but we loose control over the total size of the resume. With the second one we keep control over the total size but we loose "the sentence effect". We would like something between the two, once more. The first fragment should start by the beginning of a sentence and the others are free.

Once more it's a matter of good user experience (still the marketing department's opinion...). The problem with that is that if the sentence contains a keyword but it is far from the beginning, it could be impossible to have both the beginning of the sentence AND the highlighted keyword in the same fragment because of a too short fragment. Variable fragments length could do the trick in some cases but not all, if the sentence is longer than the total length we loose once more. Maybe the solution would be to have an additional fragment for the sentence purpose.

Once your highlighted fragments have been collected you add an extra fragment which contains the beginning of the sentence in which the first fragment was taken. This extra fragment can be short and is not forced to contain a keyword, it is just here for the glitter effect.

&lt;h2&gt;Third example&lt;/h2&gt;

The total size should never be excedeed. If you look at the previous examples you will see that the total length is sometimes longer than our desired total size. Moreover the individual lengths of the fragments do not always seem optimal, they sometime could be longer without exceeding their specified size. We don't really understand how this works.

&lt;h2&gt;Fourth example, the most important&lt;/h2&gt;

What we want:
When searching for several terms we want to get the most big number of different terms to be highlighted. This point is crucial for the perceived relevance.

What we have:
ES selects the best matches and so often takes the same keyword. In the following example we search for "dog" and "friend" and we only get "dog" highlighted, twice, because it comes first in the text. We would prefer to have "dog" and "friend" highlighted because it seems more relevant. We tried with highlighted query but did not manage to get what we wanted.

``` bash
# Deleting all indexes.
curl -XDELETE 'localhost:9200/_all?pretty=yes'

# Putting some data
curl -XPUT 'localhost:9200/index/doc/1?pretty=yes' -d '
{
    "text": "I have a dog, very cute. Maybe he is the cutest dog in the world. He is my friend, my best friend actually, and will remain it."
}
'

# Refreshing the index
curl -XPOST 'http://localhost:9200/_refresh?pretty=yes'

# Searching
curl -XGET 'localhost:9200/index/doc/_search?pretty=yes' -d '
{
    "query": {
        "match": {
            "text": {
                "query": "friend dog",
                "operator" : "and"
            }
        }
    },
    "highlight": {
        "order": "score",
        "fields": {
            "text": {
                "number_of_fragments": 2,
                "fragment_size": 30
            }
        }
    }
}
```

Resume of our requirements :
- Specify total size.
- Specify fragments number (preferred number to  be precise)
- Specify fragments size (preferrered again)
- First fragment starts as sentence.
- Fragments contain as much different keywords as possible.

Maybe this should have been posted to Lucene ? Sorry if it is the case.
</description><key id="32126745">5928</key><summary>Feature request : improvement of the perceived relevance of the highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdimiceli</reporter><labels><label>:Highlighting</label></labels><created>2014-04-24T07:38:27Z</created><updated>2015-10-14T16:14:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-24T11:49:41Z" id="41270830">I started https://github.com/wikimedia/search-highlighter to handle some of these.  In particular I'm just about done with number 4 now.  I'll write a more complete response when I get a chance.
</comment><comment author="nik9000" created="2014-04-24T14:15:01Z" id="41284804">First example is a lot like issue #3776.  Not exactly the same.

Points 2 and 3 are new to me.  Can you make some examples or have your marketing folks do it?  Examples are yummy because they become unit tests.
</comment><comment author="nik9000" created="2014-04-24T16:06:22Z" id="41298641">I released a new version of the plugin I mentioned above that should help with point 4.  It won't break the fragment in an effort to get both terms, but it'll prefer any fragments that contain more distinct terms.
</comment><comment author="mdimiceli" created="2014-04-28T08:01:07Z" id="41532245">Thank you for your answers Nik. We will test your plugin.

I detail the points 2 and 3 below.

&lt;b&gt;Example 2&lt;/b&gt;

``` bash
# Deleting all indexes.
curl -XDELETE 'localhost:9200/_all?pretty=yes'

# Putting some data
curl -XPUT 'localhost:9200/index/doc/1?pretty=yes' -d '
{
    "text": "I have a dog, very cute, maybe he is the cutest dog in the world. He is my friend, my best friend actually, and will remain it."
}
'
# Refreshing the index
curl -XPOST 'http://localhost:9200/_refresh?pretty=yes'

# Searching
curl -XGET 'localhost:9200/index/doc/_search?pretty=yes' -d '
{
    "query": {
        "match": {
            "text": {
                "query": "cutest friend",
                "operator" : "and"
            }
        }
    },
    "highlight": {
        "order": "score",
        "fields": {
            "text": {
                "highlight_size": 60
            }
        }
    }
}
```

With the previous query, we would get the following result

``` json
"highlight" : {
    "text" : [ "I have...the &lt;em&gt;cutest&lt;/em&gt; dog...is my &lt;em&gt;friend&lt;/em&gt;...best &lt;em&gt;friend&lt;/em&gt; actually" ]
}
```

&lt;b&gt;Example 3&lt;/b&gt;

``` bash
# Deleting all indexes
curl -XDELETE 'localhost:9200/_all?pretty=yes'

# Putting some data, approximately 70 characters long.
curl -XPUT 'localhost:9200/index/doc/1?pretty=yes' -d '
{
    "text": "My parents had the good idea of offering me a dog for my tenth birthday. I like him a lot."
}
'

# Refreshing the index.
curl -XPOST 'http://localhost:9200/_refresh?pretty=yes'

# Searching with two keywords for two fragments.
curl -XGET 'localhost:9200/index/doc/_search?pretty=yes' -d '
{
    "query": {
        "match": {
            "text": {
                "query": "idea birthday",
                "operator" : "and"
            }
        }
    },
    "highlight": {
        "fields": {
            "text": {
                "number_of_fragments": 2,
                "fragment_size": 20
            }
        }
    }
}
'
```

With the previous query, we get the following result

``` json
"highlight" : {
    "text" : [ " good &lt;em&gt;idea&lt;/em&gt; of", " tenth &lt;em&gt;birthday&lt;/em&gt;. I like him a lot." ]
}
```

The second fragment size exceeds the expected size (34 instead of 20). The result we expected is as follow

``` json
"highlight" : {
    "text" : [ "good &lt;em&gt;idea&lt;/em&gt; of", "my tenth &lt;em&gt;birthday&lt;/em&gt;." ]
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Lucene's getFiniteStrings to not consume Java stack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5927</link><project id="" key="" /><description>This pull request just copies in the fix for https://issues.apache.org/jira/browse/LUCENE-5628 to avoid problems like http://build.elasticsearch.com/job/es_jdk8_medium_master/34/jdk=JDK8,label=ubuntu-12-64-0-metal/testReport/junit/org.elasticsearch.search.suggest.completion/CompletionPostingsFormatTest/testDuellCompletions/ where a too-big string passed to the suggester can result in exhausting the Java stack.

Once LUCENE-5628 is fixed (I expect in 4.9), and we upgrade to it, we can remove this XSpecialOperations.
</description><key id="32107289">5927</key><summary>Fix Lucene's getFiniteStrings to not consume Java stack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T23:01:17Z</created><updated>2015-06-08T15:05:07Z</updated><resolved>2014-05-04T19:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-24T07:58:26Z" id="41253212">I agree with @spinscale we should have a static assert to remove this class once we upgrade
</comment><comment author="mikemccand" created="2014-04-24T09:11:58Z" id="41259039">Thanks @spinscale, that makes total sense, I'll do that!
</comment><comment author="rmuir" created="2014-05-04T17:18:59Z" id="42138475">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Lucene's getFiniteStrings to not consume Java stack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5926</link><project id="" key="" /><description>This pull request just copies in the fix for https://issues.apache.org/jira/browse/LUCENE-5628 to avoid problems like http://build.elasticsearch.com/job/es_jdk8_medium_master/34/jdk=JDK8,label=ubuntu-12-64-0-metal/testReport/junit/org.elasticsearch.search.suggest.completion/CompletionPostingsFormatTest/testDuellCompletions/ where a too-big string passed to the suggester can result in exhausting the Java stack.

Once LUCENE-5628 is fixed (I expect in 4.9), and we upgrade to it, we can remove this XSpecialOperations.
</description><key id="32106813">5926</key><summary>Fix Lucene's getFiniteStrings to not consume Java stack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-04-23T22:52:33Z</created><updated>2014-07-16T21:46:03Z</updated><resolved>2014-04-23T22:53:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-04-23T22:53:00Z" id="41224525">Er, crossed the streams again ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow host names (with wild cards) in Search Preference </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5925</link><project id="" key="" /><description>I have a single index (multiple shards) that is pinned to two groups nodes using shard allocation filtering.

```
curl -XPUT localhost:9200/test/_settings -d '{
    "index.routing.allocation.include._host" : "es-large-*"
    "index.routing.allocation.include._host" : "es-small-*"
}'
```

There are 16 large nodes and 4 small nodes. I have allocated 17 replicas (for a total of 18) so at least 2 of the 4 small nodes will have a copy of every shard.

I have two processes querying this same index. One of them is powering a real-time user experience and is low throughput (low query rate) and low latency queries. The other process is powering something near-real-time and executes high latency queries with high throughput (high query rate).

I want to target the small nodes for the low latency, low throughput queries and the large nodes for the high latency, high throughput queries. That way any performance impact from the high latency queries will not affect the real time user experience powered by low low latency queries. 

Is it possible to update Search Preference so that I can target the large and small nodes accordingly?  
</description><key id="32106564">5925</key><summary>Allow host names (with wild cards) in Search Preference </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppat</reporter><labels><label>:Search</label><label>discuss</label><label>enhancement</label></labels><created>2014-04-23T22:47:59Z</created><updated>2015-10-14T16:13:43Z</updated><resolved>2015-10-14T16:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T17:22:15Z" id="91627218">@ppat I think this is the same issue as #10229, which can be solved by using [shard allocation awareness](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#_automatic_preference_when_searching_geting), in that case, ES will automatically try to limit search responses within a particular awareness attribute.

Hopefully this solves this issue, let me know if we correctly understood your issue.
</comment><comment author="ppat" created="2015-04-10T18:12:04Z" id="91641122">It's not the same issue. We're already  using shard allocation awareness to ensure that nodes do not end up on the same VM host machine. We also use shard allocation filtering to pin indices to nodes. 

This request is adding another dimension at search time to target only hosts that match a specific regex. Since I created this issue about 12 months ago, I have found a work around at the cost of latency. I basically run a search shards request to figure which nodes the shards I need to query are on and then determine which one of them matches my regex for host name pattern (different regex pattern for different applications). Then use matching node-id with search preference when sending the actual search query to ES. 

&gt; On Apr 10, 2015, at 1:23 PM, Lee Hinman notifications@github.com wrote:
&gt; 
&gt; @ppat I think this is the same issue as #10229, which can be solved by using shard allocation awareness, in that case, ES will automatically try to limit search responses within a particular awareness attribute.
&gt; 
&gt; Hopefully this solves this issue, let me know if we correctly understood your issue.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2015-04-13T10:31:37Z" id="92302802">I like this idea.  Would be good to support the same node selectors as we have here: http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html
</comment><comment author="ppat" created="2015-04-13T12:52:17Z" id="92341307">Multiple ways of being able to specify node names would indeed be great. In that link, it's not clear whether host names are supported (hopefully they are... Including use of wildcards with host name). 

&gt; On Apr 13, 2015, at 6:32 AM, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; I like this idea. Would be good to support the same node selectors as we have here: http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="ppf2" created="2015-05-29T23:02:05Z" id="106955157">+1 on providing an option like: 

```
_only_nodes:&lt;node_id1&gt;,&lt;node_id2&gt;
_only_nodes:&lt;node_nam1&gt;,&lt;node_name2&gt;
```
</comment><comment author="nirmalc" created="2015-05-29T23:16:15Z" id="106956445">+1 , also on node attribs 

_only_node_attribs:&lt;food&gt; 
</comment><comment author="clintongormley" created="2015-10-14T16:13:43Z" id="148103166">closed by #11464
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene built-in checksumming </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5924</link><project id="" key="" /><description>Lucene 4.8 added checksumming for all files using `CRC32`. We use `Adler` at this point on top of the Direcotory API to check integrity on recovery etc. We should try to cut over to the lucene impl since it comes "for free" now.
</description><key id="32087998">5924</key><summary>Use Lucene built-in checksumming </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>release highlight</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T18:46:46Z</created><updated>2015-06-07T13:39:32Z</updated><resolved>2014-07-10T13:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Scriptable Metric Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5923</link><project id="" key="" /><description>It would be great if we could have a script-based metric aggregation that can 
refer to other metric aggregations from within the script.  For example, given 
the data and query given in this gist: 

https://gist.github.com/mattweber/71033b1bf2ebed1afd8e

It would like to be able to do something like the following for a "profit" calculation:

```
"cost_agg": {
    "filter": {
        "term": {
            "type": "cost"
        }
    },
    "aggs": {
        "cost_sum_agg": {
            "sum": {
                "field": "value"
            }
        }
    }
},
"sale_agg": {
    "filter": {
        "term": {
            "type": "sale"
        }
    },
    "aggs": {
        "sale_sum_agg": {
            "sum": {
                "field": "value"
            }
        }
    }
},
"profit_agg": {
    "metric_script": {
        "script": "sale_agg&gt;sale_sum_agg - cost_agg&gt;cost_sum_agg"
    }
}
```

So using the data from the gist we would get a response such as:

```
{
  "key" : "foobar",
  "doc_count" : 4,
  "sale_agg" : {
    "doc_count" : 2,
    "sale_sum_agg" : {
      "value" : 65.48
    }
  },
  "cost_agg" : {
    "doc_count" : 2,
    "cost_sum_agg" : {
      "value" : 24.68
    }
  },
  "profit_agg": {
    "value": 40.80
  }
}
```

I believe something like this would be possible given the fact that the
the `order` parameter of a Terms Aggregation can refer to metric sub-aggregation
values.  The logic and retrieving the values should be very similar.  Exposing
them to a script might be the hard part.

I will look into this further but I wanted to open the issue in case you are
working on similar functionality or have any suggestions for me.

/cc @uboness @jpountz 
</description><key id="32084726">5923</key><summary>Scriptable Metric Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T18:07:26Z</created><updated>2014-08-20T18:10:16Z</updated><resolved>2014-08-20T18:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-28T16:02:17Z" id="41576864">@mattweber The one thing we have right now in the aggs framework is that each agg is agnostic of other aggs and we'd like to keep it this way. We do have plans to come out with a scriptable metrics agg that will enable you to compute the profit (look at it as a scriptable map/reduce logic)... we'll create a public issue for it soon... I'll keep this one open and once we have the public issue for it will link the two
</comment><comment author="mattweber" created="2014-04-28T16:05:36Z" id="41577308">That sounds great, thanks for the update!
</comment><comment author="mattweber" created="2014-07-23T14:32:46Z" id="49881688">@uboness Just pinging to see if there has been any progress toward functionality that would allow this type of aggregation?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cutover to Lucene's rescorer APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5922</link><project id="" key="" /><description>In Lucene 4.8 we added new rescoring APIs (https://issues.apache.org/jira/browse/LUCENE-5489 and https://issues.apache.org/jira/browse/LUCENE-5545) which were inspired by Elasticsearch's implementations ... we should use Lucene's if we can now.
</description><key id="32081225">5922</key><summary>Cutover to Lucene's rescorer APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-04-23T17:22:54Z</created><updated>2014-07-09T12:07:20Z</updated><resolved>2014-07-09T12:07:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T18:43:48Z" id="41198964">++
</comment><comment author="mikemccand" created="2014-04-25T14:59:20Z" id="41402073">Pushing this out to 1.3 ... it's not urgent.
</comment><comment author="clintongormley" created="2014-07-09T12:07:05Z" id="48461294">Closing in favour of #6232 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow search templates stored in an index to be retrieved and used at search time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5921</link><project id="" key="" /><description>Allow running search templates stored in an index at search time

This change allows search templates stored in an index to be used at search time.
You can run an indexed template by

```
GET /_search/template
{
    'templatename' : '/index/language/id',
    'params' : {
        'param1' : 'foo'
    }
}
```

Running the template will fail if the template cannot be found, if the templatename
starts with a '/' but does not conform to the /index/type/id triple
If the named template cannot be found an error will be raised. Currently the
templates are NOT cached. We also use TransportGetAction directly instead of using
Client to avoid a circular dependency between Client and ScriptService

See #5637, #5484
</description><key id="32065990">5921</key><summary>Allow search templates stored in an index to be retrieved and used at search time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>:Indexed Scripts/Templates</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T14:34:06Z</created><updated>2015-06-06T18:36:15Z</updated><resolved>2014-07-14T13:53:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-24T02:37:55Z" id="41237227">Thinking about it loud: wondering if we should not store templates under a .template type as we do for percolation?

WDYT?
</comment><comment author="GaelTadh" created="2014-04-24T09:53:11Z" id="41262284">Aren't precolates bounded to a single index ? Templates operate across indices. I think perhaps storing templates in a .template index might be better. This also allows us to maintain specifying the language of the template script in the type.
</comment><comment author="s1monw" created="2014-04-24T10:49:08Z" id="41266621">well at this point it doesn't matter which type you store it under. I was thinking about this too to get rid of the `/index/type/id` entirely and just us the `id` but the query parser has no notion of an index at this point. If we store it under `.template` what other than consistency with the percolator are we gaining? I don't think this perfect what we have here but before I start restricting it I want to understand what we gain. I'd love to simplify it though!
</comment><comment author="s1monw" created="2014-04-24T10:55:36Z" id="41267082">&gt; Aren't precolates bounded to a single index ? Templates operate across indices. I think perhaps storing templates in a .template index might be better. This also allows us to maintain specifying the language of the template script in the type.

no percolates are bound to a type since `1.0` you can have it in any index. Yet I think you raised a good point here we need to know which lang the script is for this to be really useful. I think we should ask for a certain format here like: 

```
{
  'script' : {} | ""
  'lang' : "optional"
}
```

that way we get information in the right place?
</comment><comment author="GaelTadh" created="2014-04-24T10:57:19Z" id="41267207">So embed the language in the template itself ? Doesn't this then distance us from on disk templates, whose types are defined by the file extension (analogous to a type) ? 
</comment><comment author="s1monw" created="2014-04-24T11:05:01Z" id="41267837">I don't see any relation to on-disk templates here to be honest. I mean we can go that path but I wonder what happens to scripts that are not `JSON` but just a `String`? You also need to know the lang when you do the request which is odd IMO. 

given that it might be a good thing to use `.script` as the type and let folks only specify the index &amp; the actual ID
</comment><comment author="GaelTadh" created="2014-04-24T11:13:12Z" id="41268387">If we mask the type from the templates does that mean we will need specialized infrastructure to store, update and delete templates ? Or will we just document that they need to be indexed with a .script type ? I can see a use case around versioning templates, is the right place to version in the type or the id ?
</comment><comment author="clintongormley" created="2014-04-24T12:32:43Z" id="41274150">Few points here:
- templates (which are a sub-class of script) span indices, so should be stored in an index, rather than in a type
- we need to indicate what type of script/template a document contains, so the `_type` seems a natural place for that
- users shouldn't be able to insert an arbitrary document into any index and use that as a script - instead, scripts should only be executable if they come from a particular index. that way an admin could prevent unauthorized users from creating scripts

I would suggest having a special index called `.scripts`, and using the `_type` to indicate the type of script that a document contains, eg:

```
PUT /.scripts/mvel/hello_world
{
    "script": "return \"hello world\""
}

PUT /.scripts/template/hello_world
{
    "template": { "query": { "match": { "title": "hello world" }}}
}
```

The `template` field may contain a JSON object (as per the above example) or a string, but the contents of the field does not need to be indexed.  We should have a default mapping for the `.scripts` index like this:

```
PUT /.scripts
{
  "mappings": {
    "_default_": {
      "properties": {
        "script": { "enabled": false },
        "template": { "enabled": false }
      }
    }
  }
}
```

That way, `script` or `template` would accept a string or an object but wouldn't try to parse it any further. The contents of that field wouldn't be searchable, but that's fine.  Users can always add metadata in the same way they do for percolator docs.

Then there is the question of how to refer to these templates or scripts. Currently, we can store scripts or templates on disk and just pass the name of the file in the `template`/`script` parameter, eg:

```
GET /_search/template
{
    "template": "storedTemplate" ,
    "params": {
        "query_string": "search for these words"
    }
}
```

While this can work with a small set of files stored on disk, it won't work when we can have potentially unlimited scripts/templates in an index.  Especially with scripts, it could be tricky to distinguish between a filename, a type/id and a dynamic script.

We have some precedents for this, but the precedents are not consistent, eg we have the `terms` lookup mechanism:

```
"terms": {
    "some_field": {
        "index": "some_index",
        "type": "some_type",
        "id": "some_id",
        "path": "some.path"
    }
}
```

Then with geoshapes, we have:

```
"geo_shape": {
    "some_field": {
        "indexed_shape": {
            "index": "some_index",
            "type": "some_type",
            "id": "some_id",
            "path": "some.path"
        }
    }
}
```

(I think the `geo_shape` syntax could be changed to conform with the `terms` syntax, to make it consistent).

The equivalent for scripts could be:

```
GET /_search
{
  "script_fields": {
    "inline_script": {
      "script": "return 'foo'"
    },
    "indexed_script": {
      "lang": "mvel",     # uses "mvel" as the _type
      "id": "hello_world"
    },
    "script_in_file": {
      "lang": "mvel",
      "file": "hello_world"  # or "hello_world.mvel"
    }
  }
}
```

And for templates, we have inline templates:

```
# as string
GET /_search/template
{
  "template": "{\"query\":{\"match_all\":{}}}"
}

# as JSON object
GET /_search/template
{
  "template": {
    "query": {
      "match_all": {}
    }
  }
}
```

Templates stored in a file:

```
GET /_search/template
{
  "file": "some_template"
}
```

Or templates stored in the `.scripts` index:

```
GET /_search/template
{
  "id": "some_template"
}

# or just:
GET /_search/template/some_template
```
</comment><comment author="GaelTadh" created="2014-04-24T13:11:11Z" id="41277658">++ to storing into a .scripts index and using the types to define the type of script/template. Do we have a list of allowed types that we can validate against ?
</comment><comment author="s1monw" created="2014-04-24T14:37:18Z" id="41287509">@GaelTadh what do you mean by allowed types?
</comment><comment author="GaelTadh" created="2014-04-24T16:33:09Z" id="41301853">@s1monw nm I found it. I meant supported script engines.
</comment><comment author="GaelTadh" created="2014-05-06T13:55:13Z" id="42304544">@clintongormley do you think we should refuse to delete the .scripts index ? Always create it ? Or just create it on demand when a user creates an indexed template for the first time ?
</comment><comment author="clintongormley" created="2014-05-06T15:43:18Z" id="42319496">@GaelTadh I'd say we treat it like a normal index that we create on demand and can be deleted.

Possibly we should have a built-in index template with defaults of (eg) 1 primary shard and `0-all` replicas?
</comment><comment author="s1monw" created="2014-05-13T11:02:47Z" id="42941875">++ for the index template 
</comment><comment author="s1monw" created="2014-05-16T20:46:00Z" id="43377493">I think there is a bunch of REST API work missing here = we should talk about it next week. it's a good step! I left a bunch of comments
</comment><comment author="s1monw" created="2014-05-16T20:46:34Z" id="43377543">@GaelTadh I pushed this out to 1.3 lets take it easy here!
</comment><comment author="s1monw" created="2014-06-12T09:04:55Z" id="45845669">@GaelTadh can you quickly rebase against master, I'd love to review it soon
</comment><comment author="s1monw" created="2014-06-12T09:59:08Z" id="45850228">I left a bunch of comments. IMO we are really close, this is a pretty cool change! I think in addition to the comments we need to fix the documentation. @clintongormley do we have a central place where we can put the format for scripts just like what we did for fuzzyness?
</comment><comment author="clintongormley" created="2014-06-12T10:38:15Z" id="45875605">We have this page about scripting: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#modules-scripting

It's starting to get quite long though, probably worth breaking down into a section with several pages.
</comment><comment author="s1monw" created="2014-06-18T19:22:35Z" id="46481419">@GaelTadh I added one comment but other than the documentation I think it's good to go. @clintongormley should we make the breakout a separate issue and mark it as a blocker for the release? this one has been sitting here for a while I want to get it in!
</comment><comment author="clintongormley" created="2014-06-19T14:01:16Z" id="46563589">@s1monw do you mean make breaking the scripting docs up a separate issue?  sure, happy with that.  @GaelTadh ping me when this branch is merged and i'll reorganise the docs
</comment><comment author="GaelTadh" created="2014-07-08T09:22:05Z" id="48289734">Just pushed the client side code. Please review.
</comment><comment author="s1monw" created="2014-07-10T10:30:42Z" id="48588639">@GaelTadh I left some minor comments. I think this looks just awesome. Lets iterate quickly here and get that in!
</comment><comment author="s1monw" created="2014-07-14T09:49:18Z" id="48882076">@GaelTadh I left like 3 or 4 comment other than that LGTM
</comment><comment author="clintongormley" created="2014-07-14T09:55:07Z" id="48882540">Looking at the YAML tests, it looks good.  Only thing is that this PR doesn't include any documentation.
</comment><comment author="s1monw" created="2014-07-14T13:03:10Z" id="48896728">LGTM - Clinton and I spoke about docs and I want this to run on CI asap so please make a new issue for the docs and push this in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Start nodes using node.bench true to enable benchmark api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5920</link><project id="" key="" /><description>Enabling benchmark apis is required to run our REST tests for it. Added the parameter to node creation in `TestCluster` as well in the release script, when starting the node to run the smoke tests against.

Closes #5910
</description><key id="32063452">5920</key><summary>Test: Start nodes using node.bench true to enable benchmark api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T14:06:14Z</created><updated>2014-09-27T08:38:05Z</updated><resolved>2014-05-09T21:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:15:02Z" id="41173352">I jsut checked and saw there are no tests for this API at all in out code base. I don't like that we set this by default, I think the REST tests are an afterthought and this shoudl be tested internally first if it's such a special case.
</comment><comment author="javanna" created="2014-04-26T20:20:10Z" id="41479685">I agree making every node a bench node is not a good idea, the idea is then to have a single client node that can be used for benchmarks as well. In order for this to be possible though we need to be able to have client nodes within our test cluster, which is why I created #5949. Once we get that in we can make the client node a bench node too.
</comment><comment author="javanna" created="2014-05-09T21:04:17Z" id="42713778">I just updated this PR that randomly adds the node.bench setting to the eventually present client node. The execution of the benchmark tests now depends on the feature `benchmark` which requires at least a benchmark node to be present in the cluster.
</comment><comment author="jpountz" created="2014-05-09T21:24:30Z" id="42715763">I don't have enough experience with the rest tests to comment on whether this is the best way to do it but it looks good and I think it's important to enable tests on the benchmark API ASAP so I'd vote to get this in. We can iterate on this change later on if necessary.
</comment><comment author="aleph-zero" created="2014-05-09T21:33:05Z" id="42716411">Looks good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: NullPointerException thrown by benchmark run against multiple nodes through REST layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5919</link><project id="" key="" /><description>When using the benchmark api against multiple nodes through the REST layer, depending on the node hit by the request, a `NullPointerException` might be thrown and returned:

```
{
  "error" : "NullPointerException[null]",
  "status" : 500
}
```

The reason for this is that the [`CompetitionResult#toXContent`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bench/CompetitionResult.java#L211) method reads from the `competitionSummary` variable which is `null` when the object gets created through the empty constructor as the `readFrom` method doesn't initialize it based on the received information.

Running the REST tests with our java runner reproduces this, once nodes are started with `node.bench=true` (relates to #5910).
</description><key id="32062570">5919</key><summary>Benchmark: NullPointerException thrown by benchmark run against multiple nodes through REST layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Benchmark</label><label>bug</label></labels><created>2014-04-23T13:56:02Z</created><updated>2015-03-19T15:44:12Z</updated><resolved>2014-05-07T21:29:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-05-07T21:29:37Z" id="42485595">Recently close issue #6023 fixes this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Raise node disconnected even if the transport is stopped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5918</link><project id="" key="" /><description>during the stop process, we raise network disconnect, so it is valid to raise then while we are in stop mode, and actually, we should not miss any events in such a case.
Typically, this is not a problem, since its during the normal shutdown process on the JVM, but when running a reused cluster within the JVM (like in our test infra with the shared cluster), we should properly raise those node disconnects
</description><key id="32054710">5918</key><summary>Raise node disconnected even if the transport is stopped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T12:07:48Z</created><updated>2015-06-07T13:39:42Z</updated><resolved>2014-04-28T08:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T16:04:15Z" id="41179991">so I worked on this a bit and I had a test as well as a fix for the test here https://github.com/s1monw/elasticsearch/commit/19af66b0dbcce8239569b276e0fc0ec0432d532e I didn't work further on it since we spoke but the fix you have didn't fix my test maybe you can investigate based on that? feel free to take my test as well
</comment><comment author="s1monw" created="2014-04-26T14:08:19Z" id="41469626">one symptom of this is a test failure like this:

```
EPRODUCE WITH  : mvn test -Dtests.seed=5F9B9E72D2C3CE1F -Dtests.class=org.elasticsearch.cluster.MinimumMasterNodesTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Etc/UTC -Dtests.method="multipleNodesShutdownNonMasterNodes" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=true -Dtests.nightly=true -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:-UseCompressedOops"
  1&gt; Throwable:
  1&gt; java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_0.cfe=1, _0.si=1}
  1&gt;     __randomizedtesting.SeedInfo.seed([5F9B9E72D2C3CE1F:D9159CAB6B73CDBB]:0)
  1&gt;     org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:646)
  1&gt;     org.elasticsearch.test.store.MockDirectoryHelper$ElasticsearchMockDirectoryWrapper.close(MockDirectoryHelper.java:140)
  1&gt;     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:506)
  1&gt;     org.elasticsearch.test.ImmutableTestCluster.assertAfterTest(ImmutableTestCluster.java:79)
  1&gt;     org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:443)
  1&gt;     org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1257)
  1&gt;     [...sun.*, com.carrotsearch.randomizedtesting.*, java.lang.reflect.*]
  1&gt;     org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  1&gt;     org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     java.lang.Thread.run(Thread.java:745)
  1&gt; Caused by: java.lang.RuntimeException: unclosed IndexInput: _0.cfe
  1&gt;     org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:534)
  1&gt;     org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:578)
  1&gt;     org.elasticsearch.index.store.Store.openInputRaw(Store.java:318)
  1&gt;     org.elasticsearch.indices.recovery.RecoverySource$1$1.run(RecoverySource.java:185)
  1&gt;     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;     java.lang.Thread.run(Thread.java:745)
```

just for the record...
</comment><comment author="kimchy" created="2014-04-27T15:03:30Z" id="41498953">I was testing on the `TestCluster` class, and a cluster creates connection to itself, so my fix was working, but, not fully as you showed. We need to make sure we also clear it on stop even if for some reason node disconnects were not raised. I pushed your test and fix as well to the branch.

One additional thing, I am wondering if we should "wait for an acceptable" time to have all the callbacks raised before we exit the stop, or node disconnect callback (when we are in stop mode)? This will ensure the service has hopefully properly stopped, and log if it didn't stop within a timeout.

Btw, we do that anyhow in our ThreadPool class, so it might be enough there
</comment><comment author="s1monw" created="2014-04-27T15:07:02Z" id="41499046">yeah I think it will be ok in the threadpool cases IMO. But I wonder if we should have more tests trigger this stuff. I need to think about it but I'd like to assert that the grace period is enough in the thread pools?!
</comment><comment author="kimchy" created="2014-04-27T15:10:50Z" id="41499156">@s1monw aye!, that would be great to assert on in our test infra, that the ThreadPool always exits within the timeout, and if not, its something need fixing...

So are we good with this change then and letting thread pool to have the graceful shutdown period? 
</comment><comment author="kimchy" created="2014-04-28T08:55:40Z" id="41536174">I will pull this in, and we can open a separate issue for thread pool assertion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't lookup version for auto generated id and create</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5917</link><project id="" key="" /><description>When a create document is executed, and its an auto generated id (based on UUID), we know that the document will not exists in the index, so there is no need to try and lookup the version from the index.
For many cases, like logging, where ids are auto generated, this can improve the indexing performance, specifically for lightweight documents where analysis is not a big part of the execution.
</description><key id="32052189">5917</key><summary>Don't lookup version for auto generated id and create</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T11:22:16Z</created><updated>2015-06-07T13:39:47Z</updated><resolved>2014-04-25T12:31:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-23T12:38:40Z" id="41155443">It's extremely unlikely that the ID doesn't exist, but not completely impossible (eg if somebody previously specified the same ID manually).  What would be the consequences of a clash? Just that the version number would be incorrect? Or would you end up with duplicate docs?
</comment><comment author="kimchy" created="2014-04-23T12:55:08Z" id="41156894">@clintongormley in the extreme event that there is a clash, then we will end with duplicate docs. I don't think this will happen though. Clashes in terms of UUID are close to impossible, and generated the same id on the client side that clashes with one in ES _and_ ending up doing a create operation... (this optimization is only for create use case)
</comment><comment author="s1monw" created="2014-04-23T15:35:17Z" id="41176077">I added some comments regarding naming. Yet what I miss here is a really good test that adds a lot of stuff with auto IDs while starting up replicas etc. we might also randomize some existing tests but I think they should at least use bulk or concurrent indexing.
</comment><comment author="kimchy" created="2014-04-23T19:31:39Z" id="41204297">I added a test to master/1.x, and rebased this branch against it.
</comment><comment author="s1monw" created="2014-04-24T14:51:04Z" id="41289219">I really want to have this optimization but I think we need to fix the API for `Engine#Create` and friends to be immutable. The builder like patter is so error prone I don't think we should carry that on over this change. Other than that I left some comments &amp; it looks good
</comment><comment author="kimchy" created="2014-04-25T10:47:34Z" id="41380112">I am good with getting this in, @s1monw thanks for the improvement on making things more immutable, should we get it in?
</comment><comment author="s1monw" created="2014-04-25T10:48:11Z" id="41380144">@kimchy I left on TODO in the code can you take a look at it?
</comment><comment author="kimchy" created="2014-04-25T10:53:24Z" id="41380487">ahh, yea, I see, I think its safe to remove it, I will remove it and squash
</comment><comment author="s1monw" created="2014-04-25T10:58:34Z" id="41380851">@kimchy can you add an assertion at that place? and make the member final :)
</comment><comment author="kimchy" created="2014-04-25T10:59:46Z" id="41380922">@s1monw aye, already done, running tests
</comment><comment author="s1monw" created="2014-04-25T11:30:39Z" id="41382960">LGTM
</comment><comment author="avleen" created="2014-04-25T19:31:42Z" id="41430884">There's an interesting use case here, for which I hope someone can clarify the impact of this change:

In the logging case, it can be desirable to have the same log line generate the same ID (maybe based on the hash of the log line or something).
Eg, if I lose a shard which has no replica and I need to replay my logs to fill in the missing data, two facts are probably true:
1. I can't pick out exactly which lines are missing from a shard, out of millions in a log file.
2. I probably want to just replay the entire log and let the system handle indexing, deduplication, etc.

In this case, what would happen? From the comments, I think I'd get a lot of duplicated entries.
That may be acceptable if there's a cheap way to clean them up?
</comment><comment author="s1monw" created="2014-04-25T19:40:12Z" id="41431687">@avleen in your case you provide the id as you said `(maybe based on the hash of the log line or something)` in that case this change doesn't apply since you are not using an ES autogenerated id. This only applied to documents where the internal ID generator is used and this means the docs are by definition unique.
</comment><comment author="kimchy" created="2014-04-25T19:44:50Z" id="41432083">@avleen also, (depending on your system), if you end up replying the entire log, you are basically reindexing the data, in which case, assuming rolling indices, you might want to reindex today. Pretty much use case dependent, but thats another option. (obviously thats assuming you don't want to run with replicas, even with bulk async replication)
</comment><comment author="avleen" created="2014-04-25T22:54:07Z" id="41447096">Thanks @s1monw, @kimchy! Makes perfect sense :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disabled parent/child queries in the delete by query api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5916</link><project id="" key="" /><description>PR for #5828
</description><key id="32032796">5916</key><summary>Disabled parent/child queries in the delete by query api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>blocker</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-23T04:55:29Z</created><updated>2015-06-07T20:45:02Z</updated><resolved>2014-04-28T13:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:47:12Z" id="41177713">left one comment - look close :) thanks!
</comment><comment author="martijnvg" created="2014-04-24T07:58:05Z" id="41253189">Updated the PR to move the delete by query api usage checking into one place.
</comment><comment author="s1monw" created="2014-04-24T08:00:46Z" id="41253381">It's not super urgent but maybe we should move this method into a `QueryParserUtils` class?
</comment><comment author="martijnvg" created="2014-04-28T02:21:37Z" id="41518197">Updated the PR and moved the method to `QueryParserUtils` util class.
</comment><comment author="kimchy" created="2014-04-28T09:43:38Z" id="41539954">lets a small comment, LGTM otherwise.
</comment><comment author="martijnvg" created="2014-04-28T13:14:30Z" id="41556162">Pushed via:  https://github.com/elasticsearch/elasticsearch/commit/17a5575757317962dab4c295bbfacbdb136cc61e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>test failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5915</link><project id="" key="" /><description>Lately I have been unable to run the tests without any errors.  With java 1.7.0_55 and 1.8.0_05.  I have put the exceptions and commands to reproduce below.  Let me know if this is expected of if I should open a ticket for each failure.

Suite: org.elasticsearch.index.query.TemplateQueryTest

```
mvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.index.query.TemplateQueryTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Des.logger.level=INFO -Des.node.local=true -Dtests.heap.size=512m
```

```
ERROR   0.53s | TemplateQueryTest.testTemplateInFile &lt;&lt;&lt;
   &gt; Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[L3om1MHxTAKSpWUQy21U5A][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][5]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][2]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][8]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][8]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][9]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][9]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][7]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][0]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }
   &gt;    at __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:C7881C13C67531D2]:0)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)
   &gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
   &gt;    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   &gt;    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

```
ERROR   0.63s | TemplateQueryTest.testRawFSTemplate &lt;&lt;&lt;
   &gt; Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[L3om1MHxTAKSpWUQy21U5A][test][4]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][5]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][8]: SearchParseException[[test][8]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][9]: SearchParseException[[test][9]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][7]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][0]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }
   &gt;    at __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:43A1FBBC1F4C432F]:0)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)
   &gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
   &gt;    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   &gt;    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

```
ERROR   0.81s | TemplateQueryTest.testThatParametersCanBeSet &lt;&lt;&lt;
   &gt; Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[fldhqviNRyK_6JLp8pH8tQ][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][5]: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[L3om1MHxTAKSpWUQy21U5A][test][2]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][7]: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[L3om1MHxTAKSpWUQy21U5A][test][0]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }
   &gt;    at __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:8330A16E27FBB878]:0)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)
   &gt;    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)
   &gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
   &gt;    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
   &gt;    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
   &gt;    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
   &gt;    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
   &gt;    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
   &gt;    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
   &gt;    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
   &gt;    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
   &gt;    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Suite: org.elasticsearch.test.rest.test.FileUtilsTests

```
mvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.test.rest.test.FileUtilsTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.method="testLoadMultipleYamlSuites" -Des.logger.level=INFO -Des.node.local=true -Dtests.heap.size=512m
```

```
FAILURE 0.10s | FileUtilsTests.testLoadMultipleYamlSuites &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;1&gt;
   &gt;      got: &lt;0&gt;
   &gt; 
   &gt;    at __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:5726FA52A0F1DF9F]:0)
   &gt;    at org.junit.Assert.assertThat(Assert.java:780)
   &gt;    at org.junit.Assert.assertThat(Assert.java:738)
   &gt;    at org.elasticsearch.test.rest.test.FileUtilsTests.testLoadMultipleYamlSuites(FileUtilsTests.java:54)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
   &gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   &gt;    at java.lang.reflect.Method.invoke(Method.java:606)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)
   &gt;    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
   &gt;    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Suite: org.elasticsearch.index.query.TemplateQueryParserTest

```
mvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.index.query.TemplateQueryParserTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.method="testParserCanExtractTemplateNames" -Des.logger.level=INFO -Dtests.heap.size=512m
```

```
ERROR   1.30s | TemplateQueryParserTest.testParserCanExtractTemplateNames &lt;&lt;&lt;
   &gt; Throwable #1: org.elasticsearch.ElasticsearchParseException: Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e
   &gt;    at __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:B1A8F5E0111DEFFC]:0)
   &gt;    at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:259)
   &gt;    at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:67)
   &gt;    at org.elasticsearch.index.query.TemplateQueryParserTest.testParserCanExtractTemplateNames(TemplateQueryParserTest.java:121)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
   &gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   &gt;    at java.lang.reflect.Method.invoke(Method.java:606)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)
   &gt;    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
   &gt;    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="32029194">5915</key><summary>test failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2014-04-23T02:55:00Z</created><updated>2014-04-28T16:51:06Z</updated><resolved>2014-04-28T16:51:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-23T03:09:15Z" id="41120251">I have my elasticsearch checkout in my "Google Drive" folder and I think the space in the directory path is the issue.  If I run out of a folder with no space in the path no errors, run with one with a space I immediately get the errors above.

I can close this or maybe this is valid (think Windows that almost always has spaces in directory paths).
</comment><comment author="mattweber" created="2014-04-26T22:13:49Z" id="41482566">Found and fixed the issue, putting a PR together now.
</comment><comment author="mattweber" created="2014-04-26T22:23:41Z" id="41482775">See PR #5950.  @s1monw maybe adding URL#getPath and URL#getFile to the forbidden api's will help prevent this in the future?
</comment><comment author="s1monw" created="2014-04-27T06:58:30Z" id="41489845">@mattweber yeah seems to make sense do you wanna open a PR?
</comment><comment author="mattweber" created="2014-04-27T20:55:44Z" id="41508769">Opened PR #5954 for the forbidden api's.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Randomization script refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5914</link><project id="" key="" /><description>1) Refactored randomization logic to the top
2) Add logics to support window platforms
</description><key id="32011658">5914</key><summary>Test: Randomization script refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels><label>build</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T21:16:39Z</created><updated>2015-06-07T11:45:36Z</updated><resolved>2014-07-16T21:13:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:15:36Z" id="41173412">I am not an ruby expert maybe @karmi can take a look here?
</comment><comment author="karmi" created="2014-04-24T17:15:39Z" id="41306987">I think the patch improves on the current Ruby script a lot, I like the extraction of logic into classes and particularly the bundled unit tests, because any bugs can be covered by them once discovered in production. I think it's safe to merge.

I did mostly style-related comments, I think in the future the code can be abstracted even a bit more, so it has a single entry point for executable code (`RandomizedRunner.new(some, args, ...).run!`).
</comment><comment author="s1monw" created="2014-06-12T12:40:13Z" id="45885926">@mrsolo can we bring this forward?
</comment><comment author="mrsolo" created="2014-06-12T23:11:27Z" id="45959403">File update incorporating some of the comments.  Unfortunately, this script has to be 1.8 compatible.  Adding some additional logic to take account of window platform variations (future refactoring target)
</comment><comment author="mrsolo" created="2014-07-10T20:16:51Z" id="48658194">File updated, incorporate comments
</comment><comment author="clintongormley" created="2014-07-11T08:49:14Z" id="48707910">@karmi please could you review this again and give a LGTM if it's ok to get in for 1.3?
</comment><comment author="karmi" created="2014-07-11T09:44:20Z" id="48712307">@mrsolo @clintongormley I've left couple of notes in the diff, but those are again mostly cosmetic, it's definitely OK to include it in 1.3 -- it's used on Jenkins anyway, as it is.
</comment><comment author="mrsolo" created="2014-07-11T18:42:54Z" id="48767614">Updated with latest comments; will proceed with merge.
</comment><comment author="mrsolo" created="2014-07-16T21:13:10Z" id="49227939">Merged 

Master
https://github.com/elasticsearch/elasticsearch/commit/bdf77fec6552604478f62edc28ba241184aa2921 

1.x
https://github.com/elasticsearch/elasticsearch/commit/c16bc36c81999de78efa34c543a34ee102144645
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default recovery throttling to 50MB / sec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5913</link><project id="" key="" /><description>The current setting of 20MB/sec seems to be too conservative given
the capabilities of modern hardware / network throughput.
A 50MB default should provide better out of the box performance.
</description><key id="32003790">5913</key><summary>Change default recovery throttling to 50MB / sec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T19:42:36Z</created><updated>2015-06-07T13:40:05Z</updated><resolved>2014-04-23T13:47:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-23T09:46:13Z" id="41142891">This one got pushed, closing.
</comment><comment author="s1monw" created="2014-04-23T09:47:49Z" id="41143018">DTT!
</comment><comment author="kimchy" created="2014-04-23T11:14:47Z" id="41149316">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene's defaults for CMS settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5912</link><project id="" key="" /><description>#5882

Lucene changed the settings a while back to be more conservative but ES has the old hardwired defaults.
</description><key id="32003706">5912</key><summary>Use Lucene's defaults for CMS settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T19:41:30Z</created><updated>2015-06-08T15:05:39Z</updated><resolved>2014-04-23T15:43:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-22T19:46:14Z" id="41086618">LGTM, I suggest pushing to master and 1.x branches
</comment><comment author="mikemccand" created="2014-04-22T19:47:21Z" id="41086732">Thanks, Shay, will do...

Mike

http://blog.mikemccandless.com

On Tue, Apr 22, 2014 at 3:46 PM, Shay Banon notifications@github.comwrote:

&gt; LGTM, I suggest pushing to master and 1.x branches
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/5912#issuecomment-41086618
&gt; .
</comment><comment author="kimchy" created="2014-04-22T20:03:54Z" id="41088607">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene's ConcurrentMergeScheduler default settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5911</link><project id="" key="" /><description>#5882

Lucene changed the settings a while back to be more conservative but ES has the old hardwired defaults.
</description><key id="32003152">5911</key><summary>Use Lucene's ConcurrentMergeScheduler default settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-04-22T19:33:53Z</created><updated>2014-06-25T02:37:16Z</updated><resolved>2014-04-22T23:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Run tests with node.bench=true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5910</link><project id="" key="" /><description>The benchmark tests are failing because the nodes are not set to run with

```
-Des.node.bench=true
```

Disabled tests for now with https://github.com/elasticsearch/elasticsearch/commit/640085c45d112884cd8eedd7c1cd214bb874aafc which should be reverted once tests fixed
</description><key id="32001945">5910</key><summary>Run tests with node.bench=true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-04-22T19:18:33Z</created><updated>2014-07-16T11:58:51Z</updated><resolved>2014-05-09T21:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Another iteration on #5891 to tie in logger level=TRACE to Lucene's IndexWriter.infoStream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5909</link><project id="" key="" /><description /><key id="32001107">5909</key><summary>Another iteration on #5891 to tie in logger level=TRACE to Lucene's IndexWriter.infoStream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-04-22T19:08:02Z</created><updated>2014-07-16T21:46:06Z</updated><resolved>2014-04-24T18:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update default precision step, modulo tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5908</link><project id="" key="" /><description>See #5905
</description><key id="31998781">5908</key><summary>Update default precision step, modulo tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T18:39:56Z</created><updated>2015-06-07T14:23:22Z</updated><resolved>2014-04-23T13:20:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-22T18:58:22Z" id="41079849">I left some comments - looks good to me so far....
</comment><comment author="rmuir" created="2014-04-23T00:33:29Z" id="41112658">OK, I think this is ready. 

One change i made: was to actually set the precisionStep in the fieldType passed to Lucene. This was just for consistency, so if something like https://issues.apache.org/jira/browse/LUCENE-5605 happens, it wont break.
</comment><comment author="s1monw" created="2014-04-23T08:10:22Z" id="41135064">LGTM please squash and push to `1.x` &amp; `master`. Can you make sure you have a nice commit message that explains the content with a headline and a footer that says `Closes #5905`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add REST API spec for /_search_shards endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5907</link><project id="" key="" /><description /><key id="31996054">5907</key><summary>Add REST API spec for /_search_shards endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T18:07:07Z</created><updated>2015-06-08T15:05:52Z</updated><resolved>2014-04-23T19:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-23T10:09:47Z" id="41144710">I left some comments, it would be nice also to add some REST tests for this.
</comment><comment author="dakrone" created="2014-04-23T18:37:31Z" id="41198207">@javanna thanks for the comments! I have addressed them and added a REST test for this.
</comment><comment author="javanna" created="2014-04-23T18:55:32Z" id="41200362">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Missing benchmark should return 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5906</link><project id="" key="" /><description>Currently, aborting a non-existent benchmark returns 200 OK, eg:

```
curl -XPOST 'http://localhost:9200/_bench/abort/my_benchmark?pretty=1'

Response: 200, Took: 1 ms
{   "error" : "Benchmark with id [my_benchmark] not found" }
```

it should return a 404 not found
</description><key id="31991344">5906</key><summary>Benchmark: Missing benchmark should return 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Benchmark</label><label>bug</label></labels><created>2014-04-22T17:04:43Z</created><updated>2015-03-19T15:44:23Z</updated><resolved>2014-05-07T21:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-22T17:15:49Z" id="41067496">Also please remember to change `request` to `missing` in https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/abort_benchmark/10_basic.yaml#L7 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>change default numeric precision_step</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5905</link><project id="" key="" /><description>The current precision_step value for all numeric fields is 4. This equates to 8 terms-per-value for 32-bit types, and 16-terms-per-value for 64-bit types. This means these fields are 8x or 16x more costly than a "regular" field (in terms of space/indexing), to accelerate range queries.

The benefit of the lower precision step is to visit less terms during range queries, and these terms will be denser (so smaller integers in the postings lists, but large ones).

Several things in Lucene have changed in the past few years: visiting more terms at query-time is less costly because this is executed per-segment, postings list compression has improved, the term dictionary is faster in general, and terms with only one posting are inlined into the term dictionary.

Furthermore, the speedup has much less benefit when using filters, because in that case it only impacts "uncached" filters. Once cached, its the same either way since it is just a bitset.

I think we should change the default: its just a default. The current default IMO is too aggressive.

There are some benchmarks posted here: https://issues.apache.org/jira/browse/LUCENE-5609
</description><key id="31985775">5905</key><summary>change default numeric precision_step</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T15:57:07Z</created><updated>2014-04-23T13:05:40Z</updated><resolved>2014-04-23T13:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T08:10:55Z" id="41135107">this issue needs labels on which branches it goes and what kind of issue it is.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Remove "slowest" if only one request in benchmark API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5904</link><project id="" key="" /><description>While each competitor in the benchmark API can accept multiple requests, two competitors can be used to compare two variations of the same query, ie there is only one request per competitor.

In this case it makes sense to not output the `slowest` request as there is only one.  
</description><key id="31985120">5904</key><summary>Benchmark: Remove "slowest" if only one request in benchmark API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Benchmark</label><label>enhancement</label></labels><created>2014-04-22T15:49:57Z</created><updated>2015-03-19T15:24:52Z</updated><resolved>2014-05-07T21:26:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Use index/type in benchmark API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5903</link><project id="" key="" /><description>Currently the benchmark API supports the `indices` parameter which actually takes a path-like `index/type` setting.

For consistency, we should instead accept `index` and `type` parameters (which can be single or multiple, and handle the usual wildcards) and whose values default to the values provided in the URL.
</description><key id="31984903">5903</key><summary>Benchmark: Use index/type in benchmark API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Benchmark</label><label>enhancement</label></labels><created>2014-04-22T15:47:38Z</created><updated>2015-03-19T15:25:06Z</updated><resolved>2014-05-07T21:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Change default merge throttling to 50MB / sec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5902</link><project id="" key="" /><description>The current setting of 20MB/sec seems to be too conservative given
the capabilities of modern hardware. Even on cloud infrastructure this
seems to be too lowish. A 50MB default should provide better out of the box
performance
</description><key id="31983047">5902</key><summary>Change default merge throttling to 50MB / sec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T15:28:29Z</created><updated>2015-06-07T14:23:51Z</updated><resolved>2014-04-22T19:13:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-22T15:30:24Z" id="41054236">++, can we change the title to reflect the change more, that we also increase recovery throttling?
</comment><comment author="mikemccand" created="2014-04-22T15:36:05Z" id="41055144">+1

Once we release this we need to also update the public docs too: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html
</comment><comment author="kimchy" created="2014-04-22T15:48:18Z" id="41056829">@s1monw sorry, for some reason I saw a change on the pull request that included recovery throttling, ignore my comment regarding the title.
</comment><comment author="s1monw" created="2014-04-22T16:02:10Z" id="41058722">@kimchy yeah I will bring this in as a second PR
@mikemccand the PR contains doc changes as well so they will go live as we release
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved bloom filter hashing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5901</link><project id="" key="" /><description>Make improvements to how bloom filter hashing works based on guava 17 upcoming changes, see more here (https://code.google.com/p/guava-libraries/issues/detail?id=1119)
In order to do it, introduce a hashing enum, and use the (unused until now) hash type serialization to choose the correct hashing used based on serialized version.
Also, move to use our own optimized murmur hash for the new hashing logic.
</description><key id="31975749">5901</key><summary>Improved bloom filter hashing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T14:10:18Z</created><updated>2015-06-07T14:23:59Z</updated><resolved>2014-04-22T15:18:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-22T14:44:01Z" id="41048191">Can we keep `writeTo` and `readFrom`?  Useful for when a `BloomFilter` needs to be serialized over the transport like in #3278.
</comment><comment author="kimchy" created="2014-04-22T14:46:25Z" id="41048518">sure, added it back
</comment><comment author="mattweber" created="2014-04-22T14:56:20Z" id="41049816">Great, thanks!
</comment><comment author="imotov" created="2014-04-22T15:10:59Z" id="41051697">A possible problem with keeping `writeTo` and `readFrom` is that it might cause failures in the cluster during rolling version upgrades. Users of these methods will need to make sure that we don't ship a serialized bloom filters to the nodes with older version. Other than that - LGTM.
</comment><comment author="kimchy" created="2014-04-22T15:15:20Z" id="41052238">regarding writeTo/readFrom, aye, we don't use it for now in our codebase, so the futures that will need it will be post this hashing addition. So I think we are good for now.
</comment><comment author="mattweber" created="2014-04-22T15:15:46Z" id="41052279">I believe these methods are only used in my PR as I added them a while back.  I don't think we will have a version issue unless we change formats again.
</comment><comment author="kimchy" created="2014-04-22T15:18:32Z" id="41052623">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use unlimited `flush_threshold_ops` for translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5900</link><project id="" key="" /><description>Currently we use 5k operations as a flush threshold. Indexing 5k documents
per second is rather common which would cause the index to be committed on
the lucene level each time the flush logic runs which is 5 seconds by default.
We should rather use a size based threshold similar to the lucene index writer
that doesn't cause such agressive commits which can slow down indexing significantly
especially since they cause the underlying devices to fsync their data.
</description><key id="31968458">5900</key><summary>Use unlimited `flush_threshold_ops` for translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T12:33:44Z</created><updated>2015-06-07T14:24:26Z</updated><resolved>2014-04-22T14:48:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-22T12:41:13Z" id="41034491">LGTM, ++
</comment><comment author="javanna" created="2014-04-22T13:08:08Z" id="41036866">LGTM
</comment><comment author="s1monw" created="2014-04-22T14:34:30Z" id="41046880">@javanna I added using constants! I think it's ready
</comment><comment author="javanna" created="2014-04-22T14:38:33Z" id="41047442">Yep, looks good!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide defaults for number_of_replicas and number_of_shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5899</link><project id="" key="" /><description /><key id="31961296">5899</key><summary>Provide defaults for number_of_replicas and number_of_shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djui</reporter><labels /><created>2014-04-22T10:29:27Z</created><updated>2014-06-13T09:27:57Z</updated><resolved>2014-05-06T14:10:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-25T21:00:46Z" id="41438914">This is not a dynamic setting, so adding it on the update-settings documentation page doesn't make sense (it might confuse people into thinking the number of shards is configurable after an index has been created)
</comment><comment author="djui" created="2014-04-26T04:56:49Z" id="41460073">Ok, makes sense. But is the settings default explained somewhere else?
</comment><comment author="clintongormley" created="2014-05-06T14:11:08Z" id="42306645">I've documented the defaults in 2e03a6629b9b85300164ac48c3ef655723369b60

thanks for opening
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5898</link><project id="" key="" /><description /><key id="31960302">5898</key><summary>Fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djui</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T10:12:24Z</created><updated>2014-07-16T21:46:08Z</updated><resolved>2014-04-25T20:59:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-22T17:50:49Z" id="41071553">@djui thanks for the fix! Can you sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so I can merge this in?
</comment><comment author="djui" created="2014-04-22T21:14:56Z" id="41096686">Done.
</comment><comment author="dakrone" created="2014-04-25T20:59:08Z" id="41438765">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Allow to disable randomization of shards and replicas via system property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5897</link><project id="" key="" /><description>Needed for REST backwards compatibility tests, since we need to run older tests with the latest runner, which randomizes shards and replicas, but the tests rely on defaults (5,1).
</description><key id="31958371">5897</key><summary>[TEST] Allow to disable randomization of shards and replicas via system property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T09:39:51Z</created><updated>2014-06-20T00:08:21Z</updated><resolved>2014-04-24T20:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-22T12:03:51Z" id="41031484">if that happens again we might just use something like -Dtests.compatibility=1.0.0 or something like this that is more abstract and internally we can then just check onOrAfter with our version? I mean I think that is good for now but maybe we should rather use a compat version ?
</comment><comment author="javanna" created="2014-04-22T12:09:09Z" id="41031886">Agreed, there are much nicer ways to do the same. I think potentially version would be more elegant, although it would make things a bit more complex and we don't seem to be needing such complexity. Hope I won't have to change my mind in a few weeks :)
</comment><comment author="s1monw" created="2014-04-22T12:09:42Z" id="41031917">wait, how is this more complex?
</comment><comment author="javanna" created="2014-04-22T12:38:42Z" id="41034298">I think this applies only to our REST tests, as the yaml tests that get run can be outside of the codebase and rely on different assumptions compared to their runner. Do you see other usecases for the more generic `-Dtests.compatibility` that would apply to ordinary integration tests? 

Also, I find the version a bit misleading here as it doesn't have anything to do with the elasticsearch version that the test gets run against. It's all about the version of the yaml tests (effectively prior to a specific commit), that rely on behaviours that have been changed in the runner (the test code) later on.

At the end of the day it is not more complex, I just don't love the idea of having other ifs in our code that have to do with versions, especially when the version is not the elasticsearch version.
</comment><comment author="s1monw" created="2014-04-22T12:44:48Z" id="41034750">well for instance I can see us changing the min num nodes at some point? not sure though...
</comment><comment author="javanna" created="2014-04-22T15:26:44Z" id="41053736">Hey @s1monw I updated the PR, it now uses a more generic approach. Can you have a look?
</comment><comment author="s1monw" created="2014-04-23T15:38:45Z" id="41176545">I don't like the special version. We already have a version in `org.elasticsearch.Version` We can parse it and we have it hardcoded in our releases. We also have onOrAfter methods for it. I think we should just use it then you can run `-Dtests.version=1.0.1` or something like this
</comment><comment author="javanna" created="2014-04-24T20:04:39Z" id="41325413">Hey @s1monw I updated the PR using our ordinary version, and moved back to `ElasticsearchIntegrationTest` the `randomIndexTemplate` method as discussed.
</comment><comment author="s1monw" created="2014-04-24T20:06:02Z" id="41325565">LGTM good stuff
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomized number of replicas between 0 and the number of data nodes - 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5896</link><project id="" key="" /><description>Randomized number of replicas between 0 and the number of data nodes - 1 (rather than just between 0 and 1) when creating the random index template before each test
</description><key id="31955222">5896</key><summary>[TEST] Randomized number of replicas between 0 and the number of data nodes - 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T08:49:38Z</created><updated>2014-06-19T16:13:23Z</updated><resolved>2014-04-23T15:48:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-22T09:01:32Z" id="41017932">maybe we should use scaledRand here?
</comment><comment author="javanna" created="2014-04-22T09:15:29Z" id="41019084">Why not? Are you worried that we'd have too many replicas and that might slow down the tests?
</comment><comment author="s1monw" created="2014-04-22T10:52:48Z" id="41026343">well it might be a perf issue for tests - I don't want them to take too long
</comment><comment author="javanna" created="2014-04-22T12:11:02Z" id="41032015">Got it, I made it more rare, 9 times out of 10 it uses now either 0 or 1, 1 time potentially a higher number depending on the number of data nodes.
</comment><comment author="s1monw" created="2014-04-23T15:40:04Z" id="41176713">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve terms aggregation to perform the segment ordinal to global ordinal lookup post segment collection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5895</link><project id="" key="" /><description>In case when there are not too many unique values it is better to do the segment ordinal to global ordinal lookup after segment results have been processed.
</description><key id="31948737">5895</key><summary>Improve terms aggregation to perform the segment ordinal to global ordinal lookup post segment collection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T06:34:41Z</created><updated>2015-06-07T14:24:33Z</updated><resolved>2014-04-29T04:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-22T07:01:43Z" id="41009458">The initial commit adds an extra execution mode for terms aggregation (`global_ordinals_low_cardinality`) that performs the segment ordinal to global ordinal lookup post segment collection instead of looking up global ordinals on the fly (during segment collection). On low cardinality fields this basically cuts execution time down by half compared to using the default `global_ordinal` execution mode. (per hit one lookup takes place instead of two)
</comment><comment author="martijnvg" created="2014-04-23T18:25:24Z" id="41196848">Merged the `global_ordinals_low_cardinality` execution mode in the `GlobalOrdinalsStringTermsAggregator` class and pick the post segment global ordinal lookup or on the fly global ordinal lookup based on the number of unique terms to number of unique documents ratio on a per segment basis.
</comment><comment author="jpountz" created="2014-04-25T00:11:57Z" id="41347468">I'm concerned that this change conflicts a bit with https://github.com/elasticsearch/elasticsearch/pull/5873. For example, if you have a segment where ordinals are already global, #5873 would use them directly and this would be optimal.

On the other hand, this change would collect them into a separate structure and merge it with the global counts when the collection of the segment is terminated. Can we not collect into a different structure when ordinals are already global? (not sure how to detect it cleanly)
</comment><comment author="jpountz" created="2014-04-25T00:14:34Z" id="41347649">Moreover, I liked better when this execution mode was in its own class since it might have different runtime properties (especially memory usage)?
</comment><comment author="martijnvg" created="2014-04-25T04:28:20Z" id="41357947">Initially I put this enhancement into a different execution hint and I moved it into `global_ordinals` hint for the following reasons:
- I would have ended up with two more execution modes: `global_ordinals_low_cardinality` and `global_ordinals_low_cardinalty_hash` and I don't like that too much.
- The most important reason for me is that during parsing automatically selecting the right impl seems more difficult (in `TermsAggregatorFactory`). For example I think that `valuesSource.metaData().maxAtomicUniqueValuesCount()` is too rough. In the `setNextReader()` method in the aggregator we have more fine grained statistics (from Ordinals.Docs and Scorer), that can make a better decision what should be used. 

So maybe we should be more conservative when using this post segment collection global ordinal resolving to be sure that `segmentCounts` bigarray isn't taking too much memory. We can lower the threshold to 0.5 and include an upper bound of unique values (this what determines the memory cost of `segmentCounts`) from where we fallback to on the fly global ordinal resolving.

**update:** We can in the `TermsAggregatorFactory` iterate over all atomic readers and fetch the Ordinals.Docs to figure out what strategy is better? In that case I'm ok with putting this in post segment collection global ordinals lookup in a different strategy. 

&gt; On the other hand, this change would collect them into a separate structure and merge it with the global counts when the collection of the segment is terminated. Can we not collect into a different structure when ordinals are already global? (not sure how to detect it cleanly)

I think this can be detected. If the `globalOrdinals` field is on instance `GlobalOrdinalMapping` then we can fallback to normal collection (setting `segmentCounts` to null). This enhancement must work together with the #5873 optimization.  
</comment><comment author="jpountz" created="2014-04-25T10:24:02Z" id="41378641">I tried to think more about when to use this execution mode:
- it can only be used on leaf aggregators,
- if you are currently doing a terms under terms aggregation, we currently use the `global_ordinals_hash` mode in order to not allocate memory for every possible bucket. But if we start doing the same with this execution mode, the LongHash is probably going to kill the speedup that we gained from collecting segment ordinals.

So in the end, it looks to me like this new execution mode would be safe/useful on single-level terms aggregations? (which might still be quite common)

&gt; update: We can in the TermsAggregatorFactory iterate over all atomic readers and fetch the Ordinals.Docs to figure out what strategy is better?

+1

&gt; I think this can be detected. If the globalOrdinals field is on instance GlobalOrdinalMapping

I tend to dislike `instanceof` checks since it tends to be fragile. For example, if we get a second class impl that exposes global ordinals, this will break. :(
</comment><comment author="martijnvg" created="2014-04-25T17:49:25Z" id="41420447">Updated the PR to have a dedicated `global_ordinals_low_cardinality` implementation instead merging this enhancement into `global_ordinals` implementation.

&gt; I tend to dislike instanceof checks since it tends to be fragile. For example, if we get a second class impl that exposes global ordinals, this will break. :(

I replaced that with `if (globalOrdinals.maxOrd() != segmentOrdinals.maxOrd())`.
</comment><comment author="martijnvg" created="2014-04-27T13:16:27Z" id="41496262">Thanks for reviewing this @jpountz! I Updated PR with the following changes:
- Moved mapSegmentCountsToGlobalCounts() check for last segment to postCollect
- segmentOrdinals in `GLOBAL_ORDINALS_LOW_CARDINALITY` impl instantiated with `maxOrd`.
- Compute maxOrd in TermsAggregatorFactory if global ordinals is going to be used.
- Use maxOrd to pick GLOBAL_ORDINALS or GLOBAL_ORDINALS_LOW_CARDINALITY
- For GLOBAL_ORDINALS and GLOBAL_ORDINALS_LOW_CARDINALITY use maxOrd as estimated bucket count
</comment><comment author="jpountz" created="2014-04-27T21:02:49Z" id="41508997">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Add /_search_shards documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5894</link><project id="" key="" /><description /><key id="31939466">5894</key><summary>[DOCS] Add /_search_shards documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>docs</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T01:50:30Z</created><updated>2014-07-16T21:46:10Z</updated><resolved>2014-04-22T14:54:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-22T09:48:19Z" id="41021612">@dakrone ++
</comment><comment author="dakrone" created="2014-04-22T14:54:48Z" id="41049624">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update setup repositories documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5893</link><project id="" key="" /><description>Update doc so
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html
example is going to 1.1 instead of 0.90
</description><key id="31935227">5893</key><summary>Update setup repositories documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Benoss</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-22T00:01:00Z</created><updated>2015-06-08T15:06:05Z</updated><resolved>2014-04-25T20:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-22T17:08:38Z" id="41066695">Hi @Benoss, thanks! Can you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so I can merge this in.
</comment><comment author="Benoss" created="2014-04-22T19:55:33Z" id="41087667">Signed :)
</comment><comment author="dakrone" created="2014-04-25T20:57:44Z" id="41438638">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Supported java version for java api client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5892</link><project id="" key="" /><description>Would like to see information on supported version(s) of Java to use for Java API clients in the preface section (http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/java-api.html) of the Java API guide, eg. do we recommend using the same version/update version of Java as the server, eg. 1.7 update 25 or 55, do we support using an older (or later version of Java for the client, eg. 1.6/1.8), etc..
</description><key id="31908404">5892</key><summary>Supported java version for java api client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2014-04-21T17:14:15Z</created><updated>2014-12-30T16:41:04Z</updated><resolved>2014-12-30T16:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T16:41:04Z" id="68372214">Added here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html#jvm-version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable turning on IndexWriter's infoStream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5891</link><project id="" key="" /><description>For diagnosing low level IndexWriter issues, it's sometimes useful to enable IndexWriter's infoStream output, but one cannot do this from Elasticsearch today.
</description><key id="31905146">5891</key><summary>Enable turning on IndexWriter's infoStream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-21T16:22:01Z</created><updated>2015-06-07T14:25:17Z</updated><resolved>2014-04-24T20:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-21T16:57:55Z" id="40952153">++, I suggest this setting will be real time setting (changed using index update settings API). I think the interesting question would be into which file will it be logged? I think it should be a separate file, that is placed in the same logging dir location.
</comment><comment author="mikemccand" created="2014-04-21T17:32:18Z" id="40955550">I like the idea of having this be a real-time setting, so you can go into an existing index that's doing something strange and turn on IW.infoStream.  The challenge is, this is not a real-time setting in IndexWriter today (you must close/reopen the writer), which is annoying.

I think we should just make our own delegating InfoStream impl that we pass to IW on init, so IW always logs messages to it, but then we tell it to turn on/off whether it sends those messages to a log file (I agree, it should be a separate file in the normal logging dir).

I'll give this a shot.
</comment><comment author="kimchy" created="2014-04-21T19:15:45Z" id="40966300">Wondering if a delegate InfoStream will not hurt perf, since the string construction to log it will happen now all the time within Lucene. Potentially, this setting can be realtime, but we will close and open the IndexWriter with it within our Engine (we can do that with some settings that require a fresh IW).
</comment><comment author="rmuir" created="2014-04-21T19:21:13Z" id="40966839">This can safely be a realtime setting.

There is actually always a infoStream set, it is just a NullInfoStream that just always returns false for isEnabled? All checks in IndexWriter look like this:

```
  if (infoStream.isEnabled("DWPT")) {
    infoStream.message("DWPT", "done abort");
  }
```

So its sorta like logger apis in that sense. You can safely have one that flips on and off at will:
</comment><comment author="rmuir" created="2014-04-21T19:40:21Z" id="40968723">About linking InfoStream to a logger, it should be pretty simple. We added this to solr with this class: http://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/update/LoggingInfoStream.java

However, there was more going on there (e.g. legacy settings for infostream and so on).

Maybe for ES, there should be no option for infostream at all? it should just be set, and it should check if trace or debug level logging is enabled?

Basically: the user would be unaware of infostream. They just crank up logging and see more stuff...
</comment><comment author="mikemccand" created="2014-04-21T19:45:13Z" id="40969192">I like that solution; this way the user doesn't need to learn another setting.  This is just increased verbosity.
</comment><comment author="kimchy" created="2014-04-21T21:08:35Z" id="40977478">we could have it under a specific logging name, yea, and then its just a matter of enabling trace logging on it specifically. I missed the enhancement to InfoStream, nice!
</comment><comment author="mikemccand" created="2014-04-22T18:02:07Z" id="41073081">OK, I pushed an initial commit .... I found a LoggerInfoStream which
didn't seem to be used and whose intent seemed to be the same as this
issue (just for an older time when Lucene took PrintStream), so I
re-purposed it into a Lucene InfoStream.

I'm not sure about passing the indexSettings/shardId down, but I
want to make sure the shardId is in the log so we can separate out
the logs from multiple IndexWriters.  Is there a cleaner way?

I also hit &amp; "fixed" an unrelated NPE when I set logging to trace but
I'm unsure if it's a problem that newSearcher is null when topReader
was true?  I put a nocommit.

I turned on trace logging in the tests (-Des.logger.level=TRACE) and
confirmed the IndexWriter's infoStream messages are appearing.  I
still need to figure out how to make a test case that checks
it... maybe I need to set up a MockAppender...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] getting started tutorial</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5890</link><project id="" key="" /><description>add getting started tutorial into reference docs
</description><key id="31899927">5890</key><summary>[DOCS] getting started tutorial</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels /><created>2014-04-21T14:56:16Z</created><updated>2014-07-16T21:46:11Z</updated><resolved>2014-04-22T17:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-22T11:45:36Z" id="41030151">heya

I didn't reread the content, just looked at the layout.  Couple of comments: when referring to settings or endpoints or values like IP addresses, I prefer to render them as code, eg:

```
using the `_search` endpoint... set the IP address to `localhost`... etc
```

You want your code examples to be something that people can copy and paste.  Don't include the `$` or `mo@mwubuntu1:~$` or `&gt;` as they just get in the way.

Move the request body onto the next line, eg instead of:

```
$ curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
'
```

write:

```
curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
'
```

Finally, some pages are quite long (== overwhelming), eg the exploring your data page.  I'd remove some of the `[float]`'s so that each page is a bite-sized chunk.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update with doc_as_upsert + additional fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5889</link><project id="" key="" /><description>Hello,

how about adding a feature to update api, that allows you to perform an update with upsert defined as "the same doc plus additional fields".

Currently this can be done with this:

```
{
  doc: {
    field1: "Lorem ipsum"
  },
  upsert: {
    field1: "Lorem ipsum",
    additional_field1: 10
  }
}
```

This solution suffers from the same issue as one described here: #3153 (namely: sending potentially large data amount over the wire twice). Maybe, when in update body there are both `doc_as_upsert` and `upsert`, Elasticsearch should merge the sent `doc` with the hashmap in `upsert`? That's what I actually tried to do, but ES in this case favors `doc_as_upsert` over `upsert` content.

So, basically something like this would be nice:

```
{
  doc: {
    field1: "Lorem ipsum"
  },
  doc_as_upsert: true,
  upsert: {
    additional_field1: 10
  }
}
```

Cheers,
d
</description><key id="31897154">5889</key><summary>Update with doc_as_upsert + additional fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">d-ph</reporter><labels><label>:CRUD</label><label>adoptme</label><label>high hanging fruit</label></labels><created>2014-04-21T14:02:15Z</created><updated>2016-11-10T19:20:26Z</updated><resolved>2016-11-10T19:20:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T17:18:52Z" id="91626558">This sounds interesting, however we discussed it and I think it would be better as part of revisiting the update API in general for API cleanups/enhancements. Marking this as "adopt me" right now and we will fold it into enhancing the update API in the future as well.
</comment><comment author="clintongormley" created="2016-11-10T19:20:26Z" id="259781847">This request hasn't gained any traction, so I'm going to close it for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term query not working for capital letters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5888</link><project id="" key="" /><description>ElasticSearch 1.1.0

curl -XPOST 'http://localhost:9200/test/test' -d '{
    "letters" : "ABCD"
}'
=&gt;
{"_index":"test","_type":"test","_id":"I8X9z8S9SIahvmv3wFekzA","_version":1,"created":true}

curl -XGET 'http://localhost:9200/test/_search' -d '{
    "query": {
        "term": { "letters": "ABCD" }
    }
}'
=&gt;
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
</description><key id="31877072">5888</key><summary>Term query not working for capital letters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">evanwong</reporter><labels /><created>2014-04-21T02:11:17Z</created><updated>2016-02-25T16:01:56Z</updated><resolved>2014-04-22T12:10:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-21T02:34:27Z" id="40912232">This isn't working because elastisearches's text queries are analyzed text. The `term` query takes the term exactly as provided and looks for it in the analyzed text. Your example doesn't find anything because the default analyzer lowercases.  You have two options:
1. Use a match query. That analyzes the query text and then looks for it. This means lower case searched would find capitals and vice versa. 
2. Use the mapping API to define the letters field as not_analyzed. Now the term query for capitals will find only capitals and lowercase will find only lowercase. 

Might be worth adding something else to the docs because I've seen this kind of mistake quite a few times now. 

Sent from my iPhone

&gt; On Apr 20, 2014, at 10:11 PM, Evan Wong notifications@github.com wrote:
&gt; 
&gt; ElasticSearch 1.1.0
&gt; 
&gt; curl -XPOST 'http://localhost:9200/test/test' -d '{
&gt; "letters" : "ABCD"
&gt; }'
&gt; =&gt;
&gt; {"_index":"test","_type":"test","_id":"I8X9z8S9SIahvmv3wFekzA","_version":1,"created":true}
&gt; 
&gt; curl -XGET 'http://localhost:9200/test/_search' -d '{
&gt; "query": {
&gt; "term": { "letters": "ABCD" }
&gt; }
&gt; }'
&gt; =&gt;
&gt; {"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2014-04-22T12:10:25Z" id="41031969">&gt; Might be worth adding something else to the docs because I've seen this kind of mistake quite a few times now.

Such as this: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_exact_values_vs_full_text.html
</comment><comment author="nik9000" created="2014-04-22T13:00:57Z" id="41036155">Ah!  That covers it.
</comment><comment author="iahmad-khan" created="2016-02-24T15:20:37Z" id="188303319">@nik9000 
That seems opposite to what you said. My fields are not_analyzed in mapping , but they still have the problem of not matching captical letters. 
</comment><comment author="nik9000" created="2016-02-25T16:01:56Z" id="188853485">&gt; That seems opposite to what you said. My fields are not_analyzed in mapping , but they still have the problem of not matching captical letters.

If they are not analyzed and you can't find them when you search for the exact same text then I'd open up an issue with a recreation in curl (or sense). The curl step is super important because its kind of a lowest common denominator - we can all run the tests. It _should_ work. I mean, we test it on ever compile, but it could be that you've found something very novel.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.ClassCastException when "properties" set to []</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5887</link><project id="" key="" /><description>Some languages (e.g. PHP) don't have Map type just Array, so if "properties" or "fields" in mapping are empty - encoding it to json will result in array [], and not object {}
until 0.9 this was not an issue, but now elasticsearch refuses to create mapping with exception:

```
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.util.Map
  at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:216)
  at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:258)
  at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:216)
```

sample mapping:

```
curl -XPUT 'localhost:9200/test' -d '{
    "mappings": {
        "test": {
            "_source": {
                "enabled": true
            },
            "properties": {
                "notes": {
                    "index": "no",
                    "type": "nested",
                    "include_in_all": true,
                    "enabled": true,
                    "properties": []
                }
            }
        }
    },
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
    }
}'
```

this issue could be resolved by adding addition check to see if fieldNode is instance of Map:

```
-    } else if (fieldName.equals("properties")) {
+    } else if (fieldName.equals("properties") &amp;&amp; fieldNode instanceof Map) {
```
</description><key id="31872705">5887</key><summary>java.lang.ClassCastException when "properties" set to []</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edast</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-20T21:22:32Z</created><updated>2014-07-09T13:05:04Z</updated><resolved>2014-05-01T16:41:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="evillemez" created="2014-05-01T15:21:22Z" id="41919718">Yeah, this affected me as well, took a bit of time to track down to the root cause.
</comment><comment author="kimchy" created="2014-05-01T15:56:20Z" id="41923388">Aye, it should be fixed. I think that purposed solution might not make sense, because we need to raise a failure if an array is provided with values in properties, but maybe we can optimize for the array with 0 elements type case.
</comment><comment author="kimchy" created="2014-05-01T16:42:52Z" id="41928356">fixed in upcoming 1.2, see #6006
</comment><comment author="edast" created="2014-05-05T06:46:53Z" id="42162741">could you also change src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java (line 66) to support empty array for "fields"?

```
   } else if (fieldName.equals("fields")) {
       Map&lt;String, Object&gt; fieldsNode = (Map&lt;String, Object&gt;) fieldNode;
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem of Term Suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5886</link><project id="" key="" /><description>I have a problem with term suggester. I dont know what was happening. 

I have 3 documents: [doc1:{content: "Anh y&#234;u ta"},doc2:{content:"Anh y&#234;u ta"}, doc3:{content:"Anh y&#234;u t&#237;"}](content was indexed with vi_annalyzer)

I using term suggester as: SuggestionBuilder text = SuggestBuilder
                .termSuggestion(Suggestion.DEFAULT_NAME).field("c").text("t&#237;").minWordLength(2).size(1).suggestMode("missing");

I was received results from termsuggestion is:{text: "ta" , freq:2 , score:0.5} 

=&gt; Why term suggestion is "ta" ?. In my thinking , no term suggestion will been returned. Plz help me to explain it. what it's wrong and how to fix it. Thanks all my friends!!!

This is config vi_annalyzer: 
index:
  analysis:
    analyzer:
      vi_analyzer:
        type: custom
        tokenizer: whitespace
        filter: [trim, lowercase, hunspell_vi]
    filter:
      hunspell_vi:
        type: hunspell
        locale: vi_VN 
</description><key id="31860002">5886</key><summary>Problem of Term Suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">letrungtrung</reporter><labels /><created>2014-04-20T04:16:16Z</created><updated>2014-04-20T12:14:32Z</updated><resolved>2014-04-20T11:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-20T11:41:18Z" id="40893180">Hi @letrungtrung 

Please ask questions in the mailing list, not in the issues list

thanks
</comment><comment author="letrungtrung" created="2014-04-20T12:14:32Z" id="40893681">Hi @clinton gormley!

I asked in the mail list, but not replies from anyone. And i thought this's
 a issue or bug of term suggester , therefore i put it in te issue list.
Sorry about that. Thanks for replying to me.

P/s: If you have free time, please answer my problem. Thanks again my
friend!

On Sun, Apr 20, 2014 at 6:41 PM, Clinton Gormley
notifications@github.comwrote:

&gt; Hi @letrungtrung https://github.com/letrungtrung
&gt; 
&gt; Please ask questions in the mailing list, not in the issues list
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5886#issuecomment-40893180
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update service.bat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5885</link><project id="" key="" /><description>corrected typo, "Exiting..." instead of "Existing..."
</description><key id="31859696">5885</key><summary>Update service.bat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">igal-getrailo</reporter><labels /><created>2014-04-20T03:46:41Z</created><updated>2014-07-13T05:16:45Z</updated><resolved>2014-05-06T08:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Searcher might not be closed if store hande can't be obtained</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5884</link><project id="" key="" /><description>Today we first get a reference to the IndexSearcher in #acquireSearcher
and then futher down we try to run Store#incRef() which might throw an
exception if the store is already closed. There is a small window
that allows this to happen during InternalEngine#close() when we try
to acquire the searcher at the same time and the engine is the last
resource that holds a reference to the store.

This commit only affects unreleased code since the Store's ref counting
has not yet been released.
</description><key id="31848460">5884</key><summary>Searcher might not be closed if store hande can't be obtained</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-19T15:17:58Z</created><updated>2015-06-07T20:45:55Z</updated><resolved>2014-04-21T18:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-20T10:33:59Z" id="40892193">LGTM, sneaky one!
</comment><comment author="s1monw" created="2014-04-21T18:46:21Z" id="40963272">pushed to `1.x` &amp; `master`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fuzzy query boosts misspellings </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5883</link><project id="" key="" /><description>Fuzzy queries are mostly used to find terms in spite of misspellings.  However, the way they are currently implemented, they **favour** misspellings over the correct spelling, eg:

```
DELETE /t

PUT /t
{
  "settings": {
    "number_of_shards": 1
  }
}
```

Index the correct spelling `abcde` three times:

```
POST /t/t
{"foo": "abcde"}
POST /t/t
{"foo": "abcde"}
POST /t/t
{"foo": "abcde"}
```

Index the incorrect spelling `abcdf`once:

```
POST /t/t
{"foo": "abcdf"}
```

A `fuzzy` query for `abcde` ranks the less common `abcdf` over the correct spelling:

```
GET /_search
{
  "query": {
    "match": {
      "foo": {
        "query": "abcde",
        "fuzziness": 1
      }
    }
  }
}
```

Result:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "6YKZxTEKRM63cqmqkBMZQg",
        "_score": 1.3621485,
        "_source": {
           "foo": "abcdf"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "fFQJd4W8Sfiifqm58gdiiQ",
        "_score": 0.59394336,
        "_source": {
           "foo": "abcde"
        }
     },
```

This is clearly wrong. The `fuzzy_like_this` query, however, incorporates the edit distance into the score:

```
GET /_search
{
  "query": {
    "fuzzy_like_this": {
      "fields": [
        "foo"
      ],
      "like_text": "abcde"
    }
  }
}
```

Thus scoring `abcde` over `abcdf`:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "fFQJd4W8Sfiifqm58gdiiQ",
        "_score": 1.4260894,
        "_source": {
           "foo": "abcde"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "i6nBukT8TdesYXTQzodBZw",
        "_score": 1.4260894,
        "_source": {
           "foo": "abcde"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "gafV0v6ETq2-FA3qQnpIQg",
        "_score": 1.4260894,
        "_source": {
           "foo": "abcde"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "6YKZxTEKRM63cqmqkBMZQg",
        "_score": 0.9126973,
        "_source": {
           "foo": "abcdf"
        }
     }
  ]
```

I'm not sure quite what the right scoring algorithm here is. Should it take into account only the edit distance from the query term (so a query for `abcde` prefers `abcde`, while a query for `abcdf` prefers `abcdf`), or should we also invert the IDF to prefer more common terms (eg a fuzzy query for `abcdf` would prefer `abcde`)?
</description><key id="31847139">5883</key><summary>Fuzzy query boosts misspellings </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-04-19T13:53:30Z</created><updated>2014-12-30T16:32:51Z</updated><resolved>2014-12-30T16:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-22T13:30:07Z" id="41039190">It seems that the edit distance is taken into account:

```
DELETE /_all 

PUT /t
{
  "settings": {
    "number_of_shards": 1
  }
}

POST /t/t
{"foo": "abcde"}
POST /t/t
{"foo": "abcde"}
POST /t/t
{"foo": "abcde"}

POST /t/t
{"foo": "abcdf"}
POST /t/t
{"foo": "abcfg"}

GET /_search
{
  "query": {
    "match": {
      "foo": {
        "query": "abcde",
        "fuzziness": 2
      }
    }
  }
}
```

Results:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "nqFyLpkUTE-1lGA1c5vRRg",
        "_score": 1.2922336,
        "_source": {
           "foo": "abcdf"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "H2HYQtDRSeGhUAA7nvOgZg",
        "_score": 0.9691752,
        "_source": {
           "foo": "abcfg"
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "zgefW3DQQ3CSaw-KXPsHTg",
        "_score": 0.65808666,
        "_source": {
           "foo": "abcde"
        }
     },
```

But the results are still not intuitive.  The least common term(s) are suggested instead of the most common.
</comment><comment author="clintongormley" created="2014-12-30T16:32:51Z" id="68371443">Closed in favour of #9103
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ConcurrentMergeSchedulerProvider should use Lucene's default settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5882</link><project id="" key="" /><description>In https://issues.apache.org/jira/browse/LUCENE-4661 we changed CMS's default maxMergeCount / maxMergeThreads to be less aggressive because the previous settings (allowing too many concurrent merges) would make merging less efficient.

But Elasticsearch's ConcurrentMergeSchedulerProvider is still using the old defaults.

I think we should just cutover to using whatever Lucene defaults to?
</description><key id="31841155">5882</key><summary>ConcurrentMergeSchedulerProvider should use Lucene's default settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-19T09:40:13Z</created><updated>2014-05-12T17:34:48Z</updated><resolved>2014-05-12T17:34:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-19T11:39:52Z" id="40867507">+1
</comment><comment author="mikemccand" created="2014-05-12T17:10:01Z" id="42859717">We are changing a lot of merge / indexing throttling issues in 1.2; I think to be more conservative we should put back ES's original defaults here.
</comment><comment author="mikemccand" created="2014-05-12T17:34:48Z" id="42862741">OK I restored the original ES defaults for 1.2/2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Added blacklist to be able to skip specific REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5881</link><project id="" key="" /><description>The blacklist can be provided through `-Dtests.rest.blacklist` and supports a comma separated list of globs
e.g. `-Dtests.rest.blacklist=get/10_basic/*,index/*/*`

Also added some missing docs and made it clearer that the suite/test descriptions effectively contains their (relative) path (api/yaml_file/test section)
</description><key id="31840847">5881</key><summary>[TEST] Added blacklist to be able to skip specific REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-19T09:15:38Z</created><updated>2014-06-12T08:14:30Z</updated><resolved>2014-04-22T07:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-19T19:47:37Z" id="40878811">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for conditional highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5880</link><project id="" key="" /><description>Adds a "conditional" object under each highlighting field that can contain
other fields.  Those fields are highlighted based on if there was a match
in the containing field.

It'll let you do things like this example in the docs:

This example extracts extracts the first 100 characters from `text` if there is
a match in `title`, otherwise it highlights `text` as normal.

``` js
{
    "highlight": {
        "fields": {
            "title": {
                "conditional": {
                    "match": {
                        "text": {
                            "no_match_size": 100,
                            "skip_matching":  true
                        }
                    }
                    "no_match": {
                        "text": {
                            "no_match_size": 100
                        }
                    },
                }
            }
        }
    }
}
```

Also adds a setting to instruct the highlighter to skip its normal match
logic and just do its no_match_size stuff.   This is very useful with
conditional highlighting "chains" that end in a no_match_size extracting
a previous entry in the chain.  Like this example from the docs:

``` js
{
    "highlight": {
        "fields": {
            "text": {
                "conditional": {
                    "no_match": {
                        "auxiliary_text": {
                            "conditional": {
                                "no_match": {
                                    "file_text": {
                                        "conditional": {
                                           "no_match": {
                                               "text": {
                                                    "no_match_size": 100,
                                                    "skip_matching": true
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

Closes #4649
</description><key id="31835875">5880</key><summary>Add support for conditional highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-04-19T02:15:05Z</created><updated>2014-06-18T16:05:40Z</updated><resolved>2014-05-14T19:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-25T15:33:24Z" id="41406018">Looks like I pushed a half finished commit while I had things in my cache.....  I'm not very smart sometimes.  Anyway, amended and rebased.

While somewhat silly looking, this could really help my performance.
</comment><comment author="nik9000" created="2014-04-30T18:02:33Z" id="41828532">I hate to be that guy, does anyone have the cycles to review this?
</comment><comment author="clintongormley" created="2014-05-09T11:15:57Z" id="42655632">Hi @nik9000 

The idea makes a lot of sense, but I don't like the API.  It's really verbose and difficult to understand.  How about this instead. 

```
{
  "highlight": {
    "fields": {
      "text": {
        "rules": [
          {},
          { "field": "auxiliary_text" },
          { "field": "file_text" },
          { "field": "text", "no_match_size": 100 }
        ]
      }
    }
  }
}
```

This processes each rule in turn until it finds a result, so:
1. Uses the `title` field and does normal matching on it
2. Uses the `auxiliary_text` field and does normal matching on it
3. Uses the `text` field and does normal matching, but falls back to extracting the first 100 chars.

Then for your other example:

```
{
  "highlight": {
    "fields": {
      "title": {
        "rules": [
          {
            "require_match": "title",
            "field": "text",
            "no_match_size": 100,
            "skip_matching": true
          },
          { "field": "text" }
        ]
      }
    }
  }
}
```

Which does:
1. If the `title` field matches, then just extract the first 100 chars from `text`
2. Otherwise, do normal matching on the `text` field

Of course, in any of the rules you can include `fragment_size` or whatever other parameters you want to override. 

I think this captures all of the conditions that you want to implement, but I think it is a lot easier to understand.

Thoughts?
</comment><comment author="nik9000" created="2014-05-09T20:05:45Z" id="42707827">&gt; The idea makes a lot of sense, but I don't like the API. It's really verbose and difficult to understand. 

Yeah, it was pretty much what was easiest to put together at the time.  I'm not sure I understand yours that well though.  Is this pretty similar:

Old way (that we keep working forever because its simpler):

``` js
"highlight": {
  "fields": {
    "text": {},
    "aux_text": {},
    "file_text": {}
  }
}
```

New way:

``` js
"highlight": {
  "fields": {
    "text":
      "rules": [
        {},  // &lt;---- defaults to searching text
        { "field": "aux_text" },
        { "field": "file_text" },
        { "field": "text", "skip_match": true, "no_match_size": 100 }
      ]
    }
  }
}
```

But that really only makes sense to me when there aren't matches.  Maybe that's ok for for now.

No, not really.  Now I'm thinking and that is a scary thing.  It'd really be best if the highlighter itself got to decide how to invoke the chain.  That'd give me freedom to implement stuff like "keep going to the next field if you haven't found all the query terms.  maybe boost the ones you didn't find or something."   I know that's not a particularly helpful comment.  My heads all jumpled now.  Let me think more and write more later.
</comment><comment author="clintongormley" created="2014-05-10T10:23:04Z" id="42738146">&gt; Yeah, it was pretty much what was easiest to put together at the time. I'm not sure I understand yours that well though. Is this pretty similar:
&gt;         {},  // &lt;---- defaults to searching text

Yes, exactly.  That empty `{}` just uses the defaults, so it could also be written as:

```
{ "field": "text" }
```

&gt; But that really only makes sense to me when there aren't matches. Maybe that's ok for for now.

Your chain also worked on whether there were matches or not, no?

&gt;  That'd give me freedom to implement stuff like "keep going to the next field if you haven't found all the query terms. maybe boost the ones you didn't find or something

These more esoteric rules could be implemented in the same way as `{ "require_match": "some_field"}`.   I think the important thing about my proposed structure is that it should work like a `switch` statement, that is: process each rule in turn until you have a successful result, then stop.
</comment><comment author="nik9000" created="2014-05-12T13:42:35Z" id="42832892">So I thought about it over the weekend and the best thing I could think of is this:
0.  Abandon pretty much all the code I in this pull request.
1.  Guarantee highlighting is done in the order that it is specified in the search request.
2.  Allow highlighters to return a map of stuff in addition to the text.
3.  Have all highlighters stuff `"matched": true` in there if there are matches.
4.  Implement conditional highlighting as checks on those fields.  Like

``` js
"fields": {
  "text": { /*config*/ }
  "file_text": { "unless": {"text": "matched"} /*other config*/ }
}
```

These rule changes would allow other highlighters to add more stuff to the maps.  Say, for example, that a highlighter can detect if all query terms were included in the returned fragments.  Then you'd get this behaviour "for free".

``` js
  "file_text": { "unless": {"text": "highlighted_all_terms" } }
```

The reason for a map is so the highlighter can communicate with future invocations of itself on the same field.  Going back to the example above the highlighter could add the list of terms that didn't match to the map and it could, say, boost them when highlighting other fields.

I'll see if I can put together something more concrete in a couple of days.
</comment><comment author="clintongormley" created="2014-05-12T13:51:11Z" id="42833955">@nik9000 JSON objects are not ordered. If you want order, you need an array.
</comment><comment author="clintongormley" created="2014-05-12T13:56:49Z" id="42834671">Forgive me if I'm being dense, but I'm still having a hard time understanding all of the conditions that you want to check for.  Could you write a list of them in plain English?  It would make it easier to visualise the API.
</comment><comment author="nik9000" created="2014-05-12T14:47:52Z" id="42841023">&gt; @nik9000 JSON objects are not ordered. If you want order, you need an array.

Awe!  Its what I get for working in php so much lately.  Their array keys are maintained in the order you add them....

&gt; Forgive me if I'm being dense, but I'm still having a hard time understanding all of the conditions that you want to check for. Could you write a list of them in plain English? It would make it easier to visualise the API.

Things I can think of now:
- Some other field highlighted something
- Some other field highlighted all the terms

And their negation:
- Some other field didn't highlight anything
- Some other field didn't highlight all the terms

So in my application I highlight maybe 6 things.  Right now I always highlight all of them but it'd be really nice to do this:
1.  Title: always highlighted
2.  Redirect title: only highlighted if the title didn't find any all the query terms and boost the terms that weren't found
3.  Heading: only highlighted if the title and redirect title didn't find all the query terms and boost the terms that weren't found
4.  Text: always highlighted
5.  Auxiliary text:  only highlighted if there was no match at all in the text.
6.  File text: only highlighted if there was no match at all in either text or auxiliary text.

Now that I'm moving off the FVH the cpu cost of highlighting stuff I'm going to throw out will be less and less of a big deal for me as highlighting using postings has much lower overhead in most cases.  It'd still be nice to ignore it.

OTOH, my application is not in a good position to determine if all the query terms matched.  It doesn't even parse the query (I'll get off of query_string one day...) but the highlighter can do this.  The [one](https://github.com/wikimedia/search-highlighter) I'm moving to is 90% of the way there already.  The trick, then, is to add what it'd need to Elasticsearch to do that.  And what it needs is a way to shuffle data from previous executions to the next and a way to specify the order.

The other issue is that I don't want to make it too confusing because if it is no one else will benefit from it.  The trouble with the logic that looks like a switch statement is that it implies that the point is the get an answer and stop.  I guess that's where this pull request started, but when I got to thinking about it it was the wrong direction.  Not that it wouldn't be useful, but that its not the right abstraction for this thing I want to do in a plugin (boosting non-matched query terms and skipping entirely if there were all found).  So now I've got this annoyingly speculative proposal that really only makes sense on the context of this plugin.  Which, as far as I know, I'm the only person that uses it.
</comment><comment author="nik9000" created="2014-05-14T19:29:35Z" id="43126248">Abandoning in favor of #6178 which is less intrusive and will let me experiment with what I really need in a plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update function-score-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5879</link><project id="" key="" /><description>Grammar fix, nothing fancy.
</description><key id="31833873">5879</key><summary>Update function-score-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samuelmolinari</reporter><labels /><created>2014-04-19T00:45:59Z</created><updated>2014-07-04T09:54:09Z</updated><resolved>2014-04-20T11:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-19T01:26:58Z" id="40857138">LGTM
</comment><comment author="clintongormley" created="2014-04-19T13:00:07Z" id="40868812">Hi @samuelmolinari 

Thanks for the fix. Please could you sign our CLA so I can get your change merged in?
http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="samuelmolinari" created="2014-04-19T18:49:56Z" id="40877457">Sure, I've signed it.
</comment><comment author="clintongormley" created="2014-04-20T11:40:18Z" id="40893146">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Async replication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5878</link><project id="" key="" /><description>We are active in multiple data centers and have a need of async replication, eventual consistency is ok between the data cetners.

Currently there is no way to perform continuous async replication between 2 clusters or even within the same cluster. Earlier I thought of writing one off replication module but now I am thinking may be I should add it to the elasticsearch core. Need some advise on how to do this.
</description><key id="31822370">5878</key><summary>Async replication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mohitanchlia</reporter><labels /><created>2014-04-18T20:22:55Z</created><updated>2014-12-30T16:00:14Z</updated><resolved>2014-12-30T16:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mohitanchlia" created="2014-04-22T23:06:35Z" id="41106874">I am looking for a suggestion on how to do this. I am willing to provide a patch if necessary. My initial thoughts are to do it as part of the regular index writes i.e. build on the existing async functionality (haven't looked at the code). However, only issue I see is how to replay missed data when say node is down in the remote cluster.
</comment><comment author="clintongormley" created="2014-12-30T16:00:14Z" id="68368325">This will be possible once the changes API has been implemented.  Closing in favour of #1242
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to mvel 2.2.0.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5877</link><project id="" key="" /><description>Seems to work now on Java 8 (just now quickly ran some tests)
</description><key id="31819422">5877</key><summary>Upgrade to mvel 2.2.0.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>upgrade</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T19:29:27Z</created><updated>2015-06-07T14:26:33Z</updated><resolved>2014-04-18T19:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-18T19:52:29Z" id="40838899">we should upgrade! I think we should upgrade `1.0` and `1.1` as well it's really a bugfix
</comment><comment author="kimchy" created="2014-04-18T19:53:08Z" id="40838949">ran all the tests, work. pushing to master and 1.x, will push to 1.0 and 1.1 as well...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't block stats for read-only indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5876</link><project id="" key="" /><description>Closes #5855
</description><key id="31814315">5876</key><summary>Don't block stats for read-only indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2014-04-18T18:08:05Z</created><updated>2014-11-13T12:36:20Z</updated><resolved>2014-05-06T17:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-06T07:22:28Z" id="42273841">I am not sure if this is a bug, since it blocks on METADATA operation, which all stats/status/... check on, I don't think we need to change it.

Looked at the issue, they should set `index.blocks.read` to `true`, and then it would allow for METADATA operations as well.
</comment><comment author="imotov" created="2014-05-06T17:25:47Z" id="42332444">A possible workaround for this issue is to use `"index.blocks.write": true` instead of `read_only`. We will need to revisit this by adding more granular `METADATA_READ` and `METADATA_WRITE` blocks and implement this change across other methods such as segments, status, recovery...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update datehistogram-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5875</link><project id="" key="" /><description>Docs really need to explain how to suppress the hits portion of the response to shrink it down and speed it up. I had to do a lot of digging to figure out how and actual found it based on how to do it in facets. Although this probably effects all the various bucket aggregations. Perhaps the change should be made somewhere else in the docs. But made it here since this was where I needed it and tested it.

Hope this is helpful. Thanks!
</description><key id="31802074">5875</key><summary>Update datehistogram-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">sporty81</reporter><labels /><created>2014-04-18T14:36:54Z</created><updated>2014-08-11T10:32:12Z</updated><resolved>2014-08-11T10:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-18T14:47:49Z" id="40814016">agree that a "aggregation scope" section is missing... and it certainly need to be placed in the aggreagation.asciidoc (rather than any specific agg). We'll work on adding it... unfortunately the text you added is inaccurate and can be confusing.

The aggs scoke is defined by several things:
1. the search context (index/dices, type/s you're researching for)
2. the documents that match the query (if no query is specified, a default `match_all` query is assumed)

the `search_type` has nothing to do with the scope, but more to do with the execution mode of the search. Even if it doesn't return the actual hits, the scope is still the same (based on the 2 factors above)
</comment><comment author="sporty81" created="2014-04-18T16:23:05Z" id="40821621">Sure no problem but I'm not sure we are exactly on the same page... but you are the expert here. I was more referring to the contents of the response, not necessarily the search scope. I wasn't referring to the indexes and types selected. The docs need to tell how to suppress the "buckets" contents on bucket aggregations when it isn't needed. Which is more about the execution mode I guess and suppressing the hits portion of the response so only the aggregation itself is returned. So far setting the search_type to count is the only way I know of to suppress that info in the response. Thanks!
</comment><comment author="uboness" created="2014-04-18T20:28:13Z" id="40841583">&gt; suppress the "buckets" contents on bucket aggregations

I guess this statement confuses me. the buckets contents is always returned regardless of the `search_type` you use, the only thing with `search_type=count` would do is
1. indeed remove the hits (cause this search type only returns total matching count)
2. execute more efficiently (single round trip to the shard) as there's no need for the fetch phase

But this only impacts the `hits` part of the search response, the content of the aggregations will remain the same (regardless of the `search_type` you specify).

If you're only interested in aggs response (no hits), indeed `search_type=count` is the way to go (and we should probably mention that in the aggs docs), but `search_type` itself refers the the execution mode of the search request as a whole (not aggs specific) and we have that documented here: 

http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_search_options.html#_literal_search_type_literal
</comment><comment author="sporty81" created="2014-04-19T02:29:36Z" id="40858567">Correct. I think it would be really nice to mention this little important detail on this page somewhere within this chapter type section of the docs.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations.html
</comment><comment author="clintongormley" created="2014-05-06T14:07:08Z" id="42306123">Ping @uboness 
</comment><comment author="clintongormley" created="2014-08-07T18:34:00Z" id="51512799">This just needs a line somewhere on the aggs page saying that you can get aggs only (without hits) by using search_type=count
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field data diet.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5874</link><project id="" key="" /><description>We have lots of unused, or almost unused methods in our field data impls,
especially when dealing with ordinals. Let's nuke them.

This removes:
- `getNumOrds` (same as `getMaxOrd - 1`)
- `getNumDocs` (unused)
- gettting ordinals as a `LongsRef`
- `Ordinals.Docs.ordinals()`
</description><key id="31800580">5874</key><summary>Field data diet.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>non-issue</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T14:09:28Z</created><updated>2015-06-08T15:06:23Z</updated><resolved>2014-04-24T07:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-18T14:57:38Z" id="40814740">LGTM :) nice stats!!!! `+180  &#8722;644` ;)
</comment><comment author="martijnvg" created="2014-04-19T05:31:43Z" id="40861376">very nice :) LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use segment ordinals as global ordinals if possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5873</link><project id="" key="" /><description>Use segment ordinals as global ordinals if a segment contains all values for a field on a shard level.

PR for #5854
</description><key id="31799273">5873</key><summary>Use segment ordinals as global ordinals if possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T13:42:57Z</created><updated>2015-06-07T14:26:46Z</updated><resolved>2014-04-25T16:19:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-18T14:10:58Z" id="40811158">This looks good. I think we just need to make sure the mapping from segment ords to global ords for these segments isn't loaded into the cache.
</comment><comment author="martijnvg" created="2014-04-19T05:34:02Z" id="40861414">Updated the PR and yes, that make absolutely sense, to save a bit more memory. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5872</link><project id="" key="" /><description /><key id="31796190">5872</key><summary>0.19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhangxiaoheng</reporter><labels /><created>2014-04-18T12:24:52Z</created><updated>2014-07-16T21:46:14Z</updated><resolved>2014-04-18T13:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-18T13:21:00Z" id="40807829">Hi @zhangxiaoheng, I'm assuming the PR was opened by mistake or against the wrong branches?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field data: Ordinals should start with `0`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5871</link><project id="" key="" /><description>Our ordinals start with `1` while Lucene ordinals start with `0`. This makes the doc-values-based fielddata impls look weird since they keep rebasing ordinals. We should fix this inconsistency.
</description><key id="31795626">5871</key><summary>Field data: Ordinals should start with `0`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T12:09:46Z</created><updated>2014-06-10T23:26:36Z</updated><resolved>2014-04-28T08:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percentiles aggregations are always keyed and suggestion on non keyed response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5870</link><project id="" key="" /><description>I realize the percentiles aggregations is still experimental which is probably the cause for this: 

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java#L60

The routine that is currently in place to write the percentiles `non_keyed` output will write the aggregation like this:

```
"aggs" : {
  "my_percentiles": [
       { .. }, 
       { .. }
   ]
}
```

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/InternalPercentiles.java#L131

Making it the only aggregation to directly return an array instead of within a wrapped object. 

```
"aggs" : {
    "my_percentiles": { 
        percentiles: [
           { .. }, 
           { .. }
       ]
    }
}
```

Which makes the response very similar the non keyed range aggregation response:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-range-aggregation.html#search-aggregations-bucket-range-aggregation
</description><key id="31791174">5870</key><summary>Percentiles aggregations are always keyed and suggestion on non keyed response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T09:56:08Z</created><updated>2015-06-06T17:01:47Z</updated><resolved>2014-05-07T16:35:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-18T10:32:04Z" id="40800686">@Mpdreamz `percentiles` are not a bucketing agg. It's a metrics agg, and like all other metrics aggs, it's not keyed (see `min`/`stats`/etc...)
</comment><comment author="Mpdreamz" created="2014-04-18T10:54:45Z" id="40801554">I realise its not a bucketing agg :)

The code path for the percentiles agg always hits the [if (keyed)` code path](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/InternalPercentiles.java#L119). Making the else routine dead code.

The else routine introduces an array at a position all the other aggregations introduce an object which would make parsing the aggregations generically much much harder. 

i.e `"name_of_agg" : [` vs `"name_of_agg : {`

If we can remove the dead code and thus the chance to introduce an array at that position that would be great too.
</comment><comment author="uboness" created="2014-04-18T11:06:05Z" id="40801939">@Mpdreamz Not sure I'm following you tbh... maybe I'm missing something...

if you send `"keyed" : false`, the else path is executed, so it's not really a dead code. We decided to make the object form the default (ie. `"keyed" : true`) as we believe it's probably the form most ppl would like to get back. Like with other aggs, we did leave an option to get the percentiles as an array of values
</comment><comment author="Mpdreamz" created="2014-04-18T12:44:55Z" id="40805965">Ok my bad since keyed usually defaults to false (i.e range aggregation).

`keyed` also usually specifies the behaviour of an inner property (i.e `buckets` property inside a range aggregation) where as with percentiles it controls how the entire aggregation is returned.

More specifically:

```
"aggregations": {
      "myagg": [
         {
            "key": 1,
            "value": 60.4
         },
         {
            "key": 5,
            "value": 62
         },
         {
            "key": 25,
            "value": 70
         },
```

For `keyed:false` responses I would much rather see it return 

```
"aggregations": {
      "myagg": {
         values: [
         {
            "key": 1,
            "value": 60.4
         },
         {
            "key": 5,
            "value": 62
         },
         {
            "key": 25,
            "value": 70
         }
         ]
     }
}
```

All other aggregations follow the pattern `"name_of_agg": &lt;start_of_object&gt;` even simple metrics such as `min/max`

```
"max" : {
    value: 10
}
```

The way nonkeyed percentiles are implemented right now feels like this:

```
"max" : 10
```

And (as far as I could tell) non keyed percentiles are the only ones breaking the pattern here.
</comment><comment author="uboness" created="2014-04-18T14:25:23Z" id="40812251">yeah.. agree, I think `"values" : []` is more consistent and also makes more sense (as it's future proof)
</comment><comment author="uboness" created="2014-05-07T16:02:16Z" id="42446455">We decided to change the response structure and instead of nesting all the percentiles directly under the aggregation name, nest it under an intermediate `values` object (or when the `keyed` flag is set `false` under a `values` array).

This is a breaking change but we feel it's important to make it while percentiles are still considered experimental. The new format is more future proof as it'll allow us to potentially add additional info under the aggregation in a later phase if there'll be a need for it. The new format is also somewhat more consistent with the other metrics aggs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return the aggregation type in the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5869</link><project id="" key="" /><description>PR for #5867
</description><key id="31790679">5869</key><summary>Return the aggregation type in the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2014-04-18T09:43:51Z</created><updated>2014-06-12T07:59:52Z</updated><resolved>2014-06-12T07:59:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2014-06-12T07:59:52Z" id="45840004">This is now superseded by the more generalised _meta functionality off this PR #6465
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Added vertx elasticsearch integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5868</link><project id="" key="" /><description /><key id="31790416">5868</key><summary>[DOCS] Added vertx elasticsearch integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">larrytin</reporter><labels /><created>2014-04-18T09:36:48Z</created><updated>2014-06-12T17:03:40Z</updated><resolved>2014-05-06T15:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:02:16Z" id="42305423">Hi @larrytin 

Thanks for the PR. Sorry it has taken a while to get to it.  Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="larrytin" created="2014-05-06T14:21:01Z" id="42307867">I have signed the CLA using dev{at}goodow.com
thanks
</comment><comment author="clintongormley" created="2014-05-06T15:57:57Z" id="42321534">Thanks, merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return the aggregation type in the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5867</link><project id="" key="" /><description>Right now aggregations responses are hard to differentiate if you do not know how they were originally constructed. 

In the .NET client I manage just about to peace back together the type of the aggregations by doing some simple property inspection. 

For now this works even with the newly added 1.1 aggregations but I fear this approach won't scale in the long run. 

If I could specify a `agg_type_hint` flag on the search requests so aggregations are returned like this:

```
"aggregations": {
  "myagg": {
     "_type": "nested",
     "doc_count": 2
  }
}
```

I could parse the aggregations to a typed response a lot easier without having the hold a ref to original search request object.

Note: I'm working on a PR for this
</description><key id="31789044">5867</key><summary>Return the aggregation type in the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>feature</label></labels><created>2014-04-18T09:01:44Z</created><updated>2014-12-30T15:56:00Z</updated><resolved>2014-12-30T15:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-18T11:21:19Z" id="40802514">not sure about this... first I'm not sure it's really needed... second, if it is something ppl would really like to have, this feels like a partial feature, if we go down this route, I'd rather return the full agg configuration, not just the type (for example, I believe ppl asked in the past to get back the `interval` of the histos along with the response).

@clintongormley @jpountz what do you think?
</comment><comment author="clintongormley" created="2014-04-18T11:25:47Z" id="40802678">Hmm, it makes more sense to me to just hang on to the original request, rather than to pass the config back over the wire again, same as we do `bulk` and `mget`. But maybe this is easier to do in a scripting language than in .net
</comment><comment author="uboness" created="2014-04-18T11:33:52Z" id="40802950">yeah... I'd prefer to hang on original request as well... it feels wrong from many perspectives and inconsistent with other apis (as @clintongormley mentioned, `bulk` and `mget`, but one could also ask why we don't return the query back together with the search response).
</comment><comment author="Mpdreamz" created="2014-04-18T12:53:57Z" id="40806408">I have to respectfully disagree with both of you here :)

This is how a facet response looks like:

```
"facets": {
      "myfacet": {
         "_type": "terms",
         "missing": 0,
         "total": 2,
         "other": 0,
         "terms": []
      }
   },
```

So spitting out `_type` for aggregations makes it actually all the more consistent. 

Having deserialization state is a real pain in .NET since the whole deserialization is a single pass and most serializers can not hold state while creating custom converters. I had to piggy back in a very hackish manner in `JSON.NET` to support `multi_search` `mget` `bulk` responses and the fact that I had to do so made injecting other serializers in NEST (not Elasticsearch.Net btw) impossible..
</comment><comment author="jpountz" created="2014-04-18T13:27:07Z" id="40808197">I'm afraid giving the whole configuration back would make the response bloated. But on the other hand if we start giving options in order to only render the type or the interval, it would be a pain to test.

Maybe something that would be flexible and reasonably easy to do/test/maintain would be to let users add arbitrary metadata to aggregations at query time that we would render back in the aggregations response. This way, if a client needs the aggregation type to make deserialization easier, it could just add it to this metadata (same for histogram interval)?
</comment><comment author="Mpdreamz" created="2014-04-18T13:31:01Z" id="40808432">This isn't about configuration or request state though is about giving enough information back to be able to deserialize/know what we the thing that is being returned actually is. 

To me returning `_type` here is not analogous to returning the original query on search results or passed options to the aggregations itself.
</comment><comment author="Mpdreamz" created="2014-06-12T08:03:02Z" id="45840274">@jpountz I implemented the ability to attach arbitrary metadata to aggregations here #6465 would love a review whenever is convenient for you :)
</comment><comment author="clintongormley" created="2014-12-30T15:56:00Z" id="68367896">Closed by #8279
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The scroll, clear scroll and analyze apis should support json in request body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5866</link><project id="" key="" /><description>The scroll, clear scroll and analyze apis are the only apis not accepting json, yaml etc in the request body. In order to be consistent with all the other apis they should support this. These apis just assume the scroll_id / text to analyze is specified as a string in the request body.

Supporting json, yaml etc. can be done in bwc manner via content type sniffing and if the content type can't figured out then the request body is assumed to be consumed as a plain string.
</description><key id="31786105">5866</key><summary>The scroll, clear scroll and analyze apis should support json in request body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:REST</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T07:38:36Z</created><updated>2015-05-15T12:04:15Z</updated><resolved>2015-04-22T09:11:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-18T10:27:18Z" id="40800462">Just to note: the `_analyze` endpoint also accepts just text, instead of JSON.  This is particularly problematic for Sense because you can't send a plain text body with Sense (it has no way of detecting the end of the body, which may contain new lines).

For demo purposes, I'd love to support JSON there too.
</comment><comment author="martijnvg" created="2014-04-18T10:34:40Z" id="40800800">@clintongormley Then we should support it there too! Also by supporting json we can also write yaml tests for these apis.
</comment><comment author="clintongormley" created="2015-04-04T18:35:15Z" id="89635506">Reopening as the commit was backed out
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return missing (404) if a scroll_id is cleared that no longer exists.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5865</link><project id="" key="" /><description>PR for #5730
</description><key id="31785614">5865</key><summary>Return missing (404) if a scroll_id is cleared that no longer exists.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T07:24:10Z</created><updated>2015-06-07T14:26:56Z</updated><resolved>2014-05-12T08:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-24T08:23:06Z" id="41255034">@s1monw I updated the PR to replace noneFreed to numFreed, so now it just counts the number of freed search contexts.
</comment><comment author="martijnvg" created="2014-05-08T08:33:56Z" id="42525041">Updated PR to latest master.
</comment><comment author="spinscale" created="2014-05-08T12:57:01Z" id="42546327">left two comments. One last thing though that bothers me: In `SearchServiceTransportAction` you changed the types and thus the listener method definitions. This is not supposed to work in rolling upgrades, or does it?
</comment><comment author="martijnvg" created="2014-05-08T13:07:48Z" id="42547340">Thanks for reviewing it! 

The change of the type in `SearchFreeContextResponse` should work, because of the versioning in SearchFreeContextResponse class makes sure that same bytes are send over the wire if node version &lt; 1.2.0
</comment><comment author="martijnvg" created="2014-05-09T13:07:50Z" id="42663554">@spinscale I updated the PR and addressed the two comments.
</comment><comment author="spinscale" created="2014-05-09T13:18:19Z" id="42664451">LGTM
</comment><comment author="martijnvg" created="2014-05-12T08:06:33Z" id="42805385">Pushed to master and 1.x branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error handling on invalid mapping data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5864</link><project id="" key="" /><description>``` json
DELETE /test1
PUT /test1
PUT /test1/test/_mapping
{
  "mappings":
  {
    "test":{
      "properties":{
        "prop1":{
          "type": "string",
          "index" : "not_analyzed"
        }
      }
    }
  }
}
GET /test1/test/_mapping
```

The 2nd PUT request above is invalid but it is still accepted/acknowledged by the engine.  Would be nice to add some validation to the above use case to return an error.
</description><key id="31775461">5864</key><summary>Error handling on invalid mapping data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-18T01:12:33Z</created><updated>2014-05-12T16:50:18Z</updated><resolved>2014-05-12T16:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update forbidden-apis to 1.5.1 and remove the relaxed failOnMissingClasses setting, fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5863</link><project id="" key="" /><description /><key id="31765295">5863</key><summary>Update forbidden-apis to 1.5.1 and remove the relaxed failOnMissingClasses setting, fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T21:44:08Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-04-19T07:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-04-17T21:50:27Z" id="40767023">This should finally fix #5807.
</comment><comment author="s1monw" created="2014-04-18T07:30:14Z" id="40792327">PR Looks good - can you sign the CLA so I can pull it in?
</comment><comment author="s1monw" created="2014-04-19T07:01:12Z" id="40862744">pushed thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed leading spaces on commented config key lines in elasticsearch.ym...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5862</link><project id="" key="" /><description>Removed spaces on commented lines containing config key entries to prevent
users from inadvertently messing up the indents in elasticsearch.yml.
Closes #5842
</description><key id="31760224">5862</key><summary>Removed leading spaces on commented config key lines in elasticsearch.ym...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-04-17T20:39:36Z</created><updated>2014-07-11T20:33:17Z</updated><resolved>2014-04-18T16:04:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2014-04-17T20:45:29Z" id="40760577">:+1: I think this actually scans a lot better to weed out the actual settings vs comments
</comment><comment author="dakrone" created="2014-04-17T20:50:53Z" id="40761131">Nice, in particular this allows using something like `cat config/elasticsearch.yml | egrep "(^#[^# ]|^[^#][a-z].*$)"` to scan through all the set and unset configuration options, eliding the comments (useful when you just want to remember the name of a setting without having to scan the whole thing).
</comment><comment author="clintongormley" created="2014-04-18T10:22:48Z" id="40800281">@dakrone  or more legibly ;)

```
egrep  '^#?\w' config/elasticsearch.yml
```
</comment><comment author="s1monw" created="2014-04-18T10:28:42Z" id="40800535">LGTM
</comment><comment author="dakrone" created="2014-04-18T14:32:15Z" id="40812813">@clintongormley true! :D
</comment><comment author="seang-es" created="2014-04-18T16:04:01Z" id="40820106">merged to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trimmed the main `elasticsearch.yml` configuration file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5861</link><project id="" key="" /><description>This patch significantly trims the main `elasticsearch.yml` configuration file.

The current `elasticsearch.yml` file is a relic of the past, when Elasticsearch
documentation has been more dispersed and incomplete.

It has improved significantly, so the main configuration can be slimmed down,
and contain only the most important settings for development and production use.

Related: 8d0f1a7d123f579fc772e82ef6b9aae08f6d13fd
</description><key id="31754350">5861</key><summary>Trimmed the main `elasticsearch.yml` configuration file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/karmi/following{/other_user}', u'events_url': u'https://api.github.com/users/karmi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/karmi/orgs', u'url': u'https://api.github.com/users/karmi', u'gists_url': u'https://api.github.com/users/karmi/gists{/gist_id}', u'html_url': u'https://github.com/karmi', u'subscriptions_url': u'https://api.github.com/users/karmi/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4790?v=4', u'repos_url': u'https://api.github.com/users/karmi/repos', u'received_events_url': u'https://api.github.com/users/karmi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/karmi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'karmi', u'type': u'User', u'id': 4790, u'followers_url': u'https://api.github.com/users/karmi/followers'}</assignee><reporter username="">karmi</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T19:19:09Z</created><updated>2015-06-07T17:23:41Z</updated><resolved>2015-05-04T07:42:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-17T20:06:08Z" id="40756699">I still think I favor moving the "discovery" section to right underneath the cluster setting, as `minimum_master_nodes` is one of the most important settings that needs to be set.
</comment><comment author="seang-es" created="2014-04-17T20:34:25Z" id="40759431">I agree with @dakrone that Discovery, Gateway and Cluster should be next to each other, as they all relate to cluster topology and would tend to be modified together.
</comment><comment author="karmi" created="2014-04-19T15:54:10Z" id="40872893">Arguably, setting the node name is even more important (since it doesn't stick after restart otherwise), and also the data/log path (since it's quite probably in a different place then it should be otherwise).

The new file is 100 lines of code, and the assumption is that _all_ settings will be reviewed for a serious deployment. With that in mind, the "Network" section can be moved before "Various" just fine.
</comment><comment author="s1monw" created="2014-08-22T08:37:45Z" id="53036434">@karmi we discussed this today and we want to move on with what we have. I think we should push what we have (progress over perfection). Given the changes to the config related to scripting and jsonp I guess you should go over it again, can you talk a look at it again?
</comment><comment author="karmi" created="2014-08-22T11:31:03Z" id="53050085">&gt; (...) we want to move on with what we have.

Where by "what we have" we mean this trimmed configuration, or the current `master` one?

I'll assign the issue to myself, and will review the content against the new changes you mention next week!
</comment><comment author="karmi" created="2014-08-29T09:40:17Z" id="53856198">Force-pushed a commit rebased against current master.

I was thinking a bit about JSONP, dynamic scripting, etc., but I don't see them as something to add into the concise configuration this pull request proposes.
</comment><comment author="dakrone" created="2014-09-02T13:54:35Z" id="54154346">I think we should leave out the config for dynamic scripting since we have sandboxing now, enabling it should be a very advanced setting and not something people reach for at the beginning.
</comment><comment author="s1monw" created="2014-09-10T07:55:53Z" id="55082493">I am pushing this out to 1.5
</comment><comment author="clintongormley" created="2014-10-20T13:09:03Z" id="59747200">@karmi any chance you can give this a final check and get it merged?  thanks
</comment><comment author="karmi" created="2015-05-01T12:30:27Z" id="98123441">@clintongormley @s1monw So, what can I do to have this merged? I went over the current `elasticsearch.yml` and I don't see anything to add to this pruned version.
</comment><comment author="lukas-vlcek" created="2015-05-01T17:44:31Z" id="98185640">@karmi How about updating links `elasticsearch.org` -&gt; `elastic.co` ?
</comment><comment author="karmi" created="2015-05-01T18:04:54Z" id="98189337">@lukas-vlcek Fair point, will do, thanks!
</comment><comment author="karmi" created="2015-05-02T10:55:29Z" id="98347486">Rebased against master and update the URLs to point to http://elastic.co.
</comment><comment author="s1monw" created="2015-05-04T07:42:01Z" id="98611916">this looks good to me - I will pull this in now and turn the discussion into `discuss if you wanna add` not if you wanna remove. I think our config is a settings cheatsheet which we can have separately.
</comment><comment author="karmi" created="2015-05-04T13:22:45Z" id="98703339">@s1monw Thanks!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a way to include cumulative running average with histogram aggregation results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5860</link><project id="" key="" /><description>With something like this:

![screen shot 2014-04-16 at 12 31 14 pm](https://cloud.githubusercontent.com/assets/19060/2735042/64fd3598-c65b-11e3-8d9d-e2bd933f1365.png)

I can mentally draw a line representing the cumulative average through the histogram (because it follows a general pattern), but it would be nice if we could return the cumulative average of each bucket and its preceding buckets so Kibana could draw that line overlaid on the histogram.

This would be especially helpful for widely varying histograms, like:

![varying](https://cloud.githubusercontent.com/assets/19060/2735090/cdf57b00-c65b-11e3-980b-6398f303fccb.png)
</description><key id="31749260">5860</key><summary>Add a way to include cumulative running average with histogram aggregation results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>discuss</label><label>feature</label></labels><created>2014-04-17T18:13:11Z</created><updated>2014-10-17T10:47:36Z</updated><resolved>2014-10-17T10:47:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="magno32" created="2014-07-21T20:39:27Z" id="49662178">This would be awesome...
</comment><comment author="clintongormley" created="2014-10-17T10:47:36Z" id="59496537">Closed in favour of #8110
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove abstraction in the percentiles aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5859</link><project id="" key="" /><description>We initially added abstraction in the percentiles aggregation in order to be
able to plug in different percentiles estimators. However, only one of the 3
options that we looked into proved useful and I don't see us adding new
estimators in the future.

Moreover, because of this, we let the parser put unknown parameters into a hash
table in case these parameters would have meaning for a specific percentiles
estimator impl. But this makes parsing error-prone: for example a user reported
that his percentiles aggregation reported extremely high (in the order of
several millions while the maximum field value was `5`), and the reason was that
he had a typo and had written `fields` instead of `field`. As a consequence,
the percentiles aggregation used the parent value source which was a timestamp,
hence the large values. Parsing would now barf in case of an unknown parameter.
</description><key id="31747969">5859</key><summary>Remove abstraction in the percentiles aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T17:56:38Z</created><updated>2015-06-08T15:08:19Z</updated><resolved>2014-04-24T09:47:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove RootMapper.validate and validate the routing key up-front.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5858</link><project id="" key="" /><description>RootMapper.validate was only used by the routing field mapper, which makes
buggy assumptions about how fields are indexed. For example, it assumes that
the index representation of a field is the same as its external representation.

Close #5844
</description><key id="31737553">5858</key><summary>Remove RootMapper.validate and validate the routing key up-front.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-04-17T15:40:37Z</created><updated>2014-06-12T10:58:09Z</updated><resolved>2014-05-06T10:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-06T09:43:33Z" id="42283451">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added searching for multiple similar documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5857</link><project id="" key="" /><description>The documents are specified by a list of ids. The index and type where each document is
fetched from is assumed to be the first specified in the URL.

Relates #4075
</description><key id="31736442">5857</key><summary>Added searching for multiple similar documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T15:28:30Z</created><updated>2015-06-07T10:58:57Z</updated><resolved>2014-05-17T17:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-14T08:31:26Z" id="43055147">I left some comments but I think it's close... I will remove the review tag, can you put it back once you have a new iteration?
</comment><comment author="s1monw" created="2014-05-14T15:25:36Z" id="43095253">I like it - I think it's ready to go @jpountz do you wanna take another look?
</comment><comment author="jpountz" created="2014-05-15T08:18:27Z" id="43180689">I left comments/questions but I agree this looks very nice!
</comment><comment author="s1monw" created="2014-05-15T19:19:51Z" id="43253334">I left one comment other than that LGTM
</comment><comment author="s1monw" created="2014-05-16T18:38:17Z" id="43365097">I did another round sorry for being picky but there is still stuff missing...
</comment><comment author="s1monw" created="2014-05-17T10:25:16Z" id="43403359">one small comment other than that LGTM
</comment><comment author="s1monw" created="2014-05-17T12:36:05Z" id="43405723">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing docs and tests for _cat/segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5856</link><project id="" key="" /><description>We added `_cat/segments` api in #4711, but its docs are missing and we need to add some REST tests for it.
</description><key id="31734107">5856</key><summary>Missing docs and tests for _cat/segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">javanna</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T15:02:00Z</created><updated>2015-01-09T11:37:07Z</updated><resolved>2015-01-09T11:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-01-06T15:12:17Z" id="68877101">Hi, writing the Rest-Tests, I realized the _cat/segments command raises an "IndexMissingException" on an empty cluster where other commands like 'shards', 'aliases' etc... return silently. Want me to change that as well or test for the 404 error? 
</comment><comment author="javanna" created="2015-01-07T09:05:47Z" id="68995112">Hey @cbuescher good point! I'd say keep it consistent with other cat apis, it makes sense not to return an exception if the cluster has no indices. Maybe send a separate PR for this change, so we can isolate it since  we might need to mark it as breaking.
</comment><comment author="cbuescher" created="2015-01-08T10:50:27Z" id="69163916">Hi @javanna, do you think I should open a new issue for the possible change in behaviour when there are no indices or just open another pull request for that. If so, use which issue number for that? Just asking to get to know your process better.
</comment><comment author="javanna" created="2015-01-08T10:52:32Z" id="69164117">Hi @cbuescher I would just open a new PR for that specific change, we don't necessarily need an issue for it (PR is the issue in that case).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClusterBlockException on /_cat/indices/ on readonly indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5855</link><project id="" key="" /><description>_cat is cool!

We currently run ES 1.0.1.

While attempting to lower the overhead of lots of indices (still havent found a solution) I tried making indices readonly. Not sure yet if it has any positive impact, but I found this.

First lets make indices readonly:

```
curl -s -XPUT 'localhost:9200/logstash-pro-apache-2014.03.*/_settings?index.blocks.read_only=true'
```

Then execute:

```
curl -s 'localhost:9200/_cat/indices/logstash-pro-apache-2014.03*?v'
```

And the pretty result:

```
{
  "error" : "ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]",
  "status" : 403
}
```
</description><key id="31732514">5855</key><summary>ClusterBlockException on /_cat/indices/ on readonly indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">rtoma</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T14:43:44Z</created><updated>2015-04-23T13:21:57Z</updated><resolved>2015-04-23T13:21:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rtoma" created="2014-04-17T14:48:01Z" id="40722166">The _stats api call also fails:

```
http://127.0.0.1:9200/_stats?clear=true&amp;docs=true&amp;store=true&amp;indexing=true&amp;get=true&amp;search=true
```

Result:

```
HTTP Error 403: Forbidden
```

Maybe my expectation is wrong?
</comment><comment author="imotov" created="2014-04-18T18:10:51Z" id="40830597">@rtoma stats not working on read-only indices is a bug, but I don't think making indices read-only will save any resources.
</comment><comment author="imotov" created="2014-05-07T15:03:55Z" id="42438512">We discussed it in PR #5876 and decided that more work is needed. Moving it to v1.3.
</comment><comment author="clintongormley" created="2014-07-11T08:47:53Z" id="48707794">@imotov bumping this to 1.4
</comment><comment author="clintongormley" created="2014-11-13T12:37:15Z" id="62884205">Quoting @imotov from https://github.com/elasticsearch/elasticsearch/pull/5876#issuecomment-42332444

&gt; We will need to revisit this by adding more granular METADATA_READ and METADATA_WRITE blocks and implement this change across other methods such as segments, status, recovery...
</comment><comment author="clintongormley" created="2014-11-13T12:38:20Z" id="62884306">Also related to #8102
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve global ordinals on low-cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5854</link><project id="" key="" /><description>On low-cardinality fields, it is very likely that the large segments are going to contain the same set of values as the whole index. This means that the segments ordinals are already global and that the `segmentOrdToGlobalOrdLookup` is going to be an identity map.

We could detect such situations, and directly expose the segment ordinals as global ordinals in order to remove one layer of abstraction.
</description><key id="31732111">5854</key><summary>Improve global ordinals on low-cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T14:38:46Z</created><updated>2014-04-29T04:05:20Z</updated><resolved>2014-04-29T04:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-04-17T14:49:00Z" id="40722259">the other approach, what lucene does, is to detect low cardinality (with respect to number of matching docs) and just collect with segment ords, and then convert over in a second pass. Imagine 1 billion docs with only 5 unique values, this saves a lot of cpu since you arent remapping the same stuff over and over.
</comment><comment author="martijnvg" created="2014-04-17T15:05:47Z" id="40724244">@jpountz Nice idea! The caveat here is that the largest segment does need to have all values, but for low cardinality fields this should already be the case.

@rmuir That implies a feature using global ordinals (e.g. terms aggs) to change its behaviour (different execution mode). This nice thing about @jpountz trick is that it is behind the field data ordinals interface and features using it wouldn't know about, so it is a small change.
</comment><comment author="rmuir" created="2014-04-17T15:16:21Z" id="40725497">@martijnvg right, the two-pass approach requires a different execution mode, thats true, but it does not require any special alignment of ordinals, so its a more general optimization. When I applied this to apache solr last year, I think it doubled faceting performance for low cardinality fields: https://issues.apache.org/jira/browse/SOLR-5512
</comment><comment author="jpountz" created="2014-04-17T15:19:11Z" id="40725849">@rmuir When @martijnvg and I worked on global ordinals for Elasticsearch, we considered building aggregations with segment ordinals first and merging in a final step, but this introduced some complexity as well since we would need to add logic to merge sub aggregation buckets together (on every Aggregator impl).

On the other hand, global ordinals are very appealing since we can use global ordinals directly as bucket ordinals.

I think working with segment ordinals could be interesting for leaf terms aggregators though as in this case we don't need to merge sub aggregations.
</comment><comment author="rmuir" created="2014-04-17T15:27:32Z" id="40726845">When i benchmarked the change, it was as you would imagine, for high cardinality fields its slower to do two passes, too much overhead. The crazy heuristic to decide is something Mike came up with benchmarking lucene facets, I just stole it.
</comment><comment author="martijnvg" created="2014-04-18T14:35:13Z" id="40813042">&gt; I think working with segment ordinals could be interesting for leaf terms aggregators though as in this case we don't need to merge sub aggregations.

+1 to explore and add this for leaf terms aggs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default for script.disable_dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5853</link><project id="" key="" /><description>Please make setting script.disable_dynamic=true the default.
The current default: 'false' allows for nasty business via a _search api call. The docs already mention this, but does not mention the high level of nastiness. More details available. PM is probably better suited.
</description><key id="31727017">5853</key><summary>Change default for script.disable_dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">rtoma</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T13:38:05Z</created><updated>2014-05-20T13:33:52Z</updated><resolved>2014-04-25T21:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-17T13:41:27Z" id="40714814">I am +1 on this, better be safe and we can do it for the 1.2 release with proper documentation.
</comment><comment author="s1monw" created="2014-04-17T13:42:00Z" id="40714867">++
</comment><comment author="nik9000" created="2014-04-17T14:04:57Z" id="40717226">Dynamic scripts are so useful though!  It lets me upgrade my application without messing with Elasticsearch at all.  I can try out new features with no trouble.  I imagine you can do a lot with them, but can it be mitigated with better sandboxing?
</comment><comment author="kimchy" created="2014-04-17T14:07:11Z" id="40717453">@nik9000 agreed!, the path forward is to first fix the problem we have with mvel to pick a better scripting engine (by default). Once we do, then see how we can sandbox said scripting engine in an optimized manner. Once we do, then we can re-enable them by default, but thats a longer task.
</comment><comment author="byronvoorbach" created="2014-04-17T14:08:29Z" id="40717603">Note: If you disable the dynamic scripting, it will still be possible to use stored scripts.
That way, disabling it by default just requires some extra explanation for current users that use this feature.
</comment><comment author="rtoma" created="2014-04-17T14:28:36Z" id="40719836">Credits to @byronvoorbach for showing me very nasty proof of concepts.
</comment><comment author="dakrone" created="2014-04-25T15:10:30Z" id="41403380">@kimchy @s1monw how do we feel about making the `script.disable_dynamic` setting dynamic so it can be changed? I am in favor of making it dynamic as it makes dealing with this breaking change a little bit easier.
</comment><comment author="kimchy" created="2014-04-25T15:12:30Z" id="41403620">I think its safer to not allow for it to be dynamic I think, so I vote to not make it dynamic.
</comment><comment author="dakrone" created="2014-04-25T16:42:36Z" id="41413636">Hmm.. looking at this further, how do we feel about allowing dynamic `mustache` scripting by default? It looks like mustache is only templating instead of execution. Without allowing it, you can't allow template queries unless you enable dynamic scripting.
</comment><comment author="kimchy" created="2014-04-25T16:52:16Z" id="41414647">I think `mustache` should be safe, maybe we can have a flag that states for a scripting engine if its a template one or scripting?
</comment><comment author="kimchy" created="2014-05-01T15:08:59Z" id="41918429">Btw, it is documented that this should be disabled is someone is concerned about the security implications: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_disabling_dynamic_scripts
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Fail release if test have AwaitsFix annotations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5852</link><project id="" key="" /><description>Note this commit upgrades Forbidden APIs to version 1.5

Closes #5807
</description><key id="31725307">5852</key><summary>[BUILD] Fail release if test have AwaitsFix annotations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-17T13:15:17Z</created><updated>2014-07-16T21:46:18Z</updated><resolved>2014-04-17T14:48:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Prepend the type name to the index_name automatically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5851</link><project id="" key="" /><description>Fields in different types that share the same path end up being indexed into the same inverted index.  This can be a surprising gotcha for new users, who expect that types are as separate as tables in a traditional database.

One possibility, suggested in #4081, is to prepend the type name to the `index_name` (the name of the inverted index) automatically.  

This can be done manually now, by specifying a different `index_name` per field, but I'm wondering if it should become the default.  What would be the disadvantages of doing this?  I can think of these:
- field data would not be shared between types, leading to more memory usage
- you wouldn't be able to query the `foo` field across all types in an index, instead you'd have to query `*.foo`

The upsides are:
- different mappings would not clash
- different field data data types would not clash
- term frequencies would be preserved per type

I'm guessing that most of the time, fields with the same name in different types usually represent the same "thing", and so would be mapped in the same way, which leads me to think that we should leave things as they are, and allow users to use the `index_name` to configure this manually where needed.

Any thoughts?
</description><key id="31717333">5851</key><summary>Prepend the type name to the index_name automatically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-04-17T10:46:09Z</created><updated>2015-06-08T15:08:29Z</updated><resolved>2014-12-30T15:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2014-04-17T14:07:23Z" id="40717479">I actually wasn't aware this was happening too, and this change is important in order to preserve relevance quality.

In that respect, I don't think losing field-data sharing is a disadvantage. Just like it would lead to more memory usage when the 2 are split, it could lead to less memory usage if once they are split only one is being used for sorting / faceting.

Also does this mean aggregations were broken in such scenarios?
</comment><comment author="benjismith" created="2014-04-17T14:53:53Z" id="40722841">Thanks for creating this issue, @clintongormley!

I'd like to just point out the particular problems this behavior has caused us...

We're not using Elasticsearch as a canonical datastore, but rather as a search engine for data that's canonically stored in a SQL or NoSQL database, so our ES mappings are usually just mechanical translations of our other database schemas. Likewise, our indexing pipeline uses mechanical transformers to denormalize our SQL records into JSON documents for Elasticsearch.

As part of that process, our ES mappings often look something like this (with all the irrelevant fields omitted for brevity):

``` json
{
  "user" : {
    "_id" : { "type" : "string", "path" : "id" },
    "properties" : {
      "id" : { "type" : "string", "index" : "not_analyzed" },
      "address" : { "type" : "nested", "properties" : {
        "id" : { "type" : "integer" }
      }}
    }
  }
}
```

...where there are multiple different ID fields, some of which are integers or longs, and others are strings.

On the query side, we **always** refer to these with fully qualified field names (in this case, "id" and "address.id"), so it can be disorienting when our queries or facets against an "id" end up matching against an "address.id".

More importantly, though, we've experienced lots of hair-pulling trying to debug shard failures at query-time, caused by requests that sort hits by ID:

```
java.lang.NumberFormatException:
Invalid shift value (64) in prefixCoded bytes (is encoded value really an INT?)]
```

I probably spent 30 hours investigating that bug and implementing workarounds, eventually asking this question on the mailing list a few months ago at the height of my frustration:

[NumberFormatException when sorting by numeric document ID](https://groups.google.com/forum/#!msg/elasticsearch/74URnf4q69Y/)

Now that we understand the bug, we've implemented workarounds by mandating that all our **other** SQL and NoSQL schemas conform to the rule: no two fields (in any column of any table) may have the same name if they have different low-level types. That way, if we use mechanical tools to generate ES mappings from SQL schemas, the automatically-generated field names will be conflict-free. But it makes our schemas a lot more awkward than they ought to be.

This is a big big deal for us, and it's the only issue blocking us from adopting Elasticsearch as a canonical datastore.

Anyhow, thanks again for opening this issue. I really appreciate it!
</comment><comment author="benjismith" created="2014-06-05T22:24:01Z" id="45282923">@clintongormley It's been a few months since this issue was created, and I wanted to check back in to see if you or any of the other core members could chime in. This continues to be a big issue for us (and, I'd venture to say, for anyone else using Elasticsearch like a database).

Whenever I discuss Elasticsearch with other engineers, this is always their #2 concern (second only to consistency guarantees).
</comment><comment author="jpountz" created="2014-06-06T00:25:09Z" id="45291203">&gt; different mappings would not clash

I think this is a very important point. There are several bugs that we have that are due to the fact that different mappings can go to the same index field name: downgraded index options, wrong analyzer picked at search time when the type is not specified, parsing issues when loading field data, etc. So I don't think we have many choices but either storing the mappings on the index level or using different field names for each type.
</comment><comment author="clintongormley" created="2014-06-06T10:24:57Z" id="45322863">@jpountz what do you mean by:

&gt;  but either storing the mappings on the index level or using different field names for each type.

Are you saying you would be in favour of prepending the type name to the inverted index name?  Agreed that this would solve a number of issues, but what negative effects could it have.

Would (eg) aggregating on the `foo` field across multiple types be much slower than it is today, because it has to access many more inverted indices? 
</comment><comment author="synhershko" created="2014-06-06T10:27:02Z" id="45322992">@clintongormley slower maybe, but (finally) accurate
</comment><comment author="jpountz" created="2014-06-06T11:03:35Z" id="45325380">&gt; Are you saying you would be in favour of prepending the type name to the inverted index name?

Either that, or enforcing that field mappings that have the same index name are exactly the same (that's what I meant by storing mappings on the index level). I guess the first option would have negative effects around compression (more sparse postings lists and terms dictionaries), memory usage and speed while the 2nd option would remove flexibility since fields would need to have the same type, index options, doc values options, etc. across types.

Having different index names per type for search is ok I think, it's mostly a matter of building a disjunction query on the queried types. For everything that works on field data however (aggs, field data ranges, etc.) this would be more tricky since we would need to wrap several field data instances into a single one, which would be very costly since field data expects values to be unique and sorted. Or we would need to treat each type as a filtered index and query them like we query indices today, but this would mean ${num_types} queries per index as opposed to one today.

So maybe just enforcing that field mappings are the same on all types would be more practical. We already have the limitation today anyway (cf. field data loading issues with numeric fields, or positional queries if one of the types has `DOCS_ONLY` has index options). This makes the update mappings API look weird however: either you would need to refuse any mapping update as soon as a field is defined on two types so that they cannot diverge, or updating the mapping on one type would also update the mapping on other types, which feels wrong as well. Maybe the mapping create/update operations could be made index-wide (not per-type anymore) and then each type would not store mappings but only the list of fields that it contains?
</comment><comment author="clintongormley" created="2014-06-06T11:22:11Z" id="45326570">@jpountz good explanation.  I think that defaulting to a separate inverted index per field/type adds a lot of overhead, especially when there is a simple workaround: use a different field name.

&gt; So maybe just enforcing that field mappings are the same on all types would be more practical.

I'm in favour of this.  Currently it is a hidden gotcha which can surprise users. It'd be much better to let them know up front at mapping create/update time. That, at least, would be a fairly simple change.

Of course, there are some things which ARE type level, eg whether the `_all` field is enabled is per-type, not per-index. So I think that changing the mappings to be index-level instead of type-level would overcomplicate things as well. Better to leave it as it is, which works well for the majority of cases, but to let the user know when they try to do something which is not supported.
</comment><comment author="nickminutello" created="2014-10-08T10:43:45Z" id="58339895">&gt; I'm in favour of this. Currently it is a hidden gotcha which can surprise users. 
&gt; It'd be much better to let them know up front at mapping create/update time

Hm. Is that the root cause of my issue here? https://github.com/elasticsearch/elasticsearch/issues/7963
</comment><comment author="imotov" created="2014-10-10T11:43:19Z" id="58644656">@nickminutello yes, it is.
</comment><comment author="rore" created="2014-11-25T12:25:00Z" id="64392121">I want to raise our objection to this proposed solution (as I did also on thread #8614 ).

Enforcing the same mapping for fields with the same name under different types break the concept of type as a "table". This is the way types have been perceived **and used**, by us and by many others.

We, for instance, are counting on the type separation, and have **a lot** of cases of fields with different mapping under different types. This breaking change will be a huge issue for us with enormous implications. And I'm sure many others are in the same situation. 

I think you need to keep the type separation as it was always **presented by you** and understood by most of your users (just yesterday at the Elasticsearch meetup here Boaz referenced types as being "equivalent to DB tables").
</comment><comment author="jpountz" created="2014-11-25T12:58:43Z" id="64395686">@rore The problem is that this design currently doesn't work, and there are two main options: having types map their fields to a unique field name in the lucene index (eg. by prepending the name of the type), or enforcing that types have consistent mappings. The first option would make postings lists sparser (which means slower less compressed) and cross-type search slower (a single field term query on two types would need to internally become a disjunction on two fields), while the 2nd option would only require that fields that have the same name have consistent mappings. While I agree that there could be some exceptions, a field called `name` would usually be a string and a field called `age` a number? I only mentionned efficiency issues here, but handling inconsistent mappings across types correctly would also increase complexity quite significantly (again because there are decisions that need to be made at an index level).
</comment><comment author="rore" created="2014-11-25T14:11:50Z" id="64404470">@jpountz Lets take a not too unreasonable scenario. An index that holds data that is stored in another system. You have a type per table, say t1, t2 and t3. Each type ("table") has a field called externalID that denotes an id on another system. For t1, the externalID is an int. For t2, it's a string. For t3, it's a compound index so it's an object with 2 fields. 
Now what you're saying is that ES will no longer allow me to do this, all my current data is gone kaput, and if I want to hold this data I need to either change the names of my fields or prefix them with the type myself.
</comment><comment author="jpountz" created="2014-11-25T14:23:27Z" id="64406094">&gt; Now what you're saying is that ES will no longer allow me to do this, all my current data is gone kaput, and if I want to hold this data I need to either change the names of my fields or prefix them with the type myself.

We usually only enforce new rules on newly-created indices so all your current data are not going to be lost, you will have time to migrate.

For your use-case, you could either partition your data into several indices based on the system that your `externalID` field points to, or keep using a single index but make sure to use different field names depending on the system? Or if you only care about storing these data (no searching, no aggs, no sorting, only retrieval), you could just put them under an object field with `dynamic` configured to `false` so that elasticsearch does not even generate mappings for them.
</comment><comment author="rore" created="2014-11-25T14:30:32Z" id="64407109">This was only one example. We have a lot of indexes, a lot of types and a lot of places where this happens or might happen. Dealing with all the possible ways it could impact us is a complex issue. 

And again, on a general level, it breaks the concept of a type and why you should use it. I understand your considerations but I think you're choosing the path that will cause more damage than good, at least for existing users. 
</comment><comment author="clintongormley" created="2014-12-30T15:53:45Z" id="68367716">Closing in favour of #8870
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moved the updateMappingOnMaster logic into a single place.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5850</link><project id="" key="" /><description>Percolate, index and bulk now use the same logic.

Closes #5798
</description><key id="31716055">5850</key><summary>Moved the updateMappingOnMaster logic into a single place.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T10:22:48Z</created><updated>2015-06-07T14:31:15Z</updated><resolved>2014-04-18T12:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-17T13:17:49Z" id="40712487">I created this https://github.com/elasticsearch/elasticsearch/issues/5798 the other day which you should reverence in your commit I guess?
</comment><comment author="martijnvg" created="2014-04-18T04:16:09Z" id="40785642">@s1monw I addressed your feedback and updated the PR.
</comment><comment author="s1monw" created="2014-04-18T09:53:59Z" id="40798987">left one super minor nitpick :) LGTM 
</comment><comment author="martijnvg" created="2014-04-18T12:34:04Z" id="40805438">@s1monw Thanks for reviewing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter#getDocIdSet returns TermFilter object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5849</link><project id="" key="" /><description>Last week, my elasticsearch cluster were in strange trouble.

I have started the cluster at Apr 3, and no trouble has been found until Apr 7.
But suddenly, search request with same query parameter started to respond different (and broken) response every time
until restarting the cluster.

In elasticsearch log, following Exception were found for every query.

```
[2014-04-07 18:49:09,018][DEBUG][action.search.type       ] [search102] [855103] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [index_1][3]:    .....(omit)......         : Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:309)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:236)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalStateException: parentFilter must return FixedBitSet; got org.apache.lucene.queries.TermFilter$1@6f9aefe7
    at org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:190)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:156)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:130)
    at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:531)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:136)
    at org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:165)
    at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.scorer(DisjunctionMaxQuery.java:161)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:317)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:317)
    at org.apache.lucene.search.FilteredQuery$LeapFrogFilterStrategy.filteredScorer(FilteredQuery.java:573)
    at org.apache.lucene.search.FilteredQuery$QueryFirstFilterStrategy.filteredScorer(FilteredQuery.java:603)
    at org.elasticsearch.common.lucene.search.XFilteredQuery$CustomRandomAccessFilterStrategy.filteredScorer(XFilteredQuery.java:229)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:136)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:317)
    at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.scorer(DisjunctionMaxQuery.java:161)
    at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:531)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:136)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
    ... 7 more
```

The root of exception is [here](https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_0/lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java#L190).
In normaly, Filter#getDocIdSet returns DocIdSet instance. But in above case, that method returns termFilter.

Could Filter#getDocIdSet return termFilter?

For what it's worth, I found that the error caused by one of my cluster's HDD in the day before this trouble, and that HDD was broken in few days after.
I think this problem might be caused by HDD error but I'm not sure about the conclusion.
### Environment
- Hardware
  - cluster: 3 nodes
  - node: all physical server (NOT VM)
  - memory: 32GB
  - HDD: 100GB (RAID1)
  - OS: Linux
- Elasticsearch
  - version: 1.1.0
  - index: 11GB
  - number of replicas: 1
  - shards: 9
  - Java Version: OpenJDK 1.7 Update 51

Many thanks
</description><key id="31715710">5849</key><summary>Filter#getDocIdSet returns TermFilter object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bigwheel</reporter><labels><label>feedback_needed</label></labels><created>2014-04-17T10:16:04Z</created><updated>2014-12-30T16:11:16Z</updated><resolved>2014-12-30T16:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T15:53:11Z" id="68367659">Hi @bigwheel 

Sorry it has taken a while to get to this issue.   I'm assuming you haven't seen this issue again?

@jpountz any ideas what might have happened here?
</comment><comment author="jpountz" created="2014-12-30T16:10:44Z" id="68369307">I remember we had this kind of issues in the past, eg. when disabling the filter cache but this should be ok since 1.3.0, see https://github.com/elasticsearch/elasticsearch/pull/6280
</comment><comment author="jpountz" created="2014-12-30T16:11:16Z" id="68369351">I'm closing this issue, assuming this is fixed. Please reopen if you can still reproduce with a recent release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to aggregate on _index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5848</link><project id="" key="" /><description>When using the aggregations module I am unable to get aggregated doc counts per index.

The following query **DO NOT** return the aggregated result per index:

```
{
  "query": {
    "match_all": {}
  },
  "size": 0,
  "aggs": {
    "type": {
      "terms": {
        "field": "_index"
      }
    }
  }
}
```

When I use **Facets** instead of aggs, I get the desired result:

```
{
  "query": {
    "match_all": {}
  },
  "size": 0,
  "facets": {
    "type": {
      "terms": {
        "field": "_index"
      }
    }
  }
}
```

Is this a known issue?
</description><key id="31715697">5848</key><summary>Unable to aggregate on _index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">berggren</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T10:15:51Z</created><updated>2014-05-07T12:31:27Z</updated><resolved>2014-05-07T12:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-17T12:48:52Z" id="40710041">/cc @jpountz 

Looks like the `_index` field is not indexed. Probably facets has a workaround for it?
</comment><comment author="jpountz" created="2014-04-18T09:04:38Z" id="40796647">@clintongormley Facets indeed have special handling for this field.

I think we can have a cleaner fix for this, eg. by making field mappers responsible for producing field data, so that a field which is neither indexed nor doc-valued but knows what its field data looks like could return a non-empty instance. I need to think more about this...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail replica shards locally upon failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5847</link><project id="" key="" /><description>When a replication operation (index/delete/update) fails to be executed properly, we fail the replica and allow master to allocate a new copy of it. At the moment, the node hosting the primary shard is responsible of notifying the master of a failed replica. However, if the replica shard is initializing (`POST_RECOVERY` state), we have a racing condition between the failed shard message and moving the shard into the `STARTED` state. If the latter happen first, master will fail to resolve the fail shard message.

This PR builds on #5800 and fails the engine of the replica shard if a replication operation fails. This protects us against the above as the shard will reject the `STARTED` command from master. It also makes us more resilient to other racing conditions in this area.
</description><key id="31714282">5847</key><summary>Fail replica shards locally upon failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T09:51:05Z</created><updated>2015-06-07T14:27:26Z</updated><resolved>2014-04-18T16:58:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-17T13:54:03Z" id="40716066">one small comments but otherwise LGTM
</comment><comment author="bleskes" created="2014-04-18T07:35:52Z" id="40792573">I pushed another commit with the log message removed. I adapted the reason (which is logged by the shard failure) to include the information that was missing. I decided in the end not to add a debug logging as there is no logic and hardly any code between here and where we log it. If anyone feels strongly about it, I'll happily add it.
</comment><comment author="s1monw" created="2014-04-18T07:36:32Z" id="40792594">LGTM
</comment><comment author="bleskes" created="2014-04-18T17:00:56Z" id="40824667">thx @s1monw @kimchy . pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make use of global ordinals in parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5846</link><project id="" key="" /><description>By using global ordinals in p/c the execution will be much more efficient and therefor the time it takes to execute will much lower.

At the moment this PR only cuts over the `has_child` filter and query without score_mode (ChildrenConstantScoreQuery) to use global ordinals (via a `execution_hint` option, which is set to `global_ordinals` by default).

**Update:**
Some rough numbers regarding the performance improvements for p/c with global ordinals:
- For `has_child` filter and query without score mode the execution time has been reduced around upto 5 times.
- For `has_child` query with a score_mode the execution time has been reduced around upto 1.5 times.
- For `has_parent` filter and query without score mode the execution time has been reduced around upto 4 times.
- For `has_parent` query with score_mode the execution time around upto 1.2 times.

In general the transient memory needed at execution time for the mentioned queries has been reduced significantly (upto 50%). 

In the case that the `has_child` and `has_parent` queries are executed on an index with 1 segment per shard (optimized index) then these queries will be an additional ~2 times faster.
</description><key id="31713861">5846</key><summary>Make use of global ordinals in parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T09:43:32Z</created><updated>2015-06-07T14:27:38Z</updated><resolved>2014-04-29T05:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-17T14:30:38Z" id="40720078">I think this is a very good start!
</comment><comment author="martijnvg" created="2014-04-22T06:16:32Z" id="41007170">Updated the PR and all has_child/has_parent queries are now using the global ordinals. 
The execution mode that was initially added has been removed as it always make sense to use global ordinals for p/c queries.
</comment><comment author="jpountz" created="2014-04-25T00:51:14Z" id="41349480">This looks good, I just left minor comments.
</comment><comment author="martijnvg" created="2014-04-27T17:33:54Z" id="41502995">Updated the PR:
- Moved ChildrenConstantScoreQuery and ParentConstantScoreQuery to LongBitSet
- Moved ChildrenQuery and ParentQuery to LongHash
</comment><comment author="jpountz" created="2014-04-27T20:41:01Z" id="41508306">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make compression configurable (again)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5845</link><project id="" key="" /><description>I want to use ZFS with deduplication and compression as underlying filesystem for ES.
The problem is that currently with 1.1.x it is not possible anymore to disable compression on the ES application layer, which makes ZFS' block layer dedup and compression ineffective.

Would it be hard to reimplement the option to toggle compression?
</description><key id="31709222">5845</key><summary>make compression configurable (again)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels /><created>2014-04-17T08:23:20Z</created><updated>2014-09-05T11:07:37Z</updated><resolved>2014-09-05T11:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="faxm0dem" created="2014-07-30T05:58:54Z" id="50576578">Is there any news/comments/decision on this, please?
</comment><comment author="jpountz" created="2014-07-30T10:13:04Z" id="50596947">@faxm0dem Actually there was already some compression happening in earlier versions of Elasticsearch, even when compression was disabled. What changed is just that some lightweight compression has been added to stored fields and term vectors (the terms dictionary, live docs and postings lists were already compressed).

Depending on your needs, I think you should either solely rely on the compression that is performed at the Elaticsearch level (which has the advantage of knowing the data it is being applied to, efficient compression algorithms for postings lists (sorted integers) are completely different from those for the terms dictionary (sorted strings) or stored fields (random strings)) or add ZFS compression on top of it if you want to trade CPU for disk space.

Adding ZFS compression on top of Elasticsearch compression makes sense if you are using an algorithm that has different characteristics from the one that are being used by Elasticsearch (eg. gzip), but it can also make sense if you use an algorithm from the LZ family, that elasticsearch uses for stored fields/term vectors and that seems to be the default as well for ZFS. See for instance this [blog post](http://fastcompression.blogspot.fr/2012/07/log-file-compression.html) that suggests that LZ\* algorithms work very well when applied several times on highly-compressible content.
</comment><comment author="jpountz" created="2014-09-05T11:07:32Z" id="54612151">I am closing this issue because we don't want to offer too many option that would make backward compatibility hard to maintain.

However, we might still want to have two options for compression: one that would compress data more efficiently than today, and one that would be much lighter. The latter one would probably play better than today with filesystem-level compression. See https://github.com/elasticsearch/elasticsearch/issues/4160
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Path-based routing doesn't work with doc values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5844</link><project id="" key="" /><description>If you use routing on a field that has doc values, you might hit a `ClassCastException`.

The reason is that `RoutingFieldMapper` does unchecked casts to the `oal.document.Field` class although for doc values we use classes that only implement `oal.document.IndexableField`.
</description><key id="31709096">5844</key><summary>Path-based routing doesn't work with doc values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T08:20:35Z</created><updated>2015-06-07T20:51:58Z</updated><resolved>2014-05-06T09:58:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-17T14:21:23Z" id="40719020">I think the issue is a bit more general, this field mapper makes too many assumptions about the data, eg. it assumes:
- the the index field name is the same as the document field name,
- the that string value of an index field is the same as its external value.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The percolator needs to take deleted percolator documents into account.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5843</link><project id="" key="" /><description>This bug only occurs in non-realtime mode when query, filter, facet or aggs is used.

PR for #5840
</description><key id="31703191">5843</key><summary>The percolator needs to take deleted percolator documents into account.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>blocker</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T05:54:07Z</created><updated>2015-06-07T20:52:34Z</updated><resolved>2014-05-08T08:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:48:04Z" id="41177823">good catch - maybe we should add `FilteredQuery` to forbidden apis?
</comment><comment author="martijnvg" created="2014-04-23T18:05:15Z" id="41194471">I was thinking the same thing, I'll do that in a different commit.
</comment><comment author="martijnvg" created="2014-04-24T07:46:18Z" id="41252327">Updated the PR to add `FilteredQuery` to the forbidden apis.
</comment><comment author="kimchy" created="2014-05-06T09:20:57Z" id="42281846">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove spaces from commented config lines in elasticsearch.yml and logging.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5842</link><project id="" key="" /><description>If a user has a mix of indented and unindented lines in elasticsearch.yml or logging.yml, elasticsearch will not parse the YML file correctly on startup.  Any indented lines after an unindented line will be treated as children of the unindented line, and all of those lines will not parse correctly.

The sample config comments in elasticsearch.yml and logging.yml include space characters after the '#' character at the beginning of the lines.  This encourages users to uncomment them by removing the '#' character without touching the space.  This can lead to trouble if users later add other config keys without including the space.  It would be better if the space characters were removed.
</description><key id="31694755">5842</key><summary>Remove spaces from commented config lines in elasticsearch.yml and logging.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-17T01:38:36Z</created><updated>2015-06-07T14:27:59Z</updated><resolved>2014-04-18T15:55:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2014-04-17T16:50:03Z" id="40735942">I will remove all spaces after the '#' character on lines that include actual config keys.  The lines that include descriptive comments will remain as they are, since removing the comment there is not something we expect users to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include name of the field that caused a circuit break in the log and exception message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5841</link><project id="" key="" /><description>Fixes #5718

Now the log message looks like:

```
New used memory 19 [19b] from field [foo] would be larger than configured breaker: 15 [15b], breaking
```

And the exception on the client side looks like:

```
... ElasticsearchException[org.elasticsearch.common.breaker.CircuitBreakingException: Data too large, data for field [plot] would be larger than limit of [10485760/10mb]]; ...
```
</description><key id="31673064">5841</key><summary>Include name of the field that caused a circuit break in the log and exception message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-16T19:45:09Z</created><updated>2015-06-07T14:28:11Z</updated><resolved>2014-04-23T16:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:42:50Z" id="41177133">LGTM you should squash it and push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate ids in percolator results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5840</link><project id="" key="" /><description>If you index the same percolator query twice without a `refresh` in between, then it will show up twice in the percolator results.  Also, the percolator count API will return the wrong number of results (will return 2).

Here is a test case showing the issue (uses the Elasticsearch Ruby gem):
https://gist.github.com/cjbottaro/10920007
</description><key id="31669381">5840</key><summary>Duplicate ids in percolator results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">cjbottaro</reporter><labels /><created>2014-04-16T18:57:53Z</created><updated>2014-05-08T08:10:32Z</updated><resolved>2014-05-08T08:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-17T05:40:43Z" id="40682877">Thanks for reporting this bug! This bugs manifests if `filter`, `query`, `facet` or `aggs` is specified in the percolate request. 

When any of the above features is used, the persisted queries stored as documents in the shard (Lucene index) are used. Deleted documents usually stay around on disk before being being purged.  
The issue here is that percolator isn't taking deleted docs into account, which it should. This is why the old version of your percolator query with id `query1` is also reported as a match, which a duplicate.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend Snapshot/Restore support for non-AWS but S3-compatible API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5839</link><project id="" key="" /><description>Elasticsearch already has support for using S3 (within AWS) for snapshot/restore functionality. But there are lot of providers that have both cloud based and non-cloud based products that have S3 compatible API's. 

Is it possible to extend the current S3 support to work outside the context of AWS?
</description><key id="31662515">5839</key><summary>Extend Snapshot/Restore support for non-AWS but S3-compatible API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ppat</reporter><labels /><created>2014-04-16T17:34:46Z</created><updated>2014-12-30T16:22:37Z</updated><resolved>2014-12-30T16:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-16T17:39:13Z" id="40628757">It works outside of AWS. I am testing it on my laptop all the time.
</comment><comment author="ppat" created="2014-04-16T19:55:53Z" id="40644453">Hi Igor, that's good to hear. But I don't see how to configure host names, etc for local S3 compatible storage under https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository.
</comment><comment author="imotov" created="2014-04-16T20:04:03Z" id="40645322">Oh, sorry, I thought you want to snapshot into real S3 from a machine outside of AWS. I am not sure if it's possible to support S3-compatible implementation using the client that we use. I will need to take a look. 
</comment><comment author="ppat" created="2014-04-19T17:44:53Z" id="40875715">Changed issue title to avoid confusion.
</comment><comment author="imotov" created="2014-12-30T16:22:35Z" id="68370462">This is aws plugin specific issue, so moving it to https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/158
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse has_child query/filter after child type has been parsed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5838</link><project id="" key="" /><description>Fixes #5783
</description><key id="31660678">5838</key><summary>Parse has_child query/filter after child type has been parsed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.0.4</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-16T17:11:23Z</created><updated>2015-06-07T20:53:13Z</updated><resolved>2014-04-22T15:29:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-16T17:17:24Z" id="40626196">can we have a test that verifies it by sending building the search request programmatically maybe (and not using the query builders)?
</comment><comment author="dakrone" created="2014-04-16T17:49:56Z" id="40630040">&gt; can we have a test that verifies it

Whoops! Forgot to stage the file with the test, pushed with the test and builder change
</comment><comment author="s1monw" created="2014-04-16T19:38:18Z" id="40642502">while you are at it maybe we want to fix the `indices` query as well it has similar problems?
</comment><comment author="martijnvg" created="2014-04-17T05:15:18Z" id="40681725">Also the `top_children` and `has_parent` parsers have the same issue. Can these be addressed in this PR as well?
</comment><comment author="dakrone" created="2014-04-17T05:22:39Z" id="40682057">Sure, I will work on fixes for the `indices`, `top_children`, and `has_parent` parsers as well as factoring out the parser stuff like Simon recommended.
</comment><comment author="dakrone" created="2014-04-17T18:31:34Z" id="40746880">@s1monw @martijnvg pushed a new commit addressing the other query types and refactoring.
</comment><comment author="dakrone" created="2014-04-21T21:27:22Z" id="40979198">@martijnvg added some commits to this, it now will parse the query/filter in a streaming manner if they types are available when the query/filter is encountered.
</comment><comment author="martijnvg" created="2014-04-22T03:32:27Z" id="41000792">Thanks @dakrone! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations parsing is too lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5837</link><project id="" key="" /><description>Our parser currently accepts invalid aggregations definitions, this might trigger unexpected results like #5827.

Close #5827
</description><key id="31657547">5837</key><summary>Aggregations parsing is too lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-16T16:30:45Z</created><updated>2015-06-07T20:53:50Z</updated><resolved>2014-04-29T09:13:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-16T19:35:29Z" id="40642195">it looks cleaner now and good to me but I guess I'll let @uboness take a look at well! 
</comment><comment author="polyfractal" created="2014-04-23T16:13:39Z" id="41181138">Would be great if this could be expanded to include the usual parsing problems, like doubling up fields.  For example:

```
{
  "aggs" : {
    "my_agg" : {
       "terms" : {
          "field" : "a",
          "field" : "b"
       }
    }
  }
}
```

This agg will only use the `b` field when building the terms bucket.  The `a` field is silently ignored and no exception is thrown, which is very confusing for users unfamiliar with pull parsing.
</comment><comment author="jpountz" created="2014-04-24T07:41:27Z" id="41251980">I agree this should not fail silently. I think the best way to fix this issue would be to wrap the parser in order to make sure that objects don't contain duplicate keys.
</comment><comment author="s1monw" created="2014-04-24T08:02:43Z" id="41253526">@jpountz I like the idea of having a parser wrapper that ensures we have unique keys. Yet, that is a general thing and should be a different issue?
</comment><comment author="jpountz" created="2014-04-24T08:03:54Z" id="41253603">+1 on a different issue
</comment><comment author="uboness" created="2014-04-24T11:47:20Z" id="41270670">been thinking about it (dup keys that is)... not sure about it... wrapping the parser to check that in a generic manner comes with a cost (keeping track of all keys)... I'm wondering if it's something we should just keep simple and as cheap as possible at the price of leniency
</comment><comment author="uboness" created="2014-04-28T21:34:17Z" id="41616578">left a comment on missing tests... other than that, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Best-effort Highlighting", regardless of mapping and highlight parameters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5836</link><project id="" key="" /><description>ES' hit highlighting abilities are really sophisticated, but I'd like to suggest the API be made a bit more flexible.

One use case we have is the user performs their query against the `_all` field, and our UI needs to highlight matching terms in whatever specific fields the user happens to be viewing.

The fact that the `{"highlight"}` API lets you wildcard field names is pretty great, until you simply specify `"*"`.  What happens is that ES will complain that certain fields aren't necessarily compatible with the highlighting options specified.  For example, you might see:

```
[2014-04-09 16:48:07,009][DEBUG][action.search.type       ] [Miles Warren] [6838] Failed to execute fetch phase
org.elasticsearch.ElasticsearchIllegalArgumentException: the field [edit_date] should be indexed with term vector with position offsets to be used with fast vector highlighter
        at org.elasticsearch.search.highlight.FastVectorHighlighter.highlight(FastVectorHighlighter.java:68)
        at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)
        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

("edit_date" is defined as a date field and of course has no "index_options" on it).

And if you hack a bit at the ES sources to silently ignore these errors you'll see it also complains about ES' core fields such as `_id`, `_timestamp`, `_source`, etc.

What I'd like to propose is an option in the `{ "highlight" }` API to be lenient about mismatches between field mapping definitions and highlight parameters.  So if the highlight parameters aren't compatible with the mapping, ES will just move on to the next field, rather than bomb the entire request.  

Call it a best-efforts highlighting for "all" fields.

Maybe it could fall back to decreasingly less sophisticated highlighters, until it just can't do anything.  So if the request indicates the `fvh` be used, and that's not compatible with a particular field, it tries the `postings` highlighter, then the `plain` highlighter, then nothing.

Additionally, I'd like to see a highlight parameter to exclude fields.  Another use case we have is, "I want to highlight all matched fields (whatever they are), except this array of 'large text' fields that I'm not currently showing the user" -- this would really be a bit of a performance optimization, from the client.  Our data model includes a couple of large text fields (10k+) per document that wouldn't need to be highlighted except at very specific times -- the other fields are small and generally need to be highlighted in one call to ES.

Thoughts?

If y'all think this is a good idea, I'm happy to fork, do the work, make a pull request, and work with the committer(s) until it's accepted.  I basically need to do this anyways, so I'd rather have buy-in instead of maintaining my own patched version of ES.

I thought about doing this as a plugin, but it looked like I'd have to duplicate quite a bit of core ES classes, which doesn't seem maintainable in the long run.
</description><key id="31650849">5836</key><summary>"Best-effort Highlighting", regardless of mapping and highlight parameters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eeeebbbbrrrr</reporter><labels><label>:Highlighting</label></labels><created>2014-04-16T15:17:51Z</created><updated>2016-11-10T19:18:42Z</updated><resolved>2016-11-10T19:18:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-16T15:56:22Z" id="40616735">&gt; Maybe it could fall back to decreasingly less sophisticated highlighters, until it just can't do anything. So if the request indicates the fvh be used, and that's not compatible with a particular field, it tries the postings highlighter, then the plain highlighter, then nothing.

The highlighters don't really have an order of sophistication.  You can order them by creation date (`plain`, `fvh`, `postings`) but that is about it.  Each one targets a specific set of stored data (nothing, term vectors, postings respectively).  `postings` is the only highlighter that supports sentence based fragmentation and it only supports it.  Ditto for some of the other assumptions it makes about scoring the fragments (BM25, ignore weights in the query).  With that in mind I don't know how this kind of thing is going to work.

This is part of the reason I stared work on https://github.com/wikimedia/search-highlighter .  It isn't a silver bullet for your concerns, but might help.

&gt; Additionally, I'd like to see a highlight parameter to exclude fields. Another use case we have is, "I want to highlight all matched fields (whatever they are), except this array of 'large text' fields that I'm not currently showing the user" -- this would really be a bit of a performance optimization, from the client. Our data model includes a couple of large text fields (10k+) per document that wouldn't need to be highlighted except at very specific times -- the other fields are small and generally need to be highlighted in one call to ES.

Adding an exclusion pattern on top of the inclusion pattern makes sense to me.
</comment><comment author="eeeebbbbrrrr" created="2014-04-16T16:10:31Z" id="40618552">Fair point on the "order of sophistication" bit.  Maybe instead it's a user provided list, in the order the user would like ES to try?  

I'd also be okay with it just silently continuing to the next field if the current field's mapping isn't compatible with the specified highlighter.  That would be a BIG win for us.

Maybe it's not obvious, but right now the highlighter API requires the process requesting highlights to have a fairly detailed understanding of the data model, otherwise the request fails with an exception.  This isn't really fair in a distributed environment, so I'm looking for ways to make ES more tolerant of ignorance.

Whatdoyathinkaboutthat?

We request the entire value back (`"number_of_fragments": 0`), and so we're most interested in either getting back what `fvh` can give us (since it highlights phrases), or just nothing at all.
</comment><comment author="eeeebbbbrrrr" created="2014-04-16T21:05:40Z" id="40652194">It was pretty easy to knock out a "lenient" property to convince ES to silently skip highlighting fields with incompatible highlighters.  So I did that in bc795448dd910d2ba2442ccdcd5c025926a2f28c.

Next up will be an "excludes" array so that you can do:

``` json
{
   "highlight": {
       "*": {
            "type": "fvh",
            "lenient": true,
            "excludes": [ "bigfield1", "bigfield2", "..." ]
       }
  }
}
```

Thoughts?
</comment><comment author="synhershko" created="2014-04-17T14:02:32Z" id="40716970">&gt; Maybe it's not obvious, but right now the highlighter API requires the process requesting
&gt; highlights to have a fairly detailed understanding of the data model, otherwise the request
&gt; fails with an exception. This isn't really fair in a distributed environment, so I'm looking for ways
&gt; to make ES more tolerant of ignorance.

This is very obvious, and I'd say pretty much by design. Yes ES tries to simplify things by auto-detecting mappings etc but as your system grows you'd see you really have to specify mappings on your own.

Knowing the data model and carefully designing it and working intimately with it is pretty much a prerequisite, and not only with ES. And it's perfectly fair for a data-storage solution (or a search server, however you'd like to call it).

I'm +1 for the `lenient` setting and the whitelist / blacklist approach for highlighting, although in my experience it is ususally just easier to be explicit about what you are trying to do. Going forward with maintaining a blacklist (aka `excludes`) is a show killer for systems that have an evolving data model (e.g. new fields being introduced)

My 2 cents.
</comment><comment author="ppf2" created="2015-01-28T17:51:02Z" id="71881209">Having an exclude mode will also be helpful for use cases like this one (https://github.com/elasticsearch/kibana/issues/2782).
</comment><comment author="clintongormley" created="2016-11-10T19:18:42Z" id="259781392">Nothing further in 2.5 years, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a site plugin into list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5835</link><project id="" key="" /><description>Howdy,

I'm not sure if this is kosher but I would like to add my site plugin to the list in the ES docs.
</description><key id="31648162">5835</key><summary>Add a site plugin into list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xyu</reporter><labels /><created>2014-04-16T14:49:35Z</created><updated>2014-06-18T18:36:38Z</updated><resolved>2014-04-17T17:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-17T12:31:09Z" id="40708636">Hi @xyu 

perfectly kosher. please could you sign the CLA so that I can merge this in? http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="xyu" created="2014-04-17T14:39:06Z" id="40721078">@clintongormley Done. :)
</comment><comment author="clintongormley" created="2014-04-17T17:29:09Z" id="40740030">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize field data impls and global ordinals via random index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5834</link><project id="" key="" /><description>we randomize these settings in dedicated tests but not via the random index template - we should do it for FD impls and eager and lazy global ords
</description><key id="31642499">5834</key><summary>Randomize field data impls and global ordinals via random index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>low hanging fruit</label><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-16T13:44:47Z</created><updated>2014-05-06T13:27:30Z</updated><resolved>2014-05-06T09:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-16T16:17:24Z" id="40619414">+1!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize MergePolicy settings in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5833</link><project id="" key="" /><description>this is a very low hanging fruit since we have most of this already we just need to take the time to get the randomization properties for each MP and randomize it.
</description><key id="31641823">5833</key><summary>Randomize MergePolicy settings in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-16T13:36:21Z</created><updated>2015-04-19T11:31:29Z</updated><resolved>2015-04-19T11:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-19T11:31:29Z" id="94267815">I have done this mostly in other issues, closing this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow unittests to run against an external cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5832</link><project id="" key="" /><description>Today we run our unittests against an internal cluster but we already have support for `ImmutableCluster` that is used by tests that done't need to modify the cluster. Yet, those tests can easily run against an external cluster. as a first simple step we can just use some Junit magic to make this work ie. we add support for an annotation like `@NeedsJVMLocalCluster` or `@NeedsMutableCluster` and given the cluster we use to run the tests we can just exclude those tests. 
A pure external cluster can then run on all tests that don't have either of these annotations. In a second step we can add a `MutableExternalCluster` impl that can start external processes and tear them down as needed given the test is not annotated with `@NeedsJVMLocalCluster`
</description><key id="31641747">5832</key><summary>Allow unittests to run against an external cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2014-04-16T13:35:13Z</created><updated>2015-10-15T12:05:35Z</updated><resolved>2015-10-15T12:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T16:07:52Z" id="148101605">@s1monw you still want to do this?
</comment><comment author="rjernst" created="2015-10-14T18:57:16Z" id="148156343">This already works? You can pass -Dtests.cluster=x.x.x.x (this is how REST tests are run for release). It works for any integration tests (ie tests that require a cluster).
</comment><comment author="clintongormley" created="2015-10-15T12:05:35Z" id="148367160">great, thanks @rjernst.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize the order of the fields in XContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5831</link><project id="" key="" /><description>Due to the way we parse `XContent` in query parsers and friends it's easy to make those order dependent. We just recently had an issue related to this #/5783 - We can maybe randomize the order by injecting a randomized XContentGenerator. if we parse XContent the order of the fields should not matter.
</description><key id="31641299">5831</key><summary>Randomize the order of the fields in XContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2014-04-16T13:29:19Z</created><updated>2016-05-06T08:56:39Z</updated><resolved>2016-05-06T08:56:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-05-02T15:38:00Z" id="216270425">Random shuffling of xContent is now added in the basic roundtrip parsing tests (`testFromXContent()`) for queries, shapes, ingest metadata, aggregations, pipeline aggs, highlight builder, sort builders, rescore builders and suggestions. 
It might be useful in other cases where xContent is generated and then parsed, I'll scan the tests for possible spots but will close this issue if nothing particular comes to mind.
</comment><comment author="cbuescher" created="2016-05-06T08:56:38Z" id="217389106">I have also added random shuffling for InnerHitBuilderTest and some other smaller ones (#18093), closing this now. Please reopen if you think there is other places that need this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize codecs per field </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5830</link><project id="" key="" /><description>today we don't randomize the codecs when we run the tests. We should randomize them using our random index template.
</description><key id="31641006">5830</key><summary>Randomize codecs per field </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>won't fix</label></labels><created>2014-04-16T13:25:50Z</created><updated>2015-04-19T17:57:34Z</updated><resolved>2015-04-19T17:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-19T11:29:35Z" id="94267752">we removed this feature really so I am closing this one.
</comment><comment author="rmuir" created="2015-04-19T16:36:40Z" id="94294879">I have a simple solution for this, can be solved as part of #10656, or after it, that adds an internal "lucene_default" to CodecService. This just uses Codec.getDefault() for the raw unfiltered lucene default codec, which is randomized by LuceneTestCase.

Some tests rely on the custom postings format in ES (the completion suggester tests and rest tests), and a few other tests wire the lucene default codec, but these can all be simplified to just use `SuppressCodecs("*")`. 

Otherwise, i think let LuceneTestCase do it, it will only find bugs because of things like AssertingCodec that we have.
</comment><comment author="rmuir" created="2015-04-19T17:57:33Z" id="94301453">I hooked this in here: https://github.com/elastic/elasticsearch/commit/b09d236fc0bf2763dd69140e4b7f5fa0cc22a48b

For now we just use AssertingCodec: it found several bugs immediately: #10660, #10661. I had to mark 3 tests with AwaitsFix for those issues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Track min/max on numerics in field data per segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5829</link><project id="" key="" /><description>If we track for numeric values the min/max values in field data, we can potentially use it in several places to optimize execution.

For example, in range filter, _if_ the field data for a field is loaded, it can be used to check if the term / range filter needs to be executed at all, or it can work as a match all. Potentially, also adding improvements to boolean filter to have a special case for match all.

Another option to use this is in aggs, where this can be used to do bucket estimations.
</description><key id="31638689">5829</key><summary>Track min/max on numerics in field data per segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Search</label><label>enhancement</label><label>stalled</label></labels><created>2014-04-16T12:53:36Z</created><updated>2016-08-24T15:15:17Z</updated><resolved>2016-08-24T15:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattrco" created="2014-05-09T20:15:19Z" id="42708673">If no one else is currently working on this, I'd like to attempt it as a first contribution.

Is there a particular milestone this would be useful for? Thanks.
</comment><comment author="mikemccand" created="2014-05-09T20:17:53Z" id="42708955">Note that we added this to Lucene, in https://issues.apache.org/jira/browse/LUCENE-5610 which will be available when ES upgrades to Lucene 4.9.  So in ES we just need to call the methods in NumericUtils and then act accordingly...
</comment><comment author="mattrco" created="2014-05-10T09:02:30Z" id="42736669">Thanks. I'll keep an eye on this for when the 4.9 upgrade is happening.
</comment><comment author="jpountz" created="2014-08-01T08:56:28Z" id="50863213">There seems to be activity related to this issue at https://issues.apache.org/jira/browse/LUCENE-5860
</comment><comment author="adadevoh" created="2015-04-26T23:35:16Z" id="96448300">Hi, I just wanted to ask, what was the fix for this?
</comment><comment author="mikemccand" created="2015-05-01T09:40:21Z" id="98085925">#10523 already exposed the min/max APIs added in LUCENE-5860, on an index level, but for this issue nothing has been done to e.g. optimize range filters based on the min/max of a segment, because it's currently too costly for Lucene's postings APIs to compute the max numeric value: it requires a binary search over the terms because of how the numeric prefix terms are encoded.

Once we cutover to auto-prefix encoding for numeric terms, this becomes much cheaper and I think optimizations like this become more realistic.

I think higher level optimizations could be very worthwhile, e.g. for time-based indices, knowing that a given index won't have any hits because there is a top-level range filter, should be a big speed up in many cases ... there is a separate issue to explore this but I can't find it right now.
</comment><comment author="jpountz" created="2016-08-24T15:15:17Z" id="242100900">The discussed optimizations have been implemented in 5.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index corruption through delete_by_query of child documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5828</link><project id="" key="" /><description>we experienced index corruption (indices having UNASSIGNED shards)
during incremental indexing.

The index contains one main document type and three other document
types for child documents.

The corruption is related to delete by query requests for the child
documents.
We had cases where the index was recovered on ES restart and cases 
where ES could not fix the index any more. This might be related to
the number of replicas (in cases where recovery worked we only had
one index copy on one instance) but we did not do an exhaustive 
analysis on that.

After we changed the delete by query requests into a client side
delete by query (search the child documents, send bulk requests for
delete by document id then) the issues stopped.

Elastic search version is 1.0.2, client operates through the ruby
bindings using http (should not matter). OS is linux, the index
had 6 shards, ~ 12 mio documents (1.7 mio "main" documents,
the rest is child documents of three different types) in ~ 7.3GB.
The server has 16 GB memory, ES runs with 8 GB heap memory and 
65535 file descriptors.

Sorry, we could not try ES 1.1 because of an error regarding empty
geo points (seems to be fixed in master but not released).

Part of the problem might be, that we are not too strict to ensure
referential integrity between parent and child documents.
My - perhaps naive - expectation was, that this should not matter. 
So there might be child documents naming a parent, where the
parent document does not exist.

When we ran the incremental updates on a partial index containing
only a handful of documents (while the incremental stream was on
changes in all data) the issue did not show up.
So it's either related to the index size or (more probably (?)) to 
the question if the delete by query finds something to delete or not.

We only had one process working on the updates strictly sequential.
So it should not be a race condition between serveral changes the same
time. There were parallel updates to other indices though.

The error itself is not too enlightening (at least for a pure
es user): the indexer dies from a http timeout in an indexing request.

The ES log shows an error stating
failed to merge
and
org.apache.lucene.store.AlreadyClosedException: this IndexReader is closed
see full stack trace below.
Raising ES log level to debug did not provide additional information,
why the index reader was closed.

I'm afraid I cannot provide a full sample, how to reproduce the
problem.

best
  Morus

PS:
The initial error messages look like
[WARN ][index.merge.scheduler    ] [pjpp-production mas
ter] [candidates_v0004][5] failed to merge
org.apache.lucene.store.AlreadyClosedException: this IndexReader is closed
        at org.apache.lucene.index.IndexReader.ensureOpen(IndexReader.java:252)
        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.ja
va:102)
        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.ja
va:56)
        at org.apache.lucene.index.IndexReader.leaves(IndexReader.java:502)
        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.cont
ains(DeleteByQueryWrappingFilter.java:122)
        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.getD
ocIdSet(DeleteByQueryWrappingFilter.java:81)
        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDoc
IdSet(ApplyAcceptedDocsFilter.java:45)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(Con
stantScoreQuery.java:142)
        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.fil
teredScorer(FilteredQuery.java:533)
        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:13
3)
        at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFi
lter.java:59)
        at org.apache.lucene.index.BufferedUpdatesStream.applyQueryDeletes(Buffe
redUpdatesStream.java:546)
        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(
BufferedUpdatesStream.java:284)
        at org.apache.lucene.index.IndexWriter._mergeInit(IndexWriter.java:3844)
        at org.apache.lucene.index.IndexWriter.mergeInit(IndexWriter.java:3806)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3659)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMe
rgeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(Trac
kingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(Conc
urrentMergeScheduler.java:482)
[WARN ][index.engine.internal    ] [pjpp-production mas
ter] [candidates_v0004][5] failed engine
org.apache.lucene.index.MergePolicy$MergeException: org.apache.lucene.store.Alre
adyClosedException: this IndexReader is closed
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvi
der$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler
Provider.java:109)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(Conc
urrentMergeScheduler.java:518)
Caused by: org.apache.lucene.store.AlreadyClosedException: this IndexReader is closed
        at org.apache.lucene.index.IndexReader.ensureOpen(IndexReader.java:252)
        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.java:102)
        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.java:56)
        at org.apache.lucene.index.IndexReader.leaves(IndexReader.java:502)
        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.contains(DeleteByQueryWrappingFilter.java:122)
        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.getDocIdSet(DeleteByQueryWrappingFilter.java:81)
        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:142)
        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:533)
        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)
        at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
        at org.apache.lucene.index.BufferedUpdatesStream.applyQueryDeletes(BufferedUpdatesStream.java:546)
        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(BufferedUpdatesStream.java:284)
        at org.apache.lucene.index.IndexWriter._mergeInit(IndexWriter.java:3844)
        at org.apache.lucene.index.IndexWriter.mergeInit(IndexWriter.java:3806)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3659)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
</description><key id="31634717">5828</key><summary>index corruption through delete_by_query of child documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">morus</reporter><labels /><created>2014-04-16T11:50:35Z</created><updated>2014-08-26T21:50:20Z</updated><resolved>2014-04-28T13:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-16T14:15:07Z" id="40603493">This is indeed an issue and I can see how this can fail shards. Thanks for reporting this bug! 
</comment><comment author="martijnvg" created="2014-04-17T09:47:49Z" id="40698029">I don't see how this bug can be fixed easily and for now I recommend not to use any parent/child query or filter (has_child, has_parent, top_children) in the delete by query api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cardinality not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5827</link><project id="" key="" /><description>I have a query like this.

``` ruby
  aggregations: {
                      by_month: {
                          date_histogram: {
                              field:    "time_stamp",
                              interval: "1M",
                              format:   "yyyy-MM-dd HH:mm"
                          },
                          aggregations:   {
                              by_node_mac: {
                                  terms:        {
                                      field: "node_mac"
                                  },
                                  aggregations: {
                                      cardinality: {field: 'device_mac'}
                                  }

                              }
                          }
                      }
                  }
}

```

but I seem to be getting the wrong answers.  I am using fake data which should give me very low numbers for the cardinality but it actually seems to be counting the number of rows not the number of distinct items. The numbers are outrageously high.

I tried a precision threshold of 1000 and 100 but it seems to make no difference.
</description><key id="31634654">5827</key><summary>Cardinality not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">timuckun</reporter><labels /><created>2014-04-16T11:49:31Z</created><updated>2014-04-19T03:19:03Z</updated><resolved>2014-04-19T03:19:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-16T13:40:48Z" id="40599486">This is interesting: your sub-aggregation is missing a name and I'm wondering that this might make the parser go crazy and see the cardinality aggregation but ignore its configuration (the field parameter). When you don't define the field to use in a cardinality aggregation, it uses the field of the parent aggregation, which is a timestamp in your case so likely has a very high cardinality.

Could you verify this is the case by running the following aggregation (the same as yours, but with a name given to the cardinality aggregation).

``` yaml
  aggregations: {
                      by_month: {
                          date_histogram: {
                              field:    "time_stamp",
                              interval: "1M",
                              format:   "yyyy-MM-dd HH:mm"
                          },
                          aggregations:   {
                              by_node_mac: {
                                  terms:        {
                                      field: "node_mac"
                                  },
                                  aggregations: {
                                      device_mac_count: {
                                          cardinality: {field: 'device_mac'}
                                      }
                                  }
                              }
                          }
                      }
                  }
}
```
</comment><comment author="timuckun" created="2014-04-17T05:35:12Z" id="40682601">Ok I input the same data into postgres and ran some comparisons.

The numbers are closer when I name the bottom level aggregation but not quite the same.  What's odd is that it's not really consistent. Some values are pretty close to pg others are twice as much.  

On another note the date format isn't working this is the output I am getting

``` bash
{"took"=&gt;11135,
 "timed_out"=&gt;false,
 "_shards"=&gt;{"total"=&gt;15, "successful"=&gt;15, "failed"=&gt;0},
 "hits"=&gt;{"total"=&gt;50100313, "max_score"=&gt;0.0, "hits"=&gt;[]},
 "aggregations"=&gt;
  {"by_month"=&gt;
    {"buckets"=&gt;
      [{"key"=&gt;0,
        "doc_count"=&gt;17500,
        "by_node_mac"=&gt;
         {"buckets"=&gt;
           [{"key"=&gt;"00606487F197\n",
             "doc_count"=&gt;13306,
             "device_mac_count"=&gt;{"value"=&gt;3}},
            {"key"=&gt;"00606488148F\n",
             "doc_count"=&gt;3359,
             "device_mac_count"=&gt;{"value"=&gt;58}},
            {"key"=&gt;"006064881467\n",
             "doc_count"=&gt;835,
             "device_mac_count"=&gt;{"value"=&gt;2}}]}},
       {"key"=&gt;1391212800000,
        "doc_count"=&gt;50082500,
        "by_node_mac"=&gt;
         {"buckets"=&gt;
           [{"key"=&gt;"00606487EED7\n",
             "doc_count"=&gt;11195953,
             "device_mac_count"=&gt;{"value"=&gt;1227}},
            {"key"=&gt;"00606488139F\n",
             "doc_count"=&gt;6862796,
             "device_mac_count"=&gt;{"value"=&gt;24867}},
            {"key"=&gt;"006064880FA7\n",
             "doc_count"=&gt;4714424,
             "device_mac_count"=&gt;{"value"=&gt;18571}},
            {"key"=&gt;"00606487F137\n",
             "doc_count"=&gt;4566781,
             "device_mac_count"=&gt;{"value"=&gt;6914}},
            {"key"=&gt;"00606487F317\n",
             "doc_count"=&gt;2988669,
             "device_mac_count"=&gt;{"value"=&gt;4397}},
            {"key"=&gt;"00606487EE47\n",
             "doc_count"=&gt;1788288,
             "device_mac_count"=&gt;{"value"=&gt;4226}},
            {"key"=&gt;"00606487F58F\n",
             "doc_count"=&gt;1671451,
             "device_mac_count"=&gt;{"value"=&gt;1728}},
            {"key"=&gt;"006064881467\n",
             "doc_count"=&gt;1657192,
             "device_mac_count"=&gt;{"value"=&gt;445}},
            {"key"=&gt;"00606487ED8F\n",
             "doc_count"=&gt;1609941,
             "device_mac_count"=&gt;{"value"=&gt;2633}},
            {"key"=&gt;"006064880E5F\n",
             "doc_count"=&gt;1532838,
             "device_mac_count"=&gt;{"value"=&gt;14916}}]}}]}}}
```
</comment><comment author="jpountz" created="2014-04-17T08:46:58Z" id="40693457">I am not surprised that there are slight differences since the cardinality aggregation returns approximate results (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html for more information). However, these approximations are supposed to be quite good so I'm worried about the x2 difference (it is supposed to be in the order of a few percents).

The output of this aggregation is indeed very surprising. I don't even know how this output has been formatted (could it be milliseconds since Epoch in hexadecimal?).

Can you share your mappings (and potentially your index)?
</comment><comment author="timuckun" created="2014-04-18T09:45:59Z" id="40798614">I can't share you the index as it's over five gigs in size. Here is the mapping though

``` ruby
{
        probe: {
            properties: {

                time_stamp:      {type: 'date'},
                node_mac:        {type: 'string', index: 'not_analyzed'},
                device_mac:      {type:   'string', index:  'not_analyzed' },
                signal_strength: {type: 'float'},
            }
        }
    }
```

On another note...

I reloaded the data into postgres and ran the same group by query. The query took fifty minutes on my laptop . I  re adjusted the threshold to be 100 and re-ran the ES query which took seven seconds.  Here is the results. The third column is the count reported by postgres, the fourth column is the count reported by ES

Note that for the values that did come back they are in the same ballpark but lots of data didn't come in at all.  I am very curious about that.

``` csv
"1970-01-01 00:00:00",  "00606487F197", 4,     4
"1970-01-01 00:00:00",  "006064881467", 2,     2
"1970-01-01 00:00:00",  "00606488148F", 61,    61
"2014-02-01 00:00:00",  "00606487ED8F", 1914,   1955
"2014-02-01 00:00:00",  "00606487ED9F", 235 ,
"2014-02-01 00:00:00",  "00606487EE47", 3806,   3819
"2014-02-01 00:00:00",  "00606487EECF", 11  
"2014-02-01 00:00:00",  "00606487EED7", 1720,   1727
"2014-02-01 00:00:00",  "00606487EEEF", 381  
"2014-02-01 00:00:00",  "00606487F137", 6149,   6176
"2014-02-01 00:00:00",  "00606487F197", 105  
"2014-02-01 00:00:00",  "00606487F20F", 4032  
"2014-02-01 00:00:00",  "00606487F2A7", 1339  
"2014-02-01 00:00:00",  "00606487F317", 3997,   3937
"2014-02-01 00:00:00",  "00606487F51F", 143  
"2014-02-01 00:00:00",  "00606487F58F", 1665,   1607
"2014-02-01 00:00:00",  "006064880E5F", 7234,   7286
"2014-02-01 00:00:00",  "006064880E67", 6956  
"2014-02-01 00:00:00",  "006064880E87", 383  
"2014-02-01 00:00:00",  "006064880FA7", 18807,  18778
"2014-02-01 00:00:00",  "0060648811FF", 194  
"2014-02-01 00:00:00",  "006064881237", 61  
"2014-02-01 00:00:00",  "00606488139F", 23475,  24232
"2014-02-01 00:00:00",  "006064881467", 440,    447
"2014-02-01 00:00:00",  "00606488148F", 1940 
"2014-02-01 00:00:00",  "0060648814BF", 146  
"2014-02-01 00:00:00",  "006064881527", 668  
"2014-02-01 00:00:00",  "undefined",    80  
```
</comment><comment author="jpountz" created="2014-04-18T14:37:13Z" id="40813188">Oh, sorry, I was so worried about a bug in the cardinality aggregation that I missed the obvious: there is a limitation with the terms aggregation that it computes the top terms per shard, and only merges these top terms in the reduce phase. As a consequence, if might introduce precision loss since a term might be in the top terms on one shard but not on another (the mac addresses on this shard will be completely ignored on this shard).

Can you try to run the aggregation again and set [size=0](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_size_amp_shard_size) on the terms aggregation (or just a very high value if you are not on 1.1 yet). This will force Elasticsearch to consider all buckets.
</comment><comment author="timuckun" created="2014-04-19T03:19:03Z" id="40859411">Thanks that worked in terms of getting all the results.  I fiddled with the threshold a bit but I could not get the counts to match exactly.  They were pretty close in most cases but I am not sure if that's good enough for this project.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Understanding date format issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5826</link><project id="" key="" /><description>Hi,

I have a river with mongo which fetches events from the remote mongodb.
Now i have everything up an running but the only  issue is with the "time format" which is still in UNIX epoch for:

1.event_time
2.timestamp
3.app_event_time

here is the mapping.

curl - XGET http://ESserver.com:9200/secana/messages/_mapping?pretty=true

{
  "secana" : {
    "mappings" : {
      "messages" : {
        "properties" : {
          "app_event_time" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "event_time" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "firstName" : {
            "type" : "string"
          },
          "frecency" : {
            "type" : "long"
          },
          "hostname_id" : {
            "type" : "string"
          },
          "lastName" : {
            "type" : "string"
          },
          "message" : {
            "type" : "string"
          },
          "netloc" : {
            "type" : "string"
          },
          "signum_id" : {
            "type" : "string"
          },
          "source" : {
            "type" : "string"
          },
          "source2" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "title" : {
            "type" : "string"
          },
          "url" : {
            "type" : "string"
          },
          "visit_count" : {
            "type" : "long"
          }
        }
      }
    }
  }
}

Can someone please let me know what is the issue?
</description><key id="31634283">5826</key><summary>Understanding date format issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karnamonkster</reporter><labels /><created>2014-04-16T11:43:15Z</created><updated>2014-05-15T09:13:34Z</updated><resolved>2014-04-17T12:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-17T12:11:55Z" id="40707296">@karnamonkster please ask these questions in the mailing list instead, and when you do, you'll need to provide more information about the problem you're having.
</comment><comment author="karnamonkster" created="2014-05-15T06:11:33Z" id="43171933">unfortunately nobody responded to my issue even posted on the mailing list. Can i get any help on the same.
</comment><comment author="clintongormley" created="2014-05-15T09:13:34Z" id="43186032">Probably because you haven't provided enough information about the problem.  If you don't get an answer on the forum, repost the question providing more information.  But the issues list is not the place for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Closing an IndexReader on an already relocated / closed shard can cause memory leaks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5825</link><project id="" key="" /><description> [LUCENE-5553](https://issues.apache.org/jira/browse/LUCENE-5553) can prevent `ReaderClosedListener` from being called if the indexreader is closed and the shard was already closed or relocated. This has been fixed in Lucene 4.7.1 and will be part of the next bugfix release for the `1.1.x` series. We can't backport this to the `1.0.x` series since this branch is running on `LUCENE_46`

The problem with this bug is that we never release the fielddata or filters that are associated with the index reader that runs into this bug. If the index is moved away from a node the memory is free again as well as during shard cleanups so in production this might not be a huge issue. Yet, there are node level services like the circuit-breaker that relies on the `ReaderClosedListener` to be invoked to adjust memory which can report wrong memory consumptions for the rest of it's lifetime. 
</description><key id="31626471">5825</key><summary>Closing an IndexReader on an already relocated / closed shard can cause memory leaks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-16T09:35:11Z</created><updated>2015-06-07T20:54:23Z</updated><resolved>2014-04-16T09:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-16T09:35:49Z" id="40580434">fixed by 42b20d601fe2bcece29332a116031616b90d1323
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Out of memory Error due to shard failure </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5824</link><project id="" key="" /><description>it clearly shows out of memory exception but how to handle this gracefully.... shard failure can happen any time will adding shard_size will solve the  problem???? I am using size parameter only...
</description><key id="31613894">5824</key><summary>Out of memory Error due to shard failure </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">desaxena</reporter><labels /><created>2014-04-16T05:40:48Z</created><updated>2014-04-17T12:08:45Z</updated><resolved>2014-04-17T12:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="desaxena" created="2014-04-16T05:41:34Z" id="40564991">org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] ; shardFailures {[whK0hSszSxqPMfRBHGHj9g][bw-2014-04-15-03][2]: RemoteTransportException[Failed to deserialize response of type [org.elasticsearch.search.fetch.FetchSearchResult]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.search.fetch.FetchSearchResult]]; nested: OutOfMemoryError[GC overhead limit exceeded]; }
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:182) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.onFetchFailure(TransportSearchQueryThenFetchAction.java:174) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onFailure(TransportSearchQueryThenFetchAction.java:162) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction$10.handleException(SearchServiceTransportAction.java:426) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.9.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.9.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.7.0_07]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.7.0_07]
    at java.lang.Thread.run(Unknown Source) [na:1.7.0_07]
java.lang.OutOfMemoryError: GC overhead limit exceeded
</comment><comment author="kimchy" created="2014-04-16T08:35:34Z" id="40575492">@desaxena can you please update the title of this issue? Also, I think the best place to ask questions is the mailing list, and if there is an actual issue, we can open a relevant one here.
</comment><comment author="desaxena" created="2014-04-16T12:05:53Z" id="40590971">i ll put there can u please help me over there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Conflict using G1GC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5823</link><project id="" key="" /><description>I wanted to try the G1GC collector. 
However when i start the server it gives me an error
bin/elasticsearch -XX:+UseG1GC
Conflicting collector combinations in option list; please refer to the release notes for the combinations allowed
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

I can't understand what may be the issue. This is a fresh download of 1.1.0. Also tried on earlier versions, same problem. Do i have to set something special?

Thanks
</description><key id="31591006">5823</key><summary>Conflict using G1GC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rotastrain</reporter><labels /><created>2014-04-15T21:02:59Z</created><updated>2016-02-14T18:45:09Z</updated><resolved>2016-02-14T18:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-15T21:45:12Z" id="40538413">in `elasticsearch.bin.sh`, there is the following lines:

```
JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC"
JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
```

I wonder if you replace them with setting the G1GC it will fix it. In that case, it might make sense for us to have a simpler env var to set the GC to use.
</comment><comment author="rotastrain" created="2014-04-15T21:49:04Z" id="40538790">Ok i replaced that it it works. Thanks!
</comment><comment author="kimchy" created="2014-04-15T21:54:32Z" id="40539339">Btw, we are still seeing, mainly in Apache Lucene tests, that G1 still causes unexpected failures. So we do still recommend using the default configuration in Elasticsearch until we are sure all bugs are properly ironed out.
</comment><comment author="rotastrain" created="2014-04-16T09:35:26Z" id="40580401">Ok, 
I wanted to try the G1GC because I am having a few problems i am unable to solve.
I start my server using 
-Xms15G  -Xmx15G  mlockall:true ulimit:unlimited
I have only one index about 1 million documents about 3GB in size.
My query uses a filter in it.
What happens is that when i start my queries the heap size goes on increasing to about 97% and the entire system hangs for about 40 seconds till it frees a little heap bringing down the usage to 85% and continues.

I cannot understand what is filling up the heap . My filter cache size shows to be 3GB. 
The field size shows 0.0
Id Cache size: 0.0

Is this behavior normal? Can i do something to improve this.?
</comment><comment author="jherre" created="2014-10-20T15:47:36Z" id="59782959">Here's a convenient way to turn on G1GC using standard ES install:

``` bash
ES_JAVA_OPTS="-XX:-UseParNewGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC"
```

I tried this on a heavily loaded node that was unresponsive and it behaved much better.  But then I turned it off based on the advice of the ES team. 

We're on: `Java(TM) SE Runtime Environment (build 1.7.0_72-b14)`.  When the G1GC is ready for production please let us know.  Thanks.
</comment><comment author="Wilfred" created="2014-12-09T14:37:51Z" id="66291382">I'm also interested in running ES with G1GC. The mailing list does discuss some historical issues with trove4j and G1GC: http://elasticsearch-users.115913.n3.nabble.com/Anyone-have-G1-GC-working-What-environment-configs-td4039878.html but later discussion: http://elasticsearch-users.115913.n3.nabble.com/G1-Garbage-Collector-with-Elasticsearch-gt-1-1-td4058398.html doesn't mention any known issues in recent ES versions.

Some users are now using ES with G1GC: https://groups.google.com/forum/#!topic/elasticsearch/VQ4JhWvh1ow
</comment><comment author="portante" created="2014-12-09T15:50:20Z" id="66303833">FWIW: we are using G1GC with ElasticSearch 1.4.1 on our 4 socket, 256 GB setup.
</comment><comment author="jasontedor" created="2016-02-14T18:45:09Z" id="183948748">There's nothing to do here at this time as we still do not advise using G1GC.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>key_as_string for ip type terms not returned by java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5822</link><project id="" key="" /><description>``` json
DELETE /aggr_java
PUT /aggr_java
{
  "mappings": {
    "mytype": {
      "properties": {
        "destination_address": {
          "type": "ip"
        }
      }
    }
  }
}

PUT /aggr_java/mytype/1
{
      "destination_address":"136.34.216.63"
}

PUT /aggr_java/mytype/2
{
      "destination_address":"73.245.73.71"
}

GET /aggr_java/mytype/_search
{
  "aggs": {
    "destination_address": {
      "terms": {
        "field": "destination_address"
      }
    }
  }
}

Returns:

   "aggregations": {
      "destination_address": {
         "buckets": [
            {
               "key": 1240811847,
               "key_as_string": "73.245.73.71",
               "doc_count": 1
            },
            {
               "key": 2283984959,
               "key_as_string": "136.34.216.63",
               "doc_count": 1
            }
         ]
      }
   }
```

But if you use the Java API to fetch the buckets and check out the Terms.Bucket objects returned, you will see that key_as_string is not available to be fetched back. 

``` java
        SearchResponse sres = client.prepareSearch(INDEX_NAME).addAggregation(AggregationBuilders.terms("destination_address_return").field("destination_address")).execute().actionGet();

        Aggregations aggs = sres.getAggregations();
        Terms t = aggs.get("destination_address_return");
        List&lt;Terms.Bucket&gt; lBuckets = (List)t.getBuckets();
```
</description><key id="31579038">5822</key><summary>key_as_string for ip type terms not returned by java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Aggregations</label><label>:Java API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-04-15T18:38:25Z</created><updated>2016-02-14T10:42:29Z</updated><resolved>2016-02-14T10:42:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2014-04-15T18:50:09Z" id="40519290">A workaround is to convert the key values (eg. 2283984959) back to IP address format in the java application:

``` java
import org.elasticsearch.index.mapper.ip.IpFieldMapper;
...
...
IpFieldMapper.longToIp(Long.parseLong(b.getKey())) // where b is Terms.Bucket object
```
</comment><comment author="clintongormley" created="2016-02-13T12:16:29Z" id="183656229">@jimferenczi this seems related to #15393 - any chance you could add support for IPv4 as well?
</comment><comment author="jimczi" created="2016-02-14T10:42:25Z" id="183872435">@clintongormley the IPv4 is handled as a LongTerms internally which means that  Bucket.getKeyAsString() will now return the ip address formatted in a string (https://github.com/elastic/elasticsearch/pull/15393 in master only).
@ppf2 which version are you using ? I'll close it for now (it works in master branch), please reopen if I missed something.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Instantiate facets/aggregations during the QUERY phase.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5821</link><project id="" key="" /><description>In case of a DFS_QUERY_THEN_FETCH request, facets and aggregations are currently
instantiated during the DFS phase while they only become useful during the QUERY
phase. By instantiating during the QUERY phase instead, we can make better use
of recycling since objects will have a shorter life out of the recyclers.
</description><key id="31572666">5821</key><summary>Instantiate facets/aggregations during the QUERY phase.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-15T17:17:48Z</created><updated>2015-06-07T14:28:23Z</updated><resolved>2014-04-24T10:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-15T17:19:25Z" id="40508840">++ 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested Query score_mode `max` used `sum ` instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5820</link><project id="" key="" /><description>per [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html)

&gt; The score_mode allows to set how inner children matching affects scoring of parent. It defaults to avg, but can be total, max and none.

However with query like this `"score_mode": "max",` which return parent doc with 30 child doc, in explain it states  `"sum of:"` is used instead

```
"query": {
                "bool": {
                    "must": {
                        "nested": {
                            "path": "myObj",
                            "score_mode": "max",
                            "query": {
                                "bool": {
                                    "must": [{
                                            "mlt_field": {
                                                "myObj.myField": {
                                                    "like_text": "My Like Text",
                                                    "min_term_freq": 1,
                                                    "min_doc_freq": 1
                                                }
                                            }
                                            }
                                            ,
                                            {
                                                "match": {
                                                    "myObj.myField2": {
                                                        "query": 1
                                                    }
                                                }
                                            }
                                            ]
                                    }
                                }
                            }
                        },
                        "must_not": {
                            "match": {
                                "string_original_id": {
                                    "query": 1
                                }
                            }
                        }
                    }
                }

```

Explaination

```
_explanation: {
value: 8.011673
description: "sum of:"
details: [1]
0:  {
value: 8.011673
description: "Score based on child doc range from 21106 to 21136"
}-
-
}

```
</description><key id="31560482">5820</key><summary>Nested Query score_mode `max` used `sum ` instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vincentlaucy</reporter><labels /><created>2014-04-15T14:56:52Z</created><updated>2014-04-17T14:28:13Z</updated><resolved>2014-04-17T14:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-15T16:40:03Z" id="40504436">I think this is a problem in the explain output. do you really see the score bein a sum?
</comment><comment author="imotov" created="2014-04-15T21:50:41Z" id="40538949">@vincentlaucy the `"sum of:"` portion of the explanation comes from the `bool` query for the root object. Scores from multiple `must` clauses in the `bool` query are added together, hence the `"sum of"` in the description. The entire nested query explanation is represented as a single entrance. 

```
{
value: 8.011673
description: "Score based on child doc range from 21106 to 21136"
}
```

I did a couple of tests, the `sum` and `max` in the nested query works as expected.
</comment><comment author="vincentlaucy" created="2014-04-16T03:48:09Z" id="40560313">Thanks. I tried to reduce the query as below, however seems still get the score &amp; explanation.
I am using 1.1.0

```
{
    "size": 10,
    "explain": true,
    "min_score": 0.2,
            "query": {
                        "nested": {
                            "path": "myObj1",
                            "scoreMode": "max",
                            "query": {
                                            "mlt_field": {
                                                "myObj1.myField": {
                                                    "like_text": " Text To Like",

                                                    "min_term_freq": 1,
                                                    "min_doc_freq": 1
                                                }
                                            }
                            }
                }
            }


}
```

```
_explanation: {
value: 7.9484005
description: "Score based on child doc range from 3498 to 3499"
}
```
</comment><comment author="imotov" created="2014-04-16T22:52:02Z" id="40662222">Yes, and as you can see you no longer get anything about `sum`, because `sum` was part of the explanation for `bool` query. All you get is explanation for the `nested` query. Can we close the issue?
</comment><comment author="vincentlaucy" created="2014-04-17T13:15:22Z" id="40712230">Thanks @Imotov . originally i assumed score of mlt is of scale 0 to 1 and thus the confusion.  Are there ways to match against the text in  child doc's field and normalize the score?  Should I use fuzzy like this instead  if i want, when mlt text is "text" child doc is "text,  score as 1.0  ? (100% match in my case) 
</comment><comment author="imotov" created="2014-04-17T14:28:13Z" id="40719774">@vincentlaucy I am not completely sure what you are trying to achieve, but there are several different ways to affect scoring of your requests by using [constant_score](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-constant-score-query.html) queries, [function_score](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html) queries and [configurable similarities](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-similarity.html). If you have any questions about using them, let's continue this conversation on the mailing list. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add context values to aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5819</link><project id="" key="" /><description>We use aliases with filtering and routing to give us lightweight pseudo index-per-user.

Perhaps we should also accept "context" values in aliases, to do the same thing for the context suggester?
</description><key id="31552938">5819</key><summary>Add context values to aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>discuss</label></labels><created>2014-04-15T13:30:04Z</created><updated>2015-04-10T21:47:27Z</updated><resolved>2015-04-10T21:47:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T17:11:01Z" id="91624112">@areek can you comment on this? Is it still applicable with the new suggester stuff you're working on?
</comment><comment author="areek" created="2015-04-10T21:47:27Z" id="91702260">The new Completion Field will support filters, so this is not applicable atm.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch back to ConcurrentMergeScheduler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5818</link><project id="" key="" /><description>Load tests showed that SerialMS has problems to keep up with
the merges under high load. We should switch back to CMS
until we have a better story to balance merge
threads / efforts across shards on a single node.

Closes #5817
</description><key id="31552593">5818</key><summary>Switch back to ConcurrentMergeScheduler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-15T13:25:12Z</created><updated>2014-06-16T17:06:46Z</updated><resolved>2014-04-15T14:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-15T13:27:43Z" id="40480722">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch back to ConcurrentMergeScheduler as the default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5817</link><project id="" key="" /><description>Load tests showed that `SerialMS` has problems to keep up with the merges under high load. We should switch back to `CMS` until we have a better story to balance merge threads / efforts across shards on a single node.
</description><key id="31551728">5817</key><summary>Switch back to ConcurrentMergeScheduler as the default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>regression</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-15T13:13:53Z</created><updated>2015-06-08T00:43:53Z</updated><resolved>2014-04-15T14:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-15T13:14:30Z" id="40479370">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Don't delete data dirs after test - only delete their content.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5816</link><project id="" key="" /><description>Closes #5815
</description><key id="31551287">5816</key><summary>[TEST] Don't delete data dirs after test - only delete their content.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-15T13:07:27Z</created><updated>2014-07-16T21:46:23Z</updated><resolved>2014-04-15T15:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-04-15T13:39:46Z" id="40482007">+1
</comment><comment author="s1monw" created="2014-04-15T15:05:27Z" id="40492665">I fixed the plural thing @bleskes - will push in a sec...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>test infrastructure deletes entire data dir after test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5815</link><project id="" key="" /><description>@bleskes found a sneaky problem that our test infra deletes the entire data dir instead of just it's content. Yet this might not be a problem for 99.9% of our test but the clsuter stats sometimes fail since `File#getTotalSpace()` returns `0` as documented on the javadocs if the path doesn't point to a partition. We should only delete it's content but not the parent.
</description><key id="31550907">5815</key><summary>test infrastructure deletes entire data dir after test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-15T13:02:14Z</created><updated>2014-04-15T15:08:07Z</updated><resolved>2014-04-15T15:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Create an endpoint for analyzer config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5814</link><project id="" key="" /><description>This endpoint provides a starting point for folks that want to customize
the builtin analyzers beyond the customization allowed on the analyzer:
it returns several builtin analyzers as custom analyzers.  From there
customization is just adding or subtracting filters.

Closes #5648
</description><key id="31550681">5814</key><summary>Create an endpoint for analyzer config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-04-15T12:58:45Z</created><updated>2014-07-02T12:02:07Z</updated><resolved>2014-06-10T13:06:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-15T12:59:29Z" id="40477959">Currently a work in progress, don't merge.
</comment><comment author="nik9000" created="2014-04-15T16:09:17Z" id="40500931">No longer totally broken.  Ready for review.
</comment><comment author="s1monw" created="2014-04-17T16:09:44Z" id="40731747">@nik9000 I can see how this is useful though but I really don't like the amount of manual configuration / maintenance we have to do to make sure it's all consistent? I wonder if we can somehow annotate the analyzers / token filter factories to build this automatically?
</comment><comment author="nik9000" created="2014-04-17T16:26:02Z" id="40733486">@s1monw I'll think about how to generate but I don't have a lot of hope for it being clean.  I wonder if I can create the components with the builtin analyzer on some dummy and then tear it apart with reflection or something.  It doesn't sound pleasant. 
</comment><comment author="nik9000" created="2014-04-18T17:18:09Z" id="40826114">@s1monw I looked into doing something nasty with reflection and it was too nasty.  I thought for about a minute about some horrible stack trace and reflection hacks.  No good.  One option is to add a method to AnalyzerProvider that implements the build logic.  I'd move the private utility methods in `StandardAnalyzerProvider` to a public utility class.  That would be better at keeping things from slipping through the cracks.
</comment><comment author="nik9000" created="2014-04-25T15:34:25Z" id="41406133">@s1monw sorry to keep poking you, but what do you think about moving the construction into AnalyzerProvider?
</comment><comment author="s1monw" created="2014-04-25T15:37:34Z" id="41406501">I am not sure I can see how that would look like, can you put up a simple example?
</comment><comment author="nik9000" created="2014-04-25T16:19:53Z" id="41411269">I guess there are a bunch of ways to go.
This one makes sense if we want to teach each analyzer how to spit itself out as a custom analyzer with its current settings: https://gist.github.com/nik9000/11294459 .  That is beyond was I was originally going for but might be pretty useful.

Another option is like this:  https://gist.github.com/nik9000/11294727 and we'd have to scan the providers in AnalysisModule and look for the methods, 

This is like the last one but a bit nicer: https://gist.github.com/nik9000/11294985  we'd have to add those subclasses to the module and scan the bound &lt;T&gt; type for the provider to build.

I'm not really sure where to go from here, but at some point someone is going to have to shoulder the maintenance burden of these builds and Elasticsearch is in a better position for it then a downstream project.  I know it'd be a ton harder to test if I did this in CirrusSearch.
</comment><comment author="nik9000" created="2014-05-23T20:16:18Z" id="44055546">I'd love to reopen discussion about this.  I'm not really sure any of my proposals is a better way to go then what I have now.  They are all annoying from a maintenance perspective though.

I didn't realize that the chain of scripts that moves things from CirrusSearch's repository would cause comments to appear on this pull request....
</comment><comment author="nik9000" created="2014-05-30T14:32:14Z" id="44657262">I just replied on the mailing list to someone who needed this....  I'd love to do this in a less horrible way but I really can't think of any.

Ping, @javanna, we talked about this a few days ago.
</comment><comment author="nik9000" created="2014-06-10T13:06:34Z" id="45610457">Abandoning.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk insert queue count setting has no effect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5813</link><project id="" key="" /><description>Setting bulk queue size as persistent after setting another value as transient has no effect. 
</description><key id="31548907">5813</key><summary>Bulk insert queue count setting has no effect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">the100rabh</reporter><labels /><created>2014-04-15T12:30:52Z</created><updated>2014-08-02T07:30:37Z</updated><resolved>2014-08-02T07:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-17T11:47:12Z" id="40705711">hey,

can you help us to provide example curl calls to exactly show what you did and what you expected things to be, so we can create a test-case based on that behaviour?

Thanks a lot!
</comment><comment author="s1monw" created="2014-04-17T13:55:10Z" id="40716177">we had issues with this in early 0.90 version, which version are you using?
</comment><comment author="clintongormley" created="2014-08-02T07:30:37Z" id="50956535">This is still an issue.  There is no way to unset the transient setting, and it takes preference over the persistent setting.  Of course, you could just change the transient setting as well...

Closing in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize auto generated id indexing operation / no need to lookup version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5812</link><project id="" key="" /><description>This issue aims to track this branch: https://github.com/elasticsearch/elasticsearch/tree/enhacement/index_auto_generated_id.
</description><key id="31543706">5812</key><summary>Optimize auto generated id indexing operation / no need to lookup version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-04-15T10:56:55Z</created><updated>2014-04-25T14:23:23Z</updated><resolved>2014-04-25T14:23:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-04-25T14:23:23Z" id="41397793">Closing: https://github.com/elasticsearch/elasticsearch/pull/5917
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove garbage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5811</link><project id="" key="" /><description /><key id="31526318">5811</key><summary>Remove garbage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kou</reporter><labels /><created>2014-04-15T04:55:44Z</created><updated>2014-07-16T21:46:23Z</updated><resolved>2014-04-15T15:58:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T14:07:15Z" id="40485179">Hi @kou 

Thanks for the correction. Please could you sign the CLA so that we can get your commit merged in?
http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="kou" created="2014-04-15T14:37:53Z" id="40489025">I've done!
</comment><comment author="clintongormley" created="2014-04-15T15:58:03Z" id="40499575">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separate benchmark API endpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5810</link><project id="" key="" /><description>Separates benchmark API endpoints into separate files according to API
funtionality. This makes it easier for our tests and clients.

Closes #5787
</description><key id="31517358">5810</key><summary>Separate benchmark API endpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-15T00:38:38Z</created><updated>2014-10-21T23:42:40Z</updated><resolved>2014-04-15T03:30:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Delete Operations should not create index if it's not already existing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5809</link><project id="" key="" /><description>This relates to #5766 where pure delete bulk request recreated an already deleted index. IMO we should not create an index for delete operations only.
</description><key id="31506875">5809</key><summary>Delete Operations should not create index if it's not already existing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:CRUD</label><label>adoptme</label><label>enhancement</label></labels><created>2014-04-14T21:31:05Z</created><updated>2016-01-22T18:27:54Z</updated><resolved>2015-12-15T10:50:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T09:49:26Z" id="48026341">Rather throw an error if the index doesn't exist
</comment><comment author="kimchy" created="2014-07-11T11:50:45Z" id="48721603">we should have a flag to control this on the node level, if we decide to break backward comp. and set it to true, then this issue should be marked as breaking, with a note how to regain previous behavior.
</comment><comment author="clintongormley" created="2015-12-15T10:50:58Z" id="164724892">Closing in favour of #15425
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5808</link><project id="" key="" /><description>Moving benchmark documentation under the search section.

Closes #5786
</description><key id="31505390">5808</key><summary>Benchmark documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-14T21:13:13Z</created><updated>2014-10-21T23:42:35Z</updated><resolved>2014-04-14T21:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T21:13:50Z" id="40419005">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release should fail if test are annotated with AwaitsFix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5807</link><project id="" key="" /><description>today we don't check `AwaitsFix` if we do a `mvn deploy`. We should maybe extend forbidden APIs to do that and fail hard if we try to release with `AwaitsFix`
</description><key id="31505205">5807</key><summary>Release should fail if test are annotated with AwaitsFix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T21:11:12Z</created><updated>2015-06-07T14:28:40Z</updated><resolved>2014-04-17T14:48:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-04-15T19:16:48Z" id="40522287">I am working on that: https://code.google.com/p/forbidden-apis/issues/detail?id=8
</comment><comment author="s1monw" created="2014-04-15T21:17:04Z" id="40535649">@uschindler awesome! 
</comment><comment author="uschindler" created="2014-04-15T23:52:53Z" id="40548547">I committed the fix into forbidden-apis trunk (v1.5-dev). A snapshot is here: https://oss.sonatype.org/content/repositories/snapshots/de/thetaphi/forbiddenapis/1.5-SNAPSHOT/

I have no idea how to change "mvn deploy" so it runs the forbidden-apis Mojo. At least you can configure the plugin without a separate text file, just put the "signature" (the class name of the annotation into the properties:

``` xml
&lt;signatures&gt;package.AwaitsFix @ Please fix all bugs before release&lt;/signatures&gt;
```
</comment><comment author="rmuir" created="2014-04-16T00:07:30Z" id="40549529">Thanks a lot Uwe!
</comment><comment author="s1monw" created="2014-04-16T19:53:30Z" id="40644105">don't worry uwe I just pushed a branch that shows one possibility how to do this. Thanks for adding this - that's so much cleaner than everything else. When do you think you will release this?
</comment><comment author="uschindler" created="2014-04-16T20:25:48Z" id="40647709">OK, so you don't want to do it the "maven" way. I was expecting some special ant-like task for releasing. This of course works: Did you try it with the snapshot release? I think you must replace "package." with the real package of the annotation :-)

I plan to release this evening, because there are more changes:
- Fix some missing detections (also related to the annotations): e.g. it did not find field declarations using typed forbidden classes
- Forbidden classes in superclasses or interfaces of those types (e.g., in Java 8 MethodHandles, or Subannotations) were not detected
- Better error reporting if no line numbers are available.
</comment><comment author="s1monw" created="2014-04-16T20:30:42Z" id="40648268">@uschindler I didn't try it though but that should be enough for us. We run releases via that script which is almost trivial in the meanwhile. I will wait until you released it and then check it out :) can you ping me her once it's out there?
</comment><comment author="uschindler" created="2014-04-16T20:33:04Z" id="40648527">Ok

On 16. April 2014 22:31:09 MESZ, Simon Willnauer notifications@github.com wrote:

&gt; @uschindler I didn't try it though but that should be enough for us. We
&gt; run releases via that script which is almost trivial in the meanwhile.
&gt; I will wait until you released it and then check it out :) can you ping
&gt; me her once it's out there?
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/5807#issuecomment-40648268

## 

Uwe Schindler
H.-H.-Meier-Allee 63, 28213 Bremen
http://www.thetaphi.de
</comment><comment author="uschindler" created="2014-04-16T21:41:24Z" id="40655966">Message from:  https://oss.sonatype.org https://oss.sonatype.org

Description:

Release forbidden-apis 1.5

Deployer properties:
-   "userAgent" = "maven-artifact/2.2.1 (Java 1.5.0_22; Windows 7 6.1)"
-   "userId" = "thetaphi"
-   "ip" = "92.196.99.55"

Details:

The following artifacts have been promoted to the  https://oss.sonatype.org/content/repositories/releases "Releases" [id=releases] repository

 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5-sources.jar.asc /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5-sources.jar.asc
(SHA1: 535cd0af33d943cab486fbb348c56c23111eba95)
 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.jar /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.jar
(SHA1: ff356622d7820c0cd0a48a69ba3ab0e687475ed6)
 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.pom /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.pom
(SHA1: 9d81daae2d4833ba22fa6d6957e84fab38ad57fe)
 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.pom.asc /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.pom.asc
(SHA1: 549ded199cd068f1abf32fa005bce2fde8149320)
 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.jar.asc /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5.jar.asc
(SHA1: a7a08b8c2fae811148a57606fb356b5aef9c9888)
 https://oss.sonatype.org/content/repositories/releases/de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5-sources.jar /de/thetaphi/forbiddenapis/1.5/forbiddenapis-1.5-sources.jar
(SHA1: 710228659d8ef4aa6c00ac5485778072b19b1c1d)

Action performed by Uwe Schindler ( mailto:uwe@thetaphi.de uwe@thetaphi.de)
</comment><comment author="uschindler" created="2014-04-16T21:54:08Z" id="40657316">It is already on Maven Central!
</comment><comment author="uschindler" created="2014-04-17T21:13:02Z" id="40763486">Hi Simon,
I released a hotfix version of forbidden-apis v1.5.1, see http://repo1.maven.org/maven2/de/thetaphi/forbiddenapis/1.5.1/

You can now turn on the failOnMissingClasses setting (recommended). Please make sure that the AwaitsFix annotation is present at runtime (RetentionPolicy.RUNTIME).
</comment><comment author="uschindler" created="2014-04-17T21:14:40Z" id="40763688">Ok, that is the case:

```
  @Documented
  @Inherited
  @Retention(RetentionPolicy.RUNTIME)
  @TestGroup(enabled = false, sysProperty = SYSPROP_AWAITSFIX)
  public @interface AwaitsFix {
    /** Point to JIRA entry. */
    public String bugUrl();
  }
```
</comment><comment author="uschindler" created="2014-04-17T21:21:48Z" id="40764400">There is also a typo in the forbidden configuration (I think it has no effect), but should be fixed: `&lt;bundledSignaure&gt;jdk-system-out&lt;/bundledSignaure&gt;`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup FileSystemUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5806</link><project id="" key="" /><description>This PR cleans up a lot of scary methods in FileSystemUtils.
</description><key id="31496576">5806</key><summary>Cleanup FileSystemUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T19:46:53Z</created><updated>2015-06-07T14:28:50Z</updated><resolved>2014-04-15T11:28:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-15T10:41:46Z" id="40467855">I am good with this. A bit of history about mkdir stalls, way back when on EBS volumes, where ES would be stuck on the call to mkdir. Just so we know, if we see it again, we might want to address it back.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What is the format for these exception strings?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5805</link><project id="" key="" /><description>I can never tell how to unpack the strings that come out of exceptions. I can tell it's not valid JSON because there is no enclosing `{}` or `[]`. What is the format?

As an example:

``` json
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[9-dk0QMETtC3iz7qM-wV6Q][index][4]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][4]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[YLIcV0mwRHGGYuS1OkVfMA][index][3]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][3]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[f0OEGTrNQNCB9WxZDIbtJg][index][2]: QueryPhaseExecutionException[[index][2]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[f0OEGTrNQNCB9WxZDIbtJg][index][0]: QueryPhaseExecutionException[[index][0]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[9-dk0QMETtC3iz7qM-wV6Q][index][1]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][1]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }]",
   "status": 500
}
```
</description><key id="31488632">5805</key><summary>What is the format for these exception strings?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thejohnfreeman</reporter><labels><label>discuss</label></labels><created>2014-04-14T18:40:59Z</created><updated>2014-08-08T12:47:26Z</updated><resolved>2014-08-08T12:47:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T12:47:26Z" id="51595820">Closed in favour of #3303 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet and aggregation equivalence for date_histogram with script value?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5804</link><project id="" key="" /><description>This facet returns expected results:

``` json
{
  "facets": {
    "by_interval": {
      "date_histogram": {
        "key_field": "time",
        "interval": "day",
        "value_script": "doc[\"endDatetime\"].value - doc[\"beginDatetime\"].value"
      }
    }
  }
}
```

``` json
{
   "facets": {
      "by_interval": {
         "_type": "date_histogram",
         "entries": [
            {
               "time": 1397088000000,
               "count": 45,
               "min": 4650,
               "max": 1287401,
               "total": 4984621,
               "total_count": 45,
               "mean": 110769.35555555555
            },
...
```

But this aggregation does not:

``` json
{
  "aggregations": {
    "by_interval": {
      "date_histogram": {
        "field": "time",
        "interval": "day",
        "script": "doc[\"endDatetime\"].value - doc[\"beginDatetime\"].value"
      }
    }
  }
}
```

``` json
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[9-dk0QMETtC3iz7qM-wV6Q][index][4]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][4]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[YLIcV0mwRHGGYuS1OkVfMA][index][3]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][3]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[f0OEGTrNQNCB9WxZDIbtJg][index][2]: QueryPhaseExecutionException[[index][2]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[f0OEGTrNQNCB9WxZDIbtJg][index][0]: QueryPhaseExecutionException[[index][0]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }{[9-dk0QMETtC3iz7qM-wV6Q][index][1]: RemoteTransportException[[host][inet[/0.0.0.0:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[index][1]: query[ConstantScore(cache(_type:type))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[-1]; }]",
   "status": 500
}
```

Is there an equivalency in aggregations for the facet behavior I get?

Note: Field names have been changed to protect the innocent. Any misspellings are likely typos in this issue instead of the original query.
</description><key id="31488460">5804</key><summary>Facet and aggregation equivalence for date_histogram with script value?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thejohnfreeman</reporter><labels /><created>2014-04-14T18:38:59Z</created><updated>2014-12-09T12:11:27Z</updated><resolved>2014-04-17T11:26:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-17T11:26:14Z" id="40704436">Have you tried using a date histogram aggregation with a (extended) stats sub aggregation, that uses scripting? Might be the way to go here.

Also, please ask usability questions like this preferably on the google group, as we are trying hard to have github issues for bugs only. Thanks a lot!
</comment><comment author="thejohnfreeman" created="2014-04-17T14:05:48Z" id="40717310">I tried something similar just now and got an exception:

``` json
  {
    "aggs": {
      "docs_per_interval": {
         "date_histogram": {
            "field": "time",
            "interval": "day",
            "format": "yyyy-MM-dd HH:mm:ss",
            "aggs": {
               "users": {
                  "terms": {
                     "field": "user",
                     "size": 20
                  }
               }
            }
         }
      }
   }
```

```
 Parse Failure [date histogram can only be aggregated on date fields but  [user] is not a date field]]; }]",
```
</comment><comment author="uboness" created="2014-04-18T16:25:39Z" id="40821835">The  sub "aggs" need to be a sibling of "date_histogram"... Not an inner object
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>md5Hex no longer works in MVEL scripts (taken out 2014-07-29)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5803</link><project id="" key="" /><description>I apologize if an issue already exists for this; I am having trouble searching for issues...

I noticed that this code was removed in a "cleanup" commit ( 86ceabfbbd8192c9b64c3d33afd9f66b1d8bae3c by @kimchy ), apparently under the impression that it was only used for tests. People have been using it in their production MVEL scripts for pseudo-random shuffling. Is this something we should bring back? Is there another way to do pseudo-random shuffling (I tried using function_score, but could not get it to work)?
</description><key id="31484371">5803</key><summary>md5Hex no longer works in MVEL scripts (taken out 2014-07-29)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AaronM04</reporter><labels /><created>2014-04-14T17:48:27Z</created><updated>2014-12-30T15:40:19Z</updated><resolved>2014-12-30T15:40:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-17T11:16:13Z" id="40703809">maybe you can show an example how you use it for pseudo-random shuffling and we can check, if you can have the same behaviour with function_score, before bringing it back (seems cleaner with function score from my outside point of view)?
</comment><comment author="imotov" created="2014-04-17T15:29:44Z" id="40727127">I think figuring why function score doesn't work is the way to go. However, you can also simply implement the [functionality of the old md5Hex function](https://github.com/imotov/elasticsearch-native-script-example/blob/master/src/main/java/org/elasticsearch/examples/nativescript/script/RandomSortScriptFactory.java#L66) in your script.
</comment><comment author="AaronM04" created="2014-04-29T23:15:09Z" id="41743876">I withdraw my request. :-)
</comment><comment author="clintongormley" created="2014-12-30T15:40:19Z" id="68366492">Not an issue. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.7.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5802</link><project id="" key="" /><description>Lucene 4.7.2 is available on mvn central 

lets get it in!
</description><key id="31478392">5802</key><summary>Upgrade to Lucene 4.7.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T16:31:02Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-04-14T16:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Check for no open issues before build release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5801</link><project id="" key="" /><description>this commit adds a basic check in the release script that checks if there are any open issues on github for the release we are trying to build.
</description><key id="31477965">5801</key><summary>Check for no open issues before build release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T16:25:41Z</created><updated>2015-06-07T14:29:33Z</updated><resolved>2014-04-14T16:56:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Ensure close is called under lock in the case of an engine failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5800</link><project id="" key="" /><description>Until today we did close the engine without aqcuireing the write lock
since most calls were still holding a read lock. This commit removes
the code that holds on to the readlock when failing the engine which
means we can simply call #close()
</description><key id="31472722">5800</key><summary>Ensure close is called under lock in the case of an engine failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T15:32:41Z</created><updated>2015-06-07T14:30:14Z</updated><resolved>2014-04-16T12:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-04-15T11:54:13Z" id="40472846">I like it! it makes things cleaner. Left some comments..
</comment><comment author="s1monw" created="2014-04-15T16:44:49Z" id="40504954">@bleskes @kimchy thanks guys I commented and pushed a new commit
</comment><comment author="s1monw" created="2014-04-16T10:53:08Z" id="40586076">@bleskes thanks for the review - I pushed another commit
</comment><comment author="bleskes" created="2014-04-16T11:02:01Z" id="40586636">thx. Simon. Looking good. Left one last comment. I'm +1 on this otherwise.
</comment><comment author="s1monw" created="2014-04-16T12:05:17Z" id="40590919">I fixed your last suggestion! thanks for all the reveiws @bleskes I think it's ready, if you don't object I'd like to rebase and push it.
</comment><comment author="bleskes" created="2014-04-16T12:07:37Z" id="40591085">thx. ++1 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved SearchContext.addReleasable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5799</link><project id="" key="" /><description>For resources that have their life time effectively defined by the search
context they are attached to, it is convenient to use the search context to
schedule the release of such resources.

This commit changes aggregations to use this mechanism and also introduces
a `Lifetime` object that can be used to define how long the object should
live:
- COLLECTION: if the object only needs to live during collection time and is
  what SearchContext.addReleasable would have chosen before this change
  (used for p/c queries),
- PHASE for resources that only need to live during the current search
  phase (DFS, QUERY or FETCH),
- CONTEXT for resources that need to live until the context is
  destroyed.

Aggregators are currently registed with CONTEXT. The reason is that when
using the DFS_QUERY_THEN_FETCH search type, they are allocated during the DFS
phase but only used during the QUERY phase. However we should fix it in order
to only allocate them during the QUERY phase and use PHASE as a life
time.

Close #5703
</description><key id="31464251">5799</key><summary>Improved SearchContext.addReleasable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T14:11:43Z</created><updated>2015-06-07T21:39:10Z</updated><resolved>2014-04-14T15:51:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T14:23:34Z" id="40371062">I left some style issue comments but this looks very good!
</comment><comment author="jpountz" created="2014-04-14T14:50:10Z" id="40374210">@s1monw I just pushed new commits
</comment><comment author="s1monw" created="2014-04-14T15:15:10Z" id="40377321">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify cluster state update on mapping change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5798</link><project id="" key="" /><description>we had some bug lately in #5623 and #5776 which failed to process update operations on a mapping change. We should make sure that this is not an optional step if we do document parsing or if it is an optional step you need to actively opt out.  
</description><key id="31454643">5798</key><summary>Unify cluster state update on mapping change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T11:47:39Z</created><updated>2014-04-29T09:28:47Z</updated><resolved>2014-04-18T12:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>deleted_* stats are not updated when running delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5797</link><project id="" key="" /><description>It looks like when you delete a document by ID, the delete_total and delete_time_in_milis (I assume the delete_current, too) work just fine, but they don't when you run delete by query.

You can find a curl recreation of the problem here: https://gist.github.com/radu-gheorghe/10637236

I'm reproducing this on 1.1.0, but I assume it happens with earlier versions as well.
</description><key id="31451763">5797</key><summary>deleted_* stats are not updated when running delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels /><created>2014-04-14T10:57:42Z</created><updated>2014-12-30T15:39:53Z</updated><resolved>2014-12-30T15:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="radu-gheorghe" created="2014-04-14T11:00:47Z" id="40354243">I think this thread might be related: https://groups.google.com/forum/#!topic/elasticsearch/-3zp0biP4dQ
</comment><comment author="clintongormley" created="2014-12-30T15:39:53Z" id="68366444">Closing in favour of #7052
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Find out min/max values for correct intersection handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5796</link><project id="" key="" /><description>A multi polygon with holes didnt correctly calculate possible intersections
because it used the first edge instead of the edge with the min/max value of
the longitude (depending if positive/negative)

Closes #5773
</description><key id="31451353">5796</key><summary>Geo: Find out min/max values for correct intersection handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-04-14T10:50:29Z</created><updated>2014-07-16T21:46:27Z</updated><resolved>2014-05-09T08:43:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcuswr" created="2014-04-16T21:04:44Z" id="40652077">This patch resolves most of #5773, however, there are additional valid polygons which still fail.  Please see https://gist.github.com/marcuswr/493406918e0a9edeb509 for a set of polygons which fail  with the same ArrayIndexOutOfBoundsException.  Please let me know if an can provide any additional data.
</comment><comment author="spinscale" created="2014-04-16T21:24:30Z" id="40654222">this is awesome! thanks a lot for testing and providing all the data! I will try to check that out as soon as possible
</comment><comment author="marcuswr" created="2014-04-18T16:57:02Z" id="40824370">I did a little more testing and I ran each of the remaining polygons (from the gist) through the jts2geojson (https://github.com/bjornharrtell/jts2geojson) library, and they all parsed successfully.

import org.wololo.jts2geojson.GeoJSONReader;
...
GeoJSONReader reader = new GeoJSONReader();
Geometry geometry = reader.read(json);

Not sure if that's of any help, but I thought I'd provide another datapoint.
</comment><comment author="marcuswr" created="2014-05-08T23:53:18Z" id="42620319">I believe this commit should be abandoned, the fact that it fixed some errors is purely coincidental.  Choosing the min,max,avg or random point of the hole for the intersection test would have the same result.  I think the issue lies deeper with the treatment of line segment intersections that occur on the starting or ending coordinate of the segment (ie. intersection returns 0.0 or 1.0). 
</comment><comment author="spinscale" created="2014-05-09T08:43:10Z" id="42644932">closing, not implementing the correct behaviour
</comment><comment author="marcuswr" created="2014-05-13T20:31:52Z" id="43008604">I have a workaround for this issue (when polygons don't cross the dateline) https://github.com/marcuswr/elasticsearch/commit/7ce15ff83da8ee1d5dcdf86d2c4bf4749f1acda2
Could you take a look at this when you get a chance.  I'm working with my company to the the CLA signed, and then I'll issue a pull request.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use TransportBulkAction for internal request from IndicesTTLService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5795</link><project id="" key="" /><description>This prevents executing bulks internal autocreate indices logic
and ensures that this internal request never creates an index
automaticall.

This fixes a bug where the TTL purger thread ran after the actual
index it was purging was already closed / deleted and that re-created
that index.

Closes #5766
</description><key id="31448931">5795</key><summary>Use TransportBulkAction for internal request from IndicesTTLService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-14T10:08:26Z</created><updated>2015-06-07T21:04:35Z</updated><resolved>2014-04-15T10:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T21:45:24Z" id="40422236">@imotov I pushed a new commit based on your comments!
</comment><comment author="imotov" created="2014-04-14T22:01:01Z" id="40423670">Left a couple of comments with cosmetic changes. Everything else - LGTM. 
</comment><comment author="bleskes" created="2014-04-15T10:01:35Z" id="40464635">I wonder if we should introduce another path for bulk request execution. Instead we can add an option to the bulk request to disable index creation per request? An alternative would be to use the BulkShardRequest directly as we are operating on a single shard and don't need all the fancy resolving logic of BulkRequest. Did you consider that?
</comment><comment author="s1monw" created="2014-04-15T10:05:52Z" id="40464941">@bleskes this is a bigger change and I don't see any need to do this unless you want to refactor this class. This is a bugfix and should be least intrusive. I created a new issue #5809 to prevent this kind of behavior for delete only ops. We can optimize this later but I want to get this in first. Feel free to open a follup issue.
</comment><comment author="s1monw" created="2014-04-15T10:08:59Z" id="40465211">I pushed new commits - I think it's ready
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats API fetching all indices information if given regex has no matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5794</link><project id="" key="" /><description>Hi,
I expect to get empty response for scenario;

``` ssh
#create an index
curl -XPOST http://localhost:9200/test-1

# execute _stats for indices starting with xyz (ensure this regex has no matches)
curl -XPOST http://locahost:9200/xyz*/_stats
```

returns stats of all indices. I think this should return empty response.

Tried with 1.0.1 and 1.1.0
</description><key id="31438537">5794</key><summary>Stats API fetching all indices information if given regex has no matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ferhatsb</reporter><labels><label>:Stats</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-04-14T07:07:15Z</created><updated>2015-11-28T15:07:59Z</updated><resolved>2015-11-28T15:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ferhatsb" created="2014-04-14T07:14:44Z" id="40338623">- Same behaviour for status api.
</comment><comment author="clintongormley" created="2014-04-15T13:46:18Z" id="40482735">cc @spinscale 
</comment><comment author="awick" created="2014-10-22T12:36:29Z" id="60077687">This does work correctly for _aliases, so maybe _stats and others could be changed to use that globbing?

curl 'http://localhost:9200/xyz*/_aliases'
{}
</comment><comment author="xuzha" created="2015-11-23T07:41:10Z" id="158870840">@clintongormley, looks like this issue has been fixed. I cannot reproduce it.
</comment><comment author="clintongormley" created="2015-11-28T15:07:59Z" id="160308635">thanks @xuzha 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Partial snapshot state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5793</link><project id="" key="" /><description>Currently even if some shards of the snapshot are not snapshotted successfully, the snapshot is still marked as "SUCCESS". Users may miss the fact the there are shard failures present in the snapshot and think that snapshot was completed. This change adds a new snapshot state "PARTIAL" that provides a quick indication that the snapshot was only partially successful.

Closes #5792
</description><key id="31427049">5793</key><summary>Add Partial snapshot state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-13T23:21:34Z</created><updated>2015-08-13T15:11:15Z</updated><resolved>2014-05-16T23:01:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T09:20:31Z" id="43186784">I left one comment but this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add "PARTIAL" snapshot status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5792</link><project id="" key="" /><description>Currently even if some shards of the snapshot are not snapshotted successfully, the snapshot is still marked as "SUCCESS". Users may miss the fact the there are shard failures present in the snapshot and think that snapshot was completed successfully (#5657 and #5742). A new "PARTIAL" snapshot status should be added to provides a quick indication that the snapshot was only partially successful. 
</description><key id="31427009">5792</key><summary>Add "PARTIAL" snapshot status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-13T23:19:32Z</created><updated>2015-06-06T17:02:12Z</updated><resolved>2014-05-16T23:01:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T08:59:49Z" id="40345564">@imotov is this a bug too? It seems to be tagged as `1.1.1` which I don't think it should be tagged with from the labels that I see.
</comment><comment author="imotov" created="2014-04-14T09:56:30Z" id="40349700">Oops, got carried away. That's definitely 1.2 and above feature.
</comment><comment author="Rams20" created="2014-12-30T12:37:05Z" id="68353031">Could you please tell me how to  take snapshot without primary shards ...?
thanks in advance please help me..
</comment><comment author="Rams20" created="2015-04-27T13:18:12Z" id="96648252">why snapshot process always showing PARTIAL status...?
</comment><comment author="imotov" created="2015-04-27T15:36:29Z" id="96713335">@Rams20 it most likely means that your snapshot process was not able to snapshot all shards either because the process failed or they were unavailable. We are using github issue for bug reports and feature requests. So, if you have any additional questions about this or any other feature, please ask them on the mailing list. 
</comment><comment author="Rams20" created="2015-04-28T05:15:09Z" id="96911988">Is snapshot compatible with elasticsearch 1.4.4...?  
</comment><comment author="markwalkom" created="2015-04-28T05:22:15Z" id="96913802">Please use the mailing list. This issue was closed over a year ago.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix snapshot status with empty repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5791</link><project id="" key="" /><description>The snapshot status command with empty repository should return current status of currently running snapshots in all repositories.

Fixes #5790
</description><key id="31426435">5791</key><summary>Fix snapshot status with empty repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-13T22:46:23Z</created><updated>2015-06-07T20:56:08Z</updated><resolved>2014-04-14T23:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T08:58:40Z" id="40345474">I left one comment
</comment><comment author="imotov" created="2014-04-14T14:26:25Z" id="40371395">@s1monw I have updated the PR based on your comment.
</comment><comment author="s1monw" created="2014-04-14T20:12:22Z" id="40412180">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot Status failing without repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5790</link><project id="" key="" /><description>The snapshot status request 

```
 curl -XGET "localhost:9200/_snapshot/_status"
```

returns an error

```
{"error":"ActionRequestValidationException[Validation Failed: 1: repository is missing;]","status":500}
```

It should return all a list of currently running snapshots in all repositories instead.
</description><key id="31426389">5790</key><summary>Snapshot Status failing without repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-13T22:43:42Z</created><updated>2015-06-07T20:55:40Z</updated><resolved>2014-04-14T23:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Common fields aggregation / bucketing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5789</link><project id="" key="" /><description>Aggregations are great! It would be even more cool to have an aggregation that buckets by field occurrence (based on field name) from a given path, allowing us to apply sub-aggs on these _dynamically found_ fields.

Say we have the following product index:

```
curl -XPOST 'localhost:9200/store/product/1' -d '{"name":"Galaxy S4","manufacturer":"Samsung","type":"phone","specs":{"os":"Android","core":4,"ram":"2GB"}}'
curl -XPOST 'localhost:9200/store/product/2' -d '{"name":"iPhone 5","manufacturer":"Apple","type":"phone","specs":{"os":"iOS","core":2,"ram":"1GB"}}'
curl -XPOST 'localhost:9200/store/product/3' -d '{"name":"WF210ANW/XAA","manufacturer":"Samsung","type":"washing-machine","specs":{"capacity":3.5,"loadType":"front","presets":6}}'
curl -XPOST 'localhost:9200/store/product/4' -d '{"name":"WT4801CW","manufacturer":"LG","type":"washing-machine","specs":{"capacity":3.7,"loadType":"top","presets":9}}'
```

Here we have two different nature of products (phone and washing machine), they have consequently different features (the K/V pairs in the `specs` field).
If such an aggregation was available, we could apply it to retrieve the top _most frequent_ fields for a resulting data set and compute any kind of sub-agg over them. Combined with well-minded query/filter this could end to a nice magic agg effect :)

I tried to build my own aggs tree using [terms aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html), but I did not found any way to access the parent bucket key from a child aggregation. I only succeed to achieve the first part, ie. find the top-n field names sorted by occurence count. Anyway this seems not viable as it looks like a 2-step process: doing it this way, we need to evaluate the number of occurrences of each fields first and then compute sub-aggs on the selected ones.

```
... query and filters ...
"aggs" : {
  "dynamic_agg_step1": {
    "terms": { 
      "script": "_source.fields.keySet()",
      "exclude": "type|subtype"
    }
    "aggs": {
      "dynamic_agg_step2": {
        "terms": { 
          "script": "_source.fields.get(_myMagicBucketKey?_)"
        }
      }
    }
  }
}
```

A simple (and probably naive) approach would be to compute declared sub-aggs on all fields seen from a given path (here the `specs` path) while keeping track of field occurrences and finally only retain agg trees from the _most frequent_ top-n fields. This represents extra workload and more memory to hold aggs for all fields but if the field cardinality remains reasonable, it mights worth it. We could also imagine to exclude some fields that we know we're not interested in (as we can currently [do](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_filtering_values) with the terms agg). Obviously, the considered path necessarily have to point to a JSON hash to run correctly.

Tell me if that makes sense to you?
Maybe I missed something or the feature already exists, and if so please tell me :)

Nicolas
</description><key id="31409027">5789</key><summary>Common fields aggregation / bucketing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ncolomer</reporter><labels /><created>2014-04-13T15:56:09Z</created><updated>2015-10-15T04:59:30Z</updated><resolved>2014-10-17T06:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-14T08:29:09Z" id="40343431">Can you give an example of what the output of this aggregation should look like, given these 4 json documents?
</comment><comment author="ncolomer" created="2014-04-14T12:44:07Z" id="40361227">Hi @jpountz, sure, let's detail this washing machine use case :)

So we have the following docs indexed (removed `core` and `presets` specs, added `weight` spec to all 4 products and an `os` additional spec to one of the washing machine to improve the example):

```
curl -XPOST 'localhost:9200/store/product/1' -d '{"name":"Galaxy S4","manufacturer":"Samsung","type":"phone","specs":{"os":"Android","ram":"2GB","weight":0.13}}'
curl -XPOST 'localhost:9200/store/product/2' -d '{"name":"iPhone 5","manufacturer":"Apple","type":"phone","specs":{"os":"iOS","ram":"1GB","weight":0.112}}'
curl -XPOST 'localhost:9200/store/product/3' -d '{"name":"WF210ANW/XAA","manufacturer":"Samsung","type":"washing-machine","specs":{"capacity":3.5,"loadType":"front","weight":82.9,"os":"Android"}}'
curl -XPOST 'localhost:9200/store/product/4' -d '{"name":"WT4801CW","manufacturer":"LG","type":"washing-machine","specs":{"capacity":3.7,"loadType":"top","weight":57.6}}'
```

My **first query** filters on product type and retains phones only. In this query, the dynamic field name is accessible via the `{key}` keyword. The field may (or may not) be relative to the parent `magic_common_fields_agg` aggregation (see the `path` property).

```
{
   "query": {
      "filtered": {
         "query": {"match_all": {}},
         "filter": {"term": {"type":"phone"}}
      }
   },
   "aggs": {
      "specs": {
         "magic_common_fields_agg": {"path": "specs"},
         "aggs": {
           "values": {"terms" : { "field": "{key}"}}
         }
      }
   }
}
```

The result would be:

```
{
    ...
    "aggregations": {
        "specs": {
            "buckets": [
                {
                    "key": "weight", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": 0.13, "doc_count": 1},
                            {"key": 0.112, "doc_count": 1}
                        ]
                    }
                },
                {
                    "key": "os", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": "Android", "doc_count": 1},
                            {"key": "iOS", "doc_count": 1}
                        ]
                    }
                },
                {
                    "key": "ram", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": "1GB", "doc_count": 1},
                            {"key": "2GB", "doc_count": 1}
                        ]
                    }
                }
            ]
        }
    }
}
```

My **second query** filters on product type and retains washing machines only:

```
{
   "query": {
      "filtered": {
         "query": {"match_all": {}},
         "filter": {"term": {"type":"washing-machine"}}
      }
   },
   "aggs": {
      "specs": {
         "magic_common_fields_agg": {"path": "specs"},
         "aggs": {
           "values": {"terms" : { "field": "{key}"}}
         }
      }
   }
}
```

The result would be:

```
{
    ...
    "aggregations": {
        "specs": {
            "buckets": [
                {
                    "key": "weight", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": 82.9, "doc_count": 1},
                            {"key": 57.6, "doc_count": 1}
                        ]
                    }
                },
                {
                    "key": "capacity", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": 3.5, "doc_count": 1},
                            {"key": 3.7, "doc_count": 1}
                        ]
                    }
                },
                {
                    "key": "loadType", "doc_count": 2,
                    "values": {
                        "buckets": [
                            {"key": "front", "doc_count": 1},
                            {"key": "top", "doc_count": 1}
                        ]
                    }
                },
                {
                    "key": "os", "doc_count": 1,
                    "values": {
                        "buckets": [
                            {"key": "Android", "doc_count": 1}
                        ]
                    }
                }
            ]
        }
    }
}
```

Finally, my **third query** does not filter anything but limits the number of returned buckets to 2 (ie. we want the 2 most occurring fields only). Additionally, we can imagine add kind of field name match pattern (as with dynamic_templates matching) or even field type match (useful to avoid computing a term on high cardinality field such as numeric field) to apply a sub-agg to specific field(s) only.

```
{
   "query": {"match_all": {}},
   "aggs": {
      "specs": {
         "magic_common_fields_agg": {"path": "specs", "size": 2},
         "aggs": {
           "range": {"range" : { "field_match": "weight", "ranges": [{"to": 1},{"from": 1,"to": 20},{"from": 20}]}},
           "values": {"terms" : { "field_match": "*", "field_exclude": "weight"}}
         }
      }
   }
}
```

The result would be:

```
{
    ...
    "aggregations": {
        "specs": {
            "buckets": [
                {
                    "key": "weight", "doc_count": 4,
                    "range": {
                        "buckets": [
                            {"to": 1, "doc_count": 2},
                            {"from": 1, "to": 20, "doc_count": 0},
                            {"from": 20, "doc_count": 2}
                        ]
                    }
                },
                {
                    "key": "os", "doc_count": 3,
                    "values": {
                        "buckets": [
                            {"key": "Android", "doc_count": 2},
                            {"key": "iOS", "doc_count": 1}
                        ]
                    }
                }
            ]
        }
    }
}
```
</comment><comment author="kostiklv" created="2014-08-06T21:22:09Z" id="51399133">We are currently prototyping a very similar approach with 2 queries:

To avoid scripting, we're just indexing an additional field, like: `"spec_fields": ["os", "ram", "weight"]`. (But since 1.3 you can use [_field_names](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-field-names-field.html#mapping-field-names-field) instead)

Then, in the first query we are doing simple `terms` aggregation on `spec_fields` field and getting the list of (most popular) spec field names for a given products query. This aggregation is very lightweight and fast, as cardinality of `spec_fields` is pretty limited. BTW, you can get the actual products along with aggregation in the same query.

Then, we are building the second query, which includes a separate `terms` aggregation for each of the fields returned by the first query.

I would also be interested in ability to do this in a single query, or even better to use it in sub-aggregation. Any plans to implement that?
</comment><comment author="clintongormley" created="2014-10-17T06:50:29Z" id="59472925">You can do this using nested documents as follows:

```
DELETE /store

PUT /store
{
  "mappings": {
    "product": {
      "properties": {
        "specs": {
          "type": "nested",
          "properties": {
            "key": {
              "type": "string",
              "index": "not_analyzed"
            },
            "value": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

POST /store/product/1
{
  "name": "Galaxy S4",
  "manufacturer": "Samsung",
  "type": "phone",
  "specs": [
    {
      "key": "os",
      "value": "Android"
    },
    {
      "key": "core",
      "value": 4
    },
    {
      "key": "ram",
      "value": "2GB"
    }
  ]
}

POST /store/product/2
{
  "name": "iPhone 5",
  "manufacturer": "Apple",
  "type": "phone",
  "specs": [
    {
      "key": "os",
      "value": "iOS"
    },
    {
      "key": "core",
      "value": 2
    },
    {
      "key": "ram",
      "value": "1GB"
    }
  ]
}

POST /store/product/3
{
  "name": "WF210ANW/XAA",
  "manufacturer": "Samsung",
  "type": "washing-machine",
  "specs": [
    {
      "key": "capacity",
      "value": 3.5
    },
    {
      "key": "loadType",
      "value": "front"
    },
    {
      "key": "presets",
      "value": 6
    }
  ]
}

POST /store/product/4
{
  "name": "WT4801CW",
  "manufacturer": "LG",
  "type": "washing-machine",
  "specs": [
    {
      "key": "capacity",
      "value": 3.7
    },
    {
      "key": "loadType",
      "value": "top"
    },
    {
      "key": "presets",
      "value": 9
    }
  ]
}

GET /store/_search?search_type=count
{
  "aggs": {
    "specs": {
      "nested": {
        "path": "specs"
      },
      "aggs": {
        "key": {
          "terms": {
            "field": "specs.key"
          },
          "aggs": {
            "value": {
              "terms": {
                "field": "specs.value"
              }
            }
          }
        }
      }
    }
  }
}
```

This query returns:

```
{
   "took": 82,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 4,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "specs": {
         "doc_count": 12,
         "key": {
            "buckets": [
               {
                  "key": "capacity",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "3.5",
                           "doc_count": 1
                        },
                        {
                           "key": "3.7",
                           "doc_count": 1
                        }
                     ]
                  }
               },
               {
                  "key": "core",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "2",
                           "doc_count": 1
                        },
                        {
                           "key": "4",
                           "doc_count": 1
                        }
                     ]
                  }
               },
               {
                  "key": "loadType",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "front",
                           "doc_count": 1
                        },
                        {
                           "key": "top",
                           "doc_count": 1
                        }
                     ]
                  }
               },
               {
                  "key": "os",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "Android",
                           "doc_count": 1
                        },
                        {
                           "key": "iOS",
                           "doc_count": 1
                        }
                     ]
                  }
               },
               {
                  "key": "presets",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "6",
                           "doc_count": 1
                        },
                        {
                           "key": "9",
                           "doc_count": 1
                        }
                     ]
                  }
               },
               {
                  "key": "ram",
                  "doc_count": 2,
                  "value": {
                     "buckets": [
                        {
                           "key": "1GB",
                           "doc_count": 1
                        },
                        {
                           "key": "2GB",
                           "doc_count": 1
                        }
                     ]
                  }
               }
            ]
         }
      }
   }
}
```
</comment><comment author="clintongormley" created="2014-10-17T06:53:30Z" id="59473130">I think this solution works for all of your requirements, so I'm going to close this issue. Please reopen if I've missed something
</comment><comment author="ncolomer" created="2014-10-17T20:46:59Z" id="59573665">My approach was way so complicated... thanks a lot for the reply and lucidity!
</comment><comment author="phani546" created="2014-11-05T14:48:14Z" id="61817408">Hi @clintongormley,
   I have a database table to index the data into nested type.what is the better approach to index nested type values in to elastic search.

Thanks,
phani
</comment><comment author="clintongormley" created="2014-11-05T14:50:11Z" id="61817749">@phani546 you need to do the conversion in your application, before it reaches Elasticsearch
</comment><comment author="phani546" created="2014-11-05T14:53:15Z" id="61818273">@clintongormley Thank you for the response. i will try in that way .
</comment><comment author="phani546" created="2014-11-07T14:34:50Z" id="62152348">@clintongormley ,

   I tried with ES JAVA API to insert the data for the following index. please find the schema created.As per you suggestion i created index like this but while indexing data i am not getting best approach to do this but i am trying through java API.can you post me any sample code snippet to insert data in to elastic search.

{
   "test": {
      "mappings": {
         "testv1": {
            "properties": {
               "PROPERTY1": {
                  "type": "nested",
                  "properties": {
                     "key": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "value": {
                        "type": "string",
                        "index": "not_analyzed"
                     }
                  }
               },
               "PROPERTY2": {
                  "type": "nested",
                  "properties": {
                     "key": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "value": {
                        "type": "string",
                        "index": "not_analyzed"
                     }
                  }
               }
            }
         }
      }
   }
}

Thanks 

phani.
</comment><comment author="clintongormley" created="2014-11-07T14:38:08Z" id="62152790">@phani546 Please ask these questions in the mailing list: http://elasticsearch.org/community
</comment><comment author="phani546" created="2014-11-12T10:58:05Z" id="62701707">Hi clintongormley ,

   i followed the above procedure and done with index creation. from the builded index can i get the top 10 facets?

thanks,
phani.
</comment><comment author="binlaniua" created="2014-12-15T07:32:26Z" id="66958165">@clintongormley your solution is so perfect, i like it
</comment><comment author="kaleemullah" created="2015-10-15T04:59:30Z" id="148281971">@clintongormley Thanks for the solution but how can we filter aggregations with nested docs? Following above example, Lets say I choose 'android' as OS then aggregations should filter other buckets 'Core' and 'RAM' only for 'android' but also it should not hide the 'ios' key in 'OS' so that i can see other 'os' options also. is there any way to implement this? Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated date_formats in dynamic_mapping docs to new dynamic_date_formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5788</link><project id="" key="" /><description /><key id="31403448">5788</key><summary>Updated date_formats in dynamic_mapping docs to new dynamic_date_formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">YousefED</reporter><labels /><created>2014-04-13T12:28:59Z</created><updated>2014-07-16T21:46:28Z</updated><resolved>2014-04-15T13:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T07:43:42Z" id="40340392">`date_formats` is still supported after a quick glance, but I guess you want to make sure, that input and output are the same?

Can you sign the CLA please, so I can get it in? See http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="YousefED" created="2014-04-14T09:10:06Z" id="40346302">Done. It shouldn't matter whether it's still supported, it's been renamed so I thought let's rename this as well since I ran into it.
</comment><comment author="clintongormley" created="2014-04-15T13:44:46Z" id="40482557">Thanks. Merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[REST SPEC] separate api methods for benchmarks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5787</link><project id="" key="" /><description>I think the API spec should probably define three separate methods for the new benchmark endpoint. To emulate the [options available here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/rest/action/bench/RestBenchAction.java#L61-L72) I think we need list, create, and abort methods.

With the current spec, a user would will have to define the HTTP method to use in order to pick between those options.
</description><key id="31389780">5787</key><summary>[REST SPEC] separate api methods for benchmarks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels /><created>2014-04-12T19:42:54Z</created><updated>2014-04-15T03:30:07Z</updated><resolved>2014-04-15T03:30:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-04-14T22:21:07Z" id="40425403">@spenceralger I think most of the API endpoints are overloaded to work with multiple HTTP methods, no? What is particular to this endpoint that it causes a problem? 
</comment><comment author="spalger" created="2014-04-14T22:44:45Z" id="40427255">Yes and no. While the HTTP methods can be modified per request, that endpoint represents three logical action/methods. In similar cases we have created multiple methods preconfigured for each use (ie, index and create).
</comment><comment author="aleph-zero" created="2014-04-14T23:00:37Z" id="40428441">Would it be sufficient to do something like what the snapshot code does? It defines several JSON spec files for different methods. The URL endpoints are still overloaded, e.g. /_snapshot/{repository}/{snapshot} supports GET and DELETE. 
See: https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/snapshot.delete.json and https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/snapshot.get.json. 
</comment><comment author="spalger" created="2014-04-14T23:21:33Z" id="40429796">Exactly, that is what I meant to suggest.
</comment><comment author="aleph-zero" created="2014-04-14T23:43:03Z" id="40431122">Okay, I'll submit a PR with those changes.
</comment><comment author="spalger" created="2014-04-15T03:16:20Z" id="40441791">@aleph-zero had a question about benchmark.submit, but either way it looks good!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] benchmark docs not published</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5786</link><project id="" key="" /><description>Not sure if this was intentional but the [benchmark docs](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/indices/benchmark.asciidoc) are not published on elasticsearch.org.

This is an issue because a link to the docs was added to the [api spec](https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/bench.json), which is used to generate the JavaScript client's docs, but the page doesn't exist.

PS: The asciidoc file is within the indices directory, but the doc's ID is prefixed with search. Not sure if this should be in the [indices section](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/indices.asciidoc), the [search section](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search.asciidoc), or both...
</description><key id="31388064">5786</key><summary>[DOCS] benchmark docs not published</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels><label>docs</label></labels><created>2014-04-12T18:07:17Z</created><updated>2014-04-14T21:16:50Z</updated><resolved>2014-04-14T21:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-12T18:37:08Z" id="40288140">@aleph-zero can you fix the docs such taht this actually shows up?
</comment><comment author="aleph-zero" created="2014-04-14T21:10:52Z" id="40418681">I have moved the benchmark docs to go under the search section. Eventually I would imagine we'll have a separate benchmark section, but for now since benchmarks are only for searching I think it makes sense to have it there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't lookup version for auto generated id and create</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5785</link><project id="" key="" /><description>When a create document is executed, and its an auto generated id (based on UUID), we know that the document will not exists in the index, so there is no need to try and lookup the version from the index.
For many cases, like logging, where ids are auto generated, this can improve the indexing performance, specifically for lightweight documents where analysis is not a big part of the execution.
</description><key id="31380479">5785</key><summary>Don't lookup version for auto generated id and create</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-12T10:12:20Z</created><updated>2015-06-07T14:31:41Z</updated><resolved>2014-04-14T08:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-12T18:38:20Z" id="40288165">I don't think this should go into a bugfix release. It's an optimization rather than a bugfix so we should  only push this to `1.2`
</comment><comment author="kimchy" created="2014-04-14T07:22:22Z" id="40339071">don't have strong feeling about it, update the pull request to check on version 1.2.
</comment><comment author="s1monw" created="2014-04-14T08:04:48Z" id="40341796">LGTM
</comment><comment author="kimchy" created="2014-04-14T08:07:51Z" id="40342008">pushed.
</comment><comment author="s1monw" created="2014-04-14T10:18:55Z" id="40351334">I reverted this temporarily since it caused test-failures.
</comment><comment author="s1monw" created="2014-04-14T10:20:49Z" id="40351490">My suspicion here is that some docs get indexed more than once if we have a recovery situation since we do winged migration ie. index into the new copy and replay from translog so stuff can come in twice.
</comment><comment author="kimchy" created="2014-04-15T10:43:44Z" id="40467990">fyi, I created a branch on es repo: https://github.com/elasticsearch/elasticsearch/tree/enhacement/index_auto_generated_id, it includes a proposed fix for this (though not very clean)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added Twitter Storehaus client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5784</link><project id="" key="" /><description>Added Twitter Storehaus client
</description><key id="31370826">5784</key><summary>Added Twitter Storehaus client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">MansurAshraf</reporter><labels /><created>2014-04-11T23:46:29Z</created><updated>2014-07-16T21:46:29Z</updated><resolved>2014-05-02T10:09:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T07:27:46Z" id="40339393">Hey,

Can you sign our CLA at http://www.elasticsearch.org/contributor-agreement/ - to get the PR in?

Thanks a lot!
</comment><comment author="MansurAshraf" created="2014-04-15T01:09:57Z" id="40435747">done!
</comment><comment author="spinscale" created="2014-05-02T10:09:37Z" id="42012883">added by https://github.com/elasticsearch/elasticsearch/commit/d5f90e9803593afc2783cfd1e2f4d14c8903409a

Thanks a lot and sorry for the long waiting time!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HasChild query picks wrong type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5783</link><project id="" key="" /><description>Given the following setup:

```
curl -XPUT 'http://localhost:9200/haschildtest/'

curl -XPUT 'localhost:9200/haschildtest/posts/_mapping' -d '
{
   "posts":{
      "_parent":{
         "type":"features"
      },
      "_routing":{
         "required":true
      }
   }
}'

curl -XPUT 'localhost:9200/haschildtest/features/feature1' -d '
{
   "title": "feature title 1"
}'

curl -XPUT 'localhost:9200/haschildtest/posts/post1?parent=feature1' -d '
{
   "specials":{
      "title": "jack"
   }
}'

curl -XPUT 'localhost:9200/haschildtest/specials/special1' -d '
{
   "title": "this somehow interferes with the has_child query"
}'
```

This query runs correctly:

```
curl -XPOST 'localhost:9200/haschildtest/features/_search?pretty=true' -d '
{
   "query": {
      "has_child": {
         "type": "posts",
         "query": {
            "match": {
               "specials.title": "jack"
            }
         }
      }
   }
}'
```

Whereas this query with the parent type specifed after the query, fails:

```
curl -XPOST 'localhost:9200/haschildtest/features/_search?pretty=true' -d '
{
   "query": {
      "has_child": {
         "query": {
            "match": {
               "specials.title": "jack"
            }
         },
         "type": "posts"
      }
   }
}'
```

Somehow the second example is picking the `specials` type `title` field instead of the type specified in the `has_child` block. I have tested this on ES 1.0.2 and took me a while to figure out what was going on...
</description><key id="31367280">5783</key><summary>HasChild query picks wrong type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">MrHash</reporter><labels><label>bug</label></labels><created>2014-04-11T22:26:50Z</created><updated>2014-07-11T06:53:34Z</updated><resolved>2014-04-22T15:29:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T13:36:41Z" id="40481646">Also see #5399
</comment><comment author="MrHash" created="2014-04-15T20:54:32Z" id="40533024">I'm not entirely sure if these are related issues. The curiosity here is the position of `type` in the query. Why should its positioning after the match query affect the results?
</comment><comment author="s1monw" created="2014-04-15T21:08:24Z" id="40534598">I think it's simply ignored and then you don't have a type restriction seems like a bug
</comment><comment author="dakrone" created="2014-04-15T23:14:58Z" id="40545947">@MrHash this is a bug with the way that `has_child` queries are parsed before parsing out the "type" field, I'm working on a fix for this.
</comment><comment author="fanbiao" created="2014-07-03T04:04:28Z" id="47864787">when i use has_child to index and search ,i get no result return ,why?i have use losts of es version.please help me.
</comment><comment author="clintongormley" created="2014-07-03T10:29:10Z" id="47890471">@fanbiao Please ask these questions on the mailing list
</comment><comment author="fanbiao" created="2014-07-11T06:53:34Z" id="48699667">we could not retrieve the child document _source when i use parent-child query,how can i do that?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix detection of unsupported fields with validate API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5782</link><project id="" key="" /><description>The validate API was failing to reject JSON input that had unsupported fields placed after a supported field. This was causing invalid requests to be reported as valid.

First commit adds a test that triggers the issue. Second commit fixes it.

Ref. #5685
</description><key id="31354269">5782</key><summary>Fix detection of unsupported fields with validate API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">amarandon</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T19:12:49Z</created><updated>2015-06-07T14:46:40Z</updated><resolved>2014-05-12T10:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-10T00:37:51Z" id="42726544">Hi @amarandon, could you please sign our [CLA](elasticsearch.org/contributor-agreement/) so that we can properly review and pull in your changes?
</comment><comment author="amarandon" created="2014-05-10T09:38:09Z" id="42737356">@javanna I've just signed it.
</comment><comment author="javanna" created="2014-05-12T10:53:51Z" id="42818679">Merged, thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suboptimal performance when trying to get ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5781</link><project id="" key="" /><description>I have dense set of documents with ids from 1 to roughly 80 000 000. The whole db takes 70gb on disk.

I'm trying to iterate items in database with the following query:

``` json
{"size":20000,"fields":[],"query":{"filtered":{"filter":{"bool":{"must":[{"term":{"is_mobile":true}},{"range":{"user_id":{"gt":0,"lte":20000},"_cache":false}}]}}}}}
```

Here range changes every requests by 20 000 forward. There are around 1 000 mobile users per 20 000 users if that matters.

When I set size to 0 iteration finishes very quickly and each iteration takes 1-10ms,
but when I set size to 20 000 every step takes up to 200-300ms. The root cause of it
is fetch stage: with size=0 disk io is ~10mbps, with size=20000 disk io is ~220mbps.

Here are some screenshots from bigdesk during my tests:

![one](http://puu.sh/84Br6.png)
![two](http://puu.sh/84BrU.png)
![three](http://puu.sh/84Bte.png)

First test with size=0 has spike in query time, second test with size=20000 has spike in fetch time.

I'm using elasticsearch 1.1.0 and my first idea was that sorting was to blame like in #5573, but restored node behaved the same. This node has 10g of heap, 2g of field data cache and 3g of filter cache, total ram is 16g.

Is there a reason to fetch 220 megabytes from disk when the only thing I need from a document is an id? Looks like this instance was much faster a week ago, configuration and search/indexing didn't change.

4.5 minutes \* 60 seconds \* 220mbps = 60 gigabytes read from disk during second test, but only 19 200 000 docs were fetched. How can I figure out what causes such load?

Let me know if I could provide more info.
</description><key id="31341456">5781</key><summary>Suboptimal performance when trying to get ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bobrik</reporter><labels /><created>2014-04-11T16:27:38Z</created><updated>2015-01-15T10:25:31Z</updated><resolved>2015-01-15T10:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-11T17:33:15Z" id="40229009">I guess this behavior is expected. Operating systems generally write and read entire blocks of data at once. Let's take a block size of 4KB, this means that reading a random byte on disk might require to load a full 4-KB block into memory. So if we read 20000 random documents from disk, this means that we could expect the disk to read about 20000 \* 4096 bytes ~ 80MB.

This doesn't happen with query execution since query execution reads are sequential, so two bytes that are read consecutively are likely to be in the same block. On the other hand, stored fields reads are typically random.

This phenomenon is probably amplified a bit by the fact that Lucene stores data into blocks of about 16KB (uncompressed, so assuming a compression of 60%, which is quite common, this would make the block take about 10KB on disk). But I wanted to talk about the operating system first to make clear that it is just amplifying a bit something that is already happening at the operating system level.

Databases in general and Elasticsearch in particular are not good at fetching high numbers of random records. If you want to get such large numbers of documents from Elasticsearch, I would recommend using SCAN[1] whenever possible.

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#scan
</comment><comment author="bobrik" created="2014-04-11T18:11:51Z" id="40236629">You missed the part about 1/20 mobile users so 20000 \* 4096 bytes ~ 80MB turns out to be more like 4MB per pack. Since all search requests are sequential and take 200ms+, there are at most 4MB \* 1000ms / 200ms ~ 20MB which is an order of magnitude lower than 220MB. Also, fs cache should help when you request the same block repeatedly, but that cold not happen in this case actually.

Doesn't elasticsearch know document ids if it could respond with total hits per query?

I'll try to backup and rebuild the whole index from scratch anyway, just to be sure.
</comment><comment author="jpountz" created="2014-04-11T19:08:20Z" id="40242410">Indeed, I missed the 1/20 thing. How large are your documents on average?

&gt;  Also, fs cache should help when you request the same block repeatedly

Indeed, but you have 16GB of RAM, and 10 of them go to the JVM so the FS cache has at most 6GB for 70GB of index. The chances of a cache miss are very high.

&gt; Doesn't elasticsearch know document ids if it could respond with total hits per query?

Elasticsearch internally use Lucene doc IDs to count matches, which are just the ordinals of the document within a segment. For example, if a segment has 3 documents, the first will have `0` as an identifier, the second `1`, etc. On the other hand, the `_id` of documents are stored in Lucene stored fields, just like `_source`.
</comment><comment author="bobrik" created="2014-04-11T19:22:14Z" id="40243764">Docs are around 1-2kb on average, mobile users are usually 2kb.

Turns out that `"fields": []` could only save me network bandwidth if `_id` is stored in fields.

Anyway, reindexing is in progress :)
</comment><comment author="bobrik" created="2014-04-14T06:15:01Z" id="40335903">Well, I reindexed dataset, it's 63.4g on disk after two days of updates.

I disabled updates, cleared caches, removed `is_mobile` condition, and changed `size` to 1000:

``` json
{"size":1000,"fields":[],"query":{"filtered":{"filter":{"bool":{"must":[{"range":{"user_id":{"gt":0,"lte":1000},"_cache":false}}]}}}}}
```

I queried packs sequentially with ids up to 400 000, here is output from my script:

```
1000: 1000/1000 results in 614.9781ms
2000: 1000/1000 results in 205.9588ms
3000: 1000/1000 results in 151.7389ms
4000: 1000/1000 results in 207.2759ms
5000: 1000/1000 results in 201.7291ms
6000: 1000/1000 results in 173.6081ms
7000: 1000/1000 results in 169.4219ms
8000: 1000/1000 results in 199.9931ms
9000: 1000/1000 results in 202.5528ms
10000: 1000/1000 results in 181.7811ms
11000: 1000/1000 results in 178.7648ms
12000: 1000/1000 results in 199.1701ms
13000: 1000/1000 results in 204.5979ms
14000: 1000/1000 results in 156.7390ms
15000: 1000/1000 results in 172.9791ms
16000: 1000/1000 results in 253.1419ms
17000: 1000/1000 results in 165.1721ms
18000: 1000/1000 results in 174.8121ms
19000: 1000/1000 results in 189.6560ms
...
391000: 1000/1000 results in 169.0459ms
392000: 1000/1000 results in 183.8639ms
393000: 1000/1000 results in 184.2752ms
394000: 1000/1000 results in 200.5951ms
395000: 1000/1000 results in 163.5761ms
396000: 1000/1000 results in 162.3139ms
397000: 1000/1000 results in 134.5751ms
398000: 1000/1000 results in 185.7009ms
399000: 1000/1000 results in 167.1751ms
400000: 1000/1000 results in 160.0850ms
401000: 1000/1000 results in 141.7110ms
402000: 1000/1000 results in 162.6661ms
```

Average time is 168.206ms, for last 50 requests it's 159.316, that's 6.25 rps.

6.25 rps \* 1000 items in pack \* 4kb block = 25600000.0 bytes = 24.4140625MB.

Let's look at the pictures:

![one](http://puu.sh/882Tu.png)
![two](http://puu.sh/882Ul.png)
![three](http://puu.sh/882UP.png)
![four](http://puu.sh/882Vs.png)

And `iostat -x -d 10` before:

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00   321.20    1.50   54.60     7.20  3900.80   139.32     0.30    5.40   56.67    3.99   0.70   3.90

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00   108.80    0.00   13.60     0.00   865.60   127.29     0.20   14.85    0.00   14.85   1.40   1.90

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     7.20    0.00    6.50     0.00   625.60   192.49     0.09   13.08    0.00   13.08   5.23   3.40

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     7.30    0.40   10.20     1.60   876.00   165.58     0.05    4.81    0.00    5.00   1.70   1.80
```

In progress of searches:

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    33.80  191.30   24.90 18828.00  1283.20   186.04     0.79    3.63    2.59   11.65   0.62  13.40

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    14.70 2231.10    5.40 230444.00    96.40   206.16     5.09    2.28    2.28    0.00   0.42  94.30

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.90 2495.00    0.90 227958.40    11.20   182.68     5.57    2.23    2.23    2.22   0.38  94.20

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.70 2495.50    0.60 227886.40     5.20   182.60     5.60    2.25    2.25    0.00   0.38  94.00

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               1.00     0.60 2483.90    1.60 226898.40    24.00   182.60     5.55    2.23    2.23    0.00   0.38  93.30

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.50 2469.10    0.60 227619.20     4.40   184.33     5.47    2.22    2.22    0.00   0.38  93.80

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda              49.70    77.50 2712.10   16.50 227116.00   387.20   166.75     5.96    2.18    2.19    0.97   0.34  93.90

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    21.40 1645.90    6.50 150786.40   112.80   182.64     3.66    2.22    2.22    0.62   0.38  62.10
```

And after:

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    15.40    3.20   12.20    26.00   118.40    18.75     0.05    3.38    1.88    3.77   0.39   0.60

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.20    0.00    1.20     0.00    48.40    80.67     0.02   13.33    0.00   13.33  13.33   1.60

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     2.00    2.30    0.80    21.60    11.20    21.16     0.28   91.61   95.65   80.00  16.13   5.00
```

Well, on average there were 227886.40KB read from disk in 2495.50 read requests per second, this is quite far away from expected 24.5 megabytes. Average read size of 91KB is worrying.
</comment><comment author="bobrik" created="2014-04-29T06:51:03Z" id="41646260">I tried to update to 1.1.1, that didn't help. Then I tried to switch to `niofs`, that helped. I wanted to measure effect and switched back to `mmapfs` and all of a sudden reads become faster. Maybe that's because db on disk now takes 58gb (79m docs, 18m deleted, no merges happened during tests below).

After clearing caches (both es and page) this is what I see now with `mmapfs`:

```
1000: 1000/1000 results in 574.1880ms
2000: 1000/1000 results in 136.8101ms
3000: 1000/1000 results in 131.5050ms
4000: 1000/1000 results in 196.9988ms
5000: 1000/1000 results in 129.2441ms
6000: 1000/1000 results in 114.4180ms
7000: 1000/1000 results in 125.1080ms
8000: 1000/1000 results in 125.5410ms
9000: 1000/1000 results in 137.8851ms
10000: 1000/1000 results in 421.0851ms
11000: 1000/1000 results in 132.7579ms
12000: 1000/1000 results in 139.8439ms
13000: 1000/1000 results in 147.3451ms
14000: 1000/1000 results in 128.7601ms
15000: 1000/1000 results in 124.6841ms
16000: 1000/1000 results in 160.6929ms
17000: 1000/1000 results in 102.9320ms
18000: 1000/1000 results in 124.1219ms
19000: 1000/1000 results in 136.3180ms
20000: 1000/1000 results in 132.3440ms
21000: 1000/1000 results in 119.6802ms
22000: 1000/1000 results in 131.2690ms
23000: 1000/1000 results in 119.5798ms
24000: 1000/1000 results in 105.7951ms
25000: 1000/1000 results in 74.1820ms
26000: 1000/1000 results in 74.0089ms
27000: 1000/1000 results in 70.2801ms
28000: 1000/1000 results in 105.9880ms
29000: 1000/1000 results in 69.2880ms
30000: 1000/1000 results in 78.7618ms
31000: 1000/1000 results in 107.4059ms
32000: 1000/1000 results in 105.9480ms
33000: 1000/1000 results in 171.9649ms
34000: 1000/1000 results in 142.6880ms
35000: 1000/1000 results in 128.8581ms
36000: 1000/1000 results in 121.4101ms
37000: 1000/1000 results in 125.2351ms
38000: 1000/1000 results in 115.6292ms
...
382000: 1000/1000 results in 82.3901ms
383000: 1000/1000 results in 72.6929ms
384000: 1000/1000 results in 92.5410ms
385000: 1000/1000 results in 92.0448ms
386000: 1000/1000 results in 67.9412ms
387000: 1000/1000 results in 85.4239ms
388000: 1000/1000 results in 79.0410ms
389000: 1000/1000 results in 85.2480ms
390000: 1000/1000 results in 71.1071ms
391000: 1000/1000 results in 88.0740ms
392000: 1000/1000 results in 98.4831ms
393000: 1000/1000 results in 101.9111ms
394000: 1000/1000 results in 90.3878ms
395000: 1000/1000 results in 62.3178ms
396000: 1000/1000 results in 87.1019ms
397000: 1000/1000 results in 83.6191ms
398000: 1000/1000 results in 99.5011ms
399000: 1000/1000 results in 88.6149ms
400000: 1000/1000 results in 92.9849ms
401000: 1000/1000 results in 81.6429ms
402000: 1000/1000 results in 93.7450ms
403000: 1000/1000 results in 85.8731ms
404000: 1000/1000 results in 76.5259ms
405000: 1000/1000 results in 68.8400ms
406000: 1000/1000 results in 80.6811ms
407000: 1000/1000 results in 91.7079ms
408000: 1000/1000 results in 76.2289ms
409000: 1000/1000 results in 81.4910ms
410000: 1000/1000 results in 71.6860ms
```

Reading from non-zero offset works quite well too:

```
10001000: 1000/1000 results in 167.1340ms
10002000: 1000/1000 results in 93.1280ms
10003000: 1000/1000 results in 74.7740ms
10004000: 1000/1000 results in 111.3100ms
10005000: 1000/1000 results in 72.1872ms
10006000: 1000/1000 results in 90.1690ms
10007000: 1000/1000 results in 97.9781ms
10008000: 1000/1000 results in 93.0998ms
10009000: 1000/1000 results in 89.1528ms
10010000: 1000/1000 results in 96.3719ms
10011000: 1000/1000 results in 77.0710ms
10012000: 1000/1000 results in 69.8390ms
10013000: 998/998 results in 65.2430ms
10014000: 1000/1000 results in 94.7330ms
10015000: 0/0 results in 3.0620ms
10016000: 999/999 results in 90.8070ms
10017000: 1000/1000 results in 93.3099ms
10018000: 1000/1000 results in 89.5600ms
10019000: 1000/1000 results in 88.2859ms
10020000: 1000/1000 results in 91.9721ms
10021000: 1000/1000 results in 88.1851ms
10022000: 999/999 results in 80.3919ms
10023000: 1000/1000 results in 82.9122ms
10024000: 999/999 results in 92.6580ms
10025000: 1000/1000 results in 91.4960ms
10026000: 1000/1000 results in 106.1878ms
10027000: 1000/1000 results in 88.2740ms
...
10424000: 999/999 results in 84.5432ms
10425000: 1000/1000 results in 77.2529ms
10426000: 1000/1000 results in 81.6329ms
10427000: 1000/1000 results in 76.0500ms
10428000: 1000/1000 results in 77.8770ms
10429000: 999/999 results in 92.4761ms
10430000: 1000/1000 results in 89.7510ms
10431000: 1000/1000 results in 90.7910ms
10432000: 999/999 results in 66.0632ms
10433000: 1000/1000 results in 93.5569ms
10434000: 1000/1000 results in 105.3710ms
10435000: 1000/1000 results in 71.0671ms
10436000: 1000/1000 results in 90.0891ms
10437000: 1000/1000 results in 81.4419ms
10438000: 1000/1000 results in 69.7391ms
10439000: 1000/1000 results in 73.6330ms
10440000: 1000/1000 results in 85.9270ms
```

There are corresponding iostat outputs:

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10     1.90 2943.40    4.10 220123.20    34.40   149.39     6.15    2.09    2.09    0.24   0.30  89.60

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.30 3571.60    0.40 216859.60     2.80   121.42     6.61    1.85    1.85    0.00   0.25  89.50

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.90 4053.20    0.50 211587.60     5.60   104.40     7.39    1.82    1.82    0.00   0.22  88.60

 Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.70 3688.40    2.50 181295.20    31.20    98.26     6.73    1.83    1.83    0.80   0.20  75.60
```

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.20 3504.20    0.70 184944.00     8.00   105.54     6.73    1.92    1.91   24.29   0.22  78.30

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.90 4225.50    0.60 211811.20    33.60   100.26     7.81    1.85    1.85    0.00   0.21  88.20

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00    10.30 4370.50    6.00 209556.40    84.80    95.80     7.78    1.78    1.78    0.67   0.20  87.70

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.70 4254.30    0.60 197814.00     9.20    92.99     7.60    1.79    1.79    0.00   0.19  82.60
```

Same for `niofs`:

```
1000: 1000/1000 results in 496.1381ms
2000: 1000/1000 results in 74.4381ms
3000: 1000/1000 results in 73.3318ms
4000: 1000/1000 results in 79.1049ms
5000: 1000/1000 results in 72.4850ms
6000: 1000/1000 results in 63.8230ms
7000: 1000/1000 results in 61.4109ms
8000: 1000/1000 results in 70.1060ms
9000: 1000/1000 results in 63.4592ms
10000: 1000/1000 results in 74.9750ms
11000: 1000/1000 results in 67.6420ms
12000: 1000/1000 results in 74.9049ms
13000: 1000/1000 results in 73.5261ms
14000: 1000/1000 results in 60.2379ms
15000: 1000/1000 results in 54.9819ms
16000: 1000/1000 results in 67.2531ms
17000: 1000/1000 results in 58.7561ms
18000: 1000/1000 results in 68.6781ms
19000: 1000/1000 results in 73.9031ms
20000: 1000/1000 results in 69.7479ms
21000: 1000/1000 results in 62.8011ms
22000: 1000/1000 results in 65.1691ms
23000: 1000/1000 results in 69.0689ms
24000: 1000/1000 results in 55.1078ms
25000: 1000/1000 results in 40.6590ms
26000: 1000/1000 results in 39.3720ms
27000: 1000/1000 results in 37.1220ms
28000: 1000/1000 results in 52.6612ms
29000: 1000/1000 results in 45.9480ms
30000: 1000/1000 results in 40.6678ms
31000: 1000/1000 results in 62.4180ms
32000: 1000/1000 results in 67.7090ms
33000: 1000/1000 results in 117.3420ms
34000: 1000/1000 results in 83.6558ms
35000: 1000/1000 results in 80.6820ms
36000: 1000/1000 results in 62.2151ms
37000: 1000/1000 results in 72.4199ms
38000: 1000/1000 results in 68.6040ms
39000: 1000/1000 results in 75.7370ms
...
378000: 1000/1000 results in 52.8140ms
379000: 1000/1000 results in 40.0920ms
380000: 1000/1000 results in 47.5650ms
381000: 1000/1000 results in 51.1720ms
382000: 1000/1000 results in 49.6240ms
383000: 1000/1000 results in 43.7841ms
384000: 1000/1000 results in 48.2061ms
385000: 1000/1000 results in 51.2450ms
386000: 1000/1000 results in 45.0971ms
387000: 1000/1000 results in 46.7429ms
388000: 1000/1000 results in 46.7849ms
389000: 1000/1000 results in 49.4158ms
390000: 1000/1000 results in 37.8931ms
391000: 1000/1000 results in 47.8899ms
392000: 1000/1000 results in 52.6190ms
393000: 1000/1000 results in 56.3581ms
394000: 1000/1000 results in 54.5709ms
395000: 1000/1000 results in 39.5730ms
396000: 1000/1000 results in 44.1380ms
397000: 1000/1000 results in 51.1749ms
398000: 1000/1000 results in 59.4790ms
399000: 1000/1000 results in 57.6718ms
400000: 1000/1000 results in 48.2280ms
401000: 1000/1000 results in 51.0840ms
```

```
1001000: 1000/1000 results in 74.7449ms
1002000: 1000/1000 results in 53.4320ms
1003000: 1000/1000 results in 54.7791ms
1004000: 1000/1000 results in 52.4430ms
1005000: 1000/1000 results in 46.8009ms
1006000: 1000/1000 results in 47.2221ms
1007000: 1000/1000 results in 44.2469ms
1008000: 1000/1000 results in 42.1610ms
1009000: 1000/1000 results in 39.5839ms
1010000: 1000/1000 results in 40.0970ms
1011000: 1000/1000 results in 40.9501ms
1012000: 1000/1000 results in 38.7561ms
1013000: 1000/1000 results in 40.1518ms
1014000: 1000/1000 results in 43.7870ms
1015000: 1000/1000 results in 40.2260ms
1016000: 1000/1000 results in 35.7931ms
1017000: 1000/1000 results in 41.6169ms
1018000: 1000/1000 results in 43.9320ms
1019000: 1000/1000 results in 45.1200ms
1020000: 1000/1000 results in 40.1652ms
1021000: 1000/1000 results in 39.2389ms
1022000: 1000/1000 results in 41.3420ms
1023000: 1000/1000 results in 50.7259ms
1024000: 1000/1000 results in 44.7628ms
1025000: 1000/1000 results in 35.8820ms
1026000: 1000/1000 results in 43.0739ms
1027000: 1000/1000 results in 36.7758ms
1028000: 1000/1000 results in 36.2608ms
1029000: 1000/1000 results in 38.4901ms
1030000: 1000/1000 results in 37.1521ms
1031000: 1000/1000 results in 40.0848ms
1032000: 1000/1000 results in 43.8211ms
1033000: 1000/1000 results in 51.0271ms
1034000: 1000/1000 results in 40.8111ms
1035000: 1000/1000 results in 44.4250ms
1036000: 1000/1000 results in 38.4030ms
1037000: 1000/1000 results in 38.2321ms
1038000: 1000/1000 results in 35.8500ms
1039000: 1000/1000 results in 31.7190ms
...
1425000: 1000/1000 results in 47.9991ms
1426000: 1000/1000 results in 46.0401ms
1427000: 1000/1000 results in 40.9181ms
1428000: 1000/1000 results in 44.3571ms
1429000: 1000/1000 results in 45.2061ms
1430000: 1000/1000 results in 48.6059ms
1431000: 1000/1000 results in 43.0620ms
1432000: 1000/1000 results in 37.7240ms
1433000: 1000/1000 results in 35.8810ms
1434000: 1000/1000 results in 42.1000ms
1435000: 1000/1000 results in 48.0080ms
1436000: 1000/1000 results in 123.9400ms
1437000: 1000/1000 results in 35.1648ms
1438000: 1000/1000 results in 53.0531ms
1439000: 1000/1000 results in 37.2989ms
1440000: 1000/1000 results in 41.7180ms
1441000: 1000/1000 results in 43.6361ms
1442000: 1000/1000 results in 45.9149ms
1443000: 1000/1000 results in 52.1169ms
```

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.40 2112.90    0.80 39818.00     4.80    37.68     1.09    0.52    0.52    0.00   0.12  25.10

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.10 7294.50    2.60 146462.80    14.80    40.15     3.84    0.53    0.53    0.00   0.10  74.90

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.60 5995.10    1.30 119229.60    16.40    39.77     3.35    0.56    0.56    0.00   0.10  61.90
```

```
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.50 6329.80    0.60 147168.80    12.00    46.50     4.26    0.67    0.67    0.00   0.12  75.20

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     1.00 4711.00    1.00 165078.80    23.60    70.08     4.32    0.92    0.92    0.00   0.17  78.60

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.50  989.20    0.70 37534.40     4.80    75.84     0.66    0.66    0.63   42.86   0.23  22.30
```

`niofs` still better in terms of io utilization and speed: 97.1836ms and 86.3708ms vs 66.6095ms and 49.4988ms on average for runs above.

I also measured more real-life workloads: fetch 4.5m docs from 20 parallel clients. Restart, drop all caches, wait for indexing to catch up, measure 2 times. Here's what I've got:

`mmapfs`:
1. Avg search time: 3245.3082466ms, total search time: 4053390ms
2. Avg search time: 3232.51321057ms, total search time: 4037409ms

io for both measurements:

![io](http://puu.sh/8rX3Q.png)

`niofs`:
1. Avg search time: 1692.49319456ms, total search time: 2113924ms
2. Avg search time: 1717.33706966ms, total search time: 2144954ms

io for both measurements:

![io](http://puu.sh/8rXhi.png)

That is 2x difference in speed with similar io utilization.

Any idea what could be the reason for misbehaving `mmapfs`?
</comment><comment author="jpountz" created="2014-04-29T07:13:55Z" id="41647496">This is a big difference. What Java version are you using? In case it is an old version, it would be interesting to try it with Java [7u55](http://www.elasticsearch.org/blog/java-1-7u55-safe-use-elasticsearch-lucene/) or maybe even a Java 8 build, [which seems to have better mmap performance](http://search-lucene.com/m/WwzTb2mGQps).
</comment><comment author="bobrik" created="2014-04-29T07:17:46Z" id="41647729">```
java -version
java version "1.7.0_17"
Java(TM) SE Runtime Environment (build 1.7.0_17-b02)
Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)
```

I'll try to update and let you know.
</comment><comment author="bobrik" created="2014-12-17T11:23:23Z" id="67309435">I checked es 1.4.1 with different java versions and different storage types. Testing looked like this:
1. Set java version and store type in config
2. Restart Elasticsearch
3. Wait until indexing is up to date with queue
4. Create task to iterate the data (1 or 20 parallel workers, 6.6m docs, max 5k docs per result set)

No reindexing happened between the tests. Index is 77gb, 90m docs.

Results are here:
- java 1.7.0_17 + default store type:
  - 20 workers: 3849ms avg, 6609816ms total
  - 1 worker: 241ms avg, 414071ms total
- java 1.7.0_17 + niofs store type:
  - 20 workers: 3259ms avg, 5596472ms total
  - 1 worker: 242ms avg, 416553ms total
- java 1.8.0_25 + default store type:
  - 20 workers: 3612ms avg, 6202648ms total
  - 1 worker: 248ms avg, 426188ms total
- java 1.8.0_25 + niofs store type:
  - 20 workers: 3378ms avg, 5801105ms total
  - 1 worker: 242ms avg, 415881ms total

All results are pretty close, but worse than with niofs and 1.1.1. I haven't tried downgrading to 1.1.1, though. Any thoughts?
</comment><comment author="jpountz" created="2014-12-17T14:29:00Z" id="67329207">I just did some tests to try to see if there would be errors on the ES side, but things look ok. First in the simple case at least elasticsearch looks up each document only once. Then I hacked the source code to track the number of bytes that are read from the directory and things seem to work as expected as well: I tried various document sizes ranging from 200b to 20KB and the number of bytes read per document was between 9KB and 40KB, which is expected due to the way lucene stores stored fields. Given that the NIO directory has an internal buffer of 16KB to save system calls, I believe it could go up to 40+16=56KB in practice.

Regarding your new numbers, the new store now uses niofs for cold parts of the index (eg. stored fields) and mmap for hot parts (eg. the inverted index and doc values) so that would explain why the numbers in your last comment are similar. Regarding your previous experiments with `mmap`, I'm wondering if the slowness could be partially explained with fragmentation. (?)
</comment><comment author="bobrik" created="2015-01-15T10:25:31Z" id="70066152">Well, I'm going to close this for now since current performance is mostly ok for us.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure pending merges are updated on segment flushes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5780</link><project id="" key="" /><description>```
Due to the default of `async_merge` to `true` we never run
the merge policy on a segment flush which prevented the
pending merges from being updated and that caused actual
pending merges not to contribute to the merge decision.

This commit removes the `index.async.merge` setting. The setting is 
actually misleading since we take care of this on a different level 
(the merge scheduler) since 1.1.

This commit also adds an additional check when to run a refresh
since solely relying on the dirty flag might leave merges un-refreshed
which can cause search slowdowns and higher memory consumption.
```

Closes #5779
</description><key id="31341127">5780</key><summary>Ensure pending merges are updated on segment flushes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T16:23:04Z</created><updated>2015-06-07T20:59:23Z</updated><resolved>2014-04-12T08:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-11T16:33:36Z" id="40223349">LGTM, +1 on back porting to 1.1 and 1.x branch. I don't think we should backport to 1.0, since in 1.0 we didn't have the "enabled" merge scheduler support, right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merges might not be picked up when they are ready</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5779</link><project id="" key="" /><description>We currently play some tricks to make sure merges are not picked up on indexing threads such that the threadpool that is used for indexing is not consumed by merges. Yet, there are a couple of problems that prevent merges to be picked up since we are checking if the IW has pending merges but that datastructure might not be picked up due to a problem in the way we try to prevent the merges to happen. There is also a problem where we don't refresh a reader due to merges where we could / should do that. This does NOT have an impact on correctness but it can have an impact on the amount of segments that are used for searching and for resolving versions to. Both have performance impacts if the number of segments is large.
</description><key id="31339428">5779</key><summary>Merges might not be picked up when they are ready</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T16:00:11Z</created><updated>2014-05-02T18:26:10Z</updated><resolved>2014-04-12T08:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>_cat plugin endpoint throws an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5778</link><project id="" key="" /><description>A left-over header causes the _cat/plugin endpoint to throw : 

```
curl localhost:9200/_cat/plugins
{"error":"ElasticsearchIllegalStateException[mismatch on num
}
```
</description><key id="31333287">5778</key><summary>_cat plugin endpoint throws an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v1.1.1</label></labels><created>2014-04-11T14:46:18Z</created><updated>2014-04-15T06:40:48Z</updated><resolved>2014-04-11T14:50:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-04-11T14:50:26Z" id="40211922">Duplicate of #5715 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update core-types.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5777</link><project id="" key="" /><description>Missing bracket
</description><key id="31327722">5777</key><summary>Update core-types.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">eliasah</reporter><labels /><created>2014-04-11T13:35:32Z</created><updated>2014-07-16T21:46:30Z</updated><resolved>2014-04-15T13:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T07:27:20Z" id="40339372">Hey,

nice catch! Can you sign our CLA at http://www.elasticsearch.org/contributor-agreement/ - to get the PR in?
</comment><comment author="eliasah" created="2014-04-14T08:48:41Z" id="40344795">Done! Thanks :+1: 
</comment><comment author="clintongormley" created="2014-04-15T13:57:37Z" id="40484070">Merged. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Propagate percolate mapping changes to cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5776</link><project id="" key="" /><description>If the during percolating a new field was introduced in the local mapping service, then those changes should be updated in cluster state of the master as well.
</description><key id="31322927">5776</key><summary>Propagate percolate mapping changes to cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T12:17:43Z</created><updated>2015-06-07T20:59:58Z</updated><resolved>2014-04-15T12:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-11T12:46:04Z" id="40199337">LGTM, +1 on later brining the duplicate code into the same place.
</comment><comment author="s1monw" created="2014-04-14T09:02:55Z" id="40345776">LGTM +1 to push
</comment><comment author="s1monw" created="2014-04-15T12:35:24Z" id="40475896">@martijnvg I pushed this to all relevant branches
</comment><comment author="martijnvg" created="2014-04-16T02:22:25Z" id="40556670">Thanks Simon!

On 15 April 2014 19:35, Simon Willnauer notifications@github.com wrote:

&gt; @martijnvg https://github.com/martijnvg I pushed this to all relevant
&gt; branches
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/5776#issuecomment-40475896
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace InternalSearchResponse#EMPTY with InternalSearchResponse#empty()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5775</link><project id="" key="" /><description>Each search request should use a new InternalSearchResponse instance even in case when all shards return no hits. The InternalSearchResponse may get modified afterwards, so a new instance required at all times.
</description><key id="31317768">5775</key><summary>Replace InternalSearchResponse#EMPTY with InternalSearchResponse#empty()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T10:35:18Z</created><updated>2015-06-07T21:00:25Z</updated><resolved>2014-04-11T10:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-11T10:42:54Z" id="40191247">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"size" parameter for filtered aliases to prevent DoS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5774</link><project id="" key="" /><description>Hi,

To secure an ES installation (prevent DoS) it's necessary to restrict the number of returned documents by configuration (for all ES queries). Currently, it seems to be not possible to use the 'size' filter in filtered aliases. The "limit" filter isn't suitable, see e.g. https://stackoverflow.com/questions/21444623/elasticsearch-limit-filter-ambiguity for the difference.

Adrien mentioned (https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/86A5DbEP94E) that this would be also an interesting feature given that while Elasticsearch is fast at retrieving the top hits, going deeper in the result set can use lots of resources (both CPU-wise and memory-wise).

=&gt; Please implement the "size" parameter for filtered aliases.
=&gt; Are there any workarounds for this problem.

Thank you, BTW: ES is really great!
</description><key id="31312574">5774</key><summary>"size" parameter for filtered aliases to prevent DoS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">firla</reporter><labels /><created>2014-04-11T09:09:06Z</created><updated>2015-10-14T15:57:44Z</updated><resolved>2015-10-14T15:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-11T09:16:08Z" id="40185125">I think we should start with a limit on size that can be changed as a setting regardless of an alias, and actually hard code it to an acceptable value. Afterwards, I think we can add it as a feature to aliases, make sense?
</comment><comment author="firla" created="2014-04-11T09:36:58Z" id="40186680">Wow, lightning fast response and thats sounds like a fast first and durable perfect solution - This would be great!
</comment><comment author="firla" created="2014-06-17T18:47:31Z" id="46348704">Any news on this topic? Thanks.
</comment><comment author="dakrone" created="2015-04-10T17:09:29Z" id="91623634">Related to #9311
</comment><comment author="clintongormley" created="2015-10-14T15:57:44Z" id="148097831">Closing in favour of https://github.com/elastic/elasticsearch/issues/14116
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Valid complex polygons fail to parse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5773</link><project id="" key="" /><description>posting certain valid geojson polygons results in the following exception:

org.elasticsearch.index.mapper.MapperParsingException: failed to parse [geometry] at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:249)
...

curl -XDELETE 'http://localhost:9200/test'

curl -XPOST 'http://localhost:9200/test' -d '{
  "mappings":{
    "test":{
      "properties":{
        "geometry":{
          "type":"geo_shape",
          "tree":"quadtree",
          "tree_levels":14,
          "distance_error_pct":0.0
        }
      }
    }
  }
}'

curl -XPOST 'http://localhost:9200/test/test/1' -d '{
  "geometry":{
    "type":"Polygon",
    "coordinates":[
      [[-85.0018514,37.1311314],
       [-85.0016645,37.1315293],
       [-85.0016246,37.1317069],
       [-85.0016526,37.1318183],
       [-85.0017119,37.1319196],
       [-85.0019371,37.1321182],
       [-85.0019972,37.1322115],
       [-85.0019942,37.1323234],
       [-85.0019543,37.1324336],
       [-85.001906,37.1324985],
       [-85.001834,37.1325497],
       [-85.0016965,37.1325907],
       [-85.0016011,37.1325873],
       [-85.0014816,37.1325353],
       [-85.0011755,37.1323509],
       [-85.000955,37.1322802],
       [-85.0006241,37.1322529],
       [-85.0000002,37.1322307],
       [-84.9994,37.1323001],
       [-84.999109,37.1322864],
       [-84.998934,37.1322415],
       [-84.9988639,37.1321888],
       [-84.9987841,37.1320944],
       [-84.9987208,37.131954],
       [-84.998736,37.1316611],
       [-84.9988091,37.131334],
       [-84.9989283,37.1311337],
       [-84.9991943,37.1309198],
       [-84.9993573,37.1308459],
       [-84.9995888,37.1307924],
       [-84.9998746,37.130806],
       [-85.0000002,37.1308358],
       [-85.0004984,37.1310658],
       [-85.0008008,37.1311625],
       [-85.0009461,37.1311684],
       [-85.0011373,37.1311515],
       [-85.0016455,37.1310491],
       [-85.0018514,37.1311314]],
      [[-85.0000002,37.1317672],
       [-85.0001983,37.1317538],
       [-85.0003378,37.1317582],
       [-85.0004697,37.131792],
       [-85.0008048,37.1319439],
       [-85.0009342,37.1319838],
       [-85.0010184,37.1319463],
       [-85.0010618,37.13184],
       [-85.0010057,37.1315102],
       [-85.000977,37.1314403],
       [-85.0009182,37.1313793],
       [-85.0005366,37.1312209],
       [-85.000224,37.1311466],
       [-85.000087,37.1311356],
       [-85.0000002,37.1311433],
       [-84.9995021,37.1312336],
       [-84.9993308,37.1312859],
       [-84.9992567,37.1313252],
       [-84.9991868,37.1314277],
       [-84.9991593,37.1315381],
       [-84.9991841,37.1316527],
       [-84.9992329,37.1317117],
       [-84.9993527,37.1317788],
       [-84.9994931,37.1318061],
       [-84.9996815,37.1317979],
       [-85.0000002,37.1317672]]]
  }
}'

Expected:
  {"ok":true,"_index":"test","_type":"test","_id":"1","_version":1}
Actual:
  {"error":"MapperParsingException[failed to parse [geometry]]; nested: ArrayIndexOutOfBoundsException[-1]; ","status":400}

This is an issue with es-1.1.0.  The same requests execute successfully against es-0.2.4.

It is possible to view and validate the data in qgis.

![screen shot 2014-04-10 at 5 01 56 pm](https://cloud.githubusercontent.com/assets/6935249/2675061/cc6cfc7a-c10d-11e3-9829-7c80f8075fe8.png)
</description><key id="31293370">5773</key><summary>Geo: Valid complex polygons fail to parse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">marcuswr</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-11T00:12:40Z</created><updated>2014-07-29T08:40:13Z</updated><resolved>2014-07-24T14:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcuswr" created="2014-04-11T18:02:27Z" id="40235690">Checked additional versions of elastic search:
elasticsearch-0.20.4       PASSED
elasticsearch-0.90.13     PASSED
elasticsearch-1.0.0.RC1 FAILED
elasticsearch-1.0.2         FAILED
elasticsearch-1.1.0         FAILED
</comment><comment author="marcuswr" created="2014-04-11T21:58:08Z" id="40257412">Here is a small test case which triggers the issue in v1.1.0

import org.elasticsearch.common.geo.builders.ShapeBuilder;
import org.elasticsearch.common.xcontent.XContentParser;
import org.elasticsearch.common.xcontent.json.JsonXContent;

public class Test {
    public static void main(String[] args) throws Exception {

```
    String geoJson = "{ \"type\": \"Polygon\",\"coordinates\": [[[-85.0018514,37.1311314],[-85.0016645,37.1315293],[-85.0016246,37.1317069],[-85.0016526,37.1318183],[-85.0017119,37.1319196],[-85.0019371,37.1321182],[-85.0019972,37.1322115],[-85.0019942,37.1323234],[-85.0019543,37.1324336],[-85.001906,37.1324985],[-85.001834,37.1325497],[-85.0016965,37.1325907],[-85.0016011,37.1325873],[-85.0014816,37.1325353],[-85.0011755,37.1323509],[-85.000955,37.1322802],[-85.0006241,37.1322529],[-85.0000002,37.1322307],[-84.9994,37.1323001],[-84.999109,37.1322864],[-84.998934,37.1322415],[-84.9988639,37.1321888],[-84.9987841,37.1320944],[-84.9987208,37.131954],[-84.998736,37.1316611],[-84.9988091,37.131334],[-84.9989283,37.1311337],[-84.9991943,37.1309198],[-84.9993573,37.1308459],[-84.9995888,37.1307924],[-84.9998746,37.130806],[-85.0000002,37.1308358],[-85.0004984,37.1310658],[-85.0008008,37.1311625],[-85.0009461,37.1311684],[-85.0011373,37.1311515],[-85.0016455,37.1310491],[-85.0018514,37.1311314]],[[-85.0000002,37.1317672],[-85.0001983,37.1317538],[-85.0003378,37.1317582],[-85.0004697,37.131792],[-85.0008048,37.1319439],[-85.0009342,37.1319838],[-85.0010184,37.1319463],[-85.0010618,37.13184],[-85.0010057,37.1315102],[-85.000977,37.1314403],[-85.0009182,37.1313793],[-85.0005366,37.1312209],[-85.000224,37.1311466],[-85.000087,37.1311356],[-85.0000002,37.1311433],[-84.9995021,37.1312336],[-84.9993308,37.1312859],[-84.9992567,37.1313252],[-84.9991868,37.1314277],[-84.9991593,37.1315381],[-84.9991841,37.1316527],[-84.9992329,37.1317117],[-84.9993527,37.1317788],[-84.9994931,37.1318061],[-84.9996815,37.1317979],[-85.0000002,37.1317672]]]}";

    XContentParser parser = JsonXContent.jsonXContent.createParser(geoJson);                                                                                                                                    
   parser.nextToken();
    ShapeBuilder.parse(parser).build();
}
```

}

// stack trace
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.assign(BasePolygonBuilder.java:366)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.compose(BasePolygonBuilder.java:347)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.coordinates(BasePolygonBuilder.java:146)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.buildGeometry(BasePolygonBuilder.java:175)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:151)
</comment><comment author="spinscale" created="2014-04-14T08:08:07Z" id="40342030">Hey.

you can also use the github geo json feature as well to visualize this, see https://gist.github.com/spinscale/9cc6ba24bff03cca2be5

there has been a huge geo refactoring going on between those affected versions, will check for a regression there.

Do you have any other data where this happens with, or is it just this single polygon?

Thanks a lot for all your input!
</comment><comment author="spinscale" created="2014-04-14T08:49:34Z" id="40344859">I updated my geojson gist above and added a test with a simple polygon (rectangle with hole, which is a rectangle as well), which works... need to investigate
</comment><comment author="marcuswr" created="2014-04-14T14:38:49Z" id="40372849">I have several thousand polygons that result in this error.  I have not
pulled them all out of the logs yet, I will hopefully have time to do that
today.

On Mon, Apr 14, 2014 at 1:08 AM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hey.
&gt; 
&gt; you can also use the github geo json feature as well to visualize this,
&gt; see https://gist.github.com/spinscale/9cc6ba24bff03cca2be5
&gt; 
&gt; there has been a huge geo refactoring going on between those affected
&gt; versions, will check for a regression there.
&gt; 
&gt; Do you have any other data where this happens with, or is it just this
&gt; single polygon?
&gt; 
&gt; Thanks a lot for all your input!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5773#issuecomment-40342030
&gt; .
</comment><comment author="spinscale" created="2014-04-14T14:49:52Z" id="40374176">I'd highly appreciate it, if you could test with the PR referenced above... it solves this problem, but maybe you could check if I introduced side effects (one just came to mind, which i need to check)...
</comment><comment author="marcuswr" created="2014-04-14T20:03:46Z" id="40411275">I have put up a file containing 15k+ polygons which had the same
ArrayIndexOutOfBoundsException against ES1.0+.  It is available at
https://github.com/marcuswr/elasticsearch-polygon-data.git

Additionally, the gist at https://gist.github.com/marcuswr/493406918e0a9edeb509  contains a set of polygons which still fail against the patched version (same IndexOutOfBoundsException)

On Mon, Apr 14, 2014 at 7:50 AM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; I'd highly appreciate it, if you could test with the PR referenced
&gt; above... it solves this problem, but maybe you could check if I introduced
&gt; side effects (one just came to mind, which i need to check)...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5773#issuecomment-40374176
&gt; .
</comment><comment author="marcuswr" created="2014-04-14T22:31:07Z" id="40426177">I have added another file to the repository, with the subset of polygons,
which still failed to ingest, after the patch was applied.
https://github.com/marcuswr/elasticsearch-polygon-data/blob/master/patch_errors.geojson

On Mon, Apr 14, 2014 at 1:03 PM, Marcus Richardson
mrichardson@climate.comwrote:

&gt; I have put up a file containing 15k+ polygons which had the same
&gt; ArrayIndexOutOfBoundsException against ES1.0+.  It is available at
&gt; https://github.com/marcuswr/elasticsearch-polygon-data.git
&gt; 
&gt; Additionally, I have pulled down your fix locally (your branch of master,
&gt; and I used your patch against es 1.1).  I am able to insert some polygons
&gt; but
&gt; not all (I have not tried all of them).  I also cloned your gist to:
&gt; https://gist.github.com/marcuswr/493406918e0a9edeb509  The third
&gt; (test.geojson) renders correctly in the gist, however, does not work for me
&gt; against either patched version.  The 4th file (test1.geojson) does work in
&gt; both versions.
&gt; 
&gt; On Mon, Apr 14, 2014 at 7:50 AM, Alexander Reelsen &lt;
&gt; notifications@github.com&gt; wrote:
&gt; 
&gt; &gt; I'd highly appreciate it, if you could test with the PR referenced
&gt; &gt; above... it solves this problem, but maybe you could check if I introduced
&gt; &gt; side effects (one just came to mind, which i need to check)...
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5773#issuecomment-40374176
&gt; &gt; .
</comment><comment author="spinscale" created="2014-04-20T11:56:07Z" id="40893418">thanks a lot for all the data, I will test with the other polygons as soon as possible (travelling a bit the next days, but will try to check ASAP).
</comment><comment author="marcuswr" created="2014-04-29T22:42:11Z" id="41741379">@spinscale wondering if you had a chance to look at this?  We are looking to do a major elastic search upgrade, but will not be able to without a fix.  Please let me know if there is anything I can do to assist.  
</comment><comment author="spinscale" created="2014-04-30T14:40:23Z" id="41804246">sorry, did not yet have the time to check out all the other polygons you supplied due to traveling
</comment><comment author="marcuswr" created="2014-05-08T23:50:20Z" id="42620125">These polygons fail to ingest when the point in the hole (the first point in the LineString, or the leftmost point in the patched version https://github.com/elasticsearch/elasticsearch/pull/5796) has the same x coordinate (starting or ending) as 2 or more line segments of the shell.
I have created another gist (https://gist.github.com/marcuswr/e0490b4f6e25b344e779) with simplified polygons which demonstrate this problem (hole_aligned.geojson, hole_aligned_simple.geojson).  These will fail on the patched version.  Changing the order of the coordinates in the hole so the leftmost coordinate is first and last (repeated), should result in it failing in both patched and non-patched).  There are additional polygons shown which touch or cross the dateline (the dateline hole should be removed from these polygons -&gt; convert to multi-polygon).
These failures only occur when fixdateline = true.

Additionally, could you tell me why there is special handling of the ear in ShapeBuilder.intersections?
                     if (Double.compare(p1.x, dateline) == Double.compare(edges[i].next.next.coordinate.x, dateline)) {
                         // Ignore the ear

Also Double.compare is not guaranteed to return -1, 0, 1  I'm not sure what the equality is testing for.
</comment><comment author="spinscale" created="2014-05-09T08:42:43Z" id="42644902">thanks a lot for testing and debugging, your comments make a lot of sense, I will close the PR
</comment><comment author="olimcc" created="2014-05-20T15:02:54Z" id="43637871">Hey @spinscale sorry to bug, but do you think you could glance at this PR and at least indicate if ES would move forward with it? If so, we (I work with Marcus) can proceed locally as the finer details of the PR are worked out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve handling of store parameter updates post indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5772</link><project id="" key="" /><description>``` json
DELETE /myindex
POST /myindex/mytype/1
{
  "prop1":"test1"
}
GET /myindex/_search
{
  "fields": [
    "_timestamp"
  ], 
  "query": {
    "match_all": {}
  }
}
PUT /myindex/mytype/_mapping
{

  "mytype": {
    "_timestamp": {
    "enabled": "true",
    "store": "true"
    }
  }
}
GET /myindex/mytype/_mapping
```

If you create an index, put a doc into it, and then try to update the mapping to enable _timestamp and set store:true, you will notice that the response comes back as acknowledged:true with no error messages.  But if you get the _mapping back, you will notice that store:true is not set (by design because we do not allow changing the store parameter post indexing).  

For regular properties, if you attempt to update its store parameter post indexing, it will actually throw a 400 error indicating a MergeMappingException because of differences in store values.  

It will be nice to add the same validation and throw a similar error when users attempt to update the store value of _timestamp post-indexing.
</description><key id="31288245">5772</key><summary>Improve handling of store parameter updates post indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T22:40:46Z</created><updated>2014-09-08T15:20:06Z</updated><resolved>2014-09-08T15:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-08-04T06:24:38Z" id="51021539">Seems to the the same issue as #777
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting numeric fields issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5771</link><project id="" key="" /><description>I am not sure it is feasible or even doable but the problem I have is that when I search on _all or as _all-like field which include numeric field (as text of course) I get a hit but the numeric field (which is folded into _all) does not get highlighted if I include its name into highlights part of the query definition. It works fine for string fields but not for numbers (or dates). I tested it using postings and plain highlighters. Did not try FVH because performance is poor for me as I am highlighting close to 100 small fields)

Is it a bug or works a designed? Any chance of making it work - that would save us lots of work arounds (multifields with text representation of numbers which is used for highlighting)

As a related issue, when I search and highlight on the same numeric field using plain highlighter it does not get highlighted but works with postings
</description><key id="31285075">5771</key><summary>Highlighting numeric fields issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Highlighting</label></labels><created>2014-04-10T21:52:30Z</created><updated>2016-11-10T19:16:24Z</updated><resolved>2016-11-10T19:16:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-10T19:16:24Z" id="259780751">Since the change to use Point fields for numerics, it is no longer possible to highlight numeric fields with the postings highlighter (or any other highlighter).  If this is required, then the numeric value also needs to be indexed as a `keyword` field.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.0.2 debian package fails on removing dirs when purged</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5770</link><project id="" key="" /><description>Purging package doesn't work in any way:

When /var/lib/elasticsearch exists and is a directory, it tries to rmdir /var/lib

```
(Reading database ... 159672 files and directories currently installed.)
Removing elasticsearch (1.0.2) ...
rmdir: failed to remove directory '/var/lib'
dpkg: error processing package elasticsearch (--purge):
 subprocess installed post-removal script returned error exit status 1
Errors were encountered while processing:
 elasticsearch
E: Sub-process /usr/bin/dpkg returned an error code (1)
A package failed to install.  Trying to recover:
Press Return to continue.
```

When /var/lib/elasticsearch doesn't exist, it tries to remove it and fails:

```
(Reading database ... 159672 files and directories currently installed.)
Removing elasticsearch (1.0.2) ...
rmdir: failed to remove '/var/lib/elasticsearch': No such file or directory
dpkg: error processing package elasticsearch (--purge):
 subprocess installed post-removal script returned error exit status 1
Errors were encountered while processing:
 elasticsearch
E: Sub-process /usr/bin/dpkg returned an error code (1)
A package failed to install.  Trying to recover:
Press Return to continue.
```

When /var/lib/elasticsearch is a mounted partition, it tries to remove dir and fails at it:

```
(Reading database ... 159672 files and directories currently installed.)
Removing elasticsearch (1.0.2) ...
rmdir: failed to remove '/var/lib/elasticsearch'
dpkg: error processing package elasticsearch (--purge):
 subprocess installed post-removal script returned error exit status 1
Errors were encountered while processing:
 elasticsearch
E: Sub-process /usr/bin/dpkg returned an error code (1)
A package failed to install.  Trying to recover:
Press Return to continue.
```

It also should probably do same stuff *DB packages do: ask if you want to keep your data when purging package
</description><key id="31283713">5770</key><summary>ES 1.0.2 debian package fails on removing dirs when purged</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">XANi</reporter><labels><label>:Packaging</label></labels><created>2014-04-10T21:32:10Z</created><updated>2014-12-05T09:37:41Z</updated><resolved>2014-12-05T09:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T11:12:17Z" id="40355044">Hey,

what linux distribution did you try this on. The only call in question in the `prerm` script is

```
rmdir -p --ignore-fail-on-non-empty /var/lib/elasticsearch
```

Agreed, the `-p` parameter does not make a lot of sense. But apart from that you should not get any invalid error message, except that the `--ignore-fail-on-non-empty` is somehow ignored... can you try on the commandline, if it works?
</comment><comment author="XANi" created="2014-04-14T15:06:17Z" id="40376163">It was install-&gt;uninstall on debian wheezy/sid (i installed 1.0.2 and realized I need older version so I purged it)

"failed to remove directory '/var/lib'" only occurs when directory above (in my case /var/lib) is also a mount:

```
&#9760; mount | grep lib
/dev/mapper/rootvg-lib on /var/lib type xfs (rw,noatime,attr2,inode64,noquota)
/dev/mapper/rootvg-collectd on /var/lib/collectd type reiserfs (rw,noatime)
/dev/mapper/rootvg-elasticsearch on /var/lib/elasticsearch type xfs (rw,noatime,attr2,inode64,sunit=1024,swidth=2048,noquota)
&#9760; mkdir /usr/test; rmdir -p --ignore-fail-on-non-empty /usr/test
&#9760; mkdir /var/lib/test; rmdir -p --ignore-fail-on-non-empty /var/lib/test
rmdir: failed to remove directory '/var/lib'
```

but that is basically a bug in rmdir, it should just return silently, instead:

```
mkdir /asd;rmdir -p /asd
rmdir: failed to remove directory '/': Device or resource busy
```

and it's some old bug, as I see same behaviour on old centos 5 systems

It works if directory exists but it will fail if it is a mounted partition.

It will also fail if directory was already removed (like admin first removing mount ,then purging package).

I think just ignoring exit code of rmdir should be fine.
</comment><comment author="spinscale" created="2014-04-14T15:17:57Z" id="40377784">great explanation, thanks for digging into this.. so removing `-p` and ignoring the error seems the way to go.
</comment><comment author="XANi" created="2014-04-14T19:33:34Z" id="40408199">-p only works well when used with relative paths.

If you want to clean up empty dirs,

```
find /tmp/dir -type d -empty  -delete
```

works much better, but still exit code have to be handled/ignored
</comment><comment author="t-lo" created="2014-12-03T10:41:30Z" id="65388146">With #7078 merged does the issue still exist?
</comment><comment author="spinscale" created="2014-12-05T09:37:41Z" id="65766570">@t-lo you are right, this should be fixed. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE during significant_terms search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5769</link><project id="" key="" /><description>I got this exception in elasticsearch 1.1:

```
[2014-04-10 12:31:16,678][DEBUG][action.search.type       ] [Lunatica] [ebooktracker][0], node[29zEP2OsR_6Qd-BaUeWKGA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3137ad6b]
java.lang.NullPointerException
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.getBackgroundFrequency(SignificantTermsAggregatorFactory.java:190)
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator.buildAggregation(SignificantStringTermsAggregator.java:87)
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator$WithOrdinals.buildAggregation(SignificantStringTermsAggregator.java:129)
        at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:135)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:136)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:330)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:304)
        at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:71)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

While doing this search:

```
POST /ebooktracker/tweet/_search
{
    "fields": ["text"],
    "query" : {
        "terms" : {"text": ["kindle"]}
    },
    "aggregations" : {
        "tags" : {
            "significant_terms" : { "field" : "hashtags" }
        }
    }
}
```
</description><key id="31273997">5769</key><summary>NPE during significant_terms search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">extesy</reporter><labels /><created>2014-04-10T19:33:33Z</created><updated>2014-10-03T14:59:50Z</updated><resolved>2014-10-03T14:59:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T13:24:24Z" id="40480379">/cc @markharwood 
</comment><comment author="markharwood" created="2014-04-20T20:18:40Z" id="40903831">Trying to figure out under what conditions this would occur. 
Are you perhaps using DocValues for loading FieldData but not indexing the hashtags field?
We need to look up the background doc frequency for terms so the field has to be indexed to be able to do this. 
</comment><comment author="extesy" created="2014-04-20T21:31:38Z" id="40905433">Here's my full schema:

```
"tweet" : {
    "_source" : {"enabled": false},
    "_ttl" : {"enabled": true},
    "properties" : {
        "author" : {"type": "string", "store": true},
        "lang" : {"type": "string", "index": "not_analyzed", "store": true},
        "coordinates" : {"type": "geo_point", "store": true},
        "hashtags" : {"type": "string", "index_name": "hashtag", "index": "not_analyzed", "store": true},
        "mentions" : {"type": "string", "index_name": "mention", "index": "not_analyzed", "store": true},
        "urls" : {"type": "string", "index_name": "url", "index": "not_analyzed", "store": true},
        "domains" : {"type": "string", "index_name": "domain", "index": "not_analyzed", "store": true},
        "text" : {"type": "string", "store": true}
    }
}
```
</comment><comment author="extesy" created="2014-04-20T21:33:59Z" id="40905475">By the way, when I change aggregation field from `hashtags` to `hashtag`, it works.
</comment><comment author="markharwood" created="2014-04-20T21:41:32Z" id="40905628">OK - the incorrect field name explains it. In master this now produces an empty result set rather than null pointer exception
</comment><comment author="clintongormley" created="2014-10-03T14:59:50Z" id="57807706">Looks like this has been fixed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Added tables and fixes to upgrade.asciidoc, fixed version in READ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5768</link><project id="" key="" /><description>...ME.textile

Author: Sean Gallagher
Date: 10 Apr 2014 15:23 EDT
</description><key id="31273520">5768</key><summary>[DOCS] Added tables and fixes to upgrade.asciidoc, fixed version in READ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-04-10T19:26:48Z</created><updated>2014-07-16T21:46:31Z</updated><resolved>2014-04-10T19:28:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[Docs]  Add experimental highlighter plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5767</link><project id="" key="" /><description>Add documentation for highlighter plugin I'm working on.  Works reasonably well but still rough around the edges.
</description><key id="31264230">5767</key><summary>[Docs]  Add experimental highlighter plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T17:38:18Z</created><updated>2015-06-08T15:08:50Z</updated><resolved>2014-04-10T17:40:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-10T17:40:59Z" id="40114888">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TTL Purge Thread might bring back already deleted index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5766</link><project id="" key="" /><description>the node level TTL Purger thread fires up bulk delete request that might trigger `auto_create_index` if the purger thread runs a bulk after the index has been deleted.
</description><key id="31253296">5766</key><summary>TTL Purge Thread might bring back already deleted index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T15:26:26Z</created><updated>2014-04-15T10:56:05Z</updated><resolved>2014-04-15T10:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-10T16:49:49Z" id="40109358">Possible solutions:
- Add a bulk request option to never create new indices, even if `action.auto_create_index` is activated (however this is some sort of override-specialty and not too consistent) and always set this option in the purger
- Duplicate the check in https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java#L95-L114 in the TTLPurger and ensure that the indices do exist. Biggest problem: This again leaves a small window open between the check and the execution, where an index might be deleted

Maybe there is a cleaner solution, which I cant spot ATM
</comment><comment author="nik9000" created="2014-04-10T16:51:53Z" id="40109583">I know its technically a breaking change, but technically you could also stop any sort of DELETE requests from auto creating the index.
</comment><comment author="kimchy" created="2014-04-10T16:59:18Z" id="40110418">@nik9000 we could, and we should definitely start another issue to discuss if thats what we want to do or not (at least for master)
</comment><comment author="nik9000" created="2014-04-10T17:00:02Z" id="40110496">@kimchy makes sense to me.
</comment><comment author="s1monw" created="2014-04-14T10:08:17Z" id="40350589">@spinscale @kimchy @martijnvg  I think I have a more elegant solution for this particular problem I just added a commit and will open a PR soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly quote $JAVA in bin/plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5765</link><project id="" key="" /><description>The all new Oracle Java 7 on OSX has
JAVA_HOME=/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home
Note the happy space in "Internet Plug-Ins" - it gives plugin
major agida...
</description><key id="31248760">5765</key><summary>Properly quote $JAVA in bin/plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dieswaytoofast</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T14:42:34Z</created><updated>2015-06-07T21:05:09Z</updated><resolved>2014-07-18T12:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T11:16:49Z" id="40355384">Uh. Very valid PR. Can you sign the CLA, so I can get it in? See http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dieswaytoofast" created="2014-04-14T11:36:12Z" id="40356649">done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5764</link><project id="" key="" /><description /><key id="31244508">5764</key><summary>Update percolate.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mschirmacher</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T13:55:16Z</created><updated>2014-07-16T21:46:32Z</updated><resolved>2014-04-15T14:14:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T13:21:51Z" id="40480116">Hi @mschirmacher  

Thanks for the correction. Please could you sign our CLA so that we can get your commit merged in? http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="mschirmacher" created="2014-04-15T13:53:21Z" id="40483553">I already did this :)
</comment><comment author="clintongormley" created="2014-04-15T14:14:08Z" id="40485960">Sorry - I missed that, and it appears that this PR has already been merged in!

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5763</link><project id="" key="" /><description /><key id="31243771">5763</key><summary>Update percolate.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mschirmacher</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T13:46:27Z</created><updated>2014-07-16T21:46:33Z</updated><resolved>2014-04-11T16:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-10T17:51:28Z" id="40116527">Thanks for helping fix documentation! Can you merge it with #5764 , I think this makes sense since they both fix wording in the same section of the documentation?

I would also need you to sign our [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) before merging this change in.

Thanks!
</comment><comment author="jpountz" created="2014-04-10T17:57:00Z" id="40117207">Oops sorry, I just figured out the pull requests might have submitted this way because you created them through the web interface? I'll handle the merging by myself if you agree with that.
</comment><comment author="mschirmacher" created="2014-04-10T18:12:20Z" id="40119063">Sure thing, feel free to merge the PRs yourself (especially if that means no paperwork for me :-D)
You are right, i created them through the webinterface :wink:
</comment><comment author="jpountz" created="2014-04-10T18:21:36Z" id="40120119">&gt;  feel free to merge the PRs yourself (especially if that means no paperwork for me :-D)

Great. :-)

Can you still sign the contributor agreement? Unfortunately, I can't merge changes unless this agreement has been signed (the reasons for this are explained on the link I gave).
</comment><comment author="mschirmacher" created="2014-04-10T18:42:51Z" id="40122685">Done :)
Anything else to do?
</comment><comment author="jpountz" created="2014-04-11T16:15:00Z" id="40221458">Nope, that's all good. I just merged your pull requests. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5762</link><project id="" key="" /><description /><key id="31242818">5762</key><summary>Update percolate.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mschirmacher</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T13:35:11Z</created><updated>2014-07-16T21:46:34Z</updated><resolved>2014-04-11T16:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use startNodesAsync in more tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5761</link><project id="" key="" /><description /><key id="31239639">5761</key><summary>Use startNodesAsync in more tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-10T12:59:35Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-04-10T13:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-10T13:01:34Z" id="40074691">looks good!
</comment><comment author="s1monw" created="2014-04-10T13:01:45Z" id="40074712">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Poor aggragations/facets performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5760</link><project id="" key="" /><description>looks like using filtered aggregates/facets could be (in some cases) 10 times slower than querying count directly (see example queries below).

this aggregate query executes in around 150ms:

``` JSON
POST users/_search?search_type=count
{
   "aggs": {
      "skills": {
         "filter": {
            "fquery": {
               "query": {
                  "nested": {
                     "path": "skills",
                     "query": {
                        "match": {
                           "@name": "plumber"
                        }
                     }
                  }
               }
            }
         }
      },
      "people": {
         "filter": {
            "fquery": {
               "query": {
                  "match": {
                     "@name": "plumber"
                  }
               }
            }
         }
      }
   }
}
```

when the queries below execute in around 10ms (each):

``` JSON
POST users/_search?search_type=count
{
   "query": {
      "nested": {
         "path": "skills",
         "query": {
            "match": {
                "@name": "plumber"
            }
         }
      }
   }
}

POST user/_search?search_type=count
{
   "query": {
      "match": {
               "@name": "plumber"
            }
   }
}
```

this times are measure after multiple hits with this same query (so they all should be cached)
making two sequential calls to compute count is still faster than one aggregate/facet call that suppose to do exactly this same

context of my queries:
- es 1.0.0
- 6 servers (hosted in EC2)
- 12 shards per index
- 2 replicas
- around 40m documents 
- document size &lt; 50k
</description><key id="31229629">5760</key><summary>Poor aggragations/facets performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2014-04-10T10:08:40Z</created><updated>2016-03-15T21:38:41Z</updated><resolved>2014-04-10T17:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-10T12:36:36Z" id="40072518">instead using `fquery` try it with `term` filter

```
POST users/_search?search_type=count
{
   "aggs": {
      "skills": {
         "filter": {
            "nested" : {
                "path" : "skills",
                "filter" : { "term" : { "@name" : "plumber" } }
            }
         }
      },
      "people": {
         "filter": { "term" : { "@name" : "plumber" }}
      }
   }
}
```

The `terms` filter is more appropriate for these type of filtering, but it's also an exact match (text analysis is not applied). If you do want text analysis, you can use the `fquery` and explicitly cache it by setting `_cache` to `true`:

```
...
"fquery": {
   "_cache" : true,
   "query": {
      "nested": {
         "path": "skills",
         "query": {
            "match": {
               "@name": "plumber"
            }
         }
      }
   }
}
...
```
</comment><comment author="karol-gwaj" created="2014-04-10T13:00:12Z" id="40074560">yep, im aware that `term` filter is faster than `match` filter
sadly in my case i need to use `match` filter

also this issue is about performance of what looks like similar queries (under the hood they should be doing more or less this same)

so why when used in aggregation, the query is 10 times slower?

btw, setting _cache to true didnt make any visible difference
</comment><comment author="uboness" created="2014-04-10T17:11:21Z" id="40111670">well.. the `aggs` only request is executed with an implicit `match_all` query... so all the docs are evaluated and the filtering is only done in the agg. With the query request, not all docs are evaluated. The reason I mentioned the `term` filter a is that the `term` filter caches which can boost things up. But that won't change the fact that you're still eval'ing all the docs.... now the more sparse the value you're looking for is (among the docs), the bigger the difference will be between these two requests... the less sparse, the closer they'll be in their execution cost
</comment><comment author="karol-gwaj" created="2014-04-10T18:45:08Z" id="40122959">ok, i see your point

i included filters from aggs into query (so it is not evaluating all documents anymore):

```
POST users/_search?search_type=count
{
   "query": {
      "bool": {
         "should": [
            {
               "nested": {
                  "path": "skills",
                  "query": {
                     "match": {
                        "@name": "plumber"                    
                     }
                  }
               }
            },
            {
               "match": {
                  "@name": "plumber"                  
               }
            }
         ]
      }
   },
   "aggs": {
      "skills": {
         "filter": {
            "fquery": {
               "query": {
                  "nested": {
                     "path": "skills",
                     "query": {
                        "match": {
                           "@name": "plumber"
                        }
                     }
                  }
               }
            }
         }
      },
      "people": {
         "filter": {
            "fquery": {
               "query": {
                  "match": {
                     "@name": "plumber"
                  }
               }
            }
         }
      }
   }
}
```

and it takes now more or less this same time as running both queries separately

thx,
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UncategorizedExecutionException in Bulk Update Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5759</link><project id="" key="" /><description>I'm absolutely positive that there are no nulls in data I try to update document with; index, type and id are valid as well. Moreover, if I try to just call update API in a cycle it works fine.

This error occurs _only_ if I try to call actionGet() after execute().

Didn't happen on 0.90.9, started to bother me since 0.90.10, present in 0.90.13. We can't update to 1.1 yet as it requires a lot of work in production and the bug is still present in 1.1 as my local test shows.

I'm fine with not being able to get bulk response in this case, but it still bothers me. Happens on both my Mac OS X with Java 1.8 and in production on Ubuntu with 1.7.

```
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
    at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:88)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:49)
    at com.ark.imapworker.Spider.makeThreads(Spider.scala:342)
    at com.ark.imapworker.Spider$$anonfun$receive$1.applyOrElse(Spider.scala:539)
    at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
    at com.ark.imapworker.Spider.aroundReceive(Spider.scala:34)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
    at akka.actor.ActorCell.invoke(ActorCell.scala:487)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
    at akka.dispatch.Mailbox.run(Mailbox.scala:220)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.action.bulk.BulkResponse.writeTo(BulkResponse.java:116)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:62)
    at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler$1.onResponse(TransportBulkAction.java:309)
    at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler$1.onResponse(TransportBulkAction.java:305)
    at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:240)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:138)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:61)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
    at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler.messageReceived(TransportBulkAction.java:305)
    at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler.messageReceived(TransportBulkAction.java:294)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:212)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:109)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
```

The code is quite trivial (Scala, but oh well).

``` Scala
val request = es.prepareBulk()
for (id &#8592; ids)
  request.add(
    es.prepareUpdate(index, type, id).setDoc("threadId", threadId)
  )

// this throws
val response = request.execute().actionGet()
if (response.hasFailures)
  log.error(response.buildFailureMessage())
// this doesn't
request.execute()
```
</description><key id="31227595">5759</key><summary>UncategorizedExecutionException in Bulk Update Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fuwaneko</reporter><labels><label>feedback_needed</label></labels><created>2014-04-10T09:37:49Z</created><updated>2015-02-28T05:14:32Z</updated><resolved>2015-02-28T05:14:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T11:04:00Z" id="40354451">hey,

can you provide us a complete example to recreate this issue? That would be great! I just want to make sure, I dont fill it up with how I think it looks like and then there are differences...

Thanks a lot!
</comment><comment author="fuwaneko" created="2014-04-14T12:24:03Z" id="40359802">@spinscale yeah, I've prepared test code. But after some testing I see that it's actually mapping related, i.e. happens only with our mapping. I could show it to you, but it's private, so not here.
</comment><comment author="spinscale" created="2014-04-14T12:41:59Z" id="40361061">elasticsearch should still return a more useful error message than a NPE. If you dont want to put your code public, I'd be totally happy, if you mailed it to me, so I can create a test-case and fix it. `alexander dot reelsen @ $thisgithubprojectname dot com`
</comment><comment author="fuwaneko" created="2014-04-14T13:32:38Z" id="40365392">@spinscale funny thing is ES console does not show any errors. I've sent you a mail.
</comment><comment author="fuwaneko" created="2014-04-18T10:22:02Z" id="40800253">@spinscale Also I should mention that this happens only with TransportClient. I've tried nodeBuilder() and there's no exception.
</comment><comment author="petebriggs" created="2014-08-25T15:38:13Z" id="53280787">Hi @fuwaneko, just wondering do you use parent/child relationships in your mapping? I encountered a similar issue with bulk updates in ES 1.0.0, and in my case it turned out to be because I was trying to bulk update child documents but was not specifying their parent IDs in the update requests. Using .setParent() worked for me, e.g.:

```
request.add(
    es.prepareUpdate(index, type, id).setParent(parentId).setDoc("threadId", threadId)
)
```
</comment><comment author="fuwaneko" created="2014-08-26T06:07:20Z" id="53379124">@peatb thanks for the suggestion, but I don't have parent/child relationships unfortunately. In my case it somehow depends on how much requests in a bulk I try to send, if I split bulk into a couple of smaller ones there's no exception being thrown.
</comment><comment author="ncolomer" created="2014-10-30T21:19:18Z" id="61171363">Hi @fuwaneko @peatb, we recently faced a very similar issue:

```
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
        at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:50)
        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
        ...
Caused by: java.lang.NullPointerException
        at org.elasticsearch.action.bulk.BulkResponse.writeTo(BulkResponse.java:116)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:62)
        at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler$1.onResponse(TransportBulkAction.java:355)
        at org.elasticsearch.action.bulk.TransportBulkAction$TransportHandler$1.onResponse(TransportBulkAction.java:351)
        at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:285)
        at org.elasticsearch.action.bulk.TransportBulkAction.access$000(TransportBulkAction.java:65)
        at org.elasticsearch.action.bulk.TransportBulkAction$1.onFailure(TransportBulkAction.java:143)
        at org.elasticsearch.action.support.TransportAction$ThreadedActionListener$2.run(TransportAction.java:117)
        ... 3 more
```

The context:
- we do bulk updates via a TransportClient
- we use routing
- we're on 1.1.1

Is there any chance that you inject something other than a `String` in the `_parent` field?

In our case, we were injecting an `int` in a field that is defined as the routing path. The problem was that elasticsearch failed silently (no logged error) and the thrown exception client-side was this cryptic `UncategorizedExecutionException` (aka. `TheJockerException` :).

We were not using the elasticsearch API `setRouting()` method that forces to pass a `String` as routing key (and do so would have probably helped...) but instead we were using explicit routing path (defined in mapping) and building our own document source using `XContentBuilder` helper. 

As soon as we switched from bulk update to regular one-by-one updates, we were able to see the real cause of the problem. The exception message was the right one and simply said that the routing key was missing, what put us on the right way.

It seems Exception thrown when you use bulks request are wrapping/hidding the real cause of the problem. This probably need to be improved. Moreover, in my case, the [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-routing-field.html) do not mention explicitly that this field have to be a `String`, we can only have some intuitions from the example. I suspect the constraint is the same for the `_parent` field, even if it seems more logic since it refers to a document id, which is a `String`.

Ping @spinscale
</comment><comment author="fuwaneko" created="2014-10-31T11:10:34Z" id="61246426">&gt; It seems Exception thrown when you use bulks request are wrapping/hidding the real cause of the problem.

I tried to disable bulks and there were no exceptions at all. As I mentioned earlier, in my case it depended on how large bulks were.
</comment><comment author="clintongormley" created="2014-12-30T15:29:37Z" id="68365424">I think these issues have probably been fixed by #8506, #8378, #8163.  Please let us know if you're still seeing these problems in v1.4, or whether they have been resolved.

thanks
</comment><comment author="clintongormley" created="2015-02-28T05:14:32Z" id="76511310">No more info. Assuming fixed. Please open a new issue if you see this problem again
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ipv6 field support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5758</link><project id="" key="" /><description>What I've done is copied the `NumericTokenStream`, `NumericRangeFilter`, `NumericRangeQuery` and `NumericUtils` from lucene and change those to support `BigInteger`

the term query and range query/filter are both working, but there are still other things like field data need to do

@kimchy @jpountz @dadoonet can you have a quick look just to make sure I've done the correct thing so far?

closes #3714
</description><key id="31223471">5758</key><summary>add ipv6 field support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label></labels><created>2014-04-10T08:29:18Z</created><updated>2016-03-08T14:28:22Z</updated><resolved>2016-03-08T14:28:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-10T13:48:47Z" id="40083336">Thank you @kzwang , I think this is the right approach in general for managing ipv6 addresses.

However, I'd like to be careful here since this decides on how we are going to index these addresses, and it won't be easy to change the format in the future since we need to maintain backward compatibility. For that reason, maybe the index-time range support for types that are greater than 64 bits should be contributed to Lucene first in order to have ideas/feedback from other Lucene committers? For example, I'm wondering if we should support any fixed length data like you did or if we could just write something that would support variable lengths.

Regarding the field mapper, I'm wondering if we need a dedicated field mapper for ipv6 addresses or if we should try to write a field mapper that would both support ipv4 and ipv6 addresses. I think the latter option would be more convenient from the user perspective, but maybe it would also be a problem because it would require us to be able to distinguish ipv4 and ipv6 addresses at both query and indexing time?
</comment><comment author="uboness" created="2014-04-10T14:23:31Z" id="40087418">+1 on pushing index formatting to Lucene and not tie it to ES

&gt; Regarding the field mapper, I'm wondering if we need a dedicated field mapper for ipv6 addresses or if we should try to write a field mapper that would both support ipv4 and ipv6 addresses. I think the latter option would be more convenient from the user perspective, but maybe it would also be a problem because it would require us to be able to distinguish ipv4 and ipv6 addresses at both query and indexing time?

I'd like to see us pursue adding this support to the ip mapper we already have. Note that the type name doesn't indicate the version of the ip... so it can be confusing and inconsistent to some degree now that we'll support both. @jpountz what problems do you expect at index/query time with having a single mapper? I believe we can have a setting on the mapper for the version where the default is set to IPv4 (for bwc) 
</comment><comment author="jpountz" created="2014-04-10T17:10:16Z" id="40111553">&gt; @jpountz what problems do you expect at index/query time with having a single mapper? I believe we can have a setting on the mapper for the version where the default is set to IPv4 (for bwc) 

The scenario I was thinking about is a user indexing Apache access logs who has a field in Elasticsearch that is used to store ip addresses. I don't think we should expect clients to use different fields depending on the version of the ip address, so I guess we either need:
- to be able to support both v4 and v6 addresses in the same index field,
- or have two index fields (one for v4 and one for v6) and add a bridge on top of them that would redirect to the appropriate field based on the version of the ip that is being indexed/searched.

I think the 2nd option would make bw compatibility easier to maintain but maybe it would also raise issues (eg. currently field mappers can only expose a single `FieldMappers.Names.indexName()`).
</comment><comment author="kzwang" created="2014-04-11T01:06:16Z" id="40161775">I've created a issue in lucene (https://issues.apache.org/jira/browse/LUCENE-5596) and I'll move those code to there
</comment><comment author="uschindler" created="2014-04-15T08:43:45Z" id="40458258">Hi, we started to discuss on the Lucene issue already,
To me it looks wrong to use BigInteger at all for IPv6 addresses. IP adresses in elasticsearch should use the raw bytes as returned by InetAddress#getAddress() which are in network byte order. Network byte order is sortable as we want to have it, without any signedness issues.

The proposal in Lucene is to not provide Big numeric support at all, just allow the precision step stuff also work on binary terms. We also try to allow indexing "binary" terms (which is supported under the hood by lucene) more easily.

In that case, ES is repsonsible to create a byte[] out of the network addresse (or whatever type) and index it. DocValues and Stored fields work out of the box already, just the part that takes care of indexing and range-querying the values may need improvements in Lucene (to support fast ranges). Out of the box, you can even do a TermRangeQuery on a binary term easily, if you indexed it as binary term! It is just not using prefix encoded terms for range speedup.
</comment><comment author="clintongormley" created="2014-07-11T09:41:09Z" id="48712051">Depends on https://issues.apache.org/jira/browse/LUCENE-5596
</comment><comment author="gurvindersingh" created="2014-10-03T08:35:57Z" id="57769230">any update on this issue ?
</comment><comment author="jrideout" created="2014-10-06T18:19:29Z" id="58065152">@clintongormley I think we are awaiting: https://issues.apache.org/jira/browse/LUCENE-5879
</comment><comment author="vvaradhan" created="2015-05-05T18:04:10Z" id="99160406">@jrideout https://issues.apache.org/jira/browse/LUCENE-5879 got fixed recently in trunk and 5.2 of Lucene.  Is it possible to provide an update now for ipv6 support in ip field type?
</comment><comment author="clintongormley" created="2015-05-07T18:58:48Z" id="99980295">@vvaradhan Auto-prefixed terms in Lucene has been exposed as an experimental postings format, so it's not safe for us to use until the feature makes it to the default postings format (which is backward compatible)
</comment><comment author="mikemccand" created="2015-09-01T23:29:02Z" id="136890930">I think we could alternatively use the new (just released in Lucene 5.3.0) NumericRangeTree to implement this?

See https://github.com/elastic/elasticsearch/pull/5683#issuecomment-136890281 for some ideas on how it would work for BigInteger/Decimal ... I think the basic idea would be similar: any value that can be converted into a "same sort order" byte[] should work.
</comment><comment author="uschindler" created="2015-09-02T05:23:13Z" id="136940063">Regarding #5683: Converting of the ipv6 addresses to BigDecimal would be a waste here? Just use the byte[] directly: http://docs.oracle.com/javase/7/docs/api/java/net/Inet6Address.html#getAddress()

All signs are correct from the beginning, because its just 16 bytes (0-255), highest byte first. So order is correct by default. Theoretically we can index it as is (if we could use the new AutoPrefix terms), but for NumericRangeTree it should also be simple.
</comment><comment author="mikemccand" created="2015-09-02T10:08:44Z" id="137012181">@uschindler right, we should go straight to the byte[]!  It ought to work very well.
</comment><comment author="SKumarMN" created="2015-10-13T10:05:18Z" id="147669196">@kimchy @jpountz @dadoonet  @kzwang   

Hi,

I have used the above fix in my 1.4.4 code to support big integer by changing the IPV6 Mapper. Search and range queries works fine. Our application needs support for Bigdecimal too. Could you please provide me pointers about how can i implement big decimal support with range functionality as well..
</comment><comment author="clintongormley" created="2016-03-08T14:28:22Z" id="193803709">Closing in favour of https://github.com/elastic/elasticsearch/issues/17007
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kibana is adding unwanted semicolons to the message body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5757</link><project id="" key="" /><description>Using Kibana 3.0 with ElasticSearch 1.0.1. It seems that sometimes Kibana adds semicolons to the body of an xml message. This causes the xml to be invalid. 

An example of a fragment of what we see in Kibana: 

``` xml
&lt;soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"; xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"; xmlns:xsd="http://www.w3.org/2001/XMLSchema"&gt;
```

But when I check what ElasticSearch is sending to Kibana I see: 

``` xml
&lt;soap:Envelope xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"&gt;
```
</description><key id="31219251">5757</key><summary>Kibana is adding unwanted semicolons to the message body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edonerler</reporter><labels /><created>2014-04-10T07:00:19Z</created><updated>2014-07-01T12:37:30Z</updated><resolved>2014-07-01T12:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gwindlord" created="2014-06-24T08:41:48Z" id="46945933">Bumped into this issue with XML output:

``` xml
&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;response xmlns="http://www.w3.org/2002/06/xhtml2"&gt;;
&lt;fn name="DocFind" rc="OK"&gt;
```

Also found out that there is no semicolon neither at JSON, nor at Raw view:

``` xml
&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;response xmlns=\"http://www.w3.org/2002/06/xhtml2\"&gt;\n&lt;fn name=\"DocFind\" rc=\"OK\"&gt;\n
```

and for sure not in ElasticSearch - it exists only at Table view.
</comment><comment author="clintongormley" created="2014-07-01T12:37:30Z" id="47650310">Please open this issue on the Kibana issues list: https://github.com/elasticsearch/kibana/issues
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return valid empty JSON response when no recovery information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5756</link><project id="" key="" /><description>This is a fix to send back to the client a valid empty JSON response in
the case when we have no recovery information.

Closes #5743
</description><key id="31209980">5756</key><summary>Return valid empty JSON response when no recovery information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-10T02:05:18Z</created><updated>2014-10-21T23:43:04Z</updated><resolved>2014-04-21T23:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-10T08:11:16Z" id="40053098">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Fix for hung clients on cluster without benchmark nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5755</link><project id="" key="" /><description>This is a fix for a bug whereby a cluster that has no nodes started with
-Des.node.bench=true will cause clients to hang if they attempt to
submit a benchmark.

Closes #5754
</description><key id="31206259">5755</key><summary>Benchmark: Fix for hung clients on cluster without benchmark nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>bug</label></labels><created>2014-04-10T00:24:46Z</created><updated>2015-03-19T20:05:31Z</updated><resolved>2014-04-21T23:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Client hangs if benchmark submitted w/o any benchmark nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5754</link><project id="" key="" /><description>Attempting to submit a benchmark to a cluster w/o any nodes marked as es.node.bench=true will hang a client. 
</description><key id="31202733">5754</key><summary>Client hangs if benchmark submitted w/o any benchmark nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-09T23:07:57Z</created><updated>2014-04-21T23:05:38Z</updated><resolved>2014-04-21T23:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>spelling correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5753</link><project id="" key="" /><description /><key id="31184174">5753</key><summary>spelling correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chasebolt</reporter><labels /><created>2014-04-09T18:51:14Z</created><updated>2014-07-16T21:46:35Z</updated><resolved>2014-04-17T14:47:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T12:39:07Z" id="40476161">Hi @shanlar 

Thanks for the correction. Please could you sign our CLA so that we can get your commit merged in?  http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="dakrone" created="2014-04-17T14:47:09Z" id="40722056">This has been fixed in a different PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw error when incorrect setting applied to `auto_expand_replicas`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5752</link><project id="" key="" /><description>I tried PUTting `{"index":{"auto_expand_replicas":"all"}}` to an index and it came back with `{"acknowledged":true}`. However, in the logs there was a confusing explosion:

```
[2014-04-09 17:55:54,873][WARN ][cluster.metadata         ] [hostname] failed to set [index.auto_expand_replicas], wrong format [all]
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
```

which was I did not find to be a helpful error message.

I thought that `all` was a legitimate value for this setting, and only through reading the source code did I learn that it is always range, even when providing `all` as the upper bound.

I have a [fix](https://github.com/mdaniel/elasticsearch/commit/956fe4f74ee307c4110fcdc7380717caa091eeba) for this usability problem, but the guidelines said I should discuss it first, so let's discuss.
</description><key id="31179988">5752</key><summary>Throw error when incorrect setting applied to `auto_expand_replicas`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdaniel</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T17:59:23Z</created><updated>2015-06-07T14:31:59Z</updated><resolved>2014-06-06T23:37:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="smarts" created="2014-05-23T21:35:35Z" id="44063317">I ran into the same problem. Glad this Issue was indexed by Google! :+1: 
</comment><comment author="nik9000" created="2014-05-24T06:38:03Z" id="44079398">Left some comments.  You should probably just submit the patch as a pull request.  That is typically a fine venue for discussion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Allocation setting explanation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5751</link><project id="" key="" /><description>Closes #5748
</description><key id="31179716">5751</key><summary>[Docs] Allocation setting explanation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T17:56:16Z</created><updated>2014-06-13T12:10:57Z</updated><resolved>2014-04-09T18:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-09T17:57:28Z" id="39995729">Since I had to dig into the source to figure out that the weights weren't weights in one function but instead weights combining three functions, I figured I'd try to explain it.  I'm not sure I did a good job though.
</comment><comment author="dakrone" created="2014-04-09T17:57:52Z" id="39995777">Instead of "smooth all", what about "equalize the number of"? I think smoothing is a bit too nebulous a concept.
</comment><comment author="nik9000" created="2014-04-09T18:01:43Z" id="39996289">&gt; Instead of "smooth all", what about "equalize the number of"? I think smoothing is a bit too nebulous a concept.

Done.
</comment><comment author="dakrone" created="2014-04-09T18:13:00Z" id="39997649">Merged to 1.1, 1.x, and master in af0278b51bee306c48b845dfca09ccdc77548027; thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolation does not seem to work fully on dynamically templated fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5750</link><project id="" key="" /><description>Hi all,

  it looks like when using dynamic_templates with percolation, new fields introduced in the percolated document do not get properly picked up. Here is the test scenario:

Create an index with a type with all `custom.*` fields not analysed:

```
PUT /myindex
PUT /myindex/mytype/_mapping
{
    "mytype": {
        "dynamic" : false,
        "properties" : {
            "custom": {
                "dynamic": true, "type": "object", "include_in_all": false
            }
        },
        "dynamic_templates": [ {
            "custom_fields": {
                "path_match": "custom.*",
                "mapping": {
                    "index": "not_analyzed"
                }
            }
        }]
    }
}
```

Then register two queries one matching docs with `color:red` and the other `color:blue`

```
PUT /myindex/.percolator/redperco
{
    "query": {
        "query_string": {
           "query": "color:red"
        }
    }
}

PUT /myindex/.percolator/blueperco
{
    "query": {
        "query_string": {
           "query": "color:blue"
        }
    }
}
```

Then percolate. Note that we did not insert any document in the index. It will be the first time it sees the `custom.color` field.

```
POST /myindex/mytype/_percolate
{
    "doc" : {
        "custom": {
            "color": "blue"
        }
    }
}
```

We get no match when it should find one.

Interesting notes:
- if you use the fully qualified field name `custom.color` in the queries it works
- if you re-put the mapping and re-register the queries it works
- if you index the same document before registering the queries it works

Tests run on OSX 10.9.2.
Elasticsearch Info:

```
{
   "status": 200,
   "name": "Silver Fox",
   "version": {
      "number": "1.1.0",
      "build_hash": "2181e113dea80b4a9e31e58e9686658a2d46e363",
      "build_timestamp": "2014-03-25T15:59:51Z",
      "build_snapshot": false,
      "lucene_version": "4.7"
   },
   "tagline": "You Know, for Search"
}
```

Thanks,
Emmanuel
</description><key id="31168293">5750</key><summary>Percolation does not seem to work fully on dynamically templated fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">egueidan</reporter><labels /><created>2014-04-09T15:39:18Z</created><updated>2014-07-01T14:15:08Z</updated><resolved>2014-04-11T16:00:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-11T08:56:32Z" id="40183654">I think this doesn't has to do with dynamic templates, but how queries resolve field name that are not an exact match.

At query parse time the field is resolved in the mapping of an index. However if the field can't be found in the mapping under its exact name, the smart name resolving kicks in and tries to find the best field with the same suffix and uses that concrete field. However at the time when the percolator query is added the query parsing kicks in as well, but there field configured yet, so it fallbacks to using the name `color`. The document being percolated doesn't have this field and therefor doesn't match. However it does update the mapping, by adding the `custom.color` field and that is why when you update the mapping and reindexing the queries the queries do match with that document.

I think this is expected behaviour, since that just how the percolator works, the fields of the queries are resolved at query registration time. So one of the workarounds you mentioned should be used in order for fields to get resolved correctly. (use of full names, reindex queries, or have the mapping configured before indexing queries (either via indexing a percolator doc or adding a mapping))

Actually just registering the percolator queries should have been enough in order for the queries to match with the document being percolated. The mapping does get modified, but not properly propagated into the cluster state. This is an issue that needs to be addressed.
</comment><comment author="egueidan" created="2014-04-11T09:49:05Z" id="40187542">Hi Martijn, thanks for taking the time!
Your analysis makes a lot of sense. I wasn't aware the percolator queries would not react to mapping changes. Two points:
1. Do you want me to create a separate (more precise) issue concerning the mapping propagation issue?
2. Don't you think that making the percolator queries react to mapping changes would be useful? It seems to me that it would rather be aligned with the dynamic nature of ES. It would basically mean that you can register your interest in something you haven't seen yet and pick it up as soon as it appears while using dynamic mappings. In some cases, as a developper, you don't know exactly what'll be in the data (hence you use dynamic mappings/dynamic templates) and you don't know exactly what'll be in the query (user input). To support those cases, we would need to re-register the queries but we are left with the question: when? How can I pick up that the mapping has changed and queries should be re-registered. If that makes sense, then shall I create an enhancement request?
Cheers,
Emmanuel
</comment><comment author="martijnvg" created="2014-04-11T13:17:10Z" id="40201945">Hi Emmanuel,
1. The mapping propagation issue should be fixed soon: https://github.com/elasticsearch/elasticsearch/issues/5750
2. That idea makes absolutely sense. However I see other way than reindexing the percolator query (or al least reloading it). What happens when a query gets added is that the `query` part of the document gets parsed in a Lucene query and there isn't an easy way to just a change a field in the Lucene query because of a mapping update. Also if there is such a mechanism it can be confusing too? Maybe there're mapping updates that don't make sense for your percolator queries? 

I think it is better to approach this problem from a different perspective. You can configure `copy_to` fields (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#copy-to) in your mapping, which copies the values of fields with a specific expression into a field you know about. This field can then use in your percolator queries, since you can rely on the fact that it exists. Also the `_all` field can help here, but `copy_to` gives you more control.
</comment><comment author="egueidan" created="2014-04-11T16:00:37Z" id="40220003">Martijn, thanks a lot for the quick fix on #5776. As for the automatic reloading of the percolator query on mapping updates, I understand that it's not trivial to do. As far as we are concerned we can live with using the fully qualified names.Using the `copy_to` field to dynamically alias unknown fields to known ones is also an interesting idea.
Thanks a lot for your help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix format string for DiskThresholdDecider reroute explanation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5749</link><project id="" key="" /><description>They can be either floating point or integers, so treat them like strings.
</description><key id="31163779">5749</key><summary>Fix format string for DiskThresholdDecider reroute explanation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T14:50:43Z</created><updated>2015-06-07T21:05:52Z</updated><resolved>2014-04-09T15:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-09T15:13:40Z" id="39975788">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force smooth shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5748</link><project id="" key="" /><description>So I have one index that will get tons more traffic then all the others.  For that reason its pretty important that its shards get spread out reasonably evenly even in the face of things like rolling restarts, failed nodes, and the like.  I'm pretty sure the distribution is reasonably random right now.  Could Elasticsearch bias towards smoothly distributing the shards?

This came up because I started load testing Elasticsearch and saw this:
![bad_distribution](https://cloud.githubusercontent.com/assets/215970/2656564/6426017c-bff3-11e3-8c31-918f0319c908.png)
That is, elastic1003 isn't seeing any load at all and elastic105 and elastic1011 aren't shouldering their fair share.

On digging, I got this:

```
manybubbles@elastic1008:~$ curl -s 'localhost:9200/_cat/shards/enwiki_content_1395837666' | perl -pe 's/.*(elastic.*)/\1/' | sort | uniq -c 
      3 elastic1001 
      2 elastic1002 
      2 elastic1004 
      1 elastic1005 
      3 elastic1006 
      3 elastic1007 
      4 elastic1008 
      3 elastic1009 
      2 elastic1010 
      1 elastic1011 
      4 elastic1012 
      3 elastic1013 
      3 elastic1014 
      3 elastic1015 
      3 elastic1016 
```

Which shows that elastic1003 doesn't have any of the shards and elastic1005 and elastic1011 only have one each.

Annoying work around: I can use the allocation api to shift shards from the nodes with more to the nodes with fewer.  It'd be much better if it were automatic.
</description><key id="31162960">5748</key><summary>Force smooth shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-04-09T14:42:35Z</created><updated>2014-09-04T23:00:10Z</updated><resolved>2014-04-09T18:12:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-09T15:10:15Z" id="39975370">ok so if I understand you correctly you have an index that should be better balanced but other indices that are less "hot" prevent that from happening due to overall balance, is that what you are saying?  did you look at the [balancer options](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_balanced_shards) the index one could help you if you wanna raise it slowly you should see some re-balancing happening?
</comment><comment author="s1monw" created="2014-04-09T18:59:52Z" id="40003068">thanks so much for improving the docs on this! @nik9000 out of curiosity did it help you to raise the settings? and if so can you share some of this experience?
</comment><comment author="nik9000" created="2014-04-09T19:12:13Z" id="40004441">Sure!  I slowly raised the `index` balance relative to the others until I got something that caused the shards on this index to balance out.  This is where I ended up:

``` js
          "balance": {
            "shard": 0.195,
            "index": 0.8,
            "primary": 0.005
          }
```

Which balanced the shards out pretty well:

```
manybubbles@elastic1001:~$ curl -s 'localhost:9200/_cat/shards/enwiki_content_1395837666' | perl -pe 's/.*(elastic.*)/\1/' | sort | uniq -c
      3 elastic1001 
      2 elastic1002 
      2 elastic1003 
      2 elastic1004 
      2 elastic1005 
      2 elastic1006 
      3 elastic1007 
      3 elastic1008 
      3 elastic1009 
      2 elastic1010 
      2 elastic1011 
      3 elastic1012 
      3 elastic1013 
      3 elastic1014 
      2 elastic1015 
      3 elastic1016 
```

And the cluster is still relatively balanced any way:

```
manybubbles@elastic1001:~$ curl -s 'localhost:9200/_cat/shards' | perl -pe 's/.*(elastic.*)/\1/' | sort | uniq -c
    349 elastic1001 
    348 elastic1002 
    347 elastic1003 
    353 elastic1004 
    352 elastic1005 
    349 elastic1006 
    351 elastic1007 
    353 elastic1008 
    348 elastic1009 
    350 elastic1010 
    352 elastic1011 
    353 elastic1012 
    350 elastic1013 
    350 elastic1014 
    354 elastic1015 
    352 elastic1016 
```

I think I saw it balance my index's shards, then do the same for some other indexes.

I'm happy about how it turned out, especially because we're about to do _another_ rolling restart on those machine to repartition them.

When I reran the performance test the picture was much prettier:
![good_distribution](https://cloud.githubusercontent.com/assets/215970/2660012/aca322f0-c01a-11e3-857b-944a504bc0e5.png)

I still have more work to do to lower those but they are much better now that they are distributed.
</comment><comment author="s1monw" created="2014-04-10T09:08:37Z" id="40057345">thanks for the insight nik! I am happy that the parameters worked for you. I think in the future we somehow need to take the "weight" of an index into account to make this less manual.
</comment><comment author="nik9000" created="2014-04-10T12:19:52Z" id="40071191">I sent a half baked pr a while ago using a few decays to start automatically calculating weight. I'm sure it wasn't right but might have useful ideas. It'd be useful in, say, a log archiving scenario where you normally search and write to "current" indexes. 

Even with something like that, you'd still want to let users override whatever it comes up with with because they might have some sense of future changes.  

Even with a

Sent from my iPhone

&gt; On Apr 10, 2014, at 5:08 AM, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; thanks for the insight nik! I am happy that the parameters worked for you. I think in the future we somehow need to take the "weight" of an index into account to make this less manual.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to not download content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5747</link><project id="" key="" /><description>It would be nice to have an option to only index the filenames, sizes, etc without downloading contents.
</description><key id="31162178">5747</key><summary>Option to not download content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanitus</reporter><labels /><created>2014-04-09T14:34:01Z</created><updated>2014-12-30T15:24:48Z</updated><resolved>2014-12-30T15:24:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-09T14:46:20Z" id="39972448">Could you provide more context about what you are trying to do and what do you expect?
</comment><comment author="clintongormley" created="2014-12-30T15:24:48Z" id="68364963">No more info.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent fsync from creating 0-byte files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5746</link><project id="" key="" /><description>This is related to LUCENE-5570 where fsync creates a 0-byte file
if the file does not exists. This commit adds the patched lucene
version using Java 7 APIs as well as a note to replace this method
with the upcomeing IOUtils#fsync in Lucene 4.8
</description><key id="31161519">5746</key><summary>Prevent fsync from creating 0-byte files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T14:26:33Z</created><updated>2015-06-07T14:32:18Z</updated><resolved>2014-04-14T20:07:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-12T19:27:15Z" id="40289374">@imotov if you have time I'd love if you could take a look at this again. I added a new commit that cleans up that writeBlob method and also tries to fsync the directory that we have written too.
</comment><comment author="imotov" created="2014-04-12T21:59:46Z" id="40292903">LGTM
</comment><comment author="rmuir" created="2014-04-14T18:58:35Z" id="40404471">+1 for the fsync fix, I'm glad we factored this logic into a utility method in Lucene 4.8....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add retry count parameter to snapshot api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5745</link><project id="" key="" /><description>Allow snapshots (and restores) to have a retry count so that a failure to a single shard due to a read or write error doesn't invalidate the entire index as incomplete.

Examples would be when s3 times out and stops a particular transfer during a backup.

The ability to set the snapshot to retry x amount of times before giving up would make snapshots more robust, an especially nice feature for very large indexes.
</description><key id="31160840">5745</key><summary>Add retry count parameter to snapshot api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">geekpete</reporter><labels><label>discuss</label></labels><created>2014-04-09T14:18:58Z</created><updated>2014-12-01T04:20:51Z</updated><resolved>2014-11-25T12:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="geekpete" created="2014-11-20T00:27:19Z" id="63741056">Also logged as an issue against the elasticsearch-aws-cloud plugin in case it might be implemented directly in the plugin more easily than the elasticsearch core.

https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/140
</comment><comment author="clintongormley" created="2014-11-24T12:19:48Z" id="64186476">@imotov where should this be implemented? snapshot/restore or the aws plugin?
</comment><comment author="imotov" created="2014-11-24T14:19:45Z" id="64199430">@clintongormley I think this issue has been [already taken care of](https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/140#issuecomment-63947854) in aws plugin. We might pull some of this logic into upper layers if other plugins will need that. But so far it seems to be pretty AWS-specific. 
</comment><comment author="clintongormley" created="2014-11-25T12:50:37Z" id="64394798">thanks @imotov - closing this issue then
</comment><comment author="geekpete" created="2014-12-01T04:20:51Z" id="65019298">Thanks :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the option to set ulimit -u in rpm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5744</link><project id="" key="" /><description>It's quite useful to be able to change ulimit -u for elasticsearch.
Therefor I've added this option.
</description><key id="31153722">5744</key><summary>Added the option to set ulimit -u in rpm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">worldcitizen</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2014-04-09T12:45:17Z</created><updated>2014-10-20T13:06:58Z</updated><resolved>2014-10-20T13:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-14T11:17:33Z" id="40355426">out of curiosity - did you run into a limit here with elasticsearch that you want to increase it? Is there an obscene low default on some Linux distribution?
</comment><comment author="clintongormley" created="2014-10-20T13:06:58Z" id="59746682">No feedback in 6 months. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Empty HTTP body returned from _recovery API on empty cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5743</link><project id="" key="" /><description>When the `_recovery` API is executed against a cluster with no indices, an empty HTTP body is returned, causing errors in clients expecting a valid JSON response:

```
$ elasticsearch --cluster.name=recovery_response_error --http.port=9280
# ...
$ curl -i localhost:9280/_recovery
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 0
```

An empty JSON object (`{}`) is expected here.
</description><key id="31145893">5743</key><summary>Empty HTTP body returned from _recovery API on empty cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">karmi</reporter><labels><label>:REST</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T10:30:46Z</created><updated>2015-06-07T21:06:24Z</updated><resolved>2014-04-21T23:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Add ability to restore partial snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5742</link><project id="" key="" /><description>Is there a way to copy a single index from a snapshot marked as "incomplete" by the restore function to the data dir of a single node elastic cluster in order to restore that index data by reopening it as a closed index?

eg, to restore a single index from a multi-index snapshot (an index marked as incompleted).

from whatever source location with a dir structure like: 

```
snapshotname/indices/myindexname/*
```

to: 

```
/var/lib/elasticsearch/mycluster/nodes/0/indices/myindexname/*
```

will elasticsearch see this as a closed index and provide the ability to online it again?

Failing that, is there any other possible way to salvage data from a snapshotted index that is marked as incomplete, especially if the size of the index appears to be correct?
</description><key id="31133782">5742</key><summary>Snapshot/Restore: Add ability to restore partial snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">geekpete</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T06:59:07Z</created><updated>2014-07-16T12:02:35Z</updated><resolved>2014-07-01T01:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-10T00:46:54Z" id="40032734">You should be able to restore all indices that were successfully snapshotted except the incomplete index by excluding the incomplete index from the restore operation:

```
$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore" -d '{
    "indices": "-corrupted_index",
    "include_global_state": false
}'
```

There is no way simple to force incomplete index to be restored at the moment. What type of failures do you see when you run the following command:

```
$ curl -XGET localhost:9200/_snapshot/repository_name/snapshot_name
```
</comment><comment author="geekpete" created="2014-04-10T01:15:49Z" id="40034105">I have it working. :)

You can adjust the metadata files in the snapshot to allow the restore to work.

After restoring the incomplete index, I get an incomplete/uninitialisable shard that must be replaced with a blank one from a temporary index to get the cluster back to green (as I was able to prove the data for that shard was simply missing from the snapshot.) 

But an override to the usual behaviour would be nice to do the following:
-allow restoration of incomplete indexes from snapshots
-provide the ability to zero out the data in bad/incomplete shards if the data is missing to ensure green state when restored
-provide detailed feedback on the shards that have been reset to blank and how much data and how many documents were lost

I do realise that once again my case is probably an edge case so not that high priority compared to other features/bugs. :)

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A new ClusterStateStatus to indicate cluster state life cycles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5741</link><project id="" key="" /><description>When the ClusterService applies a new cluster state, it is first assigned as the new active one and then all listeners are called. Some of ES's features sample the current state and try to take action on it (for example index a document). If that fails, they will wait for change in the cluster state and try again (for example, wait for a shard to start and try indexing again).

If you're unlucky you sample the state after it has been assigned as the "active" state but before all listeners has done the work. In this cases the action taken (i.e., indexing a doc) will still fail (as the shard is not yet started) but waiting for a new state may take a long time or timeout.

This commit adds a new ClusterStateStatus that allows to better track the stages a cluster state goes through (currently `RECEIVED`, `BEING_APPLIED` &amp; `APPLIED`). This allows detecting that a cluster state is not yet fully applied and retry without waiting for a new state to arrive.

This commit also adds a utility class , ClusterStateObserver, to make this pattern slightly simpler and avoid common pit falls.
</description><key id="31133352">5741</key><summary>A new ClusterStateStatus to indicate cluster state life cycles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T06:48:09Z</created><updated>2015-06-07T14:40:01Z</updated><resolved>2014-04-22T08:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-09T10:55:15Z" id="39951136">I like this in general - I left some comments
</comment><comment author="bleskes" created="2014-04-11T09:39:00Z" id="40186830">Thx all. Left some comments and  pushed an update based on the feedback.
</comment><comment author="s1monw" created="2014-04-11T15:50:28Z" id="40218865">this LGTM I think we should let our CI build chew on it a bit - boaz do you wanna open a enhancement/ branch?
</comment><comment author="bleskes" created="2014-04-12T14:10:19Z" id="40281285">Thx. pushed this to enhance/cluster_state_status.
</comment><comment author="bleskes" created="2014-04-22T08:19:45Z" id="41014718">push. thx for the reviews.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>facet_filter does not seem to be working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5740</link><project id="" key="" /><description>## My version of elasticsearch:

number: 1.0.0
build_hash: a46900e9c72c0a623d71b54016357d5f94c8ea32
build_timestamp: 2014-02-12T16:18:34Z
build_snapshot: false
## lucene_version: 4.6

I tried executing the example from google group which an user was able to run:
https://gist.github.com/surajtamang/3616612
https://groups.google.com/forum/#!topic/elasticsearch/vzRoTk9OYaY

However, my output was:
https://gist.github.com/rahurkar/10231685

with no output for facets.

Did this functionality break or change from what's described in the documentation?
</description><key id="31132627">5740</key><summary>facet_filter does not seem to be working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rahurkar</reporter><labels /><created>2014-04-09T06:28:23Z</created><updated>2014-04-15T11:57:23Z</updated><resolved>2014-04-15T11:57:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-15T11:57:23Z" id="40473085">It's not the facet filter which has changed, but the way of indexing docs.  Before 1.0, you could optionally wrap the body of the doc in the `type` name, but from 1.0 onwards you can no longer do that.

So in the example you link to, instead of indexing docs like this:

```
curl -XPUT "http://localhost:9200/org/activity/901" -d '{
  "activity": {
    "title": "Engineering",
    "participating-org": [{
      "name": "DEF",
      "ref": "123CD",
      "role-code": 3,
      "role": "Leading"
    },{
      "name": "ABC",
      "ref": "123AB",
      "role-code": 3,
      "role": "Organizing"
    }]
  }
}'
```

you should index them like this:

```
curl -XPUT "http://localhost:9200/org/activity/901" -d '{
    "title": "Engineering",
    "participating-org": [{
      "name": "DEF",
      "ref": "123CD",
      "role-code": 3,
      "role": "Leading"
    },{
      "name": "ABC",
      "ref": "123AB",
      "role-code": 3,
      "role": "Organizing"
    }]
}'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change to external versioning mechanism so that version numbers starting from zero are permitted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5739</link><project id="" key="" /><description>From Binh:

&#8220;Thank you for reporting this, the plan is to revert back and allow 0 for external versioning. Not sure yet when it will be but likely in the next 1 or 2 releases.&#8221;

---------- Forwarded message ----------
    Hello team - I just switched over to 1.1.0 and changed from VersionType.EXTERNAL to VersionType.EXTERNAL_GTE. Ever since then, we are seeing the error below.

Any comments?

I looked at ES code and see this newly added line for EXTERNAL_GTE. Does it mean we dont support zero as a valid version number? Our systems use zero as the start version number.

Also to add: it seems that even VersionType.EXTERNAL is also broken in the new version (1.1.0) since it now does the validation of input version number and it does not allow a save if the input version number is zero.
Looking for inputs from ES team on how to proceed on this.

```
       @Override
        public boolean validateVersion(long version) {
            return version &gt; 0L;
        }

org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: illegal version value [0] for version type [EXTERNAL_GTE];
    at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
    at org.elasticsearch.action.index.IndexRequest.validate(IndexRequest.java:181)
    at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:63)
    at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:228)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)
    at org.elasticsearch.client.support.AbstractClient.index(AbstractClient.java:82)
    at org.elasticsearch.client.transport.TransportClient.index(TransportClient.java:330)
    at org.elasticsearch.action.index.IndexRequestBuilder.doExecute(IndexRequestBuilder.java:314)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
```

-Amit
</description><key id="31132209">5739</key><summary>Change to external versioning mechanism so that version numbers starting from zero are permitted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amitsoni13</reporter><labels /><created>2014-04-09T06:16:54Z</created><updated>2014-04-11T20:33:35Z</updated><resolved>2014-04-11T20:33:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-04-11T20:33:35Z" id="40250370">hi amit. this issue is already tracked here: https://github.com/elasticsearch/elasticsearch/issues/5662

I'm closing this as it is a duplicate
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw better error if invalid scroll id is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5738</link><project id="" key="" /><description>PR for #5730 but only dealing with invalid scroll_ids and not non existing ones.
</description><key id="31131389">5738</key><summary>Throw better error if invalid scroll id is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Exceptions</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-09T05:53:32Z</created><updated>2015-06-07T21:06:36Z</updated><resolved>2014-04-16T07:18:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-11T03:34:26Z" id="40167918">The AssertionError is now catched in the test and if running without -ea an ElasticSearchIllegalArgumentException is thrown, however I can't test this since the tests run with assertion enabled.
</comment><comment author="s1monw" created="2014-04-14T20:49:38Z" id="40416316">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5737</link><project id="" key="" /><description>mapping setting:
clean_body: {
type: "string",
store: true,
analyzer: "smartcn"
}
When I post some data, I will got error below sometime.
{"error":"ArrayIndexOutOfBoundsException[null]","status":500}
But when I delete the mapping and post the same data, there will be no any error.
</description><key id="31121845">5737</key><summary>ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">neversion</reporter><labels /><created>2014-04-09T00:47:50Z</created><updated>2014-07-02T14:21:36Z</updated><resolved>2014-07-02T14:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-09T05:40:34Z" id="39930352">Could you send more details such as a full curl recreation which helps to reproduce your issue?

Which versions are you using? (Elasticsearch and smartcn plugin)
</comment><comment author="dadoonet" created="2014-07-02T14:21:36Z" id="47781867">No update on that issue.

Closing for now. Please feel free to reopen and provide more details.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature Request] : get the offset of a highlighted field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5736</link><project id="" key="" /><description>Currently the highlighting result is returned in between pre-tag and post-tag in a context. Is it possible to provide an api that returns the offset of this highlighted term in the field of the document so more flexible operation can be done?
</description><key id="31114974">5736</key><summary>[Feature Request] : get the offset of a highlighted field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chongzhe</reporter><labels><label>:Highlighting</label><label>feature</label></labels><created>2014-04-08T22:28:58Z</created><updated>2017-07-25T16:58:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mfn" created="2015-05-02T19:22:20Z" id="98389774">I would be interested in this too, if it's technically possible.

My usecase would be: the string I'm indexing/analyzing/highlighting has additional annotational data I already use to mark up other elements and when receiving the highlighting information I've to combined both.

An example:
- String I'm indexing:&lt;br&gt;`Contains a http://link.to.somewhere/ and @someuser someone`
- Due my own annotational information this gets turned into:&lt;br&gt;`Contains a &lt;a href="http://link.to.somewhere/"&gt;http://link.to.somewhere/&lt;/a&gt; and &lt;a href="https://some.where/profile/231321131"&gt;@someuser&lt;/a&gt; someone`
- And mix this in with the highlighted result of ES when e.g. searching for `some*` I've to include this now appropriately.

Having based on my indexed data the raw offsets available would help. It is possible to do on the client by using special highlight tags to parse out that information but it would be easier if it would already be sent from ES.

Example query:

``` JSON
{
  "query": {
    "wildcard": {
      "field": {
        "value": "some*"
      }
    }   
  },
  "highlight": {
    "fields": {
      "field": {
        "number_of_fragments": 0,
        "return_as": "offsets"
      }
    }
  }
}
```

Example result payload:

``` JSON
{
   "hits": {
      "hits": [
         {
            "_index": "myindex",
            "_type": "Something",
            "_id": "asd92jsdfodsfsf",
            "_score": 1,
            "_source": {
               "field": "Contains a http://link.to.somewhere/ and @someuser someone",
            },
            "highlight": {
               "field": [
                  {
                    "start": 42,
                    "lenth": 8
                  }
               ]
            }
         },
```

My imaginary `return_as` would denote the format:
- missing or `content` would be the current behaviour
- `offsets` would return the `start` and `length` tuple objects
- `content_offsets` would return both, e.g.

``` JSON
"highlight": {
   "field": [
      {
        "start": 42,
        "length": 8,
        "content": "&lt;em&gt;someuser&lt;/em&gt;"
      }
   ]
}
```

As can be seen due the additional data the format has to changed to from an array of string to an array of objects.

This was just quickly on top of my head. I bet I forgot to think about a ton of other information :-)
</comment><comment author="lariverosc" created="2015-05-08T22:04:05Z" id="100379768">+1
</comment><comment author="chongzhe" created="2015-05-08T23:06:21Z" id="100391225">It's been more than a year since the issue is opened... is there any update on this?
</comment><comment author="nik9000" created="2015-05-08T23:34:06Z" id="100394024">I just cracked open eclipse to implement it in the experimental highlighter
plugin. But now I'm playing with my kids.
On May 8, 2015 7:06 PM, "chongzhe" notifications@github.com wrote:

&gt; It's been more than a year since the issue is opened... is there any
&gt; update on this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/5736#issuecomment-100391225
&gt; .
</comment><comment author="nik9000" created="2015-05-09T23:59:05Z" id="100564087">And patch proposed: https://gerrit.wikimedia.org/r/#/c/209956

If you are willing to use the plugin then that _might_ be enough for you.

I think @mfn can get what he wants from it using the `none` fragmenter. One thing, though, is that Elasticsearch limits me to returning text as the result of the highlight request so I can't make fancy json and you'll have to breakout the Splitters.
</comment><comment author="mfn" created="2015-05-10T09:00:53Z" id="100605708">Wow, thanks for the effort.

&gt; Elasticsearch limits me to returning text as the result of the highlight request so I can't make fancy json

Sad to hear, that's a bit of a bummer though. I'm trying to deduce a possible format from the test case result, could it look like this?

`0:0-5,18-22:22`

``` JSON
{
  "start_offset": 0,
  "end_offset": 22,
  "hits": [
    {
      "start_offset": 0,
      "end_offset": 5,
    },
    {
      "start_offset": 18,
      "end_offset": 22,
    }
  ]
}
```

`23:33-37:37`

``` JSON
{
  "start_offset": 23,
  "end_offset": 27,
  "hits": [
    {
      "start_offset": 33,
      "end_offset": 37,
    }
  ]
}
```

I understand it closely resembles the `Snippet` class, but some remarks:
- I've worked with highlighting and I always found it easier to work with the _positive_ approach, i.e. `start_offset + length = end_offset` instead of the _negative_ approach of `end_offset - start_offset = length`, i.e. preferring having the `length` instead of the end; simply because most string APIs _I_ work with, work with lengths
- I think it would be necessary to actually know which the `content` was which was highlighted, which doesn't seem to be available (see https://github.com/elastic/elasticsearch/issues/5736#issuecomment-98389774 )

Well, nothing really of value to add I guess, still glad someones igniting the fire :-)
</comment><comment author="icode" created="2017-04-21T12:10:16Z" id="296174137">+1  how to use&#65311;not fix&#65311;</comment><comment author="hugo53" created="2017-07-25T10:26:54Z" id="317696466">Wonder why this issue still remains Open for more than 3 years.
@javanna Any milestone for this helpful feature bro?</comment><comment author="nik9000" created="2017-07-25T16:58:34Z" id="317801890">Highlighting is just not at the top of anyone's list. And highlighting is complicated because there are 4 highlighters so every feature needs to be implemented four times. One day, someone is going to start really caring about highlighting again and will probably go and do this. As you can see from the issue history, highlighting used to be a big deal to me, now I have other things I spend my days working on.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>(allocation.asciidoc) typo fix in line 102</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5735</link><project id="" key="" /><description /><key id="31087867">5735</key><summary>(allocation.asciidoc) typo fix in line 102</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dvrajan</reporter><labels /><created>2014-04-08T16:45:22Z</created><updated>2014-07-16T21:46:37Z</updated><resolved>2014-04-17T14:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-08T16:56:04Z" id="39873384">Hi @dvrajan, can you sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this? Thanks!
</comment><comment author="dvrajan" created="2014-04-08T18:01:09Z" id="39880913">signed!
</comment><comment author="dakrone" created="2014-04-17T14:46:20Z" id="40721946">Looks like this has already been fixed, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The clear scroll apis should optionally accepts a scroll_id in the request body.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5734</link><project id="" key="" /><description>PR ofr #5726
</description><key id="31080000">5734</key><summary>The clear scroll apis should optionally accepts a scroll_id in the request body.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T15:19:08Z</created><updated>2015-06-07T21:17:24Z</updated><resolved>2014-04-11T04:21:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-11T04:21:21Z" id="40169597">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update "Character classes" part</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5733</link><project id="" key="" /><description>Few changes in "Character classes" part. The same thing was written two times.
</description><key id="31067293">5733</key><summary>Update "Character classes" part</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">audreyrsc</reporter><labels /><created>2014-04-08T12:50:23Z</created><updated>2014-07-16T21:46:38Z</updated><resolved>2014-05-06T14:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T14:01:39Z" id="42305329">Hi @audreyrsc 

Thanks for the PR. Sorry it has taken a while to get to it.  Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-05-06T14:06:24Z" id="42306015">Hi @audreyrsc 

Sorry I see you have already signed. Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Controlling shard placing in a globally distributed cluster.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5732</link><project id="" key="" /><description>I've read a number of articles / forums on the placing of indexes/shards but have not yet found a solution to my requirement.

Fundamentally, I want to use Logstash (+ Elasticsearch/Kibana) to build a globally distributed cluster, but I want to limit the placement of primary and replica shards to be local to the region they were created in to reduce WAN traffic, but I also want to be able to query all data as a single dataset.

Example
Let's say I have two ES nodes in UK (uknode1/uknode2), and two in US (usnode1/usnode2). If Logstash sends some data to usnode1, I want it to place the replica on usnode2, and not send this across the WAN to the uknode\* nodes.

I've tried playing around with index and routing allocation settings, but cannot stop the shards being distributed across all 4 nodes. It's slightly complicated by the fact that index names are dynamically built based on the "type" but that's another challenge for a later date. Even with one index, I can't get it to work.

node.zone: etf1

e.g. I thought this config would only sync between zones (nodes) "testlab1" and "testlab2", and explicitly exclude "zones" beginning with "l" (for live).
node.zone: testlab1
index.routing.allocation.exclude.zone: l*
index.routing.allocation.include.zone: testlab1,testlab2

I could split the cluster into regional cluster, but as I want to be able to query all nodes as a single dataset (via Kibana) that doesn't sound like  a valid option at this stage as Kibana can only query one cluster afaik.

So, is this request even possible to achieve?

The reason I ask if this is possible is what would happen if I write to an index called "myTest" on UK node, and the same index on a US node.....as this is ultimately the same index and I'm not sure how ES would handle this.

So if anyone has any suggestions, or just some words around why this is not possible, that would be very helpful.
</description><key id="31066377">5732</key><summary>Controlling shard placing in a globally distributed cluster.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schmorgs</reporter><labels /><created>2014-04-08T12:36:05Z</created><updated>2014-04-09T08:21:01Z</updated><resolved>2014-04-09T08:21:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schmorgs" created="2014-04-09T08:14:51Z" id="39938849">I haven't stopped looking into a solution for this and came across a great solution.

In Elasticsearch 1.0.1, there is a concept called Tribe nodes.

What you do is allocate a new ES node as a tribe node, and configure it to connect to all your other clusters, and when you run a query against it, it queries all clusters and returns a consolidated set of results from all of them.

So in my scenario, it looks like this.

US Region
  cluster.name: us-region
  Two nodes in this region called usnode1 and usnode2
  Both nodes are master/data nodes

UK Region
  cluster.name: uk-region
  Two nodes in this region called uknode1 and uknode2
  Both nodes are master/data nodes

The you create another ES node and add some configuration to make it a Tribe node.
elasticsearch.yml --&gt; 

node.data: false
node.master: false
tribe.blocks.write: false
tribe.blocks.metadata: false
tribe.t1.cluster.name: us-region
tribe.t1.discovery.zen.ping.unicast.hosts: ["usnode1","usnode2"]
tribe.t2.cluster.name: uk-region
tribe.t2.discovery.zen.ping.unicast.hosts: ["uknode1","uknode2"]

You then point Kibana to the tribe node and it worked brilliantly - excellent feature.

Kibana dashboards still save, although I'm not sure which cluster they save to yet :) but seems to address my question.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix P/C assertions for rewrite reader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5731</link><project id="" key="" /><description>Inner queries must be rewritten as soon as a weight is pulled ie. must
be non-null.
</description><key id="31065853">5731</key><summary>Fix P/C assertions for rewrite reader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T12:27:54Z</created><updated>2015-06-07T14:40:10Z</updated><resolved>2014-04-08T15:54:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-08T12:29:18Z" id="39841630">@martijnvg  can you look at this?
</comment><comment author="martijnvg" created="2014-04-08T12:32:37Z" id="39841917">@s1monw The changes to ChildrenConstantQuery and ParentConstantQuery look good, but we should do the similar change to ChildrenQuery and ParentQuery as well.
</comment><comment author="s1monw" created="2014-04-08T12:41:51Z" id="39842728">@martijnvg more cleanups :)
</comment><comment author="martijnvg" created="2014-04-08T12:44:49Z" id="39842943">@s1monw Thanks! and LGTM
</comment><comment author="boonhero" created="2014-04-08T12:45:04Z" id="39842971">l

Sent from Yahoo Mail on Android
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>clear scroll throws 500 on array out of bounds exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5730</link><project id="" key="" /><description>```
DELETE http://127.0.0.1:9200/_search/scroll/asdasdadasdasd HTTP/1.1
```

Returns the following response

```
HTTP/1.1 500 Internal Server Error
Content-Type: application/json; charset=UTF-8
Content-Length: 73

{
  "error" : "ArrayIndexOutOfBoundsException[1]",
  "status" : 500
}
```

While a 404 is expected. 

It would also be nice if we can allow the scroll id to be posted. I've had people hit problems with scroll ids that are too big in the past:

https://github.com/elasticsearch/elasticsearch-net/issues/318
</description><key id="31064097">5730</key><summary>clear scroll throws 500 on array out of bounds exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>bug</label></labels><created>2014-04-08T11:57:33Z</created><updated>2014-05-13T10:25:09Z</updated><resolved>2014-05-12T08:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-08T14:31:56Z" id="39854876">@Mpdreamz Actually that specific scroll id is malformed and that is where the ArrayIndexOutOfBoundsException comes from, so I think a 400 should be returned?

If a non existent scroll_id is used, it will just return and act like everything is fine. I agree a 404 would be nice.
</comment><comment author="clintongormley" created="2014-04-08T16:25:19Z" id="39869644">++404
</comment><comment author="Mpdreamz" created="2014-04-08T17:31:49Z" id="39877539">++404 and +1 on implementing #5726 @martijnvg ! 
</comment><comment author="martijnvg" created="2014-04-09T11:29:03Z" id="39953386">PR #5738 only addresses invalid scroll ids. Returning a 404 for a valid, but non existing scroll id requires more work than just validation. The clear scoll api uses an internal free search context api, which for example the search api relies on. This internal api just always returns an empty response. I can change that, so that it includes whether it actually has removed a search context, but that requires a change in the transport layer, so I like to do separate that in a different PR.
</comment><comment author="s1monw" created="2014-04-14T20:47:59Z" id="40416136">LGTM
</comment><comment author="s1monw" created="2014-04-14T20:48:38Z" id="40416202">@martijnvg can you assign the fix version here please
</comment><comment author="martijnvg" created="2014-04-16T02:34:38Z" id="40557258">@s1monw PR #5738 only handles invalid scroll ids, but this issue is also about returning a 404 when a valid scroll id doesn't exist. I will assign the proper versions in PR and leave this issue open, once the missing scroll id has been addressed this issue can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing scroll ID no longer returns exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5729</link><project id="" key="" /><description>As of commit 705c7e2469546fcb241f119570265a76262eac75 running a scroll request on a bad scroll ID no longer returns a 500 request error.  Instead, each shard returns a failure but the overall request is a 200 OK.

/cc @kimchy 
</description><key id="31060086">5729</key><summary>Missing scroll ID no longer returns exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T10:44:22Z</created><updated>2014-05-05T15:55:13Z</updated><resolved>2014-05-05T15:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2014-04-08T12:02:02Z" id="39839540">@clintongormley wondering why we don't respond with a 400 or even a 404 when the scroll id does not exist (contains out of date information) ?
</comment><comment author="clintongormley" created="2014-04-08T16:24:08Z" id="39869494">We used to respond with a 500, but maybe a 404 would be more RESTful
</comment><comment author="Mpdreamz" created="2014-04-08T17:29:45Z" id="39877278">+1 on the 404 or 400. 
</comment><comment author="pickypg" created="2014-04-11T00:06:58Z" id="40158766">500 represents an internal server error, so anything in the 400 range (404 in particular representing the resource was not found) would be a lot better, thus indicating to the end user that the problem was on their end (input), rather than putting the blame onto Elasticsearch.

So +1 to HTTP `404`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A nested `nested` aggregation falls outside of its parent `nested` aggregation bounds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5728</link><project id="" key="" /><description>Every `nested` aggs uses the non nested docs filter as parent reference. This can lead to incorrect doc counts and other metrics.

This PR sets the proper parent docs for nested `nested` aggs, which depends on which is the closest parent nested aggs.
</description><key id="31053573">5728</key><summary>A nested `nested` aggregation falls outside of its parent `nested` aggregation bounds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T09:00:03Z</created><updated>2015-06-07T21:16:28Z</updated><resolved>2014-04-11T03:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-08T10:19:14Z" id="39832086">Good catch!
</comment><comment author="dazraf" created="2014-04-08T13:46:44Z" id="39849201">Hi Martijn,

Many thanks for this. I've tested this with v1.1.0 (excluding the test case which couldn't be merged).
Works correctly. Here's the script that I used and the respective results.
https://gist.github.com/dazraf/10126360

kind regards
Fuzz.
</comment><comment author="dazraf" created="2014-04-08T13:47:37Z" id="39849317">I hope this is the right way to use the aggregation. Please let me know if there's a better way. Thanks
</comment><comment author="martijnvg" created="2014-04-08T15:09:27Z" id="39859648">@dazraf That way of using aggregations looks all right to me.
</comment><comment author="martijnvg" created="2014-04-08T17:06:50Z" id="39874622">@jpountz I tried to get around the symmetry issue you just described by setting the parentFilter field lazily in the setNextReader() method. I don't think that this is the nicest fix (parentFilter field is not final and gets set lazily), but I think it is good enough.
</comment><comment author="jpountz" created="2014-04-09T07:19:44Z" id="39935335">Agreed that it is good enough now. +1 to merge.

Thanks @martijnvg for fixing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>README.textile Searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5727</link><project id="" key="" /><description>The README contains this excerpt:

We can also use the JSON query language Elasticsearch provides instead of a query string:

curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
{ 
    "query" : { 
        "text" : { "user": "kimchy" }
    } 
}'

But when I try to run it I get this error:

{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[mp8It5yKTLSkvuIfiAX0pw][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : { \n        \"text\" : { \"user\": \"kimchy\" }\n    } \n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][4]: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : { \n        \"text\" : { \"user\": \"kimchy\" }\n    } \n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : { \n        \"text\" : { \"user\": \"kimchy\" }\n    } \n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : { \n        \"text\" : { \"user\": \"kimchy\" }\n    } \n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : { \n        \"text\" : { \"user\": \"kimchy\" }\n    } \n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }]",
  "status" : 400
}

 changed "text" in the query to "term" and it worked. When I looked up the query DSL there was no keyword "text" either.
</description><key id="31053125">5727</key><summary>README.textile Searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">sparkle-sparkle</reporter><labels><label>docs</label></labels><created>2014-04-08T08:52:26Z</created><updated>2014-04-08T08:58:25Z</updated><resolved>2014-04-08T08:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clear scroll should accept scroll_id in body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5726</link><project id="" key="" /><description>In https://github.com/elasticsearch/elasticsearch-perl/issues/24 a user was generating scroll IDs which were too long for the intervening proxy to handle in the URL or query string. The same problem would apply to the clear-scroll API.

The clear-scroll API should optionally accept the scroll_id in the body as well as in the URL
</description><key id="31052726">5726</key><summary>Clear scroll should accept scroll_id in body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T08:45:49Z</created><updated>2014-04-11T04:20:59Z</updated><resolved>2014-04-11T04:20:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Failed shards could be re-assigned to the same nodes if multiple replicas failed at once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5725</link><project id="" key="" /><description>After a shard fails on a node we assign a new replica on another node. This is important in order to avoid failing again due to node specific problems. In the rare case where two different replicas of the same shard failed in a short time span, we may fail to do so and assign one of them back to the node it's currently on. This happens if both shard failed events are processed within the same batch on the master.
</description><key id="31052254">5725</key><summary>Failed shards could be re-assigned to the same nodes if multiple replicas failed at once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T08:37:30Z</created><updated>2015-06-07T21:17:36Z</updated><resolved>2014-04-08T10:11:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-08T09:53:59Z" id="39830252">LGTM good catch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide more context variables in update scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5724</link><project id="" key="" /><description>In addition to `_source`, the following variables are available through the `ctx` map: `_index`, `_type`, `_id`,  `_version`, `_routing`, `_parent`, `_timestamp`, `_ttl`.

Some of these fields are more useful still within the context of an Update By Query, see #1607, #2230, #2231.
</description><key id="31052159">5724</key><summary>Provide more context variables in update scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">ofavre</reporter><labels><label>:CRUD</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-08T08:36:03Z</created><updated>2015-06-07T17:07:58Z</updated><resolved>2014-11-14T09:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2014-04-10T13:57:10Z" id="40084333">Changed what @pickypg proposed + rebased to current master.
</comment><comment author="clintongormley" created="2014-10-20T13:06:24Z" id="59746604">@dakrone please could you take a look at this
</comment><comment author="dakrone" created="2014-10-20T15:11:38Z" id="59775016">@ofavre this looks pretty good, can you rebase on the latest master since this is an older PR and make sure this still works now that Groovy is the default scripting language?
</comment><comment author="ofavre" created="2014-11-10T22:36:23Z" id="62468864">Thanks for the suggestions, they were valuable, and I managed to simplify the ttl assertion a bit.
I rebased on current master.
</comment><comment author="dakrone" created="2014-11-11T10:00:30Z" id="62525587">@ofavre thanks for the update! I left one more comment about a failing test, if you can fix that I'll merge this in
</comment><comment author="ofavre" created="2014-11-11T15:29:37Z" id="62563022">I somehow rebase on a 3 month old master.
Everything should now be ok.
</comment><comment author="dakrone" created="2014-11-12T08:41:26Z" id="62686082">Hi @ofavre 

I left another comment, running the tests with your change causes the following to fail:

```
  - org.elasticsearch.document.BulkTests.testBulkUpdate_largerVolume
  - org.elasticsearch.routing.AliasRoutingTests.testAliasCrudRouting
```
</comment><comment author="ofavre" created="2014-11-13T21:14:59Z" id="62967311">You're right, I've added a check for nullity in the else clause.
Sorry for that.
</comment><comment author="dakrone" created="2014-11-14T09:35:48Z" id="63031815">Pushed to 1.x and master, thanks @ofavre !
</comment><comment author="ofavre" created="2014-11-14T21:38:07Z" id="63132980">Thank you @clintongormley and @dakrone for pushing this forward! ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation of MAX_LOCKED_MEMORY, ES config vs limits.conf unclear</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5723</link><project id="" key="" /><description>re:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-service.html

(specifically ES 1.1.0 on Ubuntu 12.04 LTS)

I have seen some conflicting suggestions on the settings for
/etc/init.d/elasticsearch

specifically:
MAX_LOCKED_MEMORY, and MAX_OPEN_FILES.

If you set MAX_LOCKED_MEMORY=unlimited and MAX_OPEN_FILES=65535, do you also need to update /etc/security/limits.conf:
elasticsearch - nofile 65535
elasticsearch - memlock unlimited

or, does elasticsearch automatically configure the elasticsearch user that the service runs on based on these settings in /etc/init.d/elasticsearch?
</description><key id="31038645">5723</key><summary>Documentation of MAX_LOCKED_MEMORY, ES config vs limits.conf unclear</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffsteinmetz</reporter><labels /><created>2014-04-08T02:29:12Z</created><updated>2014-12-30T23:27:24Z</updated><resolved>2014-12-30T15:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T15:24:24Z" id="68364936">Hi @jeffsteinmetz 

Sorry it has taken a while to get to this.  Setting the environment variables in /etc/init.d/elasticsearch is sufficient.
</comment><comment author="markwalkom" created="2014-12-30T23:27:24Z" id="68410264">Setting these in /etc/defaults/elasticsearch (Debian/Ubuntu) or /etc/sysconfig/elasticsearch (RHEL/Centos) is better as your init script may change during an upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bulk.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5722</link><project id="" key="" /><description>In code example, jsonBuilder() calls are missing class references.  Should be XContentFactory.jsonBuilder().
</description><key id="31033950">5722</key><summary>Update bulk.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-04-08T00:24:17Z</created><updated>2014-07-16T21:46:40Z</updated><resolved>2014-04-08T00:47:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-08T00:32:24Z" id="39799989">@piuspfung in the document, there is a static import `import static org.elasticsearch.common.xcontent.XContentFactory.*;` so `jsonBuilder()` can be used directly
</comment><comment author="ppf2" created="2014-04-08T00:47:29Z" id="39800785">thanks i missed the static reference in the import
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added syntactic sugar over agg definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5721</link><project id="" key="" /><description>- some aggregations typically tend to be requested together. But using the normal (formal) dsl can make the request too verbose. The agg factory/parsing infrastructure was changed to enable factories to automatically add more factories, and by that enables "syntatctic sugar" to save verbosity
- one common use case for this is the `missing` aggregation. Often, when running values source aggs (e.g. terms, avg, etc...) one also wants to get the count of those document that are not associated with values (therefore were not aggregated). Now, using the new infrastructure, one can define a `track_missing` setting on any values source based aggregation, and a `missing` aggregation will automatically be injected into the request - based on the same value source.

Closes #5324
</description><key id="31032657">5721</key><summary>Added syntactic sugar over agg definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2014-04-07T23:53:49Z</created><updated>2015-03-20T21:46:05Z</updated><resolved>2015-03-20T21:45:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2014-04-08T01:27:35Z" id="39802847">@uboness I could not play with it and docs do not make iy clear if it replicate sub aggs defined within an aggs that are marked to track missing. In other words if I agg by state and then city and mark state agg to track missing there will be city buckets within missing bucket along with any accumulator aggs such as sum avg etc  defined within city?

Also I feel it would be much more convenient to have missing as part of normal bucket array with key specified when enabling missing tracking for the agg rather than as sibling node. After all missing is just another value (granted somewhat special) of that field. It is just like the databases let you handle null values almost like the normal ones and with great ease and convenience
</comment><comment author="uboness" created="2014-04-08T01:37:55Z" id="39803383">@roytmana adding `track_missing` in the states `terms` agg will add a sibling `states_missing` agg next to it. it will be an **empty** missing agg, with only the `doc_count` that will indicate the number of docs missing the field. you'll need to specify `track_missing` on the cities aggs to have `cities_missing` agg injected next to that one

&gt; Also I feel it would be much more convenient to have missing as part of normal bucket array with key specified when enabling missing tracking for the agg rather than as sibling node. After all missing is just another value (granted somewhat special) of that field. It is just like the databases let you handle null values almost like the normal ones and with great ease and convenience

so, for the missing, we decided this is the best we'll go with that regard, as adding the missing count to the aggs themselves also adds complexity we'd like to avoid. That said, we do plan very soon (this week perhaps) to let you define default values for missing docs, which will enable you to treat those like you'd treat `null` values in the db.
</comment><comment author="roytmana" created="2014-04-08T01:47:15Z" id="39803847">Thanks for the info @uboness. Then as you said it is truly is a syntactic sugar adding little value if it does not replicate sub aggs defined within the agg with tracked null values. I would like to re-open my request since it solves none of the problems I outlined in it if you do not mind?
</comment><comment author="uboness" created="2014-04-08T01:58:46Z" id="39804399">@roytmana well, it's there to solve the verbosity of the request, which seemed to be your main concern outlined above. As for the response, the difference in how you'd fetch the data is negligible I'd say:

```
aggs.states._missing
```

vs

```
aggs.states_missing.value
```

In any case, you can also wait for the default values feature I mentioned above and use that if it better fits your needs
</comment><comment author="roytmana" created="2014-04-08T02:07:01Z" id="39804815">Thank you @uboness I  will wait for default values.
The major complexity that is not solved is in having to replicate sub aggs within each missing definition. In my example I still want my city breakdown within missing for states and all the sub aggs I defined within the city.

It would have been very nice to have an optional "other" bucket in aggs as well :-)
</comment><comment author="jpountz" created="2014-04-08T08:39:07Z" id="39824239">It doesn't feel right to me to make `process` return sibling aggregators: it makes this method perform two very different things. Maybe we could instead make either the parser return several factories or the factories return several aggregators?

Although I think it is nice to users to have documentation of `track_missing` for every aggregation, I think the duplication will make it a pain to update. Maybe it should be written only once and then included or referenced in every aggregation that exposes it (so that there is a single place to update)?

Regarding the syntax, maybe we should make it future-proof in case we would also like to be able to replicate the sub-tree of aggregations under the `missing` agg? Something like:

``` javascript
"track_missing": {
    "term_missing": "counts"
}
```

where `term_missing` is the key under which the missing aggregation should be and `counts` tells that it should just track missing counts.
</comment><comment author="uboness" created="2014-04-08T10:12:20Z" id="39831656">&gt; It doesn't feel right to me to make process return sibling aggregators: it makes this method perform two very different things. Maybe we could instead make either the parser return several factories or the factories return several aggregators?

I'm not sure I agree with that, the whole idea of syntactic sugar is that it's just applied on the **syntax**, which means this syntax represents a more elaborated syntax. So this behaviour should not (almost by definition) propagate to the underlying implementation of the factories or the aggregators... it should be done on the highest level possible.

&gt; Although I think it is nice to users to have documentation of track_missing for every aggregation, I think the duplication will make it a pain to update. Maybe it should be written only once and then included or referenced in every aggregation that exposes it (so that there is a single place to update)?

I tend to agree, there's just no place for common settings in the docs now. I did try to put it on the "main" aggs and share it between related ones (e.g. range, date_range, ip_range). Maybe we should consider adding a section for common configurations (also put the values source configuration there?), but I'd do that on a different PR

&gt; Regarding the syntax, maybe we should make it future-proof in case we would also like to be able to replicate the sub-tree of aggregations under the missing agg?

I don't think it's not future proof right now... if needed, in the future we could always introduce support for objects as well (nothing prevents us from doing that)
</comment><comment author="jpountz" created="2014-04-09T12:41:21Z" id="39958792">&gt;  it should be done on the highest level possible.

I'm good with that, but can we keep concerns separated and not do both validation and return the siblings in the same method?

&gt; Maybe we should consider adding a section for common configurations (also put the values source configuration there?), but I'd do that on a different PR

OK, can you open an issue already and mark it as a blocker for 1.2 so that we don't forget?

&gt; I don't think it's not future proof right now... if needed, in the future we could always introduce support for objects as well (nothing prevents us from doing that)

OK.
</comment><comment author="s1monw" created="2015-03-20T21:45:56Z" id="84161455">this is stale for alomst 1 year. I am closing it for now - we can still revisit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update index_.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5720</link><project id="" key="" /><description>The 2 code examples referencing jsonBuilder() on this page are missing the class reference, should be XContentFactory.jsonBuilder().startObject()...., instead of jsonBuilder().startObject()....
</description><key id="31031258">5720</key><summary>Update index_.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-04-07T23:25:27Z</created><updated>2014-07-16T21:46:40Z</updated><resolved>2014-04-08T00:45:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2014-04-08T00:45:27Z" id="39800688">sorry missed the static definition in the import statement, this should be ok.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use loopback when localhost is not resolved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5719</link><project id="" key="" /><description>we use the "local host" address in sevearl places in our networking layer, if local host is not resolved for some reason, still continue and operate but using the loopback interface
</description><key id="31028953">5719</key><summary>Use loopback when localhost is not resolved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T22:43:00Z</created><updated>2015-06-07T14:40:16Z</updated><resolved>2014-04-21T18:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-21T18:51:56Z" id="40963889">I left one comment - otherwise LGTM
</comment><comment author="kimchy" created="2014-04-21T18:57:13Z" id="40964495">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include name of field that caused circuit breaking exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5718</link><project id="" key="" /><description>We could include the name of the field that is potentially blowing out memory with the exception so it's easier for people to tell which field is using the memory.

This could still be a bit misleading when loading more than one field at a time, but may be more useful than not including any field name.
</description><key id="31027931">5718</key><summary>Include name of field that caused circuit breaking exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T22:25:41Z</created><updated>2014-04-23T16:07:17Z</updated><resolved>2014-04-23T16:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-07T23:03:04Z" id="39794457">++, it would be nice to improve the failure from `data would be larger than limit of [large_number_here]` to `data would be larger than limit of [large_number_here/human number here]`, so its simpler to know right away what the limit here.
</comment><comment author="dakrone" created="2014-04-07T23:43:11Z" id="39797132">Yes, I agree, I'll add that as well.
</comment><comment author="s1monw" created="2014-04-14T20:34:01Z" id="40414594">++ - would be good to get this into `1.2.0`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Remove site dependencies generation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5717</link><project id="" key="" /><description>Removed dependencies library generation.  It stalls Jenkins static analysis job
</description><key id="31027327">5717</key><summary>[BUILD] Remove site dependencies generation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-04-07T22:16:11Z</created><updated>2014-07-16T21:46:41Z</updated><resolved>2014-04-07T23:04:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-07T22:49:31Z" id="39793429">LGTM, maybe you can add to the commit the reason why it stalls jenkins (that glassfish thingy), so we have a paper trail of the data.
</comment><comment author="mrsolo" created="2014-04-07T22:55:01Z" id="39793833">This temporary fix is to go around Jenkins build stalling on dependency report generation.  The reason for stalling is still under investigation.  

After the fix the project site will have index.html landing page that points to result of findbug, pmd and what not which is more important.

Elasticsearch project has another way to get third party license information, although not as nice

'mvn license:aggregate-add-third-party -Dlicense.generation=true'

[INFO] Generating "Dependencies" report    --- maven-project-info-reports-plugin:2.7
[ERROR] Unable to determine if resource asm:asm:jar:3.1:test exists in http://maven.glassfish.org/content/groups/glassfish
...
[ERROR] Unable to determine if resource org.apache.lucene:lucene-queryparser:jar:4.7.1:compile exists in http://maven.glassfish.org/content/groups/glassfish
[ERROR] Unable to determine if resource org.apache.lucene:lucene-sandbox:jar:4.7.1:compile exists in http://maven.glassfish.org/content/groups/glassfish
[ERROR] Unable to determine if resource org.apache.lucene:lucene-spatial:jar:4.7.1:compile exists in http://maven.glassfish.org/content/groups/glassfish
channel stopped
Build was aborted
</comment><comment author="mrsolo" created="2014-04-07T23:04:08Z" id="39794531">Committed

https://github.com/elasticsearch/elasticsearch/commit/dcc6a6e13831a362bc620fe3e02e9b2d761de293
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes typo in "Scan" search type documention</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5716</link><project id="" key="" /><description /><key id="31025351">5716</key><summary>Fixes typo in "Scan" search type documention</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndrewO</reporter><labels><label>docs</label><label>v0.90.14</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T21:47:47Z</created><updated>2014-07-04T06:02:18Z</updated><resolved>2014-04-07T22:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-07T22:02:57Z" id="39789522">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticsearchIllegalStateException when invoking _cat plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5715</link><project id="" key="" /><description>Reproducible using ES 1.1.0.  
/_cat/plugins works fine when no plugins are installed.  But once a plugin is installed (eg. latest marvel), it throws an exception indicating that the resulting table constructed has 7 fields in the header row but not enough fields in the data rows to make it a valid table.

{
  "status" : 500,
  "error" : "ElasticsearchIllegalStateException[mismatch on number of cells 6 in a row compared to header 7]"
}
</description><key id="31024582">5715</key><summary>ElasticsearchIllegalStateException when invoking _cat plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T21:37:00Z</created><updated>2015-06-07T21:17:44Z</updated><resolved>2014-04-08T09:33:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-07T22:00:51Z" id="39789339">Thanks for reporting it. Will push a fix.
</comment><comment author="dadoonet" created="2014-04-07T22:04:55Z" id="39789716">Could you list which plugins / versions do you have?
</comment><comment author="ppf2" created="2014-04-07T22:08:42Z" id="39790081">I applied the latest marvel earlier today and the jar version downloaded is
marvel-1.1.0.jar

On Mon, Apr 7, 2014 at 3:05 PM, David Pilato notifications@github.comwrote:

&gt; Could you list which plugins / versions do you have?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5715#issuecomment-39789716
&gt; .
</comment><comment author="dadoonet" created="2014-04-08T09:30:19Z" id="39828362">I found it! Thanks for reporting. Actually this issue was introduced by this commit c34d8bd. The header was not updated with this change.

Going to push a fix on 1.1 branch. Other branches are not concerned by this issue.
</comment><comment author="dadoonet" created="2014-04-08T09:33:51Z" id="39828645">Closed by abc453daf368225c4dde8d7d0fb0acdda8644f5f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch won't start anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5714</link><project id="" key="" /><description>After making a simple change on a query in Kibana, my Elasticsearch instance stopped working and I can't start it up again. I'm using ES 0.90.9 on OSX using homebrew.

Normally I would use this to start ES:
`elasticsearch -f -D es.config=/usr/local/opt/elasticsearch/config/elasticsearch.yml`

This however throws out a repeated error:
`
[2014-04-07 15:59:02,123][INFO ][node                     ] [Puck] version[0.90.9], pid[8758], build[a968646/2013-12-23T10:35:28Z]
[2014-04-07 15:59:02,128][INFO ][node                     ] [Puck] initializing ...
[2014-04-07 15:59:02,224][INFO ][plugins                  ] [Puck] loaded [mongodb-river, mapper-attachments, marvel], sites [river-mongodb, marvel]
[2014-04-07 15:59:04,553][INFO ][node                     ] [Puck] initialized
[2014-04-07 15:59:04,553][INFO ][node                     ] [Puck] starting ...
[2014-04-07 15:59:04,665][INFO ][transport                ] [Puck] bound_address {inet[/127.0.0.1:9302]}, publish_address {inet[/127.0.0.1:9302]}
[2014-04-07 15:59:07,727][INFO ][cluster.service          ] [Puck] new_master [Puck][gtub58OkR9SskDE0SfYobw][inet[/127.0.0.1:9302]], reason: zen-disco-join (elected_as_master)
[2014-04-07 15:59:07,778][INFO ][discovery                ] [Puck] elasticsearch_dannyjoris/gtub58OkR9SskDE0SfYobw
[2014-04-07 15:59:07,795][INFO ][http                     ] [Puck] bound_address {inet[/127.0.0.1:9202]}, publish_address {inet[/127.0.0.1:9202]}
[2014-04-07 15:59:07,796][INFO ][node                     ] [Puck] started
[2014-04-07 15:59:07,813][INFO ][gateway                  ] [Puck] recovered [0] indices into cluster_state
[2014-04-07 15:59:09,589][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]
java.net.SocketTimeoutException: connect timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:382)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:241)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:228)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:431)
    at java.net.Socket.connect(Socket.java:527)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:158)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:424)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:538)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:214)
    at sun.net.www.http.HttpClient.New(HttpClient.java:300)
    at sun.net.www.http.HttpClient.New(HttpClient.java:319)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:987)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:923)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:841)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:313)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:293)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.checkAndUpload(ESExporter.java:428)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.checkAndUploadIndexTemplate(ESExporter.java:464)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.checkAndUploadAllResources(ESExporter.java:341)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:190)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:246)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:134)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:274)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:174)
    at java.lang.Thread.run(Thread.java:695)
[2014-04-07 15:59:09,591][ERROR][marvel.agent.exporter    ] Could not connect to any configured elasticsearch instances: [localhost:9200]
`
</description><key id="31019195">5714</key><summary>Elasticsearch won't start anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DannyJoris</reporter><labels /><created>2014-04-07T20:30:13Z</created><updated>2014-04-07T20:41:45Z</updated><resolved>2014-04-07T20:41:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-07T20:41:45Z" id="39780728">Let&#8217;s see if we can help you using the [mailing list](http://www.elasticsearch.org/help/), and if we end up with an improvement / feature, we can open a dedicated issue for it, I will close it for now.

I saw that you posted it as well on [stack overflow](http://stackoverflow.com/questions/22922232/cant-start-elasticsearch-anymore).

BTW, the Marvel error you get does not mean that your cluster is not working.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Make sure successful operations are correct if second search phase is fast</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5713</link><project id="" key="" /><description> In TransportSearchTypeAction we  need to increment successful ops first before we increment and compare the exit condition otherwise if we are fast we could concurrently update totalOps but then preempt one of the threads which can cause the successor to read a wrong value from successfulOps if second phase is very fast ie. searchType == count etc. This can cause wrong success stats in the search response. 

Recently this caused several test failures because we assert on correct successful shards in count / search
</description><key id="31017788">5713</key><summary> Make sure successful operations are correct if second search phase is fast</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T20:12:28Z</created><updated>2015-06-07T21:19:20Z</updated><resolved>2014-04-07T20:23:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-07T20:14:19Z" id="39777739">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename readPrimitive*Array()/writePrimitive*Array() methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5712</link><project id="" key="" /><description>Make method names shorter and easier to read.

Closes #5711
</description><key id="31008177">5712</key><summary>Rename readPrimitive*Array()/writePrimitive*Array() methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-07T18:12:28Z</created><updated>2014-10-21T23:42:25Z</updated><resolved>2014-04-10T20:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-07T18:15:09Z" id="39764221">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename readPrimitive*Array()/writePrimitive*Array() methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5711</link><project id="" key="" /><description>Rename methods to have shorter names.
</description><key id="31007619">5711</key><summary>Rename readPrimitive*Array()/writePrimitive*Array() methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-04-07T18:05:17Z</created><updated>2014-04-07T18:21:56Z</updated><resolved>2014-04-07T18:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make writePrimitive*() and readPrimitive*() methods public.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5710</link><project id="" key="" /><description>These utility methods are useful for client code to read/write arrays of
primitive types.
</description><key id="31005556">5710</key><summary>Make writePrimitive*() and readPrimitive*() methods public.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T17:42:02Z</created><updated>2015-06-07T14:41:17Z</updated><resolved>2014-04-07T17:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-07T17:55:31Z" id="39761774">@aleph-zero maybe jumping a bit too late here, but why not just name the methods `writeLongArray` &amp; `readLongArray`? as far as I can see we don't have dedicated methods for non-primitive numeric arrays... would make things less verbose
</comment><comment author="aleph-zero" created="2014-04-07T17:58:10Z" id="39762115">@uboness The read/writePrimitive*Array() methods were used internally by these classes and I thought it better to not go refactoring code that didn't need it. 
</comment><comment author="uboness" created="2014-04-07T17:59:45Z" id="39762313">but that's the point... you're now exposing it to the rest of the system, so renaming the methods now is a small change (later loads of classes will be touching these methods)
</comment><comment author="aleph-zero" created="2014-04-07T18:03:58Z" id="39762818">Okay @uboness I'll open a new issue and change the names.
</comment><comment author="s1monw" created="2014-04-07T18:04:53Z" id="39762942">@aleph-zero just change it here? that's just fine a PR is kind of an issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make writePrimitive*() and readPrimitive*() methods public.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5709</link><project id="" key="" /><description>These utility methods are useful for client code to read/write arrays of
primitive types.
</description><key id="31002130">5709</key><summary>Make writePrimitive*() and readPrimitive*() methods public.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>enhancement</label></labels><created>2014-04-07T16:58:23Z</created><updated>2014-07-16T21:46:43Z</updated><resolved>2014-04-07T17:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-07T17:19:06Z" id="39757687">@aleph-zero why do these methods return `Object` instead of the actual type?
</comment><comment author="aleph-zero" created="2014-04-07T17:26:48Z" id="39758505">@s1monw Just trying to make the smallest change that works. I'll change them to return the actual array types.
</comment><comment author="s1monw" created="2014-04-07T17:27:04Z" id="39758531">++
</comment><comment author="aleph-zero" created="2014-04-07T17:36:07Z" id="39759512">Ick. I munged my repo. Going to submit a new PR. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest layer refactoring phase 2 + recycling in http layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5708</link><project id="" key="" /><description>Refactor the rest layer handlers to simplify common code paths (like handling) failures, and introduce optional (enabled for netty) rest channel bytes recycling
</description><key id="30983973">5708</key><summary>Rest layer refactoring phase 2 + recycling in http layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T13:31:13Z</created><updated>2015-06-07T14:42:11Z</updated><resolved>2014-04-07T14:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-07T14:29:13Z" id="39736703">This pull request is very unfortunate that it doesn't have multiple commits to show the individual steps. I am not even sure it could do that but it would have been awesome if it could have separated the handling of the errors and building responses as one commit and the recycling as another. I think we should land this quickly since it has a loooot of goodness... I will give a LGTM here and we should move it in VERY soon and let CI chew on it.
</comment><comment author="kimchy" created="2014-04-07T14:30:22Z" id="39736896">removing the recycling is simple, a few lines change
</comment><comment author="kimchy" created="2014-04-07T14:33:46Z" id="39737310">showed @s1monw the recycling part, I will push it with it, easy to disable if we need to afterwards
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-field Terms Facet recounts the same document (ES 1.0.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5707</link><project id="" key="" /><description>Using terms facet for multiple fields, the terms are counted once per field and not once per document. If for example, the same term appears in two fields of the same document, multi-field terms facet for those fields will count the document twice. 

See example: https://gist.github.com/abronner/10019478

Elasticsearch documentation states the following: "Also note that terms are counted once per document, even if they occur more frequently in that document." (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets.html)
</description><key id="30981318">5707</key><summary>Multi-field Terms Facet recounts the same document (ES 1.0.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abronner</reporter><labels /><created>2014-04-07T12:54:34Z</created><updated>2014-12-30T15:16:45Z</updated><resolved>2014-12-30T15:16:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T15:16:45Z" id="68364272">Closing as facets have been replaced with aggregations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added a AppendingDeltaPackedLongBuffer-based storage format to single value field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5706</link><project id="" key="" /><description>The AppendingDeltaPackedLongBuffer uses delta compression in paged fashion. For data which is roughly monotonic this results in a reduced memory signature.

By default we use the storage format expected to use the least memory. You can force a choice using a new field data setting `memory_storage_hint` which can be set to `ORDINALS`, `PACKED` or `PAGED`

Running some benchmarks on simulated time based data shows 25-30% reduction in memory usage with a very small performance overhead (current implementations uses PACKED as a memory format with 0.5 acceptable overhead ratio):

```

------------------ SUMMARY -------------------------------
docs: 5000000
match percentage: 0.1
memory format hint: PACKED
acceptable_overhead_ratio: 0.5
field data: 19mb
                     name      took    millis
                   hist_l     16.9s        33
------------------ SUMMARY -------------------------------

------------------ SUMMARY -------------------------------
docs: 5000000
match percentage: 0.1
memory format hint: PAGED
acceptable_overhead_ratio: 0.5
field data: 14.6mb
                     name      took    millis
                   hist_l     18.2s        36
------------------ SUMMARY -------------------------------

------------------ SUMMARY -------------------------------
docs: 5000000
match percentage: 0.1
memory format hint: PACKED
acceptable_overhead_ratio: 0.0
field data: 16mb
                     name      took    millis
                   hist_l     17.4s        34
------------------ SUMMARY -------------------------------

------------------ SUMMARY -------------------------------
docs: 5000000
match percentage: 0.1
memory format hint: PAGED
acceptable_overhead_ratio: 0.0
field data: 10.8mb
                     name      took    millis
                   hist_l       21s        42
------------------ SUMMARY -------------------------------

```
</description><key id="30977650">5706</key><summary>Added a AppendingDeltaPackedLongBuffer-based storage format to single value field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T11:52:33Z</created><updated>2015-06-07T14:42:19Z</updated><resolved>2014-04-11T13:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-04-11T12:49:08Z" id="40199600">@jpountz I pushed another commit. Thx for the feedback
</comment><comment author="jpountz" created="2014-04-11T12:57:29Z" id="40200230">Thanks Boaz, this looks great. +1 to merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After restart elasticsearch reads double as array of string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5705</link><project id="" key="" /><description>Hello, first of all I want to say you making crazy useful things, guys, thank you!

I have two elasticsearch twin (versions, configuration, everything) servers with the same data, replicated by logstash: ES1 and ES2. ES1 I restarted for a few hours ago and now I have a problems.

My mapping for logstash type, same for both servers.

```
"nginx_accesslog": {
        "dynamic_templates": [
                {
                        "string_template": {
                                "mapping": { "index": "not_analyzed", "type": "string" },
                                "match": "*",
                                "match_mapping_type": "string"
                        }
                }
        ],
        "_all": { "enabled": false },
        "_source": { "compress": true },
        "properties": { 
                "@timestamp": { "type": "date", "format": "dateOptionalTime" },
                "@version": { "type": "long" },
                "api_key": { "type": "string", "index": "not_analyzed" },
                "body_bytes_sent": { "type": "long" },
                "host": { "type": "string", "index": "not_analyzed" },
                "http_host": { "type": "string", "index": "not_analyzed" },
                "http_method": { "type": "string", "index": "not_analyzed" },
                "http_referer": { "type": "string", "index": "not_analyzed" },
                "http_user_agent": { "type": "string", "index": "not_analyzed" },
                "http_version": { "type": "string", "index": "not_analyzed" },
                "http_x_forwarded_for": { "type": "string", "index": "not_analyzed" },
                "message": { "type": "string", "index": "not_analyzed" },
                "path": { "type": "string", "index": "not_analyzed" },
                "remote_addr": { "type": "string", "index": "not_analyzed" },
                "remote_user": { "type": "string", "index": "not_analyzed" },
                "request": { "type": "string", "index": "not_analyzed" },
                "request_time": { "type": "double" },
                "status": { "type": "long" },
                "tags": { "type": "string", "index": "not_analyzed" },
                "type": { "type": "string", "index": "not_analyzed" }
        }
}
```

Query (I'm using Sense add-on)

```
POST /logstash-2014.04.07/_search
{
    "script_fields": {
       "s_request_time": {
          "script": "doc['request_time'].value"
       }
    },
    "size": 20
}
```

ES2, everything is normal:

```
{
   "took": 63,
   "timed_out": false,
   "_shards": {
      "total": 4,
      "successful": 4,
      "failed": 0
   },
   "hits": {
      "total": 17041240,
      "max_score": 1,
      "hits": [
         {
            "_index": "logstash-2014.04.07",
            "_type": "nginx_accesslog",
            "_id": "LSfAaBwSSDS5rL6utSHQJA",
            "_score": 1,
            "fields": {
               "s_request_time": 0.014
            }
         },
...
```

ES1: Ooops!

```
{
   "took": 51,
   "timed_out": false,
   "_shards": {
      "total": 4,
      "successful": 4,
      "failed": 0
   },
   "hits": {
      "total": 17041131,
      "max_score": 1,
      "hits": [
         {
            "_index": "logstash-2014.04.07",
            "_type": "nginx_accesslog",
            "_id": "tjZo1_JmRpOu5kE2WRfURw",
            "_score": 1,
            "fields": {
               "s_request_time": [
                  " \u0001?PZ\u000e+\u0001\u0003\t\u001c" &lt;-- WAT?!
               ]
            }
         },
```

It's most obvious demonstration of my problem. Another thing is when I'm using data_histogram facet (in kibana) I got ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData.

Purging all indexes fixes problem. Not only new, but old data too misenterpreted as arrays of one string.
Well, I want my doubles back. :)
</description><key id="30969037">5705</key><summary>After restart elasticsearch reads double as array of string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">alexey-medvedchikov</reporter><labels><label>bug</label></labels><created>2014-04-07T09:24:13Z</created><updated>2015-06-07T21:38:16Z</updated><resolved>2014-04-18T14:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-07T19:32:39Z" id="39773272">This might be related to #5623.
</comment><comment author="s1monw" created="2014-04-14T20:52:38Z" id="40416655">@imotov can you try to verify with @alexeymedved that this is fixed now?
</comment><comment author="imotov" created="2014-04-15T00:16:31Z" id="40433031">@alexeymedved could you try reproducing this issue to see if it was fixed in the latest [nightly build](https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/elasticsearch/1.1.1-SNAPSHOT/)? 
</comment><comment author="alexey-medvedchikov" created="2014-04-15T06:32:14Z" id="40449931">@s1monw @imotov With elasticsearch-1.1.1-20140415.001904-55.deb seems everything works fine. Thank you!
</comment><comment author="alexey-medvedchikov" created="2014-04-15T09:42:15Z" id="40463094">I can break it again:
- run long-running query like

```
/_all/_search {
    "facets": {
      "terms": {
          "terms": {
             "script_field": "doc['request_time'].value &lt;= 0.5 ? true : false",
             "size": 10
          }
      }
    }
}
```
- Insert big bunch of new items (by logstash for example)
- Send SIGTERM

Voila:

```
POST /logstash-2014.04.15.08/_search
{
    "script_fields": {
       "s_request_time": {
          "script": "doc['request_time'].getValues()"
       }
    }
}
...
         {
            "_index": "logstash-2014.04.15.08",
            "_type": "nginx_accesslog",
            "_id": "tjZ9fIqLQWedynr4mBhdEw",
            "_score": 1,
            "fields": {
               "s_request_time": [
                  [
                     " \u0001?Kc)}ymHZ",
                     "$\u000b|^\u001aOoNlE",
                     "(_eqT~|vd",
                     ",\u0005~/\r'wg6",
                     "0/rxj?&gt;;",
                     "4\u0002\u0017FS{s",
                     "8\u0017y&lt;5\u001f_",
                     "&lt;\u0001?Kc)}",
                     "@\u000b|^\u001aO",
                     "D_eqT",
                     "H\u0005~/\r",
                     "L/rx",
                     "P\u0002\u0017",
                     "T\u0017y",
                     "X\u0001?",
                     "\\\u000b"
                  ]
               ]
            }
         },
...
```
</comment><comment author="imotov" created="2014-04-15T11:03:27Z" id="40469442">@alexeymedved do you see any exceptions in the log files?
</comment><comment author="alexey-medvedchikov" created="2014-04-15T11:29:41Z" id="40471153">Exception for query like mentioned in my prev comment:

```
[2014-04-15 17:05:24,563][DEBUG][action.search.type       ] [Hobgoblin] [logstash-2014.04.14.10][3], node[JrhBvFHpRgyT5RXv8O9HwA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.04.14.10][3]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.RuntimeException: uncomparable values &lt;&lt; ^A?WoNlEPrX&gt;&gt; and &lt;&lt;0.05&gt;&gt;
        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperationNonNumeric(MathProcessor.java:321)
        at org.elasticsearch.common.mvel2.math.MathProcessor._doOperations(MathProcessor.java:234)
        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperations(MathProcessor.java:79)
        at org.elasticsearch.common.mvel2.ast.BinaryOperation.getReducedValueAccelerated(BinaryOperation.java:114)
        at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
        at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)
        at org.elasticsearch.search.facet.terms.strings.ScriptTermsStringFieldFacetExecutor$Collector.collect(ScriptTermsStringFieldFacetExecutor.java:147)
        at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:61)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Scorer.score(Scorer.java:65)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:256)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:111)
        ... 9 more
Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.String
        at java.lang.String.compareTo(String.java:108)
        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperationNonNumeric(MathProcessor.java:318)
        ... 25 more
```

Exceptions appeared on restart mentioned in prev comment:

```
[2014-04-15 15:45:45,129][WARN ][index.engine.internal    ] [Condor] [logstash-2014.04.14.11][0] Searcher was released twice
org.elasticsearch.ElasticsearchIllegalStateException: Double release
        at org.elasticsearch.index.engine.internal.InternalEngine$EngineSearcher.release(InternalEngine.java:1510)
        at org.elasticsearch.search.internal.DefaultSearchContext.release(DefaultSearchContext.java:212)
        at org.elasticsearch.search.SearchService.freeContext(SearchService.java:551)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:268)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```

Maybe I can provide you a trace of execution or broken index file somehow?
</comment><comment author="imotov" created="2014-04-15T11:48:41Z" id="40472467">@alexeymedved was index logstash-2014.04.15.08 created with the nightly build or before you upgraded? Could you run `curl localhost:9200/logstash-2014.04.15.08/_mapping` and post the result here? Are you using percolator on this cluster?
</comment><comment author="alexey-medvedchikov" created="2014-04-15T12:35:25Z" id="40475899">@imotov 
- index was created by nightly build
- https://gist.github.com/alexeymedved/dc7a3fb376e6f3c80f23#file-logstash-2014-04-15-08_mapping
- no, i'm not using percolator
</comment><comment author="imotov" created="2014-04-15T13:43:22Z" id="40482396">@alexeymedved your mapping looks fine after restart, so I am thinking it might be actually an MVEL issue. For some reason MVEL thinks that it should compare two doubles as strings in your case. I tried to reproduce this issue locally but didn't succeed so far. Any chance you can try reproducing it using [javascript](https://github.com/elasticsearch/elasticsearch-lang-javascript) instead of MVEL?
</comment><comment author="alexey-medvedchikov" created="2014-04-15T15:35:04Z" id="40496572">MVEL just helps me to show what data actually we operating on. Sorry for misleading queries. It's not an MVEL issue for sure because of:

&gt; Another thing is when I'm using data_histogram facet (in kibana) I got ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData.

Exception for this kind of query:

```
[2014-04-15 15:02:49,538][DEBUG][action.search.type       ] [Condor] [logstash-2014.04.14.09][0], node[OS96wzBvT5WTcPgzKrAHlw], [P], s[STARTED]: Failed to execute [org.el
asticsearch.action.search.SearchRequest@906a63f] lastShard [true]
org.elasticsearch.search.SearchParseException: [logstash-2014.04.14.09][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"facets":{"0":{"date_histogram":{"k
ey_field":"@timestamp","value_field":"request_time","interval":"10m"},"global":true,"facet_filter":{"fquery":{"query":{"filtered":{"query":{"query_string":{"query":"type:
nginx_accesslog  AND request:\\/search\\?*"}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"from":1397462569513,"to":1397548969513}}}]}}}}}}}},"size":0}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:70)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData
        at org.elasticsearch.search.facet.datehistogram.DateHistogramFacetParser.parse(DateHistogramFacetParser.java:188)
        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)
        ... 11 more
```
</comment><comment author="imotov" created="2014-04-15T16:00:56Z" id="40499942">@alexeymedved this is different. Here you are searching the yesterday's index `logstash-2014.04.14.09` that most likely contains data indexed while mapping for this index was messed up due to #5623. So you have numbers indexed as numbers and as strings. I would also suspect that mapping for this index is still messed up. Can you run `curl localhost:9200/logstash-2014.04.14.09/_mapping` to see if fields are mapped correctly? Can you also try reproducing this failure while searching only indices created with the nightly build and without using MVEL?
</comment><comment author="imotov" created="2014-04-18T14:09:32Z" id="40811053">That looks like a duplicate of #5623 after all. Please, feel free to reopen if you have any additional data points.
</comment><comment author="alexey-medvedchikov" created="2014-04-21T06:52:07Z" id="40919715">Well, I fixed this problem for myself by using long for time fields :)
</comment><comment author="UnderGreen" created="2015-03-13T10:22:28Z" id="78903583">Have same problem with Elastic 1.4.4. Query result without scripting:

```
{
    "took": 7,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 33,
        "max_score": null,
        "hits": [{
            "_index": "logstash-2015.03.13",
            "_type": "gelf_photo",
            "_id": "AUwSeR5Mf_J86mgo9hKZ",
            "_score": null,
            "_source": {
                "version": "1.0",
                "type": "gelf_photo",
                "time_done_full": "0.0009758472442627",
                "time_done": 0.0,
                "@version": "1",
                "@timestamp": "2015-03-13T09:31:09.067Z",
                "_class__": "",
                "_method__": "",
                "_extra__": "null",
                "_request_id__": "45e21fc5e190de6269567c57c5f2177f0009e188"
            },
            "sort": [1426239069067]
        }]
    }
}

```

Query result with scripting:

```
{
    "took": 11,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 33,
        "max_score": null,
        "hits": [{
            "_index": "logstash-2015.03.13",
            "_type": "gelf_photo",
            "_id": "AUwSeR5Mf_J86mgo9hKZ",
            "_score": null,
            "fields": {
                "s_time_done": [" \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"]
            },
            "sort": [1426239069067]
        }]
    }
}
```

Result of`http://logstash.test:9200/logstash-2015.03.13/_mappings`:

```
...
"time_done": {
"type": "double"
},
...
```
</comment><comment author="clintongormley" created="2015-04-05T19:54:12Z" id="89841436">@UnderGreen please can you open a new issue with all the details.  Although I have a sneaking suspicion that this is an issue with dynamic mappings being applied asynchronously. See https://github.com/elastic/elasticsearch/issues/8688 for more
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update URI of mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5704</link><project id="" key="" /><description>The method `PUT /{index}/{type}/_mapping` seems to be out of date.
("still supported for backwardscompatibility.")

Substituted them with `PUT /{index}/_mapping/{type}`.
</description><key id="30967620">5704</key><summary>[DOCS] Update URI of mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tadd</reporter><labels /><created>2014-04-07T09:01:42Z</created><updated>2014-07-16T21:46:44Z</updated><resolved>2014-05-14T10:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tadd" created="2014-04-07T09:05:06Z" id="39708953">FYI: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-put-mapping.html
</comment><comment author="javanna" created="2014-04-07T11:55:44Z" id="39721290">Good point @tadd ! Could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can merge this in?
</comment><comment author="clintongormley" created="2014-05-06T14:00:12Z" id="42305129">Hi @tadd 

Have you signed the CLA yet? I don't see you in the list.  Please could you sign then ping us on this issue so that we can get this merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-05-14T10:17:40Z" id="43063990">CLA not signed.  Treated as bug report and fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failures in constructing aggs can cause pages not to be released</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5703</link><project id="" key="" /><description>When constructing aggs, there might be failures (like validation failures) that can cause pages obtained at a previous step in the agg construction not to be released.
</description><key id="30966278">5703</key><summary>Failures in constructing aggs can cause pages not to be released</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T08:37:15Z</created><updated>2014-04-14T15:50:55Z</updated><resolved>2014-04-14T15:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add BytesValues#copyShard(BytesRef)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5702</link><project id="" key="" /><description>This allows the scratch to be reused externally outside of the FD impl.
</description><key id="30966082">5702</key><summary>Add BytesValues#copyShard(BytesRef)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-04-07T08:33:09Z</created><updated>2015-05-18T23:32:16Z</updated><resolved>2014-04-07T11:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-07T09:02:45Z" id="39708791">I agree that this would be nice but on the other hand I think this can be dangerous. Today we allow for changing the field data format on a live index, so something bad could happen like:
1. setup a string field with the paged bytes format
2. load field data
3. add more segments, change the format to FST and load field data for these new segments
4. run a query that relies on `copyShared`.

If you copy into the same `BytesRef` on all segments, then paged bytes field data would just flip pointers on the first segments so you would get references to internal data-structures, and the latest segments that use an fst would change the content of the BytesRef, effectively modifying the internal content of the paged bytes.

So if we want to add this method, I think we should make sure to not reuse the same BytesRef across segments and have a mechanism to detect such issues. This doesn't look easy though. :(
</comment><comment author="martijnvg" created="2014-04-07T11:11:38Z" id="39718247">That would indeed be dangerous! Unless we can come up with a relatively easy way to protect us from this, we should't do this. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add lucene LMSimilarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5701</link><project id="" key="" /><description>closes #5697
</description><key id="30954802">5701</key><summary>Add lucene LMSimilarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-07T02:24:07Z</created><updated>2015-06-07T14:44:12Z</updated><resolved>2014-04-07T08:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-07T07:53:02Z" id="39703951">Apart from the typo that Luca found, this looks good to me!
</comment><comment author="kzwang" created="2014-04-07T08:07:45Z" id="39705013">@javanna @jpountz fixed the typo
</comment><comment author="kzwang" created="2014-04-07T08:23:26Z" id="39706017">@javanna added license header
</comment><comment author="javanna" created="2014-04-07T08:50:26Z" id="39707869">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove clear on mock page/array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5700</link><project id="" key="" /><description>since we use a shared cluster, calling clear on the mock array / page recycler can cause removing a valid on going reference, and then when its released, the release will fail because it can't be found.
There is no real reason to call clear, checking if pages/arrays have been released takes the snapshot behavior here into account.
This change also makes sure we don't use the mock classes in places where we don't really release.

Note, with this change DoubleTermsTests fails, since it causes failures when creating aggs in the pre process phase, causing obtained arrays not to be released. This needs to be fixed before pulling this change in.
</description><key id="30951788">5700</key><summary>Remove clear on mock page/array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-06T23:54:06Z</created><updated>2015-06-07T14:43:11Z</updated><resolved>2014-04-07T10:59:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregation cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5699</link><project id="" key="" /><description>- consolidated value source parsing under a single parser that is reused in all the value source aggs parsers
- consolidated include/exclude parsing under a single parser
- cleaned up value format handling, to have consistent behaviour across all values source aggs
</description><key id="30945037">5699</key><summary>Aggregation cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-06T19:52:23Z</created><updated>2015-06-07T14:43:17Z</updated><resolved>2014-04-07T12:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-07T07:48:27Z" id="39703682">Very nice, I really like how you share the parsing of the values sources!
</comment><comment author="jpountz" created="2014-04-07T12:32:53Z" id="39724125">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimizing with wait_for_merge=false is no longer asynchronous</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5698</link><project id="" key="" /><description>After the upgrade to 1.1.0, the _optimize command doesn't seem to acknowledge wait_for_merge=false as being an asynchronous operation, and blocks until it times out.
</description><key id="30943838">5698</key><summary>Optimizing with wait_for_merge=false is no longer asynchronous</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">lusid</reporter><labels /><created>2014-04-06T18:58:03Z</created><updated>2014-09-07T11:41:13Z</updated><resolved>2014-09-07T11:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-06T19:02:04Z" id="39676973">good catch. This is because the default merge scheduler is now the serial merge scheduler in 1.1, which doesn't block merges. In theory though, even with the previous concurrent merge scheduler, the optimize request might have blocked if all threads were busy and there was a pending merge.

Seems like the way to solve it properly is in `TransportOptimizeAction`, to use "SAME" executor, and fork to the optimize thread pool explicitly in the shard action, and only wait for it to finish if `wait_for_merge` is set.
</comment><comment author="kimchy" created="2014-05-07T15:35:26Z" id="42442773">we have moved back to default to the concurrent merge scheduler, so this is not as urgent (or a regression) anymore, since we moved back to the previous behavior.
</comment><comment author="kimchy" created="2014-09-07T11:40:58Z" id="54744360">we have removed the serial merge scheduler option in #6120, so this is no longer relevant I think, closing  for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene LMSimilarity seems to be missing in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5697</link><project id="" key="" /><description>Nowadays Lucene has a configurable similarity module (actually a set of similarity modules) to compute a ranking over (query, document) pairs.

The list of similarities which are available in Lucene 4.7.1 (and have been since 4.0.0 as far as i can tell) is:
- DefaultSimilarity
- BM25Similarity
- DFR
- Information based models
- language models

(according to the documentation at: http://lucene.apache.org/core/4_7_1/core/org/apache/lucene/search/similarities/package-summary.html#sims)

ElasticSearch makes these Lucene similarities available via its similarity module. The list of similarities that ElasticSearch 1.0/1.1 supports is:
- default similarity
- bm25 similarity
- dfr similarity
- ib similarity

(this list according to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-similarity.html#_available_similarities)

So the language modeling approach seems to be missing from ElasticSearch (or maybe they are available, but they're undocumented.)
</description><key id="30938299">5697</key><summary>Lucene LMSimilarity seems to be missing in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">IsaacHaze</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-06T14:16:32Z</created><updated>2014-04-07T08:50:03Z</updated><resolved>2014-04-07T08:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T07:57:36Z" id="39704290">Thanks for reporting this @IsaacHaze!
</comment><comment author="javanna" created="2014-04-07T08:50:03Z" id="39707841">Closed via https://github.com/elasticsearch/elasticsearch/commit/ecab74fe6ca8b135ecf756ffd977322e5df960ed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Test] Added an optional budget to BackgroundIndexer allowing for better control of the number of documents indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5696</link><project id="" key="" /><description>Used it reduce indexing activity in RelocationTests and RecoveryWhileUnderLoadTests
</description><key id="30932662">5696</key><summary>[Test] Added an optional budget to BackgroundIndexer allowing for better control of the number of documents indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-04-06T07:51:25Z</created><updated>2014-07-16T21:46:47Z</updated><resolved>2014-04-06T18:54:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-06T18:02:04Z" id="39675167">LGTM
</comment><comment author="bleskes" created="2014-04-06T19:00:07Z" id="39676895">Thx. Pushed to master adn 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add some more documentation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5695</link><project id="" key="" /><description>Oh by the way I just fork()ed myself: Amelia, 51cm, 3400g.
</description><key id="30925317">5695</key><summary>Add some more documentation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-05T22:15:52Z</created><updated>2014-06-15T19:35:02Z</updated><resolved>2014-04-24T20:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-06T03:51:07Z" id="39657790">@MaineC congratulations! :D
</comment><comment author="dadoonet" created="2014-04-06T05:34:33Z" id="39659017">Congrats! Welcome Amelia! :-)
First time I see a birth announcement on Github! 
</comment><comment author="s1monw" created="2014-04-06T07:29:28Z" id="39661220">```
Amelia, 51cm, 3400g.
```

LGTM ;)
</comment><comment author="dadoonet" created="2014-04-06T07:37:54Z" id="39661331">You did not write "+1 to push"? :)
</comment><comment author="s1monw" created="2014-04-06T07:40:06Z" id="39661369">LGTM in general :)
</comment><comment author="e-user" created="2014-04-06T14:37:07Z" id="39669975">Congratulations!!! I hope the "merge" with the rest of the family will go fine as well :)
</comment><comment author="lukas-vlcek" created="2014-04-06T18:15:30Z" id="39675614">Congratulations @MaineC 
</comment><comment author="hintjens" created="2014-04-20T08:55:28Z" id="40890784">Did you commit to master? Or more of a push request? /sorry

Congratulations @MaineC this is wonderful news.
</comment><comment author="javanna" created="2014-04-24T20:15:36Z" id="41326595">Hi @MaineC you seemed kinda busy, I took the liberty to push this ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NumberFormatException in query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5694</link><project id="" key="" /><description>Hi,

In the fields parameter of my query_string  query, there is one numeric field.
It seems that when there is at least one numeric field mentioned in the fields-array, all non-numeric query words will raise the NumberFormatException .

I strongly feel that this should not happen.
If a query token cannot be converted to that field, it is obvious that there are no results from this field. It would be better to handle this exception like there were no occurrences in the field.

Of course this behavior can be extended to the other queries (match, range, etc)

Kind regards,
Peter
</description><key id="30923611">5694</key><summary>NumberFormatException in query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pweerd</reporter><labels /><created>2014-04-05T20:42:17Z</created><updated>2014-12-30T15:16:02Z</updated><resolved>2014-12-30T15:16:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-07T03:39:35Z" id="39693861">@pweerd did you try setting [`lenient`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-dsl-query-string-query) parameter to true?
</comment><comment author="pweerd" created="2014-04-07T06:03:34Z" id="39698489">@Igor: No, I didn't. With lenient it works. Thanks!

Although this does the job, I really think that the default for lenient should be true instead of false. This is way more consistent with the rest of the behavior (eq: a non exsting fields)
</comment><comment author="imotov" created="2014-04-07T19:11:31Z" id="39770894">To implement your suggestion we would have to change this for all other queries (`match`, `multi_match`, etc...) in order to be consistent. I am not really sure about this idea. I can easily see arguments for and against this change.
</comment><comment author="clintongormley" created="2014-12-30T15:16:02Z" id="68364214">Resolved - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>LongHash add/key not consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5693</link><project id="" key="" /><description>LongHash seems to be buggy and does not work like BytesRefHash.  Adds consistently fail and return an invalid index.  Adding a value already in the hash does not return the expected values as well.   I came across this working on #3278 where I had inconsistent number of terms in the LongHash.  I adapted the BytesRefHash tests for LongHash and they consistently fail as well.  See PR #5692 for the updated tests.
</description><key id="30902009">5693</key><summary>LongHash add/key not consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T23:16:39Z</created><updated>2015-06-07T14:44:36Z</updated><resolved>2014-04-07T18:10:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-04T23:18:59Z" id="39620714">/cc @uboness @jpountz As I see you have worked on LongHash.
</comment><comment author="mattweber" created="2014-04-05T04:44:43Z" id="39628923">Updated the PR to include changes to LongHash so all tests pass.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update LongHash to work like BytesRefHash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5692</link><project id="" key="" /><description /><key id="30901860">5692</key><summary>Update LongHash to work like BytesRefHash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2014-04-04T23:13:17Z</created><updated>2014-06-12T09:43:28Z</updated><resolved>2014-04-07T18:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-04-04T23:18:30Z" id="39620688">/cc @uboness @jpountz As I see you have worked on LongHash.  
</comment><comment author="mattweber" created="2014-04-05T04:43:40Z" id="39628914">Updated LongHash to basically identical to how BytesRefHash works and all tests pass.  The only thing I was not sure of is growing the keys array.  
</comment><comment author="jpountz" created="2014-04-05T10:22:27Z" id="39634186">Thanks Matt. I agree it is a bit confusing that these two classes don't work the same way.

This change doesn't seem to have any impact on performance (I just ran a few benchmarks that use it) and reduces memory usage (since the keys array is smaller) so we should get it in!

The growth of the `keys` array looks good, however since changes the semantics of the `key` method (that should maybe be renamed to `get` to be consistent with `BytesRefHash`?), you should also fix the consumers (I think there are only the long/double and geo_hash_grid aggregators). (The `key` method used to return the key at a given index of the hash table while it now returns the key associated with the given id, as returned by `add`.)
</comment><comment author="mattweber" created="2014-04-05T16:19:02Z" id="39642807">Thanks @jpountz!  Updated the PR with these changes.
</comment><comment author="jpountz" created="2014-04-07T09:23:25Z" id="39710362">Just left a minor comment. Other than that it looks good to me now!
</comment><comment author="mattweber" created="2014-04-07T16:52:34Z" id="39754816">@jpountz Just pushed this fix.  I actually think this applies to `BytesRefHash` as well, maybe we should have a new PR to update that as well?
</comment><comment author="jpountz" created="2014-04-07T16:56:31Z" id="39755207">I'm not sure what you are talking about, can you point me to the class/line where the same thing would apply to `BytesRefHash`?
</comment><comment author="mattweber" created="2014-04-07T17:02:30Z" id="39755919">I'm not sure where it would be used in aggs, but if you needed to iterate over all terms in the `BytesRefHash`, we could use the same logic 0 to size-1 as the id's are calculated the same way.
</comment><comment author="jpountz" created="2014-04-07T17:10:49Z" id="39756737">Yes indeed! It's supposed to already be the case, but please let us know if you find code that doesn't iterate over `BytesRefHash` this way!
</comment><comment author="mattweber" created="2014-04-07T17:56:46Z" id="39761928">Sure will, anything else you need from me before you can pull this in?
</comment><comment author="jpountz" created="2014-04-07T17:58:20Z" id="39762130">Nope, I'm running unit tests right now. :-)
</comment><comment author="jpountz" created="2014-04-07T18:14:01Z" id="39764084">Merged, thanks Matt!
</comment><comment author="mattweber" created="2014-04-07T18:23:02Z" id="39765203">Great, thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Releasable bytes output + use in transport / translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5691</link><project id="" key="" /><description>create a new releasable bytes output, that can be recycled, and use it in netty and the translog, 2 areas where the recycling will help nicely.
Note, opted for statically typed enforced releasble bytes output, to make sure people take the extra care to control when the bytes reference are released.
 Also, the mock page/array classes were fixed to not take into account potential recycling going during teardown, for example, on a shared cluster ping requests still happen, so recycling happen actively during teardown.
</description><key id="30884441">5691</key><summary>Releasable bytes output + use in transport / translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T18:45:01Z</created><updated>2015-06-07T14:44:51Z</updated><resolved>2014-04-06T18:24:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-04T19:37:03Z" id="39603099">This looks good to me, I left comments but they are more about cosmetics.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `getAsRatio` to Settings class, allow DiskThresholdDecider to take percentages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5690</link><project id="" key="" /><description>Adds new RatioValue class that parses ratios between 0-100% expressed in
either floating-point (0.13) or percentage (51.12%) notation.
</description><key id="30874742">5690</key><summary>Add `getAsRatio` to Settings class, allow DiskThresholdDecider to take percentages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T16:26:35Z</created><updated>2015-06-07T14:45:11Z</updated><resolved>2014-04-04T20:07:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-04T19:10:06Z" id="39600451">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Releasable extend AutoCloseable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5689</link><project id="" key="" /><description>Java7's AutoCloseable allows to manage resources more nicely using
try-with-resources statements. Since the semantics of our Releasable interface
are very close to a Closeable, let's switch to it.
</description><key id="30873411">5689</key><summary>Make Releasable extend AutoCloseable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T16:07:54Z</created><updated>2015-06-07T14:45:16Z</updated><resolved>2014-04-14T15:30:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-14T13:53:16Z" id="40367530">LGTM - I think we should get this in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove AtomicFieldData.isValuesOrdered.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5688</link><project id="" key="" /><description>This method is not used anymore.
</description><key id="30861857">5688</key><summary>Remove AtomicFieldData.isValuesOrdered.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T13:48:49Z</created><updated>2015-06-08T15:09:07Z</updated><resolved>2014-04-07T08:31:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-04T17:50:48Z" id="39592449">I like the red in the PR :) LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problems with field resolution in significant terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5687</link><project id="" key="" /><description>Hi,

I noticed (at least for me) unexpected behavior in the field resolution of the significant terms aggregations when prepending the document type to the field name:

```
"significant_terms" : { "field" : "report.crime_type" }
```

instead of

```
"significant_terms" : { "field" : "crime_type" }
```

leads to an NPE in ES 1.1.0:

```
java.lang.NullPointerException
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.getBackgroundFrequency(SignificantTermsAggregatorFactory.java:190)
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator.buildAggregation(SignificantStringTermsAggregator.java:87)
        at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator$WithOrdinals.buildAggregation(SignificantStringTermsAggregator.java:129)
        at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:135)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:136)
...
```

and in the current master this leads to "Infinity" scores:

```
"aggregations" : {
    "contentTerms" : {
      "doc_count" : 5,
      "buckets" : [ {
        "key" : "of",
        "doc_count" : 4,
        "score" : "Infinity",
        "bg_count" : 0
      }, {
        "key" : "a",
        "doc_count" : 3,
        "score" : "Infinity",
        "bg_count" : 0
      }, {
        "key" : "metals",
        "doc_count" : 5,
        "score" : "Infinity",
        "bg_count" : 0
      }
...
```

Without the document type it works fine in both ES versions. Here is a gist to reproduce it: https://gist.github.com/hkorte/9974567
</description><key id="30861581">5687</key><summary>Problems with field resolution in significant terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hkorte</reporter><labels /><created>2014-04-04T13:44:44Z</created><updated>2014-12-30T15:15:35Z</updated><resolved>2014-12-30T15:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-04-20T22:43:47Z" id="40906879">Thanks for raising this issue. 
Adding support for doctype prefixes in field names probably brings the expectation that background frequencies are also filtered by the doc type e.g. if an index contains `tweet` and `email` doc types then `tweet.text` should filter out any counts for `email.text` occurrences in the single indexed `text` field held by the es index.
This scenario will be expensive as we'll need to drop down a level into postings to count docs that match a doctype filter (ideally only if the index contains &gt; 1 doctype with that field). 
I'm not sure what would happen if the query is on an indiscriminate `text` field (so querying both `email` and `tweet` doc types) but the significant_terms analysis is requested on a qualified `email.text` field - a quick test with a plain `terms` agg suggests the counts produced in this case are not filtered by doc type. So if the foreground stats obtained from FieldData cache are unfiltered then there is a case for making the background stats unfiltered too.
</comment><comment author="clintongormley" created="2014-12-30T15:15:35Z" id="68364184">Closing in favour of #8870
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indexation versioning with 0 does not complain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5686</link><project id="" key="" /><description>I'm aware that the documentation says that internal versioning starts at 1, I was just wondering why ES didn't complain when I run the command below several times in a row.

The value 0 seems to be handled inconsistently compared to every other value; negative values are reported as illegal values, above 0 the regular version control is applied.

If ES would report this, it might help prevent bugs, especially in languages like Java where 0 is the default value for a lot of primitive types when not initialized. If one forgets to set it, the versioning will not kick without the user being aware of it.

Tested in v1.0.0 and v1.1.0

```
curl -XPUT 'http://localhost:9200/testindex/testtype/1?version=0' -d '{
"title": "test"
}'
```
</description><key id="30860573">5686</key><summary>indexation versioning with 0 does not complain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">klaplume</reporter><labels /><created>2014-04-04T13:31:35Z</created><updated>2014-12-30T15:14:52Z</updated><resolved>2014-12-30T15:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-05-19T10:26:37Z" id="43486501">Hi,

The value 0 is actually used by ES as the default and it means the indexing operation should be done without versioning. In recent changes we are moving towards using -3 as the `MATCH_ANY` value. With this we are also adding a validation error for the 0 value. See: https://github.com/elasticsearch/elasticsearch/pull/6229 
</comment><comment author="clintongormley" created="2014-12-30T15:14:52Z" id="68364098">Closing in favour of #6229
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query validation doesn't detect extra JSON properties after the query property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5685</link><project id="" key="" /><description>```
$ curl -s 'http://localhost:9200/'|grep number
    "number" : "1.1.0",
```

When validating a query, adding an extra property **after** the `query` does not trigger a validation error:

``` shell
$ curl -XPOST 'http://localhost:9200/test/_validate/query?pretty' -d '
{"query": {"term" : { "user" : "kimchy" }}, "foo": "bar"}
'
```

``` json
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  }
}
```

If the extra property is placed **before** the `query` property, a validation error is reported.

``` shell
$ curl -XPOST 'http://localhost:9200/test/_validate/query?pretty' -d '
{"foo": "bar", "query": {"term" : { "user" : "kimchy" }}}
'
```

``` json
{
  "valid" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  }
}
```

When doing an actual search however, both queries fail:

``` shell
$ curl 'http://localhost:9200/test/_search?pretty' -d '
{"foo": "bar", "query": {"term" : { "user" : "kimchy" }}}
'
```

``` json
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[FsbLm57PSNeCm5gRjO-uLA][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"foo\": \"bar\", \"query\": {\"term\" : { \"user\" : \"kimchy\" }}}\n]]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"foo\": \"bar\", \"query\": {\"term\" : { \"user\" : \"kimchy\" }}}\n]]]; nested: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"foo\": \"bar\", \"query\": {\"term\" : { \"user\" : \"kimchy\" }}}\n]]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"foo\": \"bar\", \"query\": {\"term\" : { \"user\" : \"kimchy\" }}}\n]]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"foo\": \"bar\", \"query\": {\"term\" : { \"user\" : \"kimchy\" }}}\n]]]; nested: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }]",
  "status" : 400
}
```

``` shell
$ curl 'http://localhost:9200/test/_search?pretty' -d '
{"query": {"term" : { "user" : "kimchy" }}, "foo": "bar"}
'
```

``` json
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[FsbLm57PSNeCm5gRjO-uLA][test][4]: SearchParseException[[test][4]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}\n]]]; nested: SearchParseException[[test][4]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][3]: SearchParseException[[test][3]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}\n]]]; nested: SearchParseException[[test][3]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][2]: SearchParseException[[test][2]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}\n]]]; nested: SearchParseException[[test][2]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][0]: SearchParseException[[test][0]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}\n]]]; nested: SearchParseException[[test][0]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][1]: SearchParseException[[test][1]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}\n]]]; nested: SearchParseException[[test][1]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }]",
  "status" : 400
}
```
</description><key id="30848550">5685</key><summary>Query validation doesn't detect extra JSON properties after the query property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">amarandon</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T10:16:56Z</created><updated>2014-09-08T00:25:08Z</updated><resolved>2014-05-12T10:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-04T10:30:58Z" id="39551433">the validate api only validate the `query` element, it's used to validate query instead of search request, so you shouldn't pass other element to the validate request. the search request takes more elements e.g. `fields`, `sort` etc. those are not validated by the validate api. `query` is just one element in search request. 
</comment><comment author="amarandon" created="2014-04-04T10:42:48Z" id="39552287">I appreciate that the submitted JSON document should not contain any property other than `query`. However it seems a bit odd to me that its behavior depends on the order of properties in the submitted document. AFAIK, JSON objects are not ordered. The validate API should either ignore any extra property or it should return a 400 error. The current behavior seems inconsistent.
</comment><comment author="amarandon" created="2014-04-04T12:51:00Z" id="39561169">I think I can see the problem : https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java#L298
There's a `return` in the loop as soon as `query` is found, ignoring any unsupported field that might come after. I'll try to submit a patch within the next few days.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch stats by groups return inconsistent values - v0.90.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5684</link><project id="" key="" /><description>Environment:
- Elasticsearch version 0.90.2 (I realize this is an old version, but I can't upgrade to the latest one)
- 14 nodes
- 3 indices
- 15 mil documents
- ~70 queries/s

I'm querying Elasticsearch search stats on all indexes by stats groups using
`curl http://127.0.0.1:9200/_all/_stats/search?groups=_all`
I take the `query_total` values out of the results for each index and group, however I see that I get different results on every query I do.

See example outputs: https://gist.github.com/iserko/9963195

I'm guessing the reason for this could be that it's only getting metrics from a few nodes instead of all of them. But if that's the case, can I wait in some way and make sure I have data from all the nodes? Or has this been fixed in a newer version?
</description><key id="30841834">5684</key><summary>Elasticsearch stats by groups return inconsistent values - v0.90.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iserko</reporter><labels /><created>2014-04-04T08:20:49Z</created><updated>2014-12-30T15:12:58Z</updated><resolved>2014-12-30T15:12:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T15:12:58Z" id="68363948">Hi @iserko 

Sorry it has taken a while to get to this issue.  I've tried to replicate this locally on 1.4.2 and failed.  I'm assuming this has been fixed in the interim.  If you're still seeing this issue on a more recent version, could you open an issue with a more complete recreation?

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BigInteger/BigDecimal support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5683</link><project id="" key="" /><description>For XContentBuilder/XContentParser and document mapping, this will add support for "big" numeric types BigInteger/BigDecimal.

BigInteger/BigDecimal support for XContentBuilder/XContentParser is implemented by using the existing Jackson support for the "big" numeric types. A new method `losslessDecimals()` is used to switch the XContentParser into recognizing BigInteger/BigDecimal in precedence over primitive numeric types, for better convenience when using the Java API for parsing document sources with BigInteger/BigDecimal field values.

For the document mapping, new core types `biginteger` and `bigdecimal` are introduced. With a new flag `lossless_numeric_detection`, the precedence of BigInteger/BigDecimal over primitive numeric types can be controlled in the mapping. When set to `true`, new dynamic numeric fields are assigned to "big" numeric types first. Default is `false`, where primitive numeric types still take precedence. 

Caveat: BigInteger/BigDecimal support is just meant for search and indexing/storing. The "big" numeric types are degraded to their `.longValue()` and `.doubleValue()` components when they are used in NumericRangeQuery and related contexts, so it is not recommended to use values larger than Long.MAX_VALUE or Double.MAX_VALUE in analytical queries like facets and aggregations, strange cut-offs or underflows/overflows should occur. 
</description><key id="30841602">5683</key><summary>BigInteger/BigDecimal support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>:Mapping</label><label>high hanging fruit</label></labels><created>2014-04-04T08:16:24Z</created><updated>2016-03-08T14:11:28Z</updated><resolved>2016-03-08T14:11:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-15T07:49:44Z" id="40454350">&gt; The "big" numeric types are degraded to their .longValue() and .doubleValue() components when they are used in NumericRangeQuery and related contexts

FYI there is some discussion on https://issues.apache.org/jira/browse/LUCENE-5596 in order to add this range support to types that are more than 64 bits.
</comment><comment author="jpountz" created="2014-08-22T08:49:28Z" id="53037479">Quick update: most of this change is good and we would be a good start to support big integers/decimals in the future. I added the `stalled` label, since I think it would be important to support efficient range queries on such types without information loss (either via https://issues.apache.org/jira/browse/LUCENE-5879 or https://issues.apache.org/jira/browse/LUCENE-5596). Some other thoughts/open questions:
- these types should probably be forbidden in the numeric metrics aggregations, otherwise we would either need to use big decimals there which would kill performance, or the information loss would make results unusable
- these types should probably be opt-ins only since they would have different capabilities than the other numeric fields,
- for sorting, should we use SORTED or BINARY doc value types? (I would lend towards SORTED which would make sorting faster)
- should they be specified as strings or numbers in the _source document? (would there be compatibility issues with some languages/json parsers/json generators with numbers?)
</comment><comment author="jprante" created="2014-08-22T12:52:19Z" id="53056389">@jpountz 

"these types should probably be forbidden in the numeric metrics aggregations, otherwise we would either need to use big decimals there which would kill performance, or the information loss would make results unusable"

I agree that numeric metrics aggregation must never use BigInteger/BigDecimal types. A thought is to add a special aggregation type, like "monetary/financial aggregation", where performance is less important with regard to exactness/correctness of numeric results, and BigDecimal is not converted to double/float.

"should they be specified as strings or numbers in the _source document? (would there be compatibility issues with some languages/json parsers/json generators with numbers?)"

The Jackson library maps it to "JSON Type number" http://wiki.fasterxml.com/JacksonDataBinding
there are some mechanisms to let the parser auto-detect BigInteger (no fraction), but BigDecimal must be configured to override double/float (with fraction).
</comment><comment author="clintongormley" created="2014-08-22T15:35:53Z" id="53075977">&gt; &gt; "should they be specified as strings or numbers in the _source document? (would there be compatibility issues with some languages/json parsers/json generators with numbers?)"
&gt; 
&gt; The Jackson library maps it to "JSON Type number" http://wiki.fasterxml.com/JacksonDataBinding
&gt; there are some mechanisms to let the parser auto-detect BigInteger (no fraction), but BigDecimal must be configured to override double/float (with fraction).

My concern here is more with other languages, eg Javascript can't support bigint/decimals, and we'll find lots of similar issues. It may be ok to accept them as numbers, as long as we also support coercing from strings.  That way users of languages without support can still use them.
</comment><comment author="jprante" created="2014-08-22T15:51:11Z" id="53078106">The problem of Javascript is, it has poor support of numbers, even 64bit ints fail (and I think ES/Lucene supports 64bit longs for a while now). BigInteger/BigDecimals can be added as an extension, at least to Node.js https://www.npmjs.org/package/json-bignum
</comment><comment author="kul" created="2015-01-14T11:18:39Z" id="69902363">:+1: much awaited.
</comment><comment author="mikemccand" created="2015-09-01T23:24:07Z" id="136890281">I think https://issues.apache.org/jira/browse/LUCENE-6697 (just released in Lucene 5.3.0) is a compelling way to allow fast range filters on BigInteger/Decimal values.

Values for the field must be indexed as a SortedSetDocValuesField (with the BigInteger/Decimal value converted to a byte[]) and the field must use the RangeTreeDocValuesFormat.  Then use the NumericRangeTreeQuery at search time.

Some care must be taken in the byte[] encoding, so that sort order is the same, e.g. I think this means the BigInteger field must have a max allowed value (set once up front in the mapping), and maybe the BigDecimal field must have the same up-front scale across all values (?), and the sign bit needs to be flipped like we do for NumericField.

But I think it should work well, and from my limited perf testing on the original issue, the resulting index is smaller and filters are faster than NumericField/RangeQuery.

One caveat is because this code is very new, it lives in sandbox now, and there's no guarantee of back-compat of the file-format it writes.  But then, the file format is also ridiculously simple ...
</comment><comment author="muelli" created="2015-09-29T09:38:56Z" id="144006993">Big integers are also interesting for cryptographic applications.
</comment><comment author="SKumarMN" created="2015-10-05T05:09:16Z" id="145431730">@jprante Does the above fix support range and filter queries too ?. Any Idea when Elastic Search is gonna add BigDecimal /BigInteger Support oficially 
</comment><comment author="jprante" created="2015-10-05T09:12:12Z" id="145470561">From what I can see BigDecimal/BigInteger is implemented in Lucene 5.3 which will appear in Elasticsearch 2.x (not 2.0)
</comment><comment author="SKumarMN" created="2015-10-05T11:41:51Z" id="145502592">@jprante 

Hey, I have applied this fix mentioned in this post however when I index data or fetch it data is getting rounded off. I am using the REST API calls. Am I doing anything wrong here.

Here is my mappings

 {
            "tweety": {
                "properties": {
                    "message": {
                        "type": "string"
                    },
                    "post_date": {
                        "type": "date",
                        "format": "dateOptionalTime"
                    },
                    "newint": {
                        "type": "biginteger",
                        "lossless_numeric_detection": true
                    }
                }
            }
        }

Data:

{
        "newint": 19999999999999999999999999999999999,
        "post_date": "2009-11-15T14:12:12",
        "message": "trying out Elastic-search"
    }

Get Result:

{
    "_index": "twitter",
    "_type": "tweety",
    "_id": "1",
    "_version": 1,
    "found": true,
    "_source": {
        "newint": 2e+34,
        "post_date": "2009-11-15T14:12:12",
        "message": "trying out Elastic-search"
    }
}
</comment><comment author="jprante" created="2015-10-05T12:01:31Z" id="145506104">@SKumarMN the patch is only 50% of the required work. It only means that BigInteger/BigDecimal is accepted as JSON input. The default is to downgrade the accepted values to double/float wherever possible, otherwise, the change would not be compatible to existing ES applications. REST actions would have to be changed to prefer BigInteger/BigDecimal.
</comment><comment author="clintongormley" created="2015-10-06T13:38:08Z" id="145858900">&gt; From what I can see BigDecimal/BigInteger is implemented in Lucene 5.3 which will appear in Elasticsearch 2.x (not 2.0)

This code is in the Lucene sandbox only.  We need to wait until it graduates to core before we can start using it.
</comment><comment author="mikemccand" created="2015-10-06T15:06:52Z" id="145887534">&gt; We need to wait until it graduates to core before we can start using it.

I'm working on graduating this to Lucene's core ... here's the first step: https://issues.apache.org/jira/browse/LUCENE-6825
</comment><comment author="clintongormley" created="2015-10-06T17:58:05Z" id="145946723">w00t!
</comment><comment author="SKumarMN" created="2015-10-19T11:19:32Z" id="149187963">@jpountz 

Hi,

I have used the fixhttps://github.com/elastic/elasticsearch/pull/5758 in my 1.4.4 code to support big integer by changing the IPV6 Mapper. Search and range queries works fine. Our application needs support for Bigdecimal too. Could you please provide me pointers about how can i implement big decimal support with range functionality as well..
</comment><comment author="clintongormley" created="2016-03-08T14:11:28Z" id="193798096">Closing in favour of https://github.com/elastic/elasticsearch/issues/17006
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion suggester - duplicates - `index_analyzer` and `preserve_separators` ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5682</link><project id="" key="" /><description>I would expect that `index_analyzer: standard` and `preverve_separators: true` would lowercase and normalize the `output` but it turns out that they don't. This causes the completion suggester to return duplicated results.

`curl -XPOST "/test"`

```
curl -XPOST "/test/_mapping/test" -d'
{
    "test":{
        "properties": {
            "name":{
                "type": "string",
                "copy_to":"name_suggest"
            },
            "name_suggest":{
                "type": "completion",
                "index_analyzer": "standard",
                "search_analyzer": "standard",
                "preserve_position_increments":false,
                "payloads":false,
                "preserve_separators":false
            }
        }
    }
}'
```

```
curl -XPOST "/test/test" -d'
{
    "name":"bmw"
}'
```

```
curl -XPOST "/test/test" -d'
{
    "name":"BMW"
}'
```

```
curl -XPOST "/test/test" -d'
{
    "name":"bmw "
}'
```

```
curl -XPOST "/test/_suggest" -d'
{
    "text":"b",
    "car":{
        "completion":{
            "field":"name_suggest"
        }
    }
}'
```

Result

``` javascript
{
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "car": [
      {
         "text": "b",
         "offset": 0,
         "length": 1,
         "options": [
            {
               "text": "BMW",
               "score": 1
            },
            {
               "text": "bmw",
               "score": 1
            },
            {
               "text": "bmw ",
               "score": 1
            }
         ]
      }
   ]
}
```
</description><key id="30841362">5682</key><summary>Completion suggester - duplicates - `index_analyzer` and `preserve_separators` ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">g00fy-</reporter><labels /><created>2014-04-04T08:12:13Z</created><updated>2015-10-14T15:50:42Z</updated><resolved>2015-10-14T15:50:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arildplassen" created="2015-01-28T13:07:34Z" id="71831863">What is the status on this? I'm experiencing the same problem. 

Also, both custom and predefined analyzers doesn't seem to have any effect at all (on index_analyzer, search_analyzer).
</comment><comment author="askaliuk" created="2015-06-15T23:45:55Z" id="112239004">+1
</comment><comment author="clintongormley" created="2015-10-14T15:50:42Z" id="148093454">&gt; I would expect that index_analyzer: standard and preverve_separators: true would lowercase and normalize the output but it turns out that they don't. This causes the completion suggester to return duplicated results.

I think this is expected.  The analysis chain is only for the input.  The output is supposed to be provided in the format that you want to display.  Your example basically translates to a single input with multiple outputs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix geo_point accepting null values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5681</link><project id="" key="" /><description>closes #5680
</description><key id="30838071">5681</key><summary>Fix geo_point accepting null values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2014-04-04T06:55:18Z</created><updated>2015-06-07T21:50:43Z</updated><resolved>2014-04-04T09:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-04T06:57:59Z" id="39536800">Hi @kzwang 

Thanks for the PR.

Do you think you could add a test for it as well?
</comment><comment author="kzwang" created="2014-04-04T07:03:48Z" id="39537092">@dadoonet sure, I'll add a test
</comment><comment author="lusid" created="2014-04-04T07:10:31Z" id="39537412">Much appreciated. This one caused quite a bit of panic for me.
</comment><comment author="kzwang" created="2014-04-04T07:11:47Z" id="39537493">@dadoonet added a test
</comment><comment author="s1monw" created="2014-04-04T07:42:18Z" id="39539274">LGTM this shoudl go into all `1.` branches.. I will label the issue
</comment><comment author="dadoonet" created="2014-04-04T09:54:34Z" id="39548624">Thanks @kzwang!

Pushed in 1.1, 1.x (1.2) and master branches.
1.0.2 is not affected by this issue.
</comment><comment author="jlecour" created="2014-04-08T14:56:51Z" id="39857969">this one drove me crazy. I've downgraded to 1.0.2 until 1.1.1 is released.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_point doesn't allow null values as of 1.1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5680</link><project id="" key="" /><description>After upgrading to 1.1.0, I found that my backend processes that perform bulk inserts/updates to ElasticSearch started failing with the following error:

MapperParsingException[failed to parse]; nested: ElasticsearchParseException[geo_point expected]; 

It seems that geo_point fields now require a non-null value for every document? Is there a way to bypass this behavior without having to change my ETL process to create fake geo_point coordinates?
</description><key id="30836592">5680</key><summary>geo_point doesn't allow null values as of 1.1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">lusid</reporter><labels><label>:Mapping</label><label>bug</label><label>regression</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-04T06:16:42Z</created><updated>2015-09-25T15:16:48Z</updated><resolved>2014-04-04T09:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-04T06:35:35Z" id="39535718">Confirmed. I hit the same issue yesterday while moving some code from 1.0 to 1.1.
</comment><comment author="dadoonet" created="2014-04-04T09:54:06Z" id="39548584">Pushed in 1.1, 1.x (1.2) and master branches.
1.0.2 is not affected by this issue.
</comment><comment author="hkorte" created="2014-04-04T14:02:52Z" id="39567848">I had the same problem migrating from 1.0.1 to 1.1.0, but I noticed that you can have missing values, if you completely omit the geo_point field instead of having something like "field:{}" or "field:null". See this gist: https://gist.github.com/hkorte/9936192
</comment><comment author="lusid" created="2014-04-04T15:41:05Z" id="39579041">You are probably right, as we do not include null value fields in our ETL process. However, our bulk updates do fail when the geo_point fields are left out of the bulk request.
</comment><comment author="hossein761" created="2014-04-05T14:06:58Z" id="39639346">I just hit the same issue. Today I upgraded to 1.1. Should I wait for the next release for this fix? @dadoonet I got the 1.1 dependency today but this change is not there, i don't see it in the source code.
</comment><comment author="dadoonet" created="2014-04-05T14:33:00Z" id="39639976">It will be in 1.1.1 which is not released yet. If you can't modify your injection, I guess you should wait for 1.1.1.
</comment><comment author="d2kagw" created="2014-04-07T23:00:03Z" id="39794200">+1 on the issue.
We've been able to change our code to work around the issue, but looking forward to having it fixed.
</comment><comment author="Tonkpils" created="2015-09-24T15:02:36Z" id="142955511">This still seems to be an issue in ES `1.7.1`
</comment><comment author="clintongormley" created="2015-09-25T12:24:28Z" id="143203170">@Tonkpils this works just fine in 1.7.1:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "loc": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT t/t/1
{
  "loc": null
}
```
</comment><comment author="Tonkpils" created="2015-09-25T15:16:48Z" id="143249728">You're right, it was an issue with the incoming parameters where Lon and lat were null. Apologies. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update old status of plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5679</link><project id="" key="" /><description /><key id="30828147">5679</key><summary>update old status of plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2014-04-04T01:29:54Z</created><updated>2014-07-16T21:46:50Z</updated><resolved>2014-04-14T00:20:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-14T00:20:54Z" id="40324841">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5678</link><project id="" key="" /><description /><key id="30824403">5678</key><summary>minor typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">geekpete</reporter><labels><label>docs</label></labels><created>2014-04-03T23:54:49Z</created><updated>2014-07-16T21:46:51Z</updated><resolved>2014-04-04T15:40:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T15:40:03Z" id="39578931">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take stream position into account when calculating remaining length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5677</link><project id="" key="" /><description>Currently `PagedBytesReferenceStreamInput#read(byte[],int,int)` ignores
the current pos and that causes to read past EOF

Closes #5667
</description><key id="30787284">5677</key><summary>Take stream position into account when calculating remaining length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-03T15:47:00Z</created><updated>2015-06-07T21:51:29Z</updated><resolved>2014-04-03T16:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-03T16:05:20Z" id="39470073">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding javadoc to UpdateRequestBuilder for a couple of details </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5676</link><project id="" key="" /><description>Closes #4904 (this PR is nothing more than a squashed single-commit version of #4904).
</description><key id="30786605">5676</key><summary>Adding javadoc to UpdateRequestBuilder for a couple of details </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">countjocular</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-03T15:39:14Z</created><updated>2014-07-01T00:31:23Z</updated><resolved>2014-04-04T15:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T15:57:34Z" id="39580916">Merged, thanks @countjocular &amp; @winterstein !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport client infinite retry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5675</link><project id="" key="" /><description>Firstly, the `onFailure` method in `TransportClientNodesService.RetryListener` has a weak if-check:
`if (i == nodes.size()) { /*...*/ }`  
If the variable `i` is allowed to progress _higher_ than nodes.size(), we can enter a endlessly recursive loop. This is reproducible in unit tests if we are throwing `ConnectTransportException` from every node, as the code can re-enter onFailure after "terminating".

Secondly, the reason we discovered this, is that the Java client might (and will, quite consistently) end up _forking out infinite threads_ that loop on the `onFailure` method.
In our app this happens most frequently when we hit a major GC on the filter cache, while simultaneously maintaining about 40 threads of heavy read- and write- operations. We've also seen it happening when one of our two test nodes is shutting down.

I've attached a sample from a stack trace below. This is pulled from a _live_ system, right as it suffered this error and started spawning threads spinning on RetryListener.
Pay attention to three things:
1. This is a stack trace from the _client_, when the _server_ is irresponsive due to GC.
2. The thread numbers. This is just a tiny sample, there are thousands of these threads (as many as the client can allocate before hitting ulimit). We are doing a max of 40 simultaneous _synchronous_ searches/writes, and would not expect any more simultaneous retries.
3. The number of recursive calls to `onFailure` in the second stacktrace. This is from a cluster with 2 nodes and 2 client apps. It should not recurse 10 times.

Alas, the problem is two-fold as I see it:
- Why does the client fork infinite threads upon a `ConnectTransportException`?
- Why does each loop potentially recurse deeper than the number of nodes?

Also, as a sidenote, the prefix increment of the volatile `i` variable is not threadsafe, you would probably use an AtomicInteger? Although I do not understand why several threads would want to re-use the same RetryListener instance.

This is on ElasticSearch 0.90.7, on Java 7u25 or 7u45

```
"elasticsearch[Matt Murdock][generic][T#2023]" daemon prio=10 tid=0x00007f1fe5551800 nid=0x62fe waiting for monitor entry [0x00007f1f1d198000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.ThreadLocal.createInheritedMap(ThreadLocal.java:236)
        at java.lang.Thread.init(Thread.java:415)
        at java.lang.Thread.init(Thread.java:349)
        at java.lang.Thread.&lt;init&gt;(Thread.java:674)
        at org.elasticsearch.common.util.concurrent.EsExecutors$EsThreadFactory.newThread(EsExecutors.java:102)
        at java.util.concurrent.ThreadPoolExecutor$Worker.&lt;init&gt;(ThreadPoolExecutor.java:610)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:924)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:203)
        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:68)
        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:259)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)
        at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

   Locked ownable synchronizers:
        - &lt;0x0000000791ef8570&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)

"elasticsearch[Matt Murdock][generic][T#2506]" daemon prio=10 tid=0x00007f1fe554f800 nid=0x62fd runnable [0x00007f1f1d299000]
   java.lang.Thread.State: RUNNABLE
        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:68)
        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:259)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
        at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)
        at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

   Locked ownable synchronizers:
        - &lt;0x0000000793b97d08&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)


etc, etc, etc, etc..
```
</description><key id="30777433">5675</key><summary>Transport client infinite retry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnhaug</reporter><labels /><created>2014-04-03T14:00:39Z</created><updated>2014-04-07T07:34:01Z</updated><resolved>2014-04-07T07:34:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-04-07T07:34:01Z" id="39702802">This has been fixed as part of the work for https://github.com/elasticsearch/elasticsearch/issues/4162  , which was included in 0.90.8 

I'm closing for now. If you are still seeing problems after upgrading please feel free to reopen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update advanced-scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5674</link><project id="" key="" /><description>Method name is termVectors() instead of getTermVectors()
</description><key id="30763823">5674</key><summary>Update advanced-scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wittyameta</reporter><labels><label>docs</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-03T10:35:38Z</created><updated>2015-04-02T08:49:05Z</updated><resolved>2014-04-07T13:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-03T20:18:38Z" id="39499556">Thanks for the correction! Can you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this?
</comment><comment author="wittyameta" created="2014-04-05T11:22:56Z" id="39635326">Done!
</comment><comment author="dakrone" created="2014-04-07T13:21:27Z" id="39728328">Merged to [master](https://github.com/elasticsearch/elasticsearch/commit/94278d81e32951c88075d7eb4eced643bad27c9b), 1.x, and 1.1, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Updated ruby clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5673</link><project id="" key="" /><description>marked (re)tire as retired and added searchkick
</description><key id="30758320">5673</key><summary>[DOCS] Updated ruby clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rockaBe</reporter><labels /><created>2014-04-03T09:07:10Z</created><updated>2014-07-16T21:46:53Z</updated><resolved>2014-05-14T10:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T13:58:31Z" id="42304945">Hi @rockaBe 

Thanks for the PR. Sorry it has taken a while to get to it.  Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-05-14T10:19:18Z" id="43064118">CLA not signed.  Treating as bug fix.
</comment><comment author="rockaBe" created="2014-06-01T12:58:48Z" id="44777177">Sorry for being so unresponsive, but thanks for adding the fix to the repo.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add global ordinals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5672</link><project id="" key="" /><description>Global ordinals is a data-structure on top of field data, that maintains an incremental numbering for all the terms in field data in a lexicographic order. The performance of search features like terms aggregator can be improved using global ordinals. 

This PR also adds a new execution mode `global_ordinals` to terms aggregation, that is used by default for non nested terms aggregations. 
</description><key id="30757226">5672</key><summary>Add global ordinals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-03T08:50:47Z</created><updated>2015-06-07T12:47:57Z</updated><resolved>2014-04-07T04:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-03T11:30:51Z" id="39440441">This looks really great! Maybe something that would deserve more testing is making sure that global ords get out of the cache when a top-level reader gets closed?
</comment><comment author="jpountz" created="2014-04-04T13:38:11Z" id="39565365">Just did a second round, I think this is very close!
</comment><comment author="jpountz" created="2014-04-04T17:48:36Z" id="39592201">LGTM!
</comment><comment author="martijnvg" created="2014-04-07T14:33:46Z" id="39737312">I added the benchmark results for the `terms` aggregator comparing running with execution hint `global_ordinals` and execution hint `ordinals`.

Each row is the result of executing terms aggregator on a string field a 100 times. All the fields have `index` set to `not_analyzed` and the index contains 5M docs. Also for the purpose of the benchmark the index has just a single shard. The first column is the name of field the terms aggregator was executed against. The field suffix tells the number of unique values in that field. The `took` columns reports the total amount of time it took the executes the `terms` aggregator, the `millis` report the average time to execute a single `terms` aggregator and lastly the `field size` column tells how memory the field data structures took in memory for the `terms`aggregator field.

Running `terms` aggregator with execution hint `ordinals`:

| Field name | took | millis | fieldata size |
| --- | --- | --- | --- |
| field_64 | 18.1s | 181 | 5.1mb |
| field_128 | 17.9s | 179 | 5.1mb |
| field_256 | 15.9s | 159 | 9.9mb |
| field_8192 | 23.7s | 237 | 12.2mb |
| field_32768 | 35.6s | 356 | 16.6mb |
| field_65536 | 59.4s | 594 | 23.9mb |
| field_131072 | 1.7m | 1041 | 34.3mb |
| field_524288 | 3.6m | 2197 | 46mb |
| field_1048576 | 4.3m | 2602 | 51.5mb |
| field_2097152 | 4.9m | 2991 | 55.8mb |

Running `terms` aggregator with execution hint `global_ordinals`:

| Field name | took | millis | fieldata size |
| --- | --- | --- | --- |
| field_64 | 16.5s | 165 | 5.1mb |
| field_128 | 16.4s | 164 | 5.1mb |
| field_256 | 16.3s | 163 | 9.9mb |
| field_8192 | 20.2s | 202 | 12.3mb |
| field_32768 | 21.4s | 214 | 17mb |
| field_65536 | 22.1s | 221 | 24.7mb |
| field_131072 | 23.5s | 235 | 35.8mb |
| field_524288 | 46.6s | 466 | 50.7mb |
| field_1048576 | 1.1m | 673 | 59mb |
| field_2097152 | 1.3m | 835 | 67.2mb |

Clearly for high cardinality fields using global ordinals is a big win. Comparing the last runs of both test runs, global ordinals is more than 3 times faster. On low cardinality fields there is no clear winner and the difference are small. The noise (jvm gc, hotspot) is intervening with the actual result. The memory overhead that global ordinals add to field data is several times smaller than the field data itself is taking.
</comment><comment author="jpountz" created="2014-04-07T14:53:46Z" id="39739915">&gt; The memory overhead that global ordinals add to field data is several times smaller than the field data itself is taking.

I'd add to that that global ordinals might seem wasteful memory-wise since field data reports higher memory usage on high-cardinality fields, but actually the aggregator uses significantly less transient memory since it doesn't need to load term bytes into memory anymore to compare terms across segments. In the end, if you sum up the amount of memory that is needed to store field data with the amount of memory that is needed to store/compute counts for a particular query, global ordinals very likely require less memory.
</comment><comment author="uboness" created="2014-04-07T18:53:55Z" id="39768837">++ to get a more realistice view of the mem footprint of just ordinals, you'll need to take snapshots of the agtor as well (it's not just field data)
</comment><comment author="otisg" created="2014-04-09T00:24:47Z" id="39917299">What is the guidance for when one should give global_ordinals vs. just ordinals hint.... if you don't know the field cardinality?
</comment><comment author="martijnvg" created="2014-04-09T04:21:38Z" id="39927472">@otisg Currently terms aggregation builds a cross segment bucket_id --&gt; term lookup during the execution which is similar to what global ordinals is, but on a per request basis. This logic has now moved from terms aggs to fielddata, so that once global ordinals are built they can be reused for subsequent requests. Terms aggs can just directly use the global ordinals from fielddata as bucket_ids, which like @jpountz explained make terms aggs use way less transient memory than they did before. So one should use execution hint global ordinals all the time, since it is a win for both low and high cardinality fields, this is the reason why global_ordinals is also the default.
</comment><comment author="tlrx" created="2014-04-14T06:49:41Z" id="40337353">Thanks for this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] fix incorrect field data statistics doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5671</link><project id="" key="" /><description /><key id="30737450">5671</key><summary>[DOCS] fix incorrect field data statistics doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>docs</label></labels><created>2014-04-03T00:20:12Z</created><updated>2014-07-03T06:30:40Z</updated><resolved>2014-05-06T13:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-05T02:14:25Z" id="39626483">@javanna I've updated the PR to support `fields` in url for indices stats api, and updated the docs to include `fielddata` in the url for nodes stats api.
</comment><comment author="javanna" created="2014-04-05T06:35:27Z" id="39630523">thanks @kzwang ! I'd prefer the code change to be part of a separate PR though. Also, let's wait to hear from @clintongormley. I think we should also make it possible to refer to fields in nodes stats without the index metric, but not sure...
</comment><comment author="clintongormley" created="2014-04-08T16:11:58Z" id="39867998">I didn't know that fieldnames in the URL were still supported. It wasn't in the specs for the redesigned API as it just ended up making very long paths.  The intention was to pass fieldnames only in the query string.
</comment><comment author="javanna" created="2014-04-17T15:06:54Z" id="40724381">Ok thanks for insight @clintongormley ! @kzwang could you update the PR to only adapt the docs to use the `fields` parameter? And open a new one for the needed code changes? Thanks!!!
</comment><comment author="kzwang" created="2014-05-06T04:50:27Z" id="42267819">@javanna I've updated the PR to contain only docs change and opened a new PR #6054 
</comment><comment author="clintongormley" created="2014-05-06T13:57:19Z" id="42304792">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>consume truncate filter from lucene 4.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5670</link><project id="" key="" /><description>o.a.l.a.miscellaneous.TruncateTokenFilter is added to lucene via https://issues.apache.org/jira/browse/LUCENE-5558

Once Lucene 4.8 released, Elastic Search can delete its own implementation and consume lucenes.
</description><key id="30737369">5670</key><summary>consume truncate filter from lucene 4.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iorixxx</reporter><labels><label>enhancement</label></labels><created>2014-04-03T00:18:21Z</created><updated>2015-06-07T14:55:19Z</updated><resolved>2014-05-23T20:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-03T16:28:04Z" id="39472916">we will add this together with the 4.8 Upgrade thanks for opening this
</comment><comment author="nik9000" created="2014-05-23T20:19:43Z" id="44055874">Looks like this one is done for 1.2.0.
</comment><comment author="s1monw" created="2014-05-23T20:21:41Z" id="44056065">indeed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add doc values for binary field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5669</link><project id="" key="" /><description>add doc values support for binary field

in my image plugin, the score function need to get the binary field for all documents, using doc values will be much faster than using store field
</description><key id="30736118">5669</key><summary>Add doc values for binary field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T23:51:03Z</created><updated>2016-02-08T08:47:44Z</updated><resolved>2014-04-07T08:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-04T16:37:01Z" id="39584987">I think it is a great idea to use doc values for this use-case indeed!

Your patch looks like a good start, I think there are two things to do before being able to merge it in:
1. support multiple values per document
2. add field data support

The issue with binary doc values is that they only support one value per document. So you need to encode all values of the same document into a single field instance. You can look at `NumberFieldMapper.CustomLongNumericDocValuesField` to see how we do it for numeric fields. Very likely, field values should be encoded using length encoding:

```
[length1(vInt), value1 (bytes), length2(vInt), value2(bytes), ... ]
```

Field data support is going to be useful in order to get those values at search time. For example, you can look at `BinaryDVNumericIndexFieldData` and `BinaryDVNumericAtomicFieldData` that implement numeric field data on top of binary doc values. One major difference though is that you would need to implement `BytesValues` instead of `Long` or `DoubleValues`.
</comment><comment author="kzwang" created="2014-04-05T08:45:24Z" id="39632592">@jpountz I've updated the PR.
The values are encoded as `[total num of values, length1, value1, length2, value2 ...]`

The default field data format for binary data is `disabled` as I think normally it won't be used for sorting, faceting etc.
</comment><comment author="jpountz" created="2014-04-05T09:17:29Z" id="39633118">&gt; The values are encoded as [total num of values, length1, value1, length2, value2 ...]
&gt; The default field data format for binary data is disabled as I think normally it won't be used for sorting, faceting etc.

This makes sense. I left comments to improve the PR but it already looks really great!
</comment><comment author="kzwang" created="2014-04-05T11:06:29Z" id="39635022">@jpountz I've updated the PR as per your comment
</comment><comment author="jpountz" created="2014-04-05T12:38:15Z" id="39636657">Thanks @kzwang , I just gave it another look! It's getting close!
</comment><comment author="kzwang" created="2014-04-07T01:03:06Z" id="39689112">@jpountz updated PR with those changes
</comment><comment author="jpountz" created="2014-04-07T08:20:30Z" id="39705818">Merged, thanks @kzwang !
</comment><comment author="YunchaoGongSC" created="2016-01-29T00:56:25Z" id="176502425">@kzwang Noticed this PR. One question is why do we need to store data in doc values, instead of just putting them in memory (e.g. like an encoded string). If all data is in memory, access speed should be much faster than doc value who sits on disk. Thanks!
</comment><comment author="jpountz" created="2016-02-08T08:47:44Z" id="181258285">Storing data on disk does not necessarily mean that access will be slower thanks to the filesystem cache.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid values in elasticsearch.yml causes start to silently fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5668</link><project id="" key="" /><description>This was an entirely meat based problem, however ES should ideally handle it a little more gracefully.

When starting a new node I was seeing no ES or java processes at all;

``` bash
markw@na0-esmb-001:~$ sudo service elasticsearch start
 * Starting Elasticsearch Server                                          [ OK ] 
markw@na0-esmb-001:~$ ps -ef|grep java
markw     1898  1424  0 18:23 pts/0    00:00:00 grep java
```

Here's some package details;

``` bash
markw@na0-esmb-001:~$ java -version
java version "1.7.0_51"
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
markw@na0-esmb-001:~$ dpkg -l|grep elastic
ii  elasticsearch                     1.1.0                               Open Source, Distributed, RESTful Search Engine
markw@na0-esmb-001:~$ cat /etc/debian_version 
wheezy/sid
```

I tried to diagnose this by calling the full executable with all arguments;

``` bash
markw@na0-esmb-001:~$ sudo /usr/lib/jvm/java-7-oracle/bin/java -Xms4g -Xmx4g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/var/run/elasticsearch.pid -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-1.1.0.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch
{1.1.0}: Setup Failed ...
- SettingsException[Failed to load settings from [file:/etc/elasticsearch/elasticsearch.yml]]
    ScannerException[while scanning a simple key; could not found expected ':';  in 'reader', line 25, column 1:
    discovery.zen.ping_timeout: 30s
    ^]
```

This was the actual problem, I'd made a bad mistake in the d.z.p.u.h value, I was using a multi-session iterm window and missed the typo at the time;

``` bash
discovery.zen.ping.unicast.hosts:15172.30.5.3416172.30.5.35, 172.30.5.17]
discovery.zen.ping_timeout: 30s
```

Ideally, ES shouldn't return an OK value if it doesn't actually start.

I don't know if validation of the elasticsearch.yml is in scope here, but I imagine it might take a bit of work.
</description><key id="30730356">5668</key><summary>Invalid values in elasticsearch.yml causes start to silently fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Packaging</label><label>adoptme</label></labels><created>2014-04-02T22:11:01Z</created><updated>2014-12-30T15:01:55Z</updated><resolved>2014-12-30T15:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zzbi" created="2014-10-06T19:58:20Z" id="58085129">Additionally, if you do something like specify a value of True for node.master/node.data, it will silently set these values to false.  Logs indicate a value of 'True', however this is actually false because it's not literally 'true'.

In the logs I see:
   [elastic1][yydqFncaS1qVaASPrOtMlg][brn-elastic1][inet[/10.211.3.101:9300]]{data=True, master=True}, local

But the node never becomes either data or master.
</comment><comment author="clintongormley" created="2014-10-15T15:17:24Z" id="59222465">@markwalkom agreed.  @zzbi your issue is different - I've opened #8097 for that.
</comment><comment author="t-lo" created="2014-12-03T10:45:13Z" id="65388651">It looks to me like this issue has the same root cause as #8652, which would be fixed by PR #8742.
</comment><comment author="clintongormley" created="2014-12-30T15:01:55Z" id="68363001">Closing  in favour of #8652
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE in PagedBytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5667</link><project id="" key="" /><description>I rebased #3278 to latest master and one of my benchmarks is throwing a NPE.  Traced it down to ref.bytes being null for the array copy.  See [PagedBytesReference.java#L448](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java#L448).

Not sure why, but this is the chunk of code that triggers this in my PR is:

[TermsFilterParser.java#L135](https://github.com/mattweber/elasticsearch/blob/terms_lookup_by_query/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java#L135)

```
 if ("filter".equals(currentFieldName)) {
    lookupFilter = XContentFactory.contentBuilder(parser.contentType());
    lookupFilter.copyCurrentStructure(parser);
}
```

I can push up the rebased PR that is failing if you need it to test.  Let me know.

/cc @s1monw @hhoffstaette 
</description><key id="30727961">5667</key><summary>NPE in PagedBytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T21:38:54Z</created><updated>2014-04-03T16:12:39Z</updated><resolved>2014-04-03T16:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T21:39:51Z" id="39387388">a test for this would be awesome!
</comment><comment author="mattweber" created="2014-04-02T21:41:18Z" id="39387521">Let me see if I can get something specific.  #3278 is pretty complex.
</comment><comment author="mattweber" created="2014-04-02T23:05:41Z" id="39395244">Ironically, trying to write a test for this triggered another.  

Drop this into [XContentBuilderTests.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java)

```
    @Test
    public void testCopyCurrentStructure() throws Exception {
        XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);
        builder.startObject()
            .field("test", "test field")
            .startObject("filter")
                .startObject("terms");

        // up to 20k random terms
        int numTerms = randomInt(20000) + 1;
        List&lt;String&gt; terms = new ArrayList&lt;&gt;(numTerms);
        for (int i = 0; i &lt; numTerms; i++) {
            terms.add("test" + i);
        }

        builder.field("fakefield", terms).endObject().endObject().endObject();

        XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(builder.bytes());

        XContentBuilder filterBuilder = null;
        XContentParser.Token token;
        String currentFieldName = null;
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
            if (token == XContentParser.Token.FIELD_NAME) {
                currentFieldName = parser.currentName();
            } else if (token.isValue()) {
                if ("test".equals(currentFieldName)) {
                    assertThat(parser.text(), equalTo("test field"));
                }
            } else if (token == XContentParser.Token.START_OBJECT) {
                if ("filter".equals(currentFieldName)) {
                    filterBuilder = XContentFactory.contentBuilder(parser.contentType());
                    filterBuilder.copyCurrentStructure(parser);
                }
            }
        }

        assertNotNull(filterBuilder);
        parser = XContentFactory.xContent(XContentType.JSON).createParser(filterBuilder.bytes());
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
        assertThat(parser.currentName(), equalTo("terms"));
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
        assertThat(parser.currentName(), equalTo("fakefield"));
        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_ARRAY));
        int i = 0;
        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
            assertThat(parser.text(), equalTo(terms.get(i++)));
        }

        assertThat(i, equalTo(terms.size()));
    }
```

Run test with:

```
mvn test -Dtests.seed=F330AACFAFD76C8B -Dtests.class=org.elasticsearch.common.xcontent.builder.XContentBuilderTests -Dtests.method=testCopyCurrentStructure -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
```

I am wondering if my original issue is something to do with threading.  Multiple shards on the same node all parsing the terms filter and calling copyCurrentStructure.  Not sure, still trying to reproduce in an actual test.
</comment><comment author="mattweber" created="2014-04-03T04:24:27Z" id="39411624">Actually this test is for the error I reported, tests run with -ea enabled which triggers the assertion.  In my benchmark I didn't have -ea enabled so it hits the NPE.  Running my benchmark with -ea triggers this same assertion.
</comment><comment author="mattweber" created="2014-04-03T04:25:33Z" id="39411660">Need a PR for the test or is the code above enough?
</comment><comment author="hhoffstaette" created="2014-04-03T09:18:38Z" id="39430468">It is not clear to me which assertion you are talking about in https://github.com/elasticsearch/elasticsearch/issues/5667#issuecomment-39411624 - are you referring to the assert right before the arraycopy or one in your tests? Regardless, it's not surprising that this would fail when the same PBR and/or stream is used &amp; mutated by multiple threads. Hitting the NPE has btw nothing to do with the assertion, and would just be a side effect of corruption: the condition for the assert still implies that ref.bytes is not null and valid.
</comment><comment author="mattweber" created="2014-04-03T16:11:11Z" id="39470825">Awesome @s1monw!  I will give this a try shortly!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update allocation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5666</link><project id="" key="" /><description>fixed typo: `can` instead of `con`
</description><key id="30724954">5666</key><summary>Update allocation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">christoph-frick</reporter><labels /><created>2014-04-02T21:01:30Z</created><updated>2014-07-10T12:16:33Z</updated><resolved>2014-04-17T12:43:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-02T22:01:10Z" id="39389567">Good catch! Can you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can get this merged?
</comment><comment author="christoph-frick" created="2014-04-17T09:26:35Z" id="40696428">well... i did so some time ago
</comment><comment author="clintongormley" created="2014-04-17T12:43:29Z" id="40709618">Thanks @christoph-frick - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>maxHeaderSize is exceeding default value of 8192bytes and causing exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5665</link><project id="" key="" /><description>I'm using Elasticsearch behind an Apache reverse proxy, coupled with VAS authentication.

On certain occasions, the header size is exceeding 8192 bytes (error below) and ES throws an exception which causes the request to fail.

It looks like maxHeaderSize in the netty module is defaulting to 8192 bytes, but I can't find any way of overriding (increasing) this value, and if not, can it be added as a configurable parameter in the yml file?

[2014-04-02 08:42:11,500][WARN ][http.netty               ] [xxxxx] Caught exception while handling client http traffic, closing connection [id: 0x121a4d3d, /127.0.0.1:33167 =&gt; /127.0.0.1:9200]

org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: HTTP header is larger than 8192 bytes.
        at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.readHeader(HttpMessageDecoder.java:596)
</description><key id="30718334">5665</key><summary>maxHeaderSize is exceeding default value of 8192bytes and causing exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schmorgs</reporter><labels /><created>2014-04-02T19:38:40Z</created><updated>2015-08-05T15:52:42Z</updated><resolved>2014-04-02T19:44:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-02T19:44:38Z" id="39374540">Hi @schmorgs, you should be able to configure this with:

``` yaml
http.max_header_size: 16kb
```

In elasticsearch.yml

Closing this for now, if you need to follow up the mailing list is probably the best place for questions like this: https://groups.google.com/forum/?fromgroups#!forum/elasticsearch
</comment><comment author="schmorgs" created="2014-04-02T20:20:12Z" id="39378382">Excellent, thank you so much.  I&#8217;ve spent 4 hours today looking at how to solve this :)

From: Lee Hinman [mailto:notifications@github.com] 
Sent: 02 April 2014 20:45
To: elasticsearch/elasticsearch
Cc: schmorgs
Subject: Re: [elasticsearch] maxHeaderSize is exceeding default value of 8192bytes and causing exception (#5665)

Hi @schmorgs https://github.com/schmorgs , you should be able to configure this with:

http.max_header_size: 16kb

In elasticsearch.yml

Closing this for now, if you need to follow up the mailing list is probably the best place for questions like this: https://groups.google.com/forum/?fromgroups#!forum/elasticsearch

&#8212;
Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/issues/5665#issuecomment-39374540 .  https://github.com/notifications/beacon/5444727__eyJzY29wZSI6Ik5ld3NpZXM6QmVhY29uIiwiZXhwaXJlcyI6MTcxMjA4NzEwNiwiZGF0YSI6eyJpZCI6MjkxMzA2OTZ9fQ==--a3aa48e20ffa0a3644fe1174c80845066640076a.gif 
</comment><comment author="denispeplin" created="2015-08-05T15:52:25Z" id="128047276">Thanks!
It fixed the issue.
1. Probably, it was caused by some cookies for `localhost`, sent by `Chrome`, which I use most.
2. `127.0.0.1` worked fine.
3. From `Firefox`, `localhost` worked fine.
4. From `Chrome`, it was just 'connection error'
5. The exception name, `TooLongFrameException`, gives nothing useful when googling.
6. The setting doesn't even appear in default `elasticsearch.yml` config.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup IndicesFieldDataCache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5664</link><project id="" key="" /><description>I cleaned up IndicesFieldDataCache a bit to not take null values anymore and made some stuff final here and there.
</description><key id="30692621">5664</key><summary>Cleanup IndicesFieldDataCache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-02T14:31:32Z</created><updated>2014-07-16T21:46:55Z</updated><resolved>2014-04-03T10:58:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-02T14:46:50Z" id="39338984">left one comment but other than that this looks good to me!
</comment><comment author="s1monw" created="2014-04-02T14:50:22Z" id="39339435">pushed a new commit
</comment><comment author="dakrone" created="2014-04-02T14:53:45Z" id="39339892">LGTM
</comment><comment author="s1monw" created="2014-04-02T20:16:08Z" id="39377950">@dakrone I added another commit to address your concern.... I actually added an interesting change [here](https://github.com/s1monw/elasticsearch/commit/18163786be8efc6f432009c8a7509ff7e0f7c190#diff-ccc74e97c274044aff0e80b1865e323eR139) I think we miss to assign the `key.sizeInBytes` here which can be problematic if you use soft-cache.. this could also explain some breaker problems we had... not sure yet. I think we should consolidate that code since it's more or less identical?
</comment><comment author="dakrone" created="2014-04-02T21:28:09Z" id="39386116">@s1monw yes, I see what you're saying, I've been trying to reproduce a case where the value of `AtomicFieldData` is null and `key.sizeInBytes` is not set on master (with soft field data), but I haven't been able to yet.

What do you think about adding either an assert, or an else statement that throws an exception to https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java#L102-L104 that complains when `sizeInBytes` is not set and can't be retrieved from the field data itself? Not sure though whether it's necessary since we have the assert in https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java#L51

&gt; I think we should consolidate that code since it's more or less identical?

+1 on consolidating the repeating code into a function
</comment><comment author="kimchy" created="2014-04-03T10:27:02Z" id="39435930">I don't think the sizeInBytes not being set is a problem for IndicesFieldDataCache, since its not even exposing the ability to use soft reference (bad!). But its cleaner to set it where you have it. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A Get request with a version set should always validate for equality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5663</link><project id="" key="" /><description>Currently we use the same version checking logic for get and indexing requests. The leads to the wrong semantics for the `EXTERNAL` and the `EXTERNAL_TYPE` versioning systems.  For example, doing `GET index/type/1?version=10&amp;version_type=external` will return the document if it has a version of 9 or less. 

We should make it return documents only if their version match the specified version.
</description><key id="30686244">5663</key><summary>A Get request with a version set should always validate for equality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>blocker</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T13:15:08Z</created><updated>2014-04-25T21:07:09Z</updated><resolved>2014-04-25T21:07:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Indexing a document fails when setting `version=0` &amp;  `version_type=external`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5662</link><project id="" key="" /><description>This is a regression introduced in 4e0e40644d53c701c2a7525dda7179829ac75d6c , which was release with 1.1
</description><key id="30685041">5662</key><summary>Indexing a document fails when setting `version=0` &amp;  `version_type=external`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:CRUD</label><label>blocker</label><label>regression</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T12:58:05Z</created><updated>2015-06-08T00:44:01Z</updated><resolved>2014-05-16T20:43:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-04-04T05:10:18Z" id="39532400">This is a regression added [here](https://github.com/elasticsearch/elasticsearch/blob/4e0e40644d53c701c2a7525dda7179829ac75d6c/src/main/java/org/elasticsearch/index/VersionType.java#L89), but it was the [already documented behavior](http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/docs-index_.html#index-versioning) in 0.90:

&gt; To enable this functionality, `version_type` should be set to `external`. The value provided must be a numeric, long value greater than 0, and less than around 9.2e+18.

Personally, I think that it is an improvement, particularly because it matches the existing documentation and it makes sense to start at `&gt; 0`.
</comment><comment author="amitsoni13" created="2014-04-12T23:00:07Z" id="40294006">Hi - Just to add, I think it would be incorrect to assume that the version numbers would always be positive. In our system, the first version number is zero and hence 1.1 failed for me as soon as we deployed and we had to revert.
</comment><comment author="bleskes" created="2014-04-16T13:08:19Z" id="40596140">@amitsoni13 I agree. 

@pickypg Although it was documented we didn't enforce it on the 0.90 and 1.0 so people could in practice index document with version 0 so they now have it in their index. I will be working to make 0 a valid external version. 
</comment><comment author="pickypg" created="2014-04-16T15:27:12Z" id="40612774">@bleskes Sounds good to me. `0` makes just as much sense to developers.

I'm curious though, are you going to make `0` the default starting value even without anything specified, or will that stay as `1`?
</comment><comment author="bleskes" created="2014-04-16T16:04:21Z" id="40617729">@pickypg I believe you refer to the internal version type where ES is in charge of managing versions - this will not change at it will start at `1`.  This discussion only relates to the `external` version type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Calling the Update API using EXTERNAL(_GTE) version type should throw a validation error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5661</link><project id="" key="" /><description>The update API is utility to do a get-change-and-index cycle while guarantying the document didn't change between the get and the index phase of the operation. We do so using Elasticsearch's versioning system which allows you to fail an indexing request if the document doesn't have the expected version (i.e., changed). 

At the moment, using the `EXTERNAL` or `EXTERNAL_GTE` version types break this guaranties. These should be disabled. In the future we can extend the index API to allow supplying both an expected exiting version and a version to be index at which point we could have a proper support in the update API.
</description><key id="30684863">5661</key><summary>Calling the Update API using EXTERNAL(_GTE) version type should throw a validation error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>blocker</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T12:55:21Z</created><updated>2015-01-15T19:49:10Z</updated><resolved>2014-04-25T21:07:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mjgp2" created="2014-11-10T11:32:30Z" id="62372418">So you have removed the ability to update only if the current version matches a specific (external) version?
</comment><comment author="bleskes" created="2014-11-10T12:28:55Z" id="62377702">@mjgp2 well, you can abuse it that way. The thing is that if external version semantics (update if version is lower then X) can not guarantee we don't override changes made by a concurrent indexing/update commands.
</comment><comment author="mjgp2" created="2014-11-10T14:25:22Z" id="62390457">@bleskes could you point me at where using a generated id vs using an externally generated id differs here in the code? I'm trying to understand exactly what the limitation is to work out how easy it is to fix.
</comment><comment author="bleskes" created="2014-11-10T19:51:38Z" id="62443448">@mjgp2 the update API is nothing else then a get call followed by an index call. See: 

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L76

Followed by:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java#L214
</comment><comment author="mjgp2" created="2014-11-10T23:12:07Z" id="62473754">Oh ok so it should be fairly simple to reintroduce using external ids then, as in there's no impassable barrier here by the looks of it? :)
</comment><comment author="bleskes" created="2014-11-11T08:59:19Z" id="62519189">Well, as I said , the problem is an algorithmic one - the mixture of external version semantics during the get process (exact match) and indexing process (local indexed version must be lower then the once supplied) that don&#8217;t play well together to guaranty no data is lost during concurrent indexing. In general the external version is designed to be used with an external source of truth where the update will be done and sent to ES as an indexing requests.

On Tue, Nov 11, 2014 at 12:12 AM, Matthew Painter
notifications@github.com wrote:

&gt; ## Oh ok so it should be fairly simple to reintroduce using external ids then, as in there's no impassable barrier here by the looks of it? :)
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/5661#issuecomment-62473754
</comment><comment author="cvasii" created="2014-11-24T08:56:32Z" id="64165925">But if I use version type as "force", then it should be the same thing as using version type external in pre 1.2 releases?

Actually, is not. No VersionConflictException will be thrown in this case. Ignore my question
</comment><comment author="TronPaul" created="2015-01-13T21:23:26Z" id="69822741">Please update the documentation surrounding external versioning to indicate that you shouldn't use the Update API with external versioning.

http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/optimistic-concurrency-control.html
http://www.elasticsearch.org/blog/elasticsearch-versioning-support/

Was switching to external versioning following the above links and was confused by the errors I was getting until running into this issue.
</comment><comment author="clintongormley" created="2015-01-15T19:49:10Z" id="70149319">@TronPaul the update docs already specify that external versioning is not supported: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-update.html#_parameters_3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't connect to embedded elasticsearch node via TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5660</link><project id="" key="" /><description>ES 1.0.1, Java 1.7 on Debian Linux and Osx

I've a setup were I use an embedded node that holds data and starts at the beginning of every integration test. Then, depending on the case, I use either data less nodes as clients or just the http rest api. In both situations everything works fine. The only problem is with TransportClient. I start the Transport client on the same VM (of course, to do the integration test) and I always get a connection refused. 

The code I use to set up the main embedded node is:

```
  port = 11547; // User ports range 1024 - 49151
        tcpport = 9300;
        settings.put("http.port", port);
        settings.put("transport.tcp.port", tcpport);

        Settings esSettings = settings.build();

        node = NodeBuilder.nodeBuilder().local(true).settings(esSettings).node();
        node.start();
        Client client = node.client();
           client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
```

so far so good. This works well and it's proven. But when shortly after I set up the TransportClient like this and try to index something, I will get a connection refused error:

```
        TransportClient client = new TransportClient();


        client.addTransportAddress(new InetSocketTransportAddress("127.0.0.1", getTcpPort()));

        client.prepareIndex("test", "type").setSource("field", "value").execute().actionGet();
```

stacktrace:

```
org.elasticsearch.transport.ConnectTransportException: [][inet[/127.0.0.1:9300]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:683)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:643)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:610)
    at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:133)
    at org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler.doSample(TransportClientNodesService.java:355)
    at org.elasticsearch.client.transport.TransportClientNodesService$NodeSampler.sample(TransportClientNodesService.java:301)
    at org.elasticsearch.client.transport.TransportClientNodesService.addTransportAddresses(TransportClientNodesService.java:169)
    at org.elasticsearch.client.transport.TransportClient.addTransportAddress(TransportClient.java:237)
    at com.my.estests.test.TransportClientTest.testTransportClient(TransportClientTest.java:23)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:202)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: java.net.ConnectException: Connection refused: /127.0.0.1:9300
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
[2014-04-02 12:07:36,961][TRACE][org.elasticsearch.transport.netty] [Scarlet Scarab] connect exception caught on transport layer [[id: 0xb6d34b8c]]
java.net.ConnectException: Connection refused: /127.0.0.1:9300
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```

I tried to fix it in many ways. The funny thing is that if I run the test without the embedded node, for example running the standard Elasticsearch dist from the zip, it can connect properly. 

If that can help, here is some logging with level TRACE

```
[2014-04-02 12:07:32,842][DEBUG][org.elasticsearch.cluster.service] [Immortus] publishing cluster state version 3
[2014-04-02 12:07:32,842][DEBUG][org.elasticsearch.cluster.service] [Immortus] set local cluster state to version 3
[2014-04-02 12:07:32,843][DEBUG][org.elasticsearch.river.cluster] [Immortus] processing [reroute_rivers_node_changed]: execute
[2014-04-02 12:07:32,843][INFO ][org.elasticsearch.node   ] [Immortus] started
[2014-04-02 12:07:32,843][DEBUG][org.elasticsearch.river.cluster] [Immortus] processing [reroute_rivers_node_changed]: no change in cluster_state
[2014-04-02 12:07:32,843][DEBUG][org.elasticsearch.cluster.service] [Immortus] processing [updating local node id]: done applying updated cluster_state (version: 3)
[2014-04-02 12:07:32,843][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] Elasticsearch embedded starting with settings:
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] node.local: true
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] cluster.name: 5Hn0b5Hfzc
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] path.data: /tmp/cmZnsWMDkn1333493018569044574
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] path.conf: /tmp/xA9Ueg7vTJK5cEg8897978197509056413
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] http.port: 11547
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] transport.tcp.port: 9300
[2014-04-02 12:07:32,844][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] name: Immortus
[2014-04-02 12:07:32,845][INFO ][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] path.logs: /home/drossi/work/cs/cms-content-service-search/logs
[2014-04-02 12:07:32,845][DEBUG][com.netaporter.cms.estests.esconfig.SetupEsTestHelper] Elasticsearch started
[2014-04-02 12:07:32,848][TRACE][org.elasticsearch.action.admin.cluster.health] [Immortus] Calculating health based on state version [3]
[2014-04-02 12:07:32,857][INFO ][org.elasticsearch.plugins] [Scarlet Scarab] loaded [], sites []
[2014-04-02 12:07:32,908][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [generic], type [cached], keep_alive [30s]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [index], type [fixed], size [6], queue_size [200]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [bulk], type [fixed], size [6], queue_size [50]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [get], type [fixed], size [6], queue_size [1k]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [search], type [fixed], size [18], queue_size [1k]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [suggest], type [fixed], size [6], queue_size [1k]
[2014-04-02 12:07:32,909][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [percolate], type [fixed], size [6], queue_size [1k]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [flush], type [scaling], min [1], size [3], keep_alive [5m]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [merge], type [scaling], min [1], size [3], keep_alive [5m]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [refresh], type [scaling], min [1], size [3], keep_alive [5m]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [warmer], type [scaling], min [1], size [3], keep_alive [5m]
[2014-04-02 12:07:32,910][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [snapshot], type [scaling], min [1], size [3], keep_alive [5m]
[2014-04-02 12:07:32,911][DEBUG][org.elasticsearch.threadpool] [Scarlet Scarab] creating thread_pool [optimize], type [fixed], size [1], queue_size [null]
[2014-04-02 12:07:32,912][DEBUG][org.elasticsearch.transport.netty] [Scarlet Scarab] using worker_count[12], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb-&gt;512kb]
[2014-04-02 12:07:32,913][DEBUG][org.elasticsearch.client.transport] [Scarlet Scarab] node_sampler_interval[5s]
[2014-04-02 12:07:36,941][DEBUG][org.elasticsearch.client.transport] [Scarlet Scarab] adding address [[#transport#-1][d][inet[/127.0.0.1:9300]]]
[2014-04-02 12:07:36,941][TRACE][org.elasticsearch.client.transport] [Scarlet Scarab] connecting to listed node (light) [[#transport#-1][d][inet[/127.0.0.1:9300]]]
[2014-04-02 12:07:36,960][DEBUG][org.elasticsearch.client.transport] [Scarlet Scarab] failed to connect to node [[#transport#-1][d][inet[/127.0.0.1:9300]]], removed from nodes list
org.elasticsearch.transport.ConnectTransportException: [][inet[/127.0.0.1:9300]] connect_timeout[30s]
```

Can anybody help with this?
</description><key id="30683065">5660</key><summary>Can't connect to embedded elasticsearch node via TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dariodariodario</reporter><labels /><created>2014-04-02T12:27:45Z</created><updated>2016-11-24T09:30:54Z</updated><resolved>2014-04-02T14:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-02T13:38:53Z" id="39330706">I think mailing list would be much better way to ask questions like this. You are starting local node, which is using [local transport](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-transport.html#_local_transport), which is not listening on port 9300. 
</comment><comment author="dariodariodario" created="2014-04-02T13:56:33Z" id="39332693">Sorry, I tried to post to the google group, but I didn't get much help. 

Actually it is starting on port 9300 and the transport is trying to connect to that port. Look the lines:

```
org.elasticsearch.transport.ConnectTransportException: [][inet[/127.0.0.1:9300]] connect_timeout[30s]
```
</comment><comment author="imotov" created="2014-04-02T14:07:21Z" id="39333918">Closing the issue. We will continue on the [mailing list](https://groups.google.com/d/msg/elasticsearch/WG8c3wO0aAU/XFjh3jSdDbwJ). 
</comment><comment author="OElesin" created="2015-03-06T13:29:11Z" id="77558289">Hello, I have an issue with LogStash and ElasticSeacrch. Each time I try to push data via LogStash, it hangs and I get no response. Although, earlier today, it worked and later it stopped working. No data is being transferred.

Using milestone 2 input plugin 'file'. This plugin should be stable, but if you see strange behavior, please let us know! For more information on plugin milestones, see http://logstash.net/docs/1.5.0.beta1/plugin-milestones {:level=&gt;:warn}
Mar 06, 2015 1:25:17 PM org.elasticsearch.plugins.PluginsService &lt;init&gt;
INFO: [logstash-terragon-HP-Pro-3120-Microtower-PC-24973-3846] loaded [], sites []

This is the point where it hangs. Kindly help
</comment><comment author="kimchy" created="2015-03-06T13:42:21Z" id="77559986">@OElesin maybe open ask the question on the logstash mailing list? if its a bug, open an issue on logstash repo https://github.com/elasticsearch/logstash.
</comment><comment author="shang1989" created="2016-11-24T09:30:54Z" id="262729849"> node = NodeBuilder.nodeBuilder().local(**false**).settings(esSettings).node();

set the local false, then can be connected by api

</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up `exists` and `missing` filters on high-cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5659</link><project id="" key="" /><description>The way that the `exists` filter works is by merging all postings lists. `missing` just wraps an `exists` filter into a `not` filter.

Merging all postings lists can however be very slow on high-cardinality fields. I think there are two ways to fix it:
1. make these filters run on top of field data,
2. or add a new metadata field that we could eg. call `_field_names` that would index all field names of a document.

Working on field data has the drawback of requiring a lot of stuff to be loaded into memory if the field doesn't have doc values, and the returned filter cannot skip.

I tend to like indexing field names because it would not load anything into memory with a default setup, and the returned filter could skip efficiently since it would be based on a postings list. But unfortunately it could not be used on indices that have been created before we introduce this new metadata field.
</description><key id="30678813">5659</key><summary>Speed up `exists` and `missing` filters on high-cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-02T11:13:48Z</created><updated>2015-06-07T14:55:40Z</updated><resolved>2014-06-19T10:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T11:29:28Z" id="39319393">I really like the `_field_names` approach!
</comment><comment author="uboness" created="2014-04-02T11:30:24Z" id="39319460">+1 on the `_field_names` approach... I think we'll find them to be useful for other things as well
</comment><comment author="clintongormley" created="2014-04-02T11:54:34Z" id="39321243">+1 on `_field_names` - awesome solution
</comment><comment author="dadoonet" created="2014-04-02T12:28:08Z" id="39323763">So it's like _all but for fields names?
In the future, could we need to have separate fields names and need to use a "copy_fieldname_to" feature?

Thinking about it loud. May be there is no use case for that...
</comment><comment author="clintongormley" created="2014-05-13T10:04:32Z" id="42937478">I don't want this to be forgotten, so I've added a v1.3.0 label.  No pressure ;)
</comment><comment author="jpountz" created="2014-05-23T09:17:57Z" id="43987796">@spinscale asked me about the disk footprint of this feature. In general it is very low: its index options are `DOCS_ONLY` and is only enabled for fields that are indexed or have doc values. Additionally, fields that are contained in most documents will have dense postings lists that typically compress very well.

I did some experiments in 2 extreme cases:
- 5 user-defined fields in addition to the metadata fields (_uid, etc.) that are contained in every document: overhead of ~0.5 bytes per document.
- 100 user-defined fields in the mapping and each document has 5 random fields from these 100 fields: overhead of ~4.7 bytes per document.

This looks very reasonable to me. Even the 2nd case which has very sparse documents takes less than one byte per field per document.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide upgrade functionality for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5658</link><project id="" key="" /><description>It'd be nice if there was an upgrade function for the plugin binary to save having to uninstall and then install when a new version of a plugin is released.

Even if it was just a download and overwrite to begin with, it could be expanded from there.
</description><key id="30659549">5658</key><summary>Provide upgrade functionality for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-04-02T04:57:36Z</created><updated>2014-04-02T07:05:17Z</updated><resolved>2014-04-02T07:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-02T06:47:25Z" id="39293717">Hi Mark,

I think it's a duplicate of #5064
See also #5524 

WDYT?
</comment><comment author="markwalkom" created="2014-04-02T07:05:17Z" id="39294631">Yep, sorry.

I was poking around the enhancement area and didn't see anything which is why I raised this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot succeeds even when some nodes cannot access shared repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5657</link><project id="" key="" /><description>Hi,
 I am testing out using the snapshot/restore feature in ES 1.0.0. I have created the following setup.
An ES cluster with three nodes. A shared file system which can only be accessed by Node 1[Master] and Node 2. This shared fs is registered as the repository for taking backups on Node 1.

I noticed that even if I 
a) force some of my data to be present only on Node 3, and 
b) ensure that Node 3 cannot access the shared repository

Taking a snapshot of the entire cluster still reports a success. However, when I browse the contents of the snapshot folder, I do not see any of the data from Node 3. I was expecting a "RepositoryMissing" exception to be thrown by Node 3. Have I misunderstood how ES snapshotting works?

Thanks!
</description><key id="30648577">5657</key><summary>Snapshot succeeds even when some nodes cannot access shared repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">RobbieHer</reporter><labels><label>bug</label></labels><created>2014-04-01T23:48:57Z</created><updated>2014-12-31T05:59:04Z</updated><resolved>2014-05-31T02:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-02T02:06:19Z" id="39282033">When snapshot is finished, could you execute the following command to see if there are any shard failures there: `curl -XGET "localhost:9200/_snapshot/repository_name/snapshot_name"`? If you have replicas for these indices enabled it's also possible that all primary shards are located on node 1 and node 2, in this case the snapshot might be successful since only primary shards are getting snapshotted. 
</comment><comment author="RobbieHer" created="2014-04-02T02:17:57Z" id="39282592">Thanks! I did verify the case where I had a node in a cluster with only one shard with 0 replicas. This node could not connect to the repo and spewed out errors in the elastic search logs. However, the curl command to take a snapshot did not return any errors and reported all shards to be snapshots successfully.
</comment><comment author="imotov" created="2014-04-02T02:19:59Z" id="39282689">Could you gist the errors from the log that you have seen?
</comment><comment author="RobbieHer" created="2014-04-02T05:02:20Z" id="39289111">Sure. Here are the errors from the node which had the data , but could not write to the repository.

[2014-03-31 17:33:37,684][WARN ][repositories             ] [Hermod] failed to create repository [fs][my_backup]
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]
  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.fs.FsRepository
  while locating org.elasticsearch.repositories.Repository

1 error
        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:384)
        at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:280)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:427)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]
        at org.elasticsearch.common.blobstore.fs.FsBlobStore.&lt;init&gt;(FsBlobStore.java:52)
        at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(FsRepository.java:83)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        ... 10 more
[2014-03-31 17:33:37,699][WARN ][repositories             ] [Hermod] failure updating cluster state
org.elasticsearch.repositories.RepositoryException: [my_backup] failed to create repository
        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:394)
        at org.elasticsearch.repositories.RepositoriesService.clusterChanged(RepositoriesService.java:280)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:427)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]
  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.fs.FsRepository
  while locating org.elasticsearch.repositories.Repository

1 error
        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:384)
        ... 6 more
Caused by: org.elasticsearch.common.blobstore.BlobStoreException: Failed to create directory at [C:/CanAccessOnlyFromNode1]
        at org.elasticsearch.common.blobstore.fs.FsBlobStore.&lt;init&gt;(FsBlobStore.java:52)
        at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(FsRepository.java:83)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        ... 10 more
[2014-03-31 17:33:37,738][INFO ][gateway                  ] [Hermod] recovered [4] indices into cluster_state
</comment><comment author="imotov" created="2014-04-02T12:05:12Z" id="39322099">I tried to reproduce this issue, but wasn't able to. Could you paste here the output of GET snapshot command for one of the snapshot that wasn't complete. The GET snapshot command look like this: `curl -XGET "localhost:9200/_snapshot/my_backup/snapshot_name"`, please replace `snapshot_name` with the name of your test snapshot.
</comment><comment author="RobbieHer" created="2014-04-02T17:13:00Z" id="39357096">The Get command shows that there are no failures

$ curl -XGET "localhost:9200/_snapshot/my_backup/snapshot_13"
{"snapshots":[{"snapshot":"snapshot_13","indices":["restore","newstore","store","default-encryptor-963"],"state":"SUCCESS","start_time":"2014-04-02T17:07:25.634Z","start_time_in_millis":1396458445634,"end_time":"2014-04-02T17:07:26.069Z","end_time_in_millis":1396458446069,"duration_in_millis":435,"failures":[],"shards":{"total":16,"failed":0,"successful":16}}]}
</comment><comment author="RobbieHer" created="2014-04-02T17:18:52Z" id="39357762">Could you also confirm if it is absolutely necessary for the same shared repository to be 'registered' from each of the ES nodes in a cluster? Currently, I registered the shared repository only from the node [Node 1] where I ran the backup command. When I run the 'register' command on Node 1, I see that the ES logs on Node 3 ( which cannot access the shared repo) show an error. However, the register command on Node 1 shows success.

$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
    "type": "fs",
    "settings": {
        "location": "X:/CanAccessOnlyFromNode1",
        "compress": true
    }
}'
{"acknowledged":true}
</comment><comment author="imotov" created="2014-04-03T04:44:05Z" id="39412288">As I mentioned before as long as you have this directory available on all nodes where primary shards are located, the snapshot will succeed. It looks like this is what happened to the snapshot that you provided. However, since you cannot control primary shard allocation, it's a good practice to have this directory available on all nodes.
</comment><comment author="geekpete" created="2014-04-03T07:45:36Z" id="39421577">Hi Team,

I've also seen the same symptom of snapshot being marked as successful when some nodes cannot write to the repository.

I'm using 1.0.1 with the aws-cloud plugin to store snapshots to S3.

When performing a snapshot, the snapshot is stored in S3 and marked as Successful. It also shows as an available "Successful" snapshot on my other cluster that I'm testing restore from.

When attempting a restore, elasticsearch performs some sort of consistency check on the snapshot and determines it is incomplete and unable to be restored, returning:

```
{
  "readyState": 4,
  "responseText": "{\"error\":\"SnapshotRestoreException[[test_snapshots_repo:test_snapshot_2014-04-03.1046] index [my_test_index] wasn't fully snapshotted - cannot restore]\",\"status\":500}",
  "responseJSON": {
    "error": "SnapshotRestoreException[[test_snapshots_repo:test_snapshot_2014-04-03.1046] index [my_test_index] wasn't fully snapshotted - cannot restore]",
    "status": 500
  },
  "status": 500,
  "statusText": "Internal Server Error"
}
```

This is for the same reason explained above, where some nodes cannot access the storage location, so these nodes do not get to write their lucene segments to storage, but the other nodes do complete their pieces of the snapshot successfully. In my case, this was due to one or more nodes in my cluster being too far out of time sync that S3 would reject their connections. The cloud-aws plugin did throw a nice error in the logs for this from the nodes with the issue:

```
[2014-04-02 20:29:26,251][WARN ][snapshots                ] [elastic_node_1] [[my_test_index][8]] [my_test_repo:test_snapshot_2014-04-03.1046] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [my_test_index][8] The difference between the request time and the current time is too large.
```

Which is great, except that though this error is detected by the plugin and notified in the logs, Elasticsearch still marks the snapshot as successful.

I fixed the time sync (ntp) issues on my nodes that were too far out of sync with S3 and tried again.

On trying again, one other node had a problem with the snapshot repository, some sort of problem where cluster state didn't update this one node with the repository config:

```
failed to load class with value [s3]; tried [s3, org.elasticsearch.repositories.S3RepositoryModule, org.elasticsearch.repositories.s3.S3RepositoryModule, org.elasticsearch.repositories.s3.S3RepositoryModule]
```

After restarting this node, it was then able to write to the repository fine and I got a successful backup that was able to be restored on my other cluster.

So a really nice feature/fix would be to identify whatever logic elasticsearch runs at restore time to verify the consistency of the snapshot, use this logic after a snapshot is taken to verify the snapshot worked ok and mark it as bad if it has failed. As well as that, return a message stating which nodes had issues performing their snapshot task if this is able to be fed back to the user, eg:

```
"Snapshot failed: nodes that were unable to write to the snapshot repository: elastic_node_4, elastic_node_7."
```

The user will then have the clues to go and investigate specific nodes to troubleshoot.

Cheers.
</comment><comment author="geekpete" created="2014-04-03T07:47:55Z" id="39421737">Adding the consistency check of snapshots to the API would be a great feature as well, so users can run checks on snapshots in a repository manually and check to see if there are broken snapshots in there.

:)
</comment><comment author="geekpete" created="2014-04-03T07:48:49Z" id="39421813">Also, if you wanted to reproduce my particular scenario, you could get a cluster of nodes, manually set one node to more than say 30 minutes out of sync with the rest and then try to snapshot to S3 and it should be rejected.
</comment><comment author="imotov" created="2014-05-31T02:38:12Z" id="44715958">The new "PARTIAL" state that was added by #5792 should help to distinguish between snapshots that were completely successful and snapshots that contained some shards that failed to snapshot. Closing.
</comment><comment author="Rams20" created="2014-12-30T12:46:06Z" id="68353629">why primary shards are require for Elasticsearch backup snapshot...?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes randomly disconnected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5656</link><project id="" key="" /><description>We have a cluster running 1.0.0 in Azure using unicast discovery. Recently we started seeing exceptions like these in the logs:

[2014-04-01 21:40:22,720][DEBUG][action.admin.indices.status] [ES2PROD-M01] [usg-2014-03-04][4], node[3cCeFKJrTMWaIhE3R6tlZA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@2c06e675]
org.elasticsearch.transport.NodeDisconnectedException: [ES2PROD-D07][inet[/10.0.64.68:9300]][indices/status/s] disconnected

In this case D07 is still up and running. After several dozen of these exceptions, D07 is disconnected:

[2014-04-01 21:40:24,096][INFO ][cluster.service          ] [ES2PROD-M01] removed {[ES2PROD-D07][3cCeFKJrTMWaIhE3R6tlZA][es2prod-d07][inet[/10.0.64.68:9300]]{master=false},}, reason: zen-disco-node_failed([ES2PROD-D07][3cCeFKJrTMWaIhE3R6tlZA][es2prod-d07][inet[/10.0.64.68:9300]]{master=false}), reason transport disconnected (with verified connect)

Four seconds later the same node is added back:

[2014-04-01 21:40:28,712][INFO ][cluster.service          ] [ES2PROD-M01] added {[ES2PROD-D07][3cCeFKJrTMWaIhE3R6tlZA][es2prod-d07][inet[/10.0.64.68:9300]]{master=false},}, reason: zen-disco-receive(join from node[[ES2PROD-D07][3cCeFKJrTMWaIhE3R6tlZA][es2prod-d07][inet[/10.0.64.68:9300]]{master=false}])

In the mean time the cluster goes yellow and starts recovery. This does not seem like a timeout type of issue since it happens so quickly, and then the disconnected node is  added right back.
</description><key id="30643040">5656</key><summary>Nodes randomly disconnected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hglkrijger</reporter><labels /><created>2014-04-01T22:08:04Z</created><updated>2014-04-02T04:35:16Z</updated><resolved>2014-04-02T04:35:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-02T04:35:16Z" id="39288089">Let's see if we can help you at the mailing list, and if we end up with an improvement / feature, we can open a dedicated issue for it, I will close it for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>error w/ arcDistance* in script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5655</link><project id="" key="" /><description>Been experimenting w/ ES 1.1.0 and seeing this error:

[2014-03-31 18:37:47,807][DEBUG][action.search.type       ] [Spidercide] [_river][0], node[8g5DHgKoR6G-3lwIojQcPQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@15029ae] lastShard [true]
org.elasticsearch.search.SearchParseException: [_river][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"sort":[{"modified":{"order":"desc"}},"_score"],"from":0,"script_fields":{"distance":{"params":{"lat":37.7749295,"lon":-122.4194155},"script":"doc['location2.coordinates'].arcDistanceInMiles(lat, lon)"}},"fields":["_source"],"facets":{"services":{"facet_filter":{"and":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]},"terms":{"field":"services","size":200}},"proficiencies":{"facet_filter":{"and":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]},"terms":{"field":"good_at","size":50}}},"filter":{"bool":{"must":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]}},"query":{"match_all":{}},"size":20}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.search.SearchParseException: [_river][0]: from[-1],size[-1]: Parse Failure [No mapping found for [modified] in order to sort on]
    at org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:198)
    at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:172)
    at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:80)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)
    ... 11 more
[2014-03-31 18:37:47,840][DEBUG][action.search.type       ] [Spidercide] [9733] Failed to execute fetch phase
java.lang.RuntimeException: cannot invoke method: arcDistanceInMiles
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessorNest.getValue(MapAccessorNest.java:54)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:74)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:452)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)
    ... 17 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.fielddata.ScriptDocValues$GeoPoints.arcDistanceInMiles(ScriptDocValues.java:365)
    ... 21 more
[2014-03-31 18:37:47,840][DEBUG][action.search.type       ] [Spidercide] [9732] Failed to execute fetch phase
java.lang.RuntimeException: cannot invoke method: arcDistanceInMiles
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessorNest.getValue(MapAccessorNest.java:54)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:74)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:452)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)
    ... 17 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.fielddata.ScriptDocValues$GeoPoints.arcDistanceInMiles(ScriptDocValues.java:365)
    ... 21 more

The error doesn't cause anything to actually not work -- which is to show the distance to the vendor listing.  Here is the query that produces the error:

```
{
  "sort": [
    {
      "modified": {
        "order": "desc"
      }
    },
    "_score"
  ],
  "from": 0,
  "script_fields": {
    "distance": {
      "params": {
        "lat": 37.7749295,
        "lon": -122.4194155
      },
      "script": "doc['location2.coordinates'].arcDistanceInMiles(lat, lon)"
    }
  },
  "fields": [
    "_source"
  ],
  "facets": {
    "services": {
      "facet_filter": {
        "and": [
          {
            "terms": {
              "services": [
                "52431dc3c61e364f922ce716"
              ],
              "execution": "and"
            }
          }
        ]
      },
      "terms": {
        "field": "services",
        "size": 200
      }
    },
    "proficiencies": {
      "facet_filter": {
        "and": [
          {
            "terms": {
              "services": [
                "52431dc3c61e364f922ce716"
              ],
              "execution": "and"
            }
          }
        ]
      },
      "terms": {
        "field": "good_at",
        "size": 50
      }
    }
  },
  "filter": {
    "bool": {
      "must": [
        {
          "terms": {
            "services": [
              "52431dc3c61e364f922ce716"
            ],
            "execution": "and"
          }
        }
      ]
    }
  },
  "query": {
    "match_all": {}
  },
  "size": 20
}
```

When, I remove the offending script, the error goes away but so does the functionality!  Any idea what's going on here?
</description><key id="30640992">5655</key><summary>error w/ arcDistance* in script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dblado</reporter><labels /><created>2014-04-01T21:45:41Z</created><updated>2014-04-02T19:46:40Z</updated><resolved>2014-04-02T19:46:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T08:53:09Z" id="39303932">it seems like one or more docs have no value in that field can you check if this works:

```
"script": "if (!doc['location2.coordinates'].isEmpty()) { return doc['location2.coordinates'].arcDistanceInMiles(lat, lon) } else { return 0.0 } "
```

I hope this is valid `mvel` :) but you get my point I hope
</comment><comment author="dblado" created="2014-04-02T19:46:40Z" id="39374758">That did it -- thanks!
I did see the other issues regarding isEmpty() but didn't put two and two together...my bad.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Replaced RestTestSuiteRunner with parametrized test that uses RandomizedRunner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5654</link><project id="" key="" /><description>ElasticsearchRestTests extends now ElasticsearchIntegrationTest and makes use of our ordinary test infrastructure, in particular all randomized aspects now come for free instead of having to maintain a separate (custom) tests runner

We previously parsed only the tests that needed to be run given the version of the cluster the tests are running against. This doesn't happen anymore as it didn't buy much and it would be harder to support as the tests get now parsed before the test cluster gets started. Thus all the tests are now parsed regardless of their skip sections, afterwards the ones that don't need to be run will be skipped through assume directives.

Fixed REST tests that rely on a specific number of shards as this change introduces also random number of shards and replicas (through randomIndexTemplate)
</description><key id="30633780">5654</key><summary>[TEST] Replaced RestTestSuiteRunner with parametrized test that uses RandomizedRunner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T20:16:10Z</created><updated>2014-06-28T07:15:41Z</updated><resolved>2014-04-07T15:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dweiss" created="2014-04-07T08:11:41Z" id="39705248">This looks like a nice cleanup overall. I think problems with non-unique class names can be addressed outside of ES code (once they're identified).
</comment><comment author="javanna" created="2014-04-07T08:12:27Z" id="39705308">Thanks for having a look @dweiss !
</comment><comment author="javanna" created="2014-04-07T14:59:20Z" id="39740794">I've disabled for now the replication over the different jvms (to be able to distribute the tests across them) due to reporting problems which need to be looked into. In the meantime the whole test suite will be run entirely on one of the jvms, as it used to be with the old runner too though.
</comment><comment author="s1monw" created="2014-04-07T15:02:12Z" id="39741308">LGTM let's get it in for now and take care of distribution across JVMs in a different issue!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo in the description for include_in_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5653</link><project id="" key="" /><description>I know this is uber-minor, but I was confused by the phrase "the raw field value to be copied". I assume "is" was supposed to be instead of "to"
</description><key id="30619887">5653</key><summary>Typo in the description for include_in_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels /><created>2014-04-01T17:32:16Z</created><updated>2014-07-16T21:46:56Z</updated><resolved>2014-04-02T10:03:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-02T10:03:42Z" id="39311846">Merged. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve upgrade docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5652</link><project id="" key="" /><description>Added upgrade.asciidoc and supporting links to README.textfile and setup.asciidoc.
</description><key id="30618537">5652</key><summary>Improve upgrade docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-04-01T17:14:28Z</created><updated>2014-06-20T13:59:10Z</updated><resolved>2014-04-10T02:26:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2014-04-04T21:05:38Z" id="39611387">Merged Clinton's comments and fixed the formatting.
</comment><comment author="seang-es" created="2014-04-07T17:13:41Z" id="39757054">Merged @bly2k comments
</comment><comment author="seang-es" created="2014-04-10T02:26:12Z" id="40037150">I don't know what's going on with this PR.  I merged it two days ago and it was fine.  I added some more comments and updates and pushed them to my branch, and now it's showing all commits that happened between the original push and now.  I'm going to try getting my branch back in sync with master, and applying the updates to a new PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved upgrade docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5651</link><project id="" key="" /><description /><key id="30613062">5651</key><summary>Improved upgrade docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-04-01T16:08:04Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-04-07T21:04:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove ScriptDocValues.EMPTY.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5650</link><project id="" key="" /><description>Instead the default implementation is used, but on top of empty
(Bytes|Long|Double|GeoPoint)Values. This makes sure there is no
inconsistency between documents depending on whether other documents in the
segment have values or not.

Close #5646
</description><key id="30612084">5650</key><summary>Remove ScriptDocValues.EMPTY.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T15:57:25Z</created><updated>2015-06-07T21:52:53Z</updated><resolved>2014-04-02T12:29:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T08:54:27Z" id="39304069">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Elasticsearch logs - prevent warning when using .percolator type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5649</link><project id="" key="" /><description>Seeing the following message in the logs after adding the percolator functionality to our app:

[2014-04-01 16:34:35,822][WARN ][index.mapper             ] [Thunderbird] [insight] Type [.percolate] contains a '.', it is recommended not to include it within a type name

I believe this warning should not apply to .percolator as it is recommended to do this in the documentation? http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html
</description><key id="30611811">5649</key><summary>Update Elasticsearch logs - prevent warning when using .percolator type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hackjoy</reporter><labels /><created>2014-04-01T15:54:32Z</created><updated>2014-04-03T10:51:42Z</updated><resolved>2014-04-03T10:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-01T22:56:30Z" id="39271002">@hackjoy 
when you create the percolator, the type name should be `.percolator` instead of `.percolate`, if you use `.percolator` it won't should that warning
</comment><comment author="hackjoy" created="2014-04-03T10:51:42Z" id="39437660">@kzwang 
Ok sure. I can see an instance in our app where both .percolate and .percolator are referenced in creating a type but I don't ever think the code to create the .percolate type is actually called. 

On checking whether the types exist I found the following:

$ curl -i -XHEAD localhost:9200/insight/.percolate
HTTP/1.1 404 Not Found

$ curl -i -XHEAD localhost:9200/insight/.percolator
HTTP/1.1 200 OK

I've made changes to remove all references to .percolate so i'll close this and re-open if I still see the warning. Cheers
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make customizing analyzers simpler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5648</link><project id="" key="" /><description>I've edited this issue so some of the comments may not make sense.  The edited description is below with the original after it.

Right now if you want to customize a builtin analyzer beyond a certain point you have to dig into Elasticsearch and Lucene source code to figure out how to rebuild it as a custom analyzer.  From there you can customize it but this is error prone.  I propose:

Create an endpoint that returns the json required to build any builtin analyzer as a custom analyzer.  Users could hit the endpoint and then manually customize the returned json or let code do the customizing.  Either way they have the confidence that the only changes being made are the ones they intend to make. 

Before edit for posterity:

Right now if you want to customize an analyzer you have to dig into Elasticsearch and Lucene source code to figure out what json you'd have to send Elasticsearch to build it.  Then you have to customize the json.  I propose:

Create an endpoint that returns the json required to build the analyzer as a custom analyzer.  The application could hit that endpoint, parse the json, add what it needs, and then send it back when creating analyzer.

I like making an endpoint because it should be pretty simple to test if not simple to implement.  You could spin up both the "custom" version of the analyzer that the endpoint would return and the normal version then bang random and non-random strings against them.

I prefer this option over some kind of hack on top of the analyzer configuration that allows users to add filters "in between" those that are already in the language analyzer because that sounds harder to work with and harder to test.  Giving the users the json ultimately gives them more power.

I'm happy to work on this if folks think it is the right way to go.
</description><key id="30610403">5648</key><summary>Make customizing analyzers simpler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-04-01T15:39:38Z</created><updated>2014-06-10T13:06:06Z</updated><resolved>2014-06-10T13:06:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-01T15:44:21Z" id="39221140">can you maybe provide a set of mock request / response to clarify I have a hard time to follow...
</comment><comment author="nik9000" created="2014-04-01T19:22:19Z" id="39247227">Sorry!  Something like this:

``` bash
curl localhost:9200/analyzer/standard?pretty
```

``` js
{
    "analyzer": {
        "standard": {
                        "type": "custom",
            "tokenizer": "standard",
            "filter": [
                "standard",
                "standard_lowercase"
            ]
        }
    },
    "filter": {
        "standard_lowercase": {
            "type": "lowercase"
        }
    }
}
```

``` bash
curl localhost:9200/analyzer/greek?pretty
```

``` js
{
    "analyzer": {
        "standard": {
                        "type": "custom",
            "tokenizer": "standard",
            "filter": [
                "greek_lowercase"
                "standard",
                "greek_stop",
                "greek_stem"
            ]
        }
    },
    "filter": {
        "greek_lowercase": {
            "type": "lowercase",
            "language": "greek"
        },
        "greek_stop": {
            "type": "stop",
            "stopwords": "whatever you have to put here but it might be a good idea to list all the words rather than a path so it is easier to edit them?"
        },
        "greek_stem": {
            "type": "stemmer",
            "language": "greek"
        }
    }
}
```
</comment><comment author="nik9000" created="2014-04-01T19:30:31Z" id="39248083">The idea is that I can request:

``` bash
curl localhost:9200/analyzer/greek?pretty
```

add in `word_break`, recursive merge that with the rest of my config, then submit it.

We get folks on the mailing list who are happy with the language analyzer but just want to tweak it a bit.  This provides a super convenient place for them to start.
</comment><comment author="synhershko" created="2014-04-01T19:53:31Z" id="39250745">@nik9000 if I get what you mean, you're looking to treat analyzers as documents sort to speak and have them defined globally?

I don't see a need in an API to add a token filter to an analyzer definition, but I definitely agree an easier way to add and manipulate new analyzers can come in handy. Currently if you want a globally available analyzer, it has to come from a plugin or the yml file, and then it cannot be changed.

Only remaining thing to consider is what happens when an analyzer changes - how do you handle reindexing?
</comment><comment author="nik9000" created="2014-04-01T20:19:04Z" id="39253413">@synhershko I was more thinking of having a known good description of defaults that users could pull and modify either by hand or by automatically.  That'd push the responsibility for things like reindexing to the user.  It'd also mean any customization would be a modification of the defaults at a fixed point in time rather than the current defaults.  This feels less surprising.

My ulterior motive is that I have to add sane `word_break` support to as many languages as I can and I want something to prove that I've rebuilt the language analyzers properly before I start modifying them.  If I push the job building and testing the rebuilt language analyzers to Elasticsearch (and plugins, now that you mention it) I'll be more confident that my modifications aren't breaking too much.  I've seen people with similar issues (but fewer target languages) on the mailing list.
</comment><comment author="synhershko" created="2014-04-01T20:25:22Z" id="39254075">@nik9000 problem is, when it comes to analyzers, there's no such thing as good known defaults. It all depends on your language, corpus and queries.

For me it does make sense to have an easier way to manipulate analyzers and token streams, and test them. I don't think ES needs to offer any defaults, but encourage people to assemble their own analysis chain - and this is where I second your suggestion.

If there was a way for me to easily clone, modify (token streams and token streams configs) and name an analyzer globally using just REST API, that'd be a winner feature.
</comment><comment author="nik9000" created="2014-04-01T20:50:32Z" id="39257069">&gt; @nik9000 problem is, when it comes to analyzers, there's no such thing as good known defaults. It all depends on your language, corpus and queries.

Maybe known state is a better way of putting it.  My idea is to give you an easy way to do an incremental change on top of a named analyzer.

Being able to create named analyzers would be pretty cool too but isn't what I was thinking of here.  I'm pretty sure modifying them and deleting them would require some extra thought too.
</comment><comment author="nik9000" created="2014-04-07T17:55:46Z" id="39761802">Any interest in this?
</comment><comment author="nik9000" created="2014-04-17T15:11:42Z" id="40724937">I just updated the description to make this more clear.

Also, to give an example of why writing the custom json is error prone, when I tried to do it for the Catalan analyzer I broke it twice initially.  Once I forgot to add elision and when I did add it it was configured with French articles rather then the ones for Catalan.

One of the benefits of this is that the "customized" analyzers are tested quickly and easily in a unit test in Elasticsearch.
</comment><comment author="clintongormley" created="2014-06-10T09:47:37Z" id="45593948">@nik9000 Yesterday I went through all of the language analyzers and updated the docs to show the equivalent `custom` analyzer.  Would this (simpler) approach be sufficient and obviate the need for an actual end point?
</comment><comment author="clintongormley" created="2014-06-10T09:47:45Z" id="45593959">Example: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer
</comment><comment author="nik9000" created="2014-06-10T12:36:45Z" id="45607517">Its probably good enough but its disappointing to me because I wanted to be able to programatically customize the analyzers.  Copying and pasting them is a maintenance nightmare.  Probably less important is that this strategy doesn't test that the analyzers stay in sync.  The endpoint had a test that would catch if it got out of date.

The documentation looks good though!  I'm glad to have it.
</comment><comment author="clintongormley" created="2014-06-10T12:47:26Z" id="45608562">I agree - it would be awesome if we could introspect analyzers properly.  Unfortunately your PR in #5814 essentially hardcodes them: docs as code.  This results in the same maintenance problems, except that we can't fix them in retrospect once a version is released (as opposed to the docs, which we can fix after the fact if we've missed something).

&gt;  The endpoint had a test that would catch if it got out of date.

Analyzers are difficult to test sufficiently without knowing what has changed.  Those tests could easily miss changes.

Your PR exposes a few filters which currently aren't exposed in ES - I think I'll add those as we should expose them.
</comment><comment author="nik9000" created="2014-06-10T13:06:06Z" id="45610411">&gt; Analyzers are difficult to test sufficiently without knowing what has changed. Those tests could easily miss changes.

Yeah, I don't imagine it'd be perfect.  I get the argument that if they are wrong changing them in the docs is faster.

Its going to be annoying to deal with it being in the docs rather then right there but I'll work around it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo context suggester: Require precision in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5647</link><project id="" key="" /><description>The default precision was way too exact and could lead people to
think that geo context suggestions are not working. This patch now
requires you to set the precision in the mapping, as elasticsearch itself
can never tell exactly, what the required precision for the users
suggestions are.

Closes #5621
</description><key id="30606601">5647</key><summary>Geo context suggester: Require precision in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T15:00:45Z</created><updated>2015-06-07T21:51:57Z</updated><resolved>2014-04-02T21:54:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T08:55:52Z" id="39304232">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ScriptDocValues.EMPTY doesn't implement `getValue`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5646</link><project id="" key="" /><description>This might not look like an issue since getting a single value on something that is empty makes no sense but it introduces an inconsistency between documents that have no value, depending on whether they are on a segment that has no value at all (which will use `ScriptDocValues.EMPTY`), or on a segment where at list one document has a value. In the latter case, `ScriptDocValues.Longs` (or `Doubles`) will be used and these classes implement `getValue` and return a default value (`0`) for documents with no value.

See https://github.com/elasticsearch/elasticsearch/pull/4684#issuecomment-39167864 for the original bug report.
</description><key id="30603524">5646</key><summary>ScriptDocValues.EMPTY doesn't implement `getValue`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T14:28:30Z</created><updated>2014-04-02T12:29:14Z</updated><resolved>2014-04-02T12:29:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T08:55:15Z" id="39304161">+1 to just remove the empty one!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible NPE in TribeNodeService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5645</link><project id="" key="" /><description>Closes #5643
</description><key id="30601538">5645</key><summary>Fix possible NPE in TribeNodeService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Tribe Node</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T14:06:56Z</created><updated>2015-06-07T21:53:29Z</updated><resolved>2014-04-01T14:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-01T14:12:42Z" id="39209661">LGTM, this should only need to be back ported to 1.x, since the 1.1 branch and below do not have the dropped indices logic
</comment><comment author="s1monw" created="2014-04-01T14:15:02Z" id="39209915">ok coole I will do that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Add ability to start nodes in async mode in TestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5644</link><project id="" key="" /><description>this will help speed up when starting multiple nodes in a test
</description><key id="30601268">5644</key><summary>[TEST] Add ability to start nodes in async mode in TestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T14:03:50Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-04-01T16:02:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-01T14:54:03Z" id="39214759">@s1monw improved the linearizability of nodes additions and added sync on startNode
</comment><comment author="s1monw" created="2014-04-01T15:16:14Z" id="39217596">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TribeNode throws NPE if index doesn't exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5643</link><project id="" key="" /><description>```
[2014-04-01 08:00:24,976][WARN ][tribe                    ] [Keith Kilham] failed to process [cluster event from t2, local-disco-receive(from master)]
java.lang.NullPointerException
    at org.elasticsearch.tribe.TribeService$TribeClusterStateListener$1.execute(TribeService.java:315)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</description><key id="30601066">5643</key><summary>TribeNode throws NPE if index doesn't exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T14:01:18Z</created><updated>2014-04-01T14:25:05Z</updated><resolved>2014-04-01T14:25:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>How to find maximum of minimal value of nested documents?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5642</link><project id="" key="" /><description>Here is mapping of product:

``` json
{
    "catalog_mar_31" : {
        "mappings" : {
            "PRODUCT" : {
                "properties" : {
                    "price" : {
                        "type" : "nested",
                        "include_in_parent" : true,
                        "include_in_root" : true,
                        "properties" : {
                            "id" : {
                                "type" : "string"
                            },
                            "en" : {
                                "type" : "double"
                            },
                            "de" : {
                                "type" : "double"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

price.id is identifier of discounts.

For example, we have regular (full) price, price for regular customers (80% of full price) and a product of a day (price equals to full price except one product):

|  | full price | regular customers | product of a day |
| --- | --- | --- | --- |
| product A | 100 | 80 | 100 |
| product B (day) | 200 | 160 | 130 |
| product C | 150 | 120 | 150 |

All prices are precalculated for fast sorting and filtering.
User see only the lowest price of product (full is strikethrough).

So here is my query for receiving documents from ice skating category with price filter and price sort for user that have two discounts "full_price" and "regular_cusomer":

``` json
{
    "query" : {
        "bool" : {
            "must" : [{
                    "term" : {
                        "category" : "ice_skating"
                    }
                }, {
                    "nested" : {
                        "filter" : {
                            "bool" : {
                                "must" : [{
                                        "range" : {
                                            "price.en" : {
                                                "lte" :  &lt;price_to&gt;
                                            }
                                        }
                                    }, {
                                        "terms" : {
                                            "price.id" : ["full_price", "regular_cusomer"]
                                        }
                                    }
                                ]
                            }
                        },
                        "path" : "price"
                    }
                }
            ],
            "must_not" : {
                "nested" : {
                    "filter" : {
                        "bool" : {
                            "must" : [{
                                    "range" : {
                                        "price.en" : {
                                            "lt" :  &lt;price_from&gt;
                                        }
                                    }
                                }, {
                                    "terms" : {
                                        "price.id" : ["full_price", "regular_cusomer"]
                                    }
                                }
                            ]
                        }
                    },
                    "path" : "price"
                }
            }
        }
    },
    "sort" : [{
            "price.en" : {
                "order" : "desc",
                "missing" : "_last",
                "mode" : "min",
                "nested_filter" : {
                    "terms" : {
                        "price.id" : ["full_price", "regular_cusomer"]
                    }
                }
            }
        }
    ],
    "facets" : {
        "price" : {
            "statistical" : {
                "field" : "price.en"
            },
            "facet_filter" : {
                "terms" : {
                    "price.id" : ["full_price", "regular_cusomer"]
                }
            },
            "nested" : "price"
        }
    }
}
```

&lt;price_from&gt; and &lt;price_to&gt; is green arrows:
http://forumbgz.ru/user/upload/file29943.png

So, the min and max result of statistical facet (red arrows) will be 80 - 200
But right answer is 80 - 160.

My question is how to find maximum of minimal value of nested documents?
I'm little bit stuck - can you help me with advice how to make that?

Sorry for bad English - I'm not native speaker. :confused: 
</description><key id="30597428">5642</key><summary>How to find maximum of minimal value of nested documents?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kamapcuc</reporter><labels /><created>2014-04-01T13:15:50Z</created><updated>2014-06-09T18:10:59Z</updated><resolved>2014-06-09T18:10:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Kamapcuc" created="2014-06-09T17:41:30Z" id="45519906">anyone alive here?
</comment><comment author="clintongormley" created="2014-06-09T18:10:59Z" id="45523665">@Kamapcuc Please ask questions like these in the forums, not in the github issues list
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Randomize JVM heapsize between 512 and 1024m</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5641</link><project id="" key="" /><description /><key id="30597395">5641</key><summary>[BUILD] Randomize JVM heapsize between 512 and 1024m</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-01T13:15:23Z</created><updated>2014-08-26T07:44:16Z</updated><resolved>2014-08-22T07:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T11:44:20Z" id="52909029">@mrsolo can you take a look at this I just update the PR 
</comment><comment author="mrsolo" created="2014-08-21T17:45:20Z" id="52955898">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Speedups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5640</link><project id="" key="" /><description>the commits in the PR speed up tests by 100% on slower hardward and about 80% on highly parallel HW. All tests pass. I think we should get that in quickly.
</description><key id="30586683">5640</key><summary>Test Speedups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-04-01T10:18:29Z</created><updated>2014-07-13T09:29:45Z</updated><resolved>2014-04-01T12:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-01T10:48:51Z" id="39192252">@s1monw seems lots of classes has duplicate `import org.elasticsearch.test.ElasticsearchIntegrationTest;`?
</comment><comment author="s1monw" created="2014-04-01T11:27:17Z" id="39194792">@kzwang yeah leftovers from refactorings....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5639</link><project id="" key="" /><description /><key id="30586108">5639</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gabriel-tessier</reporter><labels /><created>2014-04-01T10:09:18Z</created><updated>2014-07-16T21:46:59Z</updated><resolved>2014-05-06T13:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-04-07T22:06:09Z" id="39789844">Thanks for this! Can you sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this?
</comment><comment author="clintongormley" created="2014-05-06T13:55:25Z" id="42304572">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5638</link><project id="" key="" /><description /><key id="30582424">5638</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gabriel-tessier</reporter><labels><label>docs</label></labels><created>2014-04-01T09:14:34Z</created><updated>2014-07-16T21:46:59Z</updated><resolved>2014-04-07T07:24:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T16:10:54Z" id="39582367">Hi @gabriel-tessier thanks for your PR, could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge it in?
</comment><comment author="gabriel-tessier" created="2014-04-07T01:19:39Z" id="39689532">I sign the CLA but the link you provide is :
https://github.com/elasticsearch/elasticsearch/pull/elasticsearch.org/contributor-agreement/
instead of http://elasticsearch.org/contributor-agreement/
If you click on the link you have a 404 (I talk about the first link).

Thanks.
</comment><comment author="javanna" created="2014-04-07T07:24:42Z" id="39702278">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add predefined search templates endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5637</link><project id="" key="" /><description>New feature #5122 allows storing query templates on local nodes.

We should add some endpoints to allow:
## Put/Update a search template:

```
PUT _search/template/templatename
{
    "template" : {
        "content" : {
            "query": {
                "match_{{te_1}}": {}
            },
            "fac{{te_2}}": {
            "list-id": {
                "terms": {
                    "field": "list-{{te_3}}",
                    "size": 10
                }
            }
            }, 
            "fields": ["subject", "from"]
        }
    }
}
```
## Get a search template:

```
GET _search/template/templatename
```
## Remove a search template:

```
DELETE _search/template/templatename
```
## Run the template:

```
GET /_search/template
{
    "template": "templatename" ,
    "params": {
        "te_1" : "all",
        "te_2" : "ets",
        "te_3" : "id"
    }
}
```
## Search for all templates:

```
GET _search/template
GET _search/template/_all
GET _search/template/*
```
</description><key id="30581013">5637</key><summary>Add predefined search templates endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-04-01T08:50:29Z</created><updated>2014-07-16T12:03:24Z</updated><resolved>2014-07-14T14:40:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-01T10:27:43Z" id="39190811">just an idea, can a template be `index/type/id` then we can just use an arbitrary index for storing the templates as a start and it's replicated, versioned and the infra is tested? The search can then just do a get call and get the template?
</comment><comment author="dadoonet" created="2014-04-01T10:49:53Z" id="39192314">w00t! I love it @s1monw. Because I was not confortable of having this in cluster state! :)

I think we will need to cache the template if we don't want to load/compile it on each call.
</comment><comment author="GaelTadh" created="2014-07-14T14:40:57Z" id="48907920">Closed by https://github.com/elasticsearch/elasticsearch/commit/e79b7086de26ece61edaca74fcf7dc99a11de486
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update JNA to 4.1.0, properly warn on error, hint at noexec mount</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5636</link><project id="" key="" /><description>See issue #5493
</description><key id="30578552">5636</key><summary>Update JNA to 4.1.0, properly warn on error, hint at noexec mount</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T08:05:52Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-05-02T10:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-01T08:33:01Z" id="39181852">maybe put this in the official documentation as well? maybe here? http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory
</comment><comment author="hhoffstaette" created="2014-04-01T08:43:24Z" id="39182651">@spinscale I was wondering where to put an additional comment and looked at the config file, but the docs are better. I guess the phrase about setting the logging to DEBUG can just be removed with this change. I'd also like to add something to the effect that mlockall should be considered a last resort, since it is incredibly hard on the system and no longer necessary with recent kernels (see my Quora answer referenced in the original issue).
</comment><comment author="hhoffstaette" created="2014-04-01T10:04:35Z" id="39189139">Added some bits of documentation explaining swappiness, why it may or may not work as expected and that mlockall is the fallback for older (broken) kernels.
</comment><comment author="spinscale" created="2014-05-02T10:00:41Z" id="42012039">closed by https://github.com/elasticsearch/elasticsearch/commit/f5c9bf6f0f91cf2cd6afdc206a125010115665b8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5635</link><project id="" key="" /><description>- Removed XTermsFilter fixed in LUCENE-5502
- Switched back to automaton queries that caused failures due to LUCENE-5532
- Fixed Highlight test that has different results due to LUCENE-5538
</description><key id="30578410">5635</key><summary>Upgrade to Lucene 4.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-04-01T08:02:46Z</created><updated>2015-08-25T13:26:01Z</updated><resolved>2014-04-01T21:56:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-01T08:57:24Z" id="39183683">+1
</comment><comment author="imotov" created="2014-04-01T11:20:58Z" id="39194369">Would it make sense to remove XTermsFilter from [forbidden api check](https://github.com/elasticsearch/elasticsearch/blob/upgrade/lucene_4_7_1/pom.xml#L1052) as well?
</comment><comment author="s1monw" created="2014-04-01T11:56:38Z" id="39196855">@jpountz @imotov I pushed a new commit
</comment><comment author="imotov" created="2014-04-01T12:15:14Z" id="39198182">+1
</comment><comment author="jpountz" created="2014-04-01T12:27:44Z" id="39199139">Very good point Igor, thanks for cathing this!
</comment><comment author="s1monw" created="2014-04-01T21:58:03Z" id="39265453">I wonder if we should move this to `1.1` as well and move to `4.7.1` on that branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support bounding box aggregation on geo_shape/geo_point data types.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5634</link><project id="" key="" /><description>For applications that index spatial data it would be very useful to be able to request an aggregation that would return the bounding box (extent) of the matching hits.  A typical use case would be to zoom a map to area containing matching hits. 
</description><key id="30559389">5634</key><summary>Support bounding box aggregation on geo_shape/geo_point data types.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">yeroc</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T23:32:07Z</created><updated>2015-06-06T18:33:39Z</updated><resolved>2014-06-03T15:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fail the engine/shard when refresh failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5633</link><project id="" key="" /><description>When refresh failed, it would fail due to a serious issue in the shard (mainly corrupted index). Fail the engine in that cause, which will cause the shard to fail. The reason why its not only on CorruptedIndex failed is that any type of failure seems to be relevant here to fail the shard
</description><key id="30559060">5633</key><summary>Fail the engine/shard when refresh failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Engine</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T23:25:39Z</created><updated>2015-06-07T14:56:06Z</updated><resolved>2014-04-01T12:32:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-01T08:06:44Z" id="39180013">+1 for this - I have a random exception test that works around this I guess we can beef it up again :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add test for ElasticSearchPostingsFormat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5632</link><project id="" key="" /><description>We have a custom postingsformat at the moment, and Lucene has a useful base test class to test these.
</description><key id="30551707">5632</key><summary>add test for ElasticSearchPostingsFormat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T21:33:26Z</created><updated>2015-06-08T15:17:15Z</updated><resolved>2014-04-01T13:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T22:27:19Z" id="39150138">LGTM
</comment><comment author="s1monw" created="2014-04-01T08:40:46Z" id="39182426">cool Lets get this in! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mock Transport: Allow to simulate network failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5631</link><project id="" key="" /><description>An infrastructure that allows to simulate different network topologies failures, including 2 basic ones in failure to send requests, and unresponsive nodes
</description><key id="30545119">5631</key><summary>Mock Transport: Allow to simulate network failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T20:14:26Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-03-31T20:49:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-01T06:04:08Z" id="39173497">Left a comment other then that it looks good.

Maybe we should also have a few helper methods in TestCluster that delegate to MockTransportService? For example:

``` java
    public void addFailToSendNoConnectRule(String fromNode, String toNode) {
        ....
    }

    public void clearNoConnectRule(String fromNode, String toNode) {
        ...
    }
```

Internally these methods would fetch the MockTransportService from the right node and translate the node name into the DiscoveryNode. This would be nice since it should reduce some boiler plate code.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] added ExternalTestCluster that allows to run tests against an external cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5630</link><project id="" key="" /><description>All the ordinary test operations happen based on the `ImmutableTestCluster` base class and are executed via transport client. Will be used especially for the REST tests once migrated to the standard randomized runner.

Added new httpAddresses method to ImmutableTestCluster to be able to retrieve the http addresses to connect to for the REST tests. Both versions will look inside the cluster to figure out which nodes are available for http calls and their addresses.

The external cluster is used as global cluster if the `tests.cluster` system property is available. The property needs to contain a comma separated list of available elasticsearch nodes that will be used to connect to the cluster (e.g. localhost:9300,localhost:9301).

Only a subset of the integration tests can currently be run successfully against the external cluster, for more precision the ones that don't modify the cluster layout (don't require `cluster()` functionalities but rely only on `immutableCluster()`). Also at least two data nodes are required otherwise the `ensureGreen` calls cannot succeed.
</description><key id="30543854">5630</key><summary>[TEST] added ExternalTestCluster that allows to run tests against an external cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T19:57:49Z</created><updated>2014-07-16T21:47:02Z</updated><resolved>2014-04-01T09:47:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-01T08:48:08Z" id="39183011">I left some small comments but this looks awesome
</comment><comment author="s1monw" created="2014-04-01T09:20:45Z" id="39185639">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not propagate errors from onResult to onFailure.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5629</link><project id="" key="" /><description>The way that SearchServiceTransportAction executes actions on a local node
today would propagate exceptions thrown in onResult to onFailure.

I don't have seen it happening in practice, I'd just like to make sure the
described scenario can not happen.
</description><key id="30539529">5629</key><summary>Do not propagate errors from onResult to onFailure.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T19:03:50Z</created><updated>2015-06-07T21:53:39Z</updated><resolved>2014-04-01T08:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T19:32:58Z" id="39131009">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Blob store-based gateways shouldn't serialize transient metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5628</link><project id="" key="" /><description>Fixes #5615

I would like to merge the first commit b0172c0 to 1.1, 1.x and master since it looks like a good change anyway. The second commit will go only to 1.1 because 1.x and master no longer have blob store based gateways (S3, shared filesystem, etc). 
</description><key id="30539236">5628</key><summary>Blob store-based gateways shouldn't serialize transient metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-03-31T19:00:26Z</created><updated>2014-06-26T10:21:36Z</updated><resolved>2014-04-14T19:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Log uncaught exceptions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5627</link><project id="" key="" /><description>By default, `ThreadPoolExecutor` just ignores tasks that throw exceptions. This
change makes sure that any uncaught exception would be logged as an error.
</description><key id="30538415">5627</key><summary>Log uncaught exceptions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-03-31T18:50:49Z</created><updated>2014-07-16T21:47:03Z</updated><resolved>2014-04-02T09:38:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T19:09:07Z" id="39128458">maybe the level should be `warn` here?
</comment><comment author="jpountz" created="2014-03-31T19:17:31Z" id="39129357">I picked `error` because such uncaught exceptions would mean that something is wrong and needs to be fixed, since exceptions should be caught and handled by the `Runnable`s themselves? On the other hand, I would expect a warning to mean that the application hit something unexpected but can handle it?
</comment><comment author="s1monw" created="2014-04-02T09:00:59Z" id="39304792">At the end of the day the uncaught exception handler would take are of this though I think we don't ncessarily need that to be `error` I think `debug` would be find just to make sure we get a reasonable message in the tests where the handler might swallow?
</comment><comment author="jpountz" created="2014-04-02T09:22:12Z" id="39307067">The uncaught exception handler is only used in tests though. So it might be useful to have this message in user logs in case our tests did not catch it?
</comment><comment author="s1monw" created="2014-04-02T09:23:04Z" id="39307152">I think the default one is going to print it to STD error no? But yeah I agree....
</comment><comment author="jpountz" created="2014-04-02T09:38:35Z" id="39309148">Good point, closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to parse lat/lon as strings and coerce them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5626</link><project id="" key="" /><description>In order to be more failsafe parsing GeoPoints can support
lat/lon as strings and coerce them. Added support and test for this.
</description><key id="30535661">5626</key><summary>Allow to parse lat/lon as strings and coerce them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T18:17:45Z</created><updated>2015-06-07T14:56:23Z</updated><resolved>2014-04-01T17:27:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-31T18:28:10Z" id="39123650">Also needs to be fixed here:

```
[test][1], node[-PhRAYloSDezUgMBB--99w], [P], s[STARTED]: Failed to execute [[[]], suggestSource[{"result":{"text":"hote","completion":{"context":{"location":{"lat":"52.22","lon":"4.53"}},"field":"suggest_geo"}}}]]
org.elasticsearch.ElasticsearchException: failed to execute suggest
at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:171)
at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:61)
at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$2.run(TransportBroadcastOperationAction.java:226)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.ElasticsearchParseException: latitude must be a number
at org.elasticsearch.search.suggest.context.GeolocationContextMapping.parseQuery(GeolocationContextMapping.java:331)
at org.elasticsearch.search.suggest.context.GeolocationContextMapping.parseQuery(GeolocationContextMapping.java:65)
at org.elasticsearch.search.suggest.context.ContextMapping$ContextQuery.parseQueries(ContextMapping.java:296)
at org.elasticsearch.search.suggest.completion.CompletionSuggestParser.parse(CompletionSuggestParser.java:112)
at org.elasticsearch.search.suggest.SuggestParseElement.parseInternal(SuggestParseElement.java:90)
at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:165)
... 5 more
```
</comment><comment author="spinscale" created="2014-04-01T07:42:19Z" id="39178491">updated, can you check again?
</comment><comment author="clintongormley" created="2014-04-01T10:55:41Z" id="39192687">Tests now passing ++
</comment><comment author="spinscale" created="2014-04-01T11:42:02Z" id="39195791">I think we could also remove the duplicated code here. The reason it exists is to support these two formats in the query

``` JSON
POST services/_suggest
{
    "suggest" : {
        "text" : "m",
        "completion" : {
            "field" : "suggest_field",
            "size": 10,
            "context": {
                "location": {
                    "value": {
                        "lat": 0,
                        "lon": 0
                    },
                    "precision": "1km"
                }
            }
        }
    }
}
```

``` JSON
POST services/_suggest
{
    "suggest" : {
        "text" : "m",
        "completion" : {
            "field" : "suggest_field",
            "size": 10,
            "context": {
                "location": {
                        "lat": 0,
                        "lon": 0,
                        "precision": "1km"
                }
            }
        }
    }
}
```

If we only want to support the first format, we could remove the duplicate geo point parsing... the second one is shorter and more handy through
</comment><comment author="spinscale" created="2014-04-01T15:30:53Z" id="39219422">updated the PR to use a switch statement
</comment><comment author="s1monw" created="2014-04-01T15:31:37Z" id="39219499">LGTM
</comment><comment author="s1monw" created="2014-04-11T16:26:29Z" id="40222593">seems like we have to backport this to 1.0 branch as well @spinscale should we?
</comment><comment author="spinscale" created="2014-04-11T16:29:43Z" id="40222953">+1, will do
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added StreamOutput.writeOptionalStreamableOnOrAfter()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5625</link><project id="" key="" /><description>This method supports to specify a version, which is going to be checked
in order to prevent the longer if-statements usually used in the
writeTo() methods.
</description><key id="30535654">5625</key><summary>Added StreamOutput.writeOptionalStreamableOnOrAfter()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-03-31T18:17:35Z</created><updated>2014-08-22T08:06:56Z</updated><resolved>2014-08-22T08:06:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-22T08:06:31Z" id="53033973">Won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add cluster name to state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5624</link><project id="" key="" /><description>see #5622
</description><key id="30526225">5624</key><summary>Add cluster name to state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T16:18:21Z</created><updated>2015-06-07T21:55:33Z</updated><resolved>2014-03-31T18:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T17:43:39Z" id="39118192">left one comment, other than that, LGTM
</comment><comment author="s1monw" created="2014-03-31T18:09:35Z" id="39121414">man good catch... I pushed a new commit. 
</comment><comment author="kimchy" created="2014-03-31T18:10:29Z" id="39121536">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix mapping creation on bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5623</link><project id="" key="" /><description>When a bulk request triggers an index creation a mapping might not be
created. The reason is that when indexing documents in a bulk,
an indexing operation might fail due to a shard not yet being
started. The mapping service, however, might already
have the mapping but the mapping update is never issued to the master,
even on subsequent indexing of documents.

Instead, the mapping must be propagated to master even if the
indexing fails due to a shard not being started.
</description><key id="30526089">5623</key><summary>Fix mapping creation on bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T16:16:33Z</created><updated>2015-06-07T21:55:04Z</updated><resolved>2014-04-02T12:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T17:45:53Z" id="39118485">LGTM, wonderful catch!
</comment><comment author="brwe" created="2014-04-02T09:24:43Z" id="39307346">Thanks for the comments! I updated the commits accordingly. I will push to master 1.x an 1.1. Should I also try and push to 1.0 and 0.90?
</comment><comment author="s1monw" created="2014-04-02T11:33:25Z" id="39319680">change LGTM +1 to squash and push 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clusterstate misses the cluster name as it's identifier</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5622</link><project id="" key="" /><description> Today the cluster_state is not associated with the cluster_name which is odd since it's pretty much it's only valid identifier. Any node can send a cluster state to another node today even if it's not the same cluster. These situations can happen rarely during tests or even in the wild by accident. The problem can occur if multiple nodes run on the same machine and they join multiple clusters. If node `A` from cluster  `cluster_name=a` shuts down while the other node `B` starts up with `cluster_name=b` and this node happens to get the same port on that physical host a third node that still thinks `B` is listening on that port might send a new cluster state. This can cause weird problems and we should discard the cluster state if the cluster names don't match
</description><key id="30525803">5622</key><summary>Clusterstate misses the cluster name as it's identifier</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T16:13:26Z</created><updated>2014-03-31T18:32:59Z</updated><resolved>2014-03-31T18:32:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geo Context Suggester: Refine precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5621</link><project id="" key="" /><description>This is a followup of #5525 and #5596

The current default precision of the context suggester is 12, representing the length of the geohash being twelve chars long. 

This means, that without specifying the precision in the mapping a default geo context based query will its own geo cell and its neighbours as the context by default. This means it will cover an area of about 5x5 cm for suggestions (if I calculated it correctly)... which seems not to be a reasonable default.

We should either change this or force setting a precision in the mapping, so the user has to think about the precision.
</description><key id="30514835">5621</key><summary>Geo Context Suggester: Refine precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T14:11:29Z</created><updated>2015-06-07T22:15:28Z</updated><resolved>2014-04-02T21:54:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-31T14:14:25Z" id="39092734">Agreed. Perhaps ~5km == precision 5?
</comment><comment author="clintongormley" created="2014-03-31T14:16:17Z" id="39092967">I'd also be OK with requiring a precision - that way it forces the user to think about it.
</comment><comment author="spinscale" created="2014-03-31T14:25:09Z" id="39094041">forgot to mention that: You can define the precision as an int, which is the length of the geohash or as an string, which will be one of our distanceunits then... that is working now already... maybe it makes sense to drop the geohash length precision anyway, as noone thinks in those terms?
</comment><comment author="clintongormley" created="2014-03-31T14:33:13Z" id="39094970">I'd keep it - helps to explain why, if you say "1km" you don't get exactly 1km.
</comment><comment author="spinscale" created="2014-03-31T14:37:04Z" id="39095437">+1 on my side for forcing a precision - we really cant tell
</comment><comment author="s1monw" created="2014-03-31T14:37:38Z" id="39095514">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] introduced ImmutableTestCluster abstract base class for TestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5620</link><project id="" key="" /><description>The new base class contains all the immutable methods for a cluster, which will be extended by the future ExternalCluster impl that relies on an external cluster and won't be adding nodes etc. but only sending requests to an existing cluster whose layout never changes
</description><key id="30505756">5620</key><summary>[TEST] introduced ImmutableTestCluster abstract base class for TestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T11:55:18Z</created><updated>2014-07-16T21:47:04Z</updated><resolved>2014-03-31T12:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T12:27:56Z" id="39082277">this one look great to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made template endpoint compatible with search endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5619</link><project id="" key="" /><description>Before this the from/size parameters did not work.
Also updated the rest api spec definition file with all the query_string
parameters.

Fixes #5550
</description><key id="30503366">5619</key><summary>Made template endpoint compatible with search endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Search Templates</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T11:10:33Z</created><updated>2015-06-07T21:55:49Z</updated><resolved>2014-04-02T22:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T08:56:59Z" id="39304358">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>support `F` as false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5618</link><project id="" key="" /><description>according to document `F` should be treat as `false`
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#boolean

closes #2075
</description><key id="30495058">5618</key><summary>support `F` as false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-03-31T09:00:47Z</created><updated>2014-07-04T14:57:12Z</updated><resolved>2014-07-04T14:57:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-01T00:50:37Z" id="39159954">BTW, I think unknown values should throw an exception instead of tread as `true`, if user put some value they believe it will be treat as `false` but not handled in ES it will be very confusing.
</comment><comment author="pickypg" created="2014-04-01T01:02:26Z" id="39160587">Agree that it should be `false` or cause an exception.
</comment><comment author="clintongormley" created="2014-07-03T19:26:51Z" id="47973929">@imotov I remember you had some issues with boolean values in another ticket - what's your view on this one?
</comment><comment author="imotov" created="2014-07-03T19:29:28Z" id="47974207">@clintongormley it's tricky, especially from backward compatibility perspective. 
</comment><comment author="clintongormley" created="2014-07-04T14:57:12Z" id="48052484">Actually, rereading the docs, it doesn't say that `T` and `F` are accepted values, just that that is how they are stored internally.  Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation on big data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5617</link><project id="" key="" /><description>I have 200 million lines of data(about port scanning). I want ES to return those "ip" who open not only one port at the same time(order by count). But, considering the volume of data and very little docs have same value on "ip" field, obviously I get an out of memory error. Is there any way to finish my query mission.
</description><key id="30494397">5617</key><summary>Aggregation on big data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vircandy</reporter><labels /><created>2014-03-31T08:50:10Z</created><updated>2014-03-31T08:53:24Z</updated><resolved>2014-03-31T08:53:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-31T08:53:24Z" id="39066367">Please use the mailing list for your questions.
Github "issues" are for bugs and feature requests.

See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting with _geo_distance will throw ClassCastException exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5616</link><project id="" key="" /><description>created index with mapping:

  "mappings": {
    "city": {
      "properties": {
        "name": {
          "type": "string",
          "store": "yes",
          "index": "analyzed"
        },
        "location": {
          "type": "geo_point"
        }
      }
    }
  }

and search/sort with _geo_distance:

  "query": {
    "match_all": {}
  },
  "fields": "name",
  "sort": {
    "_geo_distance": {
      "location": [
        121.571079,
        25.049787
      ],
      "unit": "km"
    }
  }

will throw ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexGeoPointFieldData] exception.

Elasticsearch version: 1.1.0
</description><key id="30494170">5616</key><summary>Sorting with _geo_distance will throw ClassCastException exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ypyean</reporter><labels /><created>2014-03-31T08:46:23Z</created><updated>2014-04-01T05:32:40Z</updated><resolved>2014-04-01T05:19:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-31T11:30:59Z" id="39078207">do you search across several indices? can you check the mapping for each index, if it contains a `geo_point` for the `location` field?
</comment><comment author="ypyean" created="2014-04-01T02:56:28Z" id="39166182">No, I searched one index only and it contains geo_point type for location field. It is strange that I tried again, it worked fine first, then I tried to search with query_string, it threw ClassCastException exception, and then I back to try to search with match_all, it threw ClassCastException again. I tried with the same procedures that Elasticsearch Server book mentioned:

create mapping:

{
"mappings" : {
"poi" : {
"properties" : {
"name" : { "type" : "string" },
"location" : { "type" : "geo_point" }
}
}
}
}

post data:

{ "name" : "New York", "location" : "40.664167, -73.938611" }

and then query:

{
"query" : {
"matchAll" : {}
},
"sort" : [{
"_geo_distance" : {
"location" : "48.8567, 2.3508",
"unit" : "km"
}
}]
}
</comment><comment author="ypyean" created="2014-04-01T03:41:52Z" id="39168075">Now it can't work anymore, even if I re-install elasticsearch (empty) and try again. It always throws ClassCastException.
</comment><comment author="ypyean" created="2014-04-01T04:41:28Z" id="39170340">Hi, spinscale,

I think I know the reason why it throws ClassCastException. I check the location type (_mapping) after post data, it shows the location type is string. However, I have defined the location type to geo_point. It should be the problem for posting data. Elasticsearch will change the location type from geo_point to string. How can I solve this problem?
</comment><comment author="kzwang" created="2014-04-01T04:58:12Z" id="39170988">@ypyean how did you create the mapping? I tried on 1.1.0 and it works fine.

``` sh
# create index with mapping
curl -XPOST `http://localhost:9200/test` -d '
{
  "mappings": {
    "poi": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
}
'


# check mapping
curl -XGET 'http://localhost:9200/test/poi/_mapping'

# post data
curl -XPOST 'http://localhost:9200/test/poi/1' -d '
{
  "name": "New York",
  "location": "40.664167, -73.938611"
}
'

# search
curl -XPOST 'http://localhost:9200/test/poi/_search' -d '
{
  "query": {
    "matchAll": {}
  },
  "sort": [
    {
      "_geo_distance": {
        "location": "48.8567, 2.3508",
        "unit": "km"
      }
    }
  ]
}
'
```
</comment><comment author="ypyean" created="2014-04-01T05:19:51Z" id="39171772">@kzwang Thank you for your try. It works using your method to create mapping (create index and mapping at the same time). I created empty index first and then created the mapping for it. But this method seems can't work properly. Thanks again.
</comment><comment author="kzwang" created="2014-04-01T05:25:56Z" id="39171989">if you use the put mapping api, you should just put the content of the `mapping` only
i.e.

``` sh
curl -XPOST `http://localhost:9200/test/poi/_mapping` -d '
{
    "poi": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
'
```
</comment><comment author="ypyean" created="2014-04-01T05:29:37Z" id="39172132">@kzwang Yes, it works now. Thanks for your help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SnapshotMetaData.fromXContent does not match toXContent, but throws an Exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5615</link><project id="" key="" /><description>Is there any particular reason why `fromXContent` should raise an Exception, while `toXContent` serializes the information?

Source at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java#L358

This seems to cause problems for certain gateways, such as https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/68
</description><key id="30492689">5615</key><summary>SnapshotMetaData.fromXContent does not match toXContent, but throws an Exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nkvoll</reporter><labels><label>bug</label><label>v1.1.1</label></labels><created>2014-03-31T08:21:39Z</created><updated>2014-04-14T22:43:33Z</updated><resolved>2014-04-14T22:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-31T18:56:10Z" id="39126978">@nkvoll Thanks for the report. SnapshotMetaData doesn't implement `fromXContent` because it's not supposed to be serialized as part of persistent cluster state and therefore shouldn't be deserializable when cluster state is restored. I am working on a fix for this issue.
</comment><comment author="s1monw" created="2014-04-14T08:55:16Z" id="40345251">LGTM
</comment><comment author="imotov" created="2014-04-14T22:43:33Z" id="40427150">Fixed by #5628 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5614</link><project id="" key="" /><description>- removed an abstraction layer that handles the values source (consolidated values source with field data source)
- better handling of value parser/formatter in range &amp; histogram aggs
- the buckets key will now be shown by default in range agg
</description><key id="30485240">5614</key><summary>Aggregations cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T04:53:09Z</created><updated>2015-06-07T14:56:34Z</updated><resolved>2014-03-31T16:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-31T08:21:00Z" id="39063916">Is the change in the range agg backward compatible?

I spotted a lines where you reverted the migration to the diamond operator but otherwise I really really like this change!
</comment><comment author="uboness" created="2014-03-31T15:06:06Z" id="39099111">@jpountz the change in the range agg should be bwc... the main thing there is the addition of the `key` to the bucket by default (so it's more of an extension of the api rather than a break)
</comment><comment author="uboness" created="2014-03-31T15:06:42Z" id="39099194">and sure... I'll give you back your diamonds
</comment><comment author="jpountz" created="2014-03-31T15:12:25Z" id="39099902">Perfect!
</comment><comment author="uboness" created="2014-03-31T16:19:42Z" id="39108408">closed by d6636fc50cd7e5eedf042732ebae6e43e2ebc866
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing questionable PNRG behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5613</link><project id="" key="" /><description>Closes #5454 and #5578

This strengthens and simplifies the PNRG used by `random_score` by more closely mirroring the `Random.nextFloat()` method, rather than a mix of that, `nextInt` and `nextDouble`. The `docBase` or `docId` are no longer used as they were biasing the result (particularly if it was `0`, which consistently made it the highest scoring result in tests), which partially defeats the purpose of random scoring.
</description><key id="30484866">5613</key><summary>Fixing questionable PNRG behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T04:37:53Z</created><updated>2015-06-07T22:00:08Z</updated><resolved>2014-04-28T17:46:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-04-01T10:51:19Z" id="39192401">well, the reason why we added the doc id is to have better support for consistent pagination, so it guarantees consistent random number generation on a consistent point of view over the indexes.
</comment><comment author="pickypg" created="2014-04-01T16:12:00Z" id="39224625">But, if you are allowing multiple indexes to invoke it, then isn't it going
to inherently be inconsistent because you will possibly not pick up where
you left off, thereby restarting further down the seed?

Personally, I think that having only the seed gives it a more consistent
flow, and the introducing the `docBase + docId` just adds the potential for
a strong bias in the case of `0`, which must be substituted somehow, and
incidental bias thereafter, generally increasing as the sum grows as it
increases the likelyhood that the `nextFloat`'s former xor operation (`rand
^ seed`) will have a much larger or much smaller number given a seed with
similar bits. I definitely noticed this behavior while adding the unit
tests, which led me to my current "fix".

On Tue, Apr 1, 2014 at 6:51 AM, uboness notifications@github.com wrote:

&gt; well, the reason why we added the doc id is to have better support for
&gt; consistent pagination, so it guarantees consistent random number generation
&gt; on a consistent point of view over the indexes.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/5613#issuecomment-39192401
&gt; .
</comment><comment author="pickypg" created="2014-04-01T17:08:49Z" id="39231442">Thinking it through some more, the `docId` _does_ provide something if two indexes invoke it at the same time since there is no synchronization of the `seed` update (thus returning the same value for them both without it). However, as long as they are separate invocations, then it's still a random result to both (ironically even if it repeats for one of them) without it, and I am still not convinced that the bias of the `docId` is superior.

Of course, I am not an expert on random number theory, so take that with a grain of salt.
</comment><comment author="uboness" created="2014-04-01T20:56:28Z" id="39257777">yes, the idea initially was the be consistent with pagination across indices (where the starts at 0 for each index). But thinking about it more, indeed there are too many "if"s here... we can remove the id and stick to the normal PRNG implementation, and who ever wants consistent pagination can just use seed+scroll.

btw, the seed is already biased here as the original seed that the user sends (or the default current time millis) is merged with the shard id (which is a must so each shard will generate different random number per doc)

I'll work on your PR, thx!
</comment><comment author="pickypg" created="2014-04-01T21:03:06Z" id="39258588">Works for me. The shard id modification of the `seed` makes perfect sense because otherwise there will be "unexpected" collisions within the merged results.
</comment><comment author="s1monw" created="2014-04-02T09:05:21Z" id="39305263">@uboness  I labeled this issue and assigned it to you - please feel free to re-label
</comment><comment author="uboness" created="2014-04-02T09:09:02Z" id="39305628">thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup Rest Response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5612</link><project id="" key="" /><description>simplify rest response class hierarchy, by using BytesReference for content, and handling JSONP internally in the respective channel that sends the response.
Also, handle the future case where bytes might be releasable, when we start to potentially recycle bytes output stream.
</description><key id="30479972">5612</key><summary>Cleanup Rest Response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-31T01:24:46Z</created><updated>2015-06-06T17:02:59Z</updated><resolved>2014-03-31T15:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T02:09:22Z" id="39048618">Note, this change breaks backward comp. for rest transport plugins (Thrift and Wares), but we will&#160;need to break it anyhow once the next refactoring will come in with introducing recycled responses, and this forms the basis for it.
</comment><comment author="uboness" created="2014-03-31T02:14:09Z" id="39048787">LGTM
</comment><comment author="s1monw" created="2014-03-31T10:41:24Z" id="39074852">I left a bunch or responses
</comment><comment author="kimchy" created="2014-03-31T15:39:31Z" id="39103359">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>all nodes return 503 when minimum_master_nodes/recover_after_nodes set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5611</link><project id="" key="" /><description>We have 16 node cluster on 0.90.5. We built a new cluster for 1.0.1 (yes, we will upgrade to 1.1.0 soon) but we experience this problem that I would like help with. All 16 nodes are identical and all are master eligible.

In our 0.90.5 cluster, we had it configured as:

``` yaml
  discovery.zen.minimum_master_nodes: 9
  gateway.expected_nodes: 16
  gateway.recover_after_nodes: 12
```

But when we built the 1.0.1 cluster with the same settings, all the nodes return 503 when upon startup after checking status http://localhost:9200.

Had to turn that down as below to get it working and return 200 status as usual.

``` yaml
  discovery.zen.minimum_master_nodes: 1
  gateway.expected_nodes: 16
  gateway.recover_after_nodes: 1
```

Full elasticsearch yaml configs for both clusters are: https://gist.github.com/ppat/9791741

This (https://github.com/elasticsearch/elasticsearch/issues/4007) talked about renaming `minimum_master_nodes` but from the last comment it sounds like it was decided against doing that and [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election) still refers to it as it was named in 0.90.5, so I'm assuming it didn't change and this is a real issue. 
</description><key id="30476032">5611</key><summary>all nodes return 503 when minimum_master_nodes/recover_after_nodes set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppat</reporter><labels /><created>2014-03-30T22:09:04Z</created><updated>2014-03-31T20:29:30Z</updated><resolved>2014-03-31T20:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppat" created="2014-03-31T17:18:37Z" id="39115175">@clintongormley or @s1monw - any ideas on this?
</comment><comment author="clintongormley" created="2014-03-31T17:21:36Z" id="39115511">are you sure that your nodes actually saw each other initially?  what was in your logs?  can you reproduce this?
</comment><comment author="ppat" created="2014-03-31T20:28:43Z" id="39137409">@clintongormley thanks for the response. We were able to reproduce it 1.0.1 but once we upgraded 1.1.0 it went away. I see that https://github.com/elasticsearch/elasticsearch/pull/5413 and https://github.com/elasticsearch/elasticsearch/pull/5440 got fixed in 1.1.0. Maybe they were related. Either way, we're good. Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update percolate.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5610</link><project id="" key="" /><description>fix typo
</description><key id="30472035">5610</key><summary>Update percolate.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">scottwilkerson</reporter><labels /><created>2014-03-30T19:04:03Z</created><updated>2014-07-16T21:47:06Z</updated><resolved>2014-04-15T14:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-02T09:08:40Z" id="39305592">hey scott I know it's  a small fix but can you sign the CLA?
</comment><comment author="scottwilkerson" created="2014-04-14T12:58:37Z" id="40362361">Sorry for the delay, it has been signed...
</comment><comment author="clintongormley" created="2014-04-15T14:02:06Z" id="40484598">Merged. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix visibility in buffered translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5609</link><project id="" key="" /><description>- fix visiblity of last written position in translog
- while there, make sure to properly propagate the exception from sync()
</description><key id="30470375">5609</key><summary>Fix visibility in buffered translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-30T17:53:32Z</created><updated>2015-06-07T14:56:42Z</updated><resolved>2014-03-31T13:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T13:25:44Z" id="39087434">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations; Support bucket-leve calculated aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5608</link><project id="" key="" /><description>There are many cases where a metrics is based on one or more aggregated values within the same bucket. For example if you need to calculate an effectiveness rate of product development as total amount generated by a product sales divided by all the money invested into the product and group it by product, FY etc.

I would like to be able to define an aggregation as a "calculated"
"Calculated" aggregations would be evaluated last after all real ones and based on values of the real ones. 

You could go even further and have an expression language working on against aggregate level
</description><key id="30468858">5608</key><summary>Aggregations; Support bucket-leve calculated aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2014-03-30T16:43:05Z</created><updated>2014-11-13T12:05:03Z</updated><resolved>2014-11-13T12:05:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-30T20:35:39Z" id="39038662">absolutely, we have it on the roadmap for aggs, probably this year
</comment><comment author="quiiver" created="2014-11-12T22:04:17Z" id="62803382">Is there an ETA on this @uboness ?
</comment><comment author="clintongormley" created="2014-11-13T12:05:03Z" id="62880874">Closing in favour of #8110
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add _meta for all mapping fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5607</link><project id="" key="" /><description>add field level `_meta` support for all fields in mapping

closes #2857
</description><key id="30461248">5607</key><summary>add _meta for all mapping fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-03-30T08:15:08Z</created><updated>2015-02-04T17:45:31Z</updated><resolved>2014-07-25T08:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-30T19:41:36Z" id="41840675">Hi @kzwang,

Thanks for the pull request. I see that it adds `_meta` tag not only to document fields but also to all metadata fields (such as `_id`, `_parent`, `_analyzer`, etc.). While I can clearly see the value for having `_meta` tag on the field level, I am not sure I like the idea of having it (and need to maintain it) for the metadata fields. Is there a particular reason you need it there?

Igor
</comment><comment author="clintongormley" created="2014-05-02T15:56:49Z" id="42047131">Hi @kzwang - I'd be interested in hearing the use case too.

I'm assuming that the intent is to use the `_meta` element for some object&lt;-&gt;es mapper, and wondering if and why meta entries are necessary on the special fields.  
</comment><comment author="clintongormley" created="2014-07-25T08:46:06Z" id="50123296">Closing.
</comment><comment author="mindbits" created="2015-02-04T17:45:31Z" id="72901942">I think having _meta on a field level is a great way to avoid lookup tables and use cases would range from comments to access control list information etc. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replaces usage of `StringBuffer` with `StringBuilder`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5606</link><project id="" key="" /><description>Closes #5605

Also fixes stray single quote in the same `PluginInfo#toString()` method.
</description><key id="30458762">5606</key><summary>Replaces usage of `StringBuffer` with `StringBuilder`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-30T04:14:47Z</created><updated>2015-06-07T14:57:05Z</updated><resolved>2014-04-01T09:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-31T08:38:59Z" id="39065201">Hi @pickypg , did you already sign our [CLA](http://www.elasticsearch.org/contributor-agreement/)? If not could you please do it so that we can merge your PRs?
</comment><comment author="pickypg" created="2014-03-31T13:11:48Z" id="39086137">I signed it already. Thanks.

-----Original Message-----
From: "Luca Cavanna" notifications@github.com
Sent: &#8206;3/&#8206;31/&#8206;2014 4:39 AM
To: "elasticsearch/elasticsearch" elasticsearch@noreply.github.com
Cc: "Chris Earle" pickypg@gmail.com
Subject: Re: [elasticsearch] Replaces usage of `StringBuffer` with`StringBuilder` (#5606)

Hi @pickypg , did you already sign our CLA? If not could you please do it so that we can merge your PRs?
&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2014-04-01T09:02:02Z" id="39184054">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StringBuilder should be used in place of StringBuffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5605</link><project id="" key="" /><description>`StringBuilder` was made as an API compatible, drop-in replacement of `StringBuffer` without the generally unnecessary synchronization of every method.

Currently, `StringBuffer` is being used by `PluginInfo#toString()` (not vital, but still unnecessary) and `FullRestartStressTest` (less of an issue as it's test code).
</description><key id="30458402">5605</key><summary>StringBuilder should be used in place of StringBuffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-30T03:41:10Z</created><updated>2014-04-01T09:01:45Z</updated><resolved>2014-04-01T08:59:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tagging search request/response in multisearch api. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5604</link><project id="" key="" /><description>We create different search request and put it in multi search request, Sometime it may be difficult to track what query goes in what position . (Sorry for bad explanation) 

I've stated what I am trying to tell. 

For multi search request we have to do: 

```
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
{"query" : {"match_all" : {}}}
```

I think It will be better if we can do 

```
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10,"tag":"topten"}
{"query" : {"match_all" : {}},"tag":"all"}
```

So the response will look like

```
{
    "responses": [
        {
            "tag": "topten",.. all other objects
        },
        {
            "tag": "all",... all other objects
        }
    ]
}
```
</description><key id="30457482">5604</key><summary>Tagging search request/response in multisearch api. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbnacharya</reporter><labels /><created>2014-03-30T02:16:56Z</created><updated>2014-03-30T02:17:30Z</updated><resolved>2014-03-30T02:17:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tagging search request/response in multisearch api. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5603</link><project id="" key="" /><description>We create different search request and put it in multi search request, Sometime it may be difficult to track what query goes in what position . So I think it will be good if we can track search request, so that we can handle easily. 

I've stated below what I am trying to tell. 

For multi search request we have to do: 

```
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
{"query" : {"match_all" : {}}}
```

I think It will be better if we can do 

```
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10,"tag":"topten"}
{"query" : {"match_all" : {}},"tag":"all"}
```

So the response will look like

```
{
    "responses": [
        {
            "tag": "topten",.. all other objects
        },
        {
            "tag": "all",... all other objects
        }
    ]
}
```
</description><key id="30457433">5603</key><summary>Tagging search request/response in multisearch api. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rbnacharya</reporter><labels /><created>2014-03-30T02:13:03Z</created><updated>2014-12-30T15:00:48Z</updated><resolved>2014-12-30T15:00:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T15:00:48Z" id="68362875">Hi @rbnacharya 

Sorry it has taken so long to get to this issue.  I'm assuming that, in the meantime, you've figured out how to handle this (ie the results are returned in the same order as the queries).  Given that nobody else has shown interest in this particular issue, I think this extra functionality is not that useful, and I'm going to close the ticket.  If you still feel strongly about this, reopen and we can discuss further.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add MediaWiki integration to list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5602</link><project id="" key="" /><description>MediaWiki can use Elasticsearch to power its search backend via the CirrusSearch extension.
</description><key id="30418534">5602</key><summary>Add MediaWiki integration to list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">demon</reporter><labels /><created>2014-03-28T20:07:58Z</created><updated>2014-07-16T21:47:07Z</updated><resolved>2014-04-14T23:58:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-28T20:13:02Z" id="38963056">Indeed.
</comment><comment author="s1monw" created="2014-04-02T09:07:39Z" id="39305485">I'd love to pull this - can you sign the CLA please
</comment><comment author="demon" created="2014-04-14T23:58:10Z" id="40432062">CLA for just a small website addition? I'll pass... Someone else can feel motivated to restore this if they'd like.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update CORS to respond with specific origin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5601</link><project id="" key="" /><description>When put behind basic auth, CORS requests don't allow the use of
a wildcard ("*") for Access-Control-Allow-Origin:

http://www.w3.org/TR/cors/#resource-requests

We ran into this while having nginx with basic auth proxy to our es
node. It should effectively be the same as what is there for CORS
requests, but respond with the origin. Happy to update for any
changes neccessary.
</description><key id="30414354">5601</key><summary>Update CORS to respond with specific origin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">calvinfo</reporter><labels /><created>2014-03-28T19:07:48Z</created><updated>2014-06-25T16:31:21Z</updated><resolved>2014-06-25T16:31:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeromeross" created="2014-04-02T21:01:25Z" id="39383034">came here to submit this exact issue :)
applied your patch and works seamlessly! :+1: 
thank you.

@kimchy, would you review this please?
</comment><comment author="FestivalBobcats" created="2014-04-04T01:22:59Z" id="39523617">Was just about to make this pull request myself as well.  This is the only way to have CORS with HTTP Basic authentication (without using a proxy).

A big +1 from the Qbox.io team.
</comment><comment author="kimchy" created="2014-06-24T13:05:34Z" id="46967836">we will review it, we are thinking about how to better handle CORS on a more broader sense, which will also do this, right @spinscale?
</comment><comment author="calvinfo" created="2014-06-24T22:48:55Z" id="47040915">Yeah, either way--I'm not too attached to the PR, so feel free to close!
</comment><comment author="spinscale" created="2014-06-25T12:25:54Z" id="47093860">Hey there, I am going to supercede this one with a new PR for properly supporting CORS very soon also adding some more features.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot checksum verification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5600</link><project id="" key="" /><description>Disables support for non append-only codecs and adds automatic verification of all files that are being snapshotted. Closes #5593
</description><key id="30410756">5600</key><summary>Snapshot checksum verification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-03-28T18:22:06Z</created><updated>2014-07-03T16:40:34Z</updated><resolved>2014-07-03T16:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-07-03T16:40:33Z" id="47954139">Will revisit after #6548 gets in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Question: IndexMissingException while searching store</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5599</link><project id="" key="" /><description>I am new in Elastic Search.I am writing some tests to Search documents but I am receiving an IndexMissingException [indexed_store_detail] missing.I create a Node inside my tests.At the start of the test I insert the document and when I make the following call I get the exception.
SearchResponse response = getClient()
            .prepareSearch(getIndexNameV2(), getIndexTypeV2())
            .setQuery(QueryBuilders.idsQuery().addIds("1"))
            .execute().actionGet();

This is weird because the GetResponse works fine with the same and literally the same code.What could be wrong ?

GetResponse response = getClient().prepareGet(getIndexNameV2(), getIndexTypeV2(),"1")
            .execute().actionGet();

org.elasticsearch.indices.IndexMissingException: [indexed_store_detail] missing
at org.elasticsearch.cluster.metadata.MetaData.convertFromWildcards(MetaData.java:648)
at org.elasticsearch.cluster.metadata.MetaData.concreteIndices(MetaData.java:559)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:112)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:70)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:61)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:58)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:48)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:108)
at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:214)
at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:841)
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62)
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57)
at com.paypal.demandgen.places.search.server.search.TestElasticSearchResponseParserV3toV1.parsePassesForSearchResponse(TestElasticSearchResponseParserV3toV1.java:49)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
</description><key id="30406783">5599</key><summary>Question: IndexMissingException while searching store</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhijeetkushe</reporter><labels /><created>2014-03-28T17:31:13Z</created><updated>2014-03-28T17:47:38Z</updated><resolved>2014-03-28T17:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-28T17:47:38Z" id="38948039">Please use the mailing list for questions.
See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search template: Put source param into template variable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5598</link><project id="" key="" /><description>If a search template was created using the source parameter, the
content of the parameter was put as source instead of sourceTemplate

Fixes #5556
</description><key id="30404273">5598</key><summary>Search template: Put source param into template variable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T16:59:04Z</created><updated>2015-06-07T22:04:42Z</updated><resolved>2014-03-31T09:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-31T09:28:36Z" id="39069312">++ Fixes my tests
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Freq terms enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5597</link><project id="" key="" /><description>Based on the work started in https://github.com/elasticsearch/elasticsearch/pull/5489
Two new utility classes to support analysis of term frequencies used by significant_terms agg and phrase suggester. The FilterableTermsEnum base class provides access to stats with an optional filter (this will be of use in examining significant_terms against a background other than the entire index).
The FreqTermsEnum subclass adds caching support for scenarios where there are likely to be repeated calls for the same term's counts.
</description><key id="30403264">5597</key><summary>Freq terms enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T16:47:08Z</created><updated>2015-06-06T18:34:29Z</updated><resolved>2014-04-02T10:46:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-31T01:34:14Z" id="39047379">LGTM, I would add the ability to provide the filter to significant terms agg, with a note about how to properly use it, I think it will end up being very beneficial to users.
</comment><comment author="s1monw" created="2014-04-02T09:19:44Z" id="39306788">I did another review round! Looks good though!
</comment><comment author="s1monw" created="2014-04-02T09:56:24Z" id="39311127">LGTM +1 so squash and push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ContextSuggester: Adding couple of tests to catch more bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5596</link><project id="" key="" /><description>A bunch of minor fixes have been included here, especially due
to wrongly parsed mappings. Also using assertions resulted in an
NPE because they were disabled in the distribution.

Closes #5525

Also, there is one last thing, which needs discussing: Currently the default precision is so accurate, that a suggestions will fail, you index the lat/lon `52.363389/4.888695` but search with `52.3633, 4.8886` - I think this is confusing. We should either force people to set a `precision` in the mapping or have less high accuracy.
</description><key id="30401537">5596</key><summary>ContextSuggester: Adding couple of tests to catch more bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T16:27:19Z</created><updated>2015-06-07T22:15:07Z</updated><resolved>2014-03-31T14:42:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T12:24:20Z" id="39081977">I think this one looks good though I left two small comments
</comment><comment author="s1monw" created="2014-03-31T13:18:41Z" id="39086780">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some typos in documentation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5595</link><project id="" key="" /><description /><key id="30400255">5595</key><summary>Fix some typos in documentation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hkorte</reporter><labels /><created>2014-03-28T16:12:14Z</created><updated>2014-07-16T21:47:10Z</updated><resolved>2014-03-31T11:49:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T11:49:14Z" id="39079485">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failover. Lost documents on process kill.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5594</link><project id="" key="" /><description>1. Start an ES node (set index.number_of_replicas: 0);
2. Run the groovy test (find the code bellow);
3. Wait couple of seconds so that a few thousands of documents get inserted;
4. Kill the ES process (from task manager on Win, not Ctrl C);
5. Start ES again;
6. Wait for the test to finish;

The test index batches of documents, recording their IDs, then, after ES is back check that all those documents are there.
Observed : A lot of documents are missing.
Test : https://github.com/bax1989/bax/blob/master/FailOverTest.java
</description><key id="30394888">5594</key><summary>Failover. Lost documents on process kill.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaxanean</reporter><labels /><created>2014-03-28T15:10:03Z</created><updated>2014-04-01T06:26:44Z</updated><resolved>2014-04-01T06:26:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-30T14:55:38Z" id="39027355">Did you create a cluster (I suppose you did that, you didnt mention it)? However your transport client does not do any failover, then you try to execute all your writes simply on that one node, which you supposedly kill during your test.

Can you try adding all your elasticsearch nodes to the transport client? Alternatively check the `sniff` option of the transport client http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/client.html#transport-client
</comment><comment author="abaxanean" created="2014-03-30T15:03:23Z" id="39027588">I have just 1 ES node. The IDs I check for were generated by ES. If ES index a document and generate and ID for it(returned in the response), doesn't it guarantee that document is persisted ?
</comment><comment author="kimchy" created="2014-03-30T15:10:47Z" id="39027793">- Can you check if each bulk item failed or not before adding the id generated to the list of indexed docs?
</comment><comment author="spinscale" created="2014-03-30T15:12:17Z" id="39027840">How did you create the initial index? Did you wait until the index is recovered completely before executing that check (every primary shard is marked as started)? Did you check each bulk response item if indexing was successful at all?

Not sure if your check for green state works at all, if you created the index with default  `number_of_shards` and `number_of_replicas` configuration (as default number of replicas is set to 1, and this never happens on a single node system, so cluster health is never green).
</comment><comment author="abaxanean" created="2014-03-30T15:21:07Z" id="39028097">I did not create the index at all, it is automatically created by ES.
Fixed to check every bulk response.
Set the number of replicas to 0 or let as is and check for yellow status.
Anyway, the documents can't be found.
</comment><comment author="kimchy" created="2014-03-30T15:25:53Z" id="39028218">its very surprising that you are running the tests and said documents can't be found yet had to do all those change... . For example, how can the test run if you were waiting for green on an index created with 1 replica? The reason I bring it up is that something here is weird in how you conduct the test, if waiting for green (which should have consistently failed) worked before with 1 replica and single node.

When you say documents can't be found, what do you mean? We need a bit more info. Do you not find any document? Does your test finds some, but only the last ones are not found? Anything?
</comment><comment author="abaxanean" created="2014-03-30T15:43:15Z" id="39028712">"When you say documents can't be found, what do you mean?", get by ID returns nothing.
On last run, from 22700 indexed docs, 55 were not found.
</comment><comment author="spinscale" created="2014-03-30T15:52:25Z" id="39028957">Hey, 

interesting, I just tested with your program (converted the class to java and ran it) ten times, not a single problem, always all documents got indexed and could be found after restarting.

Can you tell us a bit about your setup? Operating system? JVM? Elasticsearch version? Everything non-standard is worth noting...

Here is your converted test class to java: https://gist.github.com/spinscale/3793c990292c4476b05a - maybe you could check if your test also fails with that.
</comment><comment author="abaxanean" created="2014-03-30T16:13:25Z" id="39029759">Windows 7 Enterprise.
Java 1.7.0_51 x64.
ES 1.1.0, changed only number of replicas = 0.
Changed a bit the test (set my own IDs, integer progression), from 45k docs, 1226 are 'lost', 

Found unresolved ID : 41872
Found unresolved ID : 41875
Found unresolved ID : 41878
Found unresolved ID : 41882
Found unresolved ID : 41885
Found unresolved ID : 41888
Found unresolved ID : 41892
Found unresolved ID : 41895
Found unresolved ID : 41898
. . .
Found unresolved ID : 44992
Found unresolved ID : 44993
Found unresolved ID : 44994
Found unresolved ID : 44995
Found unresolved ID : 44996
Found unresolved ID : 44997
Found unresolved ID : 44998
Found unresolved ID : 44999
Found unresolved ID : 45000
</comment><comment author="abaxanean" created="2014-03-30T16:52:01Z" id="39031156">Tried the Java test (linked it to the issue), the problem is reproducing.
</comment><comment author="abaxanean" created="2014-03-30T17:03:59Z" id="39031559">The issue does NOT fire if :
1. I close the ES node via Ctrl + C
 OR
2. I'm doing a flush after each bulk insert and only after that consider the docs as indexed (add the IDs to the list).
</comment><comment author="kimchy" created="2014-03-30T17:08:07Z" id="39031739">can you try and set `index.gateway.local.sync: 0` in the config and run the test?
</comment><comment author="abaxanean" created="2014-03-30T17:16:40Z" id="39031983">Set index.gateway.local.sync: 0, the issue is not reproducing but the indexing performance is very slow.
</comment><comment author="kimchy" created="2014-03-30T17:59:31Z" id="39033387">right, the option mentioned means that it will fsync the transaction log on each write (index) operation done. By default, it is done in a scheduled manner, since the assumption is that if HA is needed, another node with a replica will be allocated. Since in the replica case, we do sync replication, the default doesn't have to be fsync on each write.

In your case, you have several options:  
1. add another node and a replica, indexing speed will still be slower (because of sync replication), but you will have another copy of the data 
2. If you only care about bulk level "syncs", flush after each bulk request.
3. Enable fsync after each write as explained, with the fact that indexing will be slower.

On all cases, you can improve the speed by having faster disk / SSD.
</comment><comment author="abaxanean" created="2014-03-30T18:43:43Z" id="39035037">Thank you.
Is the content of the transaction log lost on process kill ?
</comment><comment author="kimchy" created="2014-03-30T18:46:20Z" id="39035113">no, its not necessarily lost, written content might be lost since the last successful fsync to a transaction log
</comment><comment author="abaxanean" created="2014-03-30T18:53:04Z" id="39035329">So, on indexing, documents are stored in memory, from time to time written to transaction log file which, in turn, is flushed to Lucene segments ?
And that "in memory" part might get lost ?
</comment><comment author="kimchy" created="2014-03-30T18:53:51Z" id="39035355">well, depends, but in general, even when writing to a file, until it is "fsync'ed" data might be lost.
</comment><comment author="abaxanean" created="2014-03-30T18:58:25Z" id="39035516">Thanks a lot.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore API: Snapshot checksum verification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5593</link><project id="" key="" /><description>The snapshot process should verify checksums for each file that is being snapshotted to make sure that created snapshot doesn't contain corrupted files. If a corrupted file is detected, the snapshot should fail with an error.

In order to implement this feature we need to have correct and verifiable checksums stored with segment files, which is only possible for files that were written by append-only codecs. All officially supported codecs that are currently in use are append-only. If there are old 3rd party codecs that are not, such codecs will no longer work with Elasticsearch. 
</description><key id="30380478">5593</key><summary>Snapshot/Restore API: Snapshot checksum verification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T11:40:20Z</created><updated>2014-08-20T14:59:45Z</updated><resolved>2014-08-20T01:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinkluge" created="2014-05-05T17:22:42Z" id="42212962">depends on #5924 
</comment><comment author="clintongormley" created="2014-07-11T08:41:24Z" id="48707277">@imotov are you going to able to get this in for 1.3?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing a polygon geo-shape where polygons share are a starting point throws an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5592</link><project id="" key="" /><description>When we index a polygon shape we allow to supply holes in that polygon. Those are supplied as extra polygon as the geojson spec dictates. If a hole start on the same coordinate as the boundary polygon we throw an ArrayIndexOutOfBoundsException.

See https://gist.github.com/mcuelenaere/c3370ac8356b7ec80724 for a reproduction.
</description><key id="30379659">5592</key><summary>Indexing a polygon geo-shape where polygons share are a starting point throws an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label></labels><created>2014-03-28T11:25:24Z</created><updated>2014-09-02T11:11:49Z</updated><resolved>2014-09-02T11:11:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rberringer" created="2014-06-11T15:22:16Z" id="45756204">I see the same. I am running version 1.1.1. If the first coordinate's x value in my inner polygon is the same as the first coordinate's x value in my outer polygon I get the ArrayIndexOutOfBoundsException.
</comment><comment author="mcuelenaere" created="2014-08-20T13:00:32Z" id="52774039">Another example:

``` json
{
  "type": "Polygon",
  "coordinates": [
    [
      [3.1019747572216, 51.0669454624],
      [3.1020102578034, 51.066935662965],
      [3.102010546753, 51.066935583214],
      [3.1021346305184, 51.066901330016],
      [3.1020901288154, 51.066866669173],
      [3.1020390125375, 51.066881010922],
      [3.1019903844109, 51.066894654364],
      [3.1017943936263, 51.066949642491],
      [3.1018217306228, 51.066987703695],
      [3.1019747572216, 51.0669454624]
    ],
    [
      [3.1020390125375, 51.066881010922],
      [3.1020792567632, 51.06687334992],
      [3.1020855169818, 51.066882954507],
      [3.1020472839155, 51.066893507128],
      [3.1020390125375, 51.066881010922]
    ]
  ]
}
```

Stacktrace when indexing this on ES 1.3.2:

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [address.shape]
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:249)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:534)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:483)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:397)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:421)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:522)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:421)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.assign(BasePolygonBuilder.java:363)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.compose(BasePolygonBuilder.java:341)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.coordinates(BasePolygonBuilder.java:140)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.buildGeometry(BasePolygonBuilder.java:169)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:145)
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:234)
    ... 14 more
```
</comment><comment author="colings86" created="2014-09-02T11:11:49Z" id="54137244">Closing as this is resolved by #7190
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed mention of Spatial4J and JTS requirement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5591</link><project id="" key="" /><description>AFAIK, on 1.0 at least (and later), those libraries are included.
</description><key id="30374116">5591</key><summary>Removed mention of Spatial4J and JTS requirement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels /><created>2014-03-28T09:51:54Z</created><updated>2014-06-16T20:24:24Z</updated><resolved>2014-05-06T12:52:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-30T15:46:21Z" id="39028796">Those libs are included in our distro, but they are still optional, as in, Elasticsearch can work without it, and then those queries will not be enabled.
</comment><comment author="clintongormley" created="2014-05-06T12:52:38Z" id="42297927">Merged, Thanks for the PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IllegalAccessError after a few queries when using script filter in elasticsearch query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5590</link><project id="" key="" /><description>I have a 'users' elasticsearch index, where a user looks like:

```
{
  "id" : 1,
  "name" : "Jeroen",
  "hours": [8,9,10,11,12,19,20,21,22,23],
  "country": "NL",
  "utc_offset": 1.0
}
```

I want to find all users of which the 'hours' field contains the current hour in their local time. So for example, I only want to find the above user when it's between 8.00-12.00 or 20.00-23.00 in the Netherlands.

My solution for this is using a script filter. I didn't know how to implement this with MVEL, so I installed the javascript plugin. Now my query looks like this:

```
{
  "query": {
    "match_all": {}
  },"filter": {
    "script": {
       "script": "var a = doc['hours'].values; var d = new Date(); d.setTime(d.getTime() + doc['utc_offset'].value * 3600 * 1000); a.indexOf('' + d.getHours()) != -1",
       "params": {}
    }
  }
}
```

So this works, but after a while elasticsearch is starting to throw exceptions, like this:

```
{
  "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[x9FlNmmsT26hJbrfnyH2uA][users][2]: QueryPhaseExecutionException[[users][2]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalAccessError[org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1]; }{[x9FlNmmsT26hJbrfnyH2uA][users][3]: QueryPhaseExecutionException[[users][3]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalAccessError[org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1]; }{[x9FlNmmsT26hJbrfnyH2uA][users][0]: QueryPhaseExecutionException[[users][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalAccessError[org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1]; }{[x9FlNmmsT26hJbrfnyH2uA][users][2]: QueryPhaseExecutionException[[users][3]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalAccessError[org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1]; }{[x9FlNmmsT26hJbrfnyH2uA][users][4]: QueryPhaseExecutionException[[users][4]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalAccessError[org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1]; }]",
"status": 500
}
```

A similar issue (#3094) was posted where it was suggested it's a problem with the JIT compiler. As a workaround it was suggested to disable it by using '-Dmvel2.disable.jit=true'. I've tried this, by putting it in ES_JAVA_OPTS in /etc/default/elasticsearch but it didn't seem to have any effect.

What am I doing wrong? Or is there an alternative way of performing this query?
</description><key id="30370213">5590</key><summary>IllegalAccessError after a few queries when using script filter in elasticsearch query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeroenr</reporter><labels /><created>2014-03-28T08:33:22Z</created><updated>2014-12-30T14:54:13Z</updated><resolved>2014-12-30T14:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-28T13:53:24Z" id="38920698">I am a bit confused @jeroenr :) You sure you are using lang-javascript instead of mvel here? Asking since you didn't specify the `lang` param in your script, and the default is mvel.
</comment><comment author="jeroenr" created="2014-03-28T14:08:48Z" id="38922272">Wow, you are actually completely right about that. I didn't realize. When I specify 'javascript' as the language my script doesn't even work.

Still I find it weird that my script does actually work for a period of time, even when I appear to be using MVEL. You have any ideas on that?
</comment><comment author="javanna" created="2014-03-28T14:10:02Z" id="38922398">No that's the strange part, I'd expect it not to work at all. Do all your documents have multiple values for the hours field?
</comment><comment author="jeroenr" created="2014-03-28T14:11:10Z" id="38922532">Yes, they do.
</comment><comment author="jeroenr" created="2014-03-28T14:17:51Z" id="38923241">When I modify my script to

```
{
           "script": "var a = doc['hours'].value || ''; var d = new Date(); d.setTime(d.getTime() + doc['utc_offset'].value * 3600 * 1000); a.indexOf('' + d.getHours()) != -1;",
           "params": {},
           "lang": "javascript"
        }
```

It's actually not complaining anymore about syntax, but I don't get the response I expected, which I do when using the previous script with MVEL.

I'm confused.
</comment><comment author="javanna" created="2014-03-28T14:33:22Z" id="38924933">Same here. A few alternatives: can't you just store hours in UTC and provide UTC hours at query time too? That way you wouldn't have to do any timezone conversion on the fly, and you probably wouldn't need a script anymore? Otherwise I'd look at writing a native script, which is the fastest script you can have.
</comment><comment author="jeroenr" created="2014-03-28T14:56:27Z" id="38927742">Great idea. Convert hours array to UTC hours and then you can just query using term filter specifying the current UTC hour and you will get all users which have that hour in their hours array, right?

I was also thinking about native scripts, but if the above would work would be way easier.
</comment><comment author="jeroenr" created="2014-03-31T18:23:15Z" id="39123050">Now I remember the reason why I didn't convert to UTC before. One might get in trouble with day light saving time, since then the UTC offset changes.
</comment><comment author="clintongormley" created="2014-12-30T14:54:13Z" id="68362329">Hi @jeroenr 

I'm assuming you're not longer seeing these exceptions, and closing this ticket.  Please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore streamInput() performance over PagedBytesReference.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5589</link><project id="" key="" /><description>The initial implementation of bulk-reading a streamInput() over PagedBytesReference was slow (byte-by-byte reading when bulk copying).

Times in &#181;s for bulk-reading a stream over plain vs. paged, averaged over 1000 runs:

| MB | plain &#181;s | paged &#181;s | Ratio |
| --: | --: | --: | --- |
| 1 | 72 | 2048 | 28.6 |
| 2 | 140 | 4127 | 29.4 |
| 3 | 218 | 6208 | 28.4 |
| 4 | 430 | 8396 | 19.5 |
| 5 | 700 | 10525 | 15.0 |
| 10 | 1739 | 21055 | 12.1 |
| 20 | 3481 | 42063 | 12.1 |
| 50 | 8701 | 105337 | 12.1 |
| 100 | 17409 | 210848 | 12.1 |

This changeset restores performance:

| MB | plain &#181;s | paged &#181;s | Ratio |
| --: | --: | --: | --- |
| 1 | 65 | 68 | 1.05 |
| 2 | 134 | 141 | 1.04 |
| 3 | 198 | 235 | 1.18 |
| 4 | 456 | 418 | 0.91 |
| 5 | 750 | 761 | 1.01 |
| 10 | 1736 | 1743 | 1.00 |
| 20 | 3514 | 3497 | 0.99 |
| 50 | 8706 | 8700 | 0.99 |
| 100 | 17608 | 17731 | 1.00 |

The performance jitters slightly due to the usual Hotspot variances, OS scheduling etc. For all practical purposes the performance is now back to what it was before.
</description><key id="30368953">5589</key><summary>Restore streamInput() performance over PagedBytesReference.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>regression</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T08:04:16Z</created><updated>2015-06-08T00:44:18Z</updated><resolved>2014-03-28T10:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-28T09:00:13Z" id="38899240">Made args final, reformatted a bit, added PR.
</comment><comment author="s1monw" created="2014-03-28T09:22:05Z" id="38900623">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator doesn't reduce CircuitBreaker stats in every case.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5588</link><project id="" key="" /><description>In the case a filter / query is in a percolator query that uses FieldData, after percolating when the in-memory index gets cleared up the circuit breaker stats don't get reduced. Resulting in a discrepancy between what fielddata and the circuit breaker are reporting.

The fix makes sure that the circuit breaker stats gets reduced even for a segment no shard id can be found, which is the case for the in-memory index the percolator is used to percolate documents.
</description><key id="30367551">5588</key><summary>Percolator doesn't reduce CircuitBreaker stats in every case.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T07:24:11Z</created><updated>2015-12-07T07:18:54Z</updated><resolved>2014-04-02T05:00:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-28T08:09:18Z" id="38896380">man good catch! I looked at the patch and I think we should rather introduce a special unload listener for this case and try to assert that the listener is never `null`.  I'd also want to make sure that the key is never `null` - I think this only happens in tests today. you also need to make sure that size is positive otherwise you will increment the breaker! 
</comment><comment author="martijnvg" created="2014-03-28T08:11:12Z" id="38896486">That makes sense! I'll update the PR.
</comment><comment author="martijnvg" created="2014-03-30T17:19:58Z" id="39032095">@s1monw I updated the PR.
</comment><comment author="kimchy" created="2014-03-30T23:23:05Z" id="39043821">I wonder if it won't be cleaner to have an "Indices" level field data cache listener, that will always reduce the circuit breaker, and will always be added as a listener to the key as well. 

Then, we can remove the logic to reduce the circuit breaker in 2 places, and keep only the shard level stats at the shard level listener, and the indices level handling on the indices level (circuit breaker). This will also mean we won't have CirecuitBreaker in the `ShardFieldData` class. The class name can be cleaner as well, something like `IndicesFieldDataCacheListener`. 

In the future, this will also allow other indices level services to register for this if they need to.
</comment><comment author="martijnvg" created="2014-03-31T07:34:00Z" id="39060484">@kimchy This makes sense, I'll update the PR.
</comment><comment author="s1monw" created="2014-03-31T10:41:59Z" id="39074889">+1 to move on an Indices level...
</comment><comment author="martijnvg" created="2014-04-01T09:19:51Z" id="39185550">@kimchy @s1monw I updated the PR.
</comment><comment author="s1monw" created="2014-04-01T09:37:17Z" id="39186973">one small comment - LGTM otherwise
</comment><comment author="justies" created="2015-12-07T07:18:54Z" id="162435568">incase if I have a diffferent types and having a common name of a field created by.When I querying with that field with respect to a particular type , will it load the data from all type to the heap memory or only from the type which i have refered. due to this frequent circuitbreaker exception occuring.. please answer
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix DateHistogramBuilder to use a String pre_offset and post_offset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5587</link><project id="" key="" /><description>Currently, if `preOffset` or `postOffset` are used in the `DateHistogramBuilder`, the generated query fails parsing in the `DateHistogramParser`.

Closes #5586
</description><key id="30356921">5587</key><summary>Fix DateHistogramBuilder to use a String pre_offset and post_offset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rusnyder</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T01:24:36Z</created><updated>2015-06-07T22:01:19Z</updated><resolved>2014-08-08T09:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-08T09:08:04Z" id="51579198">Closing as this has already been fixed by #6814
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: DateHistogramBuilder uses wrong data type for pre_offset and post_offset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5586</link><project id="" key="" /><description>The DateHistogramBuilder stores and builds the DateHistogram [with a `long` value for the `pre_offset` and `post_offset`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java#L44), which is neither what the [API docs specify](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_pre_post_offset_2) (which specify the format is the data format `1s`, `2d`, etc.) nor what the [DateHistogramParser expect](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java#L122).

This forces the improper construction of DateHistogram requests when using the Java API to construct queries.  Both the `preOffset` and `postOffset` variables should be converted to `Strings`.
</description><key id="30356029">5586</key><summary>Aggregations: DateHistogramBuilder uses wrong data type for pre_offset and post_offset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">rusnyder</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-28T00:57:45Z</created><updated>2014-07-16T09:01:13Z</updated><resolved>2014-07-10T15:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TheLudd" created="2014-05-06T14:56:31Z" id="42313014">I experience the same error so +1 on fixing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check "store" parameter for binary mapper and check "index_name" for all mappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5585</link><project id="" key="" /><description>closes #5474

the "index_name" is also ignored without throw exception, added check for that as well
</description><key id="30352685">5585</key><summary>Check "store" parameter for binary mapper and check "index_name" for all mappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T23:42:31Z</created><updated>2015-06-07T22:30:41Z</updated><resolved>2014-03-31T11:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-31T11:13:57Z" id="39077040">Merged. Thanks, Kevin!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A few grammar and word use corrections.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5584</link><project id="" key="" /><description /><key id="30351739">5584</key><summary>A few grammar and word use corrections.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kmeister2000</reporter><labels><label>docs</label></labels><created>2014-03-27T23:27:24Z</created><updated>2014-07-08T20:18:15Z</updated><resolved>2014-04-07T10:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T16:12:30Z" id="39582543">Hi @kmeister2000 thanks for your PR, would you mind signing our [CLA](elasticsearch.org/contributor-agreement/) so we can merge it in?
</comment><comment author="kmeister2000" created="2014-04-04T16:52:31Z" id="39586503">Just signed it,

Thanks!

On Fri, Apr 4, 2014 at 12:12 PM, Luca Cavanna notifications@github.comwrote:

&gt; Hi @kmeister2000 https://github.com/kmeister2000 thanks for your PR,
&gt; would you mind signing our CLAhttp://elasticsearch.org/contributor-agreement/so we can merge it in?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/5584#issuecomment-39582543
&gt; .

## 

Karl Meisterheim
karl@meisterheimconsulting.com
(614) 626-5275
</comment><comment author="javanna" created="2014-04-04T17:29:33Z" id="39590249">Merged thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Please don't forbid Math.abs(int/long)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5583</link><project id="" key="" /><description>I just reviewed the pull request https://github.com/elasticsearch/elasticsearch/pull/5562 and I strongly disagree with the change.

I'm currently using this feature to implement **_delta**_ sorting. For example, given these documents:

``` json
{ "name" : "John",  "year" : 1975 }
{ "name" : "Jim",  "year" : 1976 }
{ "name" : "James",  "year" : 1977 }
{ "name" : "Jody",  "year" : 1978 }
{ "name" : "Josh",  "year" : 1979 }
```

Consider a query like this, where I want the 1977 results to come first, and then the 1978 and 1976 results next, and the the 1975 and 1979 results last:

``` json
{
  "query" : { "match_all" : {} },
  "sort" : [{
    "_script" : {
      "script" : "abs(1977-doc['year'].value)",
      "order" : "asc"
    }
  }]
}
```

I use this kind of trick all the time (it's very common in the higher-education domain), and it depends on absolute values for int &amp; long values, so your pull request is going to break my code.

Wouldn't it make more sense to just throw a 400 BAD REQUEST when the query tries to do abs(Integer.MIN_VALUE) instead of removing the function entirely for integral types?
</description><key id="30345140">5583</key><summary>Please don't forbid Math.abs(int/long)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benjismith</reporter><labels /><created>2014-03-27T21:44:58Z</created><updated>2014-03-27T21:53:43Z</updated><resolved>2014-03-27T21:49:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-03-27T21:47:23Z" id="38864953">@benjismith I just tested this locally, forbidding `Math.abs(...)` in forbiddenapis does not prevent you from using it in scripts, I believe the forbiddenapis check is only during Elasticsearch compilation time.
</comment><comment author="benjismith" created="2014-03-27T21:49:08Z" id="38865151">Gotcha. My mistake! Thanks for the clarification :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tribe node errors on start up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5582</link><project id="" key="" /><description>When starting up a tribe node the following is seen in the log file:

```
[2014-03-27 18:47:23,080][WARN ][tribe                    ] [tribe_x.x.x.x] failed to process [cluster event from t2, updating local node id]
java.lang.NullPointerException
        at org.elasticsearch.cluster.routing.RoutingTable$Builder.add(RoutingTable.java:400)
        at org.elasticsearch.tribe.TribeService$TribeClusterStateListener$1.execute(TribeService.java:285)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:308)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

After a few minutes these errors cease and everything seems normal again.
Note: the tribe node is version 1.1.0 and the other nodes are 1.0.1 ... is this the issue?
</description><key id="30332276">5582</key><summary>tribe node errors on start up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cleesmith</reporter><labels><label>bug</label><label>v1.1.1</label></labels><created>2014-03-27T19:07:39Z</created><updated>2014-03-27T22:14:11Z</updated><resolved>2014-03-27T22:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-27T21:09:14Z" id="38860971">I looked into this, and this issue was fixed in 1.x (upcoming 1.2), I will backport just the fix to 1.1.x branch so if we do another 1.1 release
</comment><comment author="cleesmith" created="2014-03-27T22:14:11Z" id="38867599">Amazing, thanks.  You guys are on top of things ... now, if I could just keep up with the releases.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ignore missing document error in bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5581</link><project id="" key="" /><description>I am using the bulk api (specifically using the python client wrapper of bulk).  Realize this is not the Python client github repo - but I believe this needs support upstream (unless I am missing an available flag already implemented).

This bulk process contains a mix of index, updates and deletes.

Right now, if the batch contains anything other than an 'index' (sourced from a large input file row by row) I need to try/except error catch any delete/update as a **single** API call in case a document is being deleted/updated doesn't exist.  Essentially bypassing the bulk process.  For millions of rows, this is obviously not optimal.

Need an "ignore missing document" flag in bulk to ignore errors when id not found for update/delete.  (unless I am missing this flag in the docs somewhere..  Been digging around).
</description><key id="30331225">5581</key><summary>ignore missing document error in bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffsteinmetz</reporter><labels /><created>2014-03-27T18:54:46Z</created><updated>2014-04-04T16:32:15Z</updated><resolved>2014-04-04T16:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-03-27T22:53:25Z" id="38871080">bulk api won't throw exception if any requests fails 
for example, if you use the requests from the document http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html

```
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "2" } }
{ "create" : { "_index" : "test", "_type" : "type1", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "index1"} }
{ "doc" : {"field2" : "value2"} }
```

you will get result

```
{
  "took": 137,
  "errors": true,
  "items": [
    {
      "index": {
        "_index": "test",
        "_type": "type1",
        "_id": "1",
        "_version": 1,
        "status": 201
      }
    },
    {
      "delete": {
        "_index": "test",
        "_type": "type1",
        "_id": "2",
        "_version": 1,
        "status": 404,
        "found": false
      }
    },
    {
      "create": {
        "_index": "test",
        "_type": "type1",
        "_id": "3",
        "_version": 1,
        "status": 201
      }
    },
    {
      "update": {
        "_index": "index1",
        "_type": "type1",
        "_id": "1",
        "status": 404,
        "error": "DocumentMissingException[[index1][-1] [type1][1]: document missing]"
      }
    }
  ]
}
```

you can see the delete and update request fail and the index and create success.

so in your case, you can send all results as one bulk request and go though this result to find any failed items
</comment><comment author="javanna" created="2014-04-04T16:16:05Z" id="39582921">Hi @jeffsteinmetz as @kzwang said, the bulk response returns multiple items, in the same order as you specified them in the request. You need to check each item to know how things went. A single failing request won't be failing the whole bulk. Is this what you need or am I missing something?
</comment><comment author="jeffsteinmetz" created="2014-04-04T16:32:15Z" id="39584541">Thank you.  Knowing that it doesn't fail the whole bulk, and that it doesn't raise an error in the python client that needs to be caught helps.   I think it was the elasticsearch-py logging messages that were throwing me off.  Will need to test again to confirm the python client doesn't throw errors but just logs warnings and continues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update _cat/nodes documentation with all headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5580</link><project id="" key="" /><description>The current [_cat/nodes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html) documentation shows a few examples of possible output, but it does not provide the full list of possible options (headers) available.

Adding all of the options will make this page itself more user friendly and search friendly because people trying to find such information will hopefully get hits from the exhaustive list of headers and descriptions when searching, rather than potentially passing over `_cat/nodes` without realizing that it has many, many more useful fields.
</description><key id="30322948">5580</key><summary>[DOCS] Update _cat/nodes documentation with all headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">pickypg</reporter><labels /><created>2014-03-27T17:27:20Z</created><updated>2014-05-07T16:20:03Z</updated><resolved>2014-05-07T16:20:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:47:17Z" id="42297397">@drewr Please could you review this PR and merge if ok?
</comment><comment author="drewr" created="2014-05-07T16:19:58Z" id="42448712">Merged 12f758e811d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update geo-shape-type documentation to include all shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5579</link><project id="" key="" /><description>The [geo-shape-type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-shape-type.html) documentation currently does not include all of the supported (and unsupported) GeoJSON types. Adding it there would make it a lot easier to work with, and also expose the mostly hidden, non-GeoJSON, `circle` type for wider use by the community at large.
</description><key id="30322140">5579</key><summary>[DOCS] Update geo-shape-type documentation to include all shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels /><created>2014-03-27T17:18:22Z</created><updated>2014-05-06T12:43:55Z</updated><resolved>2014-05-06T12:43:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:43:55Z" id="42297131">Fixed by #5523
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RandomScoreFunction.PRNG generates weak random numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5578</link><project id="" key="" /><description>Here is some code extracted from `PRNG.random(int)`:

``` java
long rand = doc;
rand |= rand &lt;&lt; 32;
rand ^= rand;
return nextFloat(rand);
```

The issue is that `rand ^= rand;` is equivalent to `rand = 0;` so in the end, the random score generation completely discards the doc ID that was provided.
</description><key id="30318991">5578</key><summary>RandomScoreFunction.PRNG generates weak random numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T16:46:28Z</created><updated>2014-04-28T17:27:43Z</updated><resolved>2014-04-28T17:27:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-03-28T04:08:52Z" id="38887282">Note: this is a dupe of #5454 although this one explains the issue
</comment><comment author="uboness" created="2014-04-28T17:27:43Z" id="41587477">closed by https://github.com/elasticsearch/elasticsearch/commit/e8ea9d75852bfc79e931143807af99ff9297da7e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update has-parent-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5577</link><project id="" key="" /><description>"This filter return child..." =&gt; This filter returns child...
</description><key id="30303754">5577</key><summary>Update has-parent-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndrewO</reporter><labels /><created>2014-03-27T14:17:56Z</created><updated>2014-07-16T21:47:12Z</updated><resolved>2014-03-30T22:08:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-27T15:45:35Z" id="38820963">Hi @AndrewO 

many thanks for the fix. please could you sign the contributed license agreement so we can get your commit merged in: http://elasticsearch.org/contributor-agreement

many thanks
</comment><comment author="AndrewO" created="2014-03-30T20:34:05Z" id="39038603">No problem. Done.
</comment><comment author="clintongormley" created="2014-03-30T22:08:10Z" id="39041535">Many thanks - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequestBuilder#toString causes the content of the request to change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5576</link><project id="" key="" /><description>Using the Java API, If one sets the content of a search request through `SearchRequestBuilder#setSource` methods and then calls `toString` to see the result, not only the content of the request is not returned as it wasn't set through `sourceBuilder()`,  the content of the request gets also reset due to the `internalBuilder()` call in `toString`.

Here is a small failing test that demontrates it:

```
SearchRequestBuilder searchRequestBuilder = new SearchRequestBuilder(client()).setSource("{\n" +
                "            \"query\" : {\n" +
                "            \"match\" : {\n" +
                "                \"field\" : {\n" +
                "                    \"query\" : \"value\"" +
                "                }\n" +
                "            }\n" +
                "        }\n" +
                "        }");
String preToString = searchRequestBuilder.request().source().toUtf8();
searchRequestBuilder.toString();
String postToString = searchRequestBuilder.request().source().toUtf8();
assertThat(preToString, equalTo(postToString));
```
</description><key id="30295411">5576</key><summary>SearchRequestBuilder#toString causes the content of the request to change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label></labels><created>2014-03-27T12:25:05Z</created><updated>2015-04-28T09:22:39Z</updated><resolved>2015-04-28T09:22:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-27T12:37:42Z" id="38797164">From a user perspective it's pretty clear what the `toString` method should do, just print the request in json format. The problem is how, as properties can be set in so many ways that can override each other...which is why I guess the current implementation is half broken. I would consider even removing the current `toString` as it has this bad side effect. Curious on what people think about this.
</comment><comment author="s1monw" created="2014-03-27T12:48:42Z" id="38798166">good catch!
</comment><comment author="s1monw" created="2014-04-02T14:54:04Z" id="39339933">@javanna do you think you can work on this this week?
</comment><comment author="javanna" created="2014-04-02T14:55:49Z" id="39340177">I think I'll get to this soon, I'd appreciate comments on how to fix it though ;)
</comment><comment author="javanna" created="2014-10-10T08:12:30Z" id="58625151">Hey @GaelTadh I remember reviewing a PR from you for this issue, did you get it in after all?
</comment><comment author="javanna" created="2014-10-10T08:16:10Z" id="58625497">Nevermind I found it and linked this issue, I see it's not in yet, assigned this issue to you @GaelTadh 
</comment><comment author="GaelTadh" created="2014-10-10T09:04:05Z" id="58630313">Yeah I'll get it in ASAP, it got a little neglected.
</comment><comment author="javanna" created="2014-10-10T09:06:47Z" id="58630604">thanks @GaelTadh !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`flush` method for BulkProcessor class.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5575</link><project id="" key="" /><description>This is for #5570.

There is no explicit method `flush/execute` in [BulkProcessor](http://javadoc.kyubu.de/elasticsearch/v0.90.0/org/elasticsearch/action/bulk/BulkProcessor.html) class. This can be useful in certain scenarios. Currently it requires to close and create a new BulkProcessor if one wants an immediate flush.
</description><key id="30291879">5575</key><summary>`flush` method for BulkProcessor class.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">kul</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T11:27:31Z</created><updated>2015-06-07T14:57:16Z</updated><resolved>2014-04-02T17:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-27T16:48:56Z" id="38829521">I think you should add a test in addition to it in `BulkTests`?
</comment><comment author="kul" created="2014-03-28T02:46:06Z" id="38884155">Added a test case and doc string for method.
</comment><comment author="s1monw" created="2014-03-28T08:18:21Z" id="38896844">I looked at it quickly and I think you should check if we really need to flush ie. if `bulkRequest.numberOfActions() &gt; 0` ? it also seems that we can move the :

``` Java
if (closed) {
  throw new XZYException("already closed");
}
```

into a separate method called `ensureOpen()` that is called even before we add requests in `internalAdd`?
</comment><comment author="dadoonet" created="2014-03-28T08:19:09Z" id="38896883">Hi @kul 
Could you sign the CLA: http://www.elasticsearch.org/contributor-agreement/ 

Thanks.
</comment><comment author="kul" created="2014-03-28T14:47:33Z" id="38926602">@dadoonet i have signed the CLA.

@s1monw makes sense, i will add another commit to the PR.
</comment><comment author="s1monw" created="2014-04-02T09:25:51Z" id="39307458">I left small comments on the test, just style :) LGTM can you squash the commits and rebase? @dadoonet can you take it form here?
</comment><comment author="kul" created="2014-04-02T12:39:52Z" id="39324797">made the changes and squashed commits. Also made BulkProcessor class implement Closeable.
</comment><comment author="dadoonet" created="2014-04-02T17:31:38Z" id="39359288">Pushed in 1.x and master. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent aggregation support between _search and _msearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5574</link><project id="" key="" /><description>Hi,

With Elasticsearch `1.0.1` (not yet verified with `1.1`), I'm having an issue with aggregations and `_search` vs. `_msearch` APIs.

Here is a simple example : 

```
curl -XPOST http://localhost:9200/hotels/_search -d '{
  "aggs": {
    "filtered_hotels": {
      "filter": {
        "and": [{
          "terms": {
            "kinds": ["HOT", "RES"]
          }
        }]
      }
    }
  },
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "destinations": 3
        }
      }
    }
  },
  "size": 0
}'
```

``` json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 230,
    "max_score": 0.0,
    "hits": []
  },
  "aggregations": {
    "filtered_hotels": {
      "doc_count": 135
    }
  }
}
```

It works fine !

But the same query, sent to the `_msearch` API returns an error : 

```
curl -XPOST http://localhost:9200/hotels/_msearch -d '{"aggs":{"filtered_hotels":{"filter":{"and":[{"terms":{"kinds":["HOT","RES"]}}]}}},"query":{"filtered":{"filter":{"term":{"destinations":3}}}},"size":0}
'
```

``` json
{"error":"and doesn't support arrays"}
```

If I change the query to remove the `and` array (since there is a single filter in it, I also get a similar error:

```
curl -XPOST http://localhost:9200/hotels/_msearch -d '{"aggs":{"filtered_hotels":{"filter":{"terms":{"kinds":["HOT","RES"]}}}},"query":{"filtered":{"filter":{"term":{"destinations":3}}}},"size":0}
'
```

``` json
{"error":"kinds doesn't support arrays"}
```

It seems that arrays are not supported in an `_msearch` request.

Am I doing something wrong?

Thanks for your support.
</description><key id="30291616">5574</key><summary>Inconsistent aggregation support between _search and _msearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels /><created>2014-03-27T11:23:12Z</created><updated>2014-03-27T13:22:11Z</updated><resolved>2014-03-27T13:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jlecour" created="2014-03-27T11:27:21Z" id="38791899">I confirm that it's also relevant to Elasticsearch `1.1`.
</comment><comment author="kzwang" created="2014-03-27T11:32:42Z" id="38792286">I think you are not using the `_msearch` correctly, the body for `search` is different than `_msearch`

see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-multi-search.html
</comment><comment author="jlecour" created="2014-03-27T13:21:51Z" id="38801141">Ok, I've found why there is an error.

The `_msearch` API wants 2 lines per query, even if the index/type is specified in the URL, so I need to add a blank `{}` before my query.

The error was very misleading and the dicumentatdtion was not very cleat either. Sadly I have no idea how to improve them.

```
curl -XPOST http://localhost:9200/hotels/hotel/_msearch -d '{}
{"aggs":{"filtered_hotels":{"filter":{"and":[{"terms":{"kinds":["HOT","RES"]}}]}}},"query":{"filtered":{"filter":{"term":{"destinations":3}}}},"size":0}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting in 1.1 is much slower than in 1.0.1 even with empty result sets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5573</link><project id="" key="" /><description>Consider the next query (no sorting). It returns immediately, probably because filter cache is populated.

```
# curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000}'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

If you just add sorting to this query, it takes almost 6 seconds (vs 1ms without sorting).

```
# curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 5741,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

This started after 1.0.1 -&gt; 1.1.0 upgrade.

We have some daily tasks and tis upgrade was clearly the case for cpu load increase. Update happened around 10AM and daily task is scheduled for 11:30AM.

![cpu](http://puu.sh/7Ll66.png)
</description><key id="30290371">5573</key><summary>Sorting in 1.1 is much slower than in 1.0.1 even with empty result sets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-03-27T11:02:31Z</created><updated>2014-12-09T16:14:44Z</updated><resolved>2014-12-09T16:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-27T11:22:03Z" id="38791515">Is the sorting request still slow if you run it several times? (Just to make sure this is not due to field data loading)
</comment><comment author="bobrik" created="2014-03-27T11:24:40Z" id="38791705">@jpountz yes, even if i run sort/nosort/sort/nosort &#8212; results are the same. `user_id` is mapped as long if it matters.
</comment><comment author="jpountz" created="2014-03-27T11:29:31Z" id="38792049">This is surprising, longs fields are supposed to sort efficiently. Do you have a field data cache limit set that might cause field data to reloaded on each query? If you can run this costly query under a profiler, or just paste the hot-threads output while the query is running, I'd be curious to look at where CPU goes.
</comment><comment author="bobrik" created="2014-03-27T11:48:12Z" id="38793410">I supposed that sorting involved after filtering and with zero result set it should have no overhead.

Filter cache is 3/3gb, field data cache is 0.8/2gb. There were no other search requests when I was checking timings, only indexing @ 400-800 rps.

This is weird that field cache evictions happen (spotted with bigdesk) with sorting, but don't happen without sorting. Last 3 spikes &#8212; sort queries:

![bigdesk](http://puu.sh/7LmDN.png)

Hot threads for many similar queries (different ranges only) below:

```
# curl http://web580:9200/_nodes/hot_threads?threads=10
::: [portraits02][0hLgHUVzQi2IFdNz1jBMfA][web580][inet[/192.168.2.70:9300]]

   94.3% (471.6ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][search][T#4]'
     2/10 snapshots sharing following 36 elements
       org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.docs(BlockTreeTermsReader.java:2221)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:467)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     2/10 snapshots sharing following 34 elements
       org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:79)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:466)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     2/10 snapshots sharing following 38 elements
       org.apache.lucene.util.packed.GrowableWriter.get(GrowableWriter.java:56)
       org.apache.lucene.util.packed.AbstractPagedMutable.get(AbstractPagedMutable.java:89)
       org.apache.lucene.util.LongValues.get(LongValues.java:35)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$OrdinalsStore.addOrdinal(OrdinalsBuilder.java:179)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.addDoc(OrdinalsBuilder.java:325)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:471)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     4/10 snapshots sharing following 32 elements
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

   94.3% (471.5ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][search][T#9]'
     4/10 snapshots sharing following 35 elements
       org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:233)
       org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:79)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:466)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     4/10 snapshots sharing following 38 elements
       org.apache.lucene.util.packed.GrowableWriter.get(GrowableWriter.java:56)
       org.apache.lucene.util.packed.AbstractPagedMutable.get(AbstractPagedMutable.java:89)
       org.apache.lucene.util.LongValues.get(LongValues.java:35)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$OrdinalsStore.addOrdinal(OrdinalsBuilder.java:179)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.addDoc(OrdinalsBuilder.java:325)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:471)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     2/10 snapshots sharing following 32 elements
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    9.3% (46.6ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#8]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    8.2% (40.9ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][refresh][T#3]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:702)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(LinkedTransferQueue.java:1117)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    7.0% (34.9ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][flush][T#3]'
     10/10 snapshots sharing following 15 elements
       java.io.FileDescriptor.sync(Native Method)
       org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:505)
       org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:307)
       org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)
       org.elasticsearch.index.store.Store$StoreDirectory.sync(Store.java:563)
       org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:4462)
       org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2926)
       org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3022)
       org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2989)
       org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:812)
       org.elasticsearch.index.shard.service.InternalIndexShard.flush(InternalIndexShard.java:589)
       org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:194)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    6.8% (33.9ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][refresh][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:702)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(LinkedTransferQueue.java:1117)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    5.8% (28.8ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#2]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    5.2% (26.2ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#1]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    4.9% (24.6ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#7]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    2.9% (14.5ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#6]'
     9/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     unique snapshot
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2741)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2835)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2741)
       org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:746)
       org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:556)
       org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:490)
       org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:215)
       org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:408)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:616)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:469)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
       org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:384)
       org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
```
</comment><comment author="jpountz" created="2014-03-27T12:02:25Z" id="38794499">&gt; I supposed that sorting involved after filtering and with zero result set it should have no overhead.

When sorting, Elasticsearch needs to instantiate a comparator that works on top of field data, before it even starts collecting results. So even when no document matches a query, field data needs to be loaded because the comparator is instantiated up-front.

Your hot-threads output shows stack traces that are definitely related to field data loading. Does the issue still reproduce if you increase the field data cache size?
</comment><comment author="bobrik" created="2014-03-27T12:09:49Z" id="38795057">I cleared caches and this is what bigdesk said:

![bigdesk](http://puu.sh/7Lncd.png)

```
web580 ~ # curl -s -X POST http://web580:9200/_cache/clear
{"_shards":{"total":20,"successful":10,"failed":0}}
web580 ~ # curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 12003,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
web580 ~ # curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 3843,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
web580 ~ # curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 3913,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
web580 ~ # curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 3785,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
web580 ~ # curl -s -X POST http://web580:9200/portraits/user/_search?pretty -d '{"query":{"filtered":{"filter":{"and":[{"bool":{"must":[{"exists":{"field":"total_purchases"}}]}},{"range":{"user_id":{"gte":60600001,"lte":60700000}}}]}}},"size":5000,"sort":{"user_id":"asc"}}'
{
  "took" : 2638,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

As you can see, field cache evictions happen even if field cache is far from being full and search queries remain slow.

Hot threads when just one search query with sorting was issued:

```
# curl http://web580:9200/_nodes/hot_threads?threads=10
::: [portraits02][0hLgHUVzQi2IFdNz1jBMfA][web580][inet[/192.168.2.70:9300]]

   100.3% (501.5ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][search][T#15]'
     3/10 snapshots sharing following 36 elements
       org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.docs(BlockTreeTermsReader.java:2221)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:467)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     4/10 snapshots sharing following 38 elements
       org.apache.lucene.util.packed.GrowableWriter.get(GrowableWriter.java:56)
       org.apache.lucene.util.packed.AbstractPagedMutable.get(AbstractPagedMutable.java:89)
       org.apache.lucene.util.LongValues.get(LongValues.java:35)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$OrdinalsStore.addOrdinal(OrdinalsBuilder.java:179)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.addDoc(OrdinalsBuilder.java:325)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:471)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     3/10 snapshots sharing following 35 elements
       org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:233)
       org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:79)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:466)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

   23.5% (117.5ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][merge][T#4]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:702)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(LinkedTransferQueue.java:1117)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

   12.5% (62.3ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][refresh][T#4]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:702)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(LinkedTransferQueue.java:1117)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    8.0% (39.8ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][flush][T#1]'
     10/10 snapshots sharing following 15 elements
       java.io.FileDescriptor.sync(Native Method)
       org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:505)
       org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:307)
       org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)
       org.elasticsearch.index.store.Store$StoreDirectory.sync(Store.java:563)
       org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:4462)
       org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2926)
       org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3022)
       org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2989)
       org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:812)
       org.elasticsearch.index.shard.service.InternalIndexShard.flush(InternalIndexShard.java:589)
       org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:194)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    6.8% (34ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#8]'
     9/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     unique snapshot
       org.elasticsearch.common.joda.time.format.DateTimeParserBucket.saveField(DateTimeParserBucket.java:247)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$NumberFormatter.parseInto(DateTimeFormatterBuilder.java:1390)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2741)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$MatchingParser.parseInto(DateTimeFormatterBuilder.java:2835)
       org.elasticsearch.common.joda.time.format.DateTimeFormatterBuilder$Composite.parseInto(DateTimeFormatterBuilder.java:2741)
       org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:746)
       org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:556)
       org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:490)
       org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:215)
       org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:408)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:616)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:469)
       org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:517)
       org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:459)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
       org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
       org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:384)
       org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    6.5% (32.2ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#5]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    3.5% (17.3ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#1]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    3.4% (17.2ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#3]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    3.3% (16.6ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#4]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    3.3% (16.5ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][index][T#2]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:706)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(LinkedTransferQueue.java:615)
       org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.take(LinkedTransferQueue.java:1109)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
```

I got it with the next order of request (indexing requests are still in place):
1. Search query issued
2. Hot threads requested
3. Hot threads returned
4. Search query returned after another second
</comment><comment author="bobrik" created="2014-03-27T12:15:22Z" id="38795461">I paused indexing and fired 5 more search requests &#8212; it still takes 2 seconds to respond. Field cache evictions are still in place:

![bigdesk](http://puu.sh/7Lnve.png)

Hot threads should now show only the search query:

```
# curl http://web580:9200/_nodes/hot_threads?threads=10
::: [portraits02][0hLgHUVzQi2IFdNz1jBMfA][web580][inet[/192.168.2.70:9300]]

   100.2% (501.1ms out of 500ms) cpu usage by thread 'elasticsearch[portraits02][search][T#7]'
     4/10 snapshots sharing following 35 elements
       org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:233)
       org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:79)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:466)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     3/10 snapshots sharing following 36 elements
       org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.docs(BlockTreeTermsReader.java:2221)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.apache.lucene.index.FilteredTermsEnum.docs(FilteredTermsEnum.java:186)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:467)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     3/10 snapshots sharing following 38 elements
       org.apache.lucene.util.packed.GrowableWriter.get(GrowableWriter.java:56)
       org.apache.lucene.util.packed.AbstractPagedMutable.get(AbstractPagedMutable.java:89)
       org.apache.lucene.util.LongValues.get(LongValues.java:35)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$OrdinalsStore.addOrdinal(OrdinalsBuilder.java:179)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.addDoc(OrdinalsBuilder.java:325)
       org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:471)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:123)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
       org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
       org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
       org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
       org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
       org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
       org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
       org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
       org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.load(PackedArrayIndexFieldData.java:55)
       org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:66)
       org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
       org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:116)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
       org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.1% (350.7micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (106.6micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:722)

    0.0% (36.6micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][http_server_worker][T#7]{New I/O worker #41}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (33.9micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][http_server_worker][T#9]{New I/O worker #43}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (26.9micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][transport_server_worker][T#15]{New I/O worker #32}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (18.4micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][[timer]]'
     10/10 snapshots sharing following 2 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.threadpool.ThreadPool$EstimatedTimeThread.run(ThreadPool.java:514)

    0.0% (15.8micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][transport_server_worker][T#14]{New I/O worker #31}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (15.5micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][transport_server_worker][T#16]{New I/O worker #33}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    0.0% (15.3micros out of 500ms) cpu usage by thread 'elasticsearch[portraits02][transport_server_worker][T#13]{New I/O worker #30}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
```
</comment><comment author="jpountz" created="2014-03-27T12:15:32Z" id="38795477">&gt; As you can see, field cache evictions happen even if field cache is far from being full and search queries remain slow.

Even if the cache is far from full, this might happen if eg. you have a cache size of 1G, one very large segment that would require 800M of cache and several small segments that require eg. 50M each. During search execution, the loading of the small segments might evict the large one from the cache so field data will need to be loaded on it again on the next search execution even so so cache might appear pretty empty with only 300M used over 1G available.

It might also happen if you configured the cache to use soft references.
</comment><comment author="bobrik" created="2014-03-27T12:30:33Z" id="38796631">I have 80 million users in db and 80 million longs should definitely fit into 2g.

I restarted node with 6g for field cache and surprisingly query with sorting worked quite fast. Then I restarted with 2g again and it works too. Overall `took` time for 780 search requests dropped from 4172486ms to just 3447ms. That is some good news :)

1.0.1 and 1.1.0 have different lucene versions, could it be the reason for misbehaving after first restart? If so, maybe it should be mentioned in release notes.
</comment><comment author="s1monw" created="2014-03-27T12:47:00Z" id="38798021">this might be related to a circuit breaker? I think it kicks in if you have not enough RAM to load a segment and basically gives you partial results. But it tries to load the segment over and over again which might explain what you see?
</comment><comment author="bobrik" created="2014-03-27T13:00:34Z" id="38799176">@s1monw how can it explain working with the same amount of field cache after restart then? Caches were cleaned before restart, segment sizes didn't change (there were no indexing).

Circuit breaker appeared in 1.0 and I must have seen the same behavior before, but it was okay since 1.0.0 and 1.0.1 -&gt; 1.0.2 upgrade was smooth.
</comment><comment author="s1monw" created="2014-03-27T13:05:40Z" id="38799606">yeah so I know of at least on bug that is related to the breaker where it's not reset and that is still present in `1.1`  it's actually a lucene bug and should be fixed int he upcomeing 4.7.1 It might cause the breaker not to be reset and then it will prevent all segments from being loaded. Do you have the breaker stats from that node? it is likely that you run into that bug if you have searches running and you relocate a shard from one node to another that might cause it. The only way to reset it is to restart :/
</comment><comment author="bobrik" created="2014-03-27T13:11:46Z" id="38800127">This particular cluster has only one node. Unfortunately I do not have any breaker stats.
</comment><comment author="bobrik" created="2014-12-09T16:09:19Z" id="66307216">Closing this, 1.4 works fine.
</comment><comment author="s1monw" created="2014-12-09T16:14:44Z" id="66308186">thanks for reporting this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extra "edit" in the documentation's autocomplete results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5572</link><project id="" key="" /><description>Since you've updated your documentation with `Edit` links to the GitHub corresponding resource, the search results are including an extra "edit" string at the end of each path segment.

![](http://jeremy.lecour.fr/tmp/es_search_autocomplete_edit_20140327_115453.png)
</description><key id="30290269">5572</key><summary>Extra "edit" in the documentation's autocomplete results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels /><created>2014-03-27T11:00:48Z</created><updated>2014-03-27T13:51:51Z</updated><resolved>2014-03-27T13:51:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-27T13:51:51Z" id="38804379">Fixed - thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some warnings reported by Findbugs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5571</link><project id="" key="" /><description>Although high-severity bugs were mostly static variables that were not final,
it also found potential NPEs and bugs like `x ^= x;` or equality checks on
objects that don't share a common interface.
</description><key id="30289950">5571</key><summary>Fix some warnings reported by Findbugs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T10:55:10Z</created><updated>2015-06-07T22:02:23Z</updated><resolved>2014-03-31T10:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-27T16:03:26Z" id="38823482">LGTM those are good fixes
</comment><comment author="jpountz" created="2014-03-31T10:37:54Z" id="39074609">Pushed everything but the PRNG bug, that deserves a dedicated PR and more testing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[JAVA-API] BulkProcessor flush/execute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5570</link><project id="" key="" /><description>There is no explicit method `flush/execute` in [BulkProcessor](http://javadoc.kyubu.de/elasticsearch/v0.90.0/org/elasticsearch/action/bulk/BulkProcessor.html) class. This can be useful in certain scenarios. Currently it requires to close and create a new BulkProcessor if one wants an immediate flush.
</description><key id="30288934">5570</key><summary>[JAVA-API] BulkProcessor flush/execute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">kul</reporter><labels /><created>2014-03-27T10:38:37Z</created><updated>2014-04-02T17:30:55Z</updated><resolved>2014-04-02T17:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Moved the decision to load p/c fielddata eagerly to a better place.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5569</link><project id="" key="" /><description /><key id="30279796">5569</key><summary>Moved the decision to load p/c fielddata eagerly to a better place.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T07:50:52Z</created><updated>2015-06-07T12:47:57Z</updated><resolved>2014-04-03T07:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-04-03T07:40:36Z" id="39421251">LGTM, this is a nice cleanup!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add configured thread pool sizes to _cat/thread_pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5568</link><project id="" key="" /><description>Adding support for reporting the `*.min` (`*mi`) and `*.max` (`*ma`) from `ThreadPool#Info`.

Closes #5366
</description><key id="30277569">5568</key><summary>Add configured thread pool sizes to _cat/thread_pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels /><created>2014-03-27T06:52:14Z</created><updated>2014-06-19T16:40:56Z</updated><resolved>2014-04-28T10:03:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T09:08:54Z" id="39709274">Hi @pickypg thanks for opening this PR!

I left an inline comment, also, I wouldn't do output those new values by default.

Would be nice to test this also, you could add a test to https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/cat.thread_pool/10_basic.yaml for instance...ping me if you have any question.
</comment><comment author="pickypg" created="2014-04-08T08:16:25Z" id="39822372">Thanks @javanna. I agree that the values should not be displayed by default. I believe that I have them set that way (it behaves that way) with `default:false`, but let me know if I need to do something else.

While trying to figure out the yaml test script, I re-noticed that the Generic thread pool is the only one without explicitly defined min/max pool sizes. Seeing as it is the generic thread pool, it makes sense to let the JVM do whatever it thinks is best, but I decided this time to mention it.

I'll have more time to figure out the yaml script tomorrow, and I'll push another commit with tests for each pool.
</comment><comment author="pickypg" created="2014-04-09T03:12:09Z" id="39924830">Let me know if you want anything else, @javanna. Thanks
</comment><comment author="javanna" created="2014-04-17T14:53:14Z" id="40722765">Hey @pickypg I looked again at your PR and noticed you changed `ThreadPoolStats` to expose `min` and `max`. This way you copy information over from Info to Stats and affect also the output of the nodes stats api, which should only output dynamic info (not `min` and `max`). You can expose though `min` and `max` (maybe also `queue_size`?) to the cat api just doing `info.getThreadPool` and iterate over the `ThreadPool.Info` objects and read that info from there without needing to copy it.

Sorry for not pointing this out earlier! Great work with docs and tests, ping me if you need any help!
</comment><comment author="pickypg" created="2014-04-22T04:23:40Z" id="41002693">Hi @javanna I was away for the weekend, but I'll take a look at this tomorrow night. I should hopefully have the PR changes tomorrow or the day after.
</comment><comment author="javanna" created="2014-04-22T09:50:31Z" id="41021787">Hey @pickypg no worries, take all the time you need ;)
</comment><comment author="pickypg" created="2014-04-23T06:36:18Z" id="41129254">Hey @javanna, I made the suggested changes; they are much better.

I looked at removing the queue size from the passed-in stats as well, but those are the live queue sizes rather than the configured ones. As such, I went ahead and added the configured maximum _and_ the keep alive times as well. Let me know. Thanks.
</comment><comment author="javanna" created="2014-04-23T13:03:35Z" id="41157720">Looks much better @pickypg ! 

Thanks for updating, we are really close, I left a small comment. Also, it might make sense to add the thread_pool type as well which is the only missing bit at this time I think? And let's be careful with `-1` values, they get shown from time to time (e.g. `max` and `min` in the `generic` one), we might want to hide those when not relevant, as we do in nodes info api.
</comment><comment author="pickypg" created="2014-04-24T03:33:47Z" id="41239545">@drewr Changed.

@javanna Let me know if there is anything else; I was not sure how "internal" the type was, but I have added it now. Thanks
</comment><comment author="javanna" created="2014-04-28T10:03:04Z" id="41541566">Merged, thanks @pickypg ! FYI I fixed a couple of links in the docs and squashed your commits into a single one before pushing ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update nodes documentation with all headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5567</link><project id="" key="" /><description>Adds a table with the exhaustive list of all available headers with a brief description (mostly from `org.elasticsearch.rest.action.cat.RestNodesAction`) so that people do not need to go searching for them in the code like I did, or search through `nodes?help`.

Closes #5580 
</description><key id="30274508">5567</key><summary>[DOCS] Update nodes documentation with all headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">pickypg</reporter><labels /><created>2014-03-27T05:20:51Z</created><updated>2014-07-16T21:47:14Z</updated><resolved>2014-05-07T16:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:48:34Z" id="42297520">@drewr Sorry i meant this PR - please review and merge if ok
</comment><comment author="drewr" created="2014-05-07T16:19:37Z" id="42448669">Merged in 12f758e811d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exposed a PluginManager.isPluginInstalled(String name) method for easier embedded use</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5566</link><project id="" key="" /><description>Exposed a PluginManager.isPluginInstalled(String name) method to make it a bit cleaner using the PluginManager embedded (when running elastic embedded).

Did a bit of a refactor to reduce some duplication (that the above change introduced - and little bit of existing).
Not sure if it was intentional - but there was a bit of mixed usage of IllegalArgumentException and ElasticsearchIllegalArgumentException - so I standardised on ElasticsearchIllegalArgumentException.
There was also some validation on the name when removing a plugin - that is worthwhile having when installing a plugin - so I made that common.
Issue #5565
</description><key id="30273714">5566</key><summary>Exposed a PluginManager.isPluginInstalled(String name) method for easier embedded use</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-03-27T04:53:26Z</created><updated>2014-06-12T13:37:49Z</updated><resolved>2014-04-29T10:55:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-04-29T10:55:03Z" id="41662908">Hey @nickminutello!

Thanks for the PR. I rebased it on latest master branch and added a new commit to make `Plugin` inner class static as discussed with @pickypg in this thread. I think this make sense.

Closing this PR in favor of #5977. Let's continue this thread there if needed.

Thanks again!
</comment><comment author="spinscale" created="2014-05-08T14:40:26Z" id="42557782">Hey @nickminutello 

I just took a look at this PR and have a couple of questions. You said you are running elasticsearch in embedded mode. This means it is easy for you to access the running instance. Wouldnt it be easier, if you ran the nodes info API and checked the plugin results in there to find out which plugins have been loaded.

What sounds like a huge task, compared to your implementation, actually has one huge benefit: If the loading of a plugin fails (maybe due to missing dependencies), your implemented method will return true, because the directory exists. However the nodes info response will include the expected information.

Does that make sense or do you have some specific use-case, which I am not seeing at the moment and where your approach makes more sense?
</comment><comment author="nickminutello" created="2014-06-09T17:46:45Z" id="45520501">Hi @spinscale, sorry I didnt see your question till now.
I understand that using the admin api would give more "correct" results, however, I want to install the plugin _before_ Elastic starts.
This is mostly used as an dev-environment auto-configuration step. I want to make sure that the marvel plugin is automatically installed without anyone having to run a seperate step.

For prod-env deployments, we will have to include the plugin in the deployment zip, since elastic wont have access to the internet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make it easier to use PluginManager programmatically - specifically query if a plugin is already installed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5565</link><project id="" key="" /><description>I am running elastic embedded - and I want to also run the PluginManager from within my code.

Its straightforward - but a handy enhancement would be to be able to ask the plugin manager if a plugin is already installed - just so I dont try install it if its already there and get an error logged. You could list the plugins - but the filesystem format is quite different from the plugin name format.

I've made the change - I'll create a pull-request.
</description><key id="30273692">5565</key><summary>Make it easier to use PluginManager programmatically - specifically query if a plugin is already installed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-03-27T04:52:44Z</created><updated>2014-04-29T10:56:50Z</updated><resolved>2014-04-29T10:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-27T05:58:08Z" id="38771518">Why not using Nodes Info API? You will have a cluster wide overview.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-info.html#cluster-nodes-info
</comment><comment author="nickminutello" created="2014-03-27T09:22:15Z" id="38782071">Elastic won't necessarily be running.

At the moment I am installing the plugin on startup - before elastic is started. My elastic config is all in code - so regular cmd-line won't work. 
</comment><comment author="dadoonet" created="2014-04-29T10:56:50Z" id="41663025">Closing as we have a PR for it: #5977 (was #5566)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dynamic_type in dynamic_template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5564</link><project id="" key="" /><description>closes #5256
</description><key id="30273436">5564</key><summary>Fix dynamic_type in dynamic_template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T04:42:47Z</created><updated>2015-06-07T22:56:29Z</updated><resolved>2014-03-28T13:57:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-28T13:57:03Z" id="38921073">Thanks a lot @kzwang! Merged to master and backported to 1.x and 1.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add suggest stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5563</link><project id="" key="" /><description>closes #4032
</description><key id="30264919">5563</key><summary>Add suggest stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Stats</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T00:34:30Z</created><updated>2015-06-07T16:24:24Z</updated><resolved>2014-07-04T07:25:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-27T09:09:32Z" id="38781144">I think this looks great! What I'd really like is a test a that checks if the suggest stats are really filled nothing crazy just a santify check. @spinscale can you take a look at this as well?
</comment><comment author="spinscale" created="2014-03-27T09:17:56Z" id="38781743">just had quick look, looks nice (will dig deeper into this later)! A couple of tests (filling/increasing numbers and that no NPEs happen if the stats are empty) are needed, but apart from that great job!
</comment><comment author="kzwang" created="2014-03-27T09:20:58Z" id="38781988">@s1monw @spinscale I'll add some tests for this
</comment><comment author="kzwang" created="2014-03-27T10:10:05Z" id="38785993">@s1monw @spinscale I've made the change as you suggested and added a test
</comment><comment author="s1monw" created="2014-03-27T13:43:14Z" id="38803461">IMO that last commit looks pretty good though. Yet one thing that is not really obvious is that you should adjust the [REST spec](https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/indices.stats.json) for this API as well. There is a block that looks like this:

``` Json

 "metric" : {
          "type" : "list",
          "options" : ["_all", "completion", "docs", "fielddata", "filter_cache", "flush", "get", "id_cache", "indexing", "merge", "percolate", "refresh", "search", "segments", "store", "warmer"],
          "description" : "Limit the information returned the specific metrics."
        }

```

that you need to adjust and add `suggest` I guess.
</comment><comment author="kzwang" created="2014-03-27T22:04:44Z" id="38866699">@s1monw Added `suggest` to indices stats and nodes stats REST spec
</comment><comment author="s1monw" created="2014-07-04T07:25:30Z" id="48015604">this one has been merged a while ago but never closed see ceed22fe00341acf8a23de8c4ed67027d90ba6bf
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid Math.abs(int/long).</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5562</link><project id="" key="" /><description>We have had a couple of bugs because of the use of these methods without paying
attention that it might return a negative value when provided with MIN_VALUE.
There is one common and legitimate usage of this method in order to perform
a modulo operation which would always return a positive number. This use-case
has been extracted to MathUtils.mod.
</description><key id="30264230">5562</key><summary>Forbid Math.abs(int/long).</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-27T00:18:56Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-03-27T13:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-27T09:58:31Z" id="38785017">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix IndexShardRoutingTable's shard randomization to not throw out-of-bounds exceptions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5561</link><project id="" key="" /><description>I initially wanted to make the diff minimal but this ended up being quite complicated
so I finally refactored a bit the way shards are randomized. Yet, it uses the same logic:
- rotations to shuffle shards,
- an AtomicInteger to generate the distances to use for the rotations.

Close #5559
</description><key id="30262809">5561</key><summary>Fix IndexShardRoutingTable's shard randomization to not throw out-of-bounds exceptions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v0.90.14</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T23:47:39Z</created><updated>2015-06-07T22:03:44Z</updated><resolved>2014-03-27T13:49:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-27T09:35:05Z" id="38783026">this looks great - I left some commetns
</comment><comment author="jpountz" created="2014-03-27T13:23:22Z" id="38801296">@s1monw I pushed a new commit
</comment><comment author="s1monw" created="2014-03-27T13:31:27Z" id="38802172">LGTM
</comment><comment author="jpountz" created="2014-03-27T13:49:52Z" id="38804155">Thanks Simon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move to Java 1.7 on 1.x branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5560</link><project id="" key="" /><description>This PR moves the 1.x branch to Java 1.7 as min version. This means Elasticsearch 1.2 has Java 7 as min version.
</description><key id="30227694">5560</key><summary>Move to Java 1.7 on 1.x branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-03-26T16:32:19Z</created><updated>2014-07-16T21:47:16Z</updated><resolved>2014-03-27T10:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IndexShardRoutingTable might barf if it has handled lots of searches...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5559</link><project id="" key="" /><description>A user reported this on the ML:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/AxRU1UQP24U

```
Caused by: java.lang.IndexOutOfBoundsException: index (-2) must not be negative
        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:306)
        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:285)
        at org.elasticsearch.common.collect.RegularImmutableList.get(RegularImmutableList.java:65)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferNodeActiveInitializingShardsIt(IndexShardRoutingTable.java:378)
        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.preferenceActiveShardIterator(PlainOperationRouting.java:210)
        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.getShards(PlainOperationRouting.java:80)
        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:80)
        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:42)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.&lt;init&gt;(TransportShardSingleOperationAction.java:121)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.&lt;init&gt;(TransportShardSingleOperationAction.java:97)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:74)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:49)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:85)
        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:174)
        ... 9 more
```

the code that causes this is:

``` Java

  private int pickIndex() {
        return Math.abs(counter.incrementAndGet());
  }
```

which might return a negative number. `Math.abs()` returns `-1` for `Integer.MIN_VALUE` which causes the AIOOB mentioned above. The usage of this method seems to be pretty broken along those lines and we might need to think about fixing this generally...
</description><key id="30224106">5559</key><summary>IndexShardRoutingTable might barf if it has handled lots of searches...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.14</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T15:54:14Z</created><updated>2014-03-27T13:49:11Z</updated><resolved>2014-03-27T13:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Disable _id.path by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5558</link><project id="" key="" /><description>If you index a document like this:

```
POST /test/test
{ "foo": "bar", "_id": 2 }
```

it will throw an exception saying:

```
Provided id [upGeHLkQQv-aclC7mF6EWQ] does not match the content one [2]
```

In other words, it is using a `path` for the `_id` field, even though we haven't specified one, and there is no way to disable this.

I think that we should only do this check if the `path` parameter on the `_id` mapping is explicitly set. 
</description><key id="30222870">5558</key><summary>Disable _id.path by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-03-26T15:42:03Z</created><updated>2014-12-30T14:50:12Z</updated><resolved>2014-12-30T14:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Diolor" created="2014-05-21T11:16:21Z" id="43742015">Related, in 1.1.1.

If you index a document like:

```
POST test/test
{
  "_id": {
    "$oid": "5303daa1c36b445138605a0c"
  },
  "event_id": {
    "$oid": "537c2a0ebb20c7336f5e4aca"
  },
  "start_date": "2014-05-24T20:00:00-04:00",
  "country": "Canada"
}
```

The auto-generated mapping will be:

```
{
  "test": {
    "mappings": {
       "test": {
          "properties": {
             "$oid": {
                "type": "string"
             }
          }
       }
    }
  }
}
```

It will normalize the `_id` field and will ignore the other fields.
</comment><comment author="clintongormley" created="2014-12-30T14:50:12Z" id="68361977">Closing in favour of #6730 and #9059
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Eager fielddata loading not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5557</link><project id="" key="" /><description>Fixes a bug when the field has been configured with eager fielddata loadng, the fielddata is not being loaded.

Note this bug only exists in ES version: `1.1.0`
</description><key id="30222532">5557</key><summary>Eager fielddata loading not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T15:38:44Z</created><updated>2015-06-07T12:47:57Z</updated><resolved>2014-03-27T05:11:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-26T15:52:46Z" id="38701080">Thanks so much @martijnvg for adding a test, +1 to push!
</comment><comment author="s1monw" created="2014-03-26T22:11:40Z" id="38746872">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search_template does not support ?source=</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5556</link><project id="" key="" /><description>All GET-with-body endpoints should support passing the body as the `source` parameter in the query string.  `search_template` does not currently support this
</description><key id="30221602">5556</key><summary>search_template does not support ?source=</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T15:29:17Z</created><updated>2014-03-31T09:51:18Z</updated><resolved>2014-03-31T09:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-28T15:21:08Z" id="38930888">Fails with:

```
[2014-03-28 16:18:29,962][DEBUG][action.search.type       ] [Ape-Man] [test][4], node[FKDjNLD9RFq2-Mm6CgTADw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@52514fbf] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [Ecstasy][inet[/192.168.5.102:9301]][search/phase/query]
Caused by: org.elasticsearch.search.SearchParseException: [test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"params":{"template":"all"},"template":{"query":{"match_{{template}}":{}}}}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:623)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:612)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.search.SearchParseException: [test][4]: from[-1],size[-1]: Parse Failure [No parser for element [params]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:620)
    ... 9 more
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CountRequestBuilder toString()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5555</link><project id="" key="" /><description>SearchRequestBuilder has a friendly toString method, but not same for CountRequestBuilder.
Will be a good to have feature for debugging.
</description><key id="30220870">5555</key><summary>CountRequestBuilder toString()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">sangitabhagat</reporter><labels><label>enhancement</label></labels><created>2014-03-26T15:21:17Z</created><updated>2015-04-28T09:22:39Z</updated><resolved>2015-04-28T09:22:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-27T12:39:58Z" id="38797361">I agree it would be good to have, on the other hand looking at how the current `toString` works in `SearchRequestBuilder` I found #5576, which is probably even worse than not having the `toString` at all. I'll wait a bit to see what others think and see how we can fix that one and whether we can do the same here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count Per Index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5554</link><project id="" key="" /><description>SearchResponse provides the index name in the SearchHit.
But CountResponse does not provide index name even if we count across multiple incides. 
Could be a useful feature.
</description><key id="30220783">5554</key><summary>Count Per Index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sangitabhagat</reporter><labels><label>non-issue</label></labels><created>2014-03-26T15:20:25Z</created><updated>2014-04-15T12:55:47Z</updated><resolved>2014-04-15T12:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-26T15:34:27Z" id="38698551">I don't think this is anything we can fix. if you count across indices which one should we respond with? I think you know which ones you counted on so you can tell. It's really only relevant if you have search hits no?
</comment><comment author="sangitabhagat" created="2014-03-26T16:06:40Z" id="38702968">That's the point. We shouldn't be responding with just one count. It should return counts per index
To explain the use-case. We need to search across multiple data-sources. But at once only grid for one data source is active. The rest should only display counts.
</comment><comment author="s1monw" created="2014-03-26T16:08:41Z" id="38703240">can you use `_msearch` with `search_type=count`?
</comment><comment author="sangitabhagat" created="2014-03-26T16:09:44Z" id="38703393">yes ... that's one way. Just that search and count api's are inconsistent. One returns the index name other doesnt
</comment><comment author="s1monw" created="2014-03-26T16:12:38Z" id="38703789">I don't think this is an inconsistency. We merge results in both cases and return a merged result. For Search that is the top N hits. For count that is the sum of the shard counts. I think this is the entire point of this API to  return the aggregated result. For counts per index I think we might can add special options to the endpoint but I am really not sure if we should do so. Doing this from the client seems very straight forward. The corresponding API for what you are looking for is `_msearch`
</comment><comment author="s1monw" created="2014-04-15T12:55:47Z" id="40477621">I am closing this for now - the functionality to do this is there and it seems to be consistent IMO. We can still reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert TermQuery to PrefixQuery if PHRASE_PREFIX is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5553</link><project id="" key="" /><description>We miss to add a single term to a prefix query if the query in only a
single term.

Closes #5551
</description><key id="30214640">5553</key><summary>Convert TermQuery to PrefixQuery if PHRASE_PREFIX is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T14:16:14Z</created><updated>2015-06-07T22:05:23Z</updated><resolved>2014-03-26T15:49:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-26T15:13:11Z" id="38695687">Apart from a minor indentation issue, the fix looks good to me.

+1 to merge it!
</comment><comment author="s1monw" created="2014-03-26T15:14:57Z" id="38695910">pushed an update
</comment><comment author="jpountz" created="2014-03-26T15:34:31Z" id="38698559">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Generated mapping does not contain type : "object"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5552</link><project id="" key="" /><description>Insert a document

&lt;pre&gt;
PUT index/Type/1
{
    "objProperty" : {
        "innerProp" : "value"
    }
}
&lt;/pre&gt;

Check the generated mapping 

&lt;pre&gt;
GET index/Type/_mapping
&lt;/pre&gt;

The generated mapping for the type : 

&lt;pre&gt;
       "Type": {
            "properties": {
               "objProperty": {
                  "properties": {
                     "innerProp": {
                        "type": "string"
                     }
                  }
               }
            }
         }
&lt;/pre&gt;

The objProperty element should also have a "type" : "object" property apart from "properties".
</description><key id="30214530">5552</key><summary>Generated mapping does not contain type : "object"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaxanean</reporter><labels /><created>2014-03-26T14:15:07Z</created><updated>2014-03-26T16:05:25Z</updated><resolved>2014-03-26T15:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-26T15:31:31Z" id="38698175">This isn't a bug. Type `object` is the default, which is why it is not mentioned or needed.
</comment><comment author="abaxanean" created="2014-03-26T16:05:25Z" id="38702812">It would be useful to always have the type attribute in order to avoid guessing it. If definitely no, it's worth fixing the documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-object-type.html. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>match_phrase_prefix broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5551</link><project id="" key="" /><description>This breaks in 1.1:

Creation of an index and one document in it

``` Json
curl -XPOST 'localhost:9200/twitter/tweet/1' -d '
{
"user" : "kimchy",
"post_date" : "2009-11-15T14:12:12",
"message" : "trying out Elasticsearch"
}'
```

Following will work on 1.0.2 but fail on 1.1.0

``` Json
curl -XPOST 'localhost:9200/twitter/tweet/_search' -d '
{
"query": {
"match_phrase_prefix": {
"message": "try"
}
}
}'
```
</description><key id="30214359">5551</key><summary>match_phrase_prefix broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T14:12:40Z</created><updated>2014-07-07T15:38:52Z</updated><resolved>2014-03-26T15:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Spec query string params for search_template </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5550</link><project id="" key="" /><description>The current JSON spec for the `search_template` endpoint doesn't list any query string parameters, eg `routing` etc.

https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/search_template.json

Should these parameters be the same as for the `search` endpoint?
</description><key id="30210261">5550</key><summary>Spec query string params for search_template </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-03-26T13:21:08Z</created><updated>2014-04-02T22:04:29Z</updated><resolved>2014-04-02T22:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-26T13:23:06Z" id="38682098">Also, the docs param should point to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Assorted fixes for bugs in the PagedBytesReference tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5549</link><project id="" key="" /><description>Various fixes for the PBR tests, plus a typo where the wrong variable was used for a calculation.
</description><key id="30209651">5549</key><summary>Assorted fixes for bugs in the PagedBytesReference tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T13:12:40Z</created><updated>2015-06-07T22:05:41Z</updated><resolved>2014-03-26T13:28:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-26T13:21:43Z" id="38681953">The only really interesting fix here is the typo in countRequiredBuffers(), all others are just wrong test assumptions where the test generated inappropriate random boundaries for the specific test method.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportMasterNodeOperationAction: tighter check for postAdded cluster state change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5548</link><project id="" key="" /><description>If a node fails to forward a master node operation to the current master, it will schedule a retry using a listener for cluster state changes. Once the listener is in place (and future changes are guaranteed to be observed) it will double check nothing has change during the addition of the listener. This check has previously been change to use cluster state versions (see: #5499). This is however not reliable solution as master elections (which change the master) do not increment the cluster state version and thus could be missed. This commit changes the check to use reference equality making it stricter.
</description><key id="30208938">5548</key><summary>TransportMasterNodeOperationAction: tighter check for postAdded cluster state change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T13:01:53Z</created><updated>2015-06-07T22:07:06Z</updated><resolved>2014-03-31T11:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-31T11:59:45Z" id="39080243">Pushed to  master (3f37a0ff5c1fac3da95d024044c5754ba0c13d15) and 1.x (cc14c40143351bbd887267e63dd353c6cb2c14fa)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding Kopf to community list of monitoring tools.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5547</link><project id="" key="" /><description>Adding versatile monitoring and administration tool Kopf to the community section of the documentation.
</description><key id="30205148">5547</key><summary>Adding Kopf to community list of monitoring tools.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">geekpete</reporter><labels /><created>2014-03-26T11:59:48Z</created><updated>2014-07-16T21:47:18Z</updated><resolved>2014-03-27T16:08:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2014-03-26T12:41:07Z" id="38678407">:+1: 
</comment><comment author="ryanxdrake" created="2014-03-27T06:07:42Z" id="38771875">This is awesome! Should definitely be merged in!
</comment><comment author="s1monw" created="2014-03-27T08:57:56Z" id="38780347">+1 @geekpete can you sign the CLA then I can pull this in.
</comment><comment author="geekpete" created="2014-03-27T09:40:05Z" id="38783385">I've completed the CLA, merge away :)
</comment><comment author="s1monw" created="2014-03-27T16:08:32Z" id="38824175">pushed to master and 1.x thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolate response enrichment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5546</link><project id="" key="" /><description>Hey guys,

I am actually working with percolation API and I noticed, from practice and documentation that percolate response only contains ID of the queries that match the percolated document. Is there any particular reason for such a few information ?
From my point of view, percolation can be considered as a specific search on queries and as search API return the whole matching documents and additional information, why shouldn't it be the same for percolation, returning matching queries and additional information ? Is there any particular reason ? 
This would prevent user from having to perform an additional get on percolation index to retrieve his needed information.

For a registered query like this : 

```
curl -XPUT localhost:9200/_percolator/test/testquery -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    },
    "login" : "XXXXXX",
    "data" : "abcd"
    }
}'
```

Response could return something like this : 
{"ok" : true, "matches" : [{"_id" : "testquery", "login" : "XXXXXX", "data" : "abcd"}]}
</description><key id="30200969">5546</key><summary>Percolate response enrichment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tpenne</reporter><labels><label>:Percolator</label><label>adoptme</label><label>enhancement</label></labels><created>2014-03-26T10:47:57Z</created><updated>2014-12-30T14:44:10Z</updated><resolved>2014-12-30T14:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-31T06:08:29Z" id="39056411">The percolator by default returns all the matching queries and mainly for this reason it just returns the id. Retrieving the `_source` or just a field for all matches can be expensive. Although in 1.x percolator there is `size` parameter, returning just ids has always been the case.

I think it make sense to also allow the _source or specific fields to be returned in the percolator via options similar to what is is the search api, like what is suggested in #4317.
</comment><comment author="clintongormley" created="2014-12-30T14:44:10Z" id="68361478">Closing in favour of #4317
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Removed support for aliases as part of index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5545</link><project id="" key="" /><description>Now that we have explicit support for aliases when creating indices and as part of index templates, we may remove support for aliases (only names) as part of index settings. The change is marked as breaking as the following calls:

```
curl -XPUT localhost:9200/index -d '{
  "settings" : {
    "aliases" : [ "alias1"]
  }
}
```

and

```
curl -XPUT localhost:9200/index -d '{
  "settings" : {
    "index.aliases" : [ "alias1"]
  }
}
```

won't be supported anymore and will need to be replaced with

```
curl -XPUT localhost:9200/index -d '{
  "aliases" : {
    "alias1": {}
  }
}
```
</description><key id="30200098">5545</key><summary>REST API: Removed support for aliases as part of index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>breaking</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T10:33:06Z</created><updated>2015-06-06T17:03:27Z</updated><resolved>2014-08-01T12:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-04-11T14:26:31Z" id="40209121">LGTM, I wonder if this can be back ported to 1.x as well, since we stopped communicating setting based alias creation since 0.90
</comment><comment author="javanna" created="2014-04-17T14:28:02Z" id="40719750">+1 for backporting as the alias as part of settings feature was quite hidden and not promoted. Would be interesting to know how many users rely on it and would need to change their code due to this change.
</comment><comment author="clintongormley" created="2014-07-04T12:10:28Z" id="48036437">@javanna this should be merged, no?
</comment><comment author="javanna" created="2014-07-09T15:23:17Z" id="48488101">hey @clintongormley yes it should be merged, we just have to decide whether we want to backport it to `1.x` too or not...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add human readable `start_time` and `refresh_interval`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5544</link><project id="" key="" /><description>closes #5280
</description><key id="30199505">5544</key><summary>Add human readable `start_time` and `refresh_interval`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Stats</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T10:23:45Z</created><updated>2015-06-06T17:06:08Z</updated><resolved>2014-03-28T11:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-27T12:57:40Z" id="38798915">I like the change! The only thing we need to discuss is that it breaks backwards compatibility, but I don't see a way around it honestly.
</comment><comment author="s1monw" created="2014-03-27T12:59:35Z" id="38799091">IMO we should break BW compat here, what do others think @kimchy @imotov @jpountz 
</comment><comment author="bleskes" created="2014-03-27T13:27:53Z" id="38801810">I think we can change it. My gut feeling is that this number is hardly monitored/used by programs and changing it will make it consistent with the other time values .
</comment><comment author="kimchy" created="2014-03-27T23:46:06Z" id="38874948">++ on getting this in
</comment><comment author="javanna" created="2014-03-28T11:33:20Z" id="38910020">Merged, thanks @kzwang and marked the original issue as breaking
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for zero-sized content throwing off toChannelBuffer().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5543</link><project id="" key="" /><description>Also short-circuit writeTo(..) accordingly to avoid unnecessary work. This fixes an issue where 0-sized ContentLength replies (e.g. 404) would make some REST tests fail.
</description><key id="30196098">5543</key><summary>Fix for zero-sized content throwing off toChannelBuffer().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T09:30:20Z</created><updated>2015-06-07T22:08:40Z</updated><resolved>2014-03-26T09:49:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-26T09:32:58Z" id="38663941">Should we start using PagedBytesReference again in BytesStreamOutput?
</comment><comment author="hhoffstaette" created="2014-03-26T09:38:17Z" id="38664363">@jpountz Not sure I understand the question..where else would we use it?
</comment><comment author="jpountz" created="2014-03-26T09:42:43Z" id="38664703">Oops, I had not seen that you already switched back to PagedBytesReference from BytesArray in BytesStreamOutput. All right!

+1 to merge this change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Moved wipe* methods, randomIndexTemplate &amp; ensureEstimatedStats to TestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5542</link><project id="" key="" /><description>This is the first to make it possible to have a different impl of TestCluster (e.g. based on an external cluster) that has the same methods but a different impl for them (e.g. it might use the REST API to do the same instead of the Java API)
</description><key id="30195626">5542</key><summary>[TEST] Moved wipe* methods, randomIndexTemplate &amp; ensureEstimatedStats to TestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.14</label><label>v1.0.3</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T09:22:25Z</created><updated>2014-06-14T04:18:41Z</updated><resolved>2014-03-26T16:36:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-26T11:24:22Z" id="38672634">I left some comments but this LGTM in general
</comment><comment author="javanna" created="2014-03-26T13:24:10Z" id="38682200">I just rebased and applied changes according to the review.
</comment><comment author="s1monw" created="2014-03-26T13:29:28Z" id="38682741">one small comments otherwise LGTM
</comment><comment author="javanna" created="2014-03-26T15:49:13Z" id="38700578">Moved `assertAllSearchersClosed` and `assertAllFilesClosed` to `ElasticsearchAssertions`. They are both called from both `ElasticsearchTestCase#beforeClass` and `ElasticsearchIntegrationTest#after` (through `TestCluster#assertAfterTest`).
</comment><comment author="s1monw" created="2014-03-26T16:26:11Z" id="38705625">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Could not get correct results when using percolator api?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5541</link><project id="" key="" /><description>Dear all,

I need some help, please..

Our project use percolator in es for subscribe feature since 0.9 version and it works well.

The code is like follows:
#### register query step:

_0.9version_

`$url = Config::get("env.searcher.cluster") . '_percolator/subscribe/' . $subscribe;`
#### percolator step:

_0.9version_

`$url = Config::get("env.searcher.cluster") . 'subscribe/Subscribe/_percolate';`

And before we use percolator, first we would create the index and set up the mapping.

Last week, we upgrade our es to 1.0.1 version.

Just like what we did before, we create index subscribe and set up the mapping.

The new code looks like as follows:
#### register query step:

_1.0.1version_

`$url = Config::get("env.searcher.cluster") . 'subscribe/.percolator/' . $subscribe;`
#### percolator step:

_1.0.1version_

`$url = Config::get("env.searcher.cluster") . 'subscribe/.percolator/_percolate?percolate_format=ids';`

We could register successfully. But we failed to get correct percolator result. Actually we get empty results.

Next , we try to create 'subscribe' index automatically instead of creating manually.

Still, we register successfully. And we could get some results back by percolator.

But, if there existed some queries registered as 

![screen shot 2014-03-26 at 4 46 50 pm](https://f.cloud.github.com/assets/1154775/2522641/3ef13028-b4c4-11e3-85a1-f56704c72b91.png)

We could not get the correct result.

The same query works fine in search api.

Any suggestions?

Thanks.
</description><key id="30194407">5541</key><summary>Could not get correct results when using percolator api?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kinghack</reporter><labels /><created>2014-03-26T09:00:12Z</created><updated>2014-12-30T14:38:08Z</updated><resolved>2014-12-30T14:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-26T09:08:30Z" id="38662170">I seems that the type is wrong in your percolate call? You should use the type of the document being percolated instead of the `.percolate` type.

Can you share curl call instead of code samples? This makes it easier to see what your doing. I'm unfamiliar with the api calls / code that you're sharing.
</comment><comment author="Kinghack" created="2014-03-27T00:51:45Z" id="38758376">Hi, martijnvg, Thanks for reply.

I will try the curl call later.

And maybe I could describe our situation more detail first.

We have an index named _info_ which includes a type name _message_.

All the content is built into _info_ with type _message_.

Now the subscribe system needs to register query into a new index names _subscribe_  as a type _.percolator_. And this works fine.

When we used percolator api, 

The api 

`curl -XGET 'localhost:9200/indexname/typeThatDocWouldParsed/_percolate' -d '{
    "doc" : {
        "message" : "A new bonsai tree in the office"
    }
}'`

The second parameters I would regard it as _the type that the doc would parsed_. So here we set it as _message_.

But my question is , how the subscribe index know the type _message_ which includes in index _info_?

Is that means that I should build index manually and set mapping with type _message_ into _subscribe_?

Thanks.

And I will test curl call today.
</comment><comment author="Kinghack" created="2014-03-27T03:34:49Z" id="38766061">Hi, I have process the curl call.

First, create index:

step 1:
`curl -XPUT http://192.168.2.14:9200/"subscribe" -d @es_metadata_basic.json`

[es_metadata_basic.json](https://gist.github.com/Kinghack/9799101)

get result: 

`{"acknowledged":true}`

step 2:

`curl -XPUT http://192.168.2.14:9200/"subscribe/Ad/_mapping" -d @es_metadata_subscribe.json`

[es_metadata_subscribe.json](https://gist.github.com/Kinghack/9799129)

get result:

{"acknowledged":true}

step 3:

register one query:

`curl -XPOST http://192.168.2.14:9200/"subscribe/.percolator/1111" -d @query1.json`

[query1.json](https://gist.github.com/Kinghack/9799283)

get result:

`{"_index":"subscribe","_type":".percolator","_id":"1111","_version":1,"created":true}`

step 4:

percolator:

`curl -XPOST http://192.168.2.14:9200/"subscribe/.percolator/_percolate?percolate_format=ids" -d @doc1.json`

[doc1.json](https://gist.github.com/Kinghack/9799479)

result:

`{"took":2,"_shards":{"total":10,"successful":10,"failed":0},"total&#8221;:1,"matches":["1111"]}`

step 5:

register another query:

`curl -XPOST http://192.168.2.14:9200/"subscribe/.percolator/2222" -d @query2.json`

[query2.json](https://gist.github.com/Kinghack/9799541)

get results:

`{"_index":"subscribe","_type":".percolator","_id":"2222","_version":1,"created":true}`

step 6:

percolator:

`curl -XPOST http://192.168.2.14:9200/"subscribe/.percolator/_percolate?percolate_format=ids" -d @doc2.json`

[doc2.json](https://gist.github.com/Kinghack/9799563)

result:

`{"took":2,"_shards":{"total":10,"successful":10,"failed":0},"total&#8221;:1,"matches":["1111"]}`

There is no 2222 matches. But the same query could work in search api.
</comment><comment author="martijnvg" created="2014-04-01T08:03:09Z" id="39179794">I just looked into this. Something isn't right, I would expect the second query to match as well. Not sure what is the cause yet.
</comment><comment author="martijnvg" created="2014-04-11T05:36:51Z" id="40172225">The issue here is that the percolator api can't resolve the field type in the mapping for field `\u4ef7\u683c`, because for the type `.percolator` there is no such configured. What then happens is that ES tries to determine what type a field has and for floating point numbers a double field type is always picked

However when indexing percolator query with id `2222`, the field `\u4ef7\u683c` can be resolved because it uses query parsing which under the hood uses smart field resolving and tries to find fields in other mappings of other types. The percolate api doesn't use smart field resolving, because the type can be specifically specified. In this case it finds the float field type you have configured. The root cause of the percolate query `2222` not matching is, that for the long field a sub query is generated that only works on a float, but in document being percolate for that particular field a long value is generated. This causes the range query not the match and therefore the entire percolator query doesn't match.  

You shouldn't use the `.percolator` type in the percolate api, because the mapping for your document being percolated doesn't exist there. Instead you should use the type `Ad` instead. So the last percolate request should look like this instead:
`curl -XPOST http://192.168.2.14:9200/"subscribe/Ad/_percolate?percolate_format=ids" -d @doc2.json`
</comment><comment author="Kinghack" created="2014-05-20T09:03:13Z" id="43602013">Thanks @martijnvg . I just check your kindly and helpful reply.
I would look into this issue as you suggested. 
</comment><comment author="clintongormley" created="2014-12-30T14:38:08Z" id="68360981">No more info.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Capture and set start time in Delete By Query operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5540</link><project id="" key="" /><description>This is important for queries/filters that use `now` in date based queries/filters
</description><key id="30193913">5540</key><summary>Capture and set start time in Delete By Query operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T08:50:52Z</created><updated>2015-06-07T14:57:43Z</updated><resolved>2014-03-26T12:33:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-26T09:00:06Z" id="38661613">LGTM
</comment><comment author="s1monw" created="2014-03-26T12:24:34Z" id="38677090">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add uppercase token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5539</link><project id="" key="" /><description>added UpperCaseFilter which was added in Lucene 4.7

https://issues.apache.org/jira/browse/LUCENE-5369
</description><key id="30184639">5539</key><summary>Add uppercase token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels><label>:Analysis</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-26T04:19:53Z</created><updated>2015-06-06T18:34:39Z</updated><resolved>2014-03-26T08:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-26T07:11:01Z" id="38655379">+1 This change looks good to, can you add: 'closes #5539' to the commit message?
</comment><comment author="kzwang" created="2014-03-26T07:19:47Z" id="38655776">@martijnvg added
</comment><comment author="martijnvg" created="2014-03-26T07:20:20Z" id="38655802">@kzwang Thanks, I will pull it in soon.
</comment><comment author="martijnvg" created="2014-03-26T08:11:42Z" id="38658547">@kevinkluge I made a small change to it, in PreBuiltTokenFilters.java I also added the upper_case token filter, so it globally available by default, just like lowercase token filter. This will remove the need to define an uppercase token filter specifically in the settings of an index.

Thanks for your contribution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add caching support to Geohash Cell Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5538</link><project id="" key="" /><description>closes #5462

a few things not sure:
- currently it's hard code to cache if the `precision` is larger than `1000km`, should we add a config for this?
- Should we separate the builder and parser to 2 files to be consistent with other filter builder/parser?
- the `GeohashCellFilter.create()` is only used in this class and only in the parser, should we move that to the parser
</description><key id="30177638">5538</key><summary>Add caching support to Geohash Cell Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Geo</label></labels><created>2014-03-26T00:54:04Z</created><updated>2015-04-21T12:22:21Z</updated><resolved>2015-04-21T12:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T13:16:17Z" id="49029433">I am not sure we should do this at all.. IMO we should add smarter caches that realize what can be cached and what can / should not be cached based on cache hits etc...
</comment><comment author="clintongormley" created="2014-08-22T08:04:40Z" id="53033831">We should support caching for consistency's sake, but to leave the default to not cache.

@kzwang are you interested in fixing the issues above and rebasing? otherwise we'll get somebody here to take it over.

thanks
</comment><comment author="s1monw" created="2015-03-20T21:45:05Z" id="84161077">@nknize what is your take on this. I am all for closing it for now - it's your call!
</comment><comment author="nknize" created="2015-04-21T12:22:20Z" id="94774763">Hmmm, missed this one. For now I'm closing it in favor of replacing filters w/ queries and handling via smarter query cache.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Repository URL and versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5537</link><project id="" key="" /><description>It's frustrating that when a new minor version of ES is released (eg 1.1), we need to update the sources on all our hosts to the new URL - ie http://packages.elasticsearch.org/elasticsearch/1.0/debian to http://packages.elasticsearch.org/elasticsearch/1.1/debian

Is it possible to just have a major version URL and then put all the minor versions under that?
</description><key id="30169712">5537</key><summary>Repository URL and versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-03-25T22:24:07Z</created><updated>2014-03-25T22:25:55Z</updated><resolved>2014-03-25T22:24:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2014-03-25T22:24:55Z" id="38629067">https://github.com/elasticsearch/elasticsearch/issues/5536!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian repository / package naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5536</link><project id="" key="" /><description>The current (partially documented) system from http://www.elasticsearch.org/blog/apt-and-yum-repositories is non-standard for Debian and very hard to maintain.

You essentially need to change your apt sources for every Elasticsearch release.

It might be good to move to package-name based versioning, i.e:

```
# /etc/apt/sources.list.d/elasticsearch.list
deb http://packages.elasticsearch.org/elasticsearch stable main
```

Then can install the ES major version package of your choosing, leaving 'stable' to determine the latest stable minor release from ES:

```
apt-get install elasticsearch-1.1
```
</description><key id="30169277">5536</key><summary>Debian repository / package naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sammcj</reporter><labels><label>:Packaging</label></labels><created>2014-03-25T22:18:16Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2015-10-14T15:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2014-03-25T22:25:55Z" id="38629154">Agreed, I also just raised (but since closed) this - https://github.com/elasticsearch/elasticsearch/issues/5537

If they want versioning then it would be better to at least use a major version in the path rather than major+minor.
</comment><comment author="ross-w" created="2014-03-25T22:49:48Z" id="38631242">+1 to this, it's the only product we have to modify our apt sources for on every release
</comment><comment author="mablae" created="2014-03-31T14:40:37Z" id="39095898">Is there even a 1.1 repo yet?! 
</comment><comment author="ross-w" created="2014-03-31T22:24:49Z" id="39149929">@mablae Exactly. How would you know unless you checked the website or subscribe to the twitters or whatever? (Assume you know there is one by now :)

The point is there's a difference between keeping specific versions installed and having them in separate repositories. Repositories often require infrastructure changes whereas versions should only require configuration changes.
</comment><comment author="electrical" created="2014-04-01T14:57:31Z" id="39215201">hi guys,

The initially idea of versioned repositories was with 0.90 in prod and 1.0 was coming out.
At that point we decided to split it up so people wouldn't accidentally upgrade from 0.90.x to 1.0.x
Now with 1.0 and 1.1 repo's active we might need to review that strategy.

Will update this issue when we made a decision.

Thank you though for bringing this concern up! :-)
</comment><comment author="ross-w" created="2014-04-02T00:10:27Z" id="39275977">@electrical Understand the need to not have stable versions automatically upgrade, but other people solve this in the way @sammcj has mentioned - by package names. See (for example) how http://wiki.postgresql.org/wiki/Apt does it with their repo.
</comment><comment author="sammcj" created="2014-04-02T00:14:48Z" id="39276233">Correct @ross-w , the PostgreSQL apt repo is a good example of how this should be managed.
</comment><comment author="clintongormley" created="2015-10-14T15:20:31Z" id="148082364">Closed by https://github.com/elastic/elasticsearch/pull/13971
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't fetch debian packages for ES 1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5535</link><project id="" key="" /><description>http://packages.elasticsearch.org/elasticsearch-1.1/dists/stable/main/binary-amd64/Packages

= 404
</description><key id="30168945">5535</key><summary>Can't fetch debian packages for ES 1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sammcj</reporter><labels /><created>2014-03-25T22:13:28Z</created><updated>2014-03-25T22:22:02Z</updated><resolved>2014-03-25T22:22:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2014-03-25T22:14:49Z" id="38628127">"deb http://packages.elasticsearch.org/elasticsearch/1.1/debian stable main" worked fine for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>feature request: wildcard routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5534</link><project id="" key="" /><description>I propose that `?routing=_all` should match all shards
**Usecase**:
Parent-child relations `require` routing values, and it is impossible to `get` the document without knowing the routing value. As `get` is pretty cheap, this seams to be reasonable.
</description><key id="30168461">5534</key><summary>feature request: wildcard routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">g00fy-</reporter><labels /><created>2014-03-25T22:06:32Z</created><updated>2014-12-30T14:36:23Z</updated><resolved>2014-12-30T14:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="g00fy-" created="2014-03-25T22:42:43Z" id="38630613">Is there any other way I can match all shards? (or `GET` the document by id without knowing the routing value) ?
</comment><comment author="clintongormley" created="2014-12-30T14:36:23Z" id="68360854">Sorry it has taken a while to respond here. 

&gt; Is there any other way I can match all shards? (or GET the document by id without knowing the routing value) ?

Yes, run a search request on the `_id` field.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`routing` querystring comma separated not parsed as multiple routing values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5533</link><project id="" key="" /><description>According to http://www.elasticsearch.org/blog/customizing-your-document-routing/
`?routing=1,2` is should be the same as 
`?routing=1&amp;routing=2`
but the former is parsed as a string `1,2` (at least when doing `get` `/index/type/id?routing=1,2`)
</description><key id="30168445">5533</key><summary>`routing` querystring comma separated not parsed as multiple routing values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">g00fy-</reporter><labels /><created>2014-03-25T22:06:15Z</created><updated>2014-12-30T14:35:22Z</updated><resolved>2014-12-30T14:35:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-03-25T22:37:57Z" id="38630237">@g00fy- are you talking about index-time or search-time? For searching, multiple routing values work just fine:

```
DELETE /mr
{}

POST /mr
{
  "settings": {
    "number_of_shards": 10,
    "number_of_replicas": 0
  },
  "mappings": {
    "doc": {
      "properties": {
        "body": {"type": "string"}
      }
    }
  }
}

POST /mr/doc/1?routing=foo
{"body": "foo"}

POST /mr/doc/2?routing=bar
{"body": "bar"}

POST /mr/_refresh
{}

POST /mr/_search?routing=foo
{
  "query": {
    "match_all": {}
  }
}

POST /mr/_search?routing=bar
{
  "query": {
    "match_all": {}
  }
}

POST /mr/_search?routing=foo,bar
{
  "query": {
    "match_all": {}
  }
}
```

Gives the output of:

```
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"mr","_type":"doc","_id":"1","_score":1.0, "_source" : {"body": "foo"}}]}}

{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"mr","_type":"doc","_id":"2","_score":1.0, "_source" : {"body": "bar"}}]}}

{"took":1,"timed_out":false,"_shards":{"total":2,"successful":2,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"mr","_type":"doc","_id":"2","_score":1.0, "_source" : {"body": "bar"}},{"_index":"mr","_type":"doc","_id":"1","_score":1.0, "_source" : {"body": "foo"}}]}}
```
</comment><comment author="g00fy-" created="2014-03-25T22:41:41Z" id="38630533">@dakrone searching is fine, getting does not work:
`GET /mr/doc/1?routing=foo,bar`
actually neither syntax works - `GET` only supports one routing value
</comment><comment author="dakrone" created="2014-03-26T15:57:16Z" id="38701665">There isn't currently a way to do a GET request with multiple routing values, you'll have to use /_search instead for this.
</comment><comment author="javanna" created="2014-03-26T16:00:21Z" id="38702108">@g00fy- the get api goes to a single shard, based on either the id of the document, or the routing value if specified. That is why multiple routing values are not supported. You know what routing value you have used at index time thus you need to use the same when getting back the doc through get api.
</comment><comment author="clintongormley" created="2014-12-30T14:35:22Z" id="68360778">Not an issue. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing geo field causes error if searching over that field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5532</link><project id="" key="" /><description>I am using an alias to search across two indexes. One of them has a geofield, the other does not. If my search includes the geofield then I get results for one index but the other returns an error.

You can see the steps and results in this gist: https://gist.github.com/tstibbs/9762265

It seems like the error is fair enough if the field exists with a different type, but in my case the field doesn't exist at all. Why throw an error, surely if the field doesn't exist it's sufficient to just not match anything?

This appears to apply to several query types, at least the [GeoBoundingBoxFilterParser](https://github.com/elasticsearch/elasticsearch/blob/689fd15d786428869a7e311ef6c42f0094ae45a9/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java#L166) and the [GeoPolygonFilterParser](https://github.com/elasticsearch/elasticsearch/blob/689fd15d786428869a7e311ef6c42f0094ae45a9/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java#L146).
</description><key id="30138405">5532</key><summary>Missing geo field causes error if searching over that field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels><label>:Geo</label><label>adoptme</label><label>bug</label></labels><created>2014-03-25T16:14:53Z</created><updated>2014-11-29T13:46:25Z</updated><resolved>2014-11-29T13:46:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-26T08:48:54Z" id="53391928">Related to #7450 
</comment><comment author="clintongormley" created="2014-11-10T19:52:28Z" id="62443577">Related to #2801
</comment><comment author="clintongormley" created="2014-11-29T13:46:25Z" id="64952317">Closing in favour of #2801
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an ability to snapshot relocating primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5531</link><project id="" key="" /><description>Currently the snapshot process fails immediately with an error if one of the indices involved in the snapshot process has a relocating primary shard.
</description><key id="30138157">5531</key><summary>Add an ability to snapshot relocating primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T16:12:22Z</created><updated>2014-05-20T13:40:03Z</updated><resolved>2014-05-20T13:40:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Internally manipulate the terms execution hint as an enum instead of a String.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5530</link><project id="" key="" /><description /><key id="30135646">5530</key><summary>Internally manipulate the terms execution hint as an enum instead of a String.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T15:49:43Z</created><updated>2015-06-08T15:17:23Z</updated><resolved>2014-03-25T16:10:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-25T15:52:19Z" id="38581712">+1 I was already thinking about it.
</comment><comment author="s1monw" created="2014-03-25T15:53:12Z" id="38581848">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let ByteArray/BigByteArray.get() indicate whether a byte[] was materialized.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5529</link><project id="" key="" /><description>Necessary and cleaner instead of letting client code "guess" whether pages are shared or copied.
</description><key id="30108596">5529</key><summary>Let ByteArray/BigByteArray.get() indicate whether a byte[] was materialized.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T09:42:11Z</created><updated>2015-06-08T15:17:36Z</updated><resolved>2014-03-25T09:50:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-25T09:43:10Z" id="38545712">LGTM again :)
</comment><comment author="hhoffstaette" created="2014-03-25T09:50:34Z" id="38546321">:)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor common code for unmapped aggregators into `NonCollectingAggregator`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5528</link><project id="" key="" /><description>Our aggregators that are dedicated to unmapped fields all look more or less
the same so this hopefully helps remove some spaghetti code.
</description><key id="30107639">5528</key><summary>Refactor common code for unmapped aggregators into `NonCollectingAggregator`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T09:25:25Z</created><updated>2015-06-07T14:59:08Z</updated><resolved>2014-03-31T10:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-27T23:51:55Z" id="38875342">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let ByteArray/BigByteArray.get() indicate whether a byte[] was materialized.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5527</link><project id="" key="" /><description>Necessary and cleaner instead of letting client code "guess" whether pages are shared or copied.
</description><key id="30107392">5527</key><summary>Let ByteArray/BigByteArray.get() indicate whether a byte[] was materialized.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels /><created>2014-03-25T09:21:30Z</created><updated>2014-07-16T21:47:21Z</updated><resolved>2014-03-25T09:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-25T09:23:03Z" id="38544149">LGTM
</comment><comment author="hhoffstaette" created="2014-03-25T09:41:22Z" id="38545581">sorry, wrong branch..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible discrepancy in circuit breaker in parent/child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5526</link><project id="" key="" /><description>Set the actual bytes used in circuit breaker back to how it was before loading if the loading wasn't successful.
</description><key id="30106481">5526</key><summary>Fix possible discrepancy in circuit breaker in parent/child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T09:04:30Z</created><updated>2015-06-07T22:09:19Z</updated><resolved>2014-03-25T10:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-25T09:58:29Z" id="38546938">I left a comment @martijnvg 
</comment><comment author="martijnvg" created="2014-03-25T10:25:28Z" id="38549031">@s1monw I added a commit.
</comment><comment author="s1monw" created="2014-03-25T10:48:45Z" id="38550728">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Context Suggester Issues/Exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5525</link><project id="" key="" /><description>The context suggester seems to have a couple of bugs in the geo impl - when working according to the docs I could not create working suggestions.

This example does not return any suggestions (but it should return the one in Amsterdam)

``` JSON
DELETE venues
PUT venues

GET venues/_mapping
PUT venues/poi/_mapping
{
  "poi" : {
    "properties" : {
      "suggest_field": {
        "type": "completion",
        "context": {
          "loc": { 
            "type": "geo",
            "precision" : "10km"
          }
        }
      }
    }
  }
}

PUT venues/poi/1
{
  "suggest_field": {
    "input": ["Hotel Amsterdam" ],
    "context": {
      "loc": {
        "lat": 52.529172, 
        "lon": 13.407333
      }
    }
  }
}

GET venues/poi/2
PUT venues/poi/2
{
  "suggest_field": {
    "input": ["Hotel Berlin in AMS" ],
    "context": {
      "loc": {
        "lat": 52.363389, 
        "lon": 4.888695
      }
    }
  }
}

GET /venues/_search

GET /venues/_suggest
{
  "suggest" : {
    "text" : "h",
    "completion" : {
      "field" : "suggest_field",
      "context": {
        "loc": {
          "lat": 52.36,
          "lon": 4.88
        }
      }
    }
  }
}

```

This returns a parsing exception (but is mentioned in the docs like that)

``` JSON
DELETE venues
PUT venues

GET venues/_mapping
PUT venues/poi/_mapping
{
  "poi" : {
    "properties" : {
      "suggest_field": {
        "type": "completion",
        "context": {
          "location": {
              "type": "geo",
              "precision": ["1km", "5m"],
              "neighbors": true,
              "path": "pin",
              "default": {
                  "lat": 0.0,
                  "lon": 0.0
              }
          }
        }
      }
    }
  }
}
```

This is also directly from the docs, using the `value` field, results in a parse exception

``` JSON
DELETE venues
PUT venues

GET venues/_mapping
PUT venues/poi/_mapping
{
  "poi" : {
    "properties" : {
      "suggest_field": {
        "type": "completion",
        "context": {
          "loc": { 
            "type": "geo",
            "precision" : "10km"
          }
        }
      }
    }
  }
}

GET /venues/_suggest
{
    "suggest" : {
        "text" : "m",
        "completion" : {

            "field" : "suggest_field",
            "size": 10,
            "context": {
                "location": {
                    "value": {

                        "lat": 0,
                        "lon": 0
                    },
                    "precision": "1km"
                }
            }
        }

    }
}
```

Also the category suggester seems to have an issue, when you specify a path in the mapping for the context, but that path is not set on document indexing

``` JSON
DELETE services
PUT services
PUT services/service/_mapping
{
    "service": {
        "properties": {
            "suggest_field": {
                "type": "completion",
                "context": {
                    "color": { 
                        "type": "category",
                        "path": "color_field"
                    }
                }
            }
        }
    }
}

GET services/service/_mapping

PUT services/service/1
{
    "suggest_field": {
        "input": ["knacksack", "backpack", "daypack"]
    }
}
```

This was tested on the 1.x branch and master
</description><key id="30104443">5525</key><summary>Context Suggester Issues/Exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T08:28:57Z</created><updated>2014-04-02T06:43:41Z</updated><resolved>2014-03-31T14:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>allow plugins to be upgraded in place/live without first uninstalling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5524</link><project id="" key="" /><description>Since all we're doing is wiping/removing a particular subdir under plugins/ dir, can't we do the remove and install in a single step?

So instead of doing an uninstall then an install, allow an upgrade that does both in one step.

There might be some limitation if a plugin is unable to run two different versions on two nodes at the same time, but perhaps a method should be provided for plugin authors to prevent upgrade method and instead force uninstall/install if required.
</description><key id="30100208">5524</key><summary>allow plugins to be upgraded in place/live without first uninstalling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">geekpete</reporter><labels><label>enhancement</label></labels><created>2014-03-25T06:37:17Z</created><updated>2014-04-02T06:47:25Z</updated><resolved>2014-03-26T08:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-25T06:52:54Z" id="38535526">It could work for site plugins only but not for "JVM" plugins.
Although with some recent work around classloading could help to have this working for JVM plugins as well.

Are you looking for site plugins or JVM plugins?
</comment><comment author="geekpete" created="2014-03-25T07:21:43Z" id="38536722">Yeah this is a custom river plugin, JVM. 

So this is why we need the restart of the node when upgrading a plugin, the class has already been loaded and java can't unload it without a jvm restart? Ok.

So one way to reduce pain might be to only run these river plugins on non-data nodes that can be restarted without rebalancing shards or to rewrite the functionality as a groovy or other script that uses the groovy plugin that is loaded infrequently, allowing the scripts themselves to be reloaded.
</comment><comment author="geekpete" created="2014-03-25T22:21:33Z" id="38628744">Unless we can come up with a generic plugin class that gets loaded once then loads specific plugins inside that generic class, allowing them to be loaded/unloaded without needing to restart the jvm container, then this issue can probably be closed.

Upgrading would be nice for site plugins but probably best to keep the behaviour the same across all plugin types - uninstall/install.
</comment><comment author="dadoonet" created="2014-03-26T08:31:54Z" id="38659808">I think we will be able to do it when #5261 will be fixed.
So may be we should keep this issue opened for now.
</comment><comment author="costin" created="2014-03-26T08:42:18Z" id="38660414">Hi,

We have scheduled for 1.2 plugin isolation which could be used to do plugin unloading however it's unlikely that will happen since the plugin contract per-se does not specify unloading. Class unloading is tricky since it works only if all the references to it are removed (can be GC'ed) and depending on the plugin and its usage that can be hard or even impossible.
Further more, class unloading works only if its associated state can be somehow removed/discarded - which is another big if.

I'm closing this down for now since it's something we won't get too in the near future without significant code changes and maybe even breakage.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update geo-shape-type documentation to include all shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5523</link><project id="" key="" /><description>Update `geo-shape-type.asciidoc` to include all `GeoShapeType`s supported by the `org.elasticsearch.common.geo.builders.ShapeBuilder`.

Changes include:
1. A tabular mapping of GeoJSON types to Elasticsearch types
2. Listing all types, with brief examples, for all supported Elasticsearch types
3. Putting non-standard types to the bottom (really just moving Envelope to the bottom)
4. Linking to all GeoJSON types.
5. Adding whitespace around tightly nested arrays (particularly `multipolygon`) for readability

Closes #5579
</description><key id="30099411">5523</key><summary>[DOCS] Update geo-shape-type documentation to include all shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels /><created>2014-03-25T06:15:42Z</created><updated>2014-07-16T21:47:21Z</updated><resolved>2014-05-06T12:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:43:23Z" id="42297089">Many thanks for the PR - nice improvement.  Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix include_in_all for multi field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5522</link><project id="" key="" /><description>As in document http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#_include_in_all

The `include_in_all` should be ignored for multi-field. Currently it will set the `includeInAll` to `null`, then it will be default to `true` if `index` is not set to `no`. I think it should be set to `false` instead of `null`.

btw, seems `AllFieldMapper.IncludeInAll.unsetIncludeInAll()` will not be used anywhere after this change, should that be removed?

closes #5364
</description><key id="30096110">5522</key><summary>Fix include_in_all for multi field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-25T04:23:59Z</created><updated>2015-06-07T22:16:28Z</updated><resolved>2014-03-25T10:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-25T07:24:19Z" id="38536825">@kzwang Good catch! The reason why I set `includeInAll` to null, is to not serialise the include_in_all at all if it isn't used. This makes the mapping / cluster state smaller.

I think we can make a different fix, which is similar to the approach in copyTo. In ParseContext we can add a `withinMultiFields` boolean field and if that is set then the ParseContext#includeInAll(Boolean,boolean) return false. The `withinMultiFields` boolean field should be set to true in the AbstractFieldMapper.MultiFields#parse(ABF, ParseContext) and be set back to false once it exits that method.
</comment><comment author="kzwang" created="2014-03-25T07:42:40Z" id="38537693">@martijnvg yes, that's better. I'll modify the PR to do that
</comment><comment author="martijnvg" created="2014-03-25T07:43:42Z" id="38537755">@kzwang Great!
</comment><comment author="kzwang" created="2014-03-25T08:56:59Z" id="38542063">@martijnvg I've updated the PR
</comment><comment author="martijnvg" created="2014-03-25T09:13:29Z" id="38543469">Looks good! I left one comment.
</comment><comment author="martijnvg" created="2014-03-25T09:46:55Z" id="38546028">I left more one comment, you wan to change that? Other then that it looks good to me and I'll merge it in then.
</comment><comment author="martijnvg" created="2014-03-25T10:07:12Z" id="38547636">Thanks @kzwang your fix has been added to the master and 1.x branches
</comment><comment author="martijnvg" created="2014-03-27T05:58:57Z" id="38771553">Also backported it to 1.1 branch, so the fix will be in version `1.1.1` as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>why the sort result change everytime ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5521</link><project id="" key="" /><description>my query dsl is : 

```
{
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        }
      ],
      "should": []
    }
  },
  "from": 100,
  "size": 10,
  "sort": [
    {
      "weight": "desc"
    }
  ]
}
```

i get result as flows:

![es-ret-1](https://f.cloud.github.com/assets/306486/2507696/c05b6d70-b3c2-11e3-8083-739176ce953e.png)

![es-ret-2](https://f.cloud.github.com/assets/306486/2507681/7cf7beee-b3c2-11e3-8fae-698e3c5ba3c6.png)

the last two item changes every time when i search with the dsl before.

why this happend?  i have two node in my cluster

![es-head-map](https://f.cloud.github.com/assets/306486/2507706/0ab61762-b3c3-11e3-9155-f67f9ed7b1d6.png)

when i shutdown(or restart) the slave ,it works ok (no changes with the result).

but after indexing  :  (example)

```
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
```

the query result changing again. is there something wrong ?
how can i fix this problem , help me  :(
</description><key id="30092179">5521</key><summary>why the sort result change everytime ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scofier</reporter><labels /><created>2014-03-25T02:19:38Z</created><updated>2014-03-25T06:06:17Z</updated><resolved>2014-03-25T06:06:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-25T06:06:17Z" id="38533587">Please send questions to the mailing list.
We use github issues for issues and feature requests.
Thanks!

BTW change number of shards to 1 and you should see consistent results.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated gateways</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5520</link><project id="" key="" /><description>Closes #5422
</description><key id="30083238">5520</key><summary>Remove deprecated gateways</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T22:56:34Z</created><updated>2015-06-06T17:04:29Z</updated><resolved>2014-03-26T22:41:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-25T09:18:19Z" id="38543800">The diff stats are awesome. :-)
</comment><comment author="s1monw" created="2014-03-26T15:28:39Z" id="38697763">LGTM +1 to push
</comment><comment author="jpountz" created="2014-03-26T15:28:58Z" id="38697819">+1 I think this is the best PR we've had this year so far. :-)
</comment><comment author="s1monw" created="2014-03-26T15:30:29Z" id="38698043">I went a bit wild on the labels I guess... due to excitement 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the `field_value_factor` function to the function_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5519</link><project id="" key="" /><description>The `field_value_factor` function uses the value of a field in the document to influence the score. This is a common case that script scoring was previously used for.

For example, a query that looks like:

``` json
{
  "query": {
    "function_score": {
      "query": {"match": { "body": "foo" }},
      "functions": [
        {
          "field_value_factor": {
            "field": "popularity",
            "factor": 1.1,
            "modifier": "square"
          }
        }
      ],
      "score_mode": "max",
      "boost_mode": "sum"
    }
  }
}
```

Would have the score modified for each document by:

```
square(1.1 * doc['popularity'].value)
```

This is faster and less error-prone than using scripting to influence the score. Speed-wise, I used the IMDB movie database and tested with two different queries:

``` json
{
  "query": {
    "function_score": {
      "query": {"match_all": {}},
      "functions": [
        {
          "field_value_factor": {
            "field": "runtime",
            "modifier": "none"
          }
        }
      ],
      "score_mode": "max",
      "boost_mode": "sum"
    }
  }
}
```

vs:

``` json
{
  "query": {
    "function_score": {
      "query": {"match_all": {}},
      "functions": [
        {
          "script_score": {
            "script": "_score * doc['runtime'].value"
          }
        }
      ],
      "score_mode": "max",
      "boost_mode": "sum"
    }
  }
}
```

The `field_value_factor` version took about 75ms on average, the `script_score` version took about 145ms on average (after field data was loaded for both versions).
</description><key id="30049490">5519</key><summary>Add the `field_value_factor` function to the function_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T16:15:27Z</created><updated>2015-06-06T18:34:48Z</updated><resolved>2014-03-27T21:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-03-25T17:42:34Z" id="38596701">@brwe I removed the `lenient` flag entirely, but I did add in an `ignore_missing` flag that returns the original score unmodified if the field is missing a value (defaulting to false, which throws an exception if the field is missing). What do you think of this solution?
</comment><comment author="brwe" created="2014-03-26T15:53:21Z" id="38701146">I have no strong opinion about the `ignore_missing` if it is switched off per default, it might make sense in some cases. 
</comment><comment author="brwe" created="2014-03-26T16:22:56Z" id="38705190">+1 
</comment><comment author="imotov" created="2014-03-26T23:35:37Z" id="38753872">LGTM
</comment><comment author="s1monw" created="2014-03-27T09:44:50Z" id="38783731">I like the feature I left some comments on the implementation - speed is important here :)
</comment><comment author="clintongormley" created="2014-03-27T13:51:03Z" id="38804289">Please update these docs as well: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html#function-score-instead-of-boost
</comment><comment author="dakrone" created="2014-03-27T19:24:06Z" id="38849262">@s1monw updated to use IndexNumericFieldData and load the values more efficiently as you suggested. Also updated the docs like @clintongormley asked.
</comment><comment author="s1monw" created="2014-03-27T20:26:29Z" id="38856211">LGTM
</comment><comment author="ncolomer" created="2014-04-10T15:12:42Z" id="40097536">Nice work! :+1: 
Any idea when this will be available?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long field is rounded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5518</link><project id="" key="" /><description>Insert a document :

&lt;pre&gt;
PUT test/Test/1
{
    "value" : 1201169796353347552
}
&lt;/pre&gt;

Get it : 

&lt;pre&gt;
GET test/Test/1
&lt;/pre&gt;

The received value for the field is : 1201169796353347600.
When searching, only '1201169796353347552' - the original value must be used in order to get the document.
</description><key id="30044737">5518</key><summary>Long field is rounded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaxanean</reporter><labels /><created>2014-03-24T15:26:34Z</created><updated>2014-03-25T08:46:07Z</updated><resolved>2014-03-25T08:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-03-25T03:09:10Z" id="38526582">I tried on master branch, it works fine. However, in Chrome if the JSON formatter plugin is enabled and it will display `1201169796353347600` in parsed result, but will display correct `1201169796353347552` in raw result. 
Also on http://www.jsoneditoronline.org/ , if we pass `1201169796353347552` to it, it will becomes `1201169796353347600`. Seems issues with some json parser library instead of Elasticsearch
</comment><comment author="bleskes" created="2014-03-25T08:35:40Z" id="38540645">@bax1989 are you using sense by any chance? JavaScript simply can't represent such a big number. Try this in dev console:

```
a= 1201169796353347552;
```

and see the value of a.
</comment><comment author="abaxanean" created="2014-03-25T08:46:07Z" id="38541284">Yeah guys, you're right, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BytesReference.Helper should never materialize a byte[] array.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5517</link><project id="" key="" /><description>The current implementations of BytesReference.Helper.bytesEquals and
bytesHashCode materialize a byte[] when passed an instance that returns `false`
in `hasArray()`. They should instead do a byte-by-byte comparison/hashcode
computation without materializing the array.
</description><key id="30043033">5517</key><summary>BytesReference.Helper should never materialize a byte[] array.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T15:08:44Z</created><updated>2015-06-07T14:59:14Z</updated><resolved>2014-03-25T08:57:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-24T16:17:53Z" id="38464903">LGTM except for my minor comment
</comment><comment author="hhoffstaette" created="2014-03-25T07:12:02Z" id="38536284">While we're touching this can we please make sure the hashes are compatible with Arrays.hashCode()? Right now they are not since the initial hash starts at 0, not 1. PagedBytesReference and the initial bits in BigArrays already do the right thing.
</comment><comment author="jpountz" created="2014-03-25T08:22:43Z" id="38539871">I just added a new commit to change the initial hash code to 1 and add the assertion that @kimchy suggested. I'll merge soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5516</link><project id="" key="" /><description>After installing 1.0.1 rpm - I get a logstash error:
# log4j:WARN No appenders could be found for logger (node).

log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

and ES has stopped logging (ran 0.90.9 before - it logged fine).
This is on CentOS 6.

before I had ES_MIN_MEM and ES_MAX_MEM.. now I have to set:
ES_HEAP_SIZE and MAX_LOCKED_MEMORY - but one of them is being used with ulimit - which does not accept a number such as "7976m" (ie. 8GB memory) - so i get a ulimit complaint.
</description><key id="30038590">5516</key><summary>rpm bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KlavsKlavsen</reporter><labels /><created>2014-03-24T14:21:11Z</created><updated>2014-03-24T14:42:36Z</updated><resolved>2014-03-24T14:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="KlavsKlavsen" created="2014-03-24T14:41:39Z" id="38452176">ahh. turned out variables I had in my /etc/sysconfig/elasticsearch did NOT match what was expected in 1.0.1 init script :(
</comment><comment author="s1monw" created="2014-03-24T14:42:36Z" id="38452282">thx for clarifying. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize xcontent type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5515</link><project id="" key="" /><description /><key id="30033846">5515</key><summary>Randomize xcontent type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T13:32:48Z</created><updated>2014-07-16T21:47:23Z</updated><resolved>2014-03-24T17:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-24T16:35:11Z" id="38467107">LGTM, would also love to have a `rawField(XContentBuilderString)` but that can go in a different issue.
</comment><comment author="s1monw" created="2014-03-24T17:01:43Z" id="38470657">@jpountz I added a new commit can you take a look?
</comment><comment author="jpountz" created="2014-03-24T17:03:02Z" id="38470872">+1 Let's merge this change!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JsonXContentGenerator#writeRawField produces invalid JSON if raw field is the first field in the json object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5514</link><project id="" key="" /><description>``` Java
XContentBuilder xContentBuilder = XContentFactory.contentBuilder(XContentType.JSON);
xContentBuilder.startObject();
xContentBuilder.rawField("foo", new BytesArray("{\"test\":\"value\"}"));
xContentBuilder.endObject();
```

results in the following string:

``` Json
{,"foo":{"test":"value"}}
```
</description><key id="30033729">5514</key><summary>JsonXContentGenerator#writeRawField produces invalid JSON if raw field is the first field in the json object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T13:31:20Z</created><updated>2015-06-07T22:26:50Z</updated><resolved>2014-03-24T17:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-24T13:33:38Z" id="38444068">I think this is a duplicate of #2897 ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add percolate-by-query feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5513</link><project id="" key="" /><description>This would need well defined bounds like most populate documents for example.
</description><key id="30026112">5513</key><summary>Add percolate-by-query feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels><label>feedback_needed</label></labels><created>2014-03-24T11:30:13Z</created><updated>2015-02-28T05:12:20Z</updated><resolved>2015-02-28T05:12:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:34:47Z" id="68360721">Hi @geekpete 

Sorry it has taken a while to get to this issue.  I'm afraid I don't understand the feature that you are describing.  Could you provide more info please?
</comment><comment author="clintongormley" created="2015-02-28T05:12:20Z" id="76511239">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add K-means clustering feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5512</link><project id="" key="" /><description>Add k-means clustering to allow detection of clusters in data sets.
http://en.wikipedia.org/wiki/K-means_clustering

Would be useful for geo points but also other use cases too.

Thanks to https://github.com/koobs for suggesting this one in Sydney Elastic Training.
</description><key id="30025872">5512</key><summary>Add K-means clustering feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">geekpete</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label></labels><created>2014-03-24T11:26:29Z</created><updated>2017-07-28T11:15:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="savioteles" created="2014-05-20T17:23:01Z" id="43656874">It would be great! I really need this feature. Is there any estimate of when you will start coding?
</comment><comment author="geekpete" created="2014-05-20T23:16:51Z" id="43695317">Not that I'm seeking to drop her in it, but Britta https://twitter.com/a2tirb would definitely have the madskillz to build this feature but not sure of her priorities/bandwidth/interest to attack this feature with sticks.

I'm also not sure how new features are selected or voted up for prioritisation by elasticsearch overlords either.
</comment><comment author="brwe" created="2014-05-21T09:50:04Z" id="43735177">If I recall correctly, @geekpete proposed to have that in the context of aggregations, that is, build cluster and then use these as buckets inside the aggregations framework. Indeed, this would be an extremely useful feature. 

While it would be very much fun to implement unfortunately I do not think we will implement it in the near future. Anyone coming up with a pull request for this is of course more than welcome :-)

For now I can only point you to the [carrot2 plugin](https://github.com/carrot2/elasticsearch-carrot2) which does an excellent job in clustering search results. 
</comment><comment author="koobs" created="2014-05-22T02:00:36Z" id="43841019">I'll add the comment that 'k' clusters ought to user-suppliable as an argument to the aggregation for maximum value, with possible k-values being:
- N
- Random
- [Basic Rule-Of-Thumb](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Rule_of_thumb)
- [Document (m) by Term(n) matrix](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Finding_Number_of_Clusters_in_Text_Databases)
- Some other cool method :)

For context, I brought this up @ the ElasticSearch Training in response to a brief conversation about search vs 'insight' in relation to data, the former where you know what you're looking for, the latter where you dont, or might not. The specific example was geospatial result sets with arbitrary demography data fields. It was a great session @brwe!
</comment><comment author="mishakogan" created="2014-08-21T16:03:51Z" id="52941993">I also would like to cast my vote for some kind of automated clustering feature. Carrot2 is great but as far as understand can only work on small amount of data. Would be great to have something that clusters ALL the data all the time. Maybe custom clustering analyzer?
</comment><comment author="clintongormley" created="2014-11-10T19:47:40Z" id="62442769">@brwe would #8110 help here?
</comment><comment author="brwe" created="2014-11-12T11:03:06Z" id="62702222">@clintongormley not really. Bucket reducers from  #8110  would run on the final aggregation but clustering needs the documents. 
</comment><comment author="jpountz" created="2014-11-12T11:20:54Z" id="62704197">@brwe I think implementing clustering as a reducer could help reduce the cost very significantly? K-means is costly so running such an algorithm on a dataset containing lots of documents could be very slow. On the other hand, if we take geo-clustering as an example, we could make it very fast (though a bit lossy) by working on top of the output of the geo-hash grid aggregation as a bucket reducer?
</comment><comment author="brwe" created="2014-11-14T10:40:31Z" id="63040704">True, I should distinguish use cases. For up to 2d it might help indeed. For text clustering I do not see it.
</comment><comment author="yehosef" created="2015-06-29T08:09:25Z" id="116515164">just found this - would be great. +1
</comment><comment author="dsingley" created="2015-10-09T15:20:23Z" id="146901428">+1
</comment><comment author="redmouthch" created="2015-10-12T19:02:34Z" id="147492358">search for this... this would be a very great feature. Also other Mining-algorithms.
</comment><comment author="colings86" created="2015-11-13T10:53:08Z" id="156395907">Implementing this as a pipeline aggregation should now be possible. In that case we would first collect values into buckets using other aggregations and then use the pipeline aggregation to create clusters from those buckets.
</comment><comment author="lessless" created="2015-12-20T14:35:27Z" id="166124316">that would be mad!
</comment><comment author="lessless" created="2015-12-30T09:54:18Z" id="167970331">@koobs is there a recording of this session somewhere out there? 
</comment><comment author="koobs" created="2015-12-30T12:43:28Z" id="167990920">@lessless I hope not :)
</comment><comment author="irony" created="2016-01-27T17:57:09Z" id="175770565">This would really be awesome!
</comment><comment author="audriusb" created="2016-03-10T06:47:05Z" id="194697835">+1
</comment><comment author="reinier-pv" created="2016-03-10T13:02:43Z" id="194832974">:+1: 
</comment><comment author="trupin" created="2016-04-09T10:43:46Z" id="207767802">+1
</comment><comment author="chenryn" created="2016-04-28T07:18:23Z" id="215332544">+1
</comment><comment author="marfago" created="2016-05-04T15:54:12Z" id="216910187">+1
</comment><comment author="hkulekci" created="2016-05-09T15:38:27Z" id="217901216">+1
</comment><comment author="Amazium" created="2016-06-23T10:40:05Z" id="228012694">+1
</comment><comment author="s1monw" created="2016-07-01T09:59:20Z" id="229908233">I am removing the discuss label and make it adopt me - there has been enough discussion on this.
</comment><comment author="ryanrozich" created="2016-07-04T03:55:39Z" id="230200159">+1 carrot2 is good for text clustering but does not use the aggregations framework, would be great to have a text clustering option that we can build sub-aggregations / child aggregations underneath.
</comment><comment author="lessless" created="2016-08-11T05:05:26Z" id="239074788">will we be able to use it with geopoints? 
</comment><comment author="SimoneTosato" created="2016-10-07T11:05:03Z" id="252219253">+1
</comment><comment author="ddavidebor" created="2016-10-07T11:22:30Z" id="252224347">+1 This feature would be so helpful and powerful
</comment><comment author="diugalde" created="2016-10-14T07:46:26Z" id="253732124">What happened with this feature? Is there any work in progress? I think it would be really useful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to disable norms on an existing field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5511</link><project id="" key="" /><description>I followed @nik9000 's idea to carefully document the caveat of disabling
norms as opposed to trying to wrap readers at search time to hide norms.

Close #4813
</description><key id="30024951">5511</key><summary>Allow to disable norms on an existing field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-03-24T11:11:01Z</created><updated>2014-06-16T21:35:26Z</updated><resolved>2014-03-26T08:15:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-24T14:06:28Z" id="38447972">LGTM this should go 1.2 and 2.0 no?
</comment><comment author="jpountz" created="2014-03-24T14:06:50Z" id="38448013">that was my plan
</comment><comment author="jpountz" created="2014-03-25T08:48:30Z" id="38541423">@bleskes Can you check if the last commit addresses your concerns? Thanks!
</comment><comment author="bleskes" created="2014-03-25T11:01:56Z" id="38551693">@jpountz it does. Thx.
</comment><comment author="nik9000" created="2014-03-25T20:45:32Z" id="38618212">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix returning incorrect XContentParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5510</link><project id="" key="" /><description /><key id="30024673">5510</key><summary>Fix returning incorrect XContentParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Java API</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T11:06:18Z</created><updated>2015-06-07T22:27:34Z</updated><resolved>2014-03-24T11:24:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T11:24:24Z" id="38433438">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CBOR data format support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5509</link><project id="" key="" /><description>add CBOR data format support using https://github.com/FasterXML/jackson-dataformat-cbor

closes #4860
</description><key id="30024372">5509</key><summary>Add CBOR data format support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:REST</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T11:00:45Z</created><updated>2015-06-07T16:05:35Z</updated><resolved>2014-03-28T09:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-24T11:31:22Z" id="38433957">@kzwang I wonder how we can really efficiently test this. I think we should swap this format in randomly but we first need to add the infra for it. Lemme see how we can add this I'd love to get this in but I think it needs to be tested across the entire code base. stay tuned
</comment><comment author="kimchy" created="2014-03-24T16:13:48Z" id="38464257">btw, I have been looking into supporting this format, and the tricky bit is the stream separator part. We need to be able to either support it or solve it somehow (by maybe supporting cases for parsers that don't have stream separators), but thats a bigger change.

I think we can pull this in, and  fail when stream separator is needed for now, and later on solve that as well....
</comment><comment author="kzwang" created="2014-03-24T22:07:44Z" id="38507474">Currently the stream separator will throw `ElasticsearchParseException`, `yaml` also doesn't support stream separator now, probably should fix that as well
</comment><comment author="kimchy" created="2014-03-27T23:47:19Z" id="38875033">I am good with this change!, we can work on another issue to handle cases where there is no support for stream separator, to parse in that case...., /cc @s1monw 
</comment><comment author="s1monw" created="2014-03-28T09:18:29Z" id="38900384">I just left one style comment since we are on java7 and can use try with now. otherwise LGTM Can you rebase to master as well? good stuff!
</comment><comment author="kzwang" created="2014-03-28T09:37:06Z" id="38901725">@s1monw changed to use try with and rebased to master
</comment><comment author="s1monw" created="2014-03-28T09:59:57Z" id="38903290">thanks @kzwang 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast discovery enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5508</link><project id="" key="" /><description>An improvement to unicast discovery to also ping nodes the node itself received a ping from.
Also moved the unicast tests all into a single package.
</description><key id="30023669">5508</key><summary>Unicast discovery enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T10:48:11Z</created><updated>2015-06-07T14:59:20Z</updated><resolved>2014-03-31T03:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-28T00:03:54Z" id="38876060">LGTM
</comment><comment author="s1monw" created="2014-03-28T09:09:39Z" id="38899835">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `reverse_nested` aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5507</link><project id="" key="" /><description>PR for #5485

There a number of todos that need to be addressed.
</description><key id="30013824">5507</key><summary>Add `reverse_nested` aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-03-24T07:33:18Z</created><updated>2015-05-18T23:31:52Z</updated><resolved>2014-04-30T17:24:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-29T10:47:54Z" id="41662478">Rebased and updated the PR:
- The `reverse_nested` agg can only be defined inside a `nested` agg.
- The path is an optional option. If not specified the closest `nested` object field's path from `nested` agg wrapping the `reverse_nested` agg is used as `path` option.
</comment><comment author="martijnvg" created="2014-04-29T18:33:10Z" id="41714784">Updated the PR:
- Moved reverse nested code into the nested package
- Removed automatic path resolving, because it can unintuitive.
- If no path has been defined in the `reverse_nested` agg, it points to the main / root doc. (instead of defining `root` which can be an actual field)
</comment><comment author="jpountz" created="2014-04-30T13:25:36Z" id="41795515">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>merge GeoPoint specific mapping properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5506</link><project id="" key="" /><description>closes #5505
I assume only `validate_lat` and `validate_lon` can be updated, other fields will throw exceptions. Please let me know if I was wrong.

Also added `precision_step` to the document page
</description><key id="30008116">5506</key><summary>merge GeoPoint specific mapping properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T03:55:08Z</created><updated>2015-06-07T22:28:28Z</updated><resolved>2014-03-24T08:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T08:41:29Z" id="38421396">This change is good, I just merged it via bfd32363780f8ea3ee6439d9106bfca6ff34ba54.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoPointFieldMapper doesn't merge GeoPoint specific properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5505</link><project id="" key="" /><description>``` sh
# add mapping
curl -XPUT localhost:9200/test/test/_mapping -d '{
  "test": {
    "properties": {
      "testGeo": {
        "type": "geo_point",
        "validate": false
      }
    }
  }
}'

# update validate to true
curl -XPUT localhost:9200/test/test/_mapping -d '{
  "test": {
    "properties": {
      "testGeo": {
        "type": "geo_point",
        "validate": true
      }
    }
  }
}'
```

After update, the `validate` is still false and no exception is thrown. Other fields like `store` etc. works.
</description><key id="30008024">5505</key><summary>GeoPointFieldMapper doesn't merge GeoPoint specific properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T03:52:05Z</created><updated>2014-03-26T10:02:45Z</updated><resolved>2014-03-24T08:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[DOCS] add compass and compress_threshold to binary field mapping doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5504</link><project id="" key="" /><description>`compress` and `compress_threshold` are exist in BinaryFieldMapper however are not documented
</description><key id="30007148">5504</key><summary>[DOCS] add compass and compress_threshold to binary field mapping doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels /><created>2014-03-24T03:19:19Z</created><updated>2014-06-19T00:46:05Z</updated><resolved>2014-05-06T12:28:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T08:52:29Z" id="38422163">Now that stored fields are always compressed, I'm wondering if the `compress` flag of the binary field mapper still makes sense? /cc @kimchy 
</comment><comment author="jpountz" created="2014-04-03T07:47:11Z" id="39421674">I talked a bit about these settings with @kimchy and it would be nice to add a note explaining that these settings only make sense on large and highly-compressible binary fields: otherwise per-field compression is usually not worth doing since there are not always enough space savings to compensate for the overhead of the compression format (while you still have the CPU overhead). In these case, you should rather not configure any compression and just rely on the block compression of stored fields (which is enabled by default and can't be disabled).
</comment><comment author="clintongormley" created="2014-05-06T12:28:23Z" id="42295971">Updated PR with @kimchy's comments and merged.

thanks for the PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge `null_value` for boolean field and remove `include_in_all` for boolean field in doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5503</link><project id="" key="" /><description>closes #5502
</description><key id="30006832">5503</key><summary>Merge `null_value` for boolean field and remove `include_in_all` for boolean field in doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T03:08:28Z</created><updated>2015-06-08T15:18:21Z</updated><resolved>2014-03-24T10:01:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T10:01:58Z" id="38427201">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BooleanFieldMapper doesn't handle include_in_all and cannot merge null_value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5502</link><project id="" key="" /><description>According to document http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#boolean
Boolean field support `include_in_all`, however the field mapper doesn't handle that.

Also we can't update the `null_value` using put mapping api

``` sh
# add mapping
curl -XPUT localhost:9200/test/test/_mapping -d '{
  "test": {
    "properties": {
      "testBool": {
        "type": "boolean",
        "include_in_all": true,
        "null_value": false
      }
    }
  }
}'
```

the result mapping is 

``` sh
curl -XGET localhost:9200/test/test/_mapping

{
  "test": {
    "mappings": {
      "test": {
        "properties": {
          "testBool": {
            "type": "boolean",
            "null_value": false
          }
        }
      }
    }
  }
}
```

the `include_in_all` is missing

``` sh
# update null_value to true
curl -XPUT localhost:9200/test/test/_mapping -d '{
  "test": {
    "properties": {
      "testBool": {
        "type": "boolean",
        "include_in_all": true,
        "null_value": true
      }
    }
  }
}'
```

there is no exception and the mapping is still the same (i.e. `null_value` is still `false`)
</description><key id="30006788">5502</key><summary>BooleanFieldMapper doesn't handle include_in_all and cannot merge null_value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-24T03:07:02Z</created><updated>2014-03-24T10:02:11Z</updated><resolved>2014-03-24T10:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T08:49:01Z" id="38421898">Although I agree that `null_value` can be useful for boolean fields, I don't think it makes sense to include them into the `_all` field. So maybe we should rather fix the documentation, what do you think?
</comment><comment author="kzwang" created="2014-03-24T09:34:13Z" id="38425137">@jpountz yes, I think it doesn't make sense as well. I've updated the PR to merge `null_value` only and removed `include_in_all` in document
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index level blocks, index conflict settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5501</link><project id="" key="" /><description>allow to configure on the index level which blocks can optionally be applied using tribe.blocks.indices prefix settings.
allow to control what will be done when a conflict is detected on index names coming from several clusters using the tribe.on_conflict setting. Defaults remains "any", but now support also "drop" and "prefer_[tribeName]".
</description><key id="29999927">5501</key><summary>Index level blocks, index conflict settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Tribe Node</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-23T22:16:34Z</created><updated>2015-06-07T14:59:34Z</updated><resolved>2014-03-27T16:45:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-25T03:18:52Z" id="38526991">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix potential NPE, throw failure only if exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5500</link><project id="" key="" /><description /><key id="29999919">5500</key><summary>fix potential NPE, throw failure only if exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-23T22:16:17Z</created><updated>2015-06-07T22:29:03Z</updated><resolved>2014-03-24T17:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-24T16:38:47Z" id="38467556">uups! good catch +1
</comment><comment author="kimchy" created="2014-03-24T17:24:21Z" id="38473655">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A master node operation can timeout in a rare race condition when a master has a connection issue which is quickly restored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5499</link><project id="" key="" /><description>If a master node operation comes on immediately after a connection problem to the master node, the operation will be rescheduled. If the connection is quickly restored and the same master is re-elected, a rare race condition can occur causing the operation to unjustly time out.
</description><key id="29999451">5499</key><summary>A master node operation can timeout in a rare race condition when a master has a connection issue which is quickly restored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v1.2.0</label></labels><created>2014-03-23T21:54:50Z</created><updated>2014-06-13T15:57:01Z</updated><resolved>2014-03-24T08:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-23T23:46:18Z" id="38402134">LGTM
</comment><comment author="bleskes" created="2014-03-24T08:06:51Z" id="38419441">Thx. Pushed to master and 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term aggregations extraordinarily slow on Windows (ES 1.0.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5498</link><project id="" key="" /><description>I'm working on an app that makes liberal use of term facets.  I have a query right now that takes about **1.5** seconds using term facets.  I switched to try out the new aggregations, and the same query using aggregations now takes over 360 seconds. (**6 mins!**).  This is on a 3 node cluster running in a windows environment.  And while running, it literally makes the node that received the query completely unresponsive.  To the point where the cluster thinks it's gone.  Sometimes it never returns, either, and i have to bounce the service on the box.

Here is the original query:

``` javascript
   "query": {
      "match_all": {}
   },
   "size": 0,
   "facets": {
      "url": {
         "terms": {
            "field": "url",
            "size":20
         }
      }
   }
```

Here is the new query with aggregations:

``` javascript
   "query": {
      "match_all": {}
   },
   "size": 0,
   "aggs": {
      "url": {
         "terms": {
            "field": "url",
            "size":20
         }
      }
   }
```
</description><key id="29993005">5498</key><summary>Term aggregations extraordinarily slow on Windows (ES 1.0.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BrandesEric</reporter><labels /><created>2014-03-23T18:04:02Z</created><updated>2015-10-12T13:18:18Z</updated><resolved>2014-03-24T13:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-24T09:37:41Z" id="38425371">Terms aggregations are not as optimized as terms facets (yet), especially on high-cardinality fields. I suppose this is the case of your `url` field? Could you check if you get better response times if you pass [`execution_hint: map`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_execution_hint) to the terms aggregation?
</comment><comment author="jpountz" created="2014-03-24T09:59:29Z" id="38427031">I just noticed that you mentioned that the query made the node completely unresponsive, which is a typical symptom of memory pressure, because the garbage collector keeps running stop-the-world collections during which no thread is allowed to run. So this slowness might be mostly due to the higher memory usage of terms aggregations compared to facets (in which case the `map` execution hint should still help a bit since it requires less memory).
</comment><comment author="BrandesEric" created="2014-03-24T12:58:28Z" id="38440519">Ah, that did help tremendously, thanks!  Down to about 15 seconds from 6 minutes!  I think you're right that it was a GC pause that caused the issue.  It most seriously occurs on the dev environment which is a much less powerful cluster.  For now I will likely stick with the older term facets since the performance is still a bit better :)

One last question - my app uses ES primarily for faceting.  Would it be advisable to make any changes to the default field data cache setting?  Otherwise is the general rule the more memory the better?  In general some of the facets have cardinalities in the millions, so any way to optimize that would be helpful!
</comment><comment author="jpountz" created="2014-03-24T13:19:42Z" id="38442647">Thanks for the feedback. Regarding the field data cache configuration, reloading a field data cache is very costly, so you should make sure that it is large enough to hold an entry for all segments and all fields that you need to facet/aggregate on (which is what the default configuration does).

I'm closing this issue for now, but be reassured that we are working on improving memory/speed for terms aggregations.
</comment><comment author="BrandesEric" created="2014-03-24T13:23:42Z" id="38443050">Sounds good, thanks for your help!
</comment><comment author="haochun" created="2015-10-12T08:08:20Z" id="147324158">@BrandesEric  Hello, I would like to know how you solve this problem.thank you very much!
</comment><comment author="BrandesEric" created="2015-10-12T13:18:18Z" id="147396908">@haochun Moved to Linux and upgraded to the 1.4 series :)  (Of course, 1.4 is old these days, so something like the 1.7 series is likely even better)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Plain highlighter does not honor _analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5497</link><project id="" key="" /><description>Recreation at https://gist.github.com/hazzadous/9713399
</description><key id="29971523">5497</key><summary>Highlighting: Plain highlighter does not honor _analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hazzadous</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-22T20:09:01Z</created><updated>2014-05-28T07:23:42Z</updated><resolved>2014-05-28T07:23:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>FVH does not work with span_near queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5496</link><project id="" key="" /><description>I would expect highlighting to work, a recreation at https://gist.github.com/hazzadous/9671181

Groups discussion at https://groups.google.com/forum/#!topic/elasticsearch/zQ06PD_T1tw

Robert Muir mentions this would require flattening the query to achieve.
</description><key id="29971177">5496</key><summary>FVH does not work with span_near queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hazzadous</reporter><labels><label>:Highlighting</label></labels><created>2014-03-22T19:50:13Z</created><updated>2016-11-28T15:40:39Z</updated><resolved>2016-11-24T18:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-24T18:27:53Z" id="262829224">Closing in favour of #21621</comment><comment author="lami02" created="2016-11-28T15:40:39Z" id="263304116">I have this problem as well. Perhaps there is someone who could help me working around this, without waiting for the unified highlighter. I need the span_near queries to use wildcard-queries within phrases (like "*foo bar") and I need the fast vector highlighter to match multiple fields (with two different analyzers). How can I get the desired result? Is there any option I am missing?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to forbidden-apis 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5495</link><project id="" key="" /><description>Closes #5492
</description><key id="29969712">5495</key><summary>Update to forbidden-apis 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-22T18:44:55Z</created><updated>2015-08-25T13:26:01Z</updated><resolved>2014-03-22T19:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-22T18:51:47Z" id="38360684">LGTM
</comment><comment author="kimchy" created="2014-03-22T18:53:05Z" id="38360709">Question, do we use `Locale.ROOT` (which seems to be the preferred one), or `Locale.ENGLISH`? Not a big issue here in the bench sys outs, just for consistency.
</comment><comment author="rmuir" created="2014-03-22T18:55:18Z" id="38360761">I used Locale.ENGLISH because the messages had english text... but it doesnt matter either way.
</comment><comment author="s1monw" created="2014-03-22T19:02:27Z" id="38360930">I think for those messages english is ok since they are just loggs really. internally we should use ROOT though
</comment><comment author="uschindler" created="2014-03-22T21:55:12Z" id="38365395">Yes, I think that's right. If it is messages that are printed out to the user, the locale should match the message printed. For stuff like formatting numbers or dates for internal processing, it should be Locale.ROOT.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes who are not currently master do not update the ElectMasterService when dynamically setting min_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5494</link><project id="" key="" /><description>When updating the min_master_nodes setting via the Cluster Settings API, the change is propagated to all nodes. The current master node also updates the ElectMasterService and validates that is still sees enough master eligible nodes and that it's election is still valid. Other master eligible nodes do not go through this validation (good) but also didn't update the ElectMasterService with the new settings. The result is that if the current master goes away, the next election will not be done with the latest setting.

Note - min_master_node set in the elasticsearch.yml file are processed correctly
</description><key id="29962761">5494</key><summary>Nodes who are not currently master do not update the ElectMasterService when dynamically setting min_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label></labels><created>2014-03-22T15:05:45Z</created><updated>2014-06-13T15:45:33Z</updated><resolved>2014-03-22T20:54:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-22T15:07:11Z" id="38353861">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mlockall fails on noexec /tmp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5493</link><project id="" key="" /><description>I've been debugging a system where I couldn't get memory locking working. The upshot is JNA extracts out the relevant native library to /tmp and trys to load it, however if you follow security best practice and disable execute in /tmp (noexec mount option), this fails silently. The JNA documentation recommends you extract out the libraries on systems with additional security constraints; in fact the JNA package that ships with RHEL puts the relevant library in /usr/lib64/jna.

I see two potential solutions:
- src/main/java/org/elasticsearch/common/jna/CLibrary.java - log with warning on UnsatisfiedLinkError (rather than debug), and hint at noexec being a potential issue
- extract out the various libjnidispatch.so versions into the elasticsearch lib directory and get JNA to load from there
</description><key id="29961790">5493</key><summary>mlockall fails on noexec /tmp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-22T14:11:20Z</created><updated>2014-05-02T09:57:40Z</updated><resolved>2014-05-02T09:57:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-31T15:10:34Z" id="39099692">I'm very sympathetic to not have JNA unpack, but unfortunately this has several implications for packaging and deployment. However I just looked into this and found that while our current version of JNA does not support a custom directoy, the latest version does (see https://github.com/twall/jna/blob/master/src/com/sun/jna/Native.java#L1014), so that might be a way forward.

I'm not the biggest fan of simply logging the failure since that does not provide an actual solution to the problem and will only confuse too many people even more (who will then ask about how to turn off noexec).

In the meantime you can try:
- set the java.io.tempdir system property to a user-writable directory (maybe ES_HOME/tmp or somesuch)
- sysctl vm.swappiness=0 _with a recent kernel_. If you want to know why this has the same effect and is in fact less harmful to the system than mlockall, see:
  http://www.quora.com/Linux/Why-does-Linux-swap-out-pages-when-I-have-many-pages-cached-and-vm-swappiness-is-set-to-0-Shouldnt-cached-pages-get-resized-and-no-swapping-should-occur

Hope this helps for now.
</comment><comment author="jimmyjones2" created="2014-03-31T18:36:34Z" id="39124677">Hey, thanks for your response. I agree logging isn't ideal, but in its defence:
- #1194 already converted one class of mlockall failing to warn, but not this case (not sure why)
- Not many people have noexec /tmp, and those who do generally know they have, so I don't think it should confuse too many people
</comment><comment author="hhoffstaette" created="2014-04-01T08:04:15Z" id="39179859">Please have a look at https://github.com/hhoffstaette/elasticsearch/commit/3cb270cf4f7e2326eb7e924275b5187d9b140e09 and let me know if the message is clear enough.
</comment><comment author="jimmyjones2" created="2014-04-01T18:47:39Z" id="39243195">LGTM, thanks Holger!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to forbidden-apis 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5492</link><project id="" key="" /><description>Since March 19, forbidden-apis 1.4.1 is available (fully compatible with Java 8). Robert Muir added some missing signatures in the jdk-unsafe signatures file, so this should be applied to ES, too.

As far as I know, thre are no new failures, so this should be a one-line-change.
</description><key id="29959187">5492</key><summary>Update to forbidden-apis 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>enhancement</label><label>upgrade</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-22T11:18:14Z</created><updated>2015-06-07T15:00:21Z</updated><resolved>2014-03-22T19:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-03-22T21:52:24Z" id="38365323">Danke!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for lossless storage of BigDecimal numeric values in _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5491</link><project id="" key="" /><description>Issue #5380

Want to be able to save BigDecimal in and get BigDecimal back out - and not have any loss of info due to conversion to double on the way.

Not sure how it should be configured yet - just put a static field for now.
</description><key id="29949718">5491</key><summary>Add support for lossless storage of BigDecimal numeric values in _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:Mapping</label><label>stalled</label></labels><created>2014-03-22T00:04:10Z</created><updated>2016-03-08T14:11:00Z</updated><resolved>2016-03-08T14:11:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-22T08:50:21Z" id="38346372">Even if we had a configuration, I wonder if the right way is to parse all decimals to BigDecimal, that feels wrong somehow, and has bigger implications if done. Requires some thinking.

Btw, for now, we don't require someone using our XContentBuilder, or parsing, so someone can generate the JSON on their own, and get the source as bytes to parse it into a map.
</comment><comment author="jprante" created="2014-03-22T09:29:05Z" id="38346980">Maybe it is an idea to control output of lossless decimals by a method in the XContentBuilder similar to the pretty print function? Something like losslessDecimals(true) and losslessDecimals(false) which is false by default  (current behavior) ?
</comment><comment author="nickminutello" created="2014-03-22T14:52:51Z" id="38353497">That was my thinking. 

In financial apps (or any apps that do maths with big numbers) you tend to _only_ use BigDecimal. So treating all decimals as BigDecimal is fine (desired)

Yes, you could configure Jackson to do what I did here &amp; pass source in/out as a string - but it would be more convenient if it was just a feature to switch on in elastic. 

-Nick

&gt; On 22 Mar 2014, at 09:29, J&#246;rg Prante notifications@github.com wrote:
&gt; 
&gt; Maybe it is an idea to control output of lossless decimals by a method in the XContentBuilder similar to the pretty print function? Something like losslessDecimals(true) and losslessDecimals(false) which is false by default (current behavior) ?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="kimchy" created="2014-03-22T14:58:57Z" id="38353642">That would solve the builder aspect, and I am up for it (on the builder solution). Then the question is on the parser level when parsing (to Map for example)
</comment><comment author="nickminutello" created="2014-03-24T10:27:02Z" id="38429187">At the parsing level, the problem/solution is not dissimilar to what jackson has to deal with.

```
json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
```

IMO, there is a similar requirement for BigInteger - and its not dissimilar to how we parse LocalDate, DateTime ...

I think it should be something the user explicitly configures - its just a question of scope - is this a global setting for a VM? A setting for a Client - or for a given Response...
</comment><comment author="rpedela" created="2014-04-01T02:44:44Z" id="39165677">+1 for supporting BigDecimal in a lossless manner. My use case is indexing a Postgres `numeric` column with Elasticsearch which can be filtered and sorted correctly.

From a user's perspective, I am wondering if it makes more sense to add `bigdecimal` as a core type in the mapping? Or is the JSON deserialization settings all or nothing when it comes to handling floats?
</comment><comment author="jprante" created="2014-04-01T07:30:20Z" id="39177780">@rpedela one of my motivations is using BigInteger/BigDecimal in ES from the JDBC river.

It makes some sense to add BigInteger/BigDecimal also to the core mappings. The reason is, when BigInteger/BigDecimal is added only to XContentBuilder/XContentParser, and the REST API invokes doc mapper, it throws exceptions like BIG_INTEGER type missing.

I'm working on such a pull request for this, not ready yet:

https://github.com/jprante/elasticsearch/tree/bigdecimal

At the moment, for internal numeric operations such as range query, the "big" types would have to be converted to long / double - so lossless decimals are just in the field storing and matching where strings are used.

Another improvement would be adding BigDecimal/BigInteger to Lucene.
</comment><comment author="rpedela" created="2014-04-01T16:18:53Z" id="39225506">Great to hear. I would welcome Lucene support too.
</comment><comment author="nickminutello" created="2014-05-07T14:13:14Z" id="42431750">What is the plan for this (or, the more complete, I think PR #5683 ) ?
</comment><comment author="jpountz" created="2014-05-07T16:02:48Z" id="42446539">I think the PR is a good start, but on my end I'd like to see https://issues.apache.org/jira/browse/LUCENE-5596 fixed before moving forward on this issue. Otherwise these big integer/decimal types could only be realistically used for exact term queries which I don't expect to be very useful on such types.
</comment><comment author="nickminutello" created="2014-06-09T15:31:28Z" id="45504110">Can we not do this in two steps?
Its much more interesting for me to have full fidelity storage (ie I get back what I put in) than to have any lucene-level support.
</comment><comment author="jpountz" created="2014-08-22T08:03:34Z" id="53033744">@nickminutello If you only need storage, then you could store it as a string?

This other [issue](https://issues.apache.org/jira/browse/LUCENE-5879) could be an alternative to LUCENE-5596.
</comment><comment author="D1plo1d" created="2015-07-30T19:17:21Z" id="126443673">https://issues.apache.org/jira/browse/LUCENE-5879 looks as though it has been merged. Does that mean that getting an implementation of BigDecimal into ES is unblocked?

I could really use Big Decimal support!
</comment><comment author="clintongormley" created="2016-03-08T14:11:00Z" id="193798004">Closing in favour of https://github.com/elastic/elasticsearch/issues/17006
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Moved the template query documentation into search section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5490</link><project id="" key="" /><description>The template query functionality was mentioned as part of query templates, which was wrong...
</description><key id="29944583">5490</key><summary>[DOCS] Moved the template query documentation into search section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-03-21T22:29:29Z</created><updated>2014-07-16T21:47:29Z</updated><resolved>2014-03-25T09:06:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Freq Terms Enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5489</link><project id="" key="" /><description>A frequency caching terms enum, that also allows to be configured with an optional filter. To be used by both significant terms and phrase suggester.
This change extracts the frequency caching into the same code, and allow in the future to add a filter to control/customize the background frequencies
</description><key id="29942526">5489</key><summary>Freq Terms Enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-03-21T21:54:47Z</created><updated>2014-06-27T05:59:43Z</updated><resolved>2014-03-31T17:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-22T14:48:12Z" id="38353368">Aye, it is still missing (should be added in another change) the ability to parse and provide the filter form the sig terms agg
</comment><comment author="markharwood" created="2014-03-27T11:28:05Z" id="38791956">I wonder if the FreqTermsEnum needs splitting into two classes - one that handles filtering TTF and DF stats and a separate one that handles the responsibility of caching TTF and DF stats?
The SignificantTermsAggFactory has different scenarios where the decisions about filtered vs unfiltered and caching vs non caching are independent choices. If the significant terms agg is analysing a single bucket then caching is not required around calls to the underlying TermsEnum. If there is no "background stats context" defined for the significant terms (we need a new PR to design exactly how that will work) then there is no need for a filtered TermsEnum. These are independent choices.
So the ability to wrap a raw TermsEnum with layers that provide these features as required seems more appropriate.
</comment><comment author="s1monw" created="2014-03-27T12:51:27Z" id="38798392">++ @markharwood 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Able to have different _id value than the one in the _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5488</link><project id="" key="" /><description>I have edited this issue since the previous was wrong.

Is it possible to have a different `_id` value in ES than the one inside the `_source`? The following is a bulk action:

```
action = {
    '_type': 't',
    '_id': '52cb45cec36b4442751728f5',
    '_source': {
        u'city': u'Toronto',
        u'name': u'PostBeyond',
        u'_id': {
            '$oid': '52cb45cdc36b4442751728f4'
        },
        u'events': {
            u'title': u'ExtremeCachingwithPHP',
            u'event_id': {
                '$oid': '52cb45cec36b4442751728f5'
            },
            u'start_date': u'2014-01-08T00:00:00+00:00'
        }
    },
    '_index': 'i'
}
```

If we give manually the `_id` ES not to search inside the source document.

It would have been useful to cooperate with mongodb or generally if we want to have in the `_source` the field `_id`.

Relevant documentation: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-id-field.html#mapping-id-field
</description><key id="29930854">5488</key><summary>Able to have different _id value than the one in the _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Diolor</reporter><labels /><created>2014-03-21T19:49:28Z</created><updated>2014-03-26T15:46:27Z</updated><resolved>2014-03-26T15:46:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-26T13:57:49Z" id="38685833">I'm sorry but I don't understand your question at all. Could you try explaining a bit more?
</comment><comment author="Diolor" created="2014-03-26T14:58:44Z" id="38693679">Yes; sorry for the badly written issue. 

Elasticsearch always searches for an `_id` inside the document which is going to be indexed.
In MongoDB, each document has an `_id` field which is in a `{'_id': { '$oid': '52cb45cdc36b4442751728f4' } }` format. e.g. the document:

 `{'_id': { '$oid': '52cb45cdc36b4442751728f4' },'testkey':'testvalue'}`

I am working on adding elasticsearch as searching layer so I cannot keep the same structure both in MongoDB and ES. 

I either have to rename `_id` inside the source to `id` (or something else) or set `_id` to a `{'_id': '52cb45cdc36b4442751728f4' }` format happily to be handled by ES. 

Generally if I could disable ES searching inside the document for an `_id` field (at least in bulk feed where you can set/give the `_id` on the same level with _source) I would be able use the `_id` inside the _source without any limitations of its structure.

However this is a minor issue since can be solved in the webservice or end client.
</comment><comment author="clintongormley" created="2014-03-26T15:46:27Z" id="38700183">On v1.x, the `{"_id": { "$oid"...}` is ignored because it is an object rather than a string, so you should be fine when you upgrade. However, I think there is still an issue here when you use documents like `{ "_id": "xxx"}`, so I've opened #5558.

What you could do with your current version is to map the _id field to use `_id.$oid` explicitly:

```
PUT /myindex/
{
    "mappings": {
        "mytype": {
            "_id": {
                "path": "_id.$oid"
            }
        }
    }
}
```

I'll close this in favour of #5558
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default Similarity model option setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5487</link><project id="" key="" /><description>I wanted to change my default similarity to the BM25 similarity so i included this on my YAML file 

```
index.similarity.default.type: BM25
```

however im noticing my "b" option  value is set at 0.75, i would like to change this to 0

is there a way i can set a default similarity with options on the YAML?

was using this as ref: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-similarity.html#bm25
</description><key id="29930134">5487</key><summary>Default Similarity model option setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jrizzi1</reporter><labels /><created>2014-03-21T19:42:06Z</created><updated>2015-11-25T11:04:12Z</updated><resolved>2014-03-26T11:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-26T11:55:45Z" id="38674921">You can setup custom similarities, and specify your own value for `b`.  Seee http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-similarity.html#bm25
</comment><comment author="vi3k6i5" created="2015-11-25T11:04:12Z" id="159572755">Is the same thing possible in yml file ??

say something like 
index.similarity.default.b: 0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug when highlighting on field with more than 50 matching fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5486</link><project id="" key="" /><description>Hi Elasticsearch. I'm presenting an issue that has been sitting on the mailing list for a while without attention, I think that I've found a highlighting issue when a field has a "large" number of fragments, specifically edge ngram fragments.

I have a gist that easily reproduces the issue. When attempting smaller hashes with the same analyzer, the highlight result is what I would expect. It's only when using longer hash strings that this issue occurs. Sorry if this is just a misunderstanding of how to use the highlighter, but, it certainly seems to be a bug. Thanks very much for looking into this issue.

Repro gist: https://gist.github.com/jonpaul/d4a9aa7f9c8741933cf5

[Edits:]
This issue was verified on the mailing list, here: https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/DeC5JXqY78M

I think a realistic, simple solution would be to make that property (MAX_NUM_TOKENS_PER_GROUP) configurable, so that users can specify unusually large numbers if they believe their system is capable of handling it.
</description><key id="29910012">5486</key><summary>Bug when highlighting on field with more than 50 matching fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jonpaul</reporter><labels><label>:Highlighting</label></labels><created>2014-03-21T15:39:02Z</created><updated>2016-11-24T18:25:54Z</updated><resolved>2016-11-24T18:25:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-23T20:26:41Z" id="44056545">The [highlighter](https://github.com/wikimedia/search-highlighter) I've been working on seems to handle this correctly.  Might not be a super efficient use case for it, but it does work.
</comment><comment author="clintongormley" created="2016-11-24T18:25:54Z" id="262829030">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add reverse nested aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5485</link><project id="" key="" /><description>Similar to the `nested` aggregator, but in reverse. It allows to create buckets and metrics based on the parent docs in the buckets of child documents.

This aggregation only makes sense inside a `nested` aggregator. Assume a ticket system with where the comments are embedded into the issue document. The `reverse_nested` aggregator would look like this:

``` json
"aggs" : {
   "top_commenters_to_issues" : {
        "nested" : {"path" : "comments"},
        "aggs" : {
            "top_commenters" : {
                "terms" : { "field" : "username" },
                "aggs" : {
                    "top_tags_to_comments" : {
                        "reverse_nested" : {
                            "path" : "comments"
                        },
                        "aggs" : {
                            "top_tags" : { "terms" : { "field" :  "tags"}}
                        }    
                    }
                }
            }
        }
    }
}
```

This aggregation would return top username buckets and per username bucket the top tags. The important constraint that the `nested` and `reverse_nested` add here is that only tag buckets are associated to username buckets that reside in the nested structure.
</description><key id="29901015">5485</key><summary>Add reverse nested aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-21T13:55:04Z</created><updated>2015-06-06T18:34:54Z</updated><resolved>2014-04-30T17:27:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-04-30T17:27:00Z" id="41824451">Closed via: https://github.com/elasticsearch/elasticsearch/commit/be12bcdbac51d97765b8ba196e0fcfcd6bce8246
</comment><comment author="speedplane" created="2014-05-26T05:25:55Z" id="44158257">This little feature is actually quite amazing. Aggregations can now get really complex and powerful.
</comment><comment author="orenorgad" created="2014-06-08T12:23:20Z" id="45435544">Are we be able to filter the nested documents for which the aggregation will work on?
e.g, let's say I want to aggregate comments registered on specific date range (and of course distinct users). Is this possible at the moment?  
</comment><comment author="speedplane" created="2014-06-08T12:57:21Z" id="45436154">Yup, you just wrap it with a filter aggregation: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-filter-aggregation.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Allow to run scripts/templates stored in .scripts index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5484</link><project id="" key="" /><description>This will be handy if search templates for example are stored in .scripts/templates index or something of the like. This can allow for recursive execution of templates, as well as "user defined queries" more easily, since changing the query/search request is just a matter of updating the query.

This will also allow to more easily disable dynamic scripts, and just allow loading them form FS or another index, with the other index allowing to easily change existing scripts.
</description><key id="29890780">5484</key><summary>Scripting: Allow to run scripts/templates stored in .scripts index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-21T10:31:51Z</created><updated>2014-07-21T10:21:08Z</updated><resolved>2014-07-14T14:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-24T04:26:26Z" id="38411525">Thought a bit more about it, it might make sense to expose the CRUD operations on a script through dedicated APIs (that will still result in storing the scripts as a document in .scripts index). This will allow to better control the structure of it through a more structured API, and will allow to issue an internal command to nodes to clear caches of a script(s) if a script was updated.
</comment><comment author="GaelTadh" created="2014-07-14T14:40:42Z" id="48907889">By https://github.com/elasticsearch/elasticsearch/commit/e79b7086de26ece61edaca74fcf7dc99a11de486
</comment><comment author="clintongormley" created="2014-07-21T09:45:41Z" id="49587658">Closed by #5921
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unexpected mvel update NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5483</link><project id="" key="" /><description>Hello,

We hit an interesting bug a few days ago with our testing cluster. It appears that if we do 2 consecutive updates to a document, **right after an elasticsearch restart**, the second update fails with `NullPointerException`. If we run the test script a second time (without restarting the server), the update succeeds.

Our test script that demostrates the issue:

```
# sudo /etc/init.d/elasticsearch restart &amp;&amp; sleep 10 &amp;&amp; bash test_es_mvel_issue

echo "Delete everything"
curl -X DELETE 'http://localhost:9200/_all?pretty'

echo "Create test index"
curl -X PUT 'http://localhost:9200/test?pretty'

echo "Add a document"
curl -X PUT 'http://localhost:9200/test/tweet/1?pretty' -d '{
  "smth": "m"
}'

echo "Issue an update using mvel"
curl -X POST 'http://localhost:9200/test/tweet/1/_update?pretty' -d '{
  "script": "foreach(field : updates.entrySet()) { ctx._source[field.key] = field.value; }",
  "params": {
    "updates": {
      "1": 1,
      "2": 1,
      "3": 1,
      "availability": null,
      "5": 1,
      "a": 1,
      "b": 1,
      "c": 1,
      "d": 1,
      "e": 1,
      "f": 1,
      "g": 1,
      "h": 1,
      "i": 1,
      "j": 1,
      "k": 1,
      "l": 1,
      "m": 1,
      "n": 1,
      "o": 1,
      "p": 1,
      "q": 1,
      "r": 1,
      "s": 1,
      "t": 1,
      "flter": 1
    }
  }
}'

echo "Issue a second update"
curl -X POST 'http://localhost:9200/test/tweet/1/_update?pretty' -d '{
  "script": "foreach(field : updates.entrySet()) { ctx._source[field.key] = field.value; }",
  "params": {
    "updates": {
      "1": 1,
      "2": 1,
      "3": 1,
      "availability": null,
      "5": 1,
      "a": 1,
      "b": 1,
      "c": 1,
      "d": 1,
      "e": 1,
      "f": 1,
      "g": 1,
      "h": 1,
      "i": 1,
      "j": 1,
      "k": 1,
      "l": 1,
      "m": 1,
      "n": 1,
      "o": 1,
      "p": 1,
      "q": 1,
      "r": 1,
      "s": 1,
      "t": 1,
      "flter": 1
    }
  }
}'

#2014-03-20T15:20:30+02:00 [400] (0.016s)
#
# {
#   "error": "ElasticSearchIllegalArgumentException[failed to execute script]; nested: NullPointerException; ",
#   "status": 400
# 
# }

```

The bug is reproducible to all elasticsearch versions (0.90, 1.0, 1.1, master).

Some interesting points that we noticed:
- It seems to depend on the payload, if we remove an attribute from the `updates` hash everything works.
- If we add some whitespace to the second's update mvel script, the update succeeds. Probably because it skips a cache.
</description><key id="29884526">5483</key><summary>Unexpected mvel update NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctrochalakis</reporter><labels><label>v0.18.3</label></labels><created>2014-03-21T08:14:25Z</created><updated>2014-05-07T16:00:23Z</updated><resolved>2014-05-07T16:00:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-31T10:14:49Z" id="39072909">The good news: I can reproduce this successfully - thanks for the detailed instructions.

The less good news is that it looks like a bug in MVEL's embedded ASM version:
http://jira.codehaus.org/browse/MVEL-308

The bug manifests itself in that creating the document with availability=null works, but MVEL's ASM optimizer stumbles over the second update since it again contains availability=null.
I'm not sure why this happens only once, but it does so repeatably after a restart.
Updating an initially created document with a non-null value works fine.
</comment><comment author="ctrochalakis" created="2014-03-31T13:05:42Z" id="39085605">Great, I am glad that you have pinpointed the problem. Probably a typo in The MVEL source.

But, yes, it's very strange that it happens only after a restart.. And also, why does removing an non-null value from both updates fixes everything?
</comment><comment author="hhoffstaette" created="2014-03-31T13:25:24Z" id="39087401">That is indeed weird, and I have no idea why it does not trip over the same bug. Nevertheless MVEL must be fixed first, then we can see where this goes.
</comment><comment author="hhoffstaette" created="2014-04-15T15:14:04Z" id="40493806">Fixed upstream in https://github.com/mvel/mvel/commit/f61874a846114845c0a812d7782f9fd62aad9e37
</comment><comment author="spinscale" created="2014-05-05T14:38:17Z" id="42194421">@ctrochalakis is it possible to test with master/1.x branch and report if it works for you?
</comment><comment author="kevinkluge" created="2014-05-05T20:11:56Z" id="42233221">Note that @kimchy upgraded ES to MVEL 2.2.0, which was released after the f61... commit referenced by Holger.  However, Holger's referenced bug remains open.  I'm not sure what state the MVEL fix is in.
</comment><comment author="ctrochalakis" created="2014-05-06T08:46:40Z" id="42279214">No unfortunately it doesn't  work with master
`version[2.0.0-SNAPSHOT], pid[31544], build[76463ee/2014-05-06T08:35:33Z]`

I had to enable dynamic scripting for the test to run.
</comment><comment author="kimchy" created="2014-05-07T16:00:16Z" id="42446206">I will close this for now, sadly, there is nothing we can further do on our side... . We do plan to replace mvel in the near future... . Maybe for now you can try a different language plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allocation Awareness Attribute Value Bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5482</link><project id="" key="" /><description>The cluster routing allocation awareness attribute value cannot be a number only. Is this expected behavior?  For now I am marking it as a bug.

For example if I have the following in a config:
###### 

cluster.routing.allocation.awareness.force.update.values: 0,1,2
cluster.routing.allocation.awareness.attributes: update

node.update: 1
###### 

The values seem to not be recognized and routing is done incorrectly.
</description><key id="29869200">5482</key><summary>Allocation Awareness Attribute Value Bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jm4games</reporter><labels /><created>2014-03-21T00:03:07Z</created><updated>2014-12-30T14:32:53Z</updated><resolved>2014-12-30T14:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:32:53Z" id="68360571">Hi @jm4games 

Sorry it has taken a while to look at this issue.  I've just tried this out on v1.4.2 and it seems to be working correctly with numeric values 0,1,2.  

If you are still seeing this problem, please feel free to reopen with more details.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo joda-time link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5481</link><project id="" key="" /><description>Found broken link in doc of time format.
</description><key id="29863499">5481</key><summary>fix typo joda-time link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">johtani</reporter><labels><label>docs</label></labels><created>2014-03-20T22:22:04Z</created><updated>2014-07-03T13:00:52Z</updated><resolved>2014-03-21T09:06:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-21T09:06:44Z" id="38258616">Merged, thanks a lot @johtani !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node version sometimes empty in _cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5480</link><project id="" key="" /><description>Sometimes a node will not list its version in `_cat/nodes`.

```
drewr@client-002 Thu 20 17:04:28 30 0 ~()% curl localhost:9200/_cat/nodes\?h=host,ip,name,version,jvm
data-001   100.67.136.16 Nick Fury      1.0.1
client-001 100.67.182.91 Melody Guthrie
client-002 100.67.184.5  Nightwind      1.0.1
```

`client-001` has the same version of ES:

```
drewr@client-002 Thu 20 17:04:31 31 0 ~()% curl -s client-001:9200
{
  "status" : 200,
  "name" : "Melody Guthrie",
  "version" : {
    "number" : "1.0.1",
    "build_hash" : "5c03844e1978e5cc924dab2a423dc63ce881c42b",
    "build_timestamp" : "2014-03-17T02:02:39Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="29837545">5480</key><summary>Node version sometimes empty in _cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T17:07:21Z</created><updated>2015-06-07T22:29:41Z</updated><resolved>2014-05-07T16:08:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-31T09:01:11Z" id="39067068">I can't reproduce this - version always comes back. What I never get back is a value for "jvm" though. EDIT: which is apparently because there's no such value..
</comment><comment author="s1monw" created="2014-04-14T09:01:16Z" id="40345668">@drewr  do you have a fix for this?
</comment><comment author="kimchy" created="2014-05-07T15:40:58Z" id="42443531">this can happen if the node is in the cluster state, but not yet visible across the cluster, @drewr did it happen on startup, or shutdown of a node?
</comment><comment author="kimchy" created="2014-05-07T15:42:40Z" id="42443777">ok, this might happen, but we can still get the version from the `DiscoveryNode` instead from the nodes info response, I will push a fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add simple escape method for special characters to template query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5479</link><project id="" key="" /><description>The default mustache engine was using HTML escaping which breaks queries
if used with JSON etc. This commit adds escaping for:

```
\b  Backspace (ascii code 08)
\f  Form feed (ascii code 0C)
\n  New line
\r  Carriage return
\t  Tab
\v  Vertical tab
\"  Double quote
\\  Backslash
```

Closes #5473
</description><key id="29833608">5479</key><summary>Add simple escape method for special characters to template query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-03-20T16:22:52Z</created><updated>2014-06-22T15:37:30Z</updated><resolved>2014-03-20T17:55:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-03-20T17:37:50Z" id="38197637">+1
</comment><comment author="jpountz" created="2014-03-20T17:38:43Z" id="38197763">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto-Closing polygons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5478</link><project id="" key="" /><description>GeoJSON defines linear rings as close if the first and the last point is equal. This path modifies the shape builders to close these linear rings automatically.

Closes #4085
</description><key id="29827980">5478</key><summary>Auto-Closing polygons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2014-03-20T15:23:25Z</created><updated>2014-07-04T07:37:19Z</updated><resolved>2014-07-04T07:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-04T07:37:19Z" id="48016362">Closing pull request as we need to add an option in the mapping to specify whether to auto-close polygons with this change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DateHistogram.Bucket should return the date key in UTC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5477</link><project id="" key="" /><description>To keep the API consistent and predictable 
</description><key id="29814469">5477</key><summary>DateHistogram.Bucket should return the date key in UTC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T12:28:35Z</created><updated>2015-06-07T22:30:01Z</updated><resolved>2014-03-20T12:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add BoolFilterBuilder#hasClauses to be consitent with BoolQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5476</link><project id="" key="" /><description>Closes #5472
</description><key id="29813176">5476</key><summary>Add BoolFilterBuilder#hasClauses to be consitent with BoolQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T12:06:38Z</created><updated>2015-06-07T15:01:21Z</updated><resolved>2014-03-20T12:10:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-20T12:07:44Z" id="38159329">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean the query parse context after usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5475</link><project id="" key="" /><description>that will make sure we don't have any parsers lying around that are no longer used
</description><key id="29812907">5475</key><summary>Clean the query parse context after usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T12:01:41Z</created><updated>2015-06-07T15:00:55Z</updated><resolved>2014-03-20T12:17:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-20T12:08:00Z" id="38159348">LGTM 
</comment><comment author="kimchy" created="2014-03-20T12:17:06Z" id="38159999">closed in 1.x and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw error when updating binary field mapping to be stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5474</link><project id="" key="" /><description>Trying to set an existing binary field from `store:false` to `store:true` should throw an error.  Currently it is just silently ignored:

```
PUT /t
{
  "mappings": {
    "foo": {
      "properties": {
        "bar": {
          "type": "binary"
        }
      }
    }
  }
}

PUT /t/_mapping/foo
{
  "properties": {
    "bar": {
      "type": "binary",
      "store": true
    }
  }
}

GET /t/_mapping
```
</description><key id="29812089">5474</key><summary>Throw error when updating binary field mapping to be stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T11:46:55Z</created><updated>2014-03-31T11:13:36Z</updated><resolved>2014-03-31T11:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-03-24T03:57:34Z" id="38410643">Similar thing with `boolean` and `geo_point` fields.

#5502 and #5505 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mustache templates should escape JSON, not HTML</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5473</link><project id="" key="" /><description>Currently we're using the default  `escape` method from Mustache, which is intended for escaping HTML, not JSON.

This results in things like `"` -&gt; `&amp;quot;`

Instead, we should be using these escapes:

```
\b  Backspace (ascii code 08)
\f  Form feed (ascii code 0C)
\n  New line
\r  Carriage return
\t  Tab
\v  Vertical tab
\"  Double quote
\\  Backslash 
```
</description><key id="29810210">5473</key><summary>Mustache templates should escape JSON, not HTML</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T11:13:00Z</created><updated>2014-03-20T17:55:23Z</updated><resolved>2014-03-20T17:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-20T12:17:36Z" id="38160029">Test case:

```
DELETE /t

PUT /t
{
  "mappings": {
    "foo": {
      "properties": {
        "bar": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}

PUT /t/foo/1
{
  "foo": "bar&amp;"
}

GET /_search/template
{
  "template": {
    "query": {
      "term": {
        "foo": "{{foo}}"
      }
    }
  },
  "params": {
    "foo": "bar&amp;"
  }
}
```
</comment><comment author="s1monw" created="2014-03-20T12:44:55Z" id="38162142">cool I will take a look at it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add hasClauses method to BoolFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5472</link><project id="" key="" /><description>Would be nice to have the hasClauses() method as in BoolQueryBuilder.
</description><key id="29806623">5472</key><summary>Add hasClauses method to BoolFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">abaxanean</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T10:11:32Z</created><updated>2014-03-20T12:10:19Z</updated><resolved>2014-03-20T12:10:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PUT /_aliases should accept a numeric routing value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5471</link><project id="" key="" /><description>Also added REST tests for setting index/search/routing
via the PUT /_aliases endpoint

Fixes #5465
</description><key id="29804063">5471</key><summary>PUT /_aliases should accept a numeric routing value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aliases</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T09:28:29Z</created><updated>2015-06-07T22:31:49Z</updated><resolved>2014-03-20T09:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-20T09:29:43Z" id="38147949">LGTM - this should go to all branches I guess? @clintongormley can you tag the PR / issue
</comment><comment author="clintongormley" created="2014-03-20T09:38:55Z" id="38148629">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow iteration over MultiGetRequest#Item instances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5470</link><project id="" key="" /><description>Closes #3061
</description><key id="29803953">5470</key><summary>Allow iteration over MultiGetRequest#Item instances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T09:26:43Z</created><updated>2015-06-07T15:01:39Z</updated><resolved>2014-03-20T09:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-20T09:27:14Z" id="38147788">LGTM
</comment><comment author="jpountz" created="2014-03-20T09:28:05Z" id="38147854">+1 I like the unmodifiableIterator wrapper
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Category type should be called "category" instead of "field" in context suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5469</link><project id="" key="" /><description>Change according to documentation.
</description><key id="29795628">5469</key><summary>Category type should be called "category" instead of "field" in context suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-20T05:53:41Z</created><updated>2015-06-07T22:32:37Z</updated><resolved>2014-03-21T19:06:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-20T09:02:21Z" id="38146204">LGTM please go ahead and push
</comment><comment author="s1monw" created="2014-03-21T19:06:57Z" id="38311570">closeing - this in on all relevant branches see 44f52e474cc68e2dbd4515304f0fd7f109951da5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adjust PMD settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5468</link><project id="" key="" /><description>1) supressed pmd buggy LoosePackageCoupling rule
2) upgraded pmd plugin to 3.1
3) removed reports plugin warnings when generating dependencies
4) corected plugin section formatting
</description><key id="29794698">5468</key><summary>Adjust PMD settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-03-20T05:21:20Z</created><updated>2014-06-19T20:20:53Z</updated><resolved>2014-06-19T20:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-06-19T20:20:53Z" id="46611503">Not needed anymore, sonarcube is online
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rolling upgrade and full cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5467</link><project id="" key="" /><description>The 1.0.0 says "You will need to do a full cluster restart when upgrading, but we hope to make these a thing of the past in the 1.x branch&#8201;&#8212;&#8201;a number of features have been added to facilitate rolling upgrades going forward."

Does this mean going forward from 1.0.0, there will be no need to do a full cluster restart when upgrading (to a higher than 1.0.0 version)?

If, instead of using a cluster, each node is configured as a 1-node ES (i.e. not cluster), and we use 'tribe' to do federated search across multiple ES nodes, would that work if the ES nodes were at different revs?
</description><key id="29776970">5467</key><summary>rolling upgrade and full cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sganesh1</reporter><labels /><created>2014-03-19T22:03:56Z</created><updated>2014-04-04T16:21:47Z</updated><resolved>2014-04-04T16:21:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T16:21:47Z" id="39583477">Hi @sganesh1 exactly that means we have changed our infrastructure to support rolling upgrades more easily. Moving forward to 1.1, 1.2 and so on, you won't need a full cluster restart. You might need it for 2.0 though as it's a major version that will most likely break backwards compatibility.

As for your question on tribe node, it uses the internal tranport layer to communicate with the different nodes, thus the same applies. You can mix different minor versions potentially, although not recommended since it's seen as a temporary solution while upgrading only. That wouldn't work with major versions (e.g. 0.90.x with 1.x).

May I ask you to send this type of questions to the mailing list the next time please as we tend to use github issues for bugs or feature requests?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Expontial performance degradation and increase std. deviation when specifying "size" param greater than 999999</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5466</link><project id="" key="" /><description>(Now first of all, this scenario is ridiculous and quite unscientific in its method. Furthermore, this is not blocking us in any way shape or form but we found it interesting so I thought I would report it anyway.)

One of our clients has a use case where they want all search hits to be returned without pagination. Typically, in this case, results sets max out at around 300 documents. Since they want all results, and there is no ALL option for size, the developer choose to use 999999999 (9 9's) as the size. While silly, this is still well within the limits of an Integer in Java and was just meant to signify something like MAX_INT. 

The result was a query that took on average between 2000 and 5000ms for 229 total hits. They reported this issue to us and we investigated. Now the interesting part - reducing the size parameter by a factor of 10 (remove one 9) showed a similar factor of 10 reduction in performance time. So 99999999 (8 9's) loads on average in 200 to 500ms. Reduce by another factor of 10 (7 9's) and it drops almost another factor of 10. At 7 9's loads times are between 40 and 150ms. This is still a much higher std deviation than typical for repeat search's for the exact same search...likely cached.

At 6 9's, the results are more in line with expectations and have a much smaller std deviation at 20-30ms on avg. 

This test was done on an isolated 2 node cluster with no other activity on the system. The same query was executed for all tests with the only difference being the size param. 
</description><key id="29764930">5466</key><summary>Search API: Expontial performance degradation and increase std. deviation when specifying "size" param greater than 999999</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobbyhubbard</reporter><labels /><created>2014-03-19T19:31:33Z</created><updated>2014-09-23T18:13:41Z</updated><resolved>2014-09-23T17:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-19T19:47:20Z" id="38098168">To extract data from elasticsearch you should use [scan&amp;scroll API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html).

Using `size` is not the way to go.

Closing. Feel free to reopen if I misunderstood your use case. 
</comment><comment author="bobbyhubbard" created="2014-03-19T20:42:52Z" id="38104150">Its a performance defect that someone could exploit.

My point is that some unsuspecting user typing a simple search query with a size &gt;999999 could have significant performance impact on a cluster as response time seems to increase exponentially until MAX_INT at which point you get an index out of bounds from the json parser. Of course specifying a size that large is not optimal... but I'm not always the one writing the query. 

I'm unable to reopen but if I could, i would. :)
</comment><comment author="nik9000" created="2014-03-19T20:54:16Z" id="38105454">It might be worth having a cluster wide max size that could be configured
to reject requests like this.  It opens up a can of worms, too.  Like, you
should probably excuse scan type queries.

On Wed, Mar 19, 2014 at 4:42 PM, bobbyhubbard notifications@github.comwrote:

&gt; My point is that some unsuspecting user typing a simple search query with
&gt; a size &gt;999999 could have significant performance impact on a cluster as
&gt; response time seems to increase exponentially until MAX_INT when you an
&gt; index out of bounds from the json parser. Of course specifying a size that
&gt; large is not optimal... but I'm not always the one writing the query. Its a
&gt; performance defect that someone could exploit.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5466#issuecomment-38104150
&gt; .
</comment><comment author="dadoonet" created="2014-03-19T21:02:39Z" id="38106381">I agreed that we should perhaps come with reasonable defaults (500?) which can be set.
What does others think?

Reopening.
</comment><comment author="uboness" created="2014-03-20T08:23:47Z" id="38143776">It's not just about the json... There are many factors that play here (the priority queues that are responsible for the sorting and the size of the docs to name a couple). I agree that this should ideally be handled gracefully, probably by introducing another circuit breaker - and the response should be handled like all other CBs we have. It's a bit tricky though to figure out the proper thresholds for this CB... Will require some thought.

Btw, I'm not sure that the error belongs to the 500 range as this should not be perceived as a system error... 
</comment><comment author="skade" created="2014-09-23T16:57:10Z" id="56552835">I had a run-in with this bug today, where the maximum size was set to an "arbitrary high value" (9999999) because the number of potential responses was small (&lt;100) and it was meant to retrieve "all". That query degraded to a point where it brought down the whole cluster node by node. That's a naive approach I see from time to time. In the end, the query retrieved only &lt; 100kb payload.
The query was sent to the type-specific end-point `/*index*/*type*`.

The response time grew with the number of documents in the whole index, not showing itself on small datasets (dev), less on slightly larger (stage) and disastrous (largest time was 18m) on the live system. In Prod, it lead to sudden allocations of huge chunks of heap that would immediately be collected by a stop the world collection of multiple seconds, leading to nodes dropping out of the cluster and the search queue exploding. It seems like there is something leading to such a query to visit more documents then necessary. I'll try to build a testcase.

While this is misuse, I would expect Elasticsearch to handle such cases more gracefully. Also, the behaviour of "size" should be better documented.
</comment><comment author="clintongormley" created="2014-09-23T17:02:21Z" id="56553635">Closing in favour of #4026
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding "routing" setting for an alias via POST _aliases has different behavior than PUT (single alias)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5465</link><project id="" key="" /><description>Hi there,

I've noticed an issue that either points to me misunderstanding something about routing in Elasticsearch or a subtle bug in elasticsearch itself. Here's what I'm trying to do:
- create an alias with a routing parameter and a term filter ( following the [single index + routing idea](http://www.elasticsearch.org/videos/big-data-search-and-analytics/) for multi-user indices)

When trying to create aliases, we noticed that the routing parameter was getting dropped from our alias's definition. Since we put documents into the index with the routing param, but only query using the alias, documents weren't getting returned most of the time even if they were in the index. 

Upon debugging, we found that the POST _aliases endpoint silently drops the routing parameter if it's an integer, but honors it if it's a string. Oddly, though, the PUT endpoint to create a single alias works for a string or numeric routing param.

See the test case below.

```
curl -X PUT 'http://localhost:9200/test1?pretty'

curl -X POST 'http://localhost:9200/_aliases?pretty' -d '{
  "actions":[
    {
      "add":{
        "index":"test1",
        "alias":"test1_1",
        "routing":1,
        "filter":{
          "term":{
            "user_id":1
          }
        }
      }
    }
  ]
}'


curl -X GET 'http://localhost:9200/_alias/test1_1?pretty'
#2014-03-19T14:07:28-04:00 [200] (0.002s)
#
# {
#   "test1":{
#     "aliases":{
#       "test1_1":{
#         "filter":{
#           "term":{
#             "user_id":1
#           
# }
#         }
#       }
#     }
#   }
# }


curl -X DELETE 'http://localhost:9200/test1/_alias/test1_1?pretty'

curl -X PUT 'http://localhost:9200/test1/_alias/test1_1?pretty' -d '{
  "routing":1,
  "filter":{
    "term":{
      "user_id":1
    }
  }
}'

curl -X GET 'http://localhost:9200/_alias/test1_1?pretty'
#2014-03-19T14:07:28-04:00 [200] (0.002s)
#
# {
#   "test1":{
#     "aliases":{
#       "test1_1":{
#         "filter":{
#           "term":{
#             "user_id":1
#           
# }
#         },
#         "index_routing":"1",
#         "search_routing":"1"
#       }
#     }
#   }
# }

# NOW try a STRING for routing

curl -X DELETE 'http://localhost:9200/test1/_alias/test1_1?pretty'

curl -X POST 'http://localhost:9200/_aliases?pretty' -d '{
  "actions":[
    {
      "add":{
        "index":"test1",
        "alias":"test1_1",
        "routing":"1",
        "filter":{
          "term":{
            "user_id":1
          }
        }
      }
    }
  ]
}'

curl -X GET 'http://localhost:9200/_alias/test1_1?pretty'
# {
#   "test1" : {
#     "aliases" : {
#       "test1_1" : {
#         "filter" : {
#           "term" : {
#             "user_id" : 1
#           }
#         },
#         "index_routing" : "1",
#         "search_routing" : "1"
#       }
#     }
#   }
# }


```

Our ES version we're testing against:

`curl -XGET 'http://localhost:9200'`

which returns:

```
{
  "status" : 200,
  "name" : "Jamie Braddock",
  "version" : {
    "number" : "1.0.1",
    "build_hash" : "5c03844e1978e5cc924dab2a423dc63ce881c42b",
    "build_timestamp" : "2014-02-25T15:52:53Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```

Here's the ruby program we initially used to debug inside the Rails console (easier to change the routing param from int to string - i just cut &amp; pasted the curl commands from our logging there. Guess I could set an ENV var for the shell stuff, but I leave that to the user :smile: )

```
client = Rails.configuration.elasticsearch

index = "test1"
filter_id = 1
unique_alias_name = "test1_#{filter_id}"
filter_hash = { "term" =&gt; { "user_id" =&gt; filter_id } }

client.indices.create(index: index)
client.indices.update_aliases body: {
  actions: [
    { add: { index: index, alias: unique_alias_name, routing: filter_id, filter: filter_hash } }
  ]
}

result1 = client.indices.get_alias name: unique_alias_name
puts result1

client.indices.delete_alias index: index, name: unique_alias_name

if client.indices.exists_alias(name: unique_alias_name)
  puts "didn't delete" 
  exit 1
end

client.indices.put_alias index: index, name: unique_alias_name, body: {
  routing: filter_id,
  filter: filter_hash
}

result2 = client.indices.get_alias name: unique_alias_name
puts result2


unless result1 == result2
  puts "Why don't these match!!" 
  exit 1
end
```
</description><key id="29759507">5465</key><summary>Adding "routing" setting for an alias via POST _aliases has different behavior than PUT (single alias)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sujal</reporter><labels /><created>2014-03-19T18:26:59Z</created><updated>2014-03-20T09:38:37Z</updated><resolved>2014-03-20T09:38:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>1.0.1 Java API compilation error on source download</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5464</link><project id="" key="" /><description>After downloading sources i started to get compilation errors like

```
java: for-each not applicable to expression type
  required: array or java.lang.Iterable
  found:    org.elasticsearch.common.collect.UnmodifiableIterator&lt;org.elasticsearch.cluster.metadata.IndexMetaData&gt;
```

With 0.90x versions this was not a problem. What is temporary solution and what is proper solution?
</description><key id="29754199">5464</key><summary>1.0.1 Java API compilation error on source download</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">nfx</reporter><labels><label>feedback_needed</label></labels><created>2014-03-19T17:28:43Z</created><updated>2014-12-03T15:50:03Z</updated><resolved>2014-12-03T15:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-31T08:30:13Z" id="39064562">It would be helpful to see the context (aka source file + line) where you get this error.
</comment><comment author="spinscale" created="2014-03-31T11:30:09Z" id="39078156">do you run into this problem with windows with java 7? can you try to start elasticsearch like this:

```
bin/elasticsearch.bat -XX:-UseSplitVerifier
```

does this change anything?
</comment><comment author="nfx" created="2014-04-09T12:51:47Z" id="39959770">this issue does not affect ES itself, it affects working with it's Java API, when looking at the sources
</comment><comment author="clintongormley" created="2014-08-22T10:21:13Z" id="53045338">hi @nfx 

Did you manage to resolve this issue?
</comment><comment author="nfx" created="2014-08-25T13:37:12Z" id="53264158">@clintongormley hi, i managed to do it only as a hack - i had to manually run perl command replacing com.google.guava.collect with org.elasticsearch.collect or smth like it. i didn't need to do it with v0.90 though. could you maybe try to update source artifact compilation to be fixed after mvn-shade plugin gets guava and jodatime inside of elasticsearch artifact?
</comment><comment author="clintongormley" created="2014-08-25T14:29:43Z" id="53270827">Could this be related to #3557?
</comment><comment author="spinscale" created="2014-08-26T06:35:13Z" id="53380875">@clintongormley doesnt look like it
</comment><comment author="clintongormley" created="2014-08-26T08:40:44Z" id="53391132">@nfx I don't follow the problem or the solution that you employed exactly. Could you show us some example code demonstrating the issue?
</comment><comment author="nfx" created="2014-08-27T09:48:38Z" id="53548104">@clintongormley it could be related to #3557, also i might add "Avoid shading Google Guava". Would it make sense to create custom Maven build without Guava/Joda for applications that use Java API?
</comment><comment author="clintongormley" created="2014-10-17T10:45:44Z" id="59496408">@nfx We've talked about this and we don't understand exactly what you are doing here. Could you talk us through how you are setting things up?  
</comment><comment author="nfx" created="2014-10-21T22:42:45Z" id="60010911">Let's say:
i go and download http://search.maven.org/remotecontent?filepath=org/elasticsearch/elasticsearch/1.4.0.Beta1/elasticsearch-1.4.0.Beta1-sources.jar (usually done from IDE), after that i try to look at the sources - and IDE gives me compilation errors, because maven shade plugin (?) is not replacing com.google.collect with org.elasticsearch.common and IDE cannot find classes from Guava. Proof:
 $ grep "import com.google.collect" org/elasticsearch/common/collect/ImmutableOpenMap.java
import com.google.common.collect.UnmodifiableIterator;

so what i do after downloading ES sources: go to ~/.m2/repository/org/elasticsearch/elasticsearch/1.1.0/, manually unpack *-sources.jar and replace imports that are causing errors with org.elasticsearch.common. but that's a hack, i guess....
</comment><comment author="clintongormley" created="2014-10-24T08:43:37Z" id="60360029">This is very odd - Eclipse and IntelliJ show you the sources without trying to compile them.  Which IDE are you using?
</comment><comment author="nfx" created="2014-10-27T09:35:31Z" id="60567811">IntelliJ 13...
</comment><comment author="dadoonet" created="2014-10-31T09:31:38Z" id="61236837">@nfx I guess you are trying to compile your own plugin or your own application, right? 
Are you using maven? Could you paste here your `pom.xml`?

Could you also paste your `.iml` file?
</comment><comment author="nfx" created="2014-11-03T11:28:10Z" id="61465661">@dadoonet i guess my IDE tried to compile dependencies. The funny thing is that this didn't appear in 0.90.x

```
$ grep -r '&lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch' *
a.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch:1.1.0" level="project" /&gt;
b.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch-hadoop:1.3.0.M2" level="project" /&gt;
c.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch-hadoop:1.3.0.M2" level="project" /&gt;
d.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch:1.1.0" level="project" /&gt;
e.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch:1.1.0" level="project" /&gt;
f.iml:    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch:1.1.0" level="project" /&gt;
```
</comment><comment author="dadoonet" created="2014-11-26T14:59:47Z" id="64657411">@nfx Sorry to answer lately. So you are using IntelliJ 13 (may be 14 now)?
I think you are trying to attach elasticsearch source code to your own project instead of using only the JAR files.
I mean that in IntelliJ, when using Maven, you just need when importing to check "download source" button and you're done.

For example, here is what I can see when I import elasticsearch as a Maven dependency in analysis-phonetic plugin project.

``` xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4"&gt;
  &lt;component name="FacetManager"&gt;
    &lt;facet type="Python" name="Python"&gt;
      &lt;configuration sdkName="Python 3.3.3 (/usr/local/bin/python3.3)" /&gt;
    &lt;/facet&gt;
  &lt;/component&gt;
  &lt;component name="NewModuleRootManager" LANGUAGE_LEVEL="JDK_1_7" inherit-compiler-output="false"&gt;
    &lt;output url="file://$MODULE_DIR$/target/classes" /&gt;
    &lt;output-test url="file://$MODULE_DIR$/target/test-classes" /&gt;
    &lt;content url="file://$MODULE_DIR$"&gt;
      &lt;sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" /&gt;
      &lt;sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" /&gt;
      &lt;sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" /&gt;
      &lt;sourceFolder url="file://$MODULE_DIR$/src/test/resources" type="java-test-resource" /&gt;
      &lt;excludeFolder url="file://$MODULE_DIR$/target" /&gt;
    &lt;/content&gt;
    &lt;orderEntry type="inheritedJdk" /&gt;
    &lt;orderEntry type="sourceFolder" forTests="false" /&gt;
    &lt;orderEntry type="library" name="Python 3.3.3 (/usr/local/bin/python3.3) interpreter library" level="application" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: org.hamcrest:hamcrest-all:1.3" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: org.apache.lucene:lucene-test-framework:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: org.apache.lucene:lucene-codecs:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-core:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: com.carrotsearch.randomizedtesting:junit4-ant:2.1.6" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: junit:junit:4.10" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: org.apache.ant:ant:1.8.2" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: com.carrotsearch.randomizedtesting:randomizedtesting-runner:2.1.10" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.elasticsearch:elasticsearch:2.0.0-SNAPSHOT" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-backward-codecs:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-analyzers-common:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-queries:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-memory:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-highlighter:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-queryparser:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-sandbox:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-suggest:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-misc:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-join:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-grouping:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-spatial:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-expressions:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.antlr:antlr-runtime:3.5" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.ow2.asm:asm:4.1" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.ow2.asm:asm-commons:4.1" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: com.spatial4j:spatial4j:0.4.1" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: com.vividsolutions:jts:1.13" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.codehaus.groovy:groovy-all:indy:2.3.2" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: log4j:apache-log4j-extras:1.2.17" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.slf4j:slf4j-api:1.6.2" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: net.java.dev.jna:jna:4.1.0" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.fusesource:sigar:1.6.4" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: org.apache.lucene:lucene-analyzers-phonetic:5.0.0-snapshot-1641343" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: commons-codec:commons-codec:1.10" level="project" /&gt;
    &lt;orderEntry type="library" name="Maven: log4j:log4j:1.2.17" level="project" /&gt;
    &lt;orderEntry type="library" scope="TEST" name="Maven: org.elasticsearch:elasticsearch:test-jar:tests:2.0.0-SNAPSHOT" level="project" /&gt;
  &lt;/component&gt;
&lt;/module&gt;
```

You might have different settings?

May be you could paste your full `.iml` file?
</comment><comment author="dadoonet" created="2014-12-03T15:50:02Z" id="65431383">@nfx I'm closing this for now. If you have any new information on this, feel free to reopen it.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use BytesReference to write to translog files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5463</link><project id="" key="" /><description>Instead of using byte arrays, pass the BytesReference to the actual translog file, and use the new copyTo(channel) method to write. This will improve by not potentially having to convert the data to a byte array
</description><key id="29728825">5463</key><summary>Use BytesReference to write to translog files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-19T12:45:09Z</created><updated>2015-06-07T15:01:49Z</updated><resolved>2014-03-19T13:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-19T13:01:54Z" id="38047407">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add caching support to Geohash Cell Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5462</link><project id="" key="" /><description>at the moment the GeohashCellFilter is never cached. We should support to control this behavior using the `_cache` flag. We can also be smart with the default and turn on caching when precision is larger than, say 1000km.  
</description><key id="29727885">5462</key><summary>Add caching support to Geohash Cell Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-03-19T12:28:57Z</created><updated>2014-12-30T14:22:35Z</updated><resolved>2014-12-30T14:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:22:35Z" id="68359696">Closing in favour of PR #5538
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Starting a non-master, non-data instance first breaks cluster discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5461</link><project id="" key="" /><description>Hello,

I have ES 0.90.10, I couldn't test with the latest release, I honestly don't know if maybe it was fixed since.

On the same server, I have a master/data instance of ES and a second non-master. ( node.master: false )

When the master instance is started first, and the non-master afterwards, both are in the same configured cluster.

If I start the non-master first, it stops working.

I also tested with both the AWS EC2 plugin discovery and the unicast discovery.

That tells me, if the first node to start can't be master, even by starting a second instance that can, with the same cluster.name, the cluster can't be created.
</description><key id="29727835">5461</key><summary>Starting a non-master, non-data instance first breaks cluster discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost974</reporter><labels /><created>2014-03-19T12:28:16Z</created><updated>2014-03-19T14:15:32Z</updated><resolved>2014-03-19T14:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-19T12:34:53Z" id="38045196">Yea, thats what will happen. The reason is that today, we are only building the list to ping with the first port in the port range. So if the non master starts first, it will use HOST:9300, then when the master starts, it will try and join through HOST:9300 (and not try itself), where it can't be part of the cluster. You need to explicitly state in the unicast list both 9300 and 9301. This is a known issue, and it needs to be improved for better experience.
</comment><comment author="ghost974" created="2014-03-19T12:44:13Z" id="38045934">Thanks Kimchy, discovery.zen.ping.unicast.hosts: ["localhost[9300-9301]"] on the non-master worked.
</comment><comment author="ghost974" created="2014-03-19T13:05:35Z" id="38047731">Sorry, I forgot to re-enable node.master: true , and it's not working.

Edit: I hardcoded the port allocation to 9300 for the master, 9301 the non-master. With the unicast configuration it's working.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make cache-recycling operations thread-safe.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5460</link><project id="" key="" /><description>When using DFS-query-then-fetch, some facet data-structures are acquired and
released on a different thread, which is something that is not supported by
the (soft) thread-local cache recycler.

On 1.x, this limitation has been removed, and all recyclers actually support
acquisitions and relases on different threads. This commit makes sure it also
works on 0.90 by making the stack that is used to store pending objects
synchronized. The performance impact of these synchronized blocks is expected
to be very low as there would be very little contention. By the way calls to
acquire/release are synchronized on the default cache recycler on 1.x.

Close #4754
</description><key id="29723319">5460</key><summary>Make cache-recycling operations thread-safe.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-03-19T11:10:00Z</created><updated>2014-07-16T21:47:34Z</updated><resolved>2014-03-20T09:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-19T16:27:55Z" id="38072970">For the record, I also considered changing facets to make the allocation of recycled objects happen later (and in the same thread as they are released) but this was too big a change for me to feel comfortable pushing to 0.90 which is only supposed to get bug fixes now that 1.0 is out.
</comment><comment author="s1monw" created="2014-03-20T09:09:38Z" id="38146662">+1 I think safety is important here and the contention on this close to zero.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant_terms agg only creates term frequency cache when necessary </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5459</link><project id="" key="" /><description>and uses new TermsEnum wrapper to cache frequencies. Long and String-based aggs no longer need to pass an IndexReader as parameter when looking up frequencies of terms.

Closes #5450
</description><key id="29717553">5459</key><summary>Significant_terms agg only creates term frequency cache when necessary </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-19T09:35:04Z</created><updated>2015-06-07T15:04:41Z</updated><resolved>2014-03-19T15:50:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-19T10:01:00Z" id="38033714">I like the refactoring! I think something we should address is adding the ability to know whether an `Aggregator` creates buckets (like range/terms/...) or just filters them (like filter/nested/...). Maybe this could be done in a similar way to metrics aggregators? cc @uboness 
</comment><comment author="uboness" created="2014-03-19T11:29:10Z" id="38040579">A `SingleBucketAggregator` is typically a filtering aggregation, we can introduce a `MultiBucketAggregator` base class for all the agtors that actually create buckets (thought of doing it anyway)
</comment><comment author="jpountz" created="2014-03-19T14:38:55Z" id="38057685">+1 to push!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filtering aggregations </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5458</link><project id="" key="" /><description>When doing a nested aggregation against a specific doc type, and with a filter of a specific doc type, the results still return unfiltered results.  Here's an example:

```
POST _all/summary_phys/_search
{
  "aggs": {
    "summary_phys_events": {
      "filter": {
        "type": {"value": "summary_phys"}
      },
      "aggs": {
        "events_by_date": {
          "date_histogram": {
            "field": "@timestamp",
            "interval": "300s",
            "min_doc_count": 0
          },
          "aggs": {
            "events_by_host": {
              "terms": {
                "field": "host.raw",
                "min_doc_count": 0
              },
              "aggs": {
                "avg_used": {
                  "avg": {
                    "field": "used"
                  }
                },
                "max_used": {
                  "max": {
                    "field": "used"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

I get buckets with entries matching hosts that do not show up in this doc type.  For example, I have only 3 values for host in this doc type [compute-4, compute-2, compute-3], but I will get buckets back with hosts from other doc types like:

```
"events_by_host": {
                  "buckets": [
                     {
                        "key": "compute-4",
                        "doc_count": 11,
                        "max_used": {
                           "value": 4608
                        },
                        "avg_used": {
                           "value": 3677.090909090909
                        }
                     },
                     {
                        "key": "compute-2",
                        "doc_count": 8,
                        "max_used": {
                           "value": 4608
                        },
                        "avg_used": {
                           "value": 2304
                        }
                     },
                     {
                        "key": "compute-3",
                        "doc_count": 2,
                        "max_used": {
                           "value": 4608
                        },
                        "avg_used": {
                           "value": 4608
                        }
                     },
                     {
                        "key": "10.10.11.22:49509",
                        "doc_count": 0,
                        "max_used": {
                           "value": null
                        },
                        "avg_used": {
                           "value": null
                        }
                     },
                     {
                        "key": "controller",
                        "doc_count": 0,
                        "max_used": {
                           "value": null
                        },
                        "avg_used": {
                           "value": null
                        }
                     },
                     {
                        "key": "object-1",
                        "doc_count": 0,
                        "max_used": {
                           "value": null
                        },
                        "avg_used": {
                           "value": null
                        }
                     }
                  ]
            }
```

I believe that the extra hosts should be picked up by the aggregation filter if not by the URL path.
</description><key id="29694758">5458</key><summary>filtering aggregations </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jxstanford</reporter><labels /><created>2014-03-18T23:25:12Z</created><updated>2016-02-25T12:50:51Z</updated><resolved>2014-07-24T14:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T14:39:43Z" id="50026863">@jxstanford This is a known limitation of `min_doc_count=0`: it might return terms that don't match the query or filters. We just use the index in order to compute them.
</comment><comment author="bradvido" created="2015-02-09T19:48:46Z" id="73576696">Is this considered a bug? It's certainly unexpected behavior that the terms aggregation will match terms that don't match the query filter when you set min_doc_count=0.

At the very least, this should be (better) documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_minimum_document_count
</comment><comment author="FabriZZio" created="2015-02-19T19:19:31Z" id="75117096">Is there an alternative way to retrieve aggregations with a `doc count = 0` but using some query or filters?

For example: we have a use case with documents (products) in an ES index. Each product has a locale ("nl_be", "fr_fr", ...). When searching for all `locale: "nl_be"` documents, we would like to retrieve an aggregation on another field "category". 

```
#...
"query": {
    "filtered": {
      "filter": {
        "term": {
          "locale": "nl_be"
        }
      }
    }
},
"aggs": {
"nested_categories": {
  "nested": {
    "path": "categories"
  },
  "aggs": {
    "categories": {
      "terms": {
        "field": "categories.title",
        "min_doc_count": 0,
        "size": 0
      }
    }
  }
}
```

This results in `categories` containing all documents, even with `locale` other than 'nl_be'.

There probably is a very good reason why this `min_doc_count=0` behaves as is, but how can the above scenario be handled (returning category titles, with `doc count = 0` and linked to documents with `locale: "nl_be"`.

Thanks in advance!
</comment><comment author="snkiran" created="2016-02-25T10:16:26Z" id="188706342">this is my query below:
but i need to display all the missing or null values in the result or output

IP: 52.8.97.179:9200
GET article_info/article/_search
{
"fields": [
"media_type"
],
"query": {
"filtered": {
"query": {
"query_string": {
"query": "pdf_url:.pdf"
}
},
"filter": {
"range": {
"mediadate": {
"from": "02-21-2016",
"to": "02-24-2016"
}
}
}
}
},
"size": 0,
"aggs": {
"days": {
"date_histogram": {
"field": "mediadate",
"interval": "day",
"format": "MM-dd-yyyy"
},
"aggs": {
"art_count": {
"terms": {
"field": "media_type",
"order" : { "_term" : "desc" }
}
}
}
}
}
}
</comment><comment author="snkiran" created="2016-02-25T10:16:44Z" id="188706414">this is the result output: but i need to display all the missing or null values on date 02-23-2016 AND 02-24-2016,
please help me :

{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 13352,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "days": {
         "buckets": [
            {
               "key_as_string": "02-21-2016",
               "key": 1456012800000,
               "doc_count": 12496,
               "art_count": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "key": "wire",
                        "doc_count": 68
                     },
                     {
                        "key": "web",
                        "doc_count": 7371
                     },
                     {
                        "key": "social",
                        "doc_count": 36
                     },
                     {
                        "key": "service",
                        "doc_count": 68
                     },
                     {
                        "key": "print",
                        "doc_count": 101
                     },
                     {
                        "key": "media",
                        "doc_count": 36
                     },
                     {
                        "key": "blog",
                        "doc_count": 4920
                     }
                  ]
               }
            },
            {
               "key_as_string": "02-22-2016",
               "key": 1456099200000,
               "doc_count": 854,
               "art_count": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "key": "wire",
                        "doc_count": 92
                     },
                     {
                        "key": "web",
                        "doc_count": 223
                     },
                     {
                        "key": "service",
                        "doc_count": 92
                     },
                     {
                        "key": "blog",
                        "doc_count": 539
                     }
                  ]
               }
            },
            {
               "key_as_string": "02-23-2016",
               "key": 1456185600000,
               "doc_count": 1,
               "art_count": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "key": "web",
                        "doc_count": 1
                     }
                  ]
               }
            },
            {
               "key_as_string": "02-24-2016",
               "key": 1456272000000,
               "doc_count": 1,
               "art_count": {
                  "doc_count_error_upper_bound": 0,
                  "sum_other_doc_count": 0,
                  "buckets": [
                     {
                        "key": "web",
                        "doc_count": 1
                     }
                  ]
               }
            }
         ]
      }
   }
}
</comment><comment author="dadoonet" created="2016-02-25T11:44:44Z" id="188748303">@snkiran You will have a better chance to get an answer on discuss.elastic.co.
</comment><comment author="snkiran" created="2016-02-25T12:50:51Z" id="188775153">help me some one as soon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem faceting fields over 255 characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5457</link><project id="" key="" /><description>If I create an index with the following mapping (or use the default):

``` json
{"blog":{"mappings":{"post":{"properties":{"fuzz":{"type":"string","index":"not_analyzed"}}}}}}
```

and add the following two json documents:

``` json
{"fuzz": "jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf12345678901" }
{"fuzz": "jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf1234567890" }
```

then run the following query (generated by Kibana):

``` json
{"facets":{"terms":{"terms":{"field":"fuzz","size":10,"order":"count","exclude":[]},"facet_filter":{"fquery":{"query":{"filtered":{"query":{"bool":{"should":[{"query_string":{"query":"*"}}]}},"filter":{"bool":{"must":[{"match_all":{}}]}}}}}}}},"size":0}
```

I get the result:

``` json
{
  "took" : 8,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "facets" : {
    "terms" : {
      "_type" : "terms",
      "missing" : 1,
      "total" : 1,
      "other" : 0,
      "terms" : [ {
        "term" : "jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf1234567890",
        "count" : 1
      } ]
    }
  }
}
```

Which appears to facet the 255 character field fine, but counts the 256 character one as missing?
</description><key id="29688640">5457</key><summary>Problem faceting fields over 255 characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels /><created>2014-03-18T21:53:06Z</created><updated>2014-03-20T20:32:36Z</updated><resolved>2014-03-20T20:32:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-19T10:16:23Z" id="38034905">I tried to reproduce on Elasticsearch's master branch but it doesn't reproduce. What version of Elasticsearch are you using?

If you run a search on your index, can you see both field values? (to make sure the problem is not on the client)
</comment><comment author="jimmyjones2" created="2014-03-20T13:38:25Z" id="38167126">I've done some further testing:

``` bash
# Add two documents, _id:1 with a field of length 256, _id:2 with length 255
curl -XPUT 'http://localhost:9200/blog/post/1' -d'{"fuzz": "jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf12345678901" }'
curl -XPUT 'http://localhost:9200/blog/post/2' -d'{"fuzz": "jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf1234567890" }'

# Try faceting the long field, only returns _id:2
curl -XPOST 'http://localhost:9200/blog/_search?pretty=true' -d '{"facets":{"terms":{"terms":{"field":"fuzz","size":10,"order":"count","exclude":[]},"facet_filter":{"fquery":{"query":{"filtered":{"query":{"bool":{"should":[{"query_string":{"query":"*"}}]}},"filter":{"bool":{"must":[{"match_all":{}}]}}}}}}}},"size":0}'

# Try searching for both records, only returns _id:2
curl -XPOST 'http://localhost:9200/blog/_search?q=fuzz:jas*&amp;pretty=true'

# Return both documents, as expected
curl -XPOST 'http://localhost:9200/blog/_search?q=_id:1%20_id:2&amp;pretty=true'
```

Tested on 1.0.1, 1.0 branch, 1.x branch, and master. Same behaviour on all.
</comment><comment author="dakrone" created="2014-03-20T13:53:45Z" id="38168783">The default analyzer removes tokens over 255 characters long, are you sure that the `not_analyzed` setting has been correctly applied to your index? What does `curl 'localhost:9200/blog/_mapping'` return?

You can verify the dropping behavior with:

```
&#187; curl -XPOST 'localhost:9200/_analyze?pretty' -d'jashdygayusdgyuagsdyugayusdgyuagsyudgayusgdyusagduyguguygyuftftyftksdfkhsdfkushdfkuhsdfkuhsfdkuhskdufhksudhfkushdfkuhsdkfuhsdfksasdiuhsadiuhasdiuahsdiuahsdiuashdiuahsdiuhasiudhaiushdaiushdsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdfoijsdoifjsdoifjsdf12345678901'
{
  "tokens" : [ ]
}
```
</comment><comment author="jimmyjones2" created="2014-03-20T20:32:36Z" id="38217472">Don't know what I was doing wrong, but now when setting not_analyzed I get the result I expected. Thanks for both your help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Address findbugs warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5456</link><project id="" key="" /><description>Fixed some code constructed discovered by findbugs analysis

1)  no final in public static
2)  null dereferencing 
3)  NaN comparision
</description><key id="29683595">5456</key><summary>Address findbugs warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-03-18T20:54:02Z</created><updated>2014-07-16T21:47:35Z</updated><resolved>2014-04-07T09:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T09:16:43Z" id="39709906">This can be closed, the fixes have been applied meanwhile.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BytesReference usage to properly work when hasArray is not available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5455</link><project id="" key="" /><description>when a BytesReference doesn't have a backing array, properly handle the case in places where its applicable
</description><key id="29678505">5455</key><summary>BytesReference usage to properly work when hasArray is not available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-18T19:51:38Z</created><updated>2015-06-07T15:02:40Z</updated><resolved>2014-03-18T20:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-18T19:58:01Z" id="37980241">LGTM, just make sure to open an issue for the todo in the FsTranslog
</comment><comment author="kimchy" created="2014-03-18T20:04:33Z" id="37980964">@s1monw missed your notes, pushed, sorry!, will fix the additional notes
</comment><comment author="s1monw" created="2014-03-18T20:06:03Z" id="37981119">@kimchy thx
</comment><comment author="kimchy" created="2014-03-19T13:15:43Z" id="38048632">issue for translog added: #5463, and pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Illogical xor operation in RandomScoreFunction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5454</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java

In method random(int doc)

The intent of rand ^= rand isn't very clear, a possible bug.
</description><key id="29675166">5454</key><summary>Illogical xor operation in RandomScoreFunction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels><label>bug</label></labels><created>2014-03-18T19:08:55Z</created><updated>2014-04-11T12:03:03Z</updated><resolved>2014-04-11T12:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fixing context suggest documentation, missing comma after path in catego...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5453</link><project id="" key="" /><description>Fixing context suggest documentation.  Thanks to Sean Shubin for pointing out missing comma in sample.
</description><key id="29673963">5453</key><summary>Fixing context suggest documentation, missing comma after path in catego...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">davidsiefert</reporter><labels /><created>2014-03-18T18:53:55Z</created><updated>2014-06-14T05:33:49Z</updated><resolved>2014-05-13T08:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:19:37Z" id="42295311">Hi @davidsiefert 

Thanks for the PR. Sorry it has taken a while to get to it.  Please could I ask you to sign our CLA so that I can get your commits merged in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-05-13T08:36:03Z" id="42930183">No CLA received. Treating as bug fix.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix cardinality memory-usage considerations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5452</link><project id="" key="" /><description>Default precision was computed based on the number of MULTI_BUCKET parents
instead of PER_BUCKET.

The ordinals-based execution mode was almost always used although ordinals
might have non-negligible memory usage compared to the counters.
</description><key id="29646723">5452</key><summary>Fix cardinality memory-usage considerations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-18T13:57:28Z</created><updated>2015-06-07T22:32:49Z</updated><resolved>2014-03-20T09:02:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-03-19T10:11:22Z" id="38034532">Nice - tested on my problem dataset/query and the memory use is significantly reduced.
</comment><comment author="uboness" created="2014-03-19T12:58:48Z" id="38047133">LGTM (I can see the confusion with the MULTI_BUCKET vs PER_BUCKET... I'm open to other names that might be more descriptive... (though not that simple to find those names ;))
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to use Thai Analyser in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5451</link><project id="" key="" /><description>I will develop a Thai full-text search system.
But,I can't use thai analyser in Elasticsearch.
Please,someone write how to use it.
</description><key id="29635853">5451</key><summary>How to use Thai Analyser in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rikitphorn</reporter><labels /><created>2014-03-18T11:00:41Z</created><updated>2014-03-18T13:16:17Z</updated><resolved>2014-03-18T13:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-18T13:16:17Z" id="37930755">Please use the mailing list for your questions. You will get far more reader than on issues.

See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant terms aggregation should only employ caching when necessary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5450</link><project id="" key="" /><description>The significant_terms aggregation currently creates a cache of document counts per term. This is useful when there are multiple child-level significant_terms aggregations asking for the same background index frequency information for shared terms found in many different buckets.
However, for the simpler case when the significant_terms aggregation is used as a top-level aggregation examining the "single bucket" results of a query then the cache is not useful and so should not be constructed.

The logic for managing this cache is in SignificantTermsAggregatorFactory and a separate issue will be raised to address changing this cache into a new optimised utility class available for use elsewhere e.g. https://github.com/elasticsearch/elasticsearch/pull/5396
</description><key id="29634574">5450</key><summary>Significant terms aggregation should only employ caching when necessary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-18T10:39:22Z</created><updated>2014-03-19T15:58:13Z</updated><resolved>2014-03-19T15:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-03-19T15:58:13Z" id="38068478">Closed by https://github.com/elasticsearch/elasticsearch/commit/12d1bf848543b9e4c483ffa62161ac8b5be6c35d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo Point Fieldmapper: Allow distance for geohash precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5449</link><project id="" key="" /><description>Even though mentioned differently in the docs, the geohash precision needed to
be an integer instead of a DistanceUnit.

Closes #5448
</description><key id="29632770">5449</key><summary>Geo Point Fieldmapper: Allow distance for geohash precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Mapping</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-18T10:10:48Z</created><updated>2015-06-07T22:34:30Z</updated><resolved>2014-03-18T13:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-18T10:15:11Z" id="37916592">+1, this looks good!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"geohash_precision" in mapping throws error when using format like "1km"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5448</link><project id="" key="" /><description>The doc (http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/mapping-geo-point-type.html#_mapping_options) says you can specify the precision using strings like "1km" or "1m".

However, doing so returns an error, only specifying the precision using a number seems to be allowed.

E.g., create a new index like this:

```
curl -XPUT 'http://localhost:9200/test' -d '{
  "mappings": {
    "pin": {
      "properties": {
        "location": {
          "type": "geo_point",
          "geohash_precision": "1m"
        }
      }
    }
  }
}'
```

and you'll get an error like this:

```
{
  "error" : "MapperParsingException[mapping [pin]]; nested: NumberFormatException[For input string: \"1m\"]; ",
  "status" : 400
}
```

I'm using 1.0.0.
</description><key id="29629930">5448</key><summary>"geohash_precision" in mapping throws error when using format like "1km"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tstibbs</reporter><labels /><created>2014-03-18T09:23:01Z</created><updated>2014-03-18T13:31:21Z</updated><resolved>2014-03-18T13:31:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-18T09:55:20Z" id="37915095">just checked the source, we only parse integers there at the moment... should be relatively simple to fix from my birds eye view.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move to use serial merge schedule by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5447</link><project id="" key="" /><description>Today, we use ConcurrentMergeScheduler, and this can be painful since it is concurrent on a shard level, with a max of 3 threads doing concurrent merges. If there are several shards being indexed, then there will be a minor explosion of threads trying to do merges, all being throttled by our merge throttling.
Moving to serial merge scheduler will still maintain concurrency of merges across shards, as we have the merge thread pool that schedules those merges. It will just be a serial one on a specific shard.
Also, on serial merge scheduler, we now have a limit of how many merges it will do at one go, so it will let other shards get their fair chance of merging. We use the pending merges on IW to check if merges are needed or not for it.
Note, that if a merge is happening, it will not block due to a sync on the maybeMerge call at indexing (flush) time, since we wrap our merge scheduler with the EnabledMergeScheduler, where maybeMerge is not activated during indexing, only with explicit calls to IW#maybeMerge (see Merges).
</description><key id="29580568">5447</key><summary>Move to use serial merge schedule by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-17T17:23:44Z</created><updated>2015-06-07T15:05:05Z</updated><resolved>2014-03-18T12:17:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-17T20:18:28Z" id="37864844">I like the change the only thing that we need to figure out is the nameing. I also think we need a unittest for this. 
</comment><comment author="kimchy" created="2014-03-17T22:36:51Z" id="37879798">renamed + added unit test.
</comment><comment author="s1monw" created="2014-03-18T10:39:20Z" id="37918503">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the option to specify with what ES version a node should be started with</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5446</link><project id="" key="" /><description>Useful in TestCluster
</description><key id="29575814">5446</key><summary>Added the option to specify with what ES version a node should be started with</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-03-17T16:31:33Z</created><updated>2015-05-18T23:32:33Z</updated><resolved>2014-03-18T14:56:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-18T09:59:10Z" id="37915364">+1, this change is simple yet super useful for testing!
</comment><comment author="jpountz" created="2014-03-18T14:45:37Z" id="37940737">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Override clone() in p/c queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5445</link><project id="" key="" /><description /><key id="29537490">5445</key><summary>Override clone() in p/c queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-03-17T04:57:37Z</created><updated>2015-05-18T23:32:33Z</updated><resolved>2014-03-18T11:06:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-17T10:18:01Z" id="37800686">@martijnvg can you put an explicit test for this in and maybe put the reasoning in the commit message?
</comment><comment author="martijnvg" created="2014-03-18T10:26:33Z" id="37917502">@s1monw I have done this and updated the PR.
</comment><comment author="s1monw" created="2014-03-18T10:28:28Z" id="37917659">LGTM
</comment><comment author="martijnvg" created="2014-03-18T11:06:54Z" id="37920689">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added `extended_bounds` support for date_/histogram aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5444</link><project id="" key="" /><description>By default the date_/histogram returns all the buckets within the range of the data itself, that is, the documents with the smallest values (on which with histogram) will determine the min bucket (the bucket with the smallest key) and the documents with the highest values will determine the max bucket (the bucket with the highest key). Often, when when requesting empty buckets (min_doc_count : 0), this causes a confusion, specifically, when the data is also filtered.

To understand why, let's look at an example:

Lets say the you're filtering your request to get all docs from the last month, and in the date_histogram aggs you'd like to slice the data per day. You also specify min_doc_count:0 so that you'd still get empty buckets for those days to which no document belongs. By default, if the first document that fall in this last month also happen to fall on the first day of the **second week** of the month, the date_histogram will **not** return empty buckets for all those days prior to that second week. The reason for that is that by default the histogram aggregations only start building buckets when they encounter documents (hence, missing on all the days of the first week in our example).

With extended_bounds, you now can "force" the histogram aggregations to start building buckets on a specific min values and also keep on building buckets up to a max value (even if there are no documents anymore). Using extended_bounds only makes sense when min_doc_count is 0 (the empty buckets will never be returned if the min_doc_count is greater than 0).

Note that (as the name suggest) extended_bounds is **not** filtering buckets. Meaning, if the min bounds is higher than the values extracted from the documents, the documents will still dictate what the min bucket will be (and the same goes to the extended_bounds.max and the max bucket). For filtering buckets, one should nest the histogram agg under a range filter agg with the appropriate min/max.

Closes #5224
</description><key id="29534134">5444</key><summary>Added `extended_bounds` support for date_/histogram aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.1.0</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-17T02:43:31Z</created><updated>2015-06-07T10:37:19Z</updated><resolved>2014-03-20T14:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-18T08:16:06Z" id="37908223">I left minor comment but this looks good in general.
</comment><comment author="jpountz" created="2014-03-20T14:22:57Z" id="38172246">+1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest REST endpoint fix for POST/PUT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5443</link><project id="" key="" /><description>Fix an odd behavior whereby a user can create documents with id=_suggest
by issuing POST/PUT requests to
http://localhost:9200/{index}/{type}/_suggest.

This fix intercepts such requests and returns a 400 BAD_REQUEST
response.

Additionally, this commit includes an extension to the standard
integration test facilities which allows a subclass to use a RestClient
instance to issue REST requests.

Closes #5442
</description><key id="29529251">5443</key><summary>Suggest REST endpoint fix for POST/PUT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-03-16T23:06:19Z</created><updated>2014-10-21T23:42:15Z</updated><resolved>2014-07-11T09:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-17T10:05:32Z" id="37799789">I'm on the fence on this change... isn't the result the same as preventing a document to be indexed if it has `_suggest` as id? Couldn't we handle this in the index api then instead of adding support for `PUT` here to be able to throw an error?

Also, I'd much rather prefer to keep testing the REST layer using yaml tests, which are shared between all the clients as well. Handling this in the index api would make it easier to test it as well.

Maybe we'd need to consider validating the document ids and prevent users from using any of our own registered `_endpoints` just to avoid confusion?
</comment><comment author="s1monw" created="2014-03-17T10:22:08Z" id="37801031">+1 to validiate doc IDs based on the endpoints we have so it's actually dynamic!
</comment><comment author="clintongormley" created="2014-07-11T09:49:03Z" id="48712681">Just because an ID doesn't clash with an existing endpoint doesn't mean that it won't clash in the future.  I think we should say that IDs can't begin with an underscore.  

Closing in favour of #6736 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API _suggest endpoint mistakenly creates documents with id = _suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5442</link><project id="" key="" /><description>Using the REST API, it is possible to create documents with id  = _suggest if one tries to add a type parameter to the request. While technically correct, this behavior is confusing and should be altered to return a 400 BAD_REQUEST.
</description><key id="29528590">5442</key><summary>REST API _suggest endpoint mistakenly creates documents with id = _suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-03-16T22:38:02Z</created><updated>2014-12-30T14:21:40Z</updated><resolved>2014-12-30T14:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:21:40Z" id="68359625">Closing in favour of #9059
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MulticastChannel returned wrong channel in shared mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5441</link><project id="" key="" /><description>If the shared channel is used a wrong refrence was returned and
close calls couldn't find the listener since it go never registered
in that instance.
</description><key id="29521131">5441</key><summary>MulticastChannel returned wrong channel in shared mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-16T17:20:23Z</created><updated>2015-06-07T15:05:17Z</updated><resolved>2014-03-16T18:50:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-16T18:01:08Z" id="37764642">nice catch!, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count latch down if sendsPing throws exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5440</link><project id="" key="" /><description>if the async sendPingsHandler throws an unexpected exception the
corresponding latch is never counted down. This might only happen
during node shutdown but can still cause starvation or test failures.
</description><key id="29511687">5440</key><summary>Count latch down if sendsPing throws exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-16T06:40:34Z</created><updated>2015-06-07T22:34:46Z</updated><resolved>2014-03-16T15:14:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-16T09:03:11Z" id="37752417">LGTM
</comment><comment author="s1monw" created="2014-03-24T11:48:38Z" id="38435119">this is also on [1.x &amp; 1.1](1cdc6580aa1460f7348f67b76bb5c361fc1953c9),  [1.0.2](bc8ea7c40418c00cecaf1bffd86a1ab950c1595e), [0.90.13](07ec294313c93cbb35fd510fdf8387f8370bc05e)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Findbug warning supression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5439</link><project id="" key="" /><description>Added logic to enable findbug warnings suppression via annotations
Tagging on annotation requires findbug dependency during compilation stage.
</description><key id="29474291">5439</key><summary>Findbug warning supression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-03-14T21:56:58Z</created><updated>2014-07-16T21:47:39Z</updated><resolved>2014-03-17T22:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-03-17T20:40:16Z" id="37867130">Closed by https://github.com/elasticsearch/elasticsearch/commit/744eabad030f8f8c5705beeea7ab7eeb47f84969
</comment><comment author="mrsolo" created="2014-03-17T21:09:46Z" id="37870559">Current implementation introduces unwanted dependency.  Back to the drawing board. 
</comment><comment author="mrsolo" created="2014-03-17T22:21:25Z" id="37878328">No viable solution found without using customized annotation.  Findbug warning suppression will use external annotation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add slop to prefix phrase query after parsing query string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5438</link><project id="" key="" /><description>This fixes a regression introduced by #5005 where the query slop
was simply ignored when a `match_phrase_prefix` type was set.

Closes #5437
</description><key id="29461574">5438</key><summary>Add slop to prefix phrase query after parsing query string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-14T18:59:46Z</created><updated>2015-06-07T22:35:11Z</updated><resolved>2014-03-14T19:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-14T19:13:42Z" id="37684836">+1 the change looks good to me. It's unfortunate that we didn't have a test to catch it!
</comment><comment author="s1monw" created="2014-03-14T19:21:33Z" id="37685585">well the good news it it hasn't been released yet the regression was introduced in a `1.1` change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>match_phrase_prefix no longer supports slop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5437</link><project id="" key="" /><description>```
PUT /my_index/my_type/1
{
  "brand": "Johnnie Walker Black Label"
}

GET /my_index/_search
{
  "query": {
    "match": {
      "brand": {
        "type": "phrase_prefix", 
        "query": "Johnnie la",
        "slop": 10
      }
    }
  }
}
```
</description><key id="29454369">5437</key><summary>match_phrase_prefix no longer supports slop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-14T17:25:12Z</created><updated>2014-03-14T20:00:57Z</updated><resolved>2014-03-14T19:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-14T18:04:56Z" id="37678025">@s1monw I ran a git bisect on this and it was #5005 that broke this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for the Lucene FreeText suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5436</link><project id="" key="" /><description>Initial version for review of the addition of the free text suggester to the elasticsearch suggest mechanism
</description><key id="29449159">5436</key><summary>Support for the Lucene FreeText suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Suggesters</label></labels><created>2014-03-14T16:30:47Z</created><updated>2014-12-11T16:30:04Z</updated><resolved>2014-12-11T16:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-31T13:19:22Z" id="39086840">I left a bunch of comments, can you sync with master since we have now suggest stats etc. I think it looks close
</comment><comment author="spinscale" created="2014-03-31T16:32:51Z" id="39109878">rebased against master, incorporated your commit comments regarding serialization... two open issues are left IMO

First, should I add support to return the whole phrase? Havent decided yet if this is, what the user wants to have :-) - need to think about it

Second, I'd really appreciate you to take one closer look at this TODO: https://github.com/spinscale/elasticsearch/commit/97c6c7826e9c3f58ce1aaba623f45d08a6e24c92#diff-8291c198474ec70e66b52e63a8c2b1c1R385 in comparison to https://github.com/spinscale/elasticsearch/commit/97c6c7826e9c3f58ce1aaba623f45d08a6e24c92#diff-22e724cf870fe2793b6239f27a7de8daR367 where that sum operation is only applied for an `ngramCount == 1`, I didnt do this. I need to dig deeper into the code again why I did it that way, already two weeks ago...
</comment><comment author="clintongormley" created="2014-07-11T10:21:20Z" id="48715298">@spinscale any news on this, or do you want to reassign it to @areek ?
</comment><comment author="spinscale" created="2014-07-11T10:25:21Z" id="48715606">@areek already asked about this, so, changed ownership here
</comment><comment author="spinscale" created="2014-12-11T16:30:04Z" id="66645548">closing for now, will be superseded by @areek 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>equals()/hashCode() &amp; tests for ByteArrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5435</link><project id="" key="" /><description>equals/hashCode helper implementations to avoid materializing a full byte[].

Fixes #5420
</description><key id="29440624">5435</key><summary>equals()/hashCode() &amp; tests for ByteArrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels /><created>2014-03-14T15:05:55Z</created><updated>2014-06-26T23:31:39Z</updated><resolved>2014-03-14T15:32:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-14T15:08:27Z" id="37657179">After discussion with @jpountz we agreed that copy() requires more complicated internal changes to BigArrays, so for now we reduce the scope of issue #5420 to simple but memory-conserving hashCode()/equals(). Once the other internal changes are in place they might become slightly more optimized.
</comment><comment author="jpountz" created="2014-03-14T15:16:48Z" id="37658207">+1 to these helpers, we can still make it more efficient in the future if needed

I left a comment about some `int`s that should be `longs`s but other than that, +1 to push.
</comment><comment author="hhoffstaette" created="2014-03-14T15:21:06Z" id="37658700">Fixed loops from int to long. Good catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Export JAVA_HOME in RPM init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5434</link><project id="" key="" /><description>Added missing line to init script. 
</description><key id="29440026">5434</key><summary>Export JAVA_HOME in RPM init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">alex-leonhardt</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-14T14:59:25Z</created><updated>2015-06-08T00:11:24Z</updated><resolved>2014-11-01T07:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-10-22T16:34:27Z" id="60114284">hey Alex, can you sign our CLA at http://www.elasticsearch.org/contributor-agreement/ so I can get this one in?
</comment><comment author="alex-leonhardt" created="2014-10-30T16:37:48Z" id="61124159">@spinscale  done :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES RedHat/Centos Init script not exporting JAVA_HOME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5433</link><project id="" key="" /><description>Hi,

export JAVA_HOME is missing from the init script in ES RPM 1.0.1-1 noarch.

when trying to make ES use a specific version of Java it fails until one "fixes" the init script to include : 

[...]

[ -e /etc/sysconfig/$prog ] &amp;&amp; . /etc/sysconfig/$prog

export ES_HEAP_SIZE
export ES_HEAP_NEWSIZE
export ES_DIRECT_SIZE
export ES_JAVA_OPTS
export JAVA_HOME

lockfile=/var/lock/subsys/$prog

[...]

Alex
</description><key id="29439451">5433</key><summary>ES RedHat/Centos Init script not exporting JAVA_HOME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alex-leonhardt</reporter><labels><label>:Packaging</label></labels><created>2014-03-14T14:53:15Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-11-01T07:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alappe" created="2014-09-25T14:09:02Z" id="56823527">This really would be an easy fix to get a working default setup on centos&#8230; :+1: 
</comment><comment author="electrical" created="2014-09-29T18:37:13Z" id="57207320">For what specific use case would you need to export it?
If you would set JAVA_HOME in the `/etc/sysconfig/$prog` file it should be fine?
Or is it being used somewhere else as well?
</comment><comment author="alappe" created="2014-09-30T08:23:21Z" id="57281600">Searching for a solution why my installation didn't work, I came across @alex-leonhardt fix &#8211; and it worked. After posting the comment, I came to realize I didn't really understand why this makes any difference, since the variable should already be exported&#8230; but nevertheless it works here.

Maybe @alex-leonhardt has more insight&#8230; if not, I'd need to build up a clean system to test with/without, which I cannot find the time this week&#8230;
</comment><comment author="alex-leonhardt" created="2014-10-04T21:37:30Z" id="57919291">Wow, this is so long ago I can't remember exactly; there was a good reason for having to export it.
</comment><comment author="clintongormley" created="2014-10-15T11:09:29Z" id="59190149">Could it be that, when the script drops privileges, that it also resets the environment?
</comment><comment author="clintongormley" created="2014-10-15T11:10:28Z" id="59190246">Would making this change break anything? If not, I'd suggest we do it.
</comment><comment author="electrical" created="2014-10-15T11:50:12Z" id="59193868">I don't expect it to break anything if we would export it.
</comment><comment author="clintongormley" created="2014-10-16T17:10:24Z" id="59396118">@electrical OK thanks. Does that mean you're going to do it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close Directory / Store once all resources have been released</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5432</link><project id="" key="" /><description>Currently we close the store and therefor the underlying directory
when the engine / shard is closed ie. during relocation etc. We also
just close it while there are still searches going on and/or we are
recovering from it. The recoveries might fail which is ok but searches
etc. will be working like pending fetch phases.

The contract of the Directory doesn't prevent to read from a stream
that was already opened before the Directory was closed but from a
system boundary perspective and from lifecycles that we test it seems
to be the right thing to do to wait until all resources are released.

Additionally it will also help to make sure everything is closed
properly before directories are closed itself.

Note: this commit adds Object#wait &amp; Object@#notify/All to forbidden APIs
</description><key id="29435211">5432</key><summary>Close Directory / Store once all resources have been released</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-03-14T13:59:44Z</created><updated>2014-07-16T21:47:40Z</updated><resolved>2014-03-21T14:12:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-14T21:09:53Z" id="37696585">added one commit for the first review round...
</comment><comment author="kimchy" created="2014-03-17T13:59:36Z" id="37817810">LGTM, I think we should incRef in RecoveryTarget#FileChunkTransportRequestHandler#messageReceived as well
</comment><comment author="s1monw" created="2014-03-17T16:36:07Z" id="37837773">added a new commit - I will wait a bit though but I think it's ready
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce query instance checking before it wrapper as a filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5431</link><project id="" key="" /><description>We have the default QueryWrapperFilter as well as our custom one while
our wrapper is explicitly marked as no_cache such that it will never
be included in a cache. This was not consistenly used and caused several
problems during tests where p/c related queries were used as filters
and ended up in the cache. This commit adds the QueryWrapperFilter
ctor to the forbidden APIs to enforce the query instance checks.

Unfortunately this doesn't fix cases where such a query is nested in a BQ or so. Yet I want to fix it step by step but this one at least fixes some cases and prevents more misuse.
</description><key id="29433844">5431</key><summary>Enforce query instance checking before it wrapper as a filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-14T13:40:05Z</created><updated>2015-06-07T22:37:14Z</updated><resolved>2014-03-14T19:25:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-14T16:18:01Z" id="37666235">+1 This is a more structured approach than the if/else in the place where QueryFilterWrapper was being used.
</comment><comment author="s1monw" created="2014-03-14T18:48:40Z" id="37682431">@jpountz @martijnvg I added two new commits see `Explain why Factory is used` and `Improve resource handling in Parent/ChildQuery`
</comment><comment author="jpountz" created="2014-03-14T18:52:18Z" id="37682765">+1 to push
</comment><comment author="s1monw" created="2014-03-14T19:25:27Z" id="37685936">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OutOfMemoryError Java heap space while setting a high 'size' value for a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5430</link><project id="" key="" /><description>Hi,

I'm face a interesting problem while executing alot of a simple query via head plugin in a little time (about ~ 20 clicks successive)

```
{
    "size" : 90000000,
    "query" : {
        "term" : { "username" : "kimchy" }
    },
    "fields" : []
}
```

The result is the following (while Elasticsearch take time to anwser)

```
{
    error: ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: OutOfMemoryError[Java heap space];
    status: 503
}
```

But I have only 11 docs in my index.

Log debug trace

```
Failed to execute [org.elasticsearch.action.search.SearchRequest@3a44ebcb] while moving to second phase
java.lang.OutOfMemoryError: Java heap space
at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:64)
        at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:37)
        at org.elasticsearch.search.controller.ScoreDocQueue.&lt;init&gt;(ScoreDocQueue.java:31)
        at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:257)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:85)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:417)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:241)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:219)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:216)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```

a priori, in Apache Lucene PriorityQueue.class, this following line causing problem

```
heap = (T[]) new Object[heapSize];
```

Questions :
1. Could you look up for fixing this issue? heapSize should be equal 
   **heapSize = Math.min(nDocsTotalInIndex, size);** like it's did in IndexSearch class of Lucene (line 445)
2. Do you know why PriorityQueue pre-allocate a full array of length (will causing HeapSize or GC problem if we do a lot of query with high 'size' value)? Why don't simply use List &lt; Object &gt; ? Please clarify me, just for curiosity :)

Thanks

Setup:
- Elasticsearch 1.0.1 release
- java version "1.7.0_25"
  Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
  Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

PS: maybe the same problem with this one [4177](https://github.com/elasticsearch/elasticsearch/issues/4177)
</description><key id="29433592">5430</key><summary>OutOfMemoryError Java heap space while setting a high 'size' value for a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">khong07</reporter><labels /><created>2014-03-14T13:36:30Z</created><updated>2014-06-13T20:59:59Z</updated><resolved>2014-03-14T13:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-14T13:42:17Z" id="37647635">Don't forget that your index has 5 primary shards by default, so:
1. Each shard needs to assign a priority queue
2. The coordinating node doesn't know how many docs it is going to receive from each shard and so has to assign a priority queue of the size that you specify.

This is not the right way to retrieve lots of results.  Instead, use a scrolled search with `search_type=scan` and keep pulling results until you have enough. Scanning disables sorting, which makes this very efficient.

At the moment scrolling large numbers of sorted results will not work. You'll run into the same problem as you are experiencing here.  However when #4940 is merged, we will be able to support deep scrolling of sorted results.

Closing this as a duplicate of #4177 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodeDoesNotExistOnMasterException invalidly thrown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5429</link><project id="" key="" /><description>I have 2 nodes, first node is Buzz Search 5 , and 2nd node is called Buzz Search 6. Currently Buzz Search 5 is running for a day. It has minimum_master_nodes set to 1. It does NOT have the host name for Buzz Search 6 in the list of unicast hosts.

I want to join Buzz Search 6 to it. It does have the host name of BuzzSearch 6 in list of unicast hosts in config file. It has minimum_master_nodes set to 2. When I attempt to start it, I get this error:

[2014-03-13 21:22:49,334][INFO ][discovery.zen            ] [Buzz Search 6] master_left [[BuzzSumo Search 5][utWqLm8DRtCGXOI9vKy8vw][sdfsdfsd.net][inet[/122.19.11.152:9300]]], reason [do not exists on master, act as master failure]
[2014-03-13 21:22:49,335][DEBUG][cluster.service          ] [Buzz Search 6] processing [zen-disco-master_failed ([Buzz Search 5][utWqLm8DRtCGXOI9vKy8vw][sdfsdfsd.net][inet[/122.19.11.152:9300]])]: execute
[2014-03-13 21:22:49,335][DEBUG][cluster.service          ] [Buzz Search 6] processing [zen-disco-master_failed ([Buzz Search 5][utWqLm8DRtCGXOI9vKy8vw][sdfsdfsd.net][inet[/122.19.11.152:9300]])]: no change in cluster_state
[2014-03-13 21:22:58,135][DEBUG][transport.netty          ] [Buzz Search 6] connected to node [[Buzz Search 6][UVX-vS3WQrKPaZoc4hv7qA][sdfsdfsd.net][inet[/122.19.11.152:9300]]]

Why is that? It seems like I can validly join the node to the existing one. Is it because minimum_master_nodes is 1 in Buzz Search 5? Is it because Buzz Search 6's host name is not in list of unicast hosts in BuzzSearch 5's config file?
</description><key id="29404189">5429</key><summary>NodeDoesNotExistOnMasterException invalidly thrown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HenleyChiu</reporter><labels /><created>2014-03-14T01:29:46Z</created><updated>2014-12-30T14:20:11Z</updated><resolved>2014-12-30T14:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:20:11Z" id="68359502">Hi @HenleyChiu 

Sorry it has taken a while to look at this issue.  I've just tried recreating the described scenario on version 1.4.2, and it all seems to function just fine, eg:

```
./bin/elasticsearch --node.name=one --discovery.zen.minimum_master_nodes=1 --discovery.zen.ping.multicast.enabled=false --discovery.zen.ping.unicast.hosts=localhost:9300
./bin/elasticsearch --node.name=two --discovery.zen.minimum_master_nodes=2 --discovery.zen.ping.multicast.enabled=false --discovery.zen.ping.unicast.hosts=localhost:9300,localhost:9301
```

Logs from node "two":

```
[2014-12-30 15:17:55,070][INFO ][node                     ] [two] version[1.4.2], pid[78497], build[927caff/2014-12-16T14:11:12Z]
[2014-12-30 15:17:55,071][INFO ][node                     ] [two] initializing ...
[2014-12-30 15:17:55,075][INFO ][plugins                  ] [two] loaded [], sites []
[2014-12-30 15:17:57,457][INFO ][node                     ] [two] initialized
[2014-12-30 15:17:57,457][INFO ][node                     ] [two] starting ...
[2014-12-30 15:17:57,559][INFO ][transport                ] [two] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/192.168.2.183:9301]}
[2014-12-30 15:17:57,567][INFO ][discovery                ] [two] elasticsearch/ay6s9OcXSjGrC0zF-CL-AQ
[2014-12-30 15:18:00,627][INFO ][cluster.service          ] [two] detected_master [one][NK77umkBQUK2AAIgLIOr1A][Slim.local][inet[/192.168.2.183:9300]], added {[one][NK77umkBQUK2AAIgLIOr1A][Slim.local][inet[/192.168.2.183:9300]],}, reason: zen-disco-receive(from master [[one][NK77umkBQUK2AAIgLIOr1A][Slim.local][inet[/192.168.2.183:9300]]])
[2014-12-30 15:18:00,672][INFO ][http                     ] [two] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/192.168.2.183:9201]}
[2014-12-30 15:18:00,672][INFO ][node                     ] [two] started
```

I'm assuming this was either a configuration issue or has subsequently been fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce java version 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5428</link><project id="" key="" /><description>When building elasticsearch, we now require to use java 1.7 ( #5421 ).

Maven will check that before compiling any class. If Java version is incorrect, you will get the following message:

```
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:
Detected JDK Version: 1.6.0-65 is not in the allowed range [1.7,).
```
</description><key id="29394949">5428</key><summary>Enforce java version 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T22:20:33Z</created><updated>2014-03-17T07:45:02Z</updated><resolved>2014-03-17T07:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>New class PagedBytesReference: BytesReference over pages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5427</link><project id="" key="" /><description>BytesRefererence over a BigArray/ByteArray list of pages. Passes both a boatload of new tests and the full test suite. About as compatible as it can be for now; minor perf improvements require extensions to BigArrays (see #5420)
</description><key id="29385064">5427</key><summary>New class PagedBytesReference: BytesReference over pages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T20:10:49Z</created><updated>2015-06-08T15:21:38Z</updated><resolved>2014-03-25T14:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-17T13:10:38Z" id="37812967">Squashed version with non-materializing equals()/hashCode(), support for zero-copy writeTo(OutputStream), composite ChannelBuffers and randomized tests with lots of corner-case fixes esp. for slices. The Netty buffers are non-gathering for now until I've read up on how Netty implements scatter/gather and the implications.
</comment><comment author="kimchy" created="2014-03-17T18:35:26Z" id="37853196">Here is a quick breakdown of gathering in Netty. Gathering tells netty if to send the list of ByteBuffer[] to the nio layer (when true), or write it one by one to the nio layer (when false). This is only enabled on Java 7 because of bugs in sending the array of buffers in 1.6. (for Netty logic, see `SocketSendBufferPool#acquire`, 

The part that is tricky is the fact that in the NIO layer, at least in 1.7, ends up taking those ByteBuffer arrays, copying over the direct buffers, and then trying to write as much as possible. If this message is very large, that it can't be sent at a single go, then extra copying will happen over and over again. Imagine sending a 1MB, 10 of 100k buffers, and only 1 buffer was sent at one go, then it will copy back the rest of the 9 buffers to try and send them again. (see IOUtil.java in the jdk source code).

This is not the end of the world, and this is what happens today without the paged data structure, so its already an improvement (assuming we set gathering to `true`, which should be the default if we don't add any other logic). But, we can potentially do better, and only try and send in gathering mode if the total length of the bytes is less than, lets say 512k (or something we expect, in the most common case, to succeed). If its more, it might be worth it to send the buffers one by one.
</comment><comment author="jpountz" created="2014-03-24T18:56:19Z" id="38485822">It looks good to me now. Maybe one last thing that should be fixed would be to make the equals method accept any BytesReference instance by falling back to `BytesReference.Helper.bytesEquals` (that should soon not materialize arrays anymore when https://github.com/elasticsearch/elasticsearch/pull/5517 is pushed) when the other instance is not an instance of `PagedBytesReference`?
</comment><comment author="hhoffstaette" created="2014-03-25T11:27:09Z" id="38553423">@jpountz commit 7b1c94a has the last requested change using the return value from ByteArray.get(), as discussed. (not sure why old commits are suddenly showing up here?!)
</comment><comment author="jpountz" created="2014-03-25T13:12:05Z" id="38561922">&gt; not sure why old commits are suddenly showing up here?

This might be because you rebased against master?

&gt; commit 7b1c94a has the last requested change using the return value from ByteArray.get()

Lookgs good, you have my +1. Maybe @kimchy and/or @s1monw would like to do another round before it gets merged?
</comment><comment author="s1monw" created="2014-03-25T13:33:25Z" id="38564035">I only did a quick glance but it looks good - go move forward
</comment><comment author="hhoffstaette" created="2014-03-25T14:24:40Z" id="38569875">Finally in master &amp; 1.x .. \o/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cardinality aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5426</link><project id="" key="" /><description>The cardinality aggregation is a metric aggregation that allows to compute approximate unique counts based on the [HyperLogLog++](http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40671.pdf) algorithm which has the nice properties of both being close to accurate on low cardinalities and having fixed memory usage so that estimating high cardinalities doesn't blow up memory.

Example:

``` json
{
    "aggs" : {
        "author_count" : { 
            "cardinality" : { 
                "field" : "author"
            }
        }
    }
}
```
</description><key id="29376061">5426</key><summary>Cardinality aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T18:19:46Z</created><updated>2015-06-06T18:36:41Z</updated><resolved>2014-03-13T18:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Default cluster membership behavior problems on Mac OS X 10.9.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5425</link><project id="" key="" /><description>1. Download and install es1.0.1 (zip install) on Mac 10.9.1
2. bin/elasticsearch -Des.node.name=node-1 =&gt; starts as a new_master named node-1.
3. bin/elasticsearch -Des.node.name=node-2 =&gt; also starts another new_master named node-2. 

Per http://www.elasticsearch.org/webinars/getting-started-with-elasticsearch/?watch=1, we should have seen node-2 'join the cluster' formed by node-1, node-2. On irc, @drewr  confirmed that they have seen some 'discovery issues on the Mac lately'. Filing this issue to keep it in mind.
</description><key id="29371692">5425</key><summary>Default cluster membership behavior problems on Mac OS X 10.9.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kedarmhaswade</reporter><labels /><created>2014-03-13T17:33:34Z</created><updated>2014-12-30T14:13:12Z</updated><resolved>2014-12-30T14:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-14T02:40:59Z" id="37610718">@kedarmhaswade I just tried to reproduce it on Mac running 10.9.2 and everything seems to work fine. Could you uncomment `discovery: TRACE` line in `config/logging.yml`, start both nodes and post here log files from both of them?
</comment><comment author="clintongormley" created="2014-12-30T14:13:12Z" id="68358983">No more info since March. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange exception when starting ES embedded for integration testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5424</link><project id="" key="" /><description>I updated the dependency in my project to 1.0.1. Suddenly my integration tests (that have been passing for months) are broken at startup. In fact I get two types of exceptions:

```
java.lang.NoClassDefFoundError: org/apache/lucene/util/Version
    at org.elasticsearch.Version.&lt;clinit&gt;(Version.java:42)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
    at com.mydomain.cms.estests.esconfig.SetupEsTestHelper.init(SetupEsTestHelper.java:148)
    at com.mydomain.cms.estests.esconfig.BaseEsTest.beforeClass(BaseEsTest.java:37)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.junit.runners.Suite.runChild(Suite.java:127)
    at org.junit.runners.Suite.runChild(Suite.java:26)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:202)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
```

and

```
java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
    at com.mydomain.cms.estests.esconfig.SetupEsTestHelper.init(SetupEsTestHelper.java:148)
    at com.mydomain.cms.estests.esconfig.BaseEsTest.beforeClass(BaseEsTest.java:37)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:174)
    at org.junit.runners.Suite.runChild(Suite.java:127)
    at org.junit.runners.Suite.runChild(Suite.java:26)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:202)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:65)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
```
</description><key id="29355031">5424</key><summary>Strange exception when starting ES embedded for integration testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dariodariodario</reporter><labels /><created>2014-03-13T14:34:33Z</created><updated>2014-12-30T14:12:51Z</updated><resolved>2014-12-30T14:12:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-13T14:35:21Z" id="37539740">do you have lucene jars on your classpath?
</comment><comment author="dariodariodario" created="2014-03-13T14:36:22Z" id="37539868">are they needed now? Before it was a dependency coming with Elasticsearch. If I do need them now, what version and packages are needed? 
</comment><comment author="s1monw" created="2014-03-13T14:40:18Z" id="37540362">if you use mvn they should be there... not sure if you do
</comment><comment author="dariodariodario" created="2014-03-13T14:42:28Z" id="37540635">I use gradle, what I've done was just to upgrade my ES dependency to 1.0.1 from 1.0 and it broke it down.

Also I've just tried to add             'org.apache.lucene:lucene-core:4.7.0' to the dependencies, but no change. 

I get the same results when doing this from command line using gradle or from intelliJ
</comment><comment author="dariodariodario" created="2014-03-13T15:59:01Z" id="37550477">I noticed that in every project where I have elasticsearch, if I have a dependency to 1.0 it pulls the dependencies for Lucene. If I upgrade to 1.0.1 it doesn't anymore.

I've seen this in a maven project and two gradle projects.

I will open a more precise issue for this
</comment><comment author="clintongormley" created="2014-12-30T14:12:51Z" id="68358954">No more info since March. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `Releasable` in favor of `Closeable` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5423</link><project id="" key="" /><description>given that we are moving to Java 1.7 when Lucene 4.8 lands (#5421) we should take advantage of `try-with` statements and use Closeable instead since it's bascially the same thing!
</description><key id="29353294">5423</key><summary>Remove `Releasable` in favor of `Closeable` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T14:14:27Z</created><updated>2015-06-06T17:04:46Z</updated><resolved>2014-04-14T21:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-13T14:16:34Z" id="37537442">+1
</comment><comment author="jpountz" created="2014-03-13T14:17:59Z" id="37537620">Or should it be just `AutoCloseable` instead of `Closeable`? Javadocs suggest that `Closeable.close` needs to be idempotent while `AutoCloseable.close` doesn't and I think it makes sense to throw errors upon double-release (as it quite likely suggests a programming error)?
</comment><comment author="uboness" created="2014-03-13T14:23:43Z" id="37538307">Not sure I agree... Sometimes you just want to make sure something is closed even if there's a chance it's already closed... We should be lenient there
</comment><comment author="uboness" created="2014-03-13T14:25:16Z" id="37538486">We can use assertion for ensuring things are closed once
</comment><comment author="s1monw" created="2014-03-13T14:32:16Z" id="37539376">I think it's good to use AutoCloseable since we can then throw already closed exceptiosn? I like the idea - I guess we should have a base class that makes sure the actual releaseing is only happening once?
</comment><comment author="s1monw" created="2014-04-14T20:39:28Z" id="40415231">@jpountz I think we can lose this now right?
</comment><comment author="jpountz" created="2014-04-14T21:20:18Z" id="40419708">Closed via e58930180693ca4505030d76bc417a45d3e7e273. Releasable now extends AutoCloseable and can be used in try-with-resources.
</comment><comment author="s1monw" created="2014-04-14T21:21:34Z" id="40419840">thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated Gateways</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5422</link><project id="" key="" /><description>we deprecated Gateways before `1.0` and given the snapshot and restore feature they became obsolete. We gave quite some time to migrate and I think we should move away  from it in `1.2.0`. On master we can remove it already I guess but it might make sense to  merge it together? @imotov I think you should remove the gateways ;)
</description><key id="29353080">5422</key><summary>Remove deprecated Gateways</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T14:11:33Z</created><updated>2014-04-18T09:08:16Z</updated><resolved>2014-03-26T22:41:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Move to JAVA 1.7 with the once 1.1 is released following the upcoming Lucene 4.8 Release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5421</link><project id="" key="" /><description>Lucene moved to Java 1.7 as the minimum requirement here is the reasoning / official announcement:

```
the Apache Lucene/Solr committers decided with a large majority on the vote to require Java 7 for the next minor release of Apache Lucene and Apache Solr (version 4.8)!
Support for Java 6 by Oracle  already ended more than a year ago and Java 8 is coming out in a few days.

The next release will also contain some improvements for Java 7:
- Better file handling (especially on Windows) in the directory implementations. Files can now be deleted on windows, although the index is still open - like it was always possible on Unix environments (delete on last close semantics).
- Speed improvements in sorting comparators: Sorting now uses Java 7's own comparators for integer and long sorts, which are highly optimized by the Hotspot VM..

If you want to stay up-to-date with Lucene and Solr, you should upgrade your infrastructure to Java 7. Please be aware that you must use at least use Java 7u1.
The recommended version at the moment is Java 7u25. Later versions like 7u40, 7u45,... have a bug causing index corrumption. Ideally use the Java 7u60 prerelease, which has fixed this bug. Once 7u60 is out, this will be the recommended version.
In addition, there is no Oracle/BEA JRockit available for Java 7, use the official Oracle Java 7. JRockit was never working correctly with Lucene/Solr (causing index corrumption), so this should not be an issue for you. Please also review our list of JVM bugs: http://wiki.apache.org/lucene-java/JavaBugs

Apache Lucene and Apache Solr were also heavily tested with all prerelease versions of Java 8, so you can also give it a try! Looking forward to the official Java 8 release next week - I will run my indexes with that version for sure!
```

We should move as well and make Java 1.7 the minimum requirement to run Elasticsearch once we move to `Lucene 4.8`. Note, this is not yet release and it will roughly take a month or so until it will. I am putting es version `1.2.0` on this issue and mark it breaking to notify users. This means everything `&gt;= v1.2.0` will need Java 1.7 and will not run on `1.6` to be absolutely clear.
</description><key id="29352706">5421</key><summary>Move to JAVA 1.7 with the once 1.1 is released following the upcoming Lucene 4.8 Release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v1.2.0</label></labels><created>2014-03-13T14:07:09Z</created><updated>2014-05-26T15:57:25Z</updated><resolved>2014-03-27T10:09:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-13T14:11:56Z" id="37536877">I think we should add this to our pom.xml:

``` xml
&lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt;
        &lt;version&gt;1.3.1&lt;/version&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;enforce-versions&lt;/id&gt;
            &lt;goals&gt;
              &lt;goal&gt;enforce&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;rules&gt;
                &lt;requireJavaVersion&gt;
                  &lt;version&gt;1.7&lt;/version&gt;
                &lt;/requireJavaVersion&gt;
              &lt;/rules&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
```

See also: http://maven.apache.org/enforcer/enforcer-rules/requireJavaVersion.html
</comment><comment author="s1monw" created="2014-03-13T21:43:23Z" id="37590615">++ to the enforcer... @dadoonet do you wanna add it to master / or provide a PR?
</comment><comment author="dadoonet" created="2014-03-13T22:12:11Z" id="37593396">Sure.
</comment><comment author="dadoonet" created="2014-03-13T22:26:25Z" id="37594647">@s1monw done in my repo with https://github.com/dadoonet/elasticsearch/commit/4e926d8ed7b5d7597f4faf57861547eedc175f82 Wanna check?

BTW, I marked it as `1.2.0` although this change will be pushed in `master` (2.0.0). I guess that when 1.1.0 will be released I could cherry pick this commit and push it in branch `1.x`, right?
Which other label I should use to mark this issue? (`test`?)
</comment><comment author="s1monw" created="2014-03-27T10:09:07Z" id="38785922">fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BigArray/ByteArray improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5420</link><project id="" key="" /><description>We could use some extensions to BigArray/ByteArray:
- copy() - similar to System.arrayCopy()
- equals()/hashCode()

The latter is mostly driven by PagedBytesReference which currently materializes the pages into one big byte[], which is unnecessary.

@jpountz Suggestions where and how to add equals/hashCode?
</description><key id="29346302">5420</key><summary>BigArray/ByteArray improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T12:28:49Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-03-14T15:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-13T12:36:55Z" id="37527642">I think it would make sense to have these methods implemented as helper methods in `BigArrays` (quite similarly to `java.util.Arrays` for simple `byte[]` arrays)?
</comment><comment author="s1monw" created="2014-03-13T13:23:47Z" id="37531823">+1 to add helper methods like BytesRef.deepcopy() do make sure it's an actual copy and shallowCopy etc
</comment><comment author="hhoffstaette" created="2014-03-14T15:38:43Z" id="37661049">As mentioned in the PR #5435 we reduced the scope of this issue to equals/hashCode for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make aggregations CacheRecycler-free.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5419</link><project id="" key="" /><description>Aggregations were still using CacheRecycler on the reduce phase. They are now
using page-based recycling for both the aggregation phase and the reduce phase.

Close #4929
</description><key id="29346157">5419</key><summary>Make aggregations CacheRecycler-free.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T12:26:34Z</created><updated>2015-06-07T15:50:34Z</updated><resolved>2014-03-13T15:54:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-13T13:52:28Z" id="37534674">Agree, in this case it doesn't really embodies the purpose of this class... Maybe 
</comment><comment author="uboness" created="2014-03-13T13:52:42Z" id="37534706">Oops 
</comment><comment author="uboness" created="2014-03-13T13:58:26Z" id="37535350">As I was saying.... Maybe AbstractPagedHash? I just want to move it away as far as possible from java collections names
</comment><comment author="jpountz" created="2014-03-13T14:30:43Z" id="37539182">@uboness just added a commit following your recommendation
</comment><comment author="kimchy" created="2014-03-13T14:32:20Z" id="37539380">wonderful!
</comment><comment author="uboness" created="2014-03-13T14:38:07Z" id="37540072">LGTM! +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support of field boost in prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5418</link><project id="" key="" /><description>ES ignores field level boost factors specified in mapping when computing documents' score for query string rewritten to prefix query.

Sample setup:

**1. Create index and mapping.**

``` bash
curl -XPUT 'http://localhost:9200/messages/'
```

``` bash
curl -XPUT 'http://localhost:9200/messages/message/_mapping' -d '
{
    "message" : {
        "properties" : {
            "message" : {"type" : "string", "store" : true },
            "comment" : {"type" : "string", "store" : true , "boost" : 5.0 }
        }
    }
}'
```

**2. Create sample docs**

``` bash
curl -XPUT 'http://localhost:9200/messages/message/1' -d '{
    "user" : "user1",
    "message" : "test message",
    "comment" : "whatever"
}'

curl -XPUT 'http://localhost:9200/messages/message/2' -d '{
    "user" : "user2",
    "message" : "hello world",
    "comment" : "test comment"
}'
```

**3. Wait for ES to be synced**

``` bash
curl -XPOST 'http://localhost:9200/messages/_refresh' 
```

**4. Search in default catch-all field using query string rewritten to term query**

``` bash
curl -XPOST 'http://localhost:9200/messages/_search' -d '{ "query" : { "query_string" : { "query" : "test" } } , "explain" : true }' | python -mjson.tool
```

**5. Search in default catch-all field using query string rewritten to prefix query**

``` bash
curl -XPOST 'http://localhost:9200/messages/_search' -d '{ "query" : { "query_string" : { "query" : "tes*" , "rewrite" : "top_terms_boost_10" } } , "explain" : true }' | python -mjson.tool

The first query (step 4) works as expected -- document with id equal to 2 has higher score.
The second query (step 5) returns two documents with equal score.
```
</description><key id="29344195">5418</key><summary>Add support of field boost in prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ei82</reporter><labels /><created>2014-03-13T11:51:28Z</created><updated>2014-12-30T14:12:18Z</updated><resolved>2014-12-30T14:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="crowleym" created="2014-03-27T16:03:37Z" id="38823504">@s1monw would your fix for #5258 also apply in this case?
Thanks.
</comment><comment author="clintongormley" created="2014-12-30T14:12:18Z" id="68358922">Hi @ei82 

Sorry it has taken a while to get to this issue.  You are using the wrong rewrite method - instead of `top_terms_boost_10` (which applies a constant boost) you want `top_terms_10`,  which takes the actual term weights into account.

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-multi-term-rewrite.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type token_count should sum up multi-value counts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5417</link><project id="" key="" /><description>When indexing arrays of strings, the `token_count` field type should add up all of the token counts, rather than storing a count per array entry:

```
DELETE /my_index

PUT /my_index
{
  "mappings": {
    "my_type": {
      "properties": {
        "tags": {
          "type": "string",
          "fields": {
            "count": {
              "type": "token_count",
              "analyzer": "keyword",
              "store": "yes"
            }
          }
        }
      }
    }
  }
}

PUT /my_index/my_type/1
{
  "tags": [
    "search"
  ]
}

PUT /my_index/my_type/2
{
  "tags": [
    "search",
    "open_source"
  ]
}

GET /my_index/my_type/_search?fields=*
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "tags": "search"
              }
            },
            {
              "term": {
                "tags.count": 1
              }
            }
          ]
        }
      }
    }
  }
}
```

Gives:

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "my_index",
            "_type": "my_type",
            "_id": "1",
            "_score": 1,
            "fields": {
               "tags.count": [
                  1
               ]
            }
         },
         {
            "_index": "my_index",
            "_type": "my_type",
            "_id": "2",
            "_score": 1,
            "fields": {
               "tags.count": [
                  1,
                  1
               ]
            }
         }
      ]
   }
}
```
</description><key id="29339850">5417</key><summary>Type token_count should sum up multi-value counts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label></labels><created>2014-03-13T10:33:37Z</created><updated>2016-11-10T18:55:50Z</updated><resolved>2016-11-10T18:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-13T11:22:49Z" id="37522209">I wonder if this type should actually sum the token count for the relevant path on the whole document, and then add the relevant field to the Lucene Document once it summed it up in the post doc phase.
</comment><comment author="clintongormley" created="2014-03-13T11:24:16Z" id="37522301">That'd make sense to me
</comment><comment author="nik9000" created="2014-03-13T12:25:44Z" id="37526802">Makes sense to me.
</comment><comment author="clintongormley" created="2016-11-10T18:55:50Z" id="259775470">No traction. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow scripts to return more than 4 values in aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5416</link><project id="" key="" /><description>A missing call to ArrayUtil.grow prevented the array that stores the values
from growing in case the number of values returned by the script was higher
than the original size of the array.

Close #5414
</description><key id="29334921">5416</key><summary>Allow scripts to return more than 4 values in aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T09:08:49Z</created><updated>2015-06-07T22:37:38Z</updated><resolved>2014-03-14T12:54:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add/remove clusters from tribe node dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5415</link><project id="" key="" /><description>It would be nice to dynamically add or delete clusters on a tribe node, i.e. without
editing elasticsearch.yml and then having to stop/start the tribe node.

Is this possible ?

This is probably too much to wish for, but it would be nice if each node/cluster that "knew" it
was a part of a tribe could add/remove itself from the "tribe" (so to speak).
</description><key id="29333770">5415</key><summary>add/remove clusters from tribe node dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cleesmith</reporter><labels><label>:Tribe Node</label><label>high hanging fruit</label></labels><created>2014-03-13T08:47:21Z</created><updated>2016-11-24T18:01:32Z</updated><resolved>2016-11-24T18:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T15:14:45Z" id="148080798">A potential problem here is what to do when merging cluster states with conflicts.
</comment><comment author="raf64flo" created="2015-12-28T13:29:48Z" id="167568608">A solution could'nt be to fail the addition of the new cluster in the tribe in case of conflict at merge time?
</comment><comment author="agonen" created="2016-07-04T12:03:30Z" id="230275113">Hi any update on this , we need this possibility.

Our use case is as follow:
  we capture network data in several sites and index into a local ES. we are using tribe to do federate queries across several sites . 
we would like to be able to add new site **without** effecting (restart) on tribe node.

any idea ? or we must do restart ?  
</comment><comment author="clintongormley" created="2016-11-10T18:55:26Z" id="259775363">Closing in favour of https://github.com/elastic/elasticsearch/issues/21473
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripts in aggs can't return more than 4 values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5414</link><project id="" key="" /><description>Document scripts can return an array of up to 4 values, but more than 4 cause an array out of bounds exception to be thrown:

```
DELETE /myindex
PUT /myindex/t/1
{}

GET /myindex/_search
{
  "aggs": {
    "foo": {
      "date_histogram": {
        "script": "[1388534400000,1388534400000,1388534400000,1388534400000]",
        "interval": "hour"
      }
    }
  }
}
```

This throws an exception:

```
GET /myindex/_search
{
  "aggs": {
    "foo": {
      "date_histogram": {
        "script": "[1388534400000,1388534400000,1388534400000,1388534400000,1388534400000]",
        "interval": "hour"
      }
    }
  }
}
```
</description><key id="29333550">5414</key><summary>Scripts in aggs can't return more than 4 values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-13T08:42:57Z</created><updated>2014-03-18T08:20:57Z</updated><resolved>2014-03-14T12:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-17T14:54:25Z" id="37824423">this also fails on 1.0. Should we backport this?
</comment><comment author="uboness" created="2014-03-17T14:55:32Z" id="37824583">it should, yes... backported that is
</comment><comment author="jpountz" created="2014-03-18T08:20:57Z" id="37908510">I just backported it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast discovery should favour ping response with master over a ping response without master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5413</link><project id="" key="" /><description>For unicast zen discovery don't overwrite a ping response for a node if the previous ping response has a set master and the current response hasn't.

Per single main ping request we maintain the received ping response per node. Each node level ping response is mapped into that. If from a previous node level ping request the response has already been set for a node, it will be overwritten. We give higher value to the latest response. This change makes sure that this doesn't happen if the previous response has a set master and the current response hasn't a set master. Otherwise a node will lose the fact that another node has elected itself as master, the result of that would be that there would multiple master nodes in a single cluster.
</description><key id="29329489">5413</key><summary>Unicast discovery should favour ping response with master over a ping response without master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label></labels><created>2014-03-13T06:57:28Z</created><updated>2015-05-18T23:32:38Z</updated><resolved>2014-03-13T08:32:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-13T08:04:25Z" id="37508033">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor nits on aggs doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5412</link><project id="" key="" /><description /><key id="29319396">5412</key><summary>minor nits on aggs doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kurtado</reporter><labels /><created>2014-03-13T01:32:56Z</created><updated>2014-07-16T21:47:42Z</updated><resolved>2014-03-13T08:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-13T08:08:00Z" id="37508216">added.. https://github.com/elasticsearch/elasticsearch/commit/ca6a2bb79015cf29e704fa6fa4817a39db3a82d5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add retry mechanism to get snapshot method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5411</link><project id="" key="" /><description>During snapshot finalization the snapshot file is getting overwritten. If we try to read the snapshot file at this moment we can get back an empty or incomplete snapshot. This change adds a retry mechanism in case of such failure.
</description><key id="29314696">5411</key><summary>Add retry mechanism to get snapshot method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-12T23:50:30Z</created><updated>2015-06-07T22:37:49Z</updated><resolved>2014-03-13T22:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-13T07:53:35Z" id="37507523">I think this looks good (yet I think we both don't really like the impl but we have not much of a choice here) I think you should push this to 1.x, 1.0 and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to share multicast socket within jvm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5410</link><project id="" key="" /><description>Due to bugs in jvm (specifically OSX), running zen discovery tests causes for "socket close" failure on receive on multicast socket, and under some jvm versions, even crashes. This happens because of the creation of multiple multicast sockets within the same VM. In practice, in our tests, we use the same settings, so we can share the same multicast socket across multiple channels.
This change creates an abstraction called MulticastChannel, that can be shared, with ref counting. Today, the shared option is only enabled under OSX.
</description><key id="29307033">5410</key><summary>Allow to share multicast socket within jvm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-03-12T21:47:04Z</created><updated>2014-06-12T20:15:18Z</updated><resolved>2014-03-14T13:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-13T15:36:15Z" id="37547475">applied first review comments
</comment><comment author="s1monw" created="2014-03-14T10:30:12Z" id="37634237">two minor comments but other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add static analysis profile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5409</link><project id="" key="" /><description>Added pmd, findbug as well as site generation logic to top pom.xml file
Created customized pmd ruleset
</description><key id="29299893">5409</key><summary>Add static analysis profile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-03-12T20:41:18Z</created><updated>2014-07-16T21:47:43Z</updated><resolved>2014-03-13T19:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-03-13T19:38:25Z" id="37577332">Merged https://github.com/elasticsearch/elasticsearch/commit/2e5625329348a5951428de11a840fdfefebc7cbe
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deb created on Windows doesn't install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5408</link><project id="" key="" /><description>Ubuntu 12.04.  deb created from `mvn package` on Windows doesn't have any directories:

```
cloud@data-001:~$ /usr/bin/dpkg -c /var/lib/elasticsearch/elasticsearch-1.1.0-SNAPSHOT.deb | grep ^d
cloud@data-001:~$
```

Which causes:

```
cloud@data-001:~$ sudo /usr/bin/dpkg --force-confold -i /var/lib/elasticsearch/elasticsearch-1.1.0-SNAPSHOT.deb
(Reading database ... 62927 files and directories currently installed.)
Unpacking elasticsearch (from .../elasticsearch-1.1.0-SNAPSHOT.deb) ...
dpkg: error processing /var/lib/elasticsearch/elasticsearch-1.1.0-SNAPSHOT.deb (--install):
 unable to create `/usr/share/elasticsearch/bin/elasticsearch.dpkg-new' (while processing `./usr/share/elasticsearch/bin/elasticsearch'): No such file or directory
dpkg-deb: error: subprocess paste was killed by signal (Broken pipe)
Errors were encountered while processing:
 /var/lib/elasticsearch/elasticsearch-1.1.0-SNAPSHOT.deb
cloud@data-001:~$
```
</description><key id="29296908">5408</key><summary>deb created on Windows doesn't install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2014-03-12T20:05:43Z</created><updated>2015-08-13T13:55:47Z</updated><resolved>2014-12-05T10:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-02T10:31:24Z" id="42014795">I tried this with two different build debs from windows and they both installed fine. I used the current master for this one.

Can you send me one of those broken windows debian packages so I can check out that specific one? 
</comment><comment author="drewr" created="2014-05-07T15:33:50Z" id="42442558">@spinscale Need to get it from @costin...
</comment><comment author="spinscale" created="2014-05-09T16:16:31Z" id="42684227">tried this, costins package fails, where as two others have worked. I created an issue upstream, if this is a known bug, but will remove the 1.x label from this one, as for now we can simply recommend people to build packages under linux (we do this with RPM as well).
</comment><comment author="spinscale" created="2014-07-18T11:33:34Z" id="49421291">this issue was closed on the jdeb site as it could not be reliably reproduced, see https://github.com/tcurdt/jdeb/issues/155

Seems for now, we have to tell people to build with linux to make this does not happen (also doesnt happen on all windows installations)
</comment><comment author="spinscale" created="2014-12-05T10:30:01Z" id="65772057">closing this for now, as this seems to happen rare and is hard to reproduce on the jdeb side. Recommending people to build debian packages under linux for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5407</link><project id="" key="" /><description /><key id="29292711">5407</key><summary>Benchmark API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-03-12T19:11:38Z</created><updated>2014-04-09T20:39:52Z</updated><resolved>2014-04-09T20:39:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Benchmark API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5406</link><project id="" key="" /><description>Add an API endpoint at /_bench for submitting, listing, and aborting
search benchmarks. This API can be used for timing search requests,
subject to various user-defined settings.

Benchmark results provide summary and detailed statistics on such
values as min, max, and mean time. Values are reported per-node so that
it is easy to spot outliers. Slow requests are also reported.

Long running benchmarks can be viewed with a GET request, or aborted
with a POST request.

Benchmark results are optionally stored in an index for subsequent
analysis.

Closes #5407 
</description><key id="29292123">5406</key><summary>Benchmark: Benchmark API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>feature</label></labels><created>2014-03-12T19:04:25Z</created><updated>2015-06-06T17:58:17Z</updated><resolved>2014-04-09T20:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-14T19:59:18Z" id="37689042">I like this change in general. I think it will make it easier to report and debug performance issues by just providing a reproducible benchmark!
</comment><comment author="jpountz" created="2014-03-20T18:36:09Z" id="38204766">This change is quite large, so in order to move forward I would vote to fix the correctness issues that have been reported  but to postpone feature requests such as configurable percentiles to later in order to not delay this PR.

Since this is a new API, I would also like to mark this feature as experimental in the documentation for now (like we did for the completion suggester) so that we still have the opportunity to break backward compatibility in future releases as we get more feedback on the usage of this feature.
</comment><comment author="s1monw" created="2014-03-26T14:49:15Z" id="38692432">@aleph-zero are you working on an update here? I'd love to look at it once you added and updated commit?
</comment><comment author="aleph-zero" created="2014-04-01T03:04:02Z" id="39166495">@jpountz How can I mark this as experimental as you suggest? What is the convention for this? Just put a note in the benchmark docs?
</comment><comment author="jpountz" created="2014-04-01T09:02:05Z" id="39184063">@aleph-zero Yes, a note at the beginning of the reference documentation for the benchmark API. You can look at the percentiles documentation for example.
</comment><comment author="s1monw" created="2014-04-01T21:42:29Z" id="39262990">@aleph-zero I did a review of this! I really like it.. we are very close IMO. I think most of the work is really just polishing and adding all the missing docs! If you have questions please lemme know.
</comment><comment author="aleph-zero" created="2014-04-02T04:56:39Z" id="39288891">The big issues have now been resolved. Please see the updated asciidoc for examples on how to submit benchmarks. Competitors can now specify which indices to execute against, and common parameters have been moved up to the top-level. The result is a more concise, but no less descriptive, syntax. Have also added the ability to specify cache clearing behavior per-competitor. 
</comment><comment author="s1monw" created="2014-04-02T10:25:47Z" id="39313955">@aleph-zero it would be awesome if you could not squash your commits into one while we iterating that way it's easier to review. I really can't review the entire 5k lines each time :) I am waiting for you next iteration since there still seem to be lot of things not yet addressed especially documentaition wise ie. how to setup the nodes and an explained result and how to store stuff in an index?
</comment><comment author="aleph-zero" created="2014-04-02T17:33:44Z" id="39359502">Now that indices are specified in the request body I think it makes sense to remove them as URL parameters. Allowing the user to specify which indices to execute on in two different places is confusing.

For example the code currently allows this: curl -XGET http://localhost:9200/index_a/_bench -d '{
"competitors": [ {  
 "name": "competitor_1",
 "indices": [ "index_x", "index_y" ],
 .... 
}'

Which index list has precedence? It is not clear. I recommend simplifying the URL pattern from: '/{index}/{type}/_bench' to simply '/_bench'.

Comments?
</comment><comment author="s1monw" created="2014-04-02T17:34:39Z" id="39359600">well it's nice to make the url ones default if nothing is specified?
</comment><comment author="aleph-zero" created="2014-04-02T17:40:57Z" id="39360344">Ok I'll keep indices available as URL parameters and I will clearly document precedence rules.
</comment><comment author="aleph-zero" created="2014-04-03T01:03:20Z" id="39402452">Documentation has been marked as experimental.
</comment><comment author="jpountz" created="2014-04-08T15:49:24Z" id="39864997">Just did another review and I only found minor issues. I probably missed things given the size of this pull request but since it doesn't modify existing code, I think the risk of introducing bugs is very low so I'm +1 to merging it, we will improve it over time.
</comment><comment author="s1monw" created="2014-04-08T16:08:35Z" id="39867591">I left a bunch of smaller comments but I agree with @jpountz lets get those comments fixed and then move it in!
</comment><comment author="s1monw" created="2014-04-09T10:19:46Z" id="39948641">LGTM +1 to squash and push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index glob that doesn't match any index returns all indices for _stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5405</link><project id="" key="" /><description>Trying with 1.0.1

curl 'http://localhost:9200/foobar*/_stats?pretty&amp;clear=1'

The indices section has all my indices. :(

Trying with 0.90.12

{
  "error" : "IndexMissingException[[foobar-*] missing]",
  "status" : 404
}

Either the 0.90 solution or an empty indices section would be better then returning all.
This may happen with other APIs, haven't tried
</description><key id="29285834">5405</key><summary>index glob that doesn't match any index returns all indices for _stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2014-03-12T17:51:25Z</created><updated>2014-04-15T14:00:24Z</updated><resolved>2014-04-15T14:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awick" created="2014-04-14T12:37:34Z" id="40360735">Still happens with 1.1
</comment><comment author="clintongormley" created="2014-04-15T14:00:24Z" id="40484391">Closing as duplicate of #5794
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade randomized-testing to 2.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5404</link><project id="" key="" /><description>Note that the standard `atLeast` implementation has now Integer.MAX_VALUE as upper bound, thus it behaves differently from what we expect in our tests, as we never expect the upper bound to be that high.
Added our own `atLeast` to `AbstractRandomizedTest` so that it has the expected behaviour with a reasonable upper bound.
See https://github.com/carrotsearch/randomizedtesting/issues/131
</description><key id="29285224">5404</key><summary>Upgrade randomized-testing to 2.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label></labels><created>2014-03-12T17:44:09Z</created><updated>2014-06-18T05:09:47Z</updated><resolved>2014-03-14T11:03:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-13T13:59:35Z" id="37535480">I think we should actually go through all atLeast places in the code and use `scaledIntBetween` unless the calls really mean `gimme something greater than X` - luca, I can help here maybe you can just push this to the shared repo and we take it from there?
</comment><comment author="javanna" created="2014-03-13T14:12:40Z" id="37536972">Agreed, I think this gives us a chance to review our tests and improve them when possible, maybe even make them a bit faster.
</comment><comment author="s1monw" created="2014-03-14T11:03:57Z" id="37636558">merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add exceptions to GeoPointFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5403</link><project id="" key="" /><description>Add exceptions to `GeoPointFieldMapper` when parsing `geo_point` object
Closes #5390
</description><key id="29278836">5403</key><summary>Add exceptions to GeoPointFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>:Geo</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-12T16:32:43Z</created><updated>2015-06-07T22:38:20Z</updated><resolved>2014-03-19T17:02:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T16:37:20Z" id="37431233">I think this looks great can you maybe add a testcase for it to make sure it's triggered?
</comment><comment author="chilling" created="2014-03-19T11:50:10Z" id="38042078">@s1monw, @jpountz I refactored the `GeoPoint` and `GeoPointFieldMapper` to use a common method for parsing
</comment><comment author="jpountz" created="2014-03-19T15:08:52Z" id="38061605">+1 to push
</comment><comment author="chilling" created="2014-03-19T17:02:23Z" id="38078342">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested multi_field type wrapped by a custom type passing 'external values' doesn't get values passed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5402</link><project id="" key="" /><description>This issue came up when I tried to combine the **Attachment** type (elasticsearch-mapper-attachments plugin) with a nested **multi_field** type. The intention behind that was to use the fulltext content extracted by the Attachment type for creating shingles and nGrams as well.
### Prerequisites

Install the **Attachment** type plugin:

```
$ elasticsearch-1.0.1/bin/plugin --install elasticsearch/elasticsearch-mapper-attachments/2.0.0.RC1
```
### Index configuration

I've prepared a sample configuration for an elasticsearch index 'attachment' as listed below:

``` sh
# delete the index
$ curl -XDELETE 'http://localhost:9200/attachment'

# create the index with some analyzers to be used later on
$ curl -XPUT http://localhost:9200/attachment?pretty -d ' 
index :
    number_of_shards : 1
    number_of_replicas : 1

    analysis:
        analyzer:
            default:
                type: custom
                tokenizer: standard
            ngram_analyzer:
                type : custom
                tokenizer : standard
                filter: [ngram_filter]
            e_ngram_analyzer:
                type : custom
                tokenizer : standard
                filter: [e_ngram_filter]
            shingle_analyzer:
                type : custom
                tokenizer : standard
                filter: [shingle_filter]

        filter: 
            shingle_filter:
                type : shingle
                min_shingle_size : 2
                max_shingle_size : 5
                output_unigrams : true
            ngram_filter:
                type : nGram
                min_gram : 1
                max_gram : 20
            e_ngram_filter:
                type : edgeNGram
                min_gram : 1
                max_gram : 20                

    mapping:
        attachment:
            ignore_errors: false
'

# create the mapping for the index's documents
$ curl -XPUT http://localhost:9200/attachment/document/_mapping?pretty -d '
{
    "document" : {
        "properties" : {
            "file" : {
                "type" : "attachment",
                "path" : "full",
                "fields" : {
                    "file" : {
                        "type" : "multi_field",
                        "fields" : {
                            "file" : {
                                "type" : "string",
                                "store" : true,
                                "index" : "not_analyzed"
                            },
                            "shingle" : {
                                "type" : "string",
                                "store" : true,
                                "analyzer" : "shingle_analyzer"
                            },
                            "ngram" : {
                                "type" : "string",
                                "store" : true,
                                "analyzer" : "ngram_analyzer"
                            },
                            "e_ngram" : {
                                "type" : "string",
                                "store" : true,
                                "analyzer" : "e_ngram_analyzer"
                            }
                        }
                    },
                    "author" : {
                        "type" : "string",
                        "store" : true
                    },
                    "title" : {
                        "type" : "string",
                        "store" : true
                    },
                    "name" : {
                        "type" : "string",
                        "store" : true
                    },
                    "date" : {
                        "type" : "date",
                        "store" : true,
                        "format" : "dateOptionalTime"
                    },
                    "keywords" : {
                        "type" : "string",
                        "store" : true
                    },
                    "content_type" : {
                        "type" : "string",
                        "store" : true
                    },
                    "content_length" : {
                        "type" : "integer",
                        "store" : true
                    }
                }   
            }
        }
    }
}

'
```
### Adding document to the index

After adding a document, we will see, that the defined _multi_field_ fields for shingles and nGrams do NOT receive any data.

``` sh
# Adding a document to the index, as needed by the Attachment plugin binary data as base64 encoded string
$ curl -XPUT 'http://localhost:9200/attachment/document/1?pretty' -d '
{
    "file" : "VGhpcyBpcyBhIHRlc3QgZG9jdW1lbnQgdG8gZGVtb25zdHJhdGUgdGhlIGJlaGF2aW9yIG9mIHRoZSBBdHRhY2htZW50IHR5cGUgcGx1Z2luIAp3aGljaCBkb2Vzbid0IHBhc3MgdGhpcyBkb2N1bWVudCdzIGNvbnRlbnQgdG8gdGhlIGRlZmluZWQgbmVzdGVkIG11bHRpX2ZpZWxkIHBhcnRzIApmb3IgY3JlYXRpbmcgbkdyYW1zIGFuZCBzaGluZ2xlcy4="
}
'

# Searching for documents afterwards shows, that the multi_field fields are not receiving the data provided
$ curl -XGET 'http://localhost:9200/attachment/_search?q=*&amp;fields=*&amp;pretty'

# Result:
{
  "took" : 48,  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "attachment",
      "_type" : "document",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "file.e_ngram" : [ "" ],
        "file.content_length" : [ 200 ],
        "file.ngram" : [ "" ],
        "file.file" : [ "" ],
        "file.content_type" : [ "text/plain; charset=ISO-8859-1" ],
        "file.shingle" : [ "" ]
      }
    } ]
  }
}

```
### Problem in elasticsearch code

I've started investigations on this issue, as I could find discussions online about similar problems, but no solution. Digging into the depths of the elasticsearch code I've detected the issue and built a workaround inside the Attachment plugin's code for my local needs which produced the desired result. From my point of view this should be fixed inside the elasticsearch code and I will try to explain how.

From what I hopefully got right, there are two ways the **org.elasticsearch.index.mapper.ParseContext** may provide values to any subclass of **org.elasticsearch.index.mapper.core.AbstractFieldMapper&lt;T&gt;** for parsing:
- accessing the next token from the provided JSON stream: **org.elasticsearch.index.mapper.ParseContext.parser().currentToken()**
- checking for a so called 'external value' to be used instead: **org.elasticsearch.index.mapper.ParseContext.externalValue()**

Inside the **org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateFieldForString(ParseContext, String, float)** method this can be seen, where the 'external value' is tried to be **consumed** (emphasis intended) by calling _org.elasticsearch.index.mapper.ParseContext.externalValue()_. I am using the term 'consumed' as this is literally what happens as the call to the _externalValue()_ method sets a boolean flag to 'false' which is actually used for checking for existence of such an 'external value'. 

So, this is where the problem resides and I overcame it by wrapping the _ParseContext_ passed to the Attachment mapper plugin with my own implementation, that actually just delegates any call to the original context except for the _org.elasticsearch.index.mapper.ParseContext.externalValueSet()_ method which I've overwritten to not only check the boolean flag but also check for the 'external value' to be not _null_. This way it is assured that as long as the _multi_field_ fields are processed all of them get the content.
### Desired output after fixing the code

Just to make sure what the expected result should look like:

``` sh
# result of the above document being indexed with my local changes, where we can see that the nGram and shingle fields properly get content
{
  "took" : 12,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "attachment",
      "_type" : "document",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "file.e_ngram" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \nwhich doesn't pass this document's content to the defined nested multi_field parts \nfor creating nGrams and shingles.\n" ],
        "file.content_length" : [ 200 ],
        "file.ngram" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \nwhich doesn't pass this document's content to the defined nested multi_field parts \nfor creating nGrams and shingles.\n" ],
        "file.file" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \nwhich doesn't pass this document's content to the defined nested multi_field parts \nfor creating nGrams and shingles.\n" ],
        "file.content_type" : [ "text/plain; charset=ISO-8859-1" ],
        "file.shingle" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \nwhich doesn't pass this document's content to the defined nested multi_field parts \nfor creating nGrams and shingles.\n" ]
      }
    } ]
  }
}
```

Hopefully my explanations are clear to you guys. As I am not totally sure which way one should solve this issue in order to avoid any side-effects I am relying on you to get this thing fixed.

Thanks in advance
Tom
</description><key id="29272211">5402</key><summary>Nested multi_field type wrapped by a custom type passing 'external values' doesn't get values passed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">thomasm82</reporter><labels /><created>2014-03-12T15:23:16Z</created><updated>2014-07-25T15:01:14Z</updated><resolved>2014-07-25T15:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T08:38:01Z" id="48706992">@dadoonet What's the status of this? 
</comment><comment author="dadoonet" created="2014-07-11T08:39:43Z" id="48707159">@clintongormley At this exact moment, we are talking about it with @jpountz :) We would like to have it in 1.3!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when TransportClient times out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5401</link><project id="" key="" /><description>Elastic Search version: 1.0.0
Operating System: Windows 7 Enterprise
Java Version: 1.7.0_45
Server setup:
- Windows Server 2003
- Java 1.7.0_45
- Running Elastic Search as a service
- 1 Node with 1 index and 5 shards with 1 replica 

Here is the original issue:
https://groups.google.com/forum/#!msg/elasticsearch/QJx0nzVci1s/uqzE2YhKGJIJ

We are using a an instance of a Transport Client to connect to our Elastic Search server.  Since the instance is going to be living on a webserver, we need it to stay open so it is ready for whenever a user needs to perform a search, or data needs to be indexed. The problem is that if no operation is performed within an hour or so of the last operation on the transport client, it throws this error. The other scenario is if no operation is performed within 10-15 minutes of the Transport Client's last operation, and another operation is performed, it will throw the same error. 

If this error is thrown, sometimes it recovers and continues the operation, but a majority of the time it does not do this at all. Here is the stack trace:

```
Mar 08, 2014 1:15:37 AM org.elasticsearch.client.transport
INFO: [Elven] failed to get node info for [#transport#-1][WIN7-113-00726][inet[/159.140.213.87:9300]], disconnecting...
org.elasticsearch.transport.RemoteTransportException: [Server_Dev1][inet[/159.140.213.87:9300]][cluster/nodes/info]
Caused by: java.lang.NullPointerException
at org.elasticsearch.http.HttpInfo.writeTo(HttpInfo.java:82)
at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:301)
at org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse.writeTo(NodesInfoResponse.java:63)
at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:244)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:239)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.finishHim(TransportNodesOperationAction.java:225)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onOperation(TransportNodesOperationAction.java:200)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$900(TransportNodesOperationAction.java:102)
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
```
</description><key id="29265735">5401</key><summary>NPE when TransportClient times out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">roblangenfeld</reporter><labels /><created>2014-03-12T14:13:58Z</created><updated>2015-07-23T08:23:50Z</updated><resolved>2014-07-25T17:54:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-13T07:54:22Z" id="37507557">thanks for opening the issue.

Did you do any changes in your `elasticsearch.yml` configuration file? Did you disable HTTP?
</comment><comment author="roblangenfeld" created="2014-03-13T12:31:39Z" id="37527277">The only thing that was changed in the `elasticsearch.yml` file was I changed the cluster name and node name. But I appropriate applied those changes in the server code by passing them into the  TransportClient at the time instantiation. It can find the cluster just fine.
</comment><comment author="roblangenfeld" created="2014-03-13T12:34:24Z" id="37527468">Here is another error that comes up with this issue as well. I left the TransportClient open over night, came this morning performed a search and this is what I got:

```
Mar 13, 2014 7:32:37 AM com.sun.jersey.spi.container.ContainerResponse mapMappableContainerException
SEVERE: The RuntimeException could not be mapped to a response, re-throwing to the HTTP container
org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:263)
    at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)
    at org.elasticsearch.transport.TransportService$Adapter$2$1.run(TransportService.java:316)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

Mar 13, 2014 7:32:37 AM org.apache.catalina.core.StandardWrapperValve invoke
SEVERE: Servlet.service() for servlet [JerseyServlet] in context with path [/escluster] threw exception
org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:263)
    at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)
    at org.elasticsearch.transport.TransportService$Adapter$2$1.run(TransportService.java:316)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

```
</comment><comment author="spinscale" created="2014-03-13T13:04:20Z" id="37529905">A couple of questions here:

The exception in your initial bug report - where does it happen? On the side of the TransportClient or on the side of the elasticsearch "server" process?

You said, sometimes the elasticsearch recovers from that error. What do you mean with this? What do you mean with continuing the operation? I would think, if the NPE happens, you will not get back data, right?

Your second exception makes me think: How stable is your network (as a network problem might also have been an issue for the first case)? The second exception tries to connect to an elasticsearch node but fails. Is that node not reachable?

How do initiate the transport client? Do you connect to one single node or do you specify all the nodes in the cluster (there is also a feature called sniffing, where it is sufficient to connect to one node and the transport client then automatically finds out the other nodes in a cluster and uses all of them).
</comment><comment author="roblangenfeld" created="2014-03-13T13:16:58Z" id="37531130">The initial error in the bug report happens on the side of the TransportClient (I see it getting logged out on the eclipse console). 

To clarify my statement where I said it recovers, that is actually incorrect. Upon further testing it actually does not recover. I was getting that issue confused with a completely unrelated issue. So disregard that statement. When it does get the NPE it is unrecoverable.

As for our network stability it is very stable. We are using in house networking to do our initial testing.

As for our TransportClient initiation:

```
Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "escluster") //$NON-NLS-1$ //$NON-NLS-2$
                    .put("client.transport.snif", true).build(); //$NON-NLS-1$ 
TransportClient transportClient = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress(
                    serverAddress, port));
```
</comment><comment author="spinscale" created="2014-03-13T13:36:59Z" id="37533132">Might be a copy-paste error, but there is a `f` missing in `sniff`.

Sorry for not being clear with network stability: A stable network is not necessarily caused by an unreliable network itself. It might just be a badly configured firewall or router which kills long running TCP connections. So look at for any components in between your tomcat and elasticsearch, which might cause unwanted behaviour. Also it might be your operating system which kills that TCP connection (on either side).
</comment><comment author="justinuang" created="2014-04-09T02:09:21Z" id="39922183">Hi, I have the same exact issue, running ES 1.0.1 on an single in-memory node. I am inclined to think that it's not a configuration issue, since it usually works. I'm just wondering how it's possible that in line 82,

``` java
    @Override
    public void writeTo(StreamOutput out) throws IOException {
        address.writeTo(out);
        out.writeLong(maxContentLength);
    }
```

that `address` can possibly be null.
</comment><comment author="spinscale" created="2014-04-09T06:46:24Z" id="39933407">@justinuang did you change any default configuration? disable http?
</comment><comment author="justinuang" created="2014-04-09T14:21:21Z" id="39969219">```
2014-04-03 10:21:33,448 -0700 INFO  [main] http - [Clive] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/REDACTED:9200]}
```

Nope, looks like http is still running.
</comment><comment author="justinuang" created="2014-04-11T21:18:53Z" id="40254378">@spinscale do you think this is related to http?
</comment><comment author="spinscale" created="2014-04-14T07:17:53Z" id="40338791">thats where the NPE came from. Do you always use ipv6 addresses for HTTP when it fails?
</comment><comment author="justinuang" created="2014-04-17T00:31:09Z" id="40668674">Umm, the redacted `publish_address` was a ipv4 if that helps. I'm assuming that's what the client sees.
</comment><comment author="spinscale" created="2014-07-18T11:03:55Z" id="49419312">You may have hit this issue https://github.com/elasticsearch/elasticsearch/pull/6906

Could possibly test against master?
</comment><comment author="justinuang" created="2014-07-25T17:46:39Z" id="50181764">Thanks for your response. We haven't been able to repro this issue, but I'll let you know if it comes up again against master.
</comment><comment author="hash-include" created="2015-05-19T10:39:52Z" id="103432764">I am facing the same issue. Using elasticsearch 1.1.1 in amazon aws ec2 instance. System works nicely for some time (few scenarios for days) and then fails. 

I made cluster name and node name changes in elasticsearch.yml, which are reflected in java api. 

I am assuming network stability in amazon aws. I came across few points somewhere (can't find references now), based on which I increased number of open files parameter in amazon (amazon linux) machine where elasticsearch dev is deployed.
</comment><comment author="clintongormley" created="2015-05-25T11:34:11Z" id="105214710">@hash-include I suggest you upgrade to a recent version
</comment><comment author="piyushGoyal2" created="2015-07-23T08:23:49Z" id="124019829">hi @clintongormley 
The exception does not seem to go away. Although it's not an exception actually and comes as INFO in client logs, but following logs keeps on appearing randomly:
org.elasticsearch.transport.ReceiveTimeoutTransportException: [es_prod_********1][inet[/10.xxx.xx.xx:9301]][cluster/state] request_id [564] timed out after [5034ms]
at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:370)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)

Situation is ES 1.6 three node cluster. Client and master are running on the same node. However, this is not a normal query request. I am using ES-Hadoop plugin and the client tries to pull some 50K documents. Not sure if it's network issue or something related to less memory. At times when I see multiple occurrences of these logs, I eventually get NoNodeAvailableException. Any help is appreciated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distance Script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5400</link><project id="" key="" /><description>Updated docs for distance scripting and added
missing geohash distance functions
Closes #5397
</description><key id="29253023">5400</key><summary>Distance Script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>docs</label></labels><created>2014-03-12T10:51:27Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-03-20T14:43:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T10:54:13Z" id="37395805">LGTM
</comment><comment author="s1monw" created="2014-03-12T10:55:11Z" id="37395882">I think we should adjust the commit message a bit starting with `[Doc] Updated docs for distance scripting` as the headline
</comment><comment author="s1monw" created="2014-03-12T16:43:39Z" id="37432042">LGTM +1 to push to `1.0`, `1.x` and `master`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term query for _parent doesn't work for grandchild</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5399</link><project id="" key="" /><description>With a Parent &gt; Child &gt; Grandchild relationship, you can use a `term` query on the `_parent` field to find children with a particular `$parent`, but not grandchildren with parent `$Child`:

```
DELETE /myindx

PUT /myindex
{
  "mappings": {
    "Parent": {
      "properties": {
        "name": {
          "type": "string",
          "index": "analyzed"
        }
      }
    },
    "Child": {
      "_parent": {
        "type": "Parent"
      },
      "properties": {
        "name": {
          "type": "string",
          "index": "analyzed"
        }
      }
    },
    "GrandChild": {
      "_parent": {
        "type": "Child"
      },
      "properties": {
        "name": {
          "type": "string",
          "index": "analyzed"
        }
      }
    }
  }
}


POST /_bulk 
{ "index" : { "_index" : "myindex", "_type" : "Parent", "_id" : "alice" } }
{ "name" : "Alice" }
{ "index" : { "_index" : "myindex", "_type" : "Child", "_id" : "bob", "parent" : "alice" } }
{ "name" : "Bob"}
{ "index" : { "_index" : "myindex", "_type" : "GrandChild", "_id" : "gc1", "parent" : "bob", "routing" : "alice" } }
{ "name" : "grand child 1" }
{ "index" : { "_index" : "myindex", "_type" : "GrandChild", "_id" : "gc2", "parent" : "bob", "routing" : "alice" } }
{ "name" : "grand child 2"}

GET /_search?fields=_parent,_routing
```

This query works:

```
POST /myindex/Child/_search
{
  "query": {
    "term": {
      "_parent": "alice"
    }
  }
}
```

This query doesn't:

```
POST /myindex/GrandChild/_search
{
  "query": {
    "term": {
      "_parent": "bob"
    }
  }
}
```

But performing the same lookup with `has_parent` does work:

```
POST /myindex/GrandChild/_search
{
  "query": {
    "has_parent": {
      "parent_type": "Child",
      "query": {
        "filtered": {
          "filter": {
            "term": {
              "_id": "bob"
            }
          }
        }
      }
    }
  }
}
```
</description><key id="29251699">5399</key><summary>Term query for _parent doesn't work for grandchild</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Parent/Child</label><label>bug</label><label>discuss</label></labels><created>2014-03-12T10:28:53Z</created><updated>2016-01-21T12:20:58Z</updated><resolved>2016-01-21T12:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-12T13:40:21Z" id="37408739">What happens here is that the _parent mapping doesn't take the current type into account for the term filter. It simply takes the first mapping with parent it can find it prefixes the type and that is why the first term filter on _parent field works, but the second one doesn't.

I think this behaviour should change? If the type is specifically set in the url, use that type. If multiple types are specified use the first type with parent field mapping and if no type is specified fall back to all types with a _parent field.

Also it is possible to not let ES automatically detect the parent type, but specify it specifically in the term filter as a prefix in the term filter, like this:

```
POST /myindex/GrandChild/_search
{
  "query": {
    "term": {
      "_parent": "Child#bob"
    }
  }
}
```

This request does work correctly.
</comment><comment author="clintongormley" created="2014-03-12T13:46:25Z" id="37409391">Ah OK.  Yes I agree it should change. 
</comment><comment author="kaliseo" created="2015-09-09T07:57:12Z" id="138816815">Ohhh !! Thanks a lot for this tip ! It helped me a lot !
</comment><comment author="clintongormley" created="2015-10-14T15:12:48Z" id="148080302">Rethinking this..  The `has_parent` query does the right thing.  I think we should deprecate the use of the `_parent` field for search, unless something has changed in the new parent-child functionality in 2.0? /cc @martijnvg 
</comment><comment author="martijnvg" created="2015-11-15T14:22:01Z" id="156814603">Agreed, I think we should remove support to refer to the `_parent` field in the dsl. It allows us to clean things up a bit more (mapping code).

The only reason this field still exists is because the `_parent` lucene field is used in the fetch phase and get api to as source to include the parent id.

The pre 2.x parent/child join impl. relied on this field in the Lucene index, the 2.x parent/child join impl. doesn't use the `_parent` lucene field. The `_parent` mapping creates docvalues field for the join support depending on the name of the parent type.

My idea is (and was) to no longer use the `_parent` Lucene field and solely rely on the doc values fields for the join and retrieving the parent id in the fetch phase. 

I think there are still use cases for being able to return all child docs with a certain parent id, but I think we can create a separate query for it that uses the doc values fields? (instead of just using `_parent` field in queries like we do today)

A while back I started to do this for a bug fix in the `_parent` mapping for one of the 2.0 betas/rcs. Code for relying on doc values join fields for the fetch phase: https://github.com/martijnvg/elasticsearch/commit/26e2e5d95d2b3a3f778a759ed7a7ea2de9b09842#diff-b633d10e30dc0e0a476eb1e1a40736b7R41

And code for a `_parent_id` query to retrieve docs with a certain parent id value:
https://github.com/martijnvg/elasticsearch/commit/26e2e5d95d2b3a3f778a759ed7a7ea2de9b09842#diff-2be3c59879d9d984bd9a33fc42c1c940R32
</comment><comment author="timeu" created="2016-01-05T15:46:02Z" id="169039686">I ran into this issue today when I upgraded from 1.7.3 to 2.1.1. 
I need to fix the queries in my application code. The question if I should switch to the `has_parent`  query or just use the workaround with `"_parent":"type#id"` ? As far as I understand it, the use of `_parent` in a query will be deprecated right ? 
</comment><comment author="martijnvg" created="2016-01-21T12:19:59Z" id="173554629">Closing this issue as this has been fixed as part of #16045. From 3.0 there will be a `parent_id` query in the dsl that allows to easily query for child docs that refer to a specific parent id.
</comment><comment author="martijnvg" created="2016-01-21T12:20:58Z" id="173554789">@timeu The workaround you mention will remain to work in all 2.x versions. From 3.0 you should switch to the `parent_id` query instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document for HTTP RESTful API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5398</link><project id="" key="" /><description>Where can i find a full version of the HTTP RESTful API document?

For example, there is a rest api request body, i want to find  a document that describes those JSON sub-variables. Thanks.

{
  "facets": {
    "terms": {
      "terms": {
        "field": "API",
        "size": 200,
        "order": "term",
        "exclude": []
      },
      "facet_filter": {
        "fquery": {
          "query": {
            "filtered": {
              "query": {
                "bool": {
                  "should": [
                    {
                      "query_string": {
                        "query": "*"
                      }
                    }
                  ]
                }
              },
              "filter": {
                "bool": {
                  "must": [
                    {
                      "range": {
                        "@timestamp": {
                          "from": 1392004523238,
                          "to": "now"
                        }
                      }
                    }
                  ]
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
</description><key id="29236731">5398</key><summary>Document for HTTP RESTful API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fulinlin924</reporter><labels /><created>2014-03-12T04:18:50Z</created><updated>2014-03-12T05:35:16Z</updated><resolved>2014-03-12T05:35:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Distance script field is returning in meters instead of miles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5397</link><project id="" key="" /><description>I am running version 1.0.1 and connecting through the Java client. I am adding a distance script parameter to my query as follows:

addScriptField("distance", "doc['latLon'].arcDistance(lat,lon)", distanceScriptParams)

The distance field is returning a value which appears to be in meters. Based on the prior version I was running (0.90.10) and based on the documentation, this value should be in miles.

If I change my scriptField to "arcDistanceInKm" it returns a value equal to 1/1000 of the value I get when I run "arcDistance" which is what leads me to believe the value for "arcDistance" is being returned in meters.
</description><key id="29234209">5397</key><summary>Distance script field is returning in meters instead of miles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wdeupree</reporter><labels /><created>2014-03-12T02:54:51Z</created><updated>2014-03-20T11:20:14Z</updated><resolved>2014-03-20T11:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-12T04:04:41Z" id="37373680">The default behaviour was changed in https://github.com/elasticsearch/elasticsearch/issues/4515 but it looks like we didn't update [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_document_fields) accordingly. 
</comment><comment author="s1monw" created="2014-03-12T09:16:18Z" id="37388234">@chilling can you take a look at this - it should be an easy fix on the documentation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up phrase suggestion scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5396</link><project id="" key="" /><description>Two changes:
1.  In the StupidBackoffScorer only look for the trigram if there is a bigram.
2.  Cache the frequencies in WordScorer rather so we don't look them up
again and again and again.

This provides a pretty substantial speedup when there are many candidates.

Closes #5395
</description><key id="29230523">5396</key><summary>Speed up phrase suggestion scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-12T01:12:11Z</created><updated>2015-06-07T15:05:25Z</updated><resolved>2014-03-18T11:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-12T01:16:03Z" id="37365974">I'd really appreciate someone having a look at this.  I think it could save me some time.  I'll run it on a more real dataset in the morning and have more to report.

I'd also like to be able to use the candidate frequency rather than look up the term frequency again given that it is already there.  I think that would work in most cases but it doesn't seem right when you use one of the transformers.  I don't have a good grasp of those at the moment so I'm going to leave it as is.
</comment><comment author="nik9000" created="2014-03-12T01:17:35Z" id="37366051">@s1monw, is the phrase suggester still your baby?
</comment><comment author="s1monw" created="2014-03-12T09:13:37Z" id="37388041">@nik9000 that's a great patch - lets get this in soon it can speed things u dramatically I guess - I left some comments impl wise
</comment><comment author="nik9000" created="2014-03-12T19:14:42Z" id="37451505">Added a commit to improve indexRandom for this patch.  I think, though, that I should send it in another pull request and make this one dependent on it.
</comment><comment author="s1monw" created="2014-03-12T19:38:46Z" id="37454184">&gt; &gt; Added a commit to improve indexRandom for this patch. I think, though, that I should send it in another pull request and make this one dependent on it.

I like the change and I agree we should make it a sep. commit but not need to open a different PR. We can just keep this a sep commit, makes sense? I can port that commit then to `1.0` since this is a n improvement so it will only go into `1.x` and `master`
</comment><comment author="nik9000" created="2014-03-13T14:52:57Z" id="37541982">Caching helps real suggest calls with real data in the index.  Improvements get better as the phrase gets longer.  1 word phrases break even.  9 word phrases are ~75% faster.

How I figured that out:
I wanted to give this a shot with more "real world" data.  I made a [script](https://gist.github.com/nik9000/9528106) to load titles from English Wikipedia.  I didn't use the river because I only want the titles.  Anyway, then I hit it with some searches with another [script](https://gist.github.com/nik9000/9528170).  I turned off the caching and hit is again with the same script.  That spat out some [data](https://gist.github.com/nik9000/9529707).  I summarize it above.
</comment><comment author="nik9000" created="2014-03-13T16:22:28Z" id="37553235">I've cleaned up the pull request, squashing it into two commits: one for the scoring improvements and one for the indexRandom changes.
</comment><comment author="s1monw" created="2014-03-14T22:55:04Z" id="37705758">hey nik, I took a look at the TermsEnum you added and I think it goes in the right direction but it became more complex that I thought it would. I'd like to get this improvement in for 1.1 which is close but on the other hand I think the general TermsEnum is useful. What about splitting this up and go back to the `previous` version and put the enum as an inner class into `WordScorer` making all the methods that would require an actual seek throw `UnsupportedOperationException`. If it turns out that we really really need to do all the seeks etc. we can still open a sep issue? I think this would be more practical and then we can think about how to generally test it? I mean we can open a dedicated issue right away? Sorry for taking that step back now after you implemented all that stuff.
</comment><comment author="nik9000" created="2014-03-16T17:50:08Z" id="37764338">I have to admit I'm having trouble working up the willpower to back it out and work it back in in another issue.  What about slapping more warnings on it?  Just moving it to a subclass as is?
</comment><comment author="s1monw" created="2014-03-16T17:53:30Z" id="37764441">@nik9000 I can do it though no worries. Yet the reasoning here is that we don't need all the DocsEnum etc methods so no real seek is needed. I could have made up my mind earlier, `hindsight is 20/20`.... I am worries about the usecase for this general class since it's very limited and second I don't have a good answer to the testing question so I suggest make it simpler and throw UnsupportedOperationException where appropriate. No need for a sep issue.. 
</comment><comment author="nik9000" created="2014-03-16T19:51:27Z" id="37768001">The test I added isn't _too_ bad....  Anyway, I'll switch it back to the old implementation tonight. 

Sent from my iPhone

&gt; On Mar 16, 2014, at 1:53 PM, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; @nik9000 I can do it though no worries. Yet the reasoning here is that we don't need all the DocsEnum etc methods so no real seek is needed. I could have made up my mind earlier, hindsight is 20/20.... I am worries about the usecase for this general class since it's very limited and second I don't have a good answer to the testing question so I suggest make it simpler and throw UnsupportedOperationException where appropriate. No need for a sep issue..
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="nik9000" created="2014-03-17T01:26:36Z" id="37778538">Done.  Let me know what else I need to do.  I reran the (not scientific) performance tests I was running and it is marginally faster.  I assume because it is maintaining less stuff in the cache.
</comment><comment author="s1monw" created="2014-03-17T10:39:46Z" id="37802366">don't get me wrong I think you test was ok! I didn't want to make you upset or anything I really really appreciate your work here that's for sure!
</comment><comment author="kimchy" created="2014-03-17T10:49:58Z" id="37803153">Few questions on the caching:
- is it correct to have the last freq work as it does today, convoluting the doc freq with totalTermFreq? it might be very tricky down the road... . See the below point, but 2 long arrays for each freq would work well?
- Can we move the caching data structure to be BytesRefHash based (hopefully using paged data structures)? Something similar to what we ended up doing in significant terms: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java#L164.
</comment><comment author="nik9000" created="2014-03-17T11:43:44Z" id="37806805">&gt; Few questions on the caching:
&gt; 
&gt; ```
&gt; is it correct to have the last freq work as it does today, convoluting the doc freq with totalTermFreq? it might be very tricky down the road... . See the below point, but 2 long arrays for each freq would work well?
&gt; ```

It works in this context because the phrase suggester is configured to use one of the two and uses that one the whole time.  @s1monw also asked that I switch back to holding both so I'll have a look at that.

&gt; ```
&gt; Can we move the caching data structure to be BytesRefHash based (hopefully using paged data structures)? Something similar to what we ended up doing in significant terms: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java#L164.
&gt; ```

I was using that a few days ago but it required modifying a dozen classes to pass BigArrays around and handle Releasable.  I think that'd be OK but many of the classes are valid plugin extension points so maybe that isn't OK so late in the 1.1 release cycle.
</comment><comment author="nik9000" created="2014-03-17T11:59:32Z" id="37807773">Another note while I think about it, just in case folks need to further speed this process up:

Using the default configuration with `suggest_mode = always`, performance is now dominated by drawing candidates.  This is because the default only scores the top five candidates per term.  Even on many many term queries scoring is now pretty fast.  So if you wanted to make this even faster:
1.  You could get a quick win on the worst case by skipping looking at edit distance one.  Even though generating the automata for edit distance two is expensive, on my dataset edit distance one never seems to cut it.  The improvement is 10%ish or so on my tests but I can't be sure if they are truly representative here.
2.  Beyond that I don't see getting any faster without abandoning DirectSpellChecker for something that pregenerates all the candidates.  It'd be a pain to deal with, I think.
</comment><comment author="nik9000" created="2014-03-17T12:52:53Z" id="37811456">Add commit to cache both ttf and df AND to throw UnsupportedOperationExceptions.  Less hacky now.
</comment><comment author="s1monw" created="2014-03-17T16:34:02Z" id="37837487">LGTM can you maybe squash the commits together such that we have two commits, one for `Make indexRandom handle many documents better ...` and one for the speeups? Thanks Nik
</comment><comment author="kimchy" created="2014-03-17T18:18:56Z" id="37851179">++, we can work on a common frequency caching terms enum and reuse between both the spell checker and significant terms /cc @markharwood, @jpountz 
</comment><comment author="nik9000" created="2014-03-17T18:32:38Z" id="37852854">Squashed.
</comment><comment author="s1monw" created="2014-03-18T11:47:20Z" id="37923817">pushed to `1.x` and `master`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up phrase suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5395</link><project id="" key="" /><description>The phrase suggester spends quite a bit of time looking up term frequencies.  We can probably make it faster.
</description><key id="29230503">5395</key><summary>Speed up phrase suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-12T01:11:36Z</created><updated>2014-03-18T11:45:50Z</updated><resolved>2014-03-18T11:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomized number of replicas used for indices created during tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5394</link><project id="" key="" /><description>Introduced two levels of randomization for the number of replicas when running tests:

1) through the existing random index template, which now sets a random number of replicas that can either be 0 or 1 that is shared across all the indices created in the same test method unless overwritten

2)  through createIndex and prepareCreate methods, between 0 and the number of data nodes available, similar to what happens using the indexSettings method, which changes for every createIndex or prepareCreate unless overwritten (overwrites index template for what concerns the number of replicas)

Added the following facilities to deal with the random number of replicas:
- made it possible to retrieve how many data nodes are available in the `TestCluster`
- added common methods similar to indexSettings, to be used in combination with createIndex and prepareCreate method and explicitly control the second level of randomization: numberOfReplicas, minimumNumberOfReplicas and maximumNumberOfReplicas

Tests that specified the number of replicas have been reviewed:
- removed manual replicas randomization where present, replaced with ordinary one that's now available
- adapted tests that didn't need a specific number of replicas to the new random behaviour
- also done some more cleanup, used common methods like assertAcked, ensureGreen, refresh, flush and refreshAndFlush where possible
</description><key id="29224546">5394</key><summary>[TEST] Randomized number of replicas used for indices created during tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T23:31:53Z</created><updated>2014-06-12T14:11:43Z</updated><resolved>2014-03-13T12:09:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:29:46Z" id="37460293">I think the change looks good to me in general. I'd love to actually move to annotations rather than overriding methods here but we can do that later. I'd vote for getting it in very soon given that 1.1 is close? @jpountz WDYT
</comment><comment author="jpountz" created="2014-03-13T10:43:06Z" id="37519366">I think progress over perfection totally applies here, I'd vote to get this in!
</comment><comment author="javanna" created="2014-03-13T10:45:53Z" id="37519577">Agreed, we can always add annotations afterwards, going to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use patched version of TermsFilter to prevent using wrong cached results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5393</link><project id="" key="" /><description>See LUCENE-5502

Closes #5363
</description><key id="29219982">5393</key><summary>Use patched version of TermsFilter to prevent using wrong cached results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Search</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T22:20:10Z</created><updated>2015-06-07T22:43:30Z</updated><resolved>2014-03-12T01:43:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-11T22:27:43Z" id="37354435">Thank you Igor, +1 to push!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Corrected issue with throttle type setting not respected upon updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5392</link><project id="" key="" /><description>The setting "index.store.throttle.type" is not able to be updated on a live running instance using a PUT request to _settings
</description><key id="29218293">5392</key><summary>Corrected issue with throttle type setting not respected upon updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">slife</reporter><labels><label>:Settings</label><label>enhancement</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T21:19:30Z</created><updated>2015-06-07T15:05:53Z</updated><resolved>2014-03-12T09:57:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-11T21:30:47Z" id="37352241">This fix looks good! Could you please sign the [contributor agreement](http://www.elasticsearch.org/contributor-agreement/) so that we can get this change in? 
</comment><comment author="slife" created="2014-03-11T21:47:43Z" id="37352934">Signed and submitted. Thanks!
</comment><comment author="jpountz" created="2014-03-12T09:57:17Z" id="37391374">Merged via 0236a77c0bbeb9117092a34cb4b6e16acdeef6c4, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Read-only Tribe nodes and Kibana</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5391</link><project id="" key="" /><description>I have multiple ES clusters in different datacenters fed by Logstash and I would like to use one or more tribe nodes in a central location federating across them so I can have a single Kibana instance to search with, (I understand from elasticsearch/kibana#22 how this should work).

Disabling write access seems to make sense here except the problem of the `kibana-int` index which is the only index that needs to be writeable from the point of view of a Kibana user. It would be useful maybe to be able to disable writes globally to the tribes except for that particular tribe/index. At the moment I guess I have to run with writes enabled everywhere?

Although I can't help thinking the write access should possibly be enforced by the remote clusters  rather than on each tribe node...
</description><key id="29199153">5391</key><summary>Read-only Tribe nodes and Kibana</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bodgit</reporter><labels /><created>2014-03-11T17:28:39Z</created><updated>2014-07-08T18:37:19Z</updated><resolved>2014-04-08T09:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bodgit" created="2014-04-08T09:34:41Z" id="39828708">Looks like #5501 goes most of the way to fixing this.
</comment><comment author="ghost" created="2014-07-08T18:37:19Z" id="48381581">bodgit, I am facing a somewhat similar situation and would appreciate your help. I have a Tribe node as a central location with a central Kibana, but I am unable to save dashboards on the Kibana instance. Could you possibly take a look at my issue and tell me how you resolved this?

https://github.com/elasticsearch/kibana/issues/1347
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad geopoint field should throw error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5390</link><project id="" key="" /><description>If a badly formatted object is passed to a geopoint when indexing a doc, it should throw an error. Instead, any fields after the bad geopoint are just ignored:

```
PUT /test
{
  "mappings": {
    "foo": {
      "properties": {
        "loc": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT /test/foo/1
{
  "loc": { "lat": 0, "lon": 0 },
  "tag": "ok"
}

PUT /test/foo/2
{
  "loc": {
    "loc": {
      "lat": 0,
      "lon": 0
    }
  },
  "tag": "not_ok"
}

GET /test/_search?search_type=count
{
  "facets": {
    "tags": {
      "terms": {
        "field": "tag"
      }
    }
  }
}
```

Result:

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0,
      "hits": []
   },
   "facets": {
      "tags": {
         "_type": "terms",
         "missing": 1,
         "total": 1,
         "other": 0,
         "terms": [
            {
               "term": "ok",
               "count": 1
            }
         ]
      }
   }
}
```
</description><key id="29198999">5390</key><summary>Bad geopoint field should throw error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T17:26:58Z</created><updated>2014-08-11T08:27:37Z</updated><resolved>2014-03-19T16:56:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T09:22:16Z" id="37388702">+1 for throwing an error @chilling can you take a look this should be straight forward
</comment><comment author="chilling" created="2014-03-12T09:27:02Z" id="37389063">@s1monw @clintongormley I will fix it
</comment><comment author="hkorte" created="2014-04-02T15:21:44Z" id="39343586">Just in case somebody googles the corresponding exceptions. If you try to index a document containing incomplete or other invalid geo_point fields in Elasticsearch 1.1.0 and you get exceptions like "MapperParsingException[failed to parse]; nested: ElasticsearchParseException[field [lat] missing];" or "MapperParsingException[failed to parse]; nested: ElasticsearchParseException[geo_point expected];", the solution is to skip the whole geo_point field. See this gist: https://gist.github.com/hkorte/9936192
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Blocking writes on a tribe node creates a "blocks" tribe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5389</link><project id="" key="" /><description>From [the configuration](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/modules-tribe.html) a tribe node can block write operations with the following configuration:

``` yaml
tribe:
  blocks:
    write: true
  t1:
    cluster: ...
```

However when I do this, as well as getting the `t1` tribe, I also end up with a `blocks` tribe that fails to find a cluster:

```
$ curl 'localhost:9200/_cluster/state/nodes?pretty'
...
    "rOnYeMT2QmSQAWVeNjCypg" : {
      "name" : "Kohl Harder Boulder Man/blocks",
      "transport_address" : "inet[/10.0.0.1:9302]",
      "attributes" : {
        "tribe.name" : "blocks",
        "client" : "true",
        "data" : "false"
      }
    },
...
```
</description><key id="29187896">5389</key><summary>Blocking writes on a tribe node creates a "blocks" tribe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bodgit</reporter><labels><label>:Tribe Node</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T15:26:52Z</created><updated>2015-06-07T22:39:16Z</updated><resolved>2014-03-12T09:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-11T16:18:19Z" id="37315115">aye, this is a bug, will work on a fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Potential regression in Java client around match query / custom analyzer when going from 0.20.6 to 1.0.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5388</link><project id="" key="" /><description>I recently ported from ES 0.20.6 to 1.0.1 and had a regression in the rather large test suite that I am using. I extracted a Java based junit test (see gist) that succeeds on 0.20.6 and fails on 1.0.1. I was unable to reproduce using curl which makes me think this might be limited to the Java client.

The test uses a custom analyzer (composed of standard filters) and a dynamic mapping template. The custom analyzer performs a special handling for structured numbers that should prevent hits on reordered number groups. This does not seem to be working right in 1.0.1 (seems like the standard analyzer is used regardless of mapping).

gist: https://gist.github.com/jfiedler/9485429
</description><key id="29177234">5388</key><summary>Potential regression in Java client around match query / custom analyzer when going from 0.20.6 to 1.0.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels /><created>2014-03-11T13:19:22Z</created><updated>2014-12-30T14:02:52Z</updated><resolved>2014-12-30T14:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-11T13:52:06Z" id="37297014">Hiya @jfiedler 

It'd be great if you could reduce this to a smaller example, preferably using curl rather than Java (or maybe one of the Java guys will take a look at it).

One thing I noticed: your custom analyzer file is missing the `i` in `index`: https://gist.github.com/jfiedler/9485429#file-custom-analyzer-1-yml-L1
</comment><comment author="clintongormley" created="2014-03-11T13:53:13Z" id="37297148">Also, the syntax of multi-fields has changed in 1.0: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_multi_fields.html
</comment><comment author="jfiedler" created="2014-03-11T13:57:52Z" id="37297639">I fixed the analyzer file. I was not able to reproduce this in curl. I believe this might be a problem of the Java client (which makes it impossible to re-create via curl). I know the multi-field mapping changed but I used the old notation to have the same test run on 0.20.6 and 1.0.1. Using the new notation on 1.0.1 does not make a difference - the test still fails.
</comment><comment author="imotov" created="2014-03-12T03:45:39Z" id="37372954">Here is the [curl repro](https://gist.github.com/imotov/9500276). It looks like the issue is caused by the change in the way query_string query is dealing with tokens with the same position. The generated tokens are the same in both cases (master and 0.20.6). 

``` js
{
    "token": "1111",
    "start_offset": 0,
    "end_offset": 4,
    "type": "word",
    "position": 1
}, {
    "token": "1111",
    "start_offset": 0,
    "end_offset": 4,
    "type": "word",
    "position": 2
}, {
    "token": "2222",
    "start_offset": 5,
    "end_offset": 9,
    "type": "word",
    "position": 3
}, {
    "token": "11112222",
    "start_offset": 0,
    "end_offset": 9,
    "type": "word",
    "position": 3
}
```

But 0.20.6 was generating the following query:
`+myString.text:2222 +myString.text:2222 +myString.text:1111 +myString.text:22221111`

while master is generating:
`+myString.text:2222 +myString.text:2222 +(myString.text:1111 myString.text:22221111)`

So, I would say it might be inappropriate choice of token filter, but it doesn't seem like a regression to me.
</comment><comment author="clintongormley" created="2014-12-30T14:02:52Z" id="68358278">Agreed - this change was added to support query-time synonyms in the same position.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invoke postCollection on aggregation collectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5387</link><project id="" key="" /><description>Also cleanup how facet and aggs collector are used inside the QueryCollector
</description><key id="29170181">5387</key><summary>Invoke postCollection on aggregation collectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-11T11:16:27Z</created><updated>2015-06-07T22:40:31Z</updated><resolved>2014-03-13T10:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-03-13T10:01:09Z" id="37516109">The PercolatorFacetsAndAggregationsTests class with this PR applied works fine on the development branch for aggs where calling Aggregator.postCollection() is critical and was failing previously. Thanks, @martijnvg .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch with aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5386</link><project id="" key="" /><description>Whenever i try to perform an aggregated query on elasticsearch, I always get the error "Parse Failure [No parser for element [aggs]". Heres my request:

http://host:port/fusetransactions/consumers-20140311/_search

&lt;code&gt;
{ 
    "aggs": {
       "routes": {
           "terms": {
                 "field": "route"
             },
         "aggs": {
            "latencies": {
               "terms": {
                    "field": "consumerLatency"
                }  
             }
         }
  }
 }
}
&lt;/code&gt;
And Response:
... nested: SearchParseException[[fusetransactions][1]: from[-1],size[-1]: Parse Failure [No parser for element [aggs]]]; }]","status": 400}

What am I doing wrongly?

Thanks in advance.
</description><key id="29169060">5386</key><summary>Elasticsearch with aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ask4ralph</reporter><labels /><created>2014-03-11T10:56:42Z</created><updated>2014-03-11T11:01:05Z</updated><resolved>2014-03-11T11:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-11T11:01:05Z" id="37283849">Could you please use the mailing list for questions and provide more details there?

See: http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include debian repository information on download page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5385</link><project id="" key="" /><description>Installing from the repository will be the preferred method of installation for most Debian/Ubuntu users, but there's no mention of it on the download page. It would be nice to either include directly or link to the repo details. Also helpful would be to include the public key id in this information.
</description><key id="29158505">5385</key><summary>Include debian repository information on download page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2014-03-11T07:23:54Z</created><updated>2014-12-30T14:01:43Z</updated><resolved>2014-12-30T14:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:01:43Z" id="68358205">Done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Fix minor error in cluster stats example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5384</link><project id="" key="" /><description>`?human` doesn't pretty print for me (Elasticsearch 1.0.1)
</description><key id="29143707">5384</key><summary>[DOCS] Fix minor error in cluster stats example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dentarg</reporter><labels><label>docs</label></labels><created>2014-03-10T23:59:25Z</created><updated>2014-06-27T02:34:08Z</updated><resolved>2014-06-03T11:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-13T08:18:11Z" id="37508807">the `human` parameter needs to remain though in order to show all the values (bytes and times) in a human readable format as well, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#_human_readable_output

do you mind to update the PR and sign the CLA at http://www.elasticsearch.org/contributor-agreement/

thanks a lot!
</comment><comment author="dentarg" created="2014-03-13T23:27:23Z" id="37599681">Oh, right! Sorry that I missed that, I've added `human` back.

I signed the CLA before submitting the pull request.
</comment><comment author="javanna" created="2014-06-03T11:39:24Z" id="44953478">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add break iterator based segmenter for FVH</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5383</link><project id="" key="" /><description>This segmenter spits out very pretty snippets.

Closes #5382
</description><key id="29136203">5383</key><summary>Add break iterator based segmenter for FVH</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-10T21:52:30Z</created><updated>2014-07-03T19:21:43Z</updated><resolved>2014-03-31T17:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:25:08Z" id="39115949">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a segmenter to the FVH using BreakIterator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5382</link><project id="" key="" /><description>It'd be useful to have a segmenter that spits out full sentences (if possible) using the BreakIterator.  BreakIterator is nice here because it should be reasonably quick and it spits out very pretty results.
</description><key id="29136125">5382</key><summary>Add a segmenter to the FVH using BreakIterator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-10T21:51:24Z</created><updated>2014-03-31T17:25:04Z</updated><resolved>2014-03-31T17:25:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:25:04Z" id="39115939">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting is returning results not searched for</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5381</link><project id="" key="" /><description>here is my test case

index : 

``` JSON
PUT /test/1/1' -d 
{
"ZIPCODE": [
                  "92121",
                  "92120-2001",
                  "92104-3426",
                  "92116-4153",
                  "92101",
                  "92101-2892",
                  "60631-2590"
               ],
               "STREET3": " ",
               "FIRST_NAME": "James",
               "MIDDLE_NAME": "Wesley",
               "STATE_CODE": [
                  "CA",
                  "IL"
               ],
               "CITY": [
                  "San Diego",
                  "Chicago"
               ],
               "PREF_MAIL_NAME": "James W. Smith",
               "EMAIL_ADDRESS": [
                  "james.w.smith@cardinal.com",
                  "jsmith@eraeagle.com"
               ],
               "LAST_NAME": "Smith",
               "STREET2": [
                  " ",
                  "4434 Louisiana Street",
                  "850 Beech Street",
                  "6060 Northwest Highway"
               ],
               "PREF_CLASS_YEAR": "1993",
               "STREET1": [
                  "10020 Pacific Mesa Boulevard",
                  "7533 Clear Sky Rd",
                  "3584-A Villa Terrace",
                  "Apartment 8",
                  "Apartment 315",
                  "SAFCO Corporation"
               ]
               }
```

Search (notice the EMAIL_ADDRESS criteria)

``` JSON
POST /test/1/_search
{
   "query": {
      "bool": {
         "should": [
            {
               "match_phrase_prefix": {
                  "FIRST_NAME": {
                     "query": "James"
                  }
               }
            },
            {
               "match_phrase_prefix": {
                  "LAST_NAME": "Smith"
               }
            },
            {
               "match_phrase_prefix": {
                  "MIDDLE_NAME": ""
               }
            },
            {
               "match": {
                  "PREF_CLASS_YEAR": ""
               }
            },
            {
               "match": {
                  "EMAIL_ADDRESS": "jones@.stackoverflow.com"
               }
            },
            {
               "multi_match": {
                  "query": "10020 Pacific Mesa",
                  "fields": [
                     "STREET1",
                     "STREET2",
                     "STREET3"
                  ]
               }
            },
            {
               "match": {
                  "CITY": "San Diego"
               }
            },
            {
               "match": {
                  "STATE_CODE": "CA"
               }
            },
            {
               "match": {
                  "ZIPCODE": "92121"
               }
            }
         ],
         "minimum_should_match": "3"
      }
   },
   "from": "0",
   "highlight": {
      "pre_tags": "&lt;b&gt;",
      "post_tags": "&lt;/b&gt;",
      "fields": {
         "ZIPCODE": {
            "number_of_fragments": "5"
         },
         "STREET3": {
            "number_of_fragments": "5"
         },
         "FIRST_NAME": {
            "number_of_fragments": "5"
         },
         "MIDDLE_NAME": {
            "number_of_fragments": "5"
         },
         "LAST_NAME": {
            "number_of_fragments": "5"
         },
         "STREET1": {
            "number_of_fragments": "5"
         },         
         "STATE_CODE": {
            "number_of_fragments": "5"
         },
         "CITY": {
            "number_of_fragments": "5"
         },
         "STREET2": {
            "number_of_fragments": "5"
         },
         "EMAIL_ADDRESS": {
            "number_of_fragments": "5"
         },
         "PREF_CLASS_YEAR": {
            "number_of_fragments": "5"
         }

      }
   },
   "size": "5"
}  
```

The highlight result is clearly showing a value for EMAIL_ADDRESS that i did not search for, it does not even show on the explain plan

``` JSON
"highlight": {
               "ZIPCODE": [
                  "&lt;em&gt;92121&lt;/em&gt;"
               ],
               "FIRST_NAME": [
                  "&lt;em&gt;James&lt;/em&gt;"
               ],
               "STATE_CODE": [
                  "&lt;em&gt;CA&lt;/em&gt;"
               ],
               "CITY": [
                  "&lt;em&gt;San&lt;/em&gt; &lt;em&gt;Diego&lt;/em&gt;"
               ],
               "EMAIL_ADDRESS": [
                  "&lt;em&gt;james.w.smith&lt;/em&gt;@cardinal.com"
               ],
               "LAST_NAME": [
                  "&lt;em&gt;Smith&lt;/em&gt;"
               ],
               "STREET1": [
                  "&lt;em&gt;10020&lt;/em&gt; &lt;em&gt;Pacific&lt;/em&gt; &lt;em&gt;Mesa&lt;/em&gt; Boulevard"
               ]
            }
```

*update: by changing my match_phrase_prefix queries to match, the highlighting of erroneous email_address goes away...but that still makes no sense
</description><key id="29119100">5381</key><summary>Highlighting is returning results not searched for</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jrizzi1</reporter><labels><label>:Highlighting</label></labels><created>2014-03-10T18:18:37Z</created><updated>2015-04-16T09:14:37Z</updated><resolved>2015-04-16T09:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T12:22:17Z" id="48462590">By default, highlighting will try to match any of the searched for fragments.  You can use [`require_field_match`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#field-match) to only highlight a field if it matched the query
</comment><comment author="javanna" created="2015-04-16T09:14:37Z" id="93687198">I think we can close this, require_field_match should solve it. Also, in #10627 we are discussing whether we should change the default for it to be `true`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need to be able to handle BigDecimal in a lossless manner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5380</link><project id="" key="" /><description>ie if a document is created with a BigDecimal value - it should be identical (in value, scale, etc) when retrieved (from _source).
</description><key id="29115099">5380</key><summary>Need to be able to handle BigDecimal in a lossless manner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-03-10T17:31:34Z</created><updated>2014-12-30T14:00:53Z</updated><resolved>2014-12-30T14:00:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:00:53Z" id="68358155">Closing issue in favour of PR #5491
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a bug in date_histogram aggregation parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5379</link><project id="" key="" /><description>- pre_zone_adjust_large_interval was not parsed properly
- added tests for pre_zone and pre_zone_adjust_large_interval
- changed DateHistogram#getBucketByKey(String) to support date formats (next to numeric strings)
- added randomized testing for fetching the bucket by key in date_histogram tests
- added missing "format" support for the DateHistogramBuilder
  
  Closes #5375
</description><key id="29113711">5379</key><summary>Fixed a bug in date_histogram aggregation parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-10T17:15:03Z</created><updated>2015-06-07T22:40:37Z</updated><resolved>2014-03-10T19:06:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-10T17:32:39Z" id="37210120">+1
</comment><comment author="uboness" created="2014-03-10T19:06:19Z" id="37221359">closed by bf8d8dc33e994fdcc903ea0512af1084cfc4dac5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a char_filter cutting the field into sentences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5378</link><project id="" key="" /><description>Adds a char_filter that cuts the field down to the configured number of
sentences.  Defaults to just the first sentence.

There is some impedence between the incoming Reader and the CharacterIterator
that has to be passed to the BreakIterator.  We copy a configurable number
of characters into a character array and shove those into the BreakIterator
via Lucene's CharArrayIterator rather than try anything fancy.  Hopefully
this is fine from a performance perspective.

Closes #5377
</description><key id="29098582">5378</key><summary>Add a char_filter cutting the field into sentences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Analysis</label><label>adoptme</label><label>feature</label></labels><created>2014-03-10T14:22:41Z</created><updated>2016-03-08T13:06:59Z</updated><resolved>2016-03-08T13:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T15:03:06Z" id="48741474">Why did you close this one? I'd just marked it for review :)
</comment><comment author="nik9000" created="2014-07-11T15:09:55Z" id="48742346">Mostly because I'd forgotten about it and only say it again when I was going through my old pull requests!  I'm not sure I need it/can use it any more.  But I'm happy to get it finished.
</comment><comment author="rmuir" created="2014-07-15T13:24:11Z" id="49030448">I am glad you used the helper method from lucene to dodge jvm breakiterator bugs :) I think just specifying a limit is fine as well.

As a side note, an alternative to this approach if you just want to implement a tokenizer that processes sentence-at-a-time (e.g. word segmenter) is to extend http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/SegmentingTokenizerBase.java

But this appears to have a different use case, since its trying to bound the text to some number of sentences?

Is it possible to add a test that uses BaseTokenStreamTestCase.checkRandomData with this thing? its especially good at finding bugs, and will also test that the charfilter correctly plays well with other charfilters in the chain and so on.
</comment><comment author="clintongormley" created="2014-08-07T18:27:06Z" id="51511917">Hi @nik9000 I'm sure you have lots on your plate.  Are you likely to work on this at some stage, or should i look for somebody to take it over?
</comment><comment author="nik9000" created="2014-09-04T22:56:40Z" id="54557254">Oh man, I didn't realize you'd commented on this until just now.  I can take a look at it soon-ish.  A couple days?
</comment><comment author="clintongormley" created="2014-11-11T18:49:17Z" id="62596164">@nik9000 your monthly reminder :)
</comment><comment author="nik9000" created="2014-11-12T03:43:59Z" id="62666092">Ah yes well, er, I'll look at it when I get a chance!  If someone wants to
pick it up and run with it please consider it un-cookie-licked.
On Nov 11, 2014 1:49 PM, "Clinton Gormley" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 your monthly reminder :)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/5378#issuecomment-62596164
&gt; .
</comment><comment author="clintongormley" created="2016-03-08T13:06:59Z" id="193778586">This  PR is very out of date. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It'd be convenient to have a char_filter that grabs only the first few sentences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5377</link><project id="" key="" /><description>It'd be convenient to have a char_filter that grabs only the first few sentences.  Something like:

``` js
    "analysis": {
      "char_filter": {
        "first_sentence": {
          "type": "sentence",
          "locale": "en",
          "analyzed_chars": 1024
        }
      }
    }
```
</description><key id="29098283">5377</key><summary>It'd be convenient to have a char_filter that grabs only the first few sentences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-10T14:18:52Z</created><updated>2014-12-30T14:00:39Z</updated><resolved>2014-12-30T14:00:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T14:00:38Z" id="68358143">Closing issue in favour of PR #5378 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using BreakIterator inside of MVEL doesn't work the second time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5376</link><project id="" key="" /><description>I wanted to run some statistical facets on sentence length of indexed documents and I tried to use BeakIterator in MVEL and that didn't go well:

``` js
{
   "_source": false,
   "facets" : {
        "stat1" : {
            "statistical" : {
                "script" : "bi = java.text.BreakIterator.getSentenceInstance(); bi.setText(_source['text']); bi.first(); bi.next()"
            }
        }
    },
    "size": 0
}
```

Produces IllegalAccessExceptions on java.text.RuleBaseBreakIterator.  I'm not really sure what to do about it other than save the issue for posterity and provide a work around:

``` js
{
   "_source": false,
   "facets" : {
        "stat1" : {
            "statistical" : {
                "script" : "breakIterator = Class.forName('java.text.BreakIterator'); setText = breakIterator.getMethod('setText', Class.forName('java.lang.String')); first = breakIterator.getMethod('first'); next=breakIterator.getMethod('next'); bi = java.text.BreakIterator.getSentenceInstance(); setText.invoke(bi, _source['text']); first.invoke(bi); next.invoke(bi)"
            }
        }
    },
    "size": 0
}
```
</description><key id="29097307">5376</key><summary>Using BreakIterator inside of MVEL doesn't work the second time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-10T14:06:19Z</created><updated>2014-08-22T10:17:14Z</updated><resolved>2014-08-22T10:17:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-11T01:27:41Z" id="37254553">There is a simpler workaround:

``` js
{
   "_source": false,
   "facets" : {
        "stat1" : {
            "statistical" : {
                "script" : "java.text.BreakIterator bi = java.text.BreakIterator.getSentenceInstance(); bi.setText(_source['text']); bi.first(); bi.next()"
            }
        }
    },
    "size": 0
}
```
</comment><comment author="nik9000" created="2014-03-11T01:30:12Z" id="37254689">That is a better work around.  
</comment><comment author="imotov" created="2014-03-12T04:25:06Z" id="37374430">Just a quick note, I tried to reproduce this issue again today to investigate it further, but it no longer reproducible in my setup. Your original version works just fine. I suspect it might be some sort of a race condition in MVEL.
</comment><comment author="nik9000" created="2014-03-12T10:27:07Z" id="37393683">Iirc the error popped up on subsequent executions. Like the first time was ok, the second time some shards failed, the third time all failed. I'll see if I can put together a gist. 

Sent from my iPhone

&gt; On Mar 12, 2014, at 12:25 AM, Igor Motov notifications@github.com wrote:
&gt; 
&gt; Just a quick note, I tried to reproduce this issue again today to investigate it further, but it no longer reproducible in my setup. Your original version works just fine. I suspect it might be some sort of a race condition in MVEL.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2014-08-22T10:17:14Z" id="53045018">Given Mvel is deprecated, I'm going to close this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_histogram aggregation, pre_zone_adjust_large_interval attribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5375</link><project id="" key="" /><description>The pre_zone_adjust_large_interval attribute is only parsed when the value is a string.
`pre_zone_adjust_large_interval: true` 
gives the following error 
`Parse Failure [Unknown key for a VALUE_BOOLEAN in [aggregation_name]: [pre_zone_adjust_large_interval]`

Version: 1.0.0.
</description><key id="29092438">5375</key><summary>date_histogram aggregation, pre_zone_adjust_large_interval attribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rocambolesque</reporter><labels /><created>2014-03-10T12:54:58Z</created><updated>2014-09-17T17:16:40Z</updated><resolved>2014-03-10T18:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-10T13:31:37Z" id="37181765">indeed a bug... will fix, thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow edit distances &gt; 2 on FuzzyLikeThisQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5374</link><project id="" key="" /><description>Due to a regression edit distances &gt; 2 threw exceptions after unifying
the fuzziness factor in Elasticsearch `1.0`. This commit brings back the
expceted behavior.

Closes #5292
</description><key id="29080590">5374</key><summary>Allow edit distances &gt; 2 on FuzzyLikeThisQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.0.2</label><label>v1.1.1</label><label>v2.0.0-beta1</label></labels><created>2014-03-10T09:12:44Z</created><updated>2015-06-07T22:53:56Z</updated><resolved>2014-03-13T14:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-13T08:19:53Z" id="37508904">This looks good to me when using the flt query via REST but when using the Java client, I think it is surprising that `Fuzziness.fromEdits(4)` would fail while `Fuzziness.build("4")` would work?
</comment><comment author="s1monw" created="2014-03-13T08:22:23Z" id="37509035">&gt; &gt; This looks good to me when using the flt query via REST but when using the Java client, I think it is surprising that Fuzziness.fromEdits(4) would fail while Fuzziness.build("4") would work?

well if you specify a string it's up to the interpretation if you specify an int you know it's an edit distance. I don't want to loosen the interface just because we have this one exception that still uses the slow fuzzy query.
</comment><comment author="s1monw" created="2014-03-13T08:22:46Z" id="37509060">@jpountz I pushed a new commit regarding the RandomPicks
</comment><comment author="jpountz" created="2014-03-13T08:34:52Z" id="37509801">Should we fix master (2.0) in order not to allow edit distances that are greater than 2 for the FLT query?
</comment><comment author="jpountz" created="2014-03-13T08:35:07Z" id="37509826">+1 to push
</comment><comment author="s1monw" created="2014-03-13T09:18:00Z" id="37512740">I think FLT should move to a plugin - this one is crazy and I'd love to deprecate it....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should support pipeline search command like splunk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5373</link><project id="" key="" /><description>splunk is a log analysis software(http://www.splunk.com/)(http://docs.splunk.com/Documentation/Splunk/latest/SearchReference/WhatsInThisManual)

It support some kind of search command like as below:

Example 1: Return the 20 most common values of the "referer" field.
sourcetype=access_\*   |  top limit=20 referer

Example 2:Create a chart to show the average number of events in a transaction based on the duration of the transaction

sourcetype=access_\* status=200 action=purchase | transaction clientip maxspan=30m | chart avg(eventcount) by duration span=log2

Pipeline char "|" can allow us to compose a complex a rule for analysis straightforwardly!!! It's very important!

Consider it please!

labels:enhancement
</description><key id="29079941">5373</key><summary>Should support pipeline search command like splunk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huangchen007</reporter><labels /><created>2014-03-10T08:58:29Z</created><updated>2015-10-14T15:07:14Z</updated><resolved>2015-10-14T15:07:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rphadake" created="2014-12-02T13:08:30Z" id="65227942"> "+ 1 "

Searching for adding custom analysis within ES code which can work across shard with query_fetch mode semantics.  
Solr has some experimental code which allows adding custom code. https://cwiki.apache.org/confluence/display/solr/AnalyticsQuery+API
Es does have postFilter which can be useful here?

.
</comment><comment author="clintongormley" created="2015-10-14T15:07:14Z" id="148078768">Elasticsearch is a very different beast. This syntax is targeted at stream processing of raw data, while Elasticsearch gains its speed advantage by indexing everything up front.  While I could imagine somebody building something like this into a plugin or application based on elasticsearch, I don't think the syntax is right for inclusion in core.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disabled query size estimation in percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5372</link><project id="" key="" /><description>PR for #5339
</description><key id="29073290">5372</key><summary>Disabled query size estimation in percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Stats</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-10T05:17:53Z</created><updated>2015-06-07T22:41:06Z</updated><resolved>2014-03-14T08:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T19:47:39Z" id="37455204">this one LGTM - sad that it's so CPU intensive
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error detection in geo_filter parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5371</link><project id="" key="" /><description>Relates to #5370
</description><key id="29038886">5371</key><summary>Improve error detection in geo_filter parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-09T07:59:00Z</created><updated>2015-06-07T15:06:05Z</updated><resolved>2014-03-10T11:27:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-10T07:47:49Z" id="37159129">Can we have a test as well for the improvement that you added? ;)
</comment><comment author="s1monw" created="2014-03-10T08:22:33Z" id="37160768">+1 for the change +1 for a test :)
</comment><comment author="bleskes" created="2014-03-10T11:27:08Z" id="37172841">Added a test and pushed to 1.x (https://github.com/elasticsearch/elasticsearch/commit/3f446c3cbf7db9788d73bafe272a35bee66f6da6) &amp; master (https://github.com/elasticsearch/elasticsearch/commit/bb63b3fa61f749c634ac4dccdf5cda8a0cca9b9d)

thx for the feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Size parameter is ignored for filtered query that includes geo_polygon filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5370</link><project id="" key="" /><description>Query

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "geo_polygon": {
          "coordinate": {
            "points": {
              "points": [
                [
                  -84.293222919922,
                  33.865223592668
                ],
                [
                  -84.293222919922,
                  33.632776407332
                ],
                [
                  -84.482737080078,
                  33.632776407332
                ],
                [
                  -84.482737080078,
                  33.865223592668
                ],
                [
                  -84.293222919922,
                  33.865223592668
                ]
              ]
            }
          }
        }
      }
    }
  },
  "size": 2
}
```

always returns 10 results and does not consider "size" parameter.

To compare:

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "match_all": {}
      }
    }
  },
  "size": "2"
}
```

returns 2 results. 

MacOS, elastic search version: 1.0.1 (tested with 0.18), lucene_version: 4.6
</description><key id="29034727">5370</key><summary>Size parameter is ignored for filtered query that includes geo_polygon filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wizardz</reporter><labels /><created>2014-03-09T01:43:17Z</created><updated>2014-03-09T08:00:27Z</updated><resolved>2014-03-09T08:00:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-09T08:00:27Z" id="37121532">you have an extra `points` wrapper at 

```
  "coordinate": {
            "points": {
              "points": [
```

If you only have the array one it should good. I've going to improve the error reporting in this case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Excessive redundancy suggestion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5369</link><project id="" key="" /><description>Hi,

there's a case when there seems to be a place for improvement in elasticsearch. Problem is, if node dropping off the cluster is something that happens rather often, rebalancing the cluster to keep replicas in sync with index.number_of_replicas setting (especially if node gets back rather soon) produces a significant load.

There seem to be a rather simple solution for this though:
- have a higher and lower bounds for index.number_of_replicas - let me call them index.min_number_of_replicas and index.max_number_of_replicas here. By default they might be omitted in config and both equal index.number_of_replicas unless redefined explicitly
- replicate any shard only if number of its replicas is lower than index.min_number_of_replicas
- drop excessive shard replicas only if replicas count is over index.max_number_of_replicas
- new shards should be replicated to index.min_number_of_replicas
- replication should pump replica count up to index.max_number_of_replicas off-peak. There are two quick initial solutions for this: 
  a) it could be either customer's responsibility to change index.min_number_of_replicas setting to match index.max_number_of_replicas in off-peak hours and drop down when high load is expected. 
  b) it could be a command-line tool or parameter or any other directive to "replicate up to the limit"
  Eventually there could be another setting like index.off_peak_range to do that automatically, but even without it the change would be of a significant help.

Generally this would let the cluster to have some extra resources to spare for query needs rather than shards redistribution.

Apologies if this doesn't actually suit the architecture or was discussed before or else.

Cheers.
</description><key id="29023111">5369</key><summary>Excessive redundancy suggestion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hydralien</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2014-03-08T15:04:41Z</created><updated>2015-04-10T17:06:13Z</updated><resolved>2015-04-10T17:06:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T13:54:04Z" id="68357736">Related to #7288
</comment><comment author="dakrone" created="2015-04-10T17:06:13Z" id="91623074">We discussed this and this is really outside the scope of Elasticsearch itself, this is something that could be better implemented either as a plugin (listening to `clusterChanged` events and dynamically increasing/decreasing the replica count for an index) or as a client-side application that handled this. Additionally things like determining off-peak hours would be better suited to a process other than Elasticsearch itself.

Closing this for now, let me know if we have misunderstood something about it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support include_defaults in the GET-mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5368</link><project id="" key="" /><description>The get-field-mapping API only works for core field types, not for fields of type `object` or `nested`:

```
curl -XPUT "http://localhost:9200/test" -d'
{
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "object",
          "properties": {
            "bar": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}'
```

Retrieving the mapping for core fields works:

```
curl -XGET "http://localhost:9200/test/_mapping/test/field/foo.bar"
```

Result:

```
{
   "test": {
      "mappings": {
         "test": {
            "foo.bar": {
               "full_name": "foo.bar",
               "mapping": {
                  "bar": {
                     "type": "string"
                  }
               }
            }
         }
      }
   }
}
```

But not for fields of type `object` or `nested`:

```
curl -XGET "http://localhost:9200/test/_mapping/test/field/foo"
```

Result:

```
{}
```
</description><key id="29021930">5368</key><summary>Support include_defaults in the GET-mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2014-03-08T13:58:13Z</created><updated>2016-11-24T18:00:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-10T08:17:17Z" id="37160502">This tricky as the get field mappings api also supports wild cards  ( https://github.com/elasticsearch/elasticsearch/issues/4367 )  which people use to get a list of all searchable fields. This is handy for suggestions of field names (i.e., a kibana panel facet configuration).

Also - if you specify `curl -XGET "http://localhost:9200/index/type/_mapping/field/*"` it's ambiguous what you'd get - object fields only ? object fields and leaf fields unpacked?

we could potentially add flags for this but I'm not sure it's worth it. Perhaps a documentation issue or find better url structure (using a clearer word than fields)?
</comment><comment author="clintongormley" created="2014-03-10T10:47:19Z" id="37170229">The particular use case I had was wanting to see if `include_in_parent|root` were being defaulted to `false`.  There is no way of seeing this currently.

For wildcards, I'd say you include the object plus all of its properties.

&gt; This tricky as the get field mappings api also supports wild cards ( #4367 ) which people use to get a list of all searchable fields. This is handy for suggestions of field names (i.e., a kibana panel facet configuration).

Perhaps just a query string flag to distinguish between searchable fields and all fields, eg:

```
GET /index/_mapping/type/field/*?searchable
```
</comment><comment author="Mpdreamz" created="2014-08-12T08:46:31Z" id="51888257">I have to agree with @clintongormley it would be great if it did support `object|nested` this endpoint is great to zoom in whether a field was indeed properly mapped without the surrounding noise the regular `_mapping` endpoint gives you. 

When I first read the documentation I did not equate `field` with `leaf fields`.

```
GET /index/_mapping/type/field/*?leafs
```

or since the default is already to only show leafs:

```
GET /index/_mapping/type/field/*?include_object
```

Since wildcards already flatten the result it would be great if the mapping for objects excludes its own `properties`.
</comment><comment author="clintongormley" created="2014-10-17T10:24:43Z" id="59494541">I think the main thing that is missing here is support for `include_defaults` on the main GET-mapping API.   While getting a single object mapping to check that it is mapped correctly would be nice, it's a one-off debugging request, and is not as important as the current function of the GET-field-mapping: returning all searchable fields as a flat list.

So I'd be happy with just adding support for include_defaults to GET-mapping
</comment><comment author="Mpdreamz" created="2014-10-17T11:02:31Z" id="59497821">If that toggles including `object` and `nested` fields as well +1
</comment><comment author="clintongormley" created="2014-10-17T12:14:32Z" id="59503979">@Mpdreamz the change i'm suggesting is to the main get-mapping API, not the get-field-mapping API. So my request doesn't allow you to retrieve a single object field, you still have to retrieve the old mapping.  However, you can get back the mapping with the defaults included.
</comment><comment author="bleskes" created="2014-10-17T16:13:17Z" id="59536702">I'm +1 on adding an include default + filtering options (including `indexed`, which is i think is better than `searchable` because it's the name of the setting, `nested` or what ever) . Once we have that, we can depreciate the get field mappings API imho.

Caveat - The get field mapping api was added and implemented in the current form (i.e., requiring at least one assigned shard) because the defaults are currently baked in the the field mapper in memory objects. To so on the master and based on the serialized representation of the _mapping, we need to have a better grip on the defaults (or if we just choose to instantiate these objects with every request). 
</comment><comment author="csezheng" created="2015-07-02T01:04:16Z" id="117866227">+1 to add the include_defaults option to get mapping API, and also makes it work for the object type field. 
</comment><comment author="csezheng" created="2015-07-02T16:33:55Z" id="118087690">If this is not gonna be fixed soon, is there any alternative way to get the default values for the object/nested type fields? Thanks. 
</comment><comment author="jpountz" created="2015-11-13T10:44:16Z" id="156394483">We just discussed it in Fixit Friday and are +1 on @bleskes 's plan. Making it an adoptme as well as a high-hanging fruit because of the described caveats...
</comment><comment author="clintongormley" created="2016-11-11T10:16:40Z" id="259926180">Rediscussed in FixItFriday - would be good to:
- remove the field mapping API
- add support for include_defaults to the mapping API
- users can use response filtering for a particular field 
- perhaps we can support `flat_paths` to return fields like `foo.bar.baz` instead of using JSON objects

@rjernst is the following still true after the mapping refactoring you did?

&gt; Caveat - The get field mapping api was added and implemented in the current form (i.e., requiring at least one assigned shard) because the defaults are currently baked in the the field mapper in memory objects. To so on the master and based on the serialized representation of the _mapping, we need to have a better grip on the defaults (or if we just choose to instantiate these objects with every request).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>POST data should be considered for parameter data, not just query params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5367</link><project id="" key="" /><description>Many modifying operations call for POSTing to a URL and user query parameters to supply arguments for those operations.  Relying solely on query parameters imposes an unnecessary restriction on the number and length of the parameters that may be used (due to the restriction of the length of the URI).  POST data has a far greater allowable content size.

Consider the case of clearing index cache keys.  Your documentation suggests something akin to:

```
curl -XPOST 'localhost:9200/tweets/_cache/clear?filter_keys=user_2_friends'
```

If one wanted to clear many filter keys at once, they would necessarily have to submit multiple requests.  What's worse is that the caller has to ensure their URI does not exceed the maximum length and conditionally break up their calls into multiple chunks.

If the POST data was also checked for the parameters, then the user could essentially clear as many keys as they wanted while only making one HTTP request.
</description><key id="29002109">5367</key><summary>POST data should be considered for parameter data, not just query params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coryfoo</reporter><labels /><created>2014-03-07T22:00:15Z</created><updated>2015-04-29T13:22:11Z</updated><resolved>2014-12-29T14:28:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-13T08:21:23Z" id="37508990">out of curiosity, where did you hit that problem with the URI length? With the exception of browsers, any other HTTP client should not impose a limit on the length of the URI - I guess that is not the case then?
</comment><comment author="clintongormley" created="2014-12-29T14:28:59Z" id="68261411">Actually, you can already pass the `filter_keys` as part of the POST body to clear-cache.
</comment><comment author="a0s" created="2015-04-29T13:15:48Z" id="97422324">+1 for this question! My elasticsearch stay behind the proxy (that i can't control) with 4k hard limit for URI size. And i have MANY indices. How i can pass list of indices (for example,  elastic:9200/index1,index2...very_loooong_list/type/_search) through POST body ?
</comment><comment author="clintongormley" created="2015-04-29T13:22:11Z" id="97423484">@orangeudav you can use a multi-search request, which allows you to specify the indices in the body.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add configured thread pool sizes to _cat/thread_pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5366</link><project id="" key="" /><description>`_cat/thread_pool` has a `*.size` column for each thread pool.  That maps to `ThreadPoolStats.getThreads()`, which, if there hasn't been any activity in the thread pool, makes the number misleading for checking your settings.

```
% curl localhost:9200/_cat/thread_pool\?h=h,i,bulk.size\&amp;v
h          i            bulk.size
iota.local 192.168.1.68         0
```

We should add `*.max` and `*.min` columns mapped respectively to `ThreadPool$Info.getMax()` and `ThreadPool$Info.getMin()` for each pool.
</description><key id="28990218">5366</key><summary>Add configured thread pool sizes to _cat/thread_pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-07T19:09:15Z</created><updated>2015-06-07T15:06:11Z</updated><resolved>2014-04-28T10:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T19:48:32Z" id="37455325">@drewr are working on this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Multiple rescores doesn't have a coming flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5365</link><project id="" key="" /><description>Multiple rescores (#4748) didn't make it into 1.0.1 even though it is marked as being there on the issue.  We should mark it as coming in 1.1 in the docs.  Also, I'm sad because I was excited to use it finally.
</description><key id="28989852">5365</key><summary>[docs] Multiple rescores doesn't have a coming flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-07T19:04:08Z</created><updated>2014-03-10T13:22:04Z</updated><resolved>2014-03-10T08:28:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-10T08:29:59Z" id="37161192">@nik9000 `1.0.x` will not contain features only bugfixes. IMO that is important to not de-stabelize those branches with new features so we are always able to cut bugfix releases (at any time)
</comment><comment author="nik9000" created="2014-03-10T12:33:31Z" id="37177256">I don't propose adding it to 1.0.X, just express my regret that we didn't merge it there even though the feature was marked with the v1.0.0RC2 label.
</comment><comment author="s1monw" created="2014-03-10T13:14:40Z" id="37180259">yeah @nik9000 I just wanna bring across that it would not have made it there anyways since that was a bugfix release :) ie. it was tagged wrongly
</comment><comment author="nik9000" created="2014-03-10T13:22:04Z" id="37180891">Got it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"include_in_all": false not working for nested multi-field mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5364</link><project id="" key="" /><description>I'm running Elasticsearch 1.0.1 using the following $ES_HOME/config/default-mapping.json:

```
{
  "_default_": {
    "properties": {
      "foo": {
        "type": "nested",
        "include_in_all": false,
        "properties": {
          "bar": {
            "type": "string",
            "index": "not_analyzed",
            "include_in_all": false,
            "fields": {
              "lower": {
                "analyzer": "standard",
                "type": "string"
              }
            }
          }
        }
      }
    }
  }
}
```

When I post the following, the include_in_all does not seem to be recognized at any level:

```
 {
   "foo": {
     "bar": "Elasticsearch rules!"
   }
 }
```

The resulting metadata is the following:

```
{
    state: open
    settings: {
        index: {
            uuid: S2hmo2d3SGWFP51PXs6XbA
            number_of_replicas: 0
            number_of_shards: 1
            version: {
                created: 1000199
            }
        }
    }
    mappings: {
        foobar: {
            properties: {
               foo: {
                   include_in_all: false
                    properties: {
                        bar: {
                            include_in_all: false
                            index: not_analyzed
                            type: string
                            fields: {
                                lower: {
                                    analyzer: standard
                                    type: string
                                }
                            }
                        }
                    }
                    type: nested
                }
            }
        }
    }
    aliases: [ ]
}
```

I verified that _all does include the "elasticsearch rules" via the following aggregation search:

```
{
  "aggs": {
    "foobar": {
      "terms": {
        "field": "_all"
      }
    }
  },
  "size": 0
}
```

The aggs search result is the following:

```
{
    took: 36
    timed_out: false
    _shards: {
        total: 1
        successful: 1
        failed: 0
    }
    hits: {
        total: 1
        max_score: 0
        hits: [ ]
    }
    aggregations: {
        foo: {
            buckets: [
                {
                    key: elasticsearch
                    doc_count: 1
                }
                {
                    key: rule
                    doc_count: 1
                }
            ]
        }
    }
}
```

So, _all should be empty, but it is not.
</description><key id="28983608">5364</key><summary>"include_in_all": false not working for nested multi-field mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">asanderson</reporter><labels /><created>2014-03-07T17:36:21Z</created><updated>2014-03-26T07:39:53Z</updated><resolved>2014-03-26T07:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-08T13:53:24Z" id="37098037">Confirmed that this is a problem.  I also tried setting `include_in_root` and `include_in_parent` to `false`, but the values still end up in `_all`.
</comment><comment author="martijnvg" created="2014-03-26T07:39:53Z" id="38656750">This has been fixed via PR #5522 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms filter returning wrong results, maybe cache issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5363</link><project id="" key="" /><description>I'm seeing a strange behavior using elasticsearch 0.90.12.

The index contains 180269 docs

I try to perform the following query:

```
{'query':
        {'filtered':
                {'filter': {
                        'terms': {'id': [188915, 189067, 183817, 188969, 188425]}
                },
                'query': {'match_all': {}}
                }
        }
}
```

It returns more or less randomly the following list of doc ids: 

[183817, 188915, 188969, 189067, 188231]
or
[183817, 188915, 188969, 189067, 188425]

As you can see the first return value is entirely unexpected. If i add "_cache: false" to the terms filter it consistently returns the correct value. It also works as expected when i choose a bool execution strategy. It also work if i slightly change the list of requested ids (remove one, add one), only that specific list triggers the problem.

I can't reduce the problem locally, it seems to depend on the exact state of that index and that exact lis of ID.

Could it be some kind of cache collision because of the hashing algorithm used to generate the cache key, or the bitset ?

In any case this could lead to data leak since i can't trust that my filter will work as expected.. Thanks for any advice on how to debug this properly!
</description><key id="28958206">5363</key><summary>terms filter returning wrong results, maybe cache issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">rslinckx</reporter><labels><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-07T11:13:27Z</created><updated>2014-03-12T01:43:15Z</updated><resolved>2014-03-12T01:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-07T19:52:41Z" id="37061087">@rslinckx Thank you for the report. I was able to reproduce the issue locally. It looks like there is a [bug](https://issues.apache.org/jira/browse/LUCENE-5502) in the method that compares two TermsFilters. As a result of this bug, it's possible to get cached results for a different terms filter in some rare circumstances. The only workaround that I can think of is to use your own cache_key with all your terms filters until the issue is fixed. 
</comment><comment author="s1monw" created="2014-03-10T08:25:21Z" id="37160933">argh! @imotov we should add tests for hashcode / equals for all our filters.
</comment><comment author="rslinckx" created="2014-03-10T09:32:58Z" id="37165088">I'm glad you were able to find the issue, it's been haunting me for the past few days :)

Do you have any kind of timeframe when a fix can be pushed in elasticsearch ?
</comment><comment author="s1monw" created="2014-03-10T10:31:42Z" id="37169167">@rslinckx usually we port the fixes we have on lucene quickly to elasticsearch it's really just a matter of a couple of days maybe even today. This will certainly be in the next release
</comment><comment author="imotov" created="2014-03-11T00:51:33Z" id="37252641">@s1monw that might be a good idea. In many of our filters equals and hashCode are auto-generated, but we have a few that we wrote by hand. However, in this particular case having tests didn't help us much. There even was a randomized test that was generating all these different filters, but it looks like it never managed to reproduce this particular use case. It's just pretty rare condition.
</comment><comment author="rslinckx" created="2014-03-11T08:49:51Z" id="37274021">So it's really that particular combination of numbers/list length/order ?
</comment><comment author="imotov" created="2014-03-11T19:30:50Z" id="37339406">@rslinckx yes, so thanks for providing a real example! It would have been much more difficult to figure out without having a repro.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add integration client support for Spring Data Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5362</link><project id="" key="" /><description>I added this earlier but may be with new site this was removed.

can you guys add it back ?

Thanks 
Mohsin
</description><key id="28954499">5362</key><summary>Add integration client support for Spring Data Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mohsinh</reporter><labels><label>docs</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-07T10:08:01Z</created><updated>2014-07-16T21:47:49Z</updated><resolved>2014-03-13T07:48:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:12:38Z" id="37458118">@spinscale can you get this in?
</comment><comment author="spinscale" created="2014-03-13T07:48:35Z" id="37507284">added.. thanks for the PR!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>There should be common interfaces for the same name methods in java client library</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5361</link><project id="" key="" /><description>AFAIK, it would be great if two types of interfaces are defined, which would help developers to do better source reusing. And also that there maybe other things in same situation, I hope somebody else can commit them here too.

At first, for the request builders, there are SearchRequestBuilder, CountRequestBuilder and DeleteByQueryRequestBuilder which are sharing a group of methods with same name due to the completely same functions. Thus, currently, I have to define 3 methods for the separated 3 types if I want to add some common search define on the request(especially for adding routing by common logic).

Secondly, for the QueryBuilders and FilterBuilders, they are in 99% similarity logically and they are in 99% similarity in the real source but there are only separated interfaces for each other of query and filter. Hence, I have to define 2 methods for the separated 2 types if I want to create a common query define for some common business data structure.

I believe both of the above situations can be addressed by defining higher level interfaces.
</description><key id="28942028">5361</key><summary>There should be common interfaces for the same name methods in java client library</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xzer</reporter><labels><label>:Java API</label></labels><created>2014-03-07T07:02:07Z</created><updated>2015-04-13T10:14:30Z</updated><resolved>2015-04-13T10:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T16:58:42Z" id="91621489">This can be visited as part of #10217
</comment><comment author="clintongormley" created="2015-04-13T10:14:30Z" id="92300305">Agreed - closing as a duplicate of #10217 and https://github.com/elastic/elasticsearch/pull/10531
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST Testing framework enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5360</link><project id="" key="" /><description>Adding operators 'lte' and 'gte' to our REST test framework. These
operators test for, respectively, less-than-or-equal and
greater-than-or-equal.

Closes #5360 
</description><key id="28927361">5360</key><summary>REST Testing framework enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-07T00:05:29Z</created><updated>2014-10-21T23:41:24Z</updated><resolved>2014-03-10T22:24:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-08T10:05:13Z" id="37094019">Looks good to me, one thing I'd ask you to do is to treat this as a new feature, like we did we regex in cat api tests. That way we add support for this new feature to our runner and the other runners have time to do the same without having build failures in the meantime. You'd just need to add the new feature support to the static list in `Features` class and add skip sections mentioning the feature to the new tests that use `gte` and `lte`.

Once all runners implemented the feature, we can remove the skip sections and the feature from the list as well.

One more thing: your commit points to the wrong issue, can you change that when you rebase?
</comment><comment author="s1monw" created="2014-03-10T22:23:04Z" id="37242349">LGTM
</comment><comment author="javanna" created="2014-03-31T10:50:23Z" id="39075479">This was on master only. Given that the recovery api, whose tests use this feature, is on 1.x too, it's worth to backport it to 1.x. I just did it then ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a method to compute alias names for index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5359</link><project id="" key="" /><description>This is a proposal to extend the functionality to support aliases within templates #5180, #1825, #2739 

One use case that the current implementation does not cover is the ability to handle an index per day scenario. In this case it is nice to be able to present a single view of the indices which for the sake of argument look like "product_elasticsearch_20140306" as a unified alias which look like "product_elasticsearch".

The idea is to allow alias names to be formatted similarly to how python handles format strings http://docs.python.org/2/library/string.html#formatstrings

If we have a script define the value of the alias's format string parameters then regexes are not required(although still an option) to get a substring of the index name.

Here is an example:

PUT _template/template_1

``` json
{
    "template" : "product_*",
    "settings" : {
        "number_of_shards" : 1
    },
    "aliases": {
       "alias1" : {},
        "alias2" : {
            "filter" : {
                "term" : {"user" : "kimchy" }
            },
            "routing" : "kimchy"
        },
        "{index}-alias-for-{gender}": {
            "filter" : {
                "term" : {"product" : "Elasticsearch" }
            },
            "routing" : "Elasticsearch",
            "format": {
                "index": {
                    "script": "_index['name'].value",
                },
                "gender": {
                    "script": "doc['isFemale'].value ? f : m",
                    "params": {
                        "m": "males",
                        "f": "females"
                    }
                }
            }
        }
    }
}
```
</description><key id="28901766">5359</key><summary>Add a method to compute alias names for index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rbraley</reporter><labels><label>:Aliases</label><label>discuss</label><label>enhancement</label></labels><created>2014-03-06T18:20:06Z</created><updated>2017-07-04T13:16:15Z</updated><resolved>2016-12-23T10:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimmyjones2" created="2014-03-26T14:52:06Z" id="38692836">Along similar lines, the definitive elasticsearch guide says to "use aliases instead of indices in your application", however with templates and timestamped indicies this AFAIK not possible, eg. I can't have a template to match my_index_v1-2014.03.26 and create an alias of my_index-2014.03.26

This is basically the same as [Shay's comment](https://github.com/elasticsearch/elasticsearch/pull/5180#issuecomment-35837272) on #5180
</comment><comment author="gurvindersingh" created="2014-04-30T18:22:38Z" id="41831063">I would really like to have too the alias with date part added from index name, as it will make query to hit only those indexes with specific date otherwise it will hit all the indexes and degrade performance.
</comment><comment author="rbraley" created="2014-06-25T08:10:31Z" id="47072275">Would love to revisit this. It is rather clunky to hack around this with a cron job to create aliases every day.
</comment><comment author="ktonga" created="2014-09-07T05:34:23Z" id="54737960">It would be nice to be able to do some scripting for creating an alias name. But most of the times it would be used to implement the index per day pattern. So it still seems like a hack.
That's why I've created a separate issue ( #7631 ) for natively supporting this common pattern.

What do you think?
</comment><comment author="ktonga" created="2014-09-07T23:50:39Z" id="54765458">An alternative way for triggering aliases creations, instead of cronning jobs, would be to know if the index was automatically created after performing the index action. I've created a separate issue ( #7634 ) for it as well. I guess it is the easiest approach to implement (compared to scripting in templates or native rolling indexes).
</comment><comment author="ppf2" created="2015-03-27T07:18:25Z" id="86852588">+1 
</comment><comment author="rtkmhart" created="2015-03-27T12:45:38Z" id="86930306">To start with I would love to be able to specify time-based aliases. For example:

PUT /_template/my_template

```
...
"aliases": { "my_alias-{YYYY}-{MM}-{DD}" } 
```

would create my_alias-2015.03.27 on the new index, etc. Although the full scripting would be awesome.
</comment><comment author="dakrone" created="2015-04-10T16:56:06Z" id="91620930">We discussed this today and thought about the regex-based solution that Shay suggested in the linked issue, for example, if we supported full regexps in the template name with captured vars:

```
{ 
  "template": "(logstash-\d\d\d\d)-*",
  ...
  "aliases": {
    "{index:$1}": { ... }
  }
}
```

Then the alias for an index `logstash-2014-03-20` would be `logstash-2014`. I am concerned about running scripts during index creation (especially with dynamic scripts being disabled for Groovy).

@javanna what do you think of this? You having been a pretty involved in the alias templates stuff.
</comment><comment author="javanna" created="2015-04-15T13:26:17Z" id="93396676">&gt; what do you think of this?

what can I say? I don't like regexes... if I have to choose between groovy scripts and regexes...I might actually go with scripts... sorry.
</comment><comment author="rjernst" created="2015-04-15T19:41:22Z" id="93543783">Scripts seem much too complicated here. In fact we already basically support regexes in the `template` parameter (just with the only operator being `*`?), this would just extend them to be _real_ regexes so that you could use pattern subs in other places.
</comment><comment author="rbraley" created="2015-04-15T20:45:12Z" id="93563992">I think the regexes with captured vars would be sufficiently powerful to solve this problem. Would be fantastic if it worked with #7634 as well.
</comment><comment author="hvisage" created="2015-04-23T21:21:03Z" id="95722058">I was hoping for an alias name, computed/based on a field's value.
</comment><comment author="zofog" created="2015-05-06T17:29:18Z" id="99544722">+1
</comment><comment author="soenkeliebau" created="2015-05-06T21:09:53Z" id="99610506">I definitely agree that something like this is necessary. I am currently also using cronjobs to create aliases for daily indices, but solving this in the template would definitely be a better way. 
A full blown regex solution is not even really necessary, a simple fix would be to allow for substitution of the fixed part of the "template" value, so if the template applies to "logstash-*", keep the part matched by the \* but allow for substitution of the logstash part for another fixed string. 
That would effectively cater to any recurring index.
</comment><comment author="Ginja" created="2015-05-15T19:23:23Z" id="102499917">+1
</comment><comment author="felipegs" created="2015-05-26T01:57:54Z" id="105358578">+1
</comment><comment author="MaartenBo" created="2015-06-15T12:18:59Z" id="112041397">+1
</comment><comment author="nsphung" created="2015-07-01T12:35:48Z" id="117642136">+1
</comment><comment author="Mrc0113" created="2015-07-02T14:13:05Z" id="118046749">+1
</comment><comment author="archie-sh" created="2015-07-24T12:00:55Z" id="124499474">+1
</comment><comment author="benbiti" created="2015-09-06T14:14:09Z" id="138089057">+1
</comment><comment author="barravi" created="2015-09-08T09:24:35Z" id="138493664">+1
</comment><comment author="mkliu" created="2015-09-09T22:22:39Z" id="139061719">+1
</comment><comment author="webmstr" created="2015-09-12T00:51:20Z" id="139693538">+1
</comment><comment author="schast" created="2015-10-08T10:10:15Z" id="146484349">+1
</comment><comment author="sts" created="2015-11-04T12:52:57Z" id="153712327">+1
</comment><comment author="ppf2" created="2015-11-10T08:23:21Z" id="155355944">Another common use case that will benefit from extending support for aliases is shared indices with routing.  Would be helpful to be able to generate these dynamically using a template (for some deployments, can be thousands+ of them).

```
POST /_aliases
{ 
"actions": [{ 
"add": { 
"index": "sales", 
"alias": sales_&lt;CUST_ID&gt;, 
"filter": {"term": {"customer_id": &lt;CUST_ID&gt;}}, 
"routing": &lt;CUST_ID&gt; 
} 
}] 
}
```
</comment><comment author="colings86" created="2015-11-13T10:34:40Z" id="156392434">@martijnvg has this issue been solved by https://github.com/elastic/elasticsearch/pull/12209 ?
</comment><comment author="barravi" created="2015-11-13T11:43:56Z" id="156406448">Partly yes, and I'm sure it would solve my immediate issue.

More interesting would be to extend the concept, and have capturing groups in the name regex for index templates, so to be able to reuse part of the index name in the alias definition. But I can call myself happy enough with #12209 :+1: 
</comment><comment author="zippoxer" created="2016-01-07T16:28:23Z" id="169716650">If I define an alias per user and I have millions of users, this enhancement would allow me to keep one alias for all users instead of manually defining an alias per-user?

As a result of less aliases, some load would be reduced from the cluster's state, right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Updating scripting docs for geo functions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5358</link><project id="" key="" /><description>The docs for the geo functions are currently out of sync with the implementation in Elasticsearch. I have removed the geohash functions, added the factor functions, and updated the existing geo distance functions.
</description><key id="28897973">5358</key><summary>[docs] Updating scripting docs for geo functions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jetheredge</reporter><labels /><created>2014-03-06T17:30:17Z</created><updated>2014-06-23T09:23:43Z</updated><resolved>2014-03-17T11:01:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-17T11:01:13Z" id="37803958">thanks! Pushed via https://github.com/elasticsearch/elasticsearch/commit/36219a1786c9a73ed6c595ff4f04b68bd6d455d8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RemoteTransportException when trying to access :9200/_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5357</link><project id="" key="" /><description>I have a 3 node ES cluster. I just upgraded from 0.90.11 to 1.0.1 and started experiencing these exceptions. When I try to access `curl 'http://server:9200/_nodes?pretty=true'` on any of my nodes I get this exception in ES logs:

```
[2014-03-06 03:52:23,848][DEBUG][action.admin.cluster.node.info] [logserver3-la] failed to execute on node [iPvGOBIQTuOV_YhNAmLAUg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 7711
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:276)
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)
    at org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:597)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)
    at org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)
    ... 23 more
```

I created a gist of the output of _nodes that I do get here:
https://gist.github.com/daledude/c6c0fb018d06d1e45a62

The exception in the logs is the same for all nodes. Using ES 1.0.1 and Java HotSpot(TM) 64-Bit Server VM 1.7.0_25 on all nodes.

This is my config which is the same for all nodes except the hosts, rack, zone:

```
cluster.name: mycluster
node.name: "logserver1-chi"
node.rack: chi1
node.zone: chi
node.master: true
node.data: true

index.number_of_replicas: 0

# cluster discovery
discovery.zen.fd.ping_interval: 15s
discovery.zen.fd.ping_timeout: 60s
discovery.zen.fd.ping_retries: 5
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["logserver3-la.domain.com", "logserver2.domain.com"]
cluster.routing.allocation.awareness.attributes: zone

indices.memory.index_buffer_size: 20%
index.translog.flush_threshold_ops: 50000
indices.fielddata.cache.size: 30%
bootstrap.mlockall: true

threadpool.search.type: fixed
threadpool.search.size: 20
threadpool.search.queue_size: -1

threadpool.index.type: fixed
threadpool.index.size: 60
threadpool.index.queue_size: -1

action.disable_delete_all_indices: false
```
</description><key id="28872489">5357</key><summary>RemoteTransportException when trying to access :9200/_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">daledude</reporter><labels /><created>2014-03-06T12:20:11Z</created><updated>2014-09-06T13:56:38Z</updated><resolved>2014-07-16T13:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-07T10:42:49Z" id="37013346">hey,

just to understand what you tried above: 
- Getting nodes info does not work
- Getting the nodes info per node (on the specific node) works, when I interpret your gist

I would like to get more information about that issue. Can you try the following calls and tell me, if any of these requests fail and if so, which:

```
curl localhost:9200/_nodes/http
curl localhost:9200/_nodes/jvm
curl localhost:9200/_nodes/network
curl localhost:9200/_nodes/os
curl localhost:9200/_nodes/plugins
curl localhost:9200/_nodes/process
curl localhost:9200/_nodes/settings
curl localhost:9200/_nodes/thread_pool
curl localhost:9200/_nodes/transport
```
</comment><comment author="daledude" created="2014-06-10T07:24:57Z" id="45581660">Sorry for delay. I have just been able to get back to this. None of the curl commands you gave fail.

The below does still fail with "Readable byte limit exceeded".

```
curl -XGET 'http://localhost:9200/_nodes'
```

Using es-head and bigdesk I get the error on the node I'm connected to if I try to select any other node.

What I've done since I opened this ticket:
*) Upgraded to jdk 1.7.0_60-b19 and still have the error.
*) Upgraded elasticsearch to 1.2.0 and still had the error.
*) Saw the emergency update and upgraded to 1.2.1. Still had the error. Also, running "elasticsearch-fix-routing-1.0.jar" also produced the error.

```
java -jar elasticsearch-fix-routing-1.0.jar localhost 9300 myindex count
```

*) Took down all nodes and wiped out ES data directory. Started only 2 nodes completely fresh and still had the error.
*) Dumped all data using elasticdump tool from community.
*) Downgraded ES to 1.1.2 and imported all data using elasticdump again. Still receive the error.

I'm using the same Centos 6, jdk, ES versions on all nodes. I have the latest es-head and bigdesk plugins installed on some nodes. I don't have any other transport agents on the network (I've taken ES down and sniffed network for any 9200/9300 traffic and there was none). I've used lsof to make sure the processes are not loading any old, or other, files or versions.

I appreciate the assist. I must be missing something. Any further hints on how to troubleshoot this how ever technical?
</comment><comment author="spinscale" created="2014-06-12T06:46:49Z" id="45835246">I think I found it, it is a duplicate of #6325 - I will try to find a fix soon
</comment><comment author="daledude" created="2014-07-09T22:32:50Z" id="48543902">Thanks spinscale. Will this make it in 1.3.0? Or maybe it's in 1.2.2?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomized number of shards used for indices created during tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5356</link><project id="" key="" /><description>Introduced two levels of randomization for the number of shards (between 1 and 10) when running tests:

1) through the existing random index template, which now sets a random number of shards that is shared across all the indices created in the same test method unless overwritten

2) through `createIndex` and `prepareCreate` methods, similar to what happens using the `indexSettings` method, which changes for every `createIndex` or `prepareCreate` unless overwritten (overwrites index template for what concerns the number of shards)

Added the following facilities to deal with the random number of shards:
- `getNumShards` to retrieve the number of shards of a given existing index, useful when doing comparisons based on the number of shards and we can avoid specifying a static number. The method returns an object containing the number of primaries, number of replicas and the expected total number of shards for the existing index
- added `assertFailures` that checks that a shard failure happened during a search request, either partial failure or total (all shards failed). Checks also the error code and the error message related to the failure. This is needed as without knowing the number of shards upfront, when simulating errors we can run into either partial (search returns partial results and failures) or total failures (search returns an error)
- added common methods similar to `indexSettings`, to be used in combination with `createIndex` and `prepareCreate` method and explicitly control the second level of randomization: `numberOfShards`, `minimumNumberOfShards` and `maximumNumberOfShards`. Added also `numberOfReplicas` despite the number of replicas is not randomized (default not specified but can be overwritten by tests)

Tests that specified the number of shards have been reviewed:
- removed number_of_shards in node settings, ignored anyway as it would be overwritten by both mechanisms above
- remove specific number of shards when not needed
- removed manual shards randomization where present, replaced with ordinary one that's now available
- adapted tests that didn't need a specific number of shards to the new random behaviour
- fixed a couple of test bugs (e.g. 3 levels parent child test could only work on a single shard as the routing key used for grand-children wasn't correct)
- also done some cleanup, shared code through shard size facets and aggs tests and used common methods like `assertAcked`, `ensureGreen`, `refresh`, `flush` and `refreshAndFlush` where possible
</description><key id="28861088">5356</key><summary>[TEST] Randomized number of shards used for indices created during tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-06T09:39:08Z</created><updated>2014-06-16T13:25:13Z</updated><resolved>2014-03-10T12:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-06T11:06:27Z" id="36845179">This change looks awesome. I think we should try to get it in very soon so that we don't get merge conflicts or need to track new tests that would be added in the mean time.
</comment><comment author="s1monw" created="2014-03-10T09:57:37Z" id="37166866">@javanna I left one small comment! this looks awesome - please go ahead and push it asap!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Fix a typo in the reference doc. SuSe -&gt; SUSE.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5355</link><project id="" key="" /><description>`SUSE`, as a Linux distribution, reads better if all upper case. Please refer to http://en.wikipedia.org/wiki/SUSE_Linux_distributions for details. 

fixes #5354
</description><key id="28860794">5355</key><summary>[docs] Fix a typo in the reference doc. SuSe -&gt; SUSE.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lzhoucs</reporter><labels /><created>2014-03-06T09:33:51Z</created><updated>2014-07-16T21:47:52Z</updated><resolved>2014-03-17T11:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-17T11:06:59Z" id="37804369">thanks! closed via https://github.com/elasticsearch/elasticsearch/commit/5a5171cb70cff9187d442d7ab91d2a876a4f22c1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] Fix a typo in reference doc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5354</link><project id="" key="" /><description>In `docs/reference/setup/as-a-service.asciidoc`, there's a minor typo : Distributions like `SuSe` do not use... I think `SUSE` reads better, since `SUSE`, as a Linux distribution, is never lower cased. http://en.wikipedia.org/wiki/SUSE_Linux_distributions
</description><key id="28860351">5354</key><summary>[doc] Fix a typo in reference doc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lzhoucs</reporter><labels /><created>2014-03-06T09:25:30Z</created><updated>2014-03-17T11:06:11Z</updated><resolved>2014-03-17T11:06:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add dedicated /_search/template endpoint for query templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5353</link><project id="" key="" /><description>In order to simplify query template execution an own endpoint has been added
- Rest tests and another unit test added
- Changed the RestApiSpecParser to throw a more useful exception

Open question: Should the JAVA APIs change as well?
</description><key id="28858445">5353</key><summary>Add dedicated /_search/template endpoint for query templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-06T08:47:57Z</created><updated>2015-06-07T17:15:25Z</updated><resolved>2014-03-20T17:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-13T20:56:06Z" id="37585766">I like it a lot! I think it looks pretty cool left some minor comments
</comment><comment author="spinscale" created="2014-03-14T08:28:28Z" id="37625646">updated the PR based on your comments
</comment><comment author="s1monw" created="2014-03-14T11:41:31Z" id="37638908">a small amount of comments but LGTM !!
</comment><comment author="clintongormley" created="2014-03-14T11:46:40Z" id="37639241">I'd love to see some examples of more complicated templates, including mustache sections, use of arrays etc.

Also, is the automatic html encoding of output turned off?
</comment><comment author="spinscale" created="2014-03-14T17:17:28Z" id="37672805">hrm...

``` JSON
{
    "template" : {
        "query": { "filtered" : {
            "query" : { "match_all" : {} },
            "filter" : { "owner" : "{{user}}" }
         },
        "aggs" : {
        {{#aggs}}
          "{{.}}" : { "terms" : { "field" : "{{.}}" }  }
        {{/aggs}}
        }
    },
    "params" : {
        "user" : "kimchy",
        "aggs" : [ "category_id", "sub_category_id" ]
    }
}
```

something like this will not produce valid JSON, as you cannot check for the last element of an array with mustache, thanks to logicless template... need to think about this in order to solve it nicely, makes mustache somewhat limited here for our needs...
</comment><comment author="clintongormley" created="2014-03-14T20:08:37Z" id="37689894">i do wonder if mustache is the right answer here...  You can certainly make it do the right thing, but I wonder how easy it will be for the user to write.
</comment><comment author="s1monw" created="2014-03-14T20:09:32Z" id="37689988">it's just a scripting engine so you can plug in whatever produces a string?
</comment><comment author="clintongormley" created="2014-03-14T20:11:17Z" id="37690145">yeah, but i'm thinking more of the existing dsl with some added operators... not sure. need to play with it a bit
</comment><comment author="clintongormley" created="2014-03-15T14:50:07Z" id="37727612">OK I've worked through various non-trivial examples of templating using mustache, and added a number of worked examples to the docs page: See https://github.com/spinscale/elasticsearch/pull/2

There are some things (conditional clauses) which can't be expressed using the JSON form of templates, but can be expressed as strings. I've included an example in the PR.

One thing that needs to be fixed is the `escape` method. Currently it escapes characters as expected for HTML, eg `"` -&gt; `&amp;quot;`.  This is clearly wrong.  Instead, we should be using these escapes:

```
\b  Backspace (ascii code 08)
\f  Form feed (ascii code 0C)
\n  New line
\r  Carriage return
\t  Tab
\v  Vertical tab
\"  Double quote
\\  Backslash 
```
</comment><comment author="s1monw" created="2014-03-20T10:25:25Z" id="38152129">LGTM lets push this @spinscale 
</comment><comment author="clintongormley" created="2014-03-20T11:13:28Z" id="38155554">I've opened the escaping bug as a separate issue #5473
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5352</link><project id="" key="" /><description>Adds a new API endpoint at /_recovery as well as to the Java API. The
recovery API allows one to see the recovery status of all shards in the
cluster. It will report on percent complete, recovery type, and which
files are copied.

Closes #4637
</description><key id="28843957">5352</key><summary>Recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-03-06T01:54:37Z</created><updated>2014-06-26T15:24:49Z</updated><resolved>2014-03-27T23:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-06T13:26:34Z" id="36887156">Hiya

Please could you add the API details to the rest-spec, and preferably add some rest-spec tests as well.

ta
</comment><comment author="imotov" created="2014-03-10T23:54:34Z" id="37249253">Left one comment. Other than that - LGTM.
</comment><comment author="kimchy" created="2014-03-13T11:32:13Z" id="37522849">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add explain capabilities to parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5351</link><project id="" key="" /><description>Currently the `has_child`, `has_parent` and `top_children` queries don't support score explanation, which results in `not implemented yet...` messages in the explain response if one of these queries is used.
</description><key id="28821698">5351</key><summary>Add explain capabilities to parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-03-05T20:14:25Z</created><updated>2015-06-25T10:18:30Z</updated><resolved>2015-06-25T10:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mgreene" created="2014-08-22T00:52:27Z" id="53008729">I would love to see this. It's currently very difficult to troubleshoot slow queries within ES and any insight into parent/child query execution would be very helpful and much appreciated!
</comment><comment author="soeren-helbig" created="2014-09-22T11:55:04Z" id="56362414">+1

reconstruction of scoring with child/parent could be hard. would love to see it too.
</comment><comment author="speedarius" created="2014-09-22T23:12:12Z" id="56457553">+1 - this would be very nice to have
</comment><comment author="gmenegatti" created="2014-10-22T21:28:09Z" id="60158651">+1
</comment><comment author="mmahalwy" created="2014-11-07T05:58:19Z" id="62100375">+1
</comment><comment author="Gasol" created="2015-06-25T06:28:37Z" id="115122967">:+1: 
</comment><comment author="martijnvg" created="2015-06-25T10:18:29Z" id="115200216">This was implemented via #6107.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Indices stats groups in nodes api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5350</link><project id="" key="" /><description>Closes #5349
</description><key id="28820830">5350</key><summary>[docs] Indices stats groups in nodes api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-05T20:03:48Z</created><updated>2014-06-26T10:31:26Z</updated><resolved>2014-03-31T17:55:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:29:02Z" id="39116427">@clintongormley are you the one I should ping about this?
</comment><comment author="clintongormley" created="2014-03-31T17:55:17Z" id="39119626">Apparently so :)

Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Return indices stats groups from nodes stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5349</link><project id="" key="" /><description>It'd be useful to be able to get the indices groups stats from the nodes stats api so I could track the groups on a node by node level.  Most of my metrics are captured at a node by node level and it is weird that search groups are cluster wide.  It makes Ganglia's default behavior of summing the stats across all nodes very confusing as well.
</description><key id="28817365">5349</key><summary>[docs] Return indices stats groups from nodes stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-05T19:21:33Z</created><updated>2014-03-31T17:55:08Z</updated><resolved>2014-03-31T17:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-05T19:31:59Z" id="36783163">Looks like you can do it with the java api but not the rest one.
</comment><comment author="nik9000" created="2014-03-05T19:34:52Z" id="36783482">I'll add this to the rest api.  Is there a proper way to test it?
</comment><comment author="nik9000" created="2014-03-05T19:48:55Z" id="36785138">Actually it looks like you can get them by passing `?groups=whatever`.  I'll update the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster State API filter_indices doesnt work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5348</link><project id="" key="" /><description>In 0.90.12 the filter_indices parameter in the cluster state api does not seem to function.  Wonder if this fix was reverted?

https://github.com/elasticsearch/elasticsearch/issues/234
</description><key id="28815727">5348</key><summary>Cluster State API filter_indices doesnt work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brettdanger</reporter><labels /><created>2014-03-05T19:00:57Z</created><updated>2014-12-29T14:19:15Z</updated><resolved>2014-12-29T14:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-05T19:04:55Z" id="36780065">can you provide a concrete example how you expect it to work, so we can see, what you mean exactly?

Note, that this has changed in 1.0, so just make sure, you dont accidentally execute that request against a 1.0 based node.
</comment><comment author="clintongormley" created="2014-12-29T14:19:15Z" id="68260593">Hi @brettdanger 

The `filter_indices` parameter was removed in 1.x, and replaced by a path parameter instead.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Templates fail when {dynamic_type} is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5347</link><project id="" key="" /><description>There appears to be an issue with the new way of defining multi_fields (well, now really multi_field now, but. What used to be it) in conjunction with dynamic_templates.

As far as I see, this should still be supported according to the docs.

``` bash
# Add template for index
curl -XPUT localhost:9200/_template/test_template -d '
{
    "template": "test*",
    "mappings": {
        "_default_": {
            "dynamic_templates": [
                {
                    "raw_template": {
                        "match": "*",
                        "match_mapping_type": "string",
                        "mapping": {
                            "type": "{dynamic_type}",
                            "index": "analyzed",
                            "store": "yes",
                            "fields": {
                                "raw": {"type": "{dynamic_type}", "index": "not_analyzed", "store": "yes"}
                            }
                        }
                    }
                }
            ]
        }
    }
}'

# Create an index
curl -XPOST localhost:9200/test/

# Add a document
curl -XPOST localhost:9200/test/game -d '
{"title": "World of Warcraft: Warlords of Draenor"}'

# Expected a sucessfully added document, but instead an error is thrown: 
# {"error":"MapperParsingException[failed to find type parsed [{dynamic_type}] for [title]]","status":400}
# As far as I recall, this wasn't an issue in 0.90.x
# As I read the documentation, dynamic_type should still be available in 1.1.x:
# http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates
```

It works as expected if you use "string" instead of {dynamic_type}
</description><key id="28803326">5347</key><summary>Templates fail when {dynamic_type} is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HenrikOssipoff</reporter><labels /><created>2014-03-05T16:30:57Z</created><updated>2014-04-01T08:30:53Z</updated><resolved>2014-04-01T08:30:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-04-01T00:18:47Z" id="39158145">@HenrikOssipoff this should be fixed in #5564
</comment><comment author="javanna" created="2014-04-01T08:30:52Z" id="39181696">Yep I just tested it and this is the same issue as #5256, fixed in #5564. Thanks @kzwang for the heads up!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] term lookup caching conf options formatting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5346</link><project id="" key="" /><description>There seem to be inconsistent formatting of `term lookup caching` config options at
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-terms-filter.html#query-dsl-terms-filter-lookup-caching

The first two are using different visual formatting and style than the last one.
</description><key id="28793953">5346</key><summary>[doc] term lookup caching conf options formatting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2014-03-05T14:41:05Z</created><updated>2014-04-07T07:29:07Z</updated><resolved>2014-03-05T16:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-04-05T08:48:34Z" id="39632639">Just noticed this is still not fixed for `0.90` version: http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/query-dsl-terms-filter.html#query-dsl-terms-filter-lookup-caching
</comment><comment author="javanna" created="2014-04-07T07:29:07Z" id="39702522">Hi @lukas-vlcek thanks for the heads up, I just backported the fix to 0.90.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>scan_type is search &amp; small result set gives SearchContextMissingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5345</link><project id="" key="" /><description>may be duplicate of or related to https://github.com/elasticsearch/elasticsearch/pull/5170

using elasticsearch 1.0.0

I have a query where I would like to use search_type=scan to  scroll through all contacts owned by a user. It works fine if the user has enough contacts but I have one user where there are only two contacts and this fails.

So I do a GET on
http://localhost:9200/users/contact/_search?search_type=scan&amp;scroll=60m&amp;size=100
{
    "query": {
        "term": {
           "userId": {
              "value": "1o"
           }
        }
    }
}

I get back the following response
{
    "_scroll_id":"c2Nhbjs1OzIwMTpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwNDpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwMzpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwMjpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwNTpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzE7dG90YWxfaGl0czoyOw==",
    "took":1,
    "timed_out":false,
    "_shards":{
        "total":5,
        "successful":5,
        "failed":0
    },
    "hits":{
        "total":2,
        "max_score":0.0,
        "hits":[

```
    ]
}
```

}

and then http://localhost:9200/_search/scroll?scroll=60m 
c2Nhbjs1OzIwMTpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwNDpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwMzpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwMjpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzIwNTpxS1hwWW80MFJvV0hwbjdBcm5JRkF3OzE7dG90YWxfaGl0czoyOw==

fails with a SearchContextMissingException 

The same query for a user with 30000 contacts works fine and pages through the results like I would expect.  The above query returns the two results normally if I query without search_type=scan

So it only fails if the result set is smaller than the page size
</description><key id="28792333">5345</key><summary>scan_type is search &amp; small result set gives SearchContextMissingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2014-03-05T14:17:41Z</created><updated>2014-03-05T19:48:33Z</updated><resolved>2014-03-05T19:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jillesvangurp" created="2014-03-05T19:01:52Z" id="36779692">This bug may be false but there is an underlying problem that might cause me to see this that I discovered doing some more analysis on this:

I had a bit of code that used search_type that worked fine with 0.90 but broke when I tried it with 1.0.1 (upgraded this afternoon) today in several ways:

problem #1: the exit condition changed.

I used to parse the scrollId from the response and stop fetching new results when it was no longer included. Now I get the following response for the final page of results:

{
    "_scroll_id":"c2NhbjswOzE7dG90YWxfaGl0czo3NDk2Ow==",
    "took":3,
    "timed_out":false,
    "_shards":{
        "total":5,
        "successful":4,
        "failed":1,
        "failures":[
            {
                "status":500,
                "reason":"SearchContextMissingException[No search context found for id [199]]"
            }
        ]
    },
    "hits":{
        "total":7496,
        "max_score":0.0
"hits":[...]

```
}
```

}

So it is actually reporting an error for the next page of results from one of the shards and includes the final results. This looks weird to me. The only way I have of deducing that this is the final page is to look at the failures object or to keep track of the number hits I've processed. That can't be right.

problem #2: the size parameter seems to work in a weird way.

I'm not actually sure if that is a change or whether this was always broken. In any case, this seems to be per shard. So if I specify size=100, I actually get back 500 results per page, which would be the number of shards times the size. 

So getting back to my original bug, I probably am getting the results but my code fails trying to fetch another page of results because of the broken exit condition.

I would expect either the old behavior where the last result fetched no longer includes the scrollid. Alternatively, the API could be improved by explicitly including a next url and omitting that on the last page. My interpretation of the old behavior was to use the scrollid like this indeed. In any case, asking for the last page should not return any errors from any shard.
</comment><comment author="jillesvangurp" created="2014-03-05T19:15:01Z" id="36781258">actually looking closer it turns out it was a pilot error after all. I was passing in the same scrollid instead of using the one in the request. 

The minor issue of the pageSize above may be valid but that's explainable and probably not a big issue. So, closing this one.
</comment><comment author="clintongormley" created="2014-03-05T19:48:33Z" id="36785094">@jillesvangurp to explain: with the `scan` search type, there is no reduce phase, so `size` is actually per-shard, rather than per-request.  Each shard returns a max of `size` results.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elastisearch forum link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5344</link><project id="" key="" /><description>can anybody send me the elasticsearch forum link to put queries.
</description><key id="28784959">5344</key><summary>elastisearch forum link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinfashion</reporter><labels /><created>2014-03-05T12:09:49Z</created><updated>2014-03-05T13:14:37Z</updated><resolved>2014-03-05T13:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-05T13:14:37Z" id="36740489">Look on the website or google.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] add Elasticsearch Image Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5343</link><project id="" key="" /><description>add Elasticsearch Image Plugin to Plugins page

https://github.com/kzwang/elasticsearch-image
</description><key id="28784223">5343</key><summary>[DOCS] add Elasticsearch Image Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels /><created>2014-03-05T11:54:43Z</created><updated>2014-07-16T21:47:53Z</updated><resolved>2014-03-05T13:18:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-05T13:18:28Z" id="36740768">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>free text search (highlight)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5342</link><project id="" key="" /><description>I want to highlight the text which i am searching in my query.

e.g. - http://localhost:9200/productindex/_search?q=armani

I want to highlight all armani words in my response, means armani appears in different color
</description><key id="28783601">5342</key><summary>free text search (highlight)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinfashion</reporter><labels /><created>2014-03-05T11:42:59Z</created><updated>2014-03-05T17:01:34Z</updated><resolved>2014-03-05T11:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-05T11:49:11Z" id="36734688">Please use the forums to ask questions (and refer to the docs before asking).
</comment><comment author="robinfashion" created="2014-03-05T12:07:59Z" id="36735948">can you send me the link of forum.
</comment><comment author="dadoonet" created="2014-03-05T17:01:34Z" id="36765808">This could help: http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest: Path components with plus signs are decoded wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5341</link><project id="" key="" /><description>It's perfectly legal for literal '+' characters to appear in the path component on a URL, so for example:

```
$ curl 'http://localhost:9200/foo+bar' -XPOST
{"error":"InvalidIndexNameException[[foo bar] Invalid index name [foo bar], must not contain the following characters [\\, /, *, ?, \", &lt;, &gt;, |,  , ,]]","status":400}
```

should not result in an error, since the index name was "foo+bar", not "foo bar".

A workaround until this issue have been fixed is to percent-encode the plus sign using %2B, but this is not optimal, as it should not be required.
</description><key id="28770532">5341</key><summary>Rest: Path components with plus signs are decoded wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-03-05T07:34:37Z</created><updated>2017-03-13T15:36:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2014-03-05T07:37:50Z" id="36717172">Looking at the source code, the problem is caused at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/rest/RestController.java#L44, where `RestUtils.REST_DECODER` is used to decode path component values. The `REST_DECODER` is created for decoding query string components, not path components.
</comment><comment author="clintongormley" created="2015-10-14T14:58:58Z" id="148076566">As a workaround, percent encoding the plus sign will work.
</comment><comment author="paregos" created="2017-03-13T07:32:06Z" id="286033628">Hi there Im new to elastic search and I would like to try and fix this bug, if there is any additional information relating to it please let me know.
Thanks</comment><comment author="nik9000" created="2017-03-13T15:36:10Z" id="286144836">I don't know about any additional information, though in the time between when this was filed and now we've made a ton of changes. So I'd start by adding tests to make sure it is still a problem and then working from there if it is.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for sorting buckets based on sub aggregation down the current hierarchy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5340</link><project id="" key="" /><description>This is supported as long as the aggregation in the specified order path are of a single-bucket type, where the last aggregation in the path points to either a single-bucket aggregation or a metrics one. If it's a single-bucket aggregation, the sort will be applied on the document count in the bucket (i.e. doc_count), and if it is a metrics type, the sort will be applied on the pointed out metric (in case of a single-metric aggregations, such as avg, the sort will be applied on the single metric value)

 Closes #5253
</description><key id="28755854">5340</key><summary>Added support for sorting buckets based on sub aggregation down the current hierarchy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-05T00:46:57Z</created><updated>2015-06-07T15:13:36Z</updated><resolved>2014-03-05T23:22:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-05T11:14:42Z" id="36732266">This change looks good, I particularly appreciate that the aggregator that is used for comparisons is computed up-front.
Before getting this in, I think we should add validation of aggregation names so that they don't collide with special characters used in sort expressions (either as part of this PR or in a different one)?
</comment><comment author="uboness" created="2014-03-05T11:20:38Z" id="36732667">&gt; Before getting this in, I think we should add validation of aggregation names so that they don't collide with special characters used in sort expressions (either as part of this PR or in a different one)?

yeah, agreed... in particular, verify that aggregation names don't include `.`,`[`,`]`. The only thing that we should be aware of is that putting this constraint does break bwc in a way (as we never had these constraints in 1.0)
</comment><comment author="jpountz" created="2014-03-05T23:18:05Z" id="36807036">Looks good to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The size estimation for percolator queries takes longer than it should</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5339</link><project id="" key="" /><description>The percolator keeps track of the total amount memory being spent on all the queries that are in memory, this statistic is then exposed in the node stats, indices stats and cluster stats apis. Each time a percolator query is registered or deleted its size in memory is calculated and that is added or subtracted from the total size in bytes all percolator queries take in memory. The percolator uses Lucene's `RamUsageEstimator` which takes a substantial chunk of the total time being spend on registering a percolator query.

I think the total amount of memory spent on queries is a valuable statistic, but it shouldn't add a substantial overhead in registering a percolator query. Therefor I think we should compute the memory spent on percolator queries on the fly in the stats api, but only when the percolator stats are specifically requested. Downside would be that it may take a while for the `memory spent on percolator queries` statistic to estimate.

Based on: https://groups.google.com/forum/#!topic/elasticsearch/6bgUvea98Uw
</description><key id="28743918">5339</key><summary>The size estimation for percolator queries takes longer than it should</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-03-04T21:45:12Z</created><updated>2014-12-29T14:13:45Z</updated><resolved>2014-12-29T14:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rolftimmermans" created="2014-03-07T14:17:48Z" id="37027432">We're also running into this. Percolator queries may take over 10s to be inserted. If many are inserted subsequently the backlog causes insertions to take several minutes. Indexing documents and executing queries remains quite fast, though.
</comment><comment author="kimchy" created="2014-03-07T14:23:23Z" id="37027921">I suggest we disable this for now, the stat is important, but not at the mentioned cost. I am not sure about computing it at the stats call API time, since that can be very costly as well (it will take a lot of CPU, even if not as part of the indexing of a percolator query)
</comment><comment author="martijnvg" created="2014-03-07T15:01:07Z" id="37031405">I agree lets disable this statistic for now and re-think how to efficiently implement it.
</comment><comment author="jippeholwerda" created="2014-03-07T15:42:06Z" id="37035480">Any ideas on what release this will be included in? We would like to use this in production shortly.
</comment><comment author="martijnvg" created="2014-03-10T06:46:41Z" id="37156807">@jippeholwerda This fix in #5372 and will be included in version 1.0.2
</comment><comment author="jippeholwerda" created="2014-03-10T10:42:59Z" id="37169969">@martijnvg That's great news, thanks! When is release 1.0.2 due?
</comment><comment author="julesbravo" created="2014-03-11T00:56:11Z" id="37252905">Thanks guys!
</comment><comment author="martijnvg" created="2014-03-11T05:22:04Z" id="37264437">@jippeholwerda I expect a new release somewhere next week.
</comment><comment author="martijnvg" created="2014-03-14T08:36:20Z" id="37626165">The size estimation of percolator queries have been disabled via #5372. I'll keep this issue open until we find a better way of estimating the size percolator queries.
</comment><comment author="ajhalani" created="2014-03-25T16:17:13Z" id="38585559">Any ETA on the new version release? This bug makes it unfeasible to upgrade ES. 
</comment><comment author="s1monw" created="2014-03-25T16:19:29Z" id="38585906">should be there after I am done getting coffee.....
</comment><comment author="jplock" created="2014-04-08T13:04:00Z" id="39844570">Has this been fixed in 1.0.2?  I see 1.0.2 has been tagged, but I never saw an official announcement?  We're running into this issue registering a lot of percolator documents.
</comment><comment author="martijnvg" created="2014-04-08T13:14:39Z" id="39845578">@jplock The PR that disabled the size estimation of percolator queries has been pushed and is part of `1.0.2` and `1.1.0` releases.
</comment><comment author="clintongormley" created="2014-12-29T14:13:45Z" id="68260179">Closing as the size estimation has been disabled
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Refcount store</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5338</link><project id="" key="" /><description>This PR adds refcount to the store and fixes broken snapshot/restore tests and testRandomDirectoryIOExceptions test that started failing as a result
</description><key id="28740041">5338</key><summary>Add Refcount store</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-03-04T20:58:26Z</created><updated>2014-07-11T11:55:39Z</updated><resolved>2014-03-14T13:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-07T11:37:41Z" id="37016736">@imotov left a comment - will take a look at this next week once I am back...
</comment><comment author="s1monw" created="2014-03-14T13:57:34Z" id="37649202">I am taking this one over... I worked on it for a while and it seems I stabelized things... I will open new PR soon. I also applied all the comments above...
</comment><comment author="s1monw" created="2014-03-14T14:00:40Z" id="37649529">here is the new PR #5432
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A Match Query against a boolean field returns a result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5337</link><project id="" key="" /><description>Using version 0.90.5.

Supplying a string in a match query against a boolean type field with its value set to 'true' will return a hit if the query is a non-empty string. 

In the following example,  the first query returns a result, the second does not.

```
PUT /test/test/1
{
    "hello" : true
}

POST /test/_search 
{
    "query": {
        "bool": {
            "minimum_number_should_match": 1,
            "should": [
               {"match": {
                  "hello": "foo"
               }}
            ]
        }
    }
}

POST /test/_search 
{
    "query": {
        "bool": {
            "minimum_number_should_match": 1,
            "should": [
               {"match": {
                  "hello": ""
               }}
            ]
        }
    }
}
```

Obviously this is a really dumb query :smile: .We are fixing our client to not build queries like this. This does seem like odd behavior though. 
</description><key id="28730578">5337</key><summary>A Match Query against a boolean field returns a result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericdcobb</reporter><labels /><created>2014-03-04T18:51:53Z</created><updated>2014-03-05T20:36:16Z</updated><resolved>2014-03-05T20:36:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-05T20:18:02Z" id="36788342">Currently `null`, `""`, `"F"`, `"false"`, `"0"`, `"off"` and `"no"` is parsed as False. Everything else is True.
</comment><comment author="ericdcobb" created="2014-03-05T20:20:58Z" id="36788652">Thank you Igor.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Spatial4j 0.4.1 and JTS 1.13</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5336</link><project id="" key="" /><description>(This is a new pull request for #5286 because it needed to be re-based off master)

Fixes #5279.

The main think irking me is that ElasticSearch uses a global hard-coded instance of JtsSpatialContext.GEO as the one and only SpatialContext as a static final in ShapeBuilder.SPATIAL_CONTEXT. That wasn't what I had in mind when I (or was it Ryan or Chris, I forget) devised SpatialContext concept. If you look at JtsSpatialContextFactory (or don't even use JTS, look at SpatialContextFactory) you'll see a bunch of options that trigger various behavior. The most important one is "geo" (aka geodetic or geodesic, synonyms) which is a boolean that chooses between a latitude-longitude spherical world model or a flat plane (Euclidean geometry). It's not quite clear to me at this time how ElasticSearch users that want a flat world model and who pre-project their data are using this.

Ping @chilling
</description><key id="28729588">5336</key><summary>Upgrade to Spatial4j 0.4.1 and JTS 1.13</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsmiley</reporter><labels><label>:Geo</label><label>upgrade</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T18:38:35Z</created><updated>2015-08-25T13:26:01Z</updated><resolved>2014-03-20T10:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dsmiley" created="2014-03-11T18:48:11Z" id="37334375">This PR should be an easy one to apply; I don't believe there are any controversial things in it and it's tested.

My note about the global non-configurable ShapeBuilder.SPATIAL_CONTEXT is orthogonal to this PR that should be raised in a separate issue, as the problem existed before the PR.
</comment><comment author="kimchy" created="2014-03-16T11:07:51Z" id="37754354">hey @dsmiley, we didn't forget about this!
</comment><comment author="s1monw" created="2014-03-20T10:29:56Z" id="38152435">pushed thanks @dsmiley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase RamAccountingTermsEnum flush size from 1mb to 5mb</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5335</link><project id="" key="" /><description /><key id="28719907">5335</key><summary>Increase RamAccountingTermsEnum flush size from 1mb to 5mb</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T16:36:05Z</created><updated>2015-06-08T15:21:59Z</updated><resolved>2014-03-07T23:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-07T11:09:30Z" id="37015002">the change itself is pretty straightforward... maybe add a comment in the commit, **why** you did it (suppose it does not make a lot of sense to check for every megabyte of data)
</comment><comment author="dakrone" created="2014-03-07T23:31:41Z" id="37079499">Merged to 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix issue where circuit breaker was always reset to 80% upon startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5334</link><project id="" key="" /><description /><key id="28719904">5334</key><summary>Fix issue where circuit breaker was always reset to 80% upon startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T16:36:04Z</created><updated>2015-12-07T07:19:22Z</updated><resolved>2014-03-05T15:19:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-03-04T18:27:23Z" id="36656885">@spinscale I addressed your feedback from #5325 here instead, since it was split into separate PRs
</comment><comment author="spinscale" created="2014-03-05T08:01:59Z" id="36718340">left a minor comment, apart from that LGTM
</comment><comment author="dakrone" created="2014-03-05T15:19:46Z" id="36752506">Merged to 1.0, 1.x, and master.
</comment><comment author="justies" created="2015-12-07T07:19:22Z" id="162435622">incase if I have a diffferent types and having a common name of a field created by.When I querying with that field with respect to a particular type , will it load the data from all type to the heap memory or only from the type which i have refered. due to this frequent circuitbreaker exception occuring
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Higher Bulk Index memory usage with 1.0.1 versus 0.90.12</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5333</link><project id="" key="" /><description>On upgrading to version 1.0.1,
I encountered 'OutOfMemoryError: GC overhead limit exceeded' using a memory restricted development machine (limited to 512m).
Previously I was using version 0.90.3.
Profiling in VisualVM, I found the heap usage climbed just above 512m causing the error (during a Bulk index).
I downgraded to 0.90.12 and found the memory usage dropped (down to about 350m) and the total indexing time was faster (only 4s compared to about 8s).

My use case is building a small index and doing a Bulk Index Java API call for only 27K documents.

Both 0.90.12 and 1.0.1 use the same Lucene version 4.6.1, so I'm assuming this is not the issue.
Maybe this memory overhead is expected in the version 1.0.
Or this could be a configuration issue on my part.
</description><key id="28713209">5333</key><summary>Higher Bulk Index memory usage with 1.0.1 versus 0.90.12</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matt-blanchette</reporter><labels /><created>2014-03-04T15:16:08Z</created><updated>2014-12-29T14:11:45Z</updated><resolved>2014-12-29T14:11:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T14:11:45Z" id="68260024">Hi @matt-blanchette 

Sorry it has taken so long to get to this issue, which now refers to old code.  We haven't had other reports of this issue, so I'm assuming it has either been fixed or was some temporary error. If you're still seeing similar problems in more recent versions, please feel free to open a new issue with the details.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add MemoryIndex reuse when percolating doc with nested type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5332</link><project id="" key="" /><description>Also make use of the thread local memory reuse for a document being percolated with nested objects. 
The memory index will only be reused for the root doc, since most of the times that will be the biggest document.
</description><key id="28706183">5332</key><summary>Add MemoryIndex reuse when percolating doc with nested type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T13:50:47Z</created><updated>2015-06-07T15:06:31Z</updated><resolved>2014-03-10T06:54:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-06T15:02:13Z" id="36895873">once we obtain the thread local memory index in multi doc variant, we should reset it as well, right? similar to what we do in the single one.
</comment><comment author="martijnvg" created="2014-03-06T16:00:15Z" id="36902652">@kimchy Yes we should reset it, let me fix that.
</comment><comment author="martijnvg" created="2014-03-06T16:07:14Z" id="36903423">Added commit, so that the reused memory index will now also be reset in the multi index variant.
</comment><comment author="kimchy" created="2014-03-06T16:39:14Z" id="36907225">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rewrite BytesStreamOutput on top of BigArrays/ByteArray.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5331</link><project id="" key="" /><description>Make BytesStreamOutput more efficient by using paging via BigArrays/ByteArrays. Also changes two existing callers to be more correct/efficient.

Fix for #5159
</description><key id="28702970">5331</key><summary>Rewrite BytesStreamOutput on top of BigArrays/ByteArray.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T12:58:59Z</created><updated>2015-06-07T15:21:19Z</updated><resolved>2014-03-04T13:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-04T13:40:50Z" id="36624291">This looks good to me now! +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix small typo in percentiles doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5330</link><project id="" key="" /><description /><key id="28701766">5330</key><summary>Fix small typo in percentiles doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>docs</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T12:35:58Z</created><updated>2014-07-16T21:47:56Z</updated><resolved>2014-03-07T09:13:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-07T09:13:44Z" id="36979539">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CONF_FILE in /etc/sysconfig not used by startup script on RedHat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5329</link><project id="" key="" /><description>I'm facing a issue with node default parameters in the system configuration file (/etc/sysconfig/elasticsearch) on a RedHat Linux installation : the 
definition of the node configuration file is ignored by the standard startup script.

Environment:
- Redhat Linux RHEL 6.2
- Elasticsearch 1.0.1 (elasticsearch-1.0.1-1.noarch.rpm)
### Problems:

1) custom config file defined in /etc/sysconfig is ignored

The file /etc/sysconfig/elasticsearch contains default parameters used by the startup script /etc/init.d/elasticsearch to intialize the node. The parameter CONF_FILE, as its name suggests, contains the name of the configuration file. By default, it is defined like this:

```
    # Elasticsearch configuration file (elasticsearch.yml)
    CONF_FILE=/etc/elasticsearch/elasticsearch.yml 
```

If this value is changed to another location, the node will start but the new configuration will be ignored.

2) relative config file in /etc/sysconfig prevents node startup

In file /etc/sysconfig/elasticsearch, another parameter named CONF_DIR defines the Elasticsearch configuration directory, where other config files like logging.yml will be found:

```
    # Elasticsearch conf directory
    CONF_DIR=/etc/elasticsearch
```

According to the CONF_FILE comment, it should be possible to give a relative file name (elasticsearch.yml), but it does not work, the node will not start using standard service command.
### How to reproduce:

0) prerequisites
- install elasticsearch-1.0.1-1.noarch.rpm on a RedHat/Centos system
- create a new configuration file called foobar.yml in /etc/elasticsearch with specific parameters, for example the cluster name:
  
  ```
  cluster.name: foobar
  ```

1) custom config file
- edit file /etc/sysconfig/elasticsearch and change the CONF_FILE value to the new configuration file:
  
  ```
  CONF_FILE=/etc/elasticsearch/foobar.yml
  ```
- start the server using the service command:
  
  ```
  service elasticsearch start
  ```
- check the cluster name using the REST API:
  
  ```
  curl http://localhost:9200/_nodes?pretty=true
  ```
- the cluster name is still the default "elasticsearch", new configuration file was not used.    

2) relative standard config file
- edit file /etc/sysconfig/elasticsearch and change the CONF_FILE value to a relative name of an existing file in the CONF_DIR directory:
  
  ```
  CONF_FILE=elasticsearch.yml
  ```
- start the server using the service command:
  
  ```
  service elasticsearch start
  ```
- the startup fails with exit code 6 (configuration file not found), however the file $CONF_DIR/$CONF_FILE exits !      
### Workaround:

In file /etc/sysconfig/elasticsearch, pass the config file in the Java options : 

```
    ES_JAVA_OPTS="-Des.config=/etc/elasticsearch/foobar.yml"
```

If the config argument is not absolute (foobar.yml), the file will be searched in the $CONF_DIR directory during Elasticsearch startup (Java code). But it's not very clean, because :
- you can't use CONF_FILE in JAVA_OPTS, because it's defined later in the sysconfig file
- you still need an absolute CONF_FILE, because it's existence is directly checked in the init.d script  
- you'll have to remember that CONF_FILE is not really used by the node...
### How to solve:

1) in /etc/init.d/elasticsearch, line 88, in the daemon launch command line, define the es.config property using the $CONF_FILE variable :

```
    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -d \
        -Des.default.path.home=$ES_HOME \
        -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR \
        -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR \
        -Des.config=$CONF_FILE
```

(maybe es.default.config would be better than es.config ?)

This way, the configuration specified in the sysconfig file is really used by the node (solves problem n&#176;1).  

2) in /etc/init.d/elasticsearch, line 68, replace the current configuration file existence test :

```
    [ -f $CONF_FILE ] || exit 6        # line 68
```

by this one:

```
    [ -f $CONF_FILE ] || [ -f $CONF_DIR/$CONF_FILE ] || exit 6
```

This test reproduce better the one made in the Java code (first check for an absolute configuration file, then for a relative one in the configuration directory, then in the classpath, else fail). Now, the configuration file can be absolute or relative, it works without surprise (solves problem n&#176;2).

What do you think ?
</description><key id="28693215">5329</key><summary>CONF_FILE in /etc/sysconfig not used by startup script on RedHat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">lehcim</reporter><labels><label>:Packaging</label></labels><created>2014-03-04T10:07:40Z</created><updated>2015-10-07T15:19:49Z</updated><resolved>2015-10-07T08:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="piyushGoyal2" created="2015-07-01T09:28:59Z" id="117560841">+1
</comment><comment author="clintongormley" created="2015-07-01T09:31:56Z" id="117561350">@tlrx is this still an issue?
</comment><comment author="piyushGoyal2" created="2015-07-01T09:40:33Z" id="117564360">@clintongormley : I know this is not a place to ask question, but wanted to check on one thing since it's a little bit in relevance with this topic. What takes more precedence, the sysconfig file or the elasticsearch yml file if both have the same properties like log, work and data directory?
</comment><comment author="clintongormley" created="2015-07-01T10:17:21Z" id="117582168">@piyushGoyal2 good question. i _think_ the sysconfig file, as those values would get passed in as environment variables.
</comment><comment author="piyushGoyal2" created="2015-07-01T10:27:33Z" id="117584859">Thanks @clintongormley . I just tried that as well. Sysconfig always take the precedence. Thanks once again for the reply. I guess, this should be published somewhere since many folks might have faced this confusion.
</comment><comment author="brwe" created="2015-09-21T17:35:53Z" id="142052572">&gt; edit file /etc/sysconfig/elasticsearch and change the CONF_FILE value to a relative name of an existing file in the CONF_DIR directory:

I think currently the CONF_FILE is supposed to be an absolute path, this is at least what the documentations says [here](https://www.elastic.co/guide/en/elasticsearch/reference/1.4/setup-configuration.html#styles) and also how it works with -Des.path.conf and -Des.config. But that we never pass the CONF_FILE parameter to the startup command for rpm is a bug I think. I made a pr here: https://github.com/elastic/elasticsearch/pull/13687
</comment><comment author="brwe" created="2015-10-07T15:19:49Z" id="146227978">I forgot to say: We decided to remove the support for CONF_FILE altogether, see #13772
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filtering on _timestamp field (with custom format) not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5328</link><project id="" key="" /><description>run the example below:

```
DELETE test

POST test

PUT test/test/_mapping
{
    "test" : 
    {
        "_timestamp" : { "enabled" : true, "store" : true, "format": "yyyyMMddHHmmssSSS"}
    }
}

POST test/test/1?timestamp=20140301101010123
{
    "id" : 1
}

POST test/_search
{
    "fields": [
       "_timestamp"
    ], 
    "query": {
        "range": {
           "_timestamp": {
              "from": 20140301101010000,    
              "to": 20140301101010999
           }
        }
    }
}
```

i wll expect that last query will return created document (as the timestamp falls into specified range)
also after inserting, timestamp on document is not this same as the one specified in the request (it is  in fact **20140301101010124** when it should be **20140301101010123**)

context:
- es: 1.0.0
- jvm: 1.7.0_51
- hosted in EC2
</description><key id="28689592">5328</key><summary>filtering on _timestamp field (with custom format) not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels><label>:Dates</label><label>adoptme</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-03-04T09:12:05Z</created><updated>2015-06-03T16:09:06Z</updated><resolved>2015-06-03T16:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-05T17:00:46Z" id="36765715">This problem occurs because Elasticsearch [first tries](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java#L165) to parse timestamp as long (number of milliseconds since epoch). So, your timestamp is getting interpreted as `Thu, 27 Oct 2033 12:48:30 GMT`, while your ranges are getting interpreted correctly as you expect. We could flip this logic in parsing timestamp and try parsing using format first, but then it will break interface for all users who are using milliseconds (with default format just a number is interpreted as year). So, I am not really sure what would be a good solution here, but two possible workarounds here are 1) to use a format that doesn't look like long or 2) switch to milliseconds on the timestamp field. 
</comment><comment author="karol-gwaj" created="2014-03-05T17:06:24Z" id="36766399">sadly im stuck with timestamp in this format (as it comes from external source)
will be nice to  have some configuration switch that will allow to change default parsing behavior for timestamp 
</comment><comment author="clintongormley" created="2014-03-15T13:24:41Z" id="37725612">Another place where we don't do the right thing with `"1234"` vs `1234`:

```
DELETE /t

PUT /t
{
  "mappings": {
    "t": {
      "properties": {
        "date": {
          "type": "date",
          "format": "YYYYMMddHHmmss"
        }
      }
    }
  }
}

PUT /t/t/1
{
  "date": "20140101000000"
}

PUT /t/t/2
{
  "date": 20140101000000
}

GET /t/_search
{
  "script_fields": {
    "date": {
      "script": "doc['date'].value"
    }
  }
}

GET /t/_search
{
  "query": {
    "match": {
      "date": "20140101000000"
    }
  }
}

GET /t/_search
{
  "query": {
    "match": {
      "date": 20140101000000
    }
  }
}
```
</comment><comment author="rjernst" created="2015-04-10T16:49:01Z" id="91617825">As Igor said before, part of the problem is date fields have this dual behavior, where it can accept a formatted date, but also an epoch timestamp. However, in this case, the value being searched is passed as a json number. The only way we could interpret that with the original supplied format would be to write the number as a string and then try to parse. I don't think we should do this.

We could however make a couple improvements:
- Add a flag to disable interpreting as an epoch time
- Throw an error if a number is passed in when this feature is disabled 
</comment><comment author="clintongormley" created="2015-04-13T10:07:33Z" id="92299179">Another possibility would be to add `epoch` as a format, then users can control the order in which the epoch format is tried, eg:

```
 "format": "epoch|YYYY-mm-dd"
```

vs

```
 "format": "YYYY-mm-dd|epoch"
```
</comment><comment author="rjernst" created="2015-04-23T07:17:42Z" id="95472112">@clintongormley I really like that idea! I think we should do that, and change the docs to describe the default format to match the current behavior? Should we have a way to specify whether it is seconds or milliseconds epoch? We currently use the first example you gave, with milliseconds epoch.
</comment><comment author="clintongormley" created="2015-04-26T13:55:14Z" id="96384213">@rjernst yes, i think supporting epoch seconds would be an excellent idea, eg Perl uses floating point seconds.  So perhaps `epoch_seconds` and `epoch_ms`?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi field access in a nested property requires full path to name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5327</link><project id="" key="" /><description>Hi, came across this unusual behaviour where if I try and filter on a multi-field name in a nested property it won't work unless I include the name of the nested property in the field name.

Here's some code explaining what I mean:

```
curl -XPOST localhost:9200/test2 -d '
{
 "mappings": {
  "a" : {
   "properties" : {
    "b" : {
     "type" : "nested",
     "properties" : {
      "b1" : {
       "type" : "string",
       fields : {
        "b1n" : {
         "type" : "integer"
        }
       }
      }
     }
    }
   }
  }
 }
}'

{"acknowledged":true}

curl -XPUT localhost:9200/test2/a/1 -d '
{
 "a" : "a1", 
 "b" : { 
  "b1" : 7 
 }
}
'

{"_index":"test2","_type":"a","_id":"1","_version":1,"created":true}

curl -XGET localhost:9200/test2/a/_search -d '
{
 "query" : {
  "match_all" : {}
 }, 
 "filter" : {
  "nested" : {
   "path" : "b",
   "filter" : {
    "term" : {
     "b1" : 7
    }
   }
  }
 }
}'

{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test2","_type":"a","_id":"1","_score":1.0, "_source" :
{ "a":"a1", "b" : { "b1" : 7 } }
}]}}

curl -XGET localhost:9200/test2/a/_search -d '{

    "query":{
        "match_all":{
        }
    },
    "filter":{
        "nested":{
            "path":"b",
            "filter":{
                "term":{
                    "b.b1":7
                }
            }
        }
    }

}'

{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test2","_type":"a","_id":"1","_score":1.0, "_source" :
{ "a":"a1", "b" : { "b1" : 7 } }
}]}}

curl -XGET localhost:9200/test2/a/_search -d '
{
 "query" : { 
  "match_all" : {}
 }, 
 "filter" : {
  "nested" : {
   "path" : "b",
   "filter" : {
    "numeric_range" : {
     "b1.b1n" : {
      "from" : 6
     }
    }
   }
  }
 }
}'

{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[Do1-g8JBQ2m9zDxvci2ojQ][test2][0]: SearchParseException[[test2][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [ {\"query\":{ \"match_all\":{}}, \"filter\":{\"nested\":{\"path\":\"b\",\"filter\":{\"numeric_range\":{\"b1.b1n\":{\"from\":6}}}}} }]]]; nested: QueryParsingException[[test2] failed to find mapping for field [b1.b1n]]; }]","status":400}

curl -XGET localhost:9200/test2/a/_search -d '
{
 "query" : { 
  "match_all" : {}
 }, 
 "filter" : {
  "nested" : {
   "path" : "b",
   "filter" : {
    "numeric_range" : {
     "b.b1.b1n" : {
      "from" : 6
     }
    }
   }
  }
 }
}'

{"took":20,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test2","_type":"a","_id":"1","_score":1.0, "_source" :
{ "a":"a1", "b" : { "b1" : 7 } }
}]}}
```

The first two queries show that accessing a nested field works with or without the nested property name included in the field name. The next two show that when using a multi-field field inside the nested property, an error is generated if you don't include the nested property name.

I have tested this on 1.0.1.

It's pretty obscure but I couldn't think why the behaviour would be different so thought I'd log it as an issue. Thanks!
</description><key id="28672229">5327</key><summary>Multi field access in a nested property requires full path to name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benatwork99</reporter><labels /><created>2014-03-04T01:12:58Z</created><updated>2014-07-23T14:08:34Z</updated><resolved>2014-07-23T14:08:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="benatwork99" created="2014-03-04T01:15:26Z" id="36581754">FWIW I saw this issue - https://github.com/elasticsearch/elasticsearch/issues/3057 - but it was for a different cause. It inspired me to try using the full path to the field though, which worked. Maybe related?
</comment><comment author="clintongormley" created="2014-07-23T14:08:34Z" id="49878319">Closing this in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in RamAccounntingTermsEnum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5326</link><project id="" key="" /><description>In one of my custom scripts, I call:

```
(ScriptDocValues.Strings) doc().get("_uid");
```

which then throws:

```
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData$PagedBytesEstimator.bytesPerValue(PagedBytesIndexFieldData.java:147)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:84)
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData.loadDirect(PagedBytesIndexFieldData.java:99)
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData.loadDirect(PagedBytesIndexFieldData.java:41)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
```

It looks like the offending code is in `org.elasticsearch.index.fielddata.RamAccountingTermsEnum`, which has logic like 

```
public BytesRef next() throws IOException {
    BytesRef term = termsEnum.next();
    if (term == null &amp;&amp; this.flushBuffer != 0) {
        // We have reached the end of the termsEnum, flush the buffer
        flush();
    } else {
        this.flushBuffer += estimator.bytesPerValue(term);
        if (this.flushBuffer &gt;= FLUSH_BUFFER_SIZE) {
            flush();
        }
    }
    return term;
}
```

What happens when `term == null` but `this.flushBuffer == 0`? Won't it throw an exception? Is there an invariant that I'm not aware of? Unfortunately, I can't find a way to repro this.

I have attached the full stack trace below:

```
2014-03-01 11:31:28,820 +0000 ERROR [elasticsearch[Jacqueline Falsworth][search][T#17]] search.MyScript - Exception encountered while executing MyScript
org.elasticsearch.ElasticsearchException: java.lang.NullPointerException
    at org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:75)
    at org.elasticsearch.search.lookup.DocLookup.get(DocLookup.java:101)
    at com.MyCompany.MyScript.getStringForField(MyScript.java:158) // hidden name
    at com.MyCompany.MyScript.getIdentifierForDocument(MyScript.java:140)
    at com.MyCompany.MyScript.run(MyScript.java:101)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter$ScriptDocSet.matchDoc(ScriptFilterParser.java:184)
    at org.elasticsearch.common.lucene.docset.MatchDocIdSet.get(MatchDocIdSet.java:67)
    at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndBits.get(AndDocIdSet.java:106)
    at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:60)
    at org.apache.lucene.search.Scorer.score(Scorer.java:65)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:122)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:322)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:304)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:71)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:135)
    at org.elasticsearch.index.fielddata.AbstractIndexFieldData.load(AbstractIndexFieldData.java:69)
    ... 25 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData$PagedBytesEstimator.bytesPerValue(PagedBytesIndexFieldData.java:147)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:84)
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData.loadDirect(PagedBytesIndexFieldData.java:99)
    at org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData.loadDirect(PagedBytesIndexFieldData.java:41)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:139)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:135)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4739)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
    ... 29 more
```
</description><key id="28667746">5326</key><summary>NullPointerException in RamAccounntingTermsEnum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justinuang</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-03T23:41:05Z</created><updated>2015-06-07T22:47:25Z</updated><resolved>2014-03-04T17:50:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="justinuang" created="2014-03-03T23:52:05Z" id="36576242">What happens if the buffer is flushed right after the last non-null element in the `termsEnum` is iterated over? Won't `this.flushBuffer == 0`, and a `NullPointerException` will be thrown?

I see that this circuit breaking functionality was added by @dakrone in commit a7542247.
</comment><comment author="dakrone" created="2014-03-04T16:41:45Z" id="36645180">@justinuang I see the issue in `bytesPerValue` in `PagedBytesIndexFieldData`, I will fix this.
</comment><comment author="justinuang" created="2014-03-04T16:43:59Z" id="36645443">Thanks so much! If you don't mind, can you explain what the bug is?
</comment><comment author="dakrone" created="2014-03-04T16:47:34Z" id="36645860">It is like you said in the `.next()` method, if the `term` is null right after a flush we still end up calling the `.bytesPerValue()` function on it. I'm going to add a check in `.next()` that for null terms and returns null immediately.
</comment><comment author="justinuang" created="2014-03-04T16:49:47Z" id="36646122">Awesome. Thanks for your expertise. I'm guessing that your previous comment doesn't really apply to this issue then?

&gt; @justinuang I see the issue in bytesPerValue in PagedBytesIndexFieldData, I will fix this.
</comment><comment author="dakrone" created="2014-03-04T16:50:44Z" id="36646245">No, it does, that's where the actual NPE is coming from, because I assume `term` can never be null in `.bytesPerValue()` (which was incorrect)
</comment><comment author="justinuang" created="2014-03-04T16:56:28Z" id="36646893">Ah I see. Thanks again!
</comment><comment author="justinuang" created="2014-03-04T17:53:01Z" id="36653102">Cool. Do you know when the next release will be? It's hitting us in production.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add circuit breaker for parent/child id cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5325</link><project id="" key="" /><description>Changes include:
- Bumping the flush size for RamAccountingTermsEnum to reduce log messages when TRACE logging is enabled
- A breaker for parent/child id cache that I've tested against 20 million docs (10m parents, 10m children, 1 parent per child, parents having incremental ids &amp; children having randomly generated ids) as well as the stackoverflow data (see graph, x axis is the segment #, y axis is kb adjustment after segment is loaded)
- Fix for a bug I discovered that made the circuit breaker revert to 80% of the heap size even if it was configured in `elasticsearch.yml`

![adj-double-term-length](https://f.cloud.github.com/assets/19060/2315476/168db0e0-a324-11e3-8400-526ee6a816e6.png)

Closes #5325
</description><key id="28663774">5325</key><summary>Add circuit breaker for parent/child id cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-03T22:38:31Z</created><updated>2015-06-07T15:10:47Z</updated><resolved>2014-03-04T18:21:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-03-03T22:43:08Z" id="36570227">Also for reference, the graph is for ~168mb of parent/child id field data, so an adjustment of 3.75mb is pretty accurate (because we want the estimation err on the high side rather than the low side).
</comment><comment author="martijnvg" created="2014-03-04T12:43:48Z" id="36620172">The changes regarding the circuit breaker to p/c look good! Perhaps the other changes should have their own PR / issue? (maybe we want to port the other commits to 1.0?)
</comment><comment author="dakrone" created="2014-03-04T16:32:33Z" id="36643691">That's a good idea, I'll split this out and issue 3 different PRs
</comment><comment author="martijnvg" created="2014-03-04T16:43:38Z" id="36645399">+1 This looks good to me!
</comment><comment author="dakrone" created="2014-03-04T18:21:32Z" id="36656230">Pushed to master and 1.x in https://github.com/elasticsearch/elasticsearch/commit/23471cd72c9e7987d442220b61d77aaeb257501f and https://github.com/elasticsearch/elasticsearch/commit/d15d2b4327b639739b95774821a1f0c25642217b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for "missing" to all bucket aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5324</link><project id="" key="" /><description>NEED: In many (if not majority cases) when present users with business analytics, the user would want to see numbers for complete data set. No matter how you aggregate it should present the same data with the same number of documents. Inability to handle "missing" values exclude those from analysis making analyzed data set incomplete and grand totals dependent on which field(s) the aggregation is done. It is impossible to explain to the users why the lower level totals do not add up to the upper level ones!  

WORKAROUND: Currently field based bucket aggregations (term, range etc) have no way to aggregate missing values. The only way is to use missing aggregation on the same level and the same field as the term aggregation itself. It is easy enough when dealing with one level aggregations but if you have 2-3 level aggregation number of "missing" aggregations (and complete lower level aggregation to be repeated in them) mushrooms very quickly to the point that the query is huge, convoluted and not debuggable. It may affect performance  as well. Also fetched date needs to be heavily post-processed to extract multiple levels aggregation buckets from under various "missing" elements and put them inline with the regular aggregation values. Below please see a simple query to do 2 level aggregation with just one sum metrics

PROPOSAL: I would suggest that any aggregation operating on a field should have a missing option. If missing config is specified, aggregation should accumulate missing values under that value and honor any nested aggregations within. It should never assume any value like 0 or _missing since it may clash with actual keys. If it is not specified the aggregation should skip missing values as it does now.

This approach makes it entirely compatible with existing logic and give developers complete control over whether to aggregate missing and under what key. In cases when it is not needed (and not specified) there will be no performance overhead. But when it needed it will work faster as we would not need to do missing aggregation and aggregations under it separately (same goes for "other" aggregation)

To be honest, I would love to see the same handling for "other" - documents that have not been included in aggregation due to the aggregation size constraints. Again the same rationale - ability to slice complete data set regardless of aggregation structure. It is just as needed as "missing" and just as troublesome to calculate but 
I could understand if you did not add it as it may be not compatible with your algorithms but  PLEASE PLEASE add "missing" handling at least

```
{
      "total": {
        "sum": {
          "field": "money.totals.obligationTotal"
        }
      },
      "missing": {
        "missing": {
          "field": "division"
        },
        "aggs": {
          "total": {
            "sum": {
              "field": "money.totals.obligationTotal"
            }
          },
          "missing": {
            "missing": {
              "field": "fy"
            }
          },
          "group": {
            "terms": {
              "field": "fy",
              "order": { "_term": "asc" }
            },
            "aggs": {
              "total": {
                "sum": {
                  "field": "money.totals.obligationTotal"
                }
              }
            }
          }
        }
      },
      "group": {
        "terms": {
          "field": "division",
          "order": { "_term": "asc" },
          size:100
        },
        "aggs": {
          "total": {
            "sum": {
              "field": "money.totals.obligationTotal"
            }
          },
          "missing": {
            "missing": {
              "field": "fy"
            },
            "aggs": {
              "total": {
                "sum": {
                  "field": "money.totals.obligationTotal"
                }
              }
            }
          },
          "group": {
            "terms": {
              "field": "fy",
              "order": { "_term": "asc" }
            },
            "aggs": {
              "total": {
                "sum": {
                  "field": "money.totals.obligationTotal"
                }
              }
            }
          }
        }
      }
    }
```

cc @uboness, @jpountz   
</description><key id="28638484">5324</key><summary>Add support for "missing" to all bucket aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2014-03-03T17:18:28Z</created><updated>2015-07-23T08:20:58Z</updated><resolved>2015-07-23T08:16:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2014-03-30T16:45:38Z" id="39030957">Hello @uboness, @jpountz 
Any chance for this to make it to 1.2?
</comment><comment author="uboness" created="2014-03-30T18:48:53Z" id="39035190">@roytmana we're considering adding it, though can't promise it for 1.2 (we're currently working on several other enhancements/features for aggs, so we'll see if it'll fit)
</comment><comment author="j0hnsmith" created="2014-05-22T13:47:00Z" id="43889699">+1
</comment><comment author="d1nsh" created="2014-07-31T19:27:49Z" id="50806788">Any plans of picking this up? I am having the issue as well and I currently have a workaround using multi search requests. It is ok so far since I only have a couple of aggregation levels but I can see this getting messy If I go deeper.
</comment><comment author="cn081" created="2014-09-17T23:10:32Z" id="55974877">Any news on this issue?
</comment><comment author="yeroc" created="2014-09-18T23:11:54Z" id="56116452">I just ran into this -- we're trying to migrate from the deprecated facets to aggregations and are finding this a real gap in the supposed new and better way of doing things.
</comment><comment author="bradvido" created="2014-10-01T19:49:05Z" id="57526524">I'm trying to move to aggregations and this is really holding us back.  Forced to use facets for now...
</comment><comment author="roytmana" created="2014-10-01T19:53:55Z" id="57527267">Same here - lack of support for _missing as a bucket as well as lack of _other as a bucket (however inefficient it may be it still will be better than a convoluted multi-step process I would be forced to use to calculate it myself) prevents me from moving from facets to aggs
</comment><comment author="j0hnsmith" created="2014-10-01T21:20:30Z" id="57540587">Deprecating facets without a viable solution via aggregations is madness, if/when this happens, some of us won't be able update to the new ES version.
</comment><comment author="roytmana" created="2014-10-02T17:33:24Z" id="57667287">Exactly my feeling. I commented on depreciation change request a while ago and it looks like  ES team does not feel this way. They consider facet a legacy that makes it harder to move the product forward.  It is understandably but very unplesant for people who do heavy duty (and generic) analytics with facets. I wish they looked at aggs from productivity standpoint and considered how well it suits for traditional data mart style applications which typically operate on a consistent dataset regardless of grouping, rollups etc. This is where MISSING and OTHER is really handy...

I also wish theybwould look at metrics that are expressions against other metrics in query aggs...
</comment><comment author="jpountz" created="2014-10-03T11:21:07Z" id="57782198">In facets, `other` used to represent the number of values that didn't make it to the top terms. With aggregations, we tried to make everything document-based (as opposed to value). Computing the number of other documents is not possible in the general case. For example let's imagine that you have 2 documents which have a `tag` field. Document 1 has `tags: [ "red", "blue", "green" ]` and document 2 has `tags: ["red"]`. So overall we have the following counts: `{ red: 2, blue: 1, green: 1}`. Now if we run a terms aggregation with `size: 1`, we get `{red: 2}`. What should `other` be? `red` already matches all documents, so there are no really other documents. We might want to return how many documents match other terms (`blue` and `green`) but we cannot just return the sum: it is the same document that has `blue` and `green` as a tag, there is only 1 other document that has other terms.

In the end, we can't return the document count for other terms. However, we could imagine returning the sum of the document counts for other terms, would that be enough? Note that in the single-valued case, it would be equal to the number of documents that have another term, the issue that I described above only occurs with multi-valued fields.

Here is a suggested format for the response:

``` json
{
    "aggregations": {
        "colors": {
            "buckets": [
                {
                    "key": "red",
                    "doc_count": 5
                },
                {
                    "key": "green",
                    "doc_count": 3
                }
            ],
            "sum_of_other_buckets": {
                "doc_count": 3
            }
        }
    }
}
```

`sum_of_other_buckets` is a bit long but it aims at making clear that it is the sum of the document counts for other buckets and not the number of documents that have another term.

Something else that might be possible would be to return sub aggregations for the other terms, but it would suffer from the same issues in the multi-valued case, and I would like to keep it for later as it would require significant work.
</comment><comment author="clintongormley" created="2014-10-03T11:22:37Z" id="57782298">+1
</comment><comment author="rashidkpc" created="2014-10-03T14:45:21Z" id="57805598">The ability to sub-aggregate is absolutely necessary. I think the proposal to have the "other" bucket mean something slightly different is ok as long as we document its issue with multi-valued fields. As Adrien mentioned, this is not an issue with single valued properties.

However, I think its important to treat the "other" bucket as equivalent to the regular buckets. Aggregations have a defined nested structure and breaking that will harm the ability to process them in a cleanly recursive fashion. I'd rather see the other bucket be added to the bucket array, with perhaps a customizable key as suggested by the original proposal, something like:

Request:

```
{
    "aggs": {
        "colors": {
            "terms": {
                "field": "color",
                "other_bucket": "__myCustomKey__"
            }
        }
    }
}
```

Response:

```
{
    "aggregations": {
        "colors": {
            "buckets": [
                {
                    "key": "red",
                    "doc_count": 5
                },
                {
                    "key": "green",
                    "doc_count": 3
                },
                {
                    "key": "__myCustomKey__",
                    "doc_count": 3
                }
            ]
        }
    }
}
```
</comment><comment author="bradvido" created="2014-10-03T17:42:38Z" id="57828925">sum_of_other_buckets makes sense and its name clearly describes what it is. Maybe documents_in_other_buckets is clearer? 
This would be nice to have, and I'd bet that most use-cases aren't on multi-valued fields, so the gotcha won't apply (but definitely needs to be documented).
</comment><comment author="roytmana" created="2014-10-03T18:07:21Z" id="57832306">Like @rashidkpc I would love if _other was a bucket aggregation not just a metric. My primary interest is to be able to calculate metrics for _other bucket not just counts. Say, my analytics shows $ sales breakdown by store. I would like to be able to show sales for to 20 stores and then lump all other sales into _other so the total roll up for entire company does not depend on number of "visible" buckets. But if we could also support bucket sub aggs within _other bucket it would be fantastic.

For single valued field logic of _missing and _other is rather clear and it is the most common case. For multivalued fields it is not so clear as @jpountz noted. Maybe if ES provide _other bucket and let me pick metrics within and how to interpret it it would be a more generic use case? 

There could be a bucket aggregation called **Distinct** which takes parent document set and distinct it and any metrics within such bucket will not doublecount   

My example above would not work very well with multivalued field as it will be double-counting $ but so it will be double-counting if I tried to roll up visible buckets 

But I would like to say that **single value use case is arguable more important to have complete and very productive implementation** (in my mind it would be support for _missing and _other as buckets) and if I need to do analytics like in my example on a multivalued data element, I should probably structure my data so that each value of the "multi" is a document carrying its fraction of $ or accept doublecounting in some shape or form

My current solution for _other (even with facets since facets do not support _other on anything by count) is to calculate the same metric for entire dataset and then substract sum of the metric for "visible" facets. and that of course is not working for multi-valued fields. but I can 
</comment><comment author="jpountz" created="2014-10-03T23:11:00Z" id="57877528">@rashidkpc I'm concerned that it requires to know a term that doesn't exist among your documents, otherwise there could be a collision. This might not always be easy?

We might be able to do something about sub aggregations for the other bucket but this requires much more work so I would like to do it in several steps and start with just the count (however the format should allow for adding data for sub-buckets in the future).
</comment><comment author="jpountz" created="2014-10-06T13:42:37Z" id="58018129">Another concern that I have if we decide to make it a bucket is that it will break some invariants that we currently have:
- today the number of buckets is always between 0 and `size`. With `other` as a bucket it would be between 0 and `size` + 1
- if we decide to not sort it, then it will break the order of buckets,
- the `other` bucket could have a `doc_count` that is less than `min_doc_count`
- it could happen that the key that you give to the `other` bucket doesn't match the `include/exclude` rules
</comment><comment author="clintongormley" created="2014-10-09T17:29:48Z" id="58546065">&gt; The ability to sub-aggregate is absolutely necessary. 

This change will have to follow later, as it requires a significant refactoring of how we do things today.  For now, I just want to add the `missing` and `other` buckets, which is the last thing that facets do that aggregations don't.

&gt; However, I think its important to treat the "other" bucket as equivalent to the regular buckets. Aggregations have a defined nested structure and breaking that will harm the ability to process them in a cleanly recursive fashion. I'd rather see the other bucket be added to the bucket array, with perhaps a customizable key as suggested by the original proposal, something like:

This is a good point. Including "other" in `buckets` will definitely make it easier for the end user. It is up to the user to specify a key for the `other` bucket, and it means that we will only include this information if asked for.

&gt; today the number of buckets is always between 0 and size. With other as a bucket it would be between 0 and size + 1

Correct - we only include the bucket if the user requests it, so they should expect the extra bucket.

&gt; if we decide to not sort it, then it will break the order of buckets,

Is that a problem? It should always be added as the last bucket I think.

&gt; the other bucket could have a doc_count that is less than min_doc_count

Agreed - if the user asks for it, we should include it regardless.

&gt; it could happen that the key that you give to the other bucket doesn't match the include/exclude rules

That's fine too I think.

@rashidkpc does all of this make sense to you?
</comment><comment author="bleskes" created="2014-10-10T08:28:05Z" id="58626641">+1 to including the other bucket as part of the bucket list. By default I would use the `_other` key for this bucket and allow people to change it if it collides with their data (note the different is param name with the above example):

```
{
    "aggs": {
        "colors": {
            "terms": {
                "field": "color",
                "other_bucket_key": "__myCustomKey__"
            }
        }
    }
}
```

Setting `"other_bucket_key": null` would suppress returning the other bucket. 
</comment><comment author="clintongormley" created="2014-10-17T11:08:07Z" id="59498290">@rashidkpc @bleskes We're running into various issues with adding the "other" bucket in the `buckets[]` array. First, there are the issues raised by @jpountz:
- today the number of buckets is always between 0 and size. With other as a bucket it would be between 0 and size + 1
- if we decide to not sort it, then it will break the order of buckets,
- the other bucket could have a doc_count that is less than min_doc_count
- it could happen that the key that you give to the other bucket doesn't match the include/exclude rules

Then:
- The other bucket (at least at this stage) will only contain a doc count, not sub-aggs.  So it will differ substantially from the other buckets. (Later we can change it to include sub-aggs, but that will require significantly more work)
- The `other_bucket_key` needs to match the field type, eg a string, a long, an ipv4 address or a bool value, which can make choosing a unique value harder
- It seems unlikely that you would want bucket reducers (see #8110) to take the "other" bucket into consideration, so the bucket needs to be somehow special.

From the implementation side, having the other bucket in a separate namespace feels much cleaner.  But i can understand why you want it in the `buckets[]` array instead.  My question is, given all of the issue above, do you _still_  want it there?  Would it not be more flexible to provide it separately, and then you can choose whether to add it to the end of `buckets[]` or not, on a case-by-case basis?
</comment><comment author="rashidkpc" created="2014-10-17T15:59:51Z" id="59534685">&gt; today the number of buckets is always between 0 and size. With other as a bucket it would be between 0 and size + 1

This seems ok, we would just need to document the behavior. We can (should?) make the missing/other buckets optional. Need to decide default behavior here

&gt; if we decide to not sort it, then it will break the order of buckets,

Why wouldn't we sort it?

&gt; the other bucket could have a doc_count that is less than min_doc_count

Why wouldn't we drop it if this was the case? It doesn't meet the min_doc_count so it shouldn't be included? I'm not sure that makes sense either, could use some discussion on this point.

&gt; it could happen that the key that you give to the other bucket doesn't match the include/exclude rules

I suppose pick a behavior and document it. Either always include the _other bucket, or exclude/include based on the rules. Either the user needs to exclude it themselves in the results, or they need to write their rules and/or name their _other field so as not to cause a conflict.

&gt; Then:
&gt; 
&gt; The other bucket (at least at this stage) will only contain a doc count, not sub-aggs. So it will differ substantially from the other buckets. (Later we can change it to include sub-aggs, but that will require significantly more work)

I wonder how many use cases the proposed phase 1 behavior would satisfy. It doesnt work for the submitter who have used terms_stats pre-aggregations.

&gt; The other_bucket_key needs to match the field type, eg a string, a long, an ipv4 address or a bool value, which can make choosing a unique value harder

Why? I assume there's a code reason for this? It wouldn't be a JSON or javascript object issue. In general finding a term that isn't in your list shouldn't be all that tough. We ask people to find strings that aren't in their text all the time for highlighting.

&gt; It seems unlikely that you would want bucket reducers (see #8110) to take the "other" bucket into consideration, so the bucket needs to be somehow special.

Disagree here. You index the price of goods daily, you want to calculate the inflation of the top 5 highest volume food products (eg, rice, beans, wheat) as compared to all other goods.

&gt; From the implementation side, having the other bucket in a separate namespace feels much cleaner. But i can understand why you want it in the buckets[] array instead. My question is, given all of the issue above, do you still want it there? Would it not be more flexible to provide it separately, and then you can choose whether to add it to the end of buckets[] or not, on a case-by-case basis?

I would lean towards keeping it in the list. I think I understand the issues, but it does seem like a more consistent API implementation. That said, facets didn't. It would definitely make parsing the response less clean.
</comment><comment author="bleskes" created="2014-10-20T07:17:50Z" id="59692031">I generally agree with @rashidkpc . Some extra comments: (regrouping the issues)

&gt; - today the number of buckets is always between 0 and size. With other as a bucket it would be between 0 and size + 1
&gt; - the other bucket could have a doc_count that is less than min_doc_count
&gt; - it could happen that the key that you give to the other bucket doesn't match the include/exclude rules
&gt; - The other_bucket_key needs to match the field type, eg a string, a long, an ipv4 address or a bool value, which can make choosing a unique value harder

I don't think any of these are a problem if we omit `_other` by default. If the user explicitly asks for it, it's OK that it doesn't meet the normal criteria. I do see the lack of "cleanness" here, but i think it's OK from the Rest perspective (see last comment about usage bellow). I wonder what @Mpdreamz thinks as he is maintaining a strongly typed client at the moment.

&gt; - if we decide to not sort it, then it will break the order of buckets,

I think sorting is use case dependent. It feels good to me to by default always have it last as it will typically what you would want to have if you display the date as a table or a bar chart. It may be a place for future extensions to allow to configure how it is sorted.

&gt; - The other bucket (at least at this stage) will only contain a doc count, not sub-aggs. So it will differ substantially from the other buckets. (Later we can change it to include sub-aggs, but that will require significantly more work)

To me this is determines the correct behavior of this output. My main reason to support inlining the _other bucket is to allow iterating the tree in a consistent manner when building charts. When the aggs are one level deep the iteration code is simple and it's very easy to inject the _other key back into the array as a one off operation before you start. However, processing a deeper tree means you'd have to do it again at every layer. It feels cleaner to be able to not worry about the `_other` bucket and just process all the buckets as they come.  This also allows to push any custom sorting or naming logic to the ES size (once we support sorting) . If this is the way we're heading and we agree that _with sub aggs_ the bucket should be inlined, we should inline it from the beginning rather then change the structure later.
</comment><comment author="Mpdreamz" created="2014-10-20T08:28:25Z" id="59704766">For what its worth I'm also +1 on inlining the `_other` with the rest of the buckets, the behaviour constraints laid out @rashidkpc @bleskes @clintongormley seem to make perfect sense coming into this issue fresh. 

Iterator wise you'd always want it with the rest of the buckets. 

Maybe we can emit a tad more metadata with the `_other` bucket and get rid of the need to specify a key name for the other bucket? I can imagine many cases where aggregations are generated where creating a unique key does not make sense.

```
{
    "aggs": {
        "colors": {
            "terms": {
                "field": "color",
                "other_bucket": true
            }
        }
    }
}
```

```
{
    "aggregations": {
        "colors": {
            "buckets": [
                {
                    "key": "red",
                    "doc_count": 5
                },
                {
                    "key": "green",
                    "doc_count": 3
                },
                {
                    "_other": true, // OR missing: true 
                    "doc_count": 3
                }
            ]
        }
    }
}
```
</comment><comment author="jpountz" created="2014-10-20T08:34:55Z" id="59705400">Thanks @rashidkpc @Mpdreamz @bleskes for the feedback!
</comment><comment author="clintongormley" created="2014-10-21T09:42:35Z" id="59903020">@jpountz, @bleskes, and I had a long discussion about this yesterday.  The ability to calculate sub-aggregations on the "other" bucket in a performant way is hard, because you don't know which terms fall into the other bucket until you have processed all terms.

Possible approaches include:
- merge the buckets for all non-top terms - this could easily be millions of buckets (and sub-aggs) and be extremely costly
- use a breadth-first approach to first calculate the top terms before dealing with sub-aggs. this limits the sub-aggs which can be used because the `_score` is not cached, so eg `top_hits` can't be used
- use a two pass approach - the first pass calculates the top terms and the second pass calculates the buckets for everything except the top terms (an added advantage to this would be that the doc counts for top terms would be accurate)

None of these approaches is a quick win.  Ultimately, all that we can provide at the moment is a single figure: the total number of documents which are not represented by the top terms.  Essentially, this is: 

```
total_docs_in_parent - sum( doc_count in top terms)
```

(behaviour undefined in the multi-value case)

Given these limitations, I think it doesn't (yet) make sense to report this value as a bucket, but rather as a separate key, eg `sum_of_other_buckets`.  In the future, assuming that one of the above approaches works out, we will have the freedom to specify the "other" bucket fully, without the baggage of bwc.

**Question**: would this number add value? Is it worth adding it or not?
</comment><comment author="j0hnsmith" created="2014-10-21T10:00:04Z" id="59904911">For me, just the sum isn't particularly useful, I need to be able to perform the same sub aggregations that I'm running on the 'real' buckets however I do understand the difficulties these approaches present.

If the '_other/_missing' bucket can't be treated in the same way as the 'real' buckets (eg work with sub aggs etc) I agree that it shouldn't be a bucket.
</comment><comment author="john-kurkowski" created="2014-10-22T03:21:36Z" id="60031501">As a stopgap, the number alone would add value, yes.
</comment><comment author="Kallin" created="2014-10-31T14:08:55Z" id="61264907">+1 for _other bucket. Would it work for all bucketing aggs? I really need it for filter agg.
</comment><comment author="bquartier" created="2014-11-19T14:48:27Z" id="63650329">+1 for (optional) _other bucket too.

It is useful for many use case:
- Pies in Kibana 4 are useless if they do not cover the whole data set (I mean, the complete circle in a pie should represent the 100%)
- In faceted navigation, having an "other" category is useful too.
</comment><comment author="Kallin" created="2014-11-19T22:12:39Z" id="63723910">So many uses in graphing. I am working on an icicle chart like this:
http://bl.ocks.org/mbostock/1005873

This type of chart so well to aggregations, but it's a lot of extra work to ensure that every missing count is manually calculated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percentiles aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5323</link><project id="" key="" /><description>A `percentiles` aggregation would allow to compute (approximate) values of arbitrary percentiles based on the [t-digest](https://github.com/tdunning/t-digest) algorithm. Computing exact percentiles is not reasonably feasible as it would require shards to stream all values to the node that coordinates search execution, which could be gigabytes on a high-cardinality field. On the other hand, t-digest allows to trade accuracy for memory by trying to summarize the set of values that have been accumulated with interesting properties/features:
- compression is configurable, meaning that if you can configure it to have better accuracy at the cost of a higher memory usage,
- accuracy is excellent for extreme percentiles,
- percentiles are going to be accurate if few values were accumulated.

Example:

``` json
{
    "aggs" : {
        "load_time_outlier" : {
            "percentiles" : {
                "field" : "load_time",
                "percents" : [95, 99, 99.9] 
            }
        }
    }
}
```
</description><key id="28633381">5323</key><summary>Percentiles aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-03T16:17:05Z</created><updated>2015-06-06T18:37:05Z</updated><resolved>2014-03-03T17:53:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-03-03T19:40:59Z" id="36550163">Beautiful!
</comment><comment author="otisg" created="2014-03-04T16:30:32Z" id="36643413">Out of curiosity, why did you choose t-digest and not QDigest?  Did you do extensive comparison and concluded that t-digest has both lower memory footprint, speed, and accuracy?
</comment><comment author="jpountz" created="2014-03-04T16:54:19Z" id="36646640">The two main reasons why we did not consider q-digest are that it does not work with doubles and looked less accurate than t-digest. The [t-digest paper](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) also gives interesting explanations why t-digest performs better than q-digest.
</comment><comment author="otisg" created="2014-03-04T17:06:19Z" id="36648050">Thanks Adrien!  Sounds like you didn't actually run comparison tests, right? (not "blaming", just trying to understand).  @tdunning may have more speed improvements in t-digest, a little bird told me...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count distinct by field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5322</link><project id="" key="" /><description>Hi i have a large index of tweets and need to know the numbers of distinct authors of a selected tweets (sql: count(distinct user) ), e.g: I make a query fetching facets of tweets that use #elastic and need to know how many different users wrote on it. thank you this functionality is the only one think that mysql get me and elastic not on this project
</description><key id="28632757">5322</key><summary>Count distinct by field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsoler86</reporter><labels /><created>2014-03-03T16:09:32Z</created><updated>2014-03-13T18:59:54Z</updated><resolved>2014-03-13T18:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-03T17:42:32Z" id="36536144">This is a feature that we plan to add to the aggregations framework, but it is taking some time because there is some infrastructure that we want to setup in order to be able to implement such an aggregation efficiently. Typically, there are some algorithms that only require hashes of the values in order to estimate the number of unique values and this is something we could leverage (by pre-computing hashes instead of computing them on the fly) to make this aggregation fast.
</comment><comment author="davidronk" created="2014-03-11T19:53:30Z" id="37341829">We could also use a "distinct" feature.  We currently use the [elasticsearch-timefacets-plugin](https://github.com/crate/elasticsearch-timefacets-plugin) to do a distinct date histogram (but we are restricted to a fairly old ES version and would like to upgrade).  Could there be a "distinct_value_count" added to the aggregation framework (or something similar)?
</comment><comment author="jpountz" created="2014-03-11T20:31:47Z" id="37345950">We definitely have plans for this. Since last time I left a comment on this issue, we started doing experiments with an aggregation to compute unique counts under the [feature/cardinality_aggregation](https://github.com/elasticsearch/elasticsearch/tree/feature/cardinality_aggregation) branch. This is still work in progress and I can't give you any release date for this feature, but we are making progress!
</comment><comment author="davidronk" created="2014-03-12T12:59:37Z" id="37404992">Awesome, thanks for the update!  That will be very helpful!
</comment><comment author="jpountz" created="2014-03-13T18:29:58Z" id="37569691">Good news, this was just pushed and will be available in Elasticsearch 1.1, see #5426 !
</comment><comment author="davidronk" created="2014-03-13T18:55:56Z" id="37572696">:+1: 
</comment><comment author="jsoler86" created="2014-03-13T18:59:54Z" id="37573174">Great, Thank you

2014-03-13 14:26 GMT-04:30 David Ronk notifications@github.com:

&gt; [image: :+1:]
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5322#issuecomment-37572696
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for sorting buckets based on single-bucket sub aggregation...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5321</link><project id="" key="" /><description>...s (based on their doc_count)

 Closes #5253
</description><key id="28627852">5321</key><summary>Added support for sorting buckets based on single-bucket sub aggregation...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2014-03-03T15:05:47Z</created><updated>2014-07-15T07:14:12Z</updated><resolved>2014-03-05T00:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-03T15:14:32Z" id="36518986">The change looks good to me. For consistency, should we have the same feature on histograms?
</comment><comment author="uboness" created="2014-03-03T15:15:36Z" id="36519104">oh... good one... we probably should, will add it
</comment><comment author="uboness" created="2014-03-05T00:47:51Z" id="36698134">replaced by https://github.com/elasticsearch/elasticsearch/pull/5340
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5320</link><project id="" key="" /><description /><key id="28627824">5320</key><summary>1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjharvey</reporter><labels /><created>2014-03-03T15:05:21Z</created><updated>2014-07-16T21:47:58Z</updated><resolved>2014-03-04T17:52:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-03-04T17:52:50Z" id="36653089">It seems that this PR was opened by accident. Closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force merges to not happen when indexing a doc / flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5319</link><project id="" key="" /><description>Today, even though our merge policy doesn't return new merge specs on SEGMENT_FLUSH, merge on the scheduler is still called on flush time, and can cause merges to stall indexing during merges. Both for the concurrent merge scheduler (the default) and the serial merge scheduler. This behavior become worse when throttling kicks in (today at 20mb per sec).

 In order to solve it (outside of Lucene for now), we wrap the merge scheduler with an EnableMergeScheduler, where, on the thread level, using a thread local, the call to merge can be enabled/disabled.

 A Merges helper class is added where all explicit merges operations should go through. If the scheduler is the enabled one, it will enable merges before calling the relevant explicit method call. In order to make sure Merges is the only class that calls the explicit merge calls, the IW variant of them is added to the forbidden APIs list.
</description><key id="28627497">5319</key><summary>Force merges to not happen when indexing a doc / flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>enhancement</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-03T15:01:03Z</created><updated>2015-06-07T15:06:46Z</updated><resolved>2014-03-05T12:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-03T23:31:26Z" id="36574664">The change looks good.
</comment><comment author="s1monw" created="2014-03-04T09:09:52Z" id="36604739">I left some comments, IMO this is a bugfix and should go to all branches including `0.90` and `1.0`... thoughts?
</comment><comment author="jpountz" created="2014-03-04T09:20:18Z" id="36605445">agreed on the bug-fix nature of this change, +1 to pushing it on all branches
</comment><comment author="kimchy" created="2014-03-05T12:55:51Z" id="36739185">pushed the change to 0.90, 1.0, 1.1 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make BytesStreamOutput more efficient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5318</link><project id="" key="" /><description>Makes BytesStreamOutput more efficient by introducing internal paging via a (currently private) non-recycling PageCacheRecycler. This change brought to light a bug in FsTranslog, which used seek() incorrectly/ambiguously; this is fixed so that it works with both the old and new implementation.

Fix for bug #5159
</description><key id="28627253">5318</key><summary>Make BytesStreamOutput more efficient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels /><created>2014-03-03T14:58:03Z</created><updated>2014-06-28T00:27:33Z</updated><resolved>2014-03-04T11:41:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-03T16:57:34Z" id="36531195">Part of the code looks very similar to what we have in `BigByteArray` so I'm wondering if we could somehow share some code between these classes?
</comment><comment author="jpountz" created="2014-03-03T17:25:42Z" id="36534371">I just discussed with @kimchy about this change and it might make sense to use `BigArrays` internally in order to implement paging, this should make things easier to implement as it already has the logic to do writes that may span across pages, and there is a `BigArrays.NON_RECYCLING_INSTANCE` static variable that would remove the need for `NonePageCacheRecyclerService`. @hhoffstaette what do you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: Log stdout output into file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5317</link><project id="" key="" /><description>Until stdout was ignored during start up of our packages. This could
result in problems when specyfying something like an invalid heap size
(2gb instead of 2g) as this was not logged anywhere.

For init.d style startup, errors are now logged into
/var/log/elasticsearch/elasticsearch-stdout.log

For systemd startup, one can now use journalctl to see these errors.

This PR is based on #4429 (which only supported the debian package)
</description><key id="28626570">5317</key><summary>Packaging: Log stdout output into file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2014-03-03T14:49:15Z</created><updated>2015-05-29T07:35:12Z</updated><resolved>2015-05-29T07:35:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-03-03T15:09:22Z" id="36518418">Yes please
</comment><comment author="electrical" created="2014-03-03T15:10:55Z" id="36518594">Looks good to me :-)
</comment><comment author="recastrodiaz" created="2014-03-03T16:49:38Z" id="36530199">Thanks for looking into this. 

There might be one issue though. I've noticed that the init.d log file gets quite big over time. 
What I think is happening is that the ES logs are redirected to console and thus are written both to the init.d log and the ES user log, which is is probably not the desired effect.

Maybe changing this line to file only will fix it? Although this might not be the best setting for development: https://github.com/elasticsearch/elasticsearch/blob/master/config/logging.yml#L3
</comment><comment author="paravoid" created="2014-03-04T09:46:34Z" id="36607338">The whole point of daemonizing is that stdout &amp; stderr go to /dev/null. That's a basic Unix principle :) Spawning a shell that will stay running sounds like a very bad idea to me and very very dirty. Plus, this logfile will be ever-growing and logrotate wouldn't work in this case as e.g. a SIGHUP to bash would result in... no changes :)

$DAEMON should log by itself, using whatever mechanisms are used for the rest of the logs (log4j, presumably). Anything else would be a gross hack, IMHO.
</comment><comment author="nik9000" created="2014-03-04T11:22:30Z" id="36614793">The problem is the JVM which will can spit out stuff on stdout and stderr and we can't stop it.  It won't do it every time,just if there is a gross error that prevents it from starting properly.   

&gt; On Mar 4, 2014, at 4:46 AM, Faidon Liambotis notifications@github.com wrote:
&gt; 
&gt; The whole point of daemonizing is that stdout &amp; stderr go to /dev/null. That's a basic Unix principle :) Spawning a shell that will stay running sounds like a very bad idea to me and very very dirty. Plus, this logfile will be ever-growing and logrotate wouldn't work in this case as e.g. a SIGHUP to bash would result in... no changes :)
&gt; 
&gt; $DAEMON should log by itself, using whatever mechanisms are used for the rest of the logs (log4j, presumably). Anything else would be a gross hack, IMHO.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="shikhar" created="2014-07-15T22:13:01Z" id="49100159">@paravoid 

&gt;  Plus, this logfile will be ever-growing and logrotate wouldn't work in this case as e.g. a SIGHUP to bash would result in... no changes :)

Logrotate with copytruncate should work

Also it's not just startup fails, but even JVM crashes (they happen!) that can get logged here.

If you want to do thread-dump the "traditional way" with sending kill -QUIT, you'd expect the output on wherever stdout is.

There are a ton of reasons to retain stdout &amp; stderr...
</comment><comment author="shikhar" created="2014-07-15T22:16:41Z" id="49100491">@spinscale 

a) `bin/elasticsearch` is missing in this PR

b) can we have the option to direct stderr to a separate file than stdout? `$STDERR_LOG_FILE`
</comment><comment author="spinscale" created="2015-05-29T07:35:10Z" id="106727085">Closing this one for now. It's stale, parts are deprecated, other parts are not correctly implemented (no logrotation, file can outgrew, pipes confusion, etc), we will need to come up with a better solution here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_distance aggregation does not return unit as documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5316</link><project id="" key="" /><description>The `geo_distance` aggregation is documented to return bucket items in the form of:

``` json
{
          "unit": "km",
          "to": 100.0,
          "doc_count": 3
}
```

But actually returns the aggregations in the form of:

``` json
{
    "key" : "1.0-100.0",
    "from" : 1.0,
    "to" : 100.0,
    "doc_count" : 1
}
```

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-geodistance-aggregation.html
</description><key id="28618744">5316</key><summary>geo_distance aggregation does not return unit as documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2014-03-03T12:36:29Z</created><updated>2014-12-23T10:47:04Z</updated><resolved>2014-12-23T10:46:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-03-03T14:40:00Z" id="36515460">indeed... will fix
</comment><comment author="colings86" created="2014-12-23T10:46:59Z" id="67939670">This has been fixed int he documentation so closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch issue-- failed to start shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5315</link><project id="" key="" /><description>Hi,

I'm using elastic search 1.0 and logstash 1.3.3 with redis_version:2.4.10
When ever I'm trying to start logstash indexer it's giving me below error ..."failed to start shard" I deleted all the indices from elastic search before start.

[2014-03-03 17:04:33,178][WARN ][transport.netty          ] [Mantra] Message not fully read (response) for [2114] handler future(org.elasticsearch.indices.recovery.RecoveryTarget$4@14487c8), error [true], resetting
[2014-03-03 17:04:33,183][WARN ][indices.cluster          ] [Mantra] [logstash-2014.03.03][3] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [logstash-2014.03.03][3]: Recovery failed from [Brutacus][L0THepPDR6Cmx7O2jqVLdg][cloudclient1.aricent.com][inet[/10.203.251.143:9300]] into [Mantra][cKMntkkDT5Ws8g8_yeoTUg][cloudclient.aricent.com][inet[/10.203.251.142:9300]]
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:303)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$300(RecoveryTarget.java:65)
        at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:171)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
##         at java.lang.Thread.run(Thread.java:662)
## Please help to resolve the issue.

Thanks,
Subhadip
</description><key id="28616191">5315</key><summary>elasticsearch issue-- failed to start shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ibagui</reporter><labels /><created>2014-03-03T11:48:18Z</created><updated>2014-04-04T17:06:15Z</updated><resolved>2014-04-04T17:06:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T17:06:15Z" id="39587862">Hi @ibagui I think you'd get proper answers if you send this type of questions to our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch). Anyways, looks like you are using the transport layer instead with a version of logstash that doesn't support 1.0 yet. I would switch to http output then. Can you followup on the mailing list if the problem persists please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Requests.deleteByQueryRequest(...).source(...) is not fluent anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5314</link><project id="" key="" /><description>Previously in 0.9.7, you could write :

``` java
Requests.deleteByQueryRequest("...").types("...").query(QueryBuilders.matchAllQuery())
```

Now, you must write : 

``` java
QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();

querySourceBuilder.setQuery(QueryBuilders.matchAllQuery());

Requests.deleteByQueryRequest("...").types("...").source(querySourceBuilder);
```

The fact that the API was not fluent anymore (in this case), was it intentional ?
</description><key id="28609050">5314</key><summary>Requests.deleteByQueryRequest(...).source(...) is not fluent anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lpouget</reporter><labels /><created>2014-03-03T09:43:26Z</created><updated>2014-03-05T12:23:10Z</updated><resolved>2014-03-05T12:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-03T13:29:29Z" id="36509646">This was intentional, the query used to be the main part of the request body, but in &gt;= 1.0 the query is now under the top level json field `query`, this was done in order to be be consistent with the search api. The `QuerySourceBuilder` encapsulates that now.

In the `DeleteByQueryRequestBuilder` you can still use the `setQuery` method, which under the hood uses the `QuerySourceBuilder` class.
</comment><comment author="lpouget" created="2014-03-04T13:33:58Z" id="36623751">Ok so there are no more way to do this in fluent way (or I missed something ...).
</comment><comment author="martijnvg" created="2014-03-05T09:00:07Z" id="36721853">The `QuerySourceBuilder` is also fluent so you could also code in the following way:

``` java
Requests.deleteByQueryRequest("index").source(new QuerySourceBuilder().setQuery(QueryBuilders.matchAllQuery()))
```

However you can use the `DeleteByQueryRequestBuilder` and its `setQuery` method to also code in a fluent way, which is shorter than the above since you don't need to use  `QuerySourceBuilder` in your code:

``` java
client.prepareDeleteByQuery("index").setQuery(QueryBuilders.matchAllQuery()).get();
```
</comment><comment author="lpouget" created="2014-03-05T12:23:10Z" id="36736942">Ok thanks for your response !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es1.0.1 mapping config not update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5313</link><project id="" key="" /><description> in config folder ,if the index exists ,I add a new mapping file into  the mappings folder's ,the new mapping json file not effect ,I think this bug ever happen in es0.90.10 ,in es0.90.11 fixed, only delete index and new index once ,the new mapping json file effect
</description><key id="28598629">5313</key><summary>es1.0.1 mapping config not update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wszyquan</reporter><labels /><created>2014-03-03T04:36:34Z</created><updated>2014-07-08T19:16:18Z</updated><resolved>2014-07-08T19:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-03T07:51:24Z" id="36487891">can you be more specific about what exacty is working and what is not? Can you provide an example like stated in http://www.elasticsearch.org/help - so we can have a closer look and understand what you did.

Thanks a lot!
</comment><comment author="wszyquan" created="2014-03-03T12:24:46Z" id="36505339">I am sorroy ,above description is unclear
this is my test case ,there is nothing index.then I try following step
1&#12289;The folder hierarchy is:  ES_HOME\config\mappings\test_index\test_type.json
2&#12289;then start es.
3&#12289;create index [test_index] by rest url . then the type [test_type] will be create auto;

now I want to create other type in this index ,test_2_type,
so I create new Mapping json file test_2_type.json and put it into folder test_index,
the folder hierarchy is 
ES_HOME\config\mappings\test_index\test_type.json
ES_HOME\config\mappings\test_index\test_2_type.json

I hope when I insert data into test_2_type, the Mapping file test_2_type.json can  be apply.
but it not work ,the test_2_type  is system dynamic field mapping . 

I remember in es 0.90.10 when I put new Json file into Mapping folder ,the Mapping not work , only I delete the index ,and create new index with same name ,then the Mapping file work. but in es 0.90.11 fixed this bug.

this is my mapping json file content

---

{
   "log365d_visitor_type" : {
      "_all" : {
         "enabled" : false
      },
      "_source" : {
         "enabled" : true
      },
      "dynamic" : "strict",
      "properties" : {
         "_id" : {
            "include_in_all" : "false",
            "index" : "not_analyzed",
            "null_value" : "null",
            "type" : "string"
         },
     "totalCount" : {
            "include_in_all" : "false",
            "index" : "not_analyzed",
            "null_value" : "0",
            "type" : "integer"
         },
         "createDate" : {
            "format" : "yyyy-MM-dd HH:mm:ss",
            "index" : "not_analyzed",
            "type" : "date"
         }
      }
   }
}
</comment><comment author="clintongormley" created="2014-07-08T19:16:18Z" id="48386816">Hi @wszyquan 

The config file is only consulted at index creation time.  Instead of relying on config files, use the API to create mappings directly or use index templates. They are a lot more flexible than having to ensure that the same config files are on all nodes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>./bin/plugin escape JAVA path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5312</link><project id="" key="" /><description>At line 

```
exec $JAVA $JAVA_OPTS -Xmx64m -Xms16m ...
```

there need escape $JAVA path
please someone add instead

```
exec "$JAVA" $JAVA_OPTS -Xmx64m -Xms16m ...
```
</description><key id="28577332">5312</key><summary>./bin/plugin escape JAVA path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">suquant</reporter><labels /><created>2014-03-02T10:56:15Z</created><updated>2014-12-29T13:15:08Z</updated><resolved>2014-12-29T13:15:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T13:15:08Z" id="68256259">This has been done.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel throws ConnectionException on stopping elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5311</link><project id="" key="" /><description>Marvel throws a ConnectionException when closing elasticsearch. It keeps sending data to elasticsearch event though elasticsearch is about to terminate. In this case I'm running elasticsearch in foreground mode and closing it with Ctrl-C in bash.

Probably elasticsearch should inform its plugins about the termination, so that the plugins can finish operating. 

Context: elasticsearch 1.0.1
              marvel 1.0.2

Stacktrace:
^C[2014-03-02 00:15:26,288][INFO ][node                     ] [Puck] stopping ...
[2014-03-02 00:15:26,317][ERROR][marvel.agent.exporter    ] error sending data
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:763)
    at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.sendCloseExportingConnection(ESExporter.java:232)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:252)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportEvents(ESExporter.java:166)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportEvents(AgentService.java:230)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:192)
    at java.lang.Thread.run(Thread.java:744)
[2014-03-02 00:15:26,319][INFO ][node                     ] [Puck] stopped
[2014-03-02 00:15:26,319][INFO ][node                     ] [Puck] closing ...
[2014-03-02 00:15:26,325][INFO ][node                     ] [Puck] closed
</description><key id="28568444">5311</key><summary>Marvel throws ConnectionException on stopping elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paweloque</reporter><labels /><created>2014-03-01T23:20:56Z</created><updated>2014-03-03T15:31:15Z</updated><resolved>2014-03-03T15:31:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-03T15:31:15Z" id="36520851">At the moment ES does notify it's plugins of closing to give a chance to clean up. That is Marvel does and it tries to send the last queued up events - which is what you want to do in the case that data is sent to another ES cluster (our recommended production set up). This does generate an error if the data is sent to the very same node because as it is refusing new connections. I have already a note to be smarter about it but the solution is not trivial.

I'm closing the issue because it's expected (though not ideal). Thx for bringing it up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request - "type" templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5310</link><project id="" key="" /><description>Currently it's possible to define an index level template which gets applied when a new index is created, and a _default_ template which is applied to _all_ types.

We see a need for _type_ templates - this is a template that gets applies when a new type is created - even if it's not in the context of a new index.

I'll explain the use case: we have several types in the system, each has a set of fields that needs to be defined every time the type is created.  
We define those type in a template that gets applied when indexes are created. But sometimes we need to delete a type (with all its data) and recreate it. When we do that our template is not applied - since it's not a new index - and we need to explicitly put the mapping for the type. It would be much more consistent if we can define a template for the type that would be applied whenever the type is created. 
</description><key id="28563768">5310</key><summary>Feature request - "type" templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels /><created>2014-03-01T19:24:42Z</created><updated>2014-12-29T13:14:03Z</updated><resolved>2014-12-29T13:14:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T13:14:03Z" id="68256205">Hi @rore 

Sorry it has taken a while to get to this one.  With https://github.com/elasticsearch/elasticsearch/issues/8877 we are planning on removing the ability to delete types, and with #8871 types will essentially become collections of fields whose definition (internally) lives at the index level.  These changes remove the need for type templates.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc updates for date histogram interval </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5309</link><project id="" key="" /><description>closes  #5308 
</description><key id="28561429">5309</key><summary>doc updates for date histogram interval </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rphadake</reporter><labels /><created>2014-03-01T17:24:09Z</created><updated>2014-07-16T21:47:59Z</updated><resolved>2014-03-14T17:57:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rphadake" created="2014-03-14T14:58:09Z" id="37655913">please let me know if anything is pending.
</comment><comment author="jpountz" created="2014-03-14T17:08:06Z" id="37671747">It looks good! Could you please sign the [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that we can get this change in?
</comment><comment author="spinscale" created="2014-03-14T17:19:04Z" id="37672974">@rphadake you already told me that you have signed, but somehow we dont find you in our list, sorry for the confusion. please bear with us, while we try to find your sign :-)
</comment><comment author="jpountz" created="2014-03-14T17:57:39Z" id="37677191">Found it, and merged the PR. Thanks @rphadake !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc update: date histogram interval corrections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5308</link><project id="" key="" /><description>As per DateHistogramParser ctor 
-month interval in aggregations
-second interval in facets

Let me see if I can submit doc changes request
</description><key id="28560647">5308</key><summary>doc update: date histogram interval corrections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rphadake</reporter><labels><label>docs</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-03-01T16:45:35Z</created><updated>2014-03-21T10:06:53Z</updated><resolved>2014-03-14T17:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Report timed out shards next to failed shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5307</link><project id="" key="" /><description>It'd be useful to get a count of the shards that timed out next to the shards that succeeded and the shards that failed.
</description><key id="28546470">5307</key><summary>Report timed out shards next to failed shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-03-01T01:16:18Z</created><updated>2014-03-02T18:59:05Z</updated><resolved>2014-03-02T18:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-02T18:59:05Z" id="36462686">I suppose this isn't really that important if I can just check timed_out to know if _some_ shards timed out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations that can count only parent documents with matching children documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5306</link><project id="" key="" /><description>Feature request: 
Aggregations that can count only parent documents with matching children documents. 

I've been working on a BI system with ES 0.90 and we needed count "users" which have certain attributes, for instance let's say gender and star sign. A user is a parent-level document and the attributes are child documents. 

From the sample above, we were doing so by creating a query for each combination of male / female and the star signs and querying individually, as one can imagine, this was slow, but the results are exactly what we want. We could run this in roughly 2 minutes.

We considered using the msearch query to get these results in a single query and we ended up with something similar to this: https://gist.github.com/chaos-generator/9133118
The sample above runs in 40 seconds give or take.

And along came elastic search 1.0.0 and now we have aggregations, so we simplified our query to this: https://gist.github.com/chaos-generator/9133139
This runs lightning fast and we get the results in 200ms on average, which is ideal for us, BUT we get the total number of documents with the attributes, rather than the count on the parent documents.

Our problem, as you can see in the msearch gist, is that we have a parent level document and child documents, which would only be updated if another document with the exact same attributes came in, this means that a parent level user document can have three child documents that will have gender and star sign, but I only want to count the parent document, rather than each individual child document.

As we don't know in advance the attributes our users will be searching, we cannot use a script in index time to help us do this aggregation. We tried to use a script in search time like this:  https://gist.github.com/chaos-generator/9133321 , but it didn't work as we wanted too:

You can use this gist to simulate the issue we have: https://gist.github.com/chaos-generator/9143655
</description><key id="28545228">5306</key><summary>Aggregations that can count only parent documents with matching children documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">chaos-generator</reporter><labels><label>:Parent/Child</label><label>high hanging fruit</label><label>stalled</label></labels><created>2014-03-01T00:42:29Z</created><updated>2016-11-06T15:06:05Z</updated><resolved>2016-11-06T15:06:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmenegatti" created="2014-11-24T22:54:50Z" id="64280531">Hi @martijnvg and @clintongormley, please, any update regarding the possibility to implement this feature? Is it feasible?
We strongly need this feature as we currently duplicate all the child data into the parent, just to be able to perform the parent aggregation.
</comment><comment author="clintongormley" created="2014-11-25T17:22:55Z" id="64437043">Hi @gmenegatti 

Currently the `parent` aggregation is stalled - we ran into a significant barrier to implementation.  It may be that we end up reimplementing parent-child completely using a different design, so I'm going to leave this ticket open so that we can revisit it later.
</comment><comment author="clintongormley" created="2014-12-29T13:10:47Z" id="68256022">Hi @chaos-generator 

While the `parent` aggregation is  not supported, you can use the `cardinality` aggregation on the `_parent` field to get an estimated count of matching parents.  Here's an example based on the gist you provided:

```
PUT /my_test/user/1
{
  "user_id": "1",
  "user_name": "John Smith"
}

PUT /my_test/personal_data/_mapping
{
  "personal_data": {
    "_parent": {
      "type": "user"
    }
  }
}

PUT /my_test/personal_data/2?parent=1
{
  "gender": "male",
  "sign": "LEO",
  "DOB": "1979-01-01"
}

PUT /my_test/personal_data/3?parent=1
{
  "gender": "male",
  "sign": "LEO"
}

PUT /my_test/user/4
{
  "user_id": "1",
  "user_name": "Jane Smith"
}

PUT /my_test/personal_data/5?parent=4
{
  "gender": "female",
  "sign": "LEO",
  "DOB": "1979-01-01"
}

PUT /my_test/personal_data/6?parent=4
{
  "gender": "female",
  "sign": "LEO"
}

GET /my_test/_msearch?pretty=true
{}
{"size":0,"query":{"bool":{"must":[{"has_child":{"type":"personal_data","query":{"match":{"gender":"male"}}}},{"has_child":{"type":"personal_data","query":{"match":{"sign":"LEO"}}}}]}}}
{}
{"size":0,"query":{"bool":{"must":[{"has_child":{"type":"personal_data","query":{"match":{"gender":"female"}}}},{"has_child":{"type":"personal_data","query":{"match":{"sign":"LEO"}}}}]}}}

GET /_search?search_type=count
{
  "aggs": {
    "gender": {
      "terms": {
        "field": "gender"
      },
      "aggs": {
        "sign": {
          "terms": {
            "field": "sign"
          },
          "aggs": {
            "parents": {
              "cardinality": {
                "field": "_parent"
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="gmenegatti" created="2015-06-01T18:06:57Z" id="107657426">Hi @clintongormley 
Now that the Parent/Child code was refactored, do you think this can move forward or it will still be stalled?
Thanks
</comment><comment author="clintongormley" created="2015-06-02T18:24:10Z" id="108043629">Hi @gmenegatti 

I'm afraid that this aggregator still suffers from the same problem as before the parent/child refactoring.  
</comment><comment author="clintongormley" created="2016-11-06T15:06:05Z" id="258686740">It is unlikely that we're going to be able to implement a parent aggregation, which steps up from the child aggregation, so I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logging configuration is not documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5305</link><project id="" key="" /><description>There is almost no documentation for the logging.yml configuration file. Changing the logging level is simple enough -- but as someone unfamiliar with log4j, I have absolutely no idea what the options in the logging.yml file do or how to configure elasticsearch to not rotate it's own logs so that I can use a standard system service to manage log rotation.  I would rather not have to read the source code in order to understand how to configure the logging mechanism.
</description><key id="28541461">5305</key><summary>Logging configuration is not documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jesusaurus</reporter><labels><label>docs</label></labels><created>2014-02-28T23:23:35Z</created><updated>2014-11-13T10:10:08Z</updated><resolved>2014-11-13T10:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms aggs: only use ordinals on low-cardinality fields by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5304</link><project id="" key="" /><description>Ordinals tend to be slower and more wasteful memory-wise on high-cardinality fields.

Close #5303
</description><key id="28534981">5304</key><summary>Terms aggs: only use ordinals on low-cardinality fields by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T21:34:58Z</created><updated>2015-06-07T15:06:51Z</updated><resolved>2014-03-04T08:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-03-02T16:00:01Z" id="36457622">+1 LGTM
</comment><comment author="uboness" created="2014-03-03T15:19:48Z" id="36519580">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms aggregations: don't use ordinals on high-cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5303</link><project id="" key="" /><description>Ordinals help performance a lot on low-cardinality fields since they help avoid costly string comparisons. However, when the field has a high cardinality, bytes need to be compared anyway and ordinals only add overhead.

We should probably have a basic heuristic to disable ordinals on fields that have a very high cardinality.

This would of course only apply to defaults and it would still be possible to use ordinals on high-cardinality fields by passing `execution_hint=ordinals` to the aggregator parser.
</description><key id="28534370">5303</key><summary>Terms aggregations: don't use ordinals on high-cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T21:25:39Z</created><updated>2014-03-21T10:06:34Z</updated><resolved>2014-03-04T08:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES cluster failure when operating behind NAT (docker in EC2 VPC)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5302</link><project id="" key="" /><description>I'm running ES inside docker containers on EC2. Client nodes are unable to connect to the master. I can telnet to ports 9200 and 9300 from all nodes to all other nodes. My configuration on the nodes is:

cluster.name: docker_ec2_es
node.name: es.${zone}${num}.docker.fqdn.com
discovery.zen.ping.multicast.enabled: false
path.data: /var/lib/elasticsearch
discovery.zen.ping.unicast.hosts: [list of internal EC2 IP addresses]

Initially, the cluster appeared to be created ok. After creating an index, the nodes dropped out, and they each became split.

Each node then repeats the following log.

[2014-02-28 05:59:40,269][INFO ][discovery.zen            ] [es.a1.docker.fqdn.com] failed to send join request to master [[es.b2.docker.fqdn.com][26sjNF6TRPWuYkGLbDeLZQ][inet[/172.31.3.2:9300]]], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
</description><key id="28531777">5302</key><summary>ES cluster failure when operating behind NAT (docker in EC2 VPC)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsturnbull</reporter><labels><label>feedback_needed</label></labels><created>2014-02-28T20:45:14Z</created><updated>2014-10-17T13:05:13Z</updated><resolved>2014-10-17T13:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dsturnbull" created="2014-03-04T00:57:54Z" id="36580626">We think this is because of jumbo frames on the m3.xlarge instances. Will close this soon if this turns out to be the case.
</comment><comment author="mccraigmccraig" created="2014-08-18T15:16:43Z" id="52506681">i'm seeing the same behaviour : nodes running in a docker container timeout while attempting to contact (non-docker) nodes on the unicast zen discovery gossip-router list

in addition, i see these errors logged on a gossip router node :

[2014-08-18 15:03:24,281][WARN ][transport.netty          ] [Jigsaw] exception caught on transport layer [[id: 0x4f2c96d7]], closing connection
java.net.SocketException: Network is unreachable
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

which i would guess indicates that the master is trying to reach the slave on it's guest ip-address/port rather than it's host ip-address/port

i'm using 
- ubuntu 14.01
- ES 1.3.2 from official deb repos
- docker.io 0.9.1~dfsg1-2 from official deb repos
- AWS t2.micro for ES master (straight on ubuntu)
- AWS m3.medium for slaves (in docker container)
- ES dockerfile from : https://github.com/dockerfile/elasticsearch

ES master has config :

cluster.name: es-01
node.master: true
node.data: false
discovery.zen.minimum_master_nodes: 1
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["192.168.0.5"]

ES slave has config :

cluster.name: es-01
node.master: false
node.data: true
discovery.zen.minimum_master_nodes: 1
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["192.168.0.5"]
</comment><comment author="clintongormley" created="2014-10-17T09:31:40Z" id="59489299">@drewr @joehillen @ESamir @electrical Any ideas about networking issues with Docker and EC2?
</comment><comment author="ESamir" created="2014-10-17T10:28:37Z" id="59494899"> @dsturnbull  and @mccraigmccraig   are different issue 

 @dsturnbull  it is  possible the issue is caused by Jumbo frames&#8194; causing&#8194; network latency this can happened on low bandwidth links .

@mccraigmccraig  the issue is a configuration issue  , How did you configure the connection to Docker host from container , if you didn't check this discussion  https://github.com/docker/docker/issues/8395
</comment><comment author="electrical" created="2014-10-17T11:07:42Z" id="59498247">The thing with docker containers is that by default it exposes the port on the bridge interface from docker.
You can setup it in a way that it forwards that port to a random port on the host interface.

as an example:
Bridge docker0 has range 192.168.1.0/24

Instance-A has IP 192.168.1.10
Instance-B has IP 192.168.1.11

Both instances have port 9200 open on that IP.
If you want to publish the ports they get assigned to either a random port number on the host interface or you could define the port. ( see -p or -P options on `docker run --help` )

An other option is to set the network ( option --net=host ) of the containers to the host network directly.
</comment><comment author="trampoline" created="2014-10-17T12:19:03Z" id="59504440">@ESamir it's a while back, but iirc :
- i let docker manage a couple of ports, so ports 9200 and 9300 on the ES guest were routed from random ports on the host
- the difficulty was how ES instances should communicate their visible (host) ip and port : there is the network.bind_host and network.publish_host config params for the ip address, but there aren't equivalent bind/publish params for tcp and http ports
- which means either --net=host or the ports have to be mapped to the same port numbers on the host

i went with --net=host

(oops. wrong github login. i am also @mccraigmccraig)
</comment><comment author="clintongormley" created="2014-10-17T12:35:04Z" id="59506016">@mccraigmccraig I've just looked through the code and there are settings for http.bind/publish_host, they're just not documented.  I'll fix that.
</comment><comment author="mccraigmccraig" created="2014-10-17T12:38:12Z" id="59506307">@clintongormley i think it was a lack of bind/publish settings for ports that were causing my problem... (the http.bind/publish_host is documented in the annotated elasticsearch.yml config file i've got)
</comment><comment author="clintongormley" created="2014-10-17T13:05:13Z" id="59509350">@mccraigmccraig gotcha. I've open #8137 for that, and I've documented the http.*host settings.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix explain backwards compatibility for cluster state REST interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5301</link><project id="" key="" /><description /><key id="28513285">5301</key><summary>Fix explain backwards compatibility for cluster state REST interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-02-28T16:24:45Z</created><updated>2014-06-26T09:25:36Z</updated><resolved>2014-02-28T18:11:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-28T16:25:51Z" id="36367549">+1
</comment><comment author="dakrone" created="2014-02-28T18:11:55Z" id="36378148">Merged to 1.x in a14af38bcc5d9fab9abd25e5afc52983d2053b25
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse pages more agressively in BigArrays.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5300</link><project id="" key="" /><description>Pages are now going to be reused when the size of the array is greater than
${page_size}/2 (as opposed to ${page_size} currently).

Close #5299
</description><key id="28510699">5300</key><summary>Reuse pages more agressively in BigArrays.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T15:52:37Z</created><updated>2015-06-07T15:07:19Z</updated><resolved>2014-02-28T20:31:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-28T16:01:47Z" id="36364891">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BigArrays: reuse pages more agressively</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5299</link><project id="" key="" /><description>Current implementation starts using paged arrays when the size of the array is
greater than the size of a page. This means that if an array has a length of
${page_size}+1, it will use two pages, wasting about half of the memory.

I think this behavior should be symetric when the size is less than the page
size but higher than half of the page size. This means that we would start
reusing memory at ${page_size}/2 instead of ${page_size}.
</description><key id="28509958">5299</key><summary>BigArrays: reuse pages more agressively</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T15:43:26Z</created><updated>2015-06-07T15:06:58Z</updated><resolved>2014-02-28T20:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-28T15:46:17Z" id="36363273">Makes sense to me though it might be worth running performance numbers on it.  It might be that you should kick in even earlier.  I don't have any ideas on how to measure the performance of such a change though.
</comment><comment author="jpountz" created="2014-02-28T15:54:16Z" id="36364133">I thought about making it kick in earlier but then I would be afraid of wasting too much memory. However, since we are wasting 50% of space in the worst case anyway, I thought it would be useful to take advantage of it to reuse pages more agressively.
</comment><comment author="nik9000" created="2014-02-28T16:27:24Z" id="36367763">That's pretty sound logic to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to assume missing field as zero in aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5298</link><project id="" key="" /><description>To keep docs reasonably small we omit fields that has zero value, but when we use `avg` or `extended_stats` aggregation it would be nice to make missing values assumed to be zeroes too.

In example below we have 31 970 816 docs in bucket, but only 7 310 of them have non-zero value.

``` json
{
   "aggregations": {
      "country": {
         "buckets": [
            {
               "key": "RU",
               "doc_count": 31970816,
               "cents": {
                  "count": 7310,
                  "min": 8,
                  "max": 169800,
                  "avg": 514.1964432284542,
                  "sum": 3758776,
                  "sum_of_squares": 60978796462,
                  "variance": 8077434.639111836,
                  "std_deviation": 2842.0827994820693
               }
            }
        ]
    }
}
```

Maybe additional boolean parameter could be introduced for `extended_stats` and `avg` aggregations, like `assume_zeroes`?

cc @uboness
</description><key id="28507563">5298</key><summary>Ability to assume missing field as zero in aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>:Aggregations</label><label>adoptme</label></labels><created>2014-02-28T15:14:41Z</created><updated>2015-05-15T14:34:40Z</updated><resolved>2015-05-15T14:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2014-03-01T00:35:36Z" id="36409275">I was about to sumitba request on this too.
Imi would suggest that anyvaggregation operating on a field should have missing option. If specified, aggregation should accumulate missing values under that value and honor any nested aggregations within. It should never assume any value like 0 since it may clash with actual keys. 

I was planning to show examples of enormous query that is needed for a two lecel aggregation that has to cover all values including missing and other for both levels using missing aggregation.  It can be done but not only the query is huge and highly repetitive the result need to be heavily processed to move second level keys nested under missing agg into the first level buckets.

Please please do implement missing as an option in all bucketing aggs!

I am not even asking to have an option to also aggregare other - keys that were not used due to size parameter although it would be veru useful :-)
</comment><comment author="jpountz" created="2014-09-05T11:03:44Z" id="54611852">I agree the behavior feels wrong with the `avg` or `stats` aggregations. Maybe we could support a `missing` option like sorting does.
</comment><comment author="jpountz" created="2015-05-15T14:34:39Z" id="102415254">Closed through https://github.com/elastic/elasticsearch/pull/11042. Most aggs now have a `missing` option that allows to configure the value to consider when a document has no values.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add randomized plugin isolation to testing infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5297</link><project id="" key="" /><description>Since we allow plugins to be isolated or not, we should take advantage of this in our randomized testing infrastructure.

relates to #5296
</description><key id="28506416">5297</key><summary>add randomized plugin isolation to testing infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels /><created>2014-02-28T14:59:37Z</created><updated>2014-07-16T21:48:01Z</updated><resolved>2014-03-11T09:47:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-03-10T20:32:57Z" id="37230521">@spinscale @s1monw

I've changed the code completely and pushed the change to `TestCluster` - it's really minor. Basically if there is no setting for plugins.isolation, a random boolean will be used per node (again, for plugin that do not specify any isolation). I think this will stress better both ES and the plugins themselves.
</comment><comment author="costin" created="2014-03-10T20:34:05Z" id="37230633">@spinscale regarding the use of a constant, since all the other settings are used in string form (as oppose to some main class/interface where they are declared - which I would prefer), I opted for the current style.
</comment><comment author="s1monw" created="2014-03-10T21:35:07Z" id="37237451">@costin I think this can be a one line change - I added some comments on the commit...
</comment><comment author="costin" created="2014-03-11T09:43:27Z" id="37277817">@s1monw I see. Thanks - updated the commit.
</comment><comment author="s1monw" created="2014-03-11T09:44:02Z" id="37277866">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce randomized plugin isolation to test infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5296</link><project id="" key="" /><description>Since we allow plugins to be isolated or not, we should take advantage of this in our randomized testing infrastructure.
</description><key id="28506289">5296</key><summary>Introduce randomized plugin isolation to test infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>test</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T14:57:54Z</created><updated>2015-06-07T15:07:34Z</updated><resolved>2014-03-11T09:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Run script on insertion through Index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5295</link><project id="" key="" /><description>Is it possible to specify a script be executed when inserting a document into ElasticSearch using its Index API? This functionality exists when updating an existing document with new information using its Update API, by passing in a script attribute in the HTTP request body. I think it would be useful too in the Index API because perhaps there are some fields the user wants to be auto-calculated and populated during insertion, without having to send an additional Update request after the insertion to have the script be executed.

SOF reference: http://stackoverflow.com/questions/22098314/elasticsearch-run-script-on-document-insertion-insert-api
</description><key id="28505675">5295</key><summary>Run script on insertion through Index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ecbrodie</reporter><labels /><created>2014-02-28T14:48:35Z</created><updated>2014-12-29T12:59:37Z</updated><resolved>2014-12-29T12:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T12:59:37Z" id="68255340">This has been implemented: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-transform.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an option to force _optimize operations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5294</link><project id="" key="" /><description>When forced, the index will be merged even if it contains a single segment with
no deletions.

Close #5243
</description><key id="28505257">5294</key><summary>Add an option to force _optimize operations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-02-28T14:42:35Z</created><updated>2014-06-18T13:08:11Z</updated><resolved>2014-03-14T17:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-28T18:48:39Z" id="36381701">@kimchy I added a new commit that addresses your comments.
</comment><comment author="kimchy" created="2014-02-28T19:59:43Z" id="36388541">looks good, the only thing left that I see is the fact that when setting the force flag on the merge policy, it affects other potential merges that will kick in concurrently. I know the thread local flag is not the cleanest solution, but I think its better compared to the current side affect logic?
</comment><comment author="jpountz" created="2014-02-28T20:48:44Z" id="36392881">@kimchy just pushed a new commit
</comment><comment author="kimchy" created="2014-03-06T14:58:33Z" id="36895489">LGTM!
</comment><comment author="clintongormley" created="2014-03-06T17:51:00Z" id="36915571">Don't forget to update the rest-spec ;)
</comment><comment author="s1monw" created="2014-03-14T13:02:44Z" id="37644239">I left a suggestion for the force option on the IW level other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added SLES11 SP3 Init Script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5293</link><project id="" key="" /><description>SUSE Enterprise Server uses still the old init.d and the "daemon" function isn't avaible. There for is "startproc" to start processes as daemons. The "lockfile"-mechanism isn't necessary since rh_status manages that there aren't multple instance of ES.

Tested on Suse Enterprise Server 11 ServicePack 3
</description><key id="28505182">5293</key><summary>Added SLES11 SP3 Init Script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winpat</reporter><labels /><created>2014-02-28T14:41:20Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-07-18T11:04:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-03-03T14:05:11Z" id="36512327">Hi @cyris212 Init file looks great!
To avoid having to make separate packages, do you think its possible to migrate the current sysv style init script and the SLES support into a single one?
</comment><comment author="winpat" created="2014-03-04T06:22:58Z" id="36595598">I can, but I can't test if it works on RHEL... Would you do that for me?
</comment><comment author="electrical" created="2014-03-04T08:57:41Z" id="36603899">@cyris212 Of course. i got a CentOS test Vm i can run it on.
</comment><comment author="electrical" created="2014-03-14T18:26:47Z" id="37680228">Im very sorry i haven't got around to test if after the merge the RHEL/Centos part still works. I will try to do this in the weekend.
</comment><comment author="winpat" created="2014-03-14T20:17:18Z" id="37690717">No problem sir :-) 

Von meinem iPhone gesendet

&gt; Am 14.03.2014 um 19:27 schrieb "Richard Pijnenburg" notifications@github.com:
&gt; 
&gt; Im very sorry i haven't got around to test if after the merge the RHEL/Centos part still works. I will try to do this in the weekend.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="electrical" created="2014-06-13T11:42:54Z" id="46001822">Hi,

Sorry i haven't reviewed this PR. Its been crazy busy.
I've left a single note of something we have concerns about. would you be able to fix that?
I'll do the CentOS testing very soon ( i promise ) and get back to you about this.

Thanks!
</comment><comment author="winpat" created="2014-06-13T11:47:09Z" id="46002116">Hi,

No problem :-)
I don't have enough time today but I will have look at it on monday
</comment><comment author="winpat" created="2014-06-17T08:29:57Z" id="46280480">Init Script is ready for testing on CentOS
</comment><comment author="electrical" created="2014-06-17T08:31:59Z" id="46280675">@winpat thank you so much. Will do testing today and get back to you.
</comment><comment author="electrical" created="2014-06-17T10:14:18Z" id="46289544">I can confirm its working great on CentOs tests.
Could you rebase it into a single commit so we can merge it?
Also, did you sign the CLA?

Cheers.
</comment><comment author="electrical" created="2014-07-18T11:04:13Z" id="49419329">Closing in favor of #6533 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Edit distance allowed values for fuzzy_like_this query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5292</link><project id="" key="" /><description>The doc here says http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#fuzziness

&gt; Note: in all APIs except for the Fuzzy Like This Query, the maximum allowed edit distance is 2.

However when one tries to do this in Java API:

```
fuzzyLikeThisQuery(fieldName).likeText(searchString).fuzziness(Fuzziness.fromEdits(4));
```

An exception is thrown from the Fuzziness.fromEdits(4) method call

&gt;  org.elasticsearch.ElasticsearchIllegalArgumentException: Valid edit distances are [0, 1, 2] but was [4]

We thought that maybe this is a bug in the Java API and tried the same thing with the REST API. The query was this:

```
{
        "flt": {
            "fields": [
            "comment"
        ],
        "like_text": "FFFdfds",
        "fuzziness": "4"
    }
}
```

the result was this:

&gt; ElasticsearchIllegalArgumentException[Can't get similarity from fuzziness [4]]; }]

But the query works fro values 1 and 2.
So I guess either the documentation is mistaken or the implementation. We were previously using the edit distances of up to 4-5 characters. After the update we're kind of lost for now :)
</description><key id="28494024">5292</key><summary>Edit distance allowed values for fuzzy_like_this query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dnavre</reporter><labels><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-28T11:24:45Z</created><updated>2014-03-13T14:00:58Z</updated><resolved>2014-03-13T14:00:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-28T11:26:06Z" id="36341855">I will look into this - thanks for opening it
</comment><comment author="dnavre" created="2014-03-05T13:03:51Z" id="36739732">thank you for looking into this :) We're currently being late for a release date cause of this issue. Can you please tell us if we should expect any news on this in the near future?

Thanks once more
</comment><comment author="s1monw" created="2014-03-10T08:57:10Z" id="37162798">hey @dnavre a fix for this will be in the next release.. thanks for you patience 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implemented update by paths in order to overwrite document fields instead of merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5291</link><project id="" key="" /><description>We've implemented a new feature for overwriting doc fields instead of merging values in.
This is done by defining the paths to sub-fields instead of submitting an updated doc.
It gives us the opportunity to overwrite a complete object by a new one, and so deleting unwanted keys.
This is maybe most useful when using dynamic mapping on object fields.
## Demonstration Example (Documentation):
### Create an index

``` shell
curl -XPUT 'http://localhost:9200/twitter'
```
### Create a document mapping with a field of type `object`

``` shell
curl -XPUT 'http://localhost:9200/twitter/tweet/_mapping' -d '{
    "tweet" : {
        "properties" : {
            "person" : {
                "type" : "object",
                "properties" : {
                    "name" : {"type" : "object"},
                    "sid" : {"type" : "string", "index" : "not_analyzed"}
                }
            },
            "message" : {"type" : "string"}
        }
    }
}'
```
### Index a document

``` shell
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "person" : {
        "name" : {
            "first_name" : "Shay",
            "last_name" : "Banon",
            "nick_name": "kimchy"
        },
        "sid" : "12345"
    },
    "message" : "This is a tweet!"
}'
```
### Update the document in order to delete the `person.name.nick_name` field
#### Can be done by submitting the paths by using the `paths` key

``` shell
curl -XPOST 'http://localhost:9200/twitter/tweet/1/_update' -d '{
    "paths" : {
        "person.name" : {
            "first_name" : "Shay",
            "last_name" : "Banon"
        }
    }
}'
```
#### Validate the updated document (`nick_name` removed)

``` shell
 curl -XGET 'http://localhost:9200/twitter/tweet/1' 
{
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 2,
    "exists" : true,
    "_source" : {
        "person" : {
            "name" : {
                "first_name" : "Shay",
                "last_name" : "Banon"
            },
            "sid" : "12345"
        },
        "message" : "This is a tweet!"
    }
} 
```
#### Same can be achieved by using `doc` in conjunction with `doc_as_paths=true`

``` shell
curl -XPOST 'http://localhost:9200/twitter/tweet/1/_update' -d '{
    "doc" : {
        "person.name" : {
            "first_name" : "Shay"
        }
    },
    "doc_as_paths" : true
}'
```
#### Validate the updated document (now `last_name` is removed)

``` shell
 curl -XGET 'http://localhost:9200/twitter/tweet/1' 
{
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 3,
    "exists" : true,
    "_source" : {
        "person" : {
            "name" : {
                "first_name" : "Shay"
            },
            "sid" : "12345"
        },
        "message" : "This is a tweet!"
    }
} 
```

Of course `paths` can also be used to update non-objects.
</description><key id="28492118">5291</key><summary>Implemented update by paths in order to overwrite document fields instead of merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels><label>discuss</label></labels><created>2014-02-28T10:48:11Z</created><updated>2014-08-22T07:26:37Z</updated><resolved>2014-08-22T07:26:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jodok" created="2014-05-26T17:28:54Z" id="44206232">any updates on this?
</comment><comment author="clintongormley" created="2014-08-22T07:26:37Z" id="53031020">Hi @seut 

Sorry this has taken a long time to look at.  I'm not sure that this is the right approach to this problem as it is not generic enough. Overwriting paths is just one possible action.  See #7332 for another example.

There is an open issue to support RFC 6902 PATH updates (#7030) and, while I think that RFC is too limited for our purposes, we could support the functionality from both of these issues with some kind of AST or "patch language".
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure that currently running snapshot is cancelled before closing the shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5290</link><project id="" key="" /><description /><key id="28490592">5290</key><summary>Make sure that currently running snapshot is cancelled before closing the shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2014-02-28T10:20:36Z</created><updated>2014-04-29T23:17:15Z</updated><resolved>2014-04-29T23:17:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-04-29T23:17:15Z" id="41744008">It was taken care of by #5432
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_point types inherit 'path' attribute from nested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5289</link><project id="" key="" /><description>It appears that if you have a geo_point field inside a nested object, it will inherit the 'path' attribute from the nested object.
I.e. if you create an index like this:

```
curl -XPOST 'localhost:9200/test' -d '{
    "mappings" : {
        "type1" : {
            "properties" : {
                "geoInOuter" : {
                    "type" : "geo_point"
                },
                "obj1" : {
                    "type" : "nested",
                    "path" : "just_name",
                    "properties" : {
                        "geoInNested" : {
                            "type" : "geo_point"
                        }
                    }
                }
            }
        }
    }
}'
```

and then ask for the mapping back, like this

``` Shell
curl -XGET 'localhost:9200/test/_mapping'
```

the response will be this:

``` JSON
{
    "test" : {
        "mappings" : {
            "type1" : {
                "properties" : {
                    "geoInOuter" : {
                        "type" : "geo_point"
                    },
                    "obj1" : {
                        "type" : "nested",
                        "path" : "just_name",
                        "properties" : {
                            "geoInNested" : {
                                "type" : "geo_point",
                                "path" : "just_name"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

Notice the extra 'path' attribute on the 'geoInNested' field.
I haven't found any reason why this causes an issue, so probably is a pretty minor bug, but presumably the underlying bug could be causing other issues.
</description><key id="28485596">5289</key><summary>geo_point types inherit 'path' attribute from nested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels /><created>2014-02-28T08:39:36Z</created><updated>2014-07-23T12:29:38Z</updated><resolved>2014-07-23T12:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T12:29:38Z" id="49866794">The `path` attribute has been removed in favour of `copy_to`.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch OutOfMemory issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5288</link><project id="" key="" /><description>Hi,

I'm getting the below issue when trying to do paging in Kibana using elasticsearch.

[2014-02-28 11:24:29,280][DEBUG][action.search.type       ] [Nut] [182865] Failed to execute fetch phase
org.elasticsearch.ElasticsearchException: Java heap space
        at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:37)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:451)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

Please help to rectify the same.
Subhadip
</description><key id="28482086">5288</key><summary>elasticsearch OutOfMemory issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ibagui</reporter><labels /><created>2014-02-28T06:59:57Z</created><updated>2014-02-28T09:29:48Z</updated><resolved>2014-02-28T08:08:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ibagui" created="2014-02-28T07:11:44Z" id="36326959">Hi,

Getting the below error also in elastic search log.

[2014-02-28 11:29:55,121][WARN ][cluster.action.shard     ] [Nut] [logstash-2014.02.28][3] sending failed shard for [logstash-2014.02.28][3], node[3DLIN4cCRPmMM_6a_1yTCA], [P], s[STARTED], indexUUID [bWO_8-eUQQ6WDxcl2KoUkw], reason [engine failure, message [MergeException[java.lang.OutOfMemoryError: Java heap space]; nested: OutOfMemoryError[Java heap space]; ]]

My Heap allocation in elastic search is....

```
ES_HEAP_SIZE=256m

ES_DIRECT_SIZE=512m
```

----- Please help on this
Thanks,
Subhadip
</comment><comment author="dadoonet" created="2014-02-28T07:26:55Z" id="36327622">Set Heap size to something more important.
Half of the RAM of your machine.
</comment><comment author="ibagui" created="2014-02-28T07:54:50Z" id="36328758">Hi,

Please let me know how much RAM size I should give. My total RAM is 4GB and there are other applications running

Thanks,
Subhadip
</comment><comment author="dadoonet" created="2014-02-28T08:08:32Z" id="36329428">I did not notice that you opened an issue.
Please use the mailing list for questions.
</comment><comment author="ibagui" created="2014-02-28T09:06:05Z" id="36332667">Hi,

I'm new to github. cant find the mailing list. Please help by commenting here.
</comment><comment author="javanna" created="2014-02-28T09:08:52Z" id="36332853">Hi @ibagui ,
here is the mailing list: https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch .
</comment><comment author="dadoonet" created="2014-02-28T09:29:48Z" id="36334220">Yes. And here is a very useful page: http://www.elasticsearch.org/help/
It will help you probably to ask questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add separate thread pool for global repository operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5287</link><project id="" key="" /><description>Fixes #5240
</description><key id="28477688">5287</key><summary>Add separate thread pool for global repository operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-02-28T04:26:45Z</created><updated>2014-08-20T18:05:10Z</updated><resolved>2014-07-03T16:39:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-07-03T16:39:38Z" id="47954024">Fixed in #6182
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Spatial4j 0.4.1 and JTS 1.13</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5286</link><project id="" key="" /><description>This fixes #5279

The main think irking me is that ElasticSearch uses a global hard-coded instance of JtsSpatialContext.GEO as _the_ one and only SpatialContext as a static final in ShapeBuilder.SPATIAL_CONTEXT.  That wasn't what I had in mind when I (or was it Ryan or Chris, I forget) devised SpatialContext concept.  If you look at JtsSpatialContextFactory (or don't even use JTS, look at SpatialContextFactory) you'll see a bunch of options that trigger various behavior.  The most important one is "geo" (aka geodetic or geodesic, synonyms) which is a boolean that chooses between a latitude-longitude spherical world model or a flat plane (Euclidean geometry).  It's not quite clear to me at this time how ElasticSearch users that want a flat world model and who pre-project their data are using this.
</description><key id="28477367">5286</key><summary>Upgrade to Spatial4j 0.4.1 and JTS 1.13</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsmiley</reporter><labels /><created>2014-02-28T04:15:33Z</created><updated>2014-06-20T05:24:23Z</updated><resolved>2014-03-04T09:23:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dsmiley" created="2014-02-28T04:20:52Z" id="36320772">Notify @chilling 

Question: Should commentary/feedback go on the pull-request or does it go on the issue ( #5279 ) referenced by the pull-request?  For that matter is there even a point to creating an issue if I know I'm going to create a pull-request any way?
</comment><comment author="chilling" created="2014-02-28T04:46:22Z" id="36321609">@dsmiley cool stuff, Thanks. I'm currently looking at it. Feedback and comments on the actual _PR_ goes here. If we need to discuss the _Issue_ itself, it should be there. So I guess further discussion is on the PR.
</comment><comment author="dsmiley" created="2014-02-28T04:52:23Z" id="36321812">For the record, my GitHub ID is @dsmiley, not "david".  I wonder how much GH mistaken identity email he gets for all the David's out there.
</comment><comment author="chilling" created="2014-02-28T05:17:39Z" id="36322679">oh, just saw saw it.thx
</comment><comment author="david" created="2014-02-28T12:31:30Z" id="36345892">@dsmiley about once a month or so. It could definitely be worse. :)
</comment><comment author="dsmiley" created="2014-03-04T14:55:38Z" id="36631842">@jpountz I don't see evidence that this was in fact merged into ElasticSearch.  
</comment><comment author="jpountz" created="2014-03-04T15:06:18Z" id="36633236">I didn't mean to close this pull request, it looks to me that Github automatically closed it because I deleted the `elasticsearch:upgrade/lucene_47` branch, that this pull request is derived from. Maybe you can try to reopen it by branching master?

Sorry for the inconvenience!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermsFacetBuilder.scriptField from Java API sets "script" not "script_field" in resulting query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5285</link><project id="" key="" /><description>I'm seeing a problem where calling scriptField() on TermsFacetBuilder does not generate "script_field" in the resulting query.  Looking at TermsFacetBuilder in both 0.90.X and master, it appears that script() and scriptField() are both setting script and that there is no way to create script_field using the Java API.

Setting .scriptField("_source.foo") on a TermsFacetBuilder then throws a QueryPhaseExecutionException when mvel attempts to evaluate the bogus syntax on the resulting "script" field.
</description><key id="28476382">5285</key><summary>TermsFacetBuilder.scriptField from Java API sets "script" not "script_field" in resulting query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seallison</reporter><labels><label>adoptme</label></labels><created>2014-02-28T03:42:02Z</created><updated>2014-12-29T12:58:50Z</updated><resolved>2014-12-29T12:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T10:06:00Z" id="39713868">Hi @seallison I'm trying to reproduce this with no luck, could you please send a full recreation?

In fact, internally there is no difference between `script_field` and `script`. If you look at [TermsFacetParser](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetParser.java#L129), that does the parsing of each terms facet, you'll see that they both go into the same field called script, that's why the java API does the same.
</comment><comment author="seallison" created="2014-04-08T22:04:04Z" id="39907533">@javanna I was hitting a weird situation that causes an Exception to be thrown the first time, but not the second when using a script query.  I think when I hit this a month ago I didn't try the request a second time to find this behavior.  Check out the exceptions in the gist below: the first one is slightly different from the second for the exact same query.  This same faceted query converted to use .field(fieldName) instead of .script("_source."+fieldName) will work every time without any Exceptions being thrown.

https://gist.github.com/seallison/10199249
</comment><comment author="javanna" created="2014-04-17T15:00:01Z" id="40723607">Hi @seallison I'm still a bit confused, could you recreate the problem and send the exact query and the thrown error please?
</comment><comment author="lsamayoa" created="2014-05-14T23:52:25Z" id="43154084">I'm actually dealing with this bug. It can be seen in this commit where it the "feature/bug" was introduced cf5ed1d1  
</comment><comment author="clintongormley" created="2014-12-29T12:58:50Z" id="68255288">Facets have been removed in master. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node partialy breaks after 20+ min of runtime.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5284</link><project id="" key="" /><description>After a tribe node has been running for 20min to an hour or more, _hosts query fails. Others queries seem to work, unfortunately Kibana needs to use the _hosts query to work. 

The ES clusters behind the tribe node have no errors that can be seen. I've tested this with 1.0.0 and 1.0.1.

```
curl http://tribenode:9200/_hosts/
(times out)
```

No errors are seen on the ES clusters the tribe node points to.

My workaround is to restart Elasticsearch on the tribe node every 20min

Tribe node configuration

```
tribe:
  tribe_datacenter1:
    cluster.name:   prod_datacenter1
    discovery.zen.ping.multicast.enabled: false
    transport.tcp.port: 9302
    discovery.zen.ping.unicast.hosts:  ["node1.datacenter1:9300""node2.datacenter1:9300,]

  tribe_datacenter2:
    cluster.name:   prod_datacenter2
    discovery.zen.ping.multicast.enabled: false
    transport.tcp.port: 9303
    discovery.zen.ping.unicast.hosts: ["node1.datacenter2:9300""node2.datacenter2:9300,]

cluster.name: prod_tribe
node.name: tribenode.local
transport.tcp.port: 9301
path.data: /data/elasticsearch/data
path.logs: /var/log/elasticsearch
```

Some metrics pulled from the the Tribe node from Elasticsearch HQ; as you can see, not much load at all.

Heap Used:  127.6MB
Heap Committed: 2.8GB
Non Heap Used:  28.0MB
Non Heap Committed: 28.6MB
JVM Uptime: 01:47:54
Thread Count/Peak:  60 / 68
GC (Old) Count: 4
GC (Old)Time:   00:00:00
GC (Young) Count:   0
GC (Young)Time: 00:00:00
Java Version:   1.7.0_51
JVM Vendor: Oracle Corporation
JVM:    OpenJDK 64-Bit Server VM
Uptime: 00:01:12
Total Memory:   4.73 GB
Total Swap: 2.00 GB
Memory (Used/Free): 897.54 MB / 3.85 GB
Swap (Used/Free):   0 B / 2.00 GB
CPU User/Sys:   0% / 0%
CPU Idle:   99%
CPU Vendor: Intel
CPU Model:  Xeon
Total Cores:    2
Index (Queue/Peak/Active):  0/0/0
Get (Queue/Peak/Active):    0/0/0
Search (Queue/Peak/Active): 0/0/0
Bulk (Queue/Peak/Active):   0/0/0
Refresh (Queue/Peak/Active):    0/0/0
Flush (Queue/Peak/Active):  0/0/0
Merge (Queue/Peak/Active):  0/0/0
Management (Queue/Peak/Active): 0/1/1
Open File Descriptors:  389
CPU Usage:  0% of 200%
CPU System: 00:00:13
CPU User:   00:00:55
CPU Total:  00:01:08
Resident Memory:    316.3MB
Shared Memory:  12.7MB
Total Virtual Memory:   4.3GB

Errors seen on tribe node:

```
[2014-02-27 12:35:16,431][DEBUG][action.admin.cluster.node.info] [tribenode] failed to execute on node [zr5f6hDBQSGNubAhz2JX2g]
org.elasticsearch.transport.SendRequestTransportException: [tribenode/prod_datacenter2][inet[/1.1.1.1:9302]][cluster/nodes/info/n]
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:202)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.start(TransportNodesOperationAction.java:168)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$300(TransportNodesOperationAction.java:102)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction.doExecute(TransportNodesOperationAction.java:73)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction.doExecute(TransportNodesOperationAction.java:43)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.client.node.NodeClusterAdminClient.execute(NodeClusterAdminClient.java:72)
        at org.elasticsearch.client.support.AbstractClusterAdminClient.nodesInfo(AbstractClusterAdminClient.java:179)
        at org.elasticsearch.rest.action.admin.cluster.node.info.RestNodesInfoAction.handleRequest(RestNodesInfoAction.java:105)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:159)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:142)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:291)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:43)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [tribenode/prod_datacenter2][inet[/1.1.1.1:9302]] Node not connected
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:859)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:540)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:189)
        ... 43 more
```
</description><key id="28473442">5284</key><summary>Tribe node partialy breaks after 20+ min of runtime.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jeppesen-io</reporter><labels><label>:Tribe Node</label><label>adoptme</label></labels><created>2014-02-28T02:19:29Z</created><updated>2016-11-06T12:25:14Z</updated><resolved>2016-11-06T12:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Jeppesen-io" created="2014-03-12T17:11:37Z" id="37435742">This is very strange; If I run 'curl localhost:9200/_nodes' every 5min via cron, this issue goes away.

This seems to be a problem with tribe node inactivity.
</comment><comment author="gseng" created="2014-09-12T02:05:29Z" id="55353497">I had this happening to me and this was my issue.

### Issue
- All my ES nodes used the `logging.yml` to write logs to a log4j receiver.
- The machine the log4j receiver was on ran out of disk space.
- However, the symptoms were exactly like yours - no errors in the cluster, only the `NodeNotConnectedException` on the tribe.
- In fact, using the `head` plugin, the tribe node was still connected.

### Conclusion
- It's an open question whether ES should be more resilient to something like the above happening.
- I know that if my log4j receiver shuts down and comes back later, log4j reconnects.
- In my case because all machines were using the same log4j receiver, the entire cluster behaved in a very strange fashion - in general things would start working ... and then suddenly stop working. None of the error messages pointed to my actual problem ... had to spend 2 days digging before realizing what was happening.
</comment><comment author="clintongormley" created="2014-10-17T09:24:19Z" id="59488530">We should make the tribe node reconnect if the connection goes down.
</comment><comment author="yongjunj" created="2014-11-07T17:54:53Z" id="62185224">+1 - I'm hitting this issue with tribe node setup as well.
</comment><comment author="gseng" created="2014-11-08T15:39:20Z" id="62262019">@clintongormley , the tribe node does reconnect after the connection goes down. I've tested this making iptables drop the tribe's packet for sometime ... and then re-enabling it. Without intervention, the tribe reconnects.

So the problem is deeper where in some scenarios, the tribe will disconnect and somehow is unable to reconnect even if the cluster is working perfectly fine.

@yongjunj , you could try increasing the timeouts here
- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#fault-detection

We've seen increased stability by increasing all of them. Notice it's the `discovery.zen.fd`. Setting `discovery.zen.ping_timeout` did not help us.
</comment><comment author="zijian1981" created="2015-05-20T15:23:02Z" id="103924536">i hit this issue too,es didnot report any error ,sometimes es can return result ,and in other time ,it throws index is missing exception.how can I work around? Thanks 
</comment><comment author="clintongormley" created="2016-11-06T12:25:14Z" id="258677691">We haven't seen this issue in recent years.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias- vs. Request-originated filter performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5283</link><project id="" key="" /><description>I am moving a filter out of an alias and into the request.  The performance is massively worse on a index where all docs match for _type and few docs match for the second filter.  After digging through the implementation of alias filters, it appears that they are being applied at the same level as the _type filter, whereas the filter from the request is in a nested filtered query.  How can I can apply the filter at the same level as _type?

Here is what the queries look like.

With alias filter:

```
filtered(
   // query
)-&gt;+cache(_type:page)+cache(user_id:1234)
```

Without request filter:

```
filtered(
  filtered(
    // query
  )-&gt;cache(BooleanFilter(+cache(user_id:1234)))
)-&gt;cache(_type:page)
```
</description><key id="28469037">5283</key><summary>Alias- vs. Request-originated filter performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qhoxie</reporter><labels /><created>2014-02-28T00:35:02Z</created><updated>2014-07-23T14:13:42Z</updated><resolved>2014-07-23T14:13:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T14:13:42Z" id="49879024">@qhoxie This should have been fixed by https://github.com/elasticsearch/elasticsearch/issues/6247

If you are still finding this to be an issue, could you reopen?  thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel agent error connecting to [localhost:9200]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5282</link><project id="" key="" /><description>Hi I created a docker image for Elasticsearch and I use it at home; for a client and in several different servers and different containers they all get this problem after awhile.  The image that I am using is available at [damm/elasticsearch](https://index.docker.io/u/damm/elasticsearch/) 

Maybe this happens after it idles for a few days? I don't normally leave elasticsearch idle but I can do that as a test.

Below is an example of right where it started to fail; started working and failed again.

``` shell
[2014-02-25 00:01:20,852][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.25] update_mapping [syslog] (dynamic)
[2014-02-25 00:01:23,440][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.25] update_mapping [logs] (dynamic)
[2014-02-25 00:01:23,575][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.25] update_mapping [unicorn] (dynamic)
[2014-02-25 00:51:00,163][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.25] update_mapping [syslog] (dynamic)
[2014-02-25 00:54:04,271][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]
java.net.SocketTimeoutException
    at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:111)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:317)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:293)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.access$200(ESExporter.java:56)
    at org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run(ESExporter.java:734)
    at java.lang.Thread.run(Thread.java:744)
[2014-02-25 02:36:57,282][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]
java.net.SocketTimeoutException
    at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:111)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:317)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:293)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.access$200(ESExporter.java:56)
    at org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run(ESExporter.java:734)
    at java.lang.Thread.run(Thread.java:744)
2014-02-25 23:59:12,553][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.26] creating index, cause [auto(bulk api)], shards [5]/[0], mappings [_default_]
[2014-02-25 23:59:13,031][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.26] update_mapping [logs] (dynamic)
[2014-02-25 23:59:17,624][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.25] update_mapping [index_event] (dynamic)
[2014-02-26 00:00:01,154][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] creating index, cause [auto(bulk api)], shards [1]/[1], mappings [node_stats, index_stats, _default_]
[2014-02-26 00:00:01,710][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [node_stats] (dynamic)
[2014-02-26 00:00:04,526][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [shard_stats] (dynamic)
[2014-02-26 00:00:04,591][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [shard_event] (dynamic)
[2014-02-26 00:00:04,592][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [routing_event] (dynamic)
[2014-02-26 00:00:04,593][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [cluster_event] (dynamic)
[2014-02-26 00:00:04,595][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [index_event] (dynamic)
[2014-02-26 00:00:04,658][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [indices_stats] (dynamic)
[2014-02-26 00:00:04,709][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [index_stats] (dynamic)
[2014-02-26 00:00:04,833][INFO ][cluster.metadata         ] [Amphibius] [.marvel-2014.02.26] update_mapping [cluster_stats] (dynamic)
[2014-02-26 00:00:29,763][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.26] update_mapping [syslog] (dynamic)
[2014-02-26 00:41:59,618][INFO ][cluster.metadata         ] [Amphibius] [logstash-2014.02.26] update_mapping [syslog] (dynamic)
[2014-02-26 00:45:17,556][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]
java.net.SocketTimeoutException
    at java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:111)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
    at sun.net.www.http.HttpClient.New(HttpClient.java:308)
    at sun.net.www.http.HttpClient.New(HttpClient.java:326)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:317)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:293)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.access$200(ESExporter.java:56)
    at org.elasticsearch.marvel.agent.exporter.ESExporter$ConnectionKeepAliveWorker.run(ESExporter.java:734)
    at java.lang.Thread.run(Thread.java:744)
```
</description><key id="28465407">5282</key><summary>Marvel agent error connecting to [localhost:9200]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damm</reporter><labels /><created>2014-02-27T23:29:54Z</created><updated>2014-03-10T19:26:54Z</updated><resolved>2014-03-10T19:26:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-04T10:15:25Z" id="36609501">it looks like you had 3 timeouts in 24 hours (marvel makes a request using a persistent connection every 5 sec + keep alive ping every 1 sec). Was there anything else you can perhaps correlate this with? I'm thinking like a network change or a firewall the closes connections after some time. Can try it out without docker, just to see whether it's docker related or something else is the cause?
</comment><comment author="bleskes" created="2014-03-04T14:03:50Z" id="36626257">one more thing you can try is to increase the default timeout (6s) by settings marvel.agent.exporter.es.timeout: 30s in your elasticsearch.yml (for example).
</comment><comment author="damm" created="2014-03-04T20:26:50Z" id="36669981">As it's happening on 4 different servers that run the same Docker Image I'm leaning towards possibly Docker related (or something the ubuntu minimal image)

I have already added `/etc/fstab` and `/etc/mtab` to collect usage.  I will try bumping the timeout and see if I can reproduce this on an idle Elasticsearch Container.  (Yes this reproduces in an idle server, no cpu usage loss, no i/o congestion with no queries coming in our out)
</comment><comment author="damm" created="2014-03-10T18:29:37Z" id="37216966">@bleskes there haven't been any timeouts in the past 36hours so having the timeout at 30s seems to be helping.

I'm wondering with the default ES memory footprint if it's possible that we could be having long pauses due to GC? Seems plausible but unlikely.
</comment><comment author="bleskes" created="2014-03-10T19:26:54Z" id="37223533">good to here. This could in theory be caused by long GC but then you would see those in the logs. Of course everything depends on your use case but with default setting (1GB) and sane behaviour it seem indeed unlikely.

I'm closing it this issue as it seems resolved now. Thanks for sharing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API does not have a way to set global highlighting settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5281</link><project id="" key="" /><description>There is no method in the Java API on SearchRequestBuilder to set the global highlighting options for 'fragment_size' or 'number_of_fragments'.

The documentation lists this ability in the 0.90.X series as well as 1.0.X series:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/search-request-highlighting.html#highlighting-settings

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#highlighting-settings

I am working around this by using the method:

addHighlightedField(name, fragmentSize, numberOfFragments)

But that is not ideal since my fragmentSize and numberOfFragments doesn't change on a per field basis.

Please see: https://groups.google.com/forum/#!topic/elasticsearch/a4K8yfMqim4
</description><key id="28462065">5281</key><summary>Java API does not have a way to set global highlighting settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">seallison</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-27T22:48:29Z</created><updated>2015-06-07T15:08:54Z</updated><resolved>2014-03-28T17:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-28T09:02:21Z" id="36332456">Agreed the highlighting API could be clearer :)

You could work around this by using the `SearchSourceBuilder` which allows you to set the `HighlightBuilder`, which is the object that allows you to control everything about highlighting.
</comment><comment author="javanna" created="2014-03-28T15:26:41Z" id="38931569">Actually what I suggested was wrong as the `HighlightBuilder` doesn't expose those global options either. Fixing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add human readable JVM start_time and process refresh_interval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5280</link><project id="" key="" /><description>In the JVM and process display, the `human` flag sets add human-readable heap data, but does not include human-readable start_time and refresh_interval.  Adding human-readable data here would be helpful.
</description><key id="28459532">5280</key><summary>Add human readable JVM start_time and process refresh_interval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-27T22:13:24Z</created><updated>2014-03-28T11:32:28Z</updated><resolved>2014-03-28T11:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-03-28T11:10:40Z" id="38908620">Marking this issue as breaking as it breaks the output of the nodes info api, since `start_time` and `refresh_interval` will be replaced by default by `start_time_in_millis` and `refresh_interval_in_millis`. With the `human` flag `start_time` and `refresh_interval` will be returned as well and will contain the human readable variations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Spatial4j to 0.4.1 and JTS to 1.13</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5279</link><project id="" key="" /><description>Spatial4j 0.4.1 is required by Lucene-spatial 4.7.0's new SerializedDVStrategy, plus it has various bug fixes and performance improvements (especially index() use of JTS PreparedGeometry).

JTS 1.13 is JTS's latest version which has various improvements.  Thread-safety of PreparedGeometry comes to mind.

(I'm working on these things right now on my fork)
</description><key id="28450282">5279</key><summary>Upgrade Spatial4j to 0.4.1 and JTS to 1.13</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dsmiley</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-27T20:10:39Z</created><updated>2014-03-20T10:29:18Z</updated><resolved>2014-03-20T10:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-27T20:11:38Z" id="36285675">@dsmiley cool stuff can you make sure you sign the CLA as well 
</comment><comment author="dsmiley" created="2014-02-27T20:21:06Z" id="36286675">@s1monw Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change sentence fragments into headings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5278</link><project id="" key="" /><description>Add a whitespace to make these separate paragraphs rather than repetitive sentence fragments.
</description><key id="28435953">5278</key><summary>Change sentence fragments into headings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rwstauner</reporter><labels /><created>2014-02-27T17:07:37Z</created><updated>2014-07-11T03:57:53Z</updated><resolved>2014-03-17T11:23:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-17T11:23:05Z" id="37805460">thanks! Added via https://github.com/elasticsearch/elasticsearch/commits
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API gives inconsistent SearchHits results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5277</link><project id="" key="" /><description>I was testing my application that uses the Java API (specifically for a data less node that I run locally and I join to a cluster). 

I've this snippet of code:

``` java
SearchResponse searchResponse = getNode().client().prepareSearch("maincontentindex")
                    .setTypes("rendition")
                    .setSearchType(SearchType.DEFAULT)
                    .setQuery(qb)
                    .execute()
                    .actionGet();

 if (searchResponse.getHits().totalHits() == 0)
                throw new DocumentNotFoundException("Document not found");
```

so if I test this against the cluster using jmeter to stress a bit the app, then suddenly I shut down one non-master node,  totalHits() will be &gt; 0, but at a later line where I have

``` java
 SearchHit selectedHit = hits.getAt(0);
```

I get an IndexOutOfBoundsException (or so). Which is unexepected because there should have been at least one element.

Internally ES cries with the following exception (that I tried to catch around the ActionGet, with no luck):

```
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [search02-mydomain.com][inet[/172.29.131.40:9300]] Node not connected
```

I'd like first to understand the behaviour of SearchHits, then to know how to gracefully recover from one node going down without throwing out exceptions.

This is related to 1.0
</description><key id="28434749">5277</key><summary>Java API gives inconsistent SearchHits results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dariodariodario</reporter><labels><label>discuss</label></labels><created>2014-02-27T16:53:16Z</created><updated>2015-05-28T20:57:22Z</updated><resolved>2014-12-29T14:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2014-02-28T15:51:52Z" id="36363865">How many concurrent requests are you sending with JMeter? Do you see the same behavior with a SearchType of QUERY_AND_FETCH? The default value is equal to QUERY_THEN_FETCH. I think what may be happening when you shut down the node is the query phase has executed and gets the information about the document. However, since the node has gone down the document does not get retrieved during the fetch phase.
</comment><comment author="dariodariodario" created="2014-02-28T16:19:19Z" id="36366788">By using QUERY_AND_FETCH I have been able to get rid of the errors when I stop a non master node. But if I throw down a master node (albeit it recovers soon), I still get ShardFailures.

When a node goes down I also see a couple of warnings like:

```
[2014-02-28 16:13:52,426][WARN ][org.elasticsearch.transport.netty] [d-eslocalnode] Message not fully read (response) for [1074736] handler org.elasticsearch.search.action.SearchServiceTransportAction$7@3f3b511e, error [true], resetting
```

Regarding your question about JMeter, I test my app with about 200 threads (the app runs locally, but not the ES data and master nodes).

Is there a way to get rid of those shards failures? Or do I have to implement some retry strategy on my own?
</comment><comment author="clintongormley" created="2014-12-29T12:55:53Z" id="68255093">Hi @dariodariodario 

Sorry it has taken a long time to get to this issue.  Is this something that you are still seeing in more recent versions on Elasticsearch?
</comment><comment author="dariodariodario" created="2014-12-29T13:00:27Z" id="68255380">I don't know, I've changed job in the meanwhile. So I've not been working
on that anymore. Thanks anyway.
On 29 Dec 2014 13:56, "Clinton Gormley" notifications@github.com wrote:

&gt; Hi @dariodariodario https://github.com/dariodariodario
&gt; 
&gt; Sorry it has taken a long time to get to this issue. Is this something
&gt; that you are still seeing in more recent versions on Elasticsearch?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5277#issuecomment-68255093
&gt; .
</comment><comment author="clintongormley" created="2014-12-29T14:40:53Z" id="68262440">thanks for letting me know @dariodariodario 

I'll close this for now
</comment><comment author="kristoffer-dyrkorn" created="2015-05-28T13:08:29Z" id="106304781">Please reopen @clintongormley - we see this on 1.5.2.
</comment><comment author="clintongormley" created="2015-05-28T13:11:27Z" id="106305311">@kristoffer-dyrkorn please open a new ticket with all of the relevant details instead
</comment><comment author="kjbekkelund" created="2015-05-28T20:26:20Z" id="106586131">@clintongormley I can fill in with some more info. This is the exception we get:

```
java.lang.ArrayIndexOutOfBoundsException: 0
    at org.elasticsearch.search.internal.InternalSearchHits.getAt(InternalSearchHits.java:147) ~[elasticsearch-1.5.2.jar:na]
    at com.svv.datainn.dataimport.elasticsearch.DataQualityRepository.extractProgress(DataQualityRepository.java:102) ~[dataimport-2.0.1-35.jar:na]
```

And this is the relevant code:

```
private Maybe&lt;Progress&gt; extractProgress(SearchHits hits) {
    if(hits.getTotalHits() == 0) {
        return Maybe.empty();
    }
    else {
        SearchHit hit = hits.getAt(0);
        // ...
    }
}
```

This is the search:

```
SearchResponse response = esClient.prepareSearch("some_index")
        .setTypes("some_type")
        .setFrom(0)
        .setSize(1)
        .setQuery(filteredQuery(matchAllQuery(), termFilter("some_field", someValue)))
        .execute()
        .actionGet();

return extractProgress(response.getHits());
```

So what's mentioned in this issue definitely appears to still occur on 1.5.2. Should we still open a new ticket even if it appears to be the same problem?

We haven't had time to debug this thoroughly yet, so I'll try to get back with more info early next week.
</comment><comment author="dadoonet" created="2015-05-28T20:57:22Z" id="106595758">I gave a quick look at your code and I think you need to be aware that totalHits is different than the size of the hits array.

This test:

&gt; if(hits.getTotalHits() == 0)
&gt; Looks wrong to me if you want to make sure you can do:
&gt; hits.getAt(0);

My 2 cents.

Please discuss that on discuss.elastic.co so we can help you there.

&gt; Le 28 mai 2015 &#224; 22:26, Kim Joar Bekkelund notifications@github.com a &#233;crit :
&gt; 
&gt; @clintongormley I can fill in with some more info. This is the exception we get:
&gt; 
&gt; java.lang.ArrayIndexOutOfBoundsException: 0
&gt;     at org.elasticsearch.search.internal.InternalSearchHits.getAt(InternalSearchHits.java:147) ~[elasticsearch-1.5.2.jar:na]
&gt;     at com.svv.datainn.dataimport.elasticsearch.DataQualityRepository.extractProgress(DataQualityRepository.java:102) ~[dataimport-2.0.1-35.jar:na]
&gt; And this is the relevant code:
&gt; 
&gt; private Maybe&lt;Progress&gt; extractProgress(SearchHits hits) {
&gt;     if(hits.getTotalHits() == 0) {
&gt;         return Maybe.empty();
&gt;     }
&gt;     else {
&gt;         SearchHit hit = hits.getAt(0);
&gt;         // ...
&gt;     }
&gt; }
&gt; This is the search:
&gt; 
&gt; SearchResponse response = esClient.prepareSearch("some_index")
&gt;         .setTypes("some_type")
&gt;         .setFrom(0)
&gt;         .setSize(1)
&gt;         .setQuery(filteredQuery(matchAllQuery(), termFilter("some_field", someValue)))
&gt;         .execute()
&gt;         .actionGet();
&gt; 
&gt; return extractProgress(response.getHits());
&gt; So what's mentioned in this issue definitely appears to still occur on 1.5.2. Should we still open a new ticket even if it appears to be the same problem?
&gt; 
&gt; We haven't had time to debug this thoroughly yet, so I'll try to get back with more info early next week.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add circuit breaker functionality to parent/child id field data cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5276</link><project id="" key="" /><description>Now that the parent/child id cache is part of the field data instead of a dedicated cache, we should add the circuit breaker functionality to it to prevent OOMEs when it is loaded.
</description><key id="28431870">5276</key><summary>Add circuit breaker functionality to parent/child id field data cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-27T16:19:16Z</created><updated>2015-08-13T15:29:50Z</updated><resolved>2014-03-04T23:48:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-27T16:19:48Z" id="36259376">+1!
</comment><comment author="dakrone" created="2014-03-04T23:48:05Z" id="36694072">Fixed in https://github.com/elasticsearch/elasticsearch/commit/23471cd72c9e7987d442220b61d77aaeb257501f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Attempt to reword clear-scroll sentence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5275</link><project id="" key="" /><description>This may not be the best fix but the sentence didn't read correctly the way it was.
Change it as you see fit.
</description><key id="28426822">5275</key><summary>[docs] Attempt to reword clear-scroll sentence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rwstauner</reporter><labels /><created>2014-02-27T15:22:37Z</created><updated>2014-07-16T21:48:04Z</updated><resolved>2014-03-17T11:11:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-17T11:11:49Z" id="37804700">thanks! closed via https://github.com/elasticsearch/elasticsearch/commit/1486188a3b4f21ee189f95a88c27a877e5e14213
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[guide] Searching on the 0.90 version of the guide links to the 1.x version of the guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5274</link><project id="" key="" /><description>Searching on the 0.90 version of the guide links to the 1.x version of the guide.
1.  Go to http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/index.html
2.  Type "gateway" into the search box
3.  Click "Gateway &gt;&gt; elasticsearch"

Expected:
You go to http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/modules-gateway.html

Actual:
You go to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html
</description><key id="28418251">5274</key><summary>[guide] Searching on the 0.90 version of the guide links to the 1.x version of the guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-27T13:24:47Z</created><updated>2014-12-29T12:53:07Z</updated><resolved>2014-12-29T12:53:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T12:53:07Z" id="68254949">This has been fixed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use FieldMapper to create the low level term queries in CommonTermQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5273</link><project id="" key="" /><description>I added a patch to lucene to fix this upstream: 
https://issues.apache.org/jira/browse/LUCENE-5478

Closes #5258
</description><key id="28413074">5273</key><summary>Use FieldMapper to create the low level term queries in CommonTermQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-27T11:54:40Z</created><updated>2015-06-07T22:55:24Z</updated><resolved>2014-02-27T14:06:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-27T13:22:24Z" id="36240693">Nice! I left minor comments but other than that, +1 to push!
</comment><comment author="jpountz" created="2014-02-27T13:51:59Z" id="36243102">let's push!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor doc issue in docs/reference/query-dsl/queries/range-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5272</link><project id="" key="" /><description>s/bool/boost/ in the line

```
`boost`::   Sets the bool value of the query, defaults to `1.0`
```
</description><key id="28412251">5272</key><summary>Minor doc issue in docs/reference/query-dsl/queries/range-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">antbell</reporter><labels><label>docs</label></labels><created>2014-02-27T11:39:38Z</created><updated>2014-03-26T13:55:09Z</updated><resolved>2014-03-26T13:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow to filter by metrics of parent aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5271</link><project id="" key="" /><description>Would be awesome to have a filtered bucket which has access to the results of its parent aggregation(s) to define the filter-criteria.  
</description><key id="28410481">5271</key><summary>Allow to filter by metrics of parent aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pixelvitamina</reporter><labels><label>feedback_needed</label></labels><created>2014-02-27T11:07:18Z</created><updated>2014-12-29T12:52:38Z</updated><resolved>2014-12-29T12:52:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-24T17:59:47Z" id="56712190">Hi @pixelvitamina 

Could you provide a more detailed use case? What is it that you would want to achieve?
</comment><comment author="clintongormley" created="2014-12-29T12:52:38Z" id="68254917">No more feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>alternative for SearchQuerySet.order_by('?')</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5270</link><project id="" key="" /><description>I am using django/haystack/elasticsearch. I want to display 12 random items using a SearchQuerySet. SearchQuerySet.order_by('?')[:12] works with solr. I am curious if there is a way to do this via haystack with elasticsearch.

Thanks
</description><key id="28369830">5270</key><summary>alternative for SearchQuerySet.order_by('?')</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trikosuave</reporter><labels /><created>2014-02-26T21:11:03Z</created><updated>2014-04-04T17:53:27Z</updated><resolved>2014-04-04T17:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-04T17:53:27Z" id="39592705">Hi @trikosuave let's see if we can help you on our [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch), and if we end up with an improvement / feature, we can open a dedicated issue for it, I would close this for now though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove id cache from stats apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5269</link><project id="" key="" /><description>Since #4930 is in the id cache can now also be removed from the stats apis. The id cache inES it self has already been removed.
</description><key id="28363233">5269</key><summary>Remove id cache from stats apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-02-26T19:47:17Z</created><updated>2015-05-15T12:40:46Z</updated><resolved>2015-05-15T12:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Move master to Java 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5268</link><project id="" key="" /><description>Closes #5267
</description><key id="28363229">5268</key><summary>Move master to Java 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2014-02-26T19:47:15Z</created><updated>2015-08-25T13:26:01Z</updated><resolved>2014-02-27T14:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-26T19:47:44Z" id="36168180">@mrsolo can you take a look at the changes I made to the build randomization please?
</comment><comment author="s1monw" created="2014-02-26T21:02:54Z" id="36176194">@mrsolo I pushed an update - lemme know once you are ready with the infra and if it looks good
</comment><comment author="mrsolo" created="2014-02-26T22:39:38Z" id="36186929">jenkins is ready
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make master aka. Elasticsearch 2.0 use Java 1.7 or higher</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5267</link><project id="" key="" /><description>It seems Java 8 is months out and `1.6` has reached EOL. We should move `master` to `1.7` only. `1.x` will still run on `1.6` but once `2.0` is released Elasticsearch will only run on Java 7 or higher.
</description><key id="28360717">5267</key><summary>Make master aka. Elasticsearch 2.0 use Java 1.7 or higher</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-02-26T19:17:29Z</created><updated>2015-06-07T15:11:10Z</updated><resolved>2014-02-27T14:22:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-26T19:17:49Z" id="36164815">+100
</comment><comment author="martijnvg" created="2014-02-26T19:18:51Z" id="36164921">+1!
</comment><comment author="kimchy" created="2014-02-26T20:46:21Z" id="36174348">+1, 2.0 is far enough to require 1.7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slow Parent/Child Searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5266</link><project id="" key="" /><description>Elasticsearch seems to be empty out the cache on a fairly regular basis when using moderately complex parent/child relationship searches. Using the plugins to monitor the cache I see it load properly and at that point I can run searches normally and they return fine. After about 10-15 minutes of inactivity I see it drop about 75%, and then after another few minutes, the cache is fully emptied. We get a low number of searches but the majority time out as the cache has emptied by the time it goes to do the next search. 

I'm currently running elasticsearch a single dedicated AWS 35GB memory instance with 50% of the memory dedicated to elasticsearch and with ulimit -l unlimited set and working. I realize I should be running multiple machines but it's not really a possibility at this point. We've got around 100,000,000 documents total which from what i've seen, isn't really considered that much for elasticsearch. I think this may be an implementation level issue but wanted to get opinions first.

The query being run is as follows:

``` json
{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "or": [
                            {
                                "query": {
                                    "query_string": {
                                        "query": "kittens",
                                        "default_operator": "OR"
                                    }
                                }
                            },
                            {
                                "has_child": {
                                    "type": "dot",
                                    "query": {
                                        "query_string": {
                                            "query": "kittens",
                                            "default_operator": "OR"
                                        }
                                    }
                                }
                            }
                        ]
                    },
                    {
                        "bool": {
                            "must": [
                                {
                                    "terms": {
                                        "entity_type": [
                                            "user",
                                            "site"
                                        ]
                                    }
                                },
                                {
                                    "or": [
                                        {
                                            "exists": {
                                                "field": "caption"
                                            }
                                        },
                                        {
                                            "not": {
                                                "term": {
                                                    "dots_count": 0
                                                }
                                            }
                                        }
                                    ]
                                }
                            ],
                            "must_not": [
                                {
                                    "term": {
                                        "restricted": true
                                    }
                                },
                                {
                                    "range": {
                                        "publish_at": {
                                            "gte": "2014-02-26T19:04:51+00:00"
                                        }
                                    }
                                }
                            ]
                        }
                    }
                ]
            }
        }
    },
    "sort": [
        {
            "id": "desc"
        }
    ]
}
```
</description><key id="28360351">5266</key><summary>Slow Parent/Child Searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikecx</reporter><labels><label>feedback_needed</label></labels><created>2014-02-26T19:13:07Z</created><updated>2014-08-01T09:34:17Z</updated><resolved>2014-08-01T09:34:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-02-26T21:27:16Z" id="36179405">Hi @mikecx ,

Some questions:
- What ES version are using? 
- What cache are you exactly monitoring?
- How much of the jvm heap space is being used and how much does the cache take of that?
- How long does it take to complete the search request?
- How many shards does this index have? 
</comment><comment author="mikecx" created="2014-02-26T21:51:40Z" id="36181892">ES Version 1.0.0
Field Data and Filter Cache using the Bigdesk Plugin
Not sure how to tell
Roughly 90-120 seconds when the cache is emptied, 1-5 when it's full
total: 10, successful: 5, failed: 0 (using curl -X GET localhost:9200/photos/_stats)
</comment><comment author="martijnvg" created="2014-02-27T09:42:08Z" id="36225399">Internally has_child uses the id_cache, does this cache get emptied as well?
How much memory does the id_cache take?
Are you running low on jvm memory (how much of the heap used)? 
</comment><comment author="mikecx" created="2014-02-27T18:05:19Z" id="36271683">@martijnvg what's the correct way to check these values? I've temporarily disabled out parent/child search but can turn it back on to get them. 

Using _cluster/stats shows

``` json
{
  "timestamp" : 1393524089083,
  "cluster_name" : "*******",
  "status" : "yellow",
  "indices" : {
    "count" : 3,
    "shards" : {
      "total" : 15,
      "primaries" : 15,
      "replication" : 0.0,
      "index" : {
        "shards" : {
          "min" : 5,
          "max" : 5,
          "avg" : 5.0
        },
        "primaries" : {
          "min" : 5,
          "max" : 5,
          "avg" : 5.0
        },
        "replication" : {
          "min" : 0.0,
          "max" : 0.0,
          "avg" : 0.0
        }
      }
    },
    "docs" : {
      "count" : 100430285,
      "deleted" : 6917877
    },
    "store" : {
      "size_in_bytes" : 121506847305,
      "throttle_time_in_millis" : 8542
    },
    "fielddata" : {
      "memory_size_in_bytes" : 103599992,
      "evictions" : 0
    },
    "filter_cache" : {
      "memory_size_in_bytes" : 8462368,
      "evictions" : 0
    },
    "id_cache" : {
      "memory_size_in_bytes" : 3004
    },
    "completion" : {
      "size_in_bytes" : 1341459
    },
    "segments" : {
      "count" : 283,
      "memory_in_bytes" : 96120449579
    },
    "percolate" : {
      "total" : 0,
      "time_in_millis" : 0,
      "current" : 0,
      "memory_size_in_bytes" : 0,
      "memory_size" : "0b",
      "queries" : 0
    }
  },
  "nodes" : {
    "count" : {
      "total" : 1,
      "master_only" : 0,
      "data_only" : 0,
      "master_data" : 1,
      "client" : 0
    },
    "versions" : [ "1.0.0" ],
    "os" : {
      "available_processors" : 4,
      "mem" : {
        "total_in_bytes" : 36700303360
      },
      "cpu" : [ {
        "vendor" : "Intel",
        "model" : "Xeon",
        "mhz" : 2666,
        "total_cores" : 4,
        "total_sockets" : 4,
        "cores_per_socket" : 16,
        "cache_size_in_bytes" : 8192,
        "count" : 1
      } ]
    },
    "process" : {
      "cpu" : {
        "percent" : 72
      },
      "open_file_descriptors" : {
        "min" : 170,
        "max" : 170,
        "avg" : 170
      }
    },
    "jvm" : {
      "max_uptime_in_millis" : 168293404,
      "versions" : [ {
        "version" : "1.7.0_25",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "23.25-b01",
        "vm_vendor" : "Oracle Corporation",
        "count" : 1
      } ],
      "mem" : {
        "heap_used_in_bytes" : 1507721568,
        "heap_max_in_bytes" : 18315214848
      },
      "threads" : 70
    },
    "fs" : {
      "total_in_bytes" : 1082259591168,
      "free_in_bytes" : 953415294976,
      "available_in_bytes" : 898439716864,
      "disk_reads" : 746884,
      "disk_writes" : 517775,
      "disk_io_op" : 1264659,
      "disk_read_size_in_bytes" : 25807029248,
      "disk_write_size_in_bytes" : 5354119168,
      "disk_io_size_in_bytes" : 31161148416
    },
    "plugins" : [ ]
  }
}
```

without having run a parent/child query in a while. If this is the right way to test I can run it again both before and after running one.
</comment><comment author="martijnvg" created="2014-03-04T21:53:16Z" id="36683021">To get back to this, I would be surprised to see if the id_cache is cleared by it self (go to 0), this shouldn't happen since the id_cache is loaded eagerly.

The fielddata cache, which is used by sorting and facets/aggregations and filter cache can decrease over time when new data is added (and no searches happen during that time). If that is the case you may want to configure eager field data loading to make sure the latest entries are in field data for the new data and warmers for the filter cache.
</comment><comment author="clintongormley" created="2014-07-11T10:20:35Z" id="48715218">@mikecx Any more feedback on this issue or can it be closed?
</comment><comment author="clintongormley" created="2014-08-01T09:34:17Z" id="50866312">No feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api json typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5265</link><project id="" key="" /><description>[DOCS] Fixed JSON typo in Java API example.
</description><key id="28359702">5265</key><summary>Java api json typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels /><created>2014-02-26T19:05:05Z</created><updated>2014-06-16T08:04:02Z</updated><resolved>2014-03-03T23:28:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-26T20:27:43Z" id="36172513">Could you prefix your git comment with [DOCS]?

Left a small comment: +1 for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tracking of allocated arrays.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5264</link><project id="" key="" /><description>The BigArrays utility class is useful to generate arrays of various sizes: when
small, arrays will be allocated directly on the heap while larger arrays are
going to be paged and to recycle pages through PageCacheRecycler. We already
have tracking for pages but this is not triggered very often since it only
happens on large amounts of data while our tests work on small amounts of data
in order to be fast.

Tracking arrays directly helps make sure that we never forget to release them.

This pull request also improves testing by:
- putting random content in the arrays upon release: this makes sure that
  consumers don't use these arrays anymore when they are released as their
  content may be subject to use for another purpose since pages are recycled
- putting random content in the arrays upon creation and resize when
  `clearOnResize` is `false`.

The major difference with `master` is that the `BigArrays` class is now
instanciable, injected via Guice and usually available through the
`SearchContext`. This way, it can be mocked for tests.
</description><key id="28359071">5264</key><summary>Add tracking of allocated arrays.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-26T18:57:08Z</created><updated>2015-06-07T15:11:45Z</updated><resolved>2014-02-27T13:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-26T19:08:30Z" id="36163797">Neat.
</comment><comment author="s1monw" created="2014-02-26T19:11:01Z" id="36164084">cool stuff @jpountz 
</comment><comment author="s1monw" created="2014-02-26T20:53:09Z" id="36175084">I left some comments but in general this looks awesome
</comment><comment author="jpountz" created="2014-02-27T10:52:05Z" id="36230400">Thanks for the feedback, @s1monw. I rebased in order to get @martijnvg 's changes around p/c field data and added a new commit.
</comment><comment author="s1monw" created="2014-02-27T11:10:59Z" id="36231669">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Faster binary fields data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5263</link><project id="" key="" /><description>Currently, binary fields only support to be stored in an Elasticsearch index. 

I need to do customized search scoring based on binary field values for large result sets, but the performance of stored data is not good enough for this use case.

It would be great to have fast access to binary values by adding indexing or doc values support for binary fields.
</description><key id="28347859">5263</key><summary>Faster binary fields data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">belevian</reporter><labels><label>feedback_needed</label></labels><created>2014-02-26T16:36:18Z</created><updated>2015-02-28T05:03:25Z</updated><resolved>2015-02-28T05:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T12:51:16Z" id="68254840">Hi @belevian 

Sorry it has taken a while to get to this issue.  Could you expand on your use case a bit?  What type of searches are you trying to perform on this binary data?
</comment><comment author="clintongormley" created="2015-02-28T05:03:25Z" id="76510950">No more info provide. Closing for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PR for plugin isolation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5262</link><project id="" key="" /><description>first draft of plugin isolation

the classpath semantics (for finding es-plugin.properties) are respected without going through the CL (can have side-effects)
CL unit tests
Plugin integration tests
class loader isolation sanity checks
</description><key id="28335623">5262</key><summary>PR for plugin isolation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels /><created>2014-02-26T14:12:54Z</created><updated>2014-06-27T14:04:37Z</updated><resolved>2014-02-28T13:45:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-27T09:37:13Z" id="36224953">left a bunch of comments - mainly cosmetic! looks good costin, lemme know once you have another commit and I do another round!
</comment><comment author="costin" created="2014-02-27T15:08:30Z" id="36250918">@s1monw Fixed all comments. let me know if there's anything else.

Cheers!
</comment><comment author="s1monw" created="2014-02-27T16:41:55Z" id="36262049">hmm maybe I missed to add it or I was just thinking about it but is there a way to add the isolate setting to all the plugin tests we have randomly so we sometimes do it and sometimes not? other than that +1 to push
</comment><comment author="costin" created="2014-02-27T17:07:13Z" id="36265078">@s1monw we can use set the `plugins.isolation` setting randomly to true/false on each test which would result in the plugins being isolated or not. The alternative is to add the setting per plugin but that's tricky in a test since the `isolation` property needs to be defined in the `es-plugin.properties` file.
</comment><comment author="s1monw" created="2014-02-27T20:18:58Z" id="36286457">&gt; we can use set the plugins.isolation setting randomly to true/false on each test which would result in the plugins being isolated or not.

@costin maybe that is worth it? should we give that a try?
</comment><comment author="costin" created="2014-02-27T23:25:29Z" id="36304870">We could. But I would do this separately from this PR if that okay to not postpone it any longer.
</comment><comment author="costin" created="2014-02-28T13:45:04Z" id="36351355">Committed in master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin isolation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5261</link><project id="" key="" /><description>For 1.x, we talked about having class space isolated between plugins so their dependencies don't interact with each other.
To solve this issue, a parent-last `ClassLoader` will be used meaning that each plugin can be loaded into its own `ClassLoader` which will first try to load classes from itself (meaning the plugin class space) and only then fall back to its parent, or the Elasticsearch `ClassLoader`.

Under the current enhancement can have an additional `property` in its `es-plugin.properties` called `isolation` which indicates whether the plugin requires its own class space or not. As of now, all existing plugins lack the property which means the default ES settings will be applied.
ES provides a new setting, `plugin.isolation` which indicates the behaviour to use for plugins that do not specify any isolation. By default this would be true but can be changed to false to revert to the default / 1.0 behaviour.

By enabling isolation out of the box, we theoretically prevent any clashing problems regarding the classpath and at the same time, keep a close eye on problematic plugins. If they appear, folks can turn off `plugin.isolation` (by setting it to false) or simply cherry pick the problematic plugin and configure no isolation for it.
</description><key id="28333228">5261</key><summary>Plugin isolation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>enhancement</label><label>stalled</label></labels><created>2014-02-26T13:33:26Z</created><updated>2015-06-07T15:11:53Z</updated><resolved>2015-01-29T19:36:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-02-27T09:16:12Z" id="36223444">PR raised at #5262
</comment><comment author="costin" created="2014-03-17T12:43:12Z" id="37810695">Moving to 1.2
</comment><comment author="clintongormley" created="2014-07-11T08:36:39Z" id="48706902">@costin what's the status on this issue?
</comment><comment author="costin" created="2014-07-11T09:25:19Z" id="48710764">@clintongormley currently on hold/stalled.
</comment><comment author="salyh" created="2015-01-29T14:48:55Z" id="72036441">Will plugin isolation be targeted for 2.0?
</comment><comment author="clintongormley" created="2015-01-29T19:36:04Z" id="72089237">@salyh I think we've pretty much given up on this idea.  Proved too tricky.
</comment><comment author="jprante" created="2015-02-03T18:17:43Z" id="72704097">Hint: in my deploy plugin, I use classloader isolation https://github.com/jprante/elasticsearch-plugin-deploy
</comment><comment author="nik9000" created="2015-02-03T18:28:04Z" id="72705953">&gt; Hint: in my deploy plugin, I use classloader isolation https://github.com/jprante/elasticsearch-plugin-deploy

Nice!  I think its worth doing if 2.0 doesn't have work to significantly speed up rolling restarts.  I (almost) only care about plugin isolation for reloading and I only care about that because rolling restarts take many many hours.
</comment><comment author="salyh" created="2015-02-03T18:46:30Z" id="72709356">People from apache karaf (and me) thinking about introducing OSGI to elasticsearch for this and other purposes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexRequestBuilder.setSource(Map&lt;String, Object&gt; source) does not handle BigDecimal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5260</link><project id="" key="" /><description>Although XContentBuilder has support for BigDecimal, the XContentBuilder.writeMap() - or in fact XContentBuilder.writeValue() does not have specific treatment for BigDecimal (or BigInteger)

The result is that BigDecimal gets indexed as a string.
</description><key id="28328109">5260</key><summary>IndexRequestBuilder.setSource(Map&lt;String, Object&gt; source) does not handle BigDecimal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-02-26T12:00:46Z</created><updated>2014-12-29T12:48:07Z</updated><resolved>2014-12-29T12:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2014-04-04T08:30:49Z" id="39542470">see pull request at #5683
</comment><comment author="clintongormley" created="2014-12-29T12:48:07Z" id="68254634">Closing in favour of #5683
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoDistance.Bucket no get min() and max() distance methods in Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5259</link><project id="" key="" /><description>The GeoDistanceFacet.Entry has methods getMin() and getMax() that return the closest and furthest distances to a location used in a GeoDistanceFacetBuilder query.

```
        SearchResponse searchResponse = client.prepareSearch("myIndex").setTypes("myType").setQuery(constantScoreQueryBuilder)
        .addFacet(FacetBuilders.geoDistanceFacet("geoDistanceFacet").field("location").point(latitude, longitude).addRange(0, 50).unit(DistanceUnit.KILOMETERS))
        .execute().actionGet();

        GeoDistanceFacet geoDistanceFacet = (GeoDistanceFacet) searchResponse.getFacets().facetsAsMap().get("geoDistanceFacet");
        for (GeoDistanceFacet.Entry entry : geoDistanceFacet) {
            logger.debug(entry.getFrom());
            logger.debug(entry.getTo());
            logger.debug(entry.getMin());
            logger.debug(entry.getMax());
            logger.debug(entry.getCount());
        }
```

However, the new GeoDistance.Bucket used with aggregations does not have these two methods.

```
        SearchResponse searchResponse = client.prepareSearch("myIndex").setTypes("myType").setQuery(constantScoreQueryBuilder) 
                .addAggregation(terms("myTerm").field("termName").size(10).shardSize(20)
                        .subAggregation(geoDistance("distance")
                                .field("location")
                                .unit(DistanceUnit.KILOMETERS)
                                .lat(latitude)
                                .lon(longitude)
                                .addRange(0, 50)))
                .execute().actionGet();

        GeoDistance geoDist;
        GeoDistance.Bucket bucket;
        for (Terms.Bucket termsBucket : termsBuckets) {
            geoDist = termsBucket.getAggregations().get("distance");            
            bucket = geoDist.getBucketByKey("*-" + DISTANCE);
            logger.debug(bucket.getFrom());
            logger.debug(bucket.getTo());
            //logger.debug(bucket.getMin()); // DOES NOT EXIST
            //logger.debug(bucket.getMax()); // DOES NOT EXIST
            logger.debug(bucket.getDocCount());
        }
```

As such, one cannot get this important data using the new aggregations. Could these please be added.
</description><key id="28322299">5259</key><summary>GeoDistance.Bucket no get min() and max() distance methods in Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AnotherSpecialOne</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2014-02-26T10:18:43Z</created><updated>2014-12-29T14:41:28Z</updated><resolved>2014-12-29T14:41:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T12:47:34Z" id="68254598">@jpountz any particular reason these two methods are no longer supported in aggs?
</comment><comment author="jpountz" created="2014-12-29T13:31:06Z" id="68257268">@clintongormley I don't think we should have these methods on the GeoDistance.Bucket object. This would rather belong to a sub-aggregation I think?
</comment><comment author="clintongormley" created="2014-12-29T14:41:28Z" id="68262495">@jpountz makes sense to me, thanks

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support of field boost in common terms query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5258</link><project id="" key="" /><description>As for now, ES ignores field level boost factors specified in mapping during computation of documents' score for common terms query.
This places certain restrictions on the client of ES. 
E.g., the client has to specify different boost factors manually if he want to have distinct scores for the same term retrieved from two fields.
This quickly becomes unmanageable if the client has no direct control over search fields or if this number grows fast.
The absence of the described functionally looks especially weird comparing to the way how regular terms query works (it does honor field level boosts, see below).

Sample setup:

**1. Create index and mapping.**

``` bash
curl -XPUT 'http://localhost:9200/messages/'
```

``` bash
curl -XPUT 'http://localhost:9200/messages/message/_mapping' -d '
{
    "message" : {
        "properties" : {
            "message" : {"type" : "string", "store" : true },
            "comment" : {"type" : "string", "store" : true , "boost" : 5.0 }
        }
    }
}'
```

**2. Create sample docs**

``` bash
curl -XPUT 'http://localhost:9200/messages/message/1' -d '{
    "user" : "user1",
    "message" : "test message",
    "comment" : "whatever"
}'

curl -XPUT 'http://localhost:9200/messages/message/2' -d '{
    "user" : "user2",
    "message" : "hello world",
    "comment" : "test comment"
}'
```

**3. Wait for ES to be synced**

``` bash
curl -XPOST 'http://localhost:9200/messages/_refresh' 
```

**4. Search in default catch-all field using term query**

``` bash
curl -XPOST 'http://localhost:9200/messages/_search' -d '{ "query" : { "query_string" : { "query" : "test" } } , "explain" : true }' | python -mjson.tool
```

**5. Search in default catch-all field using common terms query**

``` bash
curl -XPOST 'http://localhost:9200/messages/_search' -d '{ "query" : { "common" : { "_all" : { "query" : "test" } } } , "explain" : true }' | python -mjson.tool
```

The first query (step 4) works as expected -- document with id equal to 2 has higher score.
The result of second query (step 5) is opposite.
</description><key id="28279793">5258</key><summary>Add support of field boost in common terms query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ei82</reporter><labels><label>bug</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T20:22:36Z</created><updated>2014-03-27T16:03:37Z</updated><resolved>2014-02-27T14:06:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-26T16:30:45Z" id="36144806">ok I see the problem here I guess I have to fix that in lucene as well.... thanks for reporting this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify range aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5257</link><project id="" key="" /><description>[DOCS] Add note to range aggregations for to and from values.
</description><key id="28275572">5257</key><summary>Clarify range aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels /><created>2014-02-25T19:30:19Z</created><updated>2014-07-16T21:48:06Z</updated><resolved>2014-02-28T19:54:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-26T09:05:52Z" id="36104735">+1
</comment><comment author="javanna" created="2014-02-28T21:42:11Z" id="36397387">Doesn't this need to be backported to 1.x and 1.0?
</comment><comment author="uboness" created="2014-02-28T23:29:24Z" id="36405520">Only to 1.x   

On 28 February 2014 13:42:14 GMT-8, Luca Cavanna notifications@github.com wrote:Doesn't this need to be backported to 1.x and 1.0?  &#8212;Reply to this email directly or view it on GitHub.
</comment><comment author="bly2k" created="2014-03-03T23:51:21Z" id="36576193">I just pushed to 1.x, btw. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible  issue with dynamic mapping in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5256</link><project id="" key="" /><description>ElasticSearch 1.0.

When using dynamic_template described here:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates

the MapperParsingException occurs. The gist with an example identical as in documentation is here:

https://gist.github.com/nnegativ/9213854
</description><key id="28267284">5256</key><summary>Possible  issue with dynamic mapping in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nnegativ</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T17:43:06Z</created><updated>2014-04-01T08:30:53Z</updated><resolved>2014-03-28T13:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>geo_distance - can't filter by inner (object) type ` UncheckedExecutionException - java.lang.NumberFormatException`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5255</link><project id="" key="" /><description>According to [docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-geo-distance-filter.html) I should be able to filter by inner (not nested) object attribute (in my case place.point)

My mapping looks like:

``` javascript
{
  "proposalsets": {
    "_parent": {
      "type": "service"
    },
    "_ttl": {
      "enabled": true
    },
    "properties": {
      "place": {
        "type": "object",
        "properties": {
          "name": {
            "type": "string",
            "index": "analyzed",
            "analyzer": "polish",
            "fields": {
              "raw": {
                "type": "string",
                "index": "not_analyzed"
              }
            }
          },
          "point": {
            "type": "geo_point",
            "fielddata": {
              "format": "compressed",
              "precision": "3m"
            }
          },
          "city": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      }
    }
  }
}
```

The query I am trygin to execute is :

``` javascript
{
   "query": {
      "filtered": {
         "filter": {
            "geo_distance": {
               "distance": "100km",
               "place.point": {
                  "lat": 40.73,
                  "lon": -74.1
               }
            }
         }
      }
   }
}
```

``` javascript
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[OnU2dSO5ROKwYQWpQzpmEA][pl][5]: QueryPhaseExecutionException[[pl][5]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"@\u0018B\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"@\u0018B\"]; nested: NumberFormatException[For input string: \"@\u0018B\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][4]: QueryPhaseExecutionException[[pl][4]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: empty String]; nested: UncheckedExecutionException[java.lang.NumberFormatException: empty String]; nested: NumberFormatException[empty String]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][7]: QueryPhaseExecutionException[[pl][7]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"(`\u0012:v\n~\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"(`\u0012:v\n~\"]; nested: NumberFormatException[For input string: \"(`\u0012:v\n~\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][6]: QueryPhaseExecutionException[[pl][6]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"@%\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"@%\"]; nested: NumberFormatException[For input string: \"@%\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][1]: QueryPhaseExecutionException[[pl][1]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\"]; nested: NumberFormatException[For input string: \"$\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][0]: QueryPhaseExecutionException[[pl][0]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"@\u0019&amp;\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"@\u0019&amp;\"]; nested: NumberFormatException[For input string: \"@\u0019&amp;\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][3]: QueryPhaseExecutionException[[pl][3]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\f\u0001C\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\f\u0001C\"]; nested: NumberFormatException[For input string: \"$\f\u0001C\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][2]: QueryPhaseExecutionException[[pl][2]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\"]; nested: NumberFormatException[For input string: \"$\"]; }]",
   "status": 500
}
```

This happens even when the index is empty (but has mapping). 
When I move the point from the place to the root of the document, there seems to be no problem.
</description><key id="28254478">5255</key><summary>geo_distance - can't filter by inner (object) type ` UncheckedExecutionException - java.lang.NumberFormatException`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">g00fy-</reporter><labels /><created>2014-02-25T15:14:26Z</created><updated>2014-12-29T12:45:07Z</updated><resolved>2014-12-29T12:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-26T09:43:47Z" id="36107562">hey,

I just tried this with 1.0.1 and it worked... (didnt return any results on an empty index as expected).
Can you provide a full recreation please, so I can check what happens? Please start with a completely empty elasticsearch and show all the curl calls you did in order to reach the above state? I just might have missed something to reproduce

Also please provide information which elasticsearch version you are using (or better, try with the latest 0.90 or 1.0 one).

Thanks a lot!
</comment><comment author="g00fy-" created="2014-02-26T12:09:56Z" id="36118595">@spinscale 

``` javascript
{
number: "1.0.0",
build_hash: "a46900e9c72c0a623d71b54016357d5f94c8ea32",
build_timestamp: "2014-02-12T16:18:34Z",
build_snapshot: false,
lucene_version: "4.6"
}
```

I will try to reproduce the same bug on clean install. 
However I did remove the type and recreated the mappings (didn't delete the index though).

Edit:
So I tried to do the same thing on a clean index - this time it worked. 
_Is it possible this may be caused by templates?_

Edit (again)
1. I deleted the template and the mapping 
2. I run the query on empty type (OK)
3. I started to pipe some data in to ES
4. During the data beeing sent to ES, I run the query few times (OK) but suddenly the same error was thrown

I think this is caused by the fact that elasticsearch accepts invalid data.

``` javascript
QueryPhaseExecutionException[[pl][2]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 1.0E7, 40.73, -74.1)))-&gt;cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: "$"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: "$"]; nested: NumberFormatException[For input string: "$"];
```
</comment><comment author="g00fy-" created="2014-02-28T11:11:07Z" id="36340956">I could not reproduce this error on a clean cluster.
</comment><comment author="spinscale" created="2014-02-28T14:51:03Z" id="36357150">Hey,

do you have by chance the complete stacktrace of the SearchPhaseExecutionException from your first post in your logfiles? If so, could you attach it here?
</comment><comment author="g00fy-" created="2014-04-29T13:54:43Z" id="41678285">After a few weeks running with out this problem this it came up again today with 1.1.1 version. But this time only 4 of 5 shards failed to execute the query:

```
{
   "took": 11,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 1,
      "failed": 4,
      "failures": [
         {
            "index": "pl",
            "shard": 4,
            "status": 500,
            "reason": "QueryPhaseExecutionException[[pl][4]: query[filtered(ConstantScore(GeoDistanceFilter(places.point, SLOPPY_ARC, 20000.0, 52.0, 21.0)) +(name:carolina^3.0 _all:carolina))-&gt;cache(_type:account)],from[0],size[20]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: NumberFormatException[For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; "
         },
         {
            "index": "pl",
            "shard": 1,
            "status": 500,
            "reason": "QueryPhaseExecutionException[[pl][1]: query[filtered(ConstantScore(GeoDistanceFilter(places.point, SLOPPY_ARC, 20000.0, 52.0, 21.0)) +(name:carolina^3.0 _all:carolina))-&gt;cache(_type:account)],from[0],size[20]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: NumberFormatException[For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; "
         },
         {
            "index": "pl",
            "shard": 0,
            "status": 500,
            "reason": "QueryPhaseExecutionException[[pl][0]: query[filtered(ConstantScore(GeoDistanceFilter(places.point, SLOPPY_ARC, 20000.0, 52.0, 21.0)) +(name:carolina^3.0 _all:carolina))-&gt;cache(_type:account)],from[0],size[20]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: NumberFormatException[For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; "
         },
         {
            "index": "pl",
            "shard": 2,
            "status": 500,
            "reason": "QueryPhaseExecutionException[[pl][2]: query[filtered(ConstantScore(GeoDistanceFilter(places.point, SLOPPY_ARC, 20000.0, 52.0, 21.0)) +(name:carolina^3.0 _all:carolina))-&gt;cache(_type:account)],from[0],size[20]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; nested: NumberFormatException[For input string: \"$\f\u0002(\u0016\u000bP\u0011f\"]; "
         }
      ]
   },
```
</comment><comment author="spinscale" created="2014-04-29T14:16:44Z" id="41680994">Hey there. Has this index been freshly created? do you have a stacktrace in the logs that you can attach? Is this now reproducible with every request?
</comment><comment author="g00fy-" created="2014-04-29T14:44:44Z" id="41684656">@spinscale this error is not reproducible. It comes on random. As you see only 4 of 5 shards failed to execute the query. 
I can even rebuild the index with the same input data and it won't raise this exception.

I am sure that the mapping I have provided for the index all include geo_point for `point` so I have no idea what to look for.
</comment><comment author="spinscale" created="2014-04-30T14:39:10Z" id="41804080">hm, weird. Lets keep this one open and think if there are any possibilities of reproducing this weird behaviour.. thanks for notifying!

In case you find any emerging pattern, what could cause this, please add it here! I'll try to take a look together with other developers
</comment><comment author="g00fy-" created="2014-05-15T13:04:02Z" id="43206193">@spinscale What is happening is very wierd. 
What I have done:
1. We update the index daily, exporting almost the same data.
2. On every nth day the index breaks (only the geo_distance query)
3. We can reindex the data, and sometimes this helps
4. We have checked the data manually, and it seems correct (no strage data there, all valid points in format `{"lon":50,"lat":23}` (some documents have null values)

The process of checking the data in the index goes as follows:
1. Check what shard is failing the query ( in our case the 1st shard)
2. Run the query on the selected shard (?preference=_shards:1)
3. Purge some data from the index 
4. Repeat step 2. and check if query runs ok

We end up with 1 document left in the shard - and it had valid point data, so to make shure I also restarted elasticsearch. Even with the single document in the shard, it still failed to execute the query.

After that we deleted the last document and then the query run with no errors.

We could reindex the data again (exacly the same data that we deleted) and elasticsearch didn't fail to execute the query.

I have no idea what can be happening inside. 

Is there any way I could debug this problem ?
</comment><comment author="spinscale" created="2014-05-16T09:07:13Z" id="43311502">When this happens again, can you clear all the caches first for that index and see if the problems persists? See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-clearcache.html#indices-clearcache
</comment><comment author="g00fy-" created="2014-05-16T12:49:04Z" id="43327085">@spinscale clearing the cache didn't help
</comment><comment author="sshamov" created="2014-09-08T09:35:29Z" id="54795468">I managed to get [the steps to reproduce the problem](https://gist.github.com/sshamov/7097e75857aac94a32c9), which is very similar to this.
</comment><comment author="clintongormley" created="2014-09-08T09:44:52Z" id="54796528">Hi @sshamov 

In your gist, you delete the mapping for `type2` then reindex the document, which makes `location` type `double`.  This clashes with the loading of fielddata in the next query.

Fields with the same name in different types in the same index must have the same mapping.
</comment><comment author="sshamov" created="2014-09-08T09:59:32Z" id="54797858">Really? Didn't know that. I thought the bug is that a different mapping in different types is not handled correctly.
</comment><comment author="clintongormley" created="2014-09-08T10:11:56Z" id="54798964">@sshamov in a future version of ES, we're planning on enforcing this, to avoid exactly these errors.  See #4081
</comment><comment author="charlez" created="2014-09-30T18:48:45Z" id="57362813">I've observed this problem when issuing search_type=scan with a geo_distance filter.

The problem can arise when a temporary search result set is written with the URL for the search result set specifying a different type.  Writing/creating the same document structure to the temporary search result set is problematic, because dynamic mapping doesn't create a geo_point for the lon/lat term within the search result set.

At first I didn't realize my scenario is essentially the case described above (and to be addressed by &lt;a href="https://github.com/elasticsearch/elasticsearch/issues/4081"&gt;#4081&lt;/a&gt;).
</comment><comment author="clintongormley" created="2014-12-29T12:45:07Z" id="68254430">Closing in favour of #8870
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove thread local recycler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5254</link><project id="" key="" /><description>the thread local recycler requires obtain and recycle to be called on the same thread, while other recyclers do not. Also, it can create heavy recycle usage since it depends on the threads that its being used on. The concurrent / pinned thread base one is by far better than the pure thread local (and is the default) one since it more easily bounds the elements recycled, while still allowing to mix obtain and recycle across threads.

We will end up using the paged recyclers more and more, for example, in our networking output buffer, where obtaining will happen on one thread, while recycling can potentially occur on another thread (the callback thread). Since the limit of binding to a thread of the 2 calls is not really needed, and our best implementation supports going cross threads, there is no real need to impose this restriction.
</description><key id="28247331">5254</key><summary>Remove thread local recycler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T13:36:26Z</created><updated>2015-06-07T15:12:07Z</updated><resolved>2014-02-25T14:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-25T13:50:15Z" id="36008532">+1
</comment><comment author="kimchy" created="2014-02-25T14:00:44Z" id="36009478">pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting on SingleBucketAggregation's should be possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5253</link><project id="" key="" /><description>Aggregations are a really great feature to create tableviews of analytics with ease. And the support for sorting on sub-aggregations gives the user the freedom to sort on any given column like he/she is used to. Although not every metric thinkable of can be used in sorts.

For example, if you have two columns where you show the number of males and females occuring with a term you would get a facet like this:

```
{
  "size": 0,
  "aggs": {
    "someterm": {
      "terms": {
        "field": "somefield"
      },
      "aggs": {
        "male": {
          "filter": {
            "term": {
              "gender": "male"
            }
          }
        },
        "female": {
          "filter": {
            "term": {
              "gender": "female"
            }
          }
        }
      }
    }
  }   
}
```

Unfortunately as soon as you want to sort the terms on a specific gender you get a message that it is unable to do so since the filter aggregation is not of a metric type.

Looking in to the code I think it would be technically possible to sort on the count of a filtered aggregation but is not yet implemented. I purpose a way to sort on sub aggregations of terms by its path.

Take the following aggregation:

```
{
  "size": 0,
  "aggs": {
    "someterm": {
      "terms": {
        "field": "somefield",
        "order": {
          "male._count": "desc" // would count on the doc_count of the 'male' sub-aggregation in a descending way
        }
      },
      "aggs": {
        "male": {
          "filter": {
            "term": {
              "gender": "male"
            }
          }
        },
        "female": {
          "filter": {
            "term": {
              "gender": "female"
            }
          }
        }
      }
    }
  }   
}
```

Currently it responds with an error explaining the sort can only be on metric aggregations. This seems a limitation which is not technical. The only limitation is that comparators for `MetricsAggregation` are implemented in `InternalOrder`, and for that matter in `MultiBucketsAggregation`.

By implementing a sorter for instances of `SingleBucketAggregation` we can sort not only on the count, but also on aggregations within this filtered aggregation (eg. an average of a filtered aggregation) by specifying a path to the aggregation the same way a path is provided now but with an extra element in the path.

```
{
  "size": 0,
  "aggs": {
    "some_agg": {
      "terms": {
        "field": "some_field",
        "order": {
          "count_of_a.again_some_field.avg": "desc"
        }
      },
      "aggs": {
        "count_of_a": {
          "filter": {
            "term": {
              "some_other_field": "a"
            }
          },
          "aggs": {
            "again_some_field": {
              "stats": {
                "field": "again_some_field"
              }
            }
          }
        }
      }
    }
  }
}
```

In this example you would sort on the average value of `again_some_field` filtered where `some_other_field` has a value of `a`.

To show that it is possible I [hacked together](https://github.com/thanodnl/elasticsearch/commit/61993a33a7073b94b3f813478630a72a11c01c17) a POC, but it turned out to be more complex than I imagened at first. It supports atleast the sorting on the count of a filter-aggregation, and even the sub-metrics of such a filter. I hope you can pick this up in the roadmap of Elasticsearch v1.*
</description><key id="28244314">5253</key><summary>Sorting on SingleBucketAggregation's should be possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thanodnl</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T12:44:34Z</created><updated>2014-07-03T10:00:46Z</updated><resolved>2014-03-05T23:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="felixbarny" created="2014-07-02T16:22:07Z" id="47798627">From release notes:

&gt; Aggregations: aggregation names can now only contain alpha-numeric, hyphen (&#8220;-&#8221;) and underscore (&#8220;_&#8221;) characters, due to the enhancement which allows sub-aggregation sorting #5253

Not understaning the reason why only alpa-numeric chars are allowed, wouldn't it be possible to allow any char by internally working with hexadecimal encoded strings? I would really wish, that arbitrary chars would still be allowed. My current workaround is to do the hex encoding manually. My aggregation names are computed dynamically and are displayed on the frontend, so avoiding non-alpha-numerics is not an option for me.
</comment><comment author="colings86" created="2014-07-03T09:01:18Z" id="47882813">Opened issue #6702 to address the above comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Class name typo.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5252</link><project id="" key="" /><description /><key id="28242519">5252</key><summary>Class name typo.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels /><created>2014-02-25T12:11:41Z</created><updated>2014-07-16T21:48:07Z</updated><resolved>2014-02-26T08:57:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-26T08:57:23Z" id="36104147">merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor cleanups to test comments.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5251</link><project id="" key="" /><description /><key id="28241670">5251</key><summary>Minor cleanups to test comments.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels /><created>2014-02-25T11:55:51Z</created><updated>2014-07-16T21:48:09Z</updated><resolved>2014-02-26T08:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-26T08:50:59Z" id="36103619">LGTM
</comment><comment author="s1monw" created="2014-02-26T08:57:33Z" id="36104163">merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE/AIOOBE when building a bucket which has not been collected.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5250</link><project id="" key="" /><description>Close #5048
</description><key id="28239947">5250</key><summary>Fix NPE/AIOOBE when building a bucket which has not been collected.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T11:25:08Z</created><updated>2015-06-07T23:28:57Z</updated><resolved>2014-03-04T08:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item></channel></rss>